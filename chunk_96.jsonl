{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_manifest_list(image, registry, insecure=False, dockercfg_path=None):\n    version = 'v2_list'\n    registry_session = RegistrySession(registry, insecure=insecure, dockercfg_path=dockercfg_path)\n    response, _ = get_manifest(image, registry_session, version)\n    return response", "response": "Get manifest list for image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_manifests(image, registry, insecure=False, dockercfg_path=None,\n                      versions=('v1', 'v2', 'v2_list')):\n    \"\"\"Return manifest digests for image.\n\n    :param image: ImageName, the remote image to inspect\n    :param registry: str, URI for registry, if URI schema is not provided,\n                          https:// will be used\n    :param insecure: bool, when True registry's cert is not verified\n    :param dockercfg_path: str, dirname of .dockercfg location\n    :param versions: tuple, for which manifest schema versions to fetch manifests\n\n    :return: dict of successful responses, with versions as keys\n    \"\"\"\n    digests = {}\n    registry_session = RegistrySession(registry, insecure=insecure, dockercfg_path=dockercfg_path)\n    for version in versions:\n        response, _ = get_manifest(image, registry_session, version)\n        if response:\n            digests[version] = response\n\n    return digests", "response": "Get all manifest digests for the given image."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget inspect for an image.", "response": "def get_inspect_for_image(image, registry, insecure=False, dockercfg_path=None):\n    \"\"\"Return inspect for image.\n\n    :param image: ImageName, the remote image to inspect\n    :param registry: str, URI for registry, if URI schema is not provided,\n                          https:// will be used\n    :param insecure: bool, when True registry's cert is not verified\n    :param dockercfg_path: str, dirname of .dockercfg location\n\n    :return: dict of inspected image\n    \"\"\"\n    all_man_digests = get_all_manifests(image, registry, insecure=insecure,\n                                        dockercfg_path=dockercfg_path)\n    blob_config = None\n    config_digest = None\n    image_inspect = {}\n\n    # we have manifest list (get digest for 1st platform)\n    if 'v2_list' in all_man_digests:\n        man_list_json = all_man_digests['v2_list'].json()\n        if man_list_json['manifests'][0]['mediaType'] != MEDIA_TYPE_DOCKER_V2_SCHEMA2:\n            raise RuntimeError('Image {image_name}: v2 schema 1 '\n                               'in manifest list'.format(image_name=image))\n\n        v2_digest = man_list_json['manifests'][0]['digest']\n        blob_config, config_digest = get_config_and_id_from_registry(image, registry, v2_digest,\n                                                                     insecure=insecure,\n                                                                     version='v2',\n                                                                     dockercfg_path=dockercfg_path)\n    # get config for v2 digest\n    elif 'v2' in all_man_digests:\n        blob_config, config_digest = get_config_and_id_from_registry(image, registry, image.tag,\n                                                                     insecure=insecure,\n                                                                     version='v2',\n                                                                     dockercfg_path=dockercfg_path)\n    # read config from v1\n    elif 'v1' in all_man_digests:\n        v1_json = all_man_digests['v1'].json()\n        if PY2:\n            blob_config = json.loads(v1_json['history'][0]['v1Compatibility'].decode('utf-8'))\n        else:\n            blob_config = json.loads(v1_json['history'][0]['v1Compatibility'])\n    else:\n        raise RuntimeError(\"Image {image_name} not found: No v2 schema 1 image, or v2 schema 2 \"\n                           \"image or list, found\".format(image_name=image))\n\n    # dictionary to convert config keys to inspect keys\n    config_2_inspect = {\n        'created': 'Created',\n        'os': 'Os',\n        'container_config': 'ContainerConfig',\n        'architecture': 'Architecture',\n        'docker_version': 'DockerVersion',\n        'config': 'Config',\n    }\n\n    if not blob_config:\n        raise RuntimeError(\"Image {image_name}: Couldn't get inspect data \"\n                           \"from digest config\".format(image_name=image))\n\n    # set Id, which isn't in config blob, won't be set for v1, as for that image has to be pulled\n    image_inspect['Id'] = config_digest\n    # only v2 has rootfs, not v1\n    if 'rootfs' in blob_config:\n        image_inspect['RootFS'] = blob_config['rootfs']\n\n    for old_key, new_key in config_2_inspect.items():\n        image_inspect[new_key] = blob_config[old_key]\n\n    return image_inspect"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the config and id of a given image from a given registry", "response": "def get_config_and_id_from_registry(image, registry, digest, insecure=False,\n                                    dockercfg_path=None, version='v2'):\n    \"\"\"Return image config by digest\n\n    :param image: ImageName, the remote image to inspect\n    :param registry: str, URI for registry, if URI schema is not provided,\n                          https:// will be used\n    :param digest: str, digest of the image manifest\n    :param insecure: bool, when True registry's cert is not verified\n    :param dockercfg_path: str, dirname of .dockercfg location\n    :param version: str, which manifest schema versions to fetch digest\n\n    :return: dict, versions mapped to their digest\n    \"\"\"\n    registry_session = RegistrySession(registry, insecure=insecure, dockercfg_path=dockercfg_path)\n\n    response = query_registry(\n        registry_session, image, digest=digest, version=version)\n    response.raise_for_status()\n    manifest_config = response.json()\n    config_digest = manifest_config['config']['digest']\n\n    config_response = query_registry(\n        registry_session, image, digest=config_digest, version=version, is_blob=True)\n    config_response.raise_for_status()\n\n    blob_config = config_response.json()\n\n    context = '/'.join([x for x in [image.namespace, image.repo] if x])\n    tag = image.tag\n    logger.debug('Image %s:%s has config:\\n%s', context, tag, blob_config)\n\n    return blob_config, config_digest"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config_from_registry(image, registry, digest, insecure=False,\n                             dockercfg_path=None, version='v2'):\n    \"\"\"Return image config by digest\n\n    :param image: ImageName, the remote image to inspect\n    :param registry: str, URI for registry, if URI schema is not provided,\n                          https:// will be used\n    :param digest: str, digest of the image manifest\n    :param insecure: bool, when True registry's cert is not verified\n    :param dockercfg_path: str, dirname of .dockercfg location\n    :param version: str, which manifest schema versions to fetch digest\n\n    :return: dict, versions mapped to their digest\n    \"\"\"\n    blob_config, _ = get_config_and_id_from_registry(image, registry, digest, insecure=insecure,\n                                                     dockercfg_path=dockercfg_path,\n                                                     version=version)\n    return blob_config", "response": "Get the image config by digest from a given registry"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps for Dockerfile_parse s DockerfileParser that takes into account parent image information", "response": "def df_parser(df_path, workflow=None, cache_content=False, env_replace=True, parent_env=None):\n    \"\"\"\n    Wrapper for dockerfile_parse's DockerfileParser that takes into account\n    parent_env inheritance.\n\n    :param df_path: string, path to Dockerfile (normally in DockerBuildWorkflow instance)\n    :param workflow: DockerBuildWorkflow object instance, used to find parent image information\n    :param cache_content: bool, tells DockerfileParser to cache Dockerfile content\n    :param env_replace: bool, replace ENV declarations as part of DockerfileParser evaluation\n    :param parent_env: dict, parent ENV key:value pairs to be inherited\n\n    :return: DockerfileParser object instance\n    \"\"\"\n\n    p_env = {}\n\n    if parent_env:\n        # If parent_env passed in, just use that\n        p_env = parent_env\n\n    elif workflow:\n\n        # If parent_env is not provided, but workflow is then attempt to inspect\n        # the workflow for the parent_env\n\n        try:\n            parent_config = workflow.builder.base_image_inspect[INSPECT_CONFIG]\n        except (AttributeError, TypeError, KeyError):\n            logger.debug(\"base image unable to be inspected\")\n        else:\n            try:\n                tmp_env = parent_config[\"Env\"]\n                logger.debug(\"Parent Config ENV: %s\" % tmp_env)\n\n                if isinstance(tmp_env, dict):\n                    p_env = tmp_env\n                elif isinstance(tmp_env, list):\n                    try:\n                        for key_val in tmp_env:\n                            key, val = key_val.split(\"=\", 1)\n                            p_env[key] = val\n\n                    except ValueError:\n                        logger.debug(\"Unable to parse all of Parent Config ENV\")\n\n            except KeyError:\n                logger.debug(\"Parent Environment not found, not applied to Dockerfile\")\n\n    try:\n        dfparser = DockerfileParser(\n            df_path,\n            cache_content=cache_content,\n            env_replace=env_replace,\n            parent_env=p_env\n        )\n    except TypeError:\n        logger.debug(\"Old version of dockerfile-parse detected, unable to set inherited parent \"\n                     \"ENVs\")\n        dfparser = DockerfileParser(\n            df_path,\n            cache_content=cache_content,\n            env_replace=env_replace,\n        )\n\n    return dfparser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef are_plugins_in_order(plugins_conf, *plugins_names):\n    all_plugins_names = [plugin['name'] for plugin in plugins_conf or []]\n    start_index = 0\n    for plugin_name in plugins_names:\n        try:\n            start_index = all_plugins_names.index(plugin_name, start_index)\n        except ValueError:\n            return False\n    return True", "response": "Check if plugins are configured in given order."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_yaml(yaml_data, schema):\n    try:\n        resource = resource_stream('atomic_reactor', schema)\n        schema = codecs.getreader('utf-8')(resource)\n    except (IOError, TypeError):\n        logger.error('unable to extract JSON schema, cannot validate')\n        raise\n\n    try:\n        schema = json.load(schema)\n    except ValueError:\n        logger.error('unable to decode JSON schema, cannot validate')\n        raise\n    data = yaml.safe_load(yaml_data)\n    validator = jsonschema.Draft4Validator(schema=schema)\n    try:\n        jsonschema.Draft4Validator.check_schema(schema)\n        validator.validate(data)\n    except jsonschema.SchemaError:\n        logger.error('invalid schema, cannot validate')\n        raise\n    except jsonschema.ValidationError:\n        for error in validator.iter_errors(data):\n            path = \"\".join(\n                ('[{}]' if isinstance(element, int) else '.{}').format(element)\n                for element in error.path\n            )\n\n            if path.startswith('.'):\n                path = path[1:]\n\n            logger.error('validation error (%s): %s', path or 'at top level', error.message)\n\n        raise\n\n    return data", "response": "Reads the given yaml data and validates it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_parent_image_koji_data(workflow):\n    koji_parent = workflow.prebuild_results.get(PLUGIN_KOJI_PARENT_KEY) or {}\n    image_metadata = {}\n\n    parents = {}\n    for img, build in (koji_parent.get(PARENT_IMAGES_KOJI_BUILDS) or {}).items():\n        if not build:\n            parents[str(img)] = None\n        else:\n            parents[str(img)] = {key: val for key, val in build.items() if key in ('id', 'nvr')}\n    image_metadata[PARENT_IMAGE_BUILDS_KEY] = parents\n\n    # ordered list of parent images\n    image_metadata[PARENT_IMAGES_KEY] = workflow.builder.parents_ordered\n\n    # don't add parent image id key for scratch\n    if workflow.builder.base_from_scratch:\n        return image_metadata\n\n    base_info = koji_parent.get(BASE_IMAGE_KOJI_BUILD) or {}\n    parent_id = base_info.get('id')\n    if parent_id is not None:\n        try:\n            parent_id = int(parent_id)\n        except ValueError:\n            logger.exception(\"invalid koji parent id %r\", parent_id)\n        else:\n            image_metadata[BASE_IMAGE_BUILD_ID_KEY] = parent_id\n    return image_metadata", "response": "Transform koji_parent plugin results into metadata dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_item(self, item):\n        # append here just in case .get bellow fails\n        self._parsed_logs.append(item)\n\n        # make sure the log item is a dictionary object\n        if isinstance(item, dict):\n            lines = item.get(\"stream\", \"\")\n        else:\n            lines = item\n            item = None\n\n        for line in lines.splitlines():\n            line = line.strip()\n            self._logs.append(line)\n            if line:\n                logger.debug(line)\n\n        if item is not None:\n            self._error = item.get(\"error\", None)\n            self._error_detail = item.get(\"errorDetail\", None)\n            if self._error:\n                logger.error(item)", "response": "Parse the log item and store the result in the internal list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode and unpack base64 auth credentials from config file.", "response": "def unpack_auth_b64(self, docker_registry):\n        \"\"\"Decode and unpack base64 'auth' credentials from config file.\n\n        :param docker_registry: str, registry reference in config file\n\n        :return: namedtuple, UnpackedAuth (or None if no 'auth' is available)\n        \"\"\"\n        UnpackedAuth = namedtuple('UnpackedAuth', ['raw_str', 'username', 'password'])\n        credentials = self.get_credentials(docker_registry)\n        auth_b64 = credentials.get('auth')\n        if auth_b64:\n            raw_str = b64decode(auth_b64).decode('utf-8')\n            unpacked_credentials = raw_str.split(':', 1)\n            if len(unpacked_credentials) == 2:\n                return UnpackedAuth(raw_str, *unpacked_credentials)\n            else:\n                raise ValueError(\"Failed to parse 'auth' in '%s'\" % self.json_secret_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default manifest schema version.", "response": "def default(self):\n        \"\"\"Return the default manifest schema version.\n\n        Depending on the docker version, <= 1.9, used to push\n        the image to the registry, v2 schema may not be available.\n        In such case, the v1 schema should be used when interacting\n        with the registry. An OCI digest will only be present when\n        the manifest was pushed as an OCI digest.\n        \"\"\"\n        return self.v2_list or self.oci_index or self.oci or self.v2 or self.v1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of log files from server and return them as a list of Output objects", "response": "def get_log_files(self, osbs, build_id):\n        \"\"\"\n        Build list of log files\n\n        :return: list, of log files\n        \"\"\"\n\n        logs = None\n        output = []\n\n        # Collect logs from server\n        try:\n            logs = osbs.get_orchestrator_build_logs(build_id)\n        except OsbsException as ex:\n            self.log.error(\"unable to get build logs: %r\", ex)\n            return output\n        except TypeError:\n            # Older osbs-client has no get_orchestrator_build_logs\n            self.log.error(\"OSBS client does not support get_orchestrator_build_logs\")\n            return output\n\n        platform_logs = {}\n        for entry in logs:\n            platform = entry.platform\n            if platform not in platform_logs:\n                filename = 'orchestrator' if platform is None else platform\n                platform_logs[platform] = NamedTemporaryFile(prefix=\"%s-%s\" %\n                                                             (build_id, filename),\n                                                             suffix=\".log\", mode='r+b')\n            platform_logs[platform].write((entry.line + '\\n').encode('utf-8'))\n\n        for platform, logfile in platform_logs.items():\n            logfile.flush()\n            filename = 'orchestrator' if platform is None else platform\n            metadata = self.get_log_metadata(logfile.name, \"%s.log\" % filename)\n            output.append(Output(file=logfile, metadata=metadata))\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_image_digest(self, image, platform, digest):\n        image_name_tag = self._key(image)\n\n        image_name = image.to_str(tag=False)\n        name_digest = '{}@{}'.format(image_name, digest)\n\n        image_digests = self._images_digests.setdefault(image_name_tag, {})\n        image_digests[platform] = name_digest", "response": "Update the parent image digest for specific platform"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_image_digests(self, image):\n        image_name_tag = self._key(image)\n        image_digests = self._images_digests.get(image_name_tag)\n        if image_digests is None:\n            raise KeyError('Image {} has no digest records'.format(image_name_tag))\n        return image_digests", "response": "Get the platform digests of the specified image"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_image_platform_digest(self, image, platform):\n        image_digests = self.get_image_digests(image)\n        digest = image_digests.get(platform)\n        if digest is None:\n            raise KeyError(\n                'Image {} has no digest record for platform {}'.format(image, platform)\n            )\n\n        return digest", "response": "Get the digest of the specified image and platform"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_from_dict(self, source):\n        assert isinstance(source, dict)\n        source_copy = deepcopy(source)  # no mutable side effects\n        self._images_digests.update(source_copy)", "response": "Update the records of the digests of images from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rpm_qf_args(tags=None, separator=';'):\n\n    if tags is None:\n        tags = image_component_rpm_tags\n\n    fmt = separator.join([\"%%{%s}\" % tag for tag in tags])\n    return r\"-qa --qf '{0}\\n'\".format(fmt)", "response": "Return the arguments to pass to rpm to list RPMs in the format expected by parse_rpm_output by parse_rpm_output by parse_rpm_output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_rpm_output(output, tags=None, separator=';'):\n\n    if tags is None:\n        tags = image_component_rpm_tags\n\n    def field(tag):\n        \"\"\"\n        Get a field value by name\n        \"\"\"\n        try:\n            value = fields[tags.index(tag)]\n        except ValueError:\n            return None\n\n        if value == '(none)':\n            return None\n\n        return value\n\n    components = []\n    sigmarker = 'Key ID '\n    for rpm in output:\n        fields = rpm.rstrip('\\n').split(separator)\n        if len(fields) < len(tags):\n            continue\n\n        signature = field('SIGPGP:pgpsig') or field('SIGGPG:pgpsig')\n        if signature:\n            parts = signature.split(sigmarker, 1)\n            if len(parts) > 1:\n                signature = parts[1]\n\n        component_rpm = {\n            'type': 'rpm',\n            'name': field('NAME'),\n            'version': field('VERSION'),\n            'release': field('RELEASE'),\n            'arch': field('ARCH'),\n            'sigmd5': field('SIGMD5'),\n            'signature': signature,\n        }\n\n        # Special handling for epoch as it must be an integer or None\n        epoch = field('EPOCH')\n        if epoch is not None:\n            epoch = int(epoch)\n\n        component_rpm['epoch'] = epoch\n\n        if component_rpm['name'] != 'gpg-pubkey':\n            components.append(component_rpm)\n\n    return components", "response": "Parse the output of the rpm query and return a list of dicts describing each rpm package."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchooses the correct login method based on the available credentials, and call that method on the provided session object. :param session: koji.ClientSession instance :param proxyuser: str, proxy user :param ssl_certs_dir: str, path to \"cert\" (required), and \"serverca\" (optional) :param krb_principal: str, name of Kerberos principal :param krb_keytab: str, Kerberos keytab :return: None", "response": "def koji_login(session,\n               proxyuser=None,\n               ssl_certs_dir=None,\n               krb_principal=None,\n               krb_keytab=None):\n    \"\"\"\n    Choose the correct login method based on the available credentials,\n    and call that method on the provided session object.\n\n    :param session: koji.ClientSession instance\n    :param proxyuser: str, proxy user\n    :param ssl_certs_dir: str, path to \"cert\" (required), and \"serverca\" (optional)\n    :param krb_principal: str, name of Kerberos principal\n    :param krb_keytab: str, Kerberos keytab\n    :return: None\n    \"\"\"\n\n    kwargs = {}\n    if proxyuser:\n        kwargs['proxyuser'] = proxyuser\n\n    if ssl_certs_dir:\n        # Use certificates\n        logger.info(\"Using SSL certificates for Koji authentication\")\n        kwargs['cert'] = os.path.join(ssl_certs_dir, 'cert')\n\n        # serverca is not required in newer versions of koji, but if set\n        # koji will always ensure file exists\n        # NOTE: older versions of koji may require this to be set, in\n        # that case, make sure serverca is passed in\n        serverca_path = os.path.join(ssl_certs_dir, 'serverca')\n        if os.path.exists(serverca_path):\n            kwargs['serverca'] = serverca_path\n\n        # Older versions of koji actually require this parameter, even though\n        # it's completely ignored.\n        kwargs['ca'] = None\n\n        result = session.ssl_login(**kwargs)\n    else:\n        # Use Kerberos\n        logger.info(\"Using Kerberos for Koji authentication\")\n        if krb_principal and krb_keytab:\n            kwargs['principal'] = krb_principal\n            kwargs['keytab'] = krb_keytab\n\n        result = session.krb_login(**kwargs)\n\n    if not result:\n        raise RuntimeError('Unable to perform Koji authentication')\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_koji_session(hub_url, auth_info=None):\n    session = KojiSessionWrapper(koji.ClientSession(hub_url, opts={'krb_rdns': False}))\n\n    if auth_info is not None:\n        koji_login(session, **auth_info)\n\n    return session", "response": "Creates and returns a Koji session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_koji_module_build(session, module_spec):\n\n    if module_spec.context is not None:\n        # The easy case - we can build the koji \"name-version-release\" out of the\n        # module spec.\n        koji_nvr = \"{}-{}-{}.{}\".format(module_spec.name,\n                                        module_spec.stream.replace(\"-\", \"_\"),\n                                        module_spec.version,\n                                        module_spec.context)\n        logger.info(\"Looking up module build %s in Koji\", koji_nvr)\n        build = session.getBuild(koji_nvr)\n    else:\n        # Without the context, we have to retrieve all builds for the module, and\n        # find the one we want. This is pretty inefficient, but won't be needed\n        # long-term.\n        logger.info(\"Listing all builds for %s in Koji\", module_spec.name)\n        package_id = session.getPackageID(module_spec.name)\n        builds = session.listBuilds(packageID=package_id, type='module',\n                                    state=koji.BUILD_STATES['COMPLETE'])\n        build = None\n\n        for b in builds:\n            if '.' in b['release']:\n                version, context = b['release'].split('.', 1)\n                if version == module_spec.version:\n                    if build is not None:\n                        raise RuntimeError(\"Multiple builds found for {}\"\n                                           .format(module_spec.to_str()))\n                    else:\n                        build = b\n\n        if build is None:\n            raise RuntimeError(\"No build found for {}\".format(module_spec.to_str()))\n\n    archives = session.listArchives(buildID=build['build_id'])\n    # The RPM list for the 'modulemd.txt' archive has all the RPMs, recent\n    # versions of MBS also write upload 'modulemd.<arch>.txt' archives with\n    # architecture subsets.\n    archives = [a for a in archives if a['filename'] == 'modulemd.txt']\n    assert len(archives) == 1\n\n    rpm_list = session.listRPMs(imageID=archives[0]['id'])\n\n    return build, rpm_list", "response": "Get the build information for a module in the Koji database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the filename to be used for saving the repo file.", "response": "def filename(self):\n        '''Returns the filename to be used for saving the repo file.\n\n        The filename is derived from the repo url by injecting a suffix\n        after the name and before the file extension. This suffix is a\n        partial md5 checksum of the full repourl. This avoids multiple\n        repos from being written to the same file.\n        '''\n        urlpath = unquote(urlsplit(self.repourl, allow_fragments=False).path)\n        basename = os.path.basename(urlpath)\n        if not basename.endswith(REPO_SUFFIX):\n            basename += REPO_SUFFIX\n        if self.add_hash:\n            suffix = '-' + md5(self.repourl.encode('utf-8')).hexdigest()[:5]\n        else:\n            suffix = ''\n        final_name = suffix.join(os.path.splitext(basename))\n        return final_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rpm_compare(self, a, b):\n        if a['version'] == b['version'] and \\\n           a['release'] == b['release'] and \\\n           a['signature'] == b['signature']:\n            return\n\n        raise ValueError(\"%s != %s\" % (a, b))", "response": "Compare rpm component version a with b."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the component lists from each worker build.", "response": "def get_component_list_from_workers(self, worker_metadatas):\n        \"\"\"\n        Find the component lists from each worker build.\n\n        The components that are interesting are under the 'output' key.  The\n        buildhost's components are ignored.\n\n        Inside the 'output' key are various 'instances'.  The only 'instance'\n        with a 'component list' is the 'docker-image' instance.  The 'log'\n        instances are ignored for now.\n\n        Reference plugin post_koji_upload for details on how this is created.\n\n        :return: list of component lists\n        \"\"\"\n        comp_list = []\n        for platform in sorted(worker_metadatas.keys()):\n            for instance in worker_metadatas[platform]['output']:\n                if instance['type'] == 'docker-image':\n                    if 'components' not in instance or not instance['components']:\n                        self.log.warn(\"Missing 'components' key in 'output' metadata instance: %s\",\n                                      instance)\n                        continue\n\n                    comp_list.append(instance['components'])\n\n        return comp_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a list of installed RPMs in the format required for the the metadata.", "response": "def get_rpms(self):\n        \"\"\"\n        Build a list of installed RPMs in the format required for the\n        metadata.\n        \"\"\"\n\n        tags = [\n            'NAME',\n            'VERSION',\n            'RELEASE',\n            'ARCH',\n            'EPOCH',\n            'SIGMD5',\n            'SIGPGP:pgpsig',\n            'SIGGPG:pgpsig',\n        ]\n\n        cmd = \"/bin/rpm \" + rpm_qf_args(tags)\n        try:\n            # py3\n            (status, output) = subprocess.getstatusoutput(cmd)\n        except AttributeError:\n            # py2\n            with open('/dev/null', 'r+') as devnull:\n                p = subprocess.Popen(cmd,\n                                     shell=True,\n                                     stdin=devnull,\n                                     stdout=subprocess.PIPE,\n                                     stderr=devnull)\n\n                (stdout, stderr) = p.communicate()\n                status = p.wait()\n                output = stdout.decode()\n\n        if status != 0:\n            self.log.debug(\"%s: stderr output: %s\", cmd, stderr)\n            raise RuntimeError(\"%s: exit code %s\" % (cmd, status))\n\n        return parse_rpm_output(output.splitlines(), tags)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the metadata for a file by its metadata.", "response": "def get_output_metadata(self, path, filename):\n        \"\"\"\n        Describe a file by its metadata.\n\n        :return: dict\n        \"\"\"\n\n        checksums = get_checksums(path, ['md5'])\n        metadata = {'filename': filename,\n                    'filesize': os.path.getsize(path),\n                    'checksum': checksums['md5sum'],\n                    'checksum_type': 'md5'}\n\n        if self.metadata_only:\n            metadata['metadata_only'] = True\n\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding out the docker ID of the buildroot image we are in.", "response": "def get_builder_image_id(self):\n        \"\"\"\n        Find out the docker ID of the buildroot image we are in.\n        \"\"\"\n\n        try:\n            buildroot_tag = os.environ[\"OPENSHIFT_CUSTOM_BUILD_BASE_IMAGE\"]\n        except KeyError:\n            return ''\n\n        try:\n            pod = self.osbs.get_pod_for_build(self.build_id)\n            all_images = pod.get_container_image_ids()\n        except OsbsException as ex:\n            self.log.error(\"unable to find image id: %r\", ex)\n            return buildroot_tag\n\n        try:\n            return all_images[buildroot_tag]\n        except KeyError:\n            self.log.error(\"Unable to determine buildroot image ID for %s\",\n                           buildroot_tag)\n            return buildroot_tag"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the buildroot entry of the metadata.", "response": "def get_buildroot(self, build_id):\n        \"\"\"\n        Build the buildroot entry of the metadata.\n\n        :return: dict, partial metadata\n        \"\"\"\n\n        docker_info = self.tasker.get_info()\n        host_arch, docker_version = get_docker_architecture(self.tasker)\n\n        buildroot = {\n            'id': 1,\n            'host': {\n                'os': docker_info['OperatingSystem'],\n                'arch': host_arch,\n            },\n            'content_generator': {\n                'name': PROG,\n                'version': atomic_reactor_version,\n            },\n            'container': {\n                'type': 'docker',\n                'arch': os.uname()[4],\n            },\n            'tools': [\n                {\n                    'name': tool['name'],\n                    'version': tool['version'],\n                }\n                for tool in get_version_of_tools()] + [\n                {\n                    'name': 'docker',\n                    'version': docker_version,\n                },\n            ],\n            'components': self.get_rpms(),\n            'extra': {\n                'osbs': {\n                    'build_id': build_id,\n                    'builder_image_id': self.get_builder_image_id(),\n                }\n            },\n        }\n\n        return buildroot"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the logs for the build.", "response": "def get_logs(self):\n        \"\"\"\n        Build the logs entry for the metadata 'output' section\n\n        :return: list, Output instances\n        \"\"\"\n\n        output = []\n\n        # Collect logs from server\n        try:\n            logs = self.osbs.get_build_logs(self.build_id)\n        except OsbsException as ex:\n            self.log.error(\"unable to get build logs: %r\", ex)\n        else:\n            # Deleted once closed\n            logfile = NamedTemporaryFile(prefix=self.build_id,\n                                         suffix=\".log\",\n                                         mode='wb')\n            try:\n                logfile.write(logs)\n            except (TypeError, UnicodeEncodeError):\n                # Older osbs-client versions returned Unicode objects\n                logfile.write(logs.encode('utf-8'))\n            logfile.flush()\n            metadata = self.get_output_metadata(logfile.name,\n                                                \"openshift-final.log\")\n            output.append(Output(file=logfile, metadata=metadata))\n\n        docker_logs = NamedTemporaryFile(prefix=\"docker-%s\" % self.build_id,\n                                         suffix=\".log\",\n                                         mode='wb')\n        docker_logs.write(\"\\n\".join(self.workflow.build_result.logs).encode('utf-8'))\n        docker_logs.flush()\n        output.append(Output(file=docker_logs,\n                             metadata=self.get_output_metadata(docker_logs.name,\n                                                               \"build.log\")))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of image components that are required by the rpmqa plugin.", "response": "def get_image_components(self):\n        \"\"\"\n        Re-package the output of the rpmqa plugin into the format required\n        for the metadata.\n        \"\"\"\n\n        output = self.workflow.image_components\n        if output is None:\n            self.log.error(\"%s plugin did not run!\",\n                           PostBuildRPMqaPlugin.key)\n            output = []\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_image_output(self, arch):\n\n        saved_image = self.workflow.exported_image_sequence[-1].get('path')\n        image_name = get_image_upload_filename(self.workflow.exported_image_sequence[-1],\n                                               self.workflow.builder.image_id,\n                                               arch)\n        if self.metadata_only:\n            metadata = self.get_output_metadata(os.path.devnull, image_name)\n            output = Output(file=None, metadata=metadata)\n        else:\n            metadata = self.get_output_metadata(saved_image, image_name)\n            output = Output(file=open(saved_image), metadata=metadata)\n\n        return metadata, output", "response": "Create the output for the image"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a map of images to their digests", "response": "def get_digests(self):\n        \"\"\"\n        Returns a map of images to their digests\n        \"\"\"\n\n        try:\n            pulp = get_manifests_in_pulp_repository(self.workflow)\n        except KeyError:\n            pulp = None\n\n        digests = {}  # repository -> digests\n        for registry in self.workflow.push_conf.docker_registries:\n            for image in self.workflow.tag_conf.images:\n                image_str = image.to_str()\n                if image_str in registry.digests:\n                    image_digests = registry.digests[image_str]\n                    if pulp is None:\n                        digest_list = [image_digests.default]\n                    else:\n                        # If Pulp is enabled, only report digests that\n                        # were synced into Pulp. This may not be all\n                        # of them, depending on whether Pulp has\n                        # schema 2 support.\n                        digest_list = [digest for digest in (image_digests.v1,\n                                                             image_digests.v2)\n                                       if digest in pulp]\n\n                    digests[image.to_str(registry=False)] = digest_list\n\n        return digests"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the repositories metadata for the current node.", "response": "def get_repositories(self, digests):\n        \"\"\"\n        Build the repositories metadata\n\n        :param digests: dict, image -> digests\n        \"\"\"\n        if self.workflow.push_conf.pulp_registries:\n            # If pulp was used, only report pulp images\n            registries = self.workflow.push_conf.pulp_registries\n        else:\n            # Otherwise report all the images we pushed\n            registries = self.workflow.push_conf.all_registries\n\n        output_images = []\n        for registry in registries:\n            image = self.pullspec_image.copy()\n            image.registry = registry.uri\n            pullspec = image.to_str()\n\n            output_images.append(pullspec)\n\n            digest_list = digests.get(image.to_str(registry=False), ())\n            for digest in digest_list:\n                digest_pullspec = image.to_str(tag=False) + \"@\" + digest\n                output_images.append(digest_pullspec)\n\n        return output_images"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the output section of the metadata.", "response": "def get_output(self, buildroot_id):\n        \"\"\"\n        Build the 'output' section of the metadata.\n\n        :return: list, Output instances\n        \"\"\"\n\n        def add_buildroot_id(output):\n            logfile, metadata = output\n            metadata.update({'buildroot_id': buildroot_id})\n            return Output(file=logfile, metadata=metadata)\n\n        def add_log_type(output):\n            logfile, metadata = output\n            metadata.update({'type': 'log', 'arch': 'noarch'})\n            return Output(file=logfile, metadata=metadata)\n\n        output_files = [add_log_type(add_buildroot_id(metadata))\n                        for metadata in self.get_logs()]\n\n        # Parent of squashed built image is base image\n        image_id = self.workflow.builder.image_id\n        parent_id = None\n        if not self.workflow.builder.base_from_scratch:\n            parent_id = self.workflow.builder.base_image_inspect['Id']\n\n        # Read config from the registry using v2 schema 2 digest\n        registries = self.workflow.push_conf.docker_registries\n        if registries:\n            config = copy.deepcopy(registries[0].config)\n        else:\n            config = {}\n\n        # We don't need container_config section\n        if config and 'container_config' in config:\n            del config['container_config']\n\n        digests = self.get_digests()\n        repositories = self.get_repositories(digests)\n        arch = os.uname()[4]\n        tags = set(image.tag for image in self.workflow.tag_conf.primary_images)\n        metadata, output = self.get_image_output(arch)\n        metadata.update({\n            'arch': arch,\n            'type': 'docker-image',\n            'components': self.get_image_components(),\n            'extra': {\n                'image': {\n                    'arch': arch,\n                },\n                'docker': {\n                    'id': image_id,\n                    'parent_id': parent_id,\n                    'repositories': repositories,\n                    'layer_sizes': self.workflow.layer_sizes,\n                    'tags': list(tags),\n                    'config': config\n                },\n            },\n        })\n\n        digests_with_types = self.get_digests_with_types()\n        if digests_with_types:\n            metadata['extra']['docker']['digests'] = digests_with_types\n\n        if self.workflow.builder.base_from_scratch:\n            del metadata['extra']['docker']['parent_id']\n        if not config:\n            del metadata['extra']['docker']['config']\n\n        # Add the 'docker save' image to the output\n        image = add_buildroot_id(output)\n        output_files.append(image)\n\n        return output_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading a file to koji and returns the path on the serverdir", "response": "def upload_file(self, session, output, serverdir):\n        \"\"\"\n        Upload a file to koji\n\n        :return: str, pathname on server\n        \"\"\"\n        name = output.metadata['filename']\n        self.log.debug(\"uploading %r to %r as %r\",\n                       output.file.name, serverdir, name)\n\n        kwargs = {}\n        if self.blocksize is not None:\n            kwargs['blocksize'] = self.blocksize\n            self.log.debug(\"using blocksize %d\", self.blocksize)\n\n        upload_logger = KojiUploadLogger(self.log)\n        session.uploadWrapper(output.file.name, serverdir, name=name,\n                              callback=upload_logger.callback, **kwargs)\n        path = os.path.join(serverdir, name)\n        self.log.debug(\"uploaded %r\", path)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the koji build process.", "response": "def run(self):\n        \"\"\"\n        Run the plugin.\n        \"\"\"\n        # Only run if the build was successful\n        if self.workflow.build_process_failed:\n            self.log.info(\"Not promoting failed build to koji\")\n            return\n\n        self.koji_session = get_koji_session(self.workflow, self.koji_fallback)\n        koji_metadata, output_files = self.get_metadata()\n\n        try:\n            server_dir = self.get_upload_server_dir()\n            for output in output_files:\n                if output.file:\n                    self.upload_file(self.koji_session, output, server_dir)\n        finally:\n            for output in output_files:\n                if output.file:\n                    output.file.close()\n\n        try:\n            build_info = self.koji_session.CGImport(koji_metadata, server_dir)\n        except Exception:\n            self.log.debug(\"metadata: %r\", koji_metadata)\n            raise\n\n        # Older versions of CGImport do not return a value.\n        build_id = build_info.get(\"id\") if build_info else None\n\n        self.log.debug(\"Build information: %s\",\n                       json.dumps(build_info, sort_keys=True, indent=4))\n\n        # If configured, koji_tag_build plugin will perform build tagging\n        tag_later = are_plugins_in_order(self.workflow.exit_plugins_conf,\n                                         PLUGIN_KOJI_PROMOTE_PLUGIN_KEY,\n                                         PLUGIN_KOJI_TAG_BUILD_KEY)\n        if not tag_later and build_id is not None and self.target is not None:\n            tag_koji_build(self.koji_session, build_id, self.target,\n                           poll_interval=self.poll_interval)\n\n        return build_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if manifest list only?", "response": "def get_manifest_list_only_expectation(self):\n        \"\"\"\n        Get expectation for manifest list only\n\n        :return: bool, expect manifest list only?\n        \"\"\"\n        if not self.workflow.postbuild_results.get(PLUGIN_GROUP_MANIFESTS_KEY):\n            self.log.debug('Cannot check if only manifest list digest should be returned '\n                           'because group manifests plugin did not run')\n            return False\n\n        platforms = get_platforms(self.workflow)\n        if not platforms:\n            self.log.debug('Cannot check if only manifest list digest should be returned '\n                           'because we have no platforms list')\n            return False\n\n        try:\n            platform_to_goarch = get_platform_to_goarch_mapping(self.workflow)\n        except KeyError:\n            self.log.debug('Cannot check if only manifest list digest should be returned '\n                           'because there are no platform descriptors')\n            return False\n\n        for plat in platforms:\n            if platform_to_goarch[plat] == 'amd64':\n                self.log.debug('amd64 was built, all media types available')\n                return False\n\n        self.log.debug('amd64 was not built, only manifest list digest is available')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        build_json = get_build_json()\n        current_platform = platform.processor() or 'x86_64'\n        self.manifest_list_cache = {}\n        organization = get_registries_organization(self.workflow)\n\n        for nonce, parent in enumerate(sorted(self.workflow.builder.parent_images.keys(),\n                                              key=str)):\n            if base_image_is_custom(parent.to_str()):\n                continue\n\n            image = parent\n            is_base_image = False\n            # original_base_image is an ImageName, so compare parent as an ImageName also\n            if image == self.workflow.builder.original_base_image:\n                is_base_image = True\n                image = self._resolve_base_image(build_json)\n\n            image = self._ensure_image_registry(image)\n\n            if organization:\n                image.enclose(organization)\n                parent.enclose(organization)\n\n            if self.check_platforms:\n                # run only at orchestrator\n                self._validate_platforms_in_image(image)\n                self._collect_image_digests(image)\n\n            # try to stay with digests\n            image_with_digest = self._get_image_with_digest(image, current_platform)\n            if image_with_digest is None:\n                self.log.warning(\"Cannot resolve platform '%s' specific digest for image '%s'\",\n                                 current_platform, image)\n            else:\n                self.log.info(\"Replacing image '%s' with '%s'\", image, image_with_digest)\n                image = image_with_digest\n\n            if self.check_platforms:\n                new_arch_image = self._get_image_for_different_arch(image, current_platform)\n                if new_arch_image:\n                    image = new_arch_image\n\n            if self.inspect_only:\n                new_image = image\n            else:\n                new_image = self._pull_and_tag_image(image, build_json, str(nonce))\n            self.workflow.builder.recreate_parent_images()\n            self.workflow.builder.parent_images[parent] = new_image\n\n            if is_base_image:\n                if organization:\n                    # we want to be sure we have original_base_image enclosed as well\n                    self.workflow.builder.original_base_image.enclose(organization)\n                self.workflow.builder.set_base_image(\n                    str(new_image), insecure=self.parent_registry_insecure,\n                    dockercfg_path=self.parent_registry_dockercfg_path\n                )\n        self.workflow.builder.parents_pulled = not self.inspect_only\n        self.workflow.builder.base_image_insecure = self.parent_registry_insecure", "response": "Pulls parent images and retag them uniquely for this build."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_image_for_different_arch(self, image, platform):\n        parents_digests = self.workflow.builder.parent_images_digests\n        try:\n            digests = parents_digests.get_image_digests(image)\n        except KeyError:\n            return None\n\n        if not digests:\n            return None\n        platform_digest = digests.get(platform)\n        if platform_digest is None:\n            # exact match is not found, get random platform\n            platform_digest = tuple(digests.values())[0]\n\n        new_image = ImageName.parse(platform_digest)\n        return new_image", "response": "Get image from random arch\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve the base image from the build spec.", "response": "def _resolve_base_image(self, build_json):\n        \"\"\"If this is an auto-rebuild, adjust the base image to use the triggering build\"\"\"\n        spec = build_json.get(\"spec\")\n        try:\n            image_id = spec['triggeredBy'][0]['imageChangeBuild']['imageID']\n        except (TypeError, KeyError, IndexError):\n            # build not marked for auto-rebuilds; use regular base image\n            base_image = self.workflow.builder.base_image\n            self.log.info(\"using %s as base image.\", base_image)\n        else:\n            # build has auto-rebuilds enabled\n            self.log.info(\"using %s from build spec[triggeredBy] as base image.\", image_id)\n            base_image = ImageName.parse(image_id)  # any exceptions will propagate\n\n        return base_image"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that the image uses the parent registry.", "response": "def _ensure_image_registry(self, image):\n        \"\"\"If plugin configured with a parent registry, ensure the image uses it\"\"\"\n        image_with_registry = image.copy()\n        if self.parent_registry:\n            # if registry specified in Dockerfile image, ensure it's the one allowed by config\n            if image.registry and image.registry != self.parent_registry:\n                error = (\n                    \"Registry specified in dockerfile image doesn't match configured one. \"\n                    \"Dockerfile: '%s'; expected registry: '%s'\"\n                    % (image, self.parent_registry))\n                self.log.error(\"%s\", error)\n                raise RuntimeError(error)\n\n            image_with_registry.registry = self.parent_registry\n\n        return image_with_registry"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_manifest_list(self, image):\n        if image in self.manifest_list_cache:\n            return self.manifest_list_cache[image]\n\n        manifest_list = get_manifest_list(image, image.registry,\n                                          insecure=self.parent_registry_insecure,\n                                          dockercfg_path=self.parent_registry_dockercfg_path)\n        if '@sha256:' in str(image) and not manifest_list:\n            # we want to adjust the tag only for manifest list fetching\n            image = image.copy()\n\n            try:\n                config_blob = get_config_from_registry(\n                    image, image.registry, image.tag, insecure=self.parent_registry_insecure,\n                    dockercfg_path=self.parent_registry_dockercfg_path\n                )\n            except (HTTPError, RetryError, Timeout) as ex:\n                self.log.warning('Unable to fetch config for %s, got error %s',\n                                 image, ex.response.status_code)\n                raise RuntimeError('Unable to fetch config for base image')\n\n            release = config_blob['config']['Labels']['release']\n            version = config_blob['config']['Labels']['version']\n            docker_tag = \"%s-%s\" % (version, release)\n            image.tag = docker_tag\n\n            manifest_list = get_manifest_list(image, image.registry,\n                                              insecure=self.parent_registry_insecure,\n                                              dockercfg_path=self.parent_registry_dockercfg_path)\n        self.manifest_list_cache[image] = manifest_list\n        return self.manifest_list_cache[image]", "response": "Get the manifest list for the image"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures that the base image provides all platforms expected for the build.", "response": "def _validate_platforms_in_image(self, image):\n        \"\"\"Ensure that the image provides all platforms expected for the build.\"\"\"\n        expected_platforms = get_platforms(self.workflow)\n        if not expected_platforms:\n            self.log.info('Skipping validation of available platforms '\n                          'because expected platforms are unknown')\n            return\n        if len(expected_platforms) == 1:\n            self.log.info('Skipping validation of available platforms for base image '\n                          'because this is a single platform build')\n            return\n\n        if not image.registry:\n            self.log.info('Cannot validate available platforms for base image '\n                          'because base image registry is not defined')\n            return\n\n        try:\n            platform_to_arch = get_platform_to_goarch_mapping(self.workflow)\n        except KeyError:\n            self.log.info('Cannot validate available platforms for base image '\n                          'because platform descriptors are not defined')\n            return\n\n        manifest_list = self._get_manifest_list(image)\n\n        if not manifest_list:\n            raise RuntimeError('Unable to fetch manifest list for base image')\n\n        all_manifests = manifest_list.json()['manifests']\n        manifest_list_arches = set(\n            manifest['platform']['architecture'] for manifest in all_manifests)\n\n        expected_arches = set(\n            platform_to_arch[platform] for platform in expected_platforms)\n\n        self.log.info('Manifest list arches: %s, expected arches: %s',\n                      manifest_list_arches, expected_arches)\n        assert manifest_list_arches >= expected_arches, \\\n            'Missing arches in manifest list for base image'\n\n        self.log.info('Base image is a manifest list for all required platforms')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dockercfg_credentials(self, docker_registry):\n        if not self.registry_secret_path:\n            return {}\n\n        dockercfg = Dockercfg(self.registry_secret_path)\n        registry_creds = dockercfg.get_credentials(docker_registry)\n        if 'username' not in registry_creds:\n            return {}\n\n        return {\n            'basic_auth_username': registry_creds['username'],\n            'basic_auth_password': registry_creds['password'],\n        }", "response": "Read the. dockercfg file and return an empty dict or else a dict\n        with keys basic_auth_username and basic_auth_password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_compose(self, source_type, source, packages=None, sigkeys=None, arches=None,\n                      flags=None, multilib_arches=None, multilib_method=None,\n                      modular_koji_tags=None):\n        \"\"\"Start a new ODCS compose\n\n        :param source_type: str, the type of compose to request (tag, module, pulp)\n        :param source: str, if source_type \"tag\" is used, the name of the Koji tag\n                       to use when retrieving packages to include in compose;\n                       if source_type \"module\", white-space separated NAME-STREAM or\n                       NAME-STREAM-VERSION list of modules to include in compose;\n                       if source_type \"pulp\", white-space separated list of context-sets\n                       to include in compose\n        :param packages: list<str>, packages which should be included in a compose. Only\n                         relevant when source_type \"tag\" is used.\n        :param sigkeys: list<str>, IDs of signature keys. Only packages signed by one of\n                        these keys will be included in a compose.\n        :param arches: list<str>, List of additional Koji arches to build this compose for.\n                        By default, the compose is built only for \"x86_64\" arch.\n        :param multilib_arches: list<str>, List of Koji arches to build as multilib in this\n                        compose. By default, no arches are built as multilib.\n        :param multilib_method: list<str>, list of methods to determine which packages should\n                        be included in a multilib compose. Defaults to none, but the value\n                        of ['devel', 'runtime] will be passed to ODCS if multilib_arches is\n                        not empty and no mulitlib_method value is provided.\n        :param modular_koji_tags: list<str>, the koji tags which are tagged to builds from the\n                        modular Koji Content Generator.  Builds with matching tags will be\n                        included in the compose.\n\n        :return: dict, status of compose being created by request.\n        \"\"\"\n        body = {\n            'source': {\n                'type': source_type,\n                'source': source\n            }\n        }\n        if source_type == \"tag\":\n            body['source']['packages'] = packages or []\n\n        if sigkeys is not None:\n            body['source']['sigkeys'] = sigkeys\n\n        if flags is not None:\n            body['flags'] = flags\n\n        if arches is not None:\n            body['arches'] = arches\n\n        if multilib_arches:\n            body['multilib_arches'] = multilib_arches\n            body['multilib_method'] = multilib_method or MULTILIB_METHOD_DEFAULT\n\n        if source_type == \"module\" and modular_koji_tags:\n            body['modular_koji_tags'] = modular_koji_tags\n\n        logger.info(\"Starting compose: %s\", body)\n        response = self.session.post('{}composes/'.format(self.url),\n                                     json=body)\n        response.raise_for_status()\n\n        return response.json()", "response": "Start a new compose for a given source type and source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef renew_compose(self, compose_id):\n        logger.info(\"Renewing compose %d\", compose_id)\n        response = self.session.patch('{}composes/{}'.format(self.url, compose_id))\n        response.raise_for_status()\n        response_json = response.json()\n        compose_id = response_json['id']\n        logger.info(\"Renewed compose is %d\", compose_id)\n        return response_json", "response": "Renew or extend existing compose."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for compose request to finalize the compose.", "response": "def wait_for_compose(self, compose_id,\n                         burst_retry=1,\n                         burst_length=30,\n                         slow_retry=10,\n                         timeout=1800):\n        \"\"\"Wait for compose request to finalize\n\n        :param compose_id: int, compose ID to wait for\n        :param burst_retry: int, seconds to wait between retries prior to exceeding\n                            the burst length\n        :param burst_length: int, seconds to switch to slower retry period\n        :param slow_retry: int, seconds to wait between retries after exceeding\n                           the burst length\n        :param timeout: int, when to give up waiting for compose request\n\n        :return: dict, updated status of compose.\n        :raise RuntimeError: if state_name becomes 'failed'\n        \"\"\"\n        logger.debug(\"Getting compose information for information for compose_id={}\"\n                     .format(compose_id))\n        url = '{}composes/{}'.format(self.url, compose_id)\n        start_time = time.time()\n        while True:\n            response = self.session.get(url)\n            response.raise_for_status()\n            response_json = response.json()\n\n            if response_json['state_name'] == 'failed':\n                state_reason = response_json.get('state_reason', 'Unknown')\n                logger.error(dedent(\"\"\"\\\n                   Compose %s failed: %s\n                   Details: %s\n                   \"\"\"), compose_id, state_reason, json.dumps(response_json, indent=4))\n                raise RuntimeError('Failed request for compose_id={}: {}'\n                                   .format(compose_id, state_reason))\n\n            if response_json['state_name'] not in ['wait', 'generating']:\n                logger.debug(\"Retrieved compose information for compose_id={}: {}\"\n                             .format(compose_id, json.dumps(response_json, indent=4)))\n                return response_json\n\n            elapsed = time.time() - start_time\n            if elapsed > timeout:\n                raise RuntimeError(\"Retrieving %s timed out after %s seconds\" %\n                                   (url, timeout))\n            else:\n                logger.debug(\"Retrying request compose_id={}, elapsed_time={}\"\n                             .format(compose_id, elapsed))\n\n                if elapsed > burst_length:\n                    time.sleep(slow_retry)\n                else:\n                    time.sleep(burst_retry)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        source_path = self.workflow.source.path\n        cur_dir = os.getcwd()\n        os.chdir(source_path)\n        try:\n            subprocess.check_call(self.command.split())\n        finally:\n            os.chdir(cur_dir)", "response": "Run the command and return the output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_stages(self):\n        stages = []\n        end = last_user_found = None\n        for part in reversed(self.dfp.structure):\n            if end is None:\n                end = part\n\n            if part['instruction'] == 'USER' and not last_user_found:\n                # we will reuse the line verbatim\n                last_user_found = part['content']\n\n            if part['instruction'] == 'FROM':\n                stages.insert(0, {'from_structure': part,\n                                  'end_structure': end,\n                                  'stage_user': last_user_found})\n                end = last_user_found = None\n\n        return stages", "response": "Find limits of each Dockerfile stage"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a map of repositories to digests", "response": "def get_digests(self):\n        \"\"\"\n        Returns a map of repositories to digests\n        \"\"\"\n\n        digests = {}  # repository -> digest\n        for registry in self.workflow.push_conf.docker_registries:\n            for image in self.workflow.tag_conf.images:\n                image_str = image.to_str()\n                if image_str in registry.digests:\n                    digest = registry.digests[image_str]\n                    digests[image.to_str(registry=False)] = digest\n\n        return digests"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_registries(self):\n        if self.workflow.buildstep_result.get(PLUGIN_BUILD_ORCHESTRATE_KEY):\n            registries = self.workflow.push_conf.pulp_registries\n            if not registries:\n                registries = self.workflow.push_conf.all_registries\n            return registries\n        else:\n            return self.workflow.push_conf.all_registries", "response": "Returns a list of all registries that this build was updated."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_logs(self):\n\n        build_logs = NamedTemporaryFile(prefix=\"buildstep-%s\" % self.build_id,\n                                        suffix=\".log\",\n                                        mode='wb')\n        build_logs.write(\"\\n\".join(self.workflow.build_result.logs).encode('utf-8'))\n        build_logs.flush()\n        filename = \"{platform}-build.log\".format(platform=self.platform)\n        return [Output(file=build_logs,\n                       metadata=self.get_output_metadata(build_logs.name,\n                                                         filename))]", "response": "Build the logs entry for the metadata output section"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_image_output(self):\n\n        saved_image = self.workflow.exported_image_sequence[-1].get('path')\n        image_name = get_image_upload_filename(self.workflow.exported_image_sequence[-1],\n                                               self.workflow.builder.image_id,\n                                               self.platform)\n        metadata = self.get_output_metadata(saved_image, image_name)\n        output = Output(file=open(saved_image), metadata=metadata)\n\n        return metadata, output", "response": "Create the output for the image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_repositories_and_digests(self):\n        digests = {}  # image -> digests\n        typed_digests = {}  # media_type -> digests\n        for registry in self.workflow.push_conf.docker_registries:\n            for image in self.workflow.tag_conf.images:\n                image_str = image.to_str()\n                if image_str in registry.digests:\n                    image_digests = registry.digests[image_str]\n                    if self.report_multiple_digests and get_pulp(self.workflow, None):\n                        digest_list = [digest for digest in (image_digests.v1,\n                                                             image_digests.v2)\n                                       if digest]\n                    else:\n                        digest_list = [self.select_digest(image_digests)]\n                    digests[image.to_str(registry=False)] = digest_list\n                    for digest_version in image_digests.content_type:\n                        if digest_version not in image_digests:\n                            continue\n                        if not get_pulp(self.workflow, None) and digest_version == 'v1':\n                            continue\n                        digest_type = get_manifest_media_type(digest_version)\n                        typed_digests[digest_type] = image_digests[digest_version]\n\n        if self.workflow.push_conf.pulp_registries:\n            # If pulp was used, only report pulp images\n            registries = self.workflow.push_conf.pulp_registries\n        else:\n            # Otherwise report all the images we pushed\n            registries = self.workflow.push_conf.all_registries\n        repositories = []\n        for registry in registries:\n            image = self.pullspec_image.copy()\n            image.registry = registry.uri\n            pullspec = image.to_str()\n\n            repositories.append(pullspec)\n\n            digest_list = digests.get(image.to_str(registry=False), ())\n            for digest in digest_list:\n                digest_pullspec = image.to_str(tag=False) + \"@\" + digest\n                repositories.append(digest_pullspec)\n\n        return repositories, typed_digests", "response": "Returns a map of images to their repositories and media types to each digest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the output section of the metadata.", "response": "def get_output(self, buildroot_id):\n        \"\"\"\n        Build the 'output' section of the metadata.\n\n        :return: list, Output instances\n        \"\"\"\n\n        def add_buildroot_id(output):\n            logfile, metadata = output\n            metadata.update({'buildroot_id': buildroot_id})\n            return Output(file=logfile, metadata=metadata)\n\n        def add_log_type(output, arch):\n            logfile, metadata = output\n            metadata.update({'type': 'log', 'arch': arch})\n            return Output(file=logfile, metadata=metadata)\n\n        arch = os.uname()[4]\n        output_files = [add_log_type(add_buildroot_id(metadata), arch)\n                        for metadata in self.get_logs()]\n\n        # Parent of squashed built image is base image\n        image_id = self.workflow.builder.image_id\n        parent_id = None\n        if not self.workflow.builder.base_from_scratch:\n            parent_id = self.workflow.builder.base_image_inspect['Id']\n\n        # Read config from the registry using v2 schema 2 digest\n        registries = self.workflow.push_conf.docker_registries\n        if registries:\n            config = copy.deepcopy(registries[0].config)\n        else:\n            config = {}\n\n        # We don't need container_config section\n        if config and 'container_config' in config:\n            del config['container_config']\n\n        repositories, typed_digests = self.get_repositories_and_digests()\n        tags = set(image.tag for image in self.workflow.tag_conf.images)\n        metadata, output = self.get_image_output()\n\n        metadata.update({\n            'arch': arch,\n            'type': 'docker-image',\n            'components': self.get_image_components(),\n            'extra': {\n                'image': {\n                    'arch': arch,\n                },\n                'docker': {\n                    'id': image_id,\n                    'parent_id': parent_id,\n                    'repositories': repositories,\n                    'layer_sizes': self.workflow.layer_sizes,\n                    'tags': list(tags),\n                    'config': config,\n                    'digests': typed_digests\n                },\n            },\n        })\n\n        if self.workflow.builder.base_from_scratch:\n            del metadata['extra']['docker']['parent_id']\n        if not config:\n            del metadata['extra']['docker']['config']\n        if not typed_digests:\n            del metadata['extra']['docker']['digests']\n\n        # Add the 'docker save' image to the output\n        image = add_buildroot_id(output)\n        output_files.append(image)\n\n        # add operator manifests to output\n        operator_manifests_path = (self.workflow.postbuild_results\n                                   .get(PLUGIN_EXPORT_OPERATOR_MANIFESTS_KEY))\n        if operator_manifests_path:\n            operator_manifests_file = open(operator_manifests_path)\n            manifests_metadata = self.get_output_metadata(operator_manifests_path,\n                                                          OPERATOR_MANIFESTS_ARCHIVE)\n            operator_manifests_output = Output(file=operator_manifests_file,\n                                               metadata=manifests_metadata)\n            # We use log type here until a more appropriate type name is supported by koji\n            operator_manifests_output.metadata.update({'arch': arch, 'type': 'log'})\n            operator_manifests = add_buildroot_id(operator_manifests_output)\n            output_files.append(operator_manifests)\n\n        return output_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the buildroot with the final koji information in the buildroot under extra. osbs .", "response": "def update_buildroot_koji(self, buildroot, output):\n        \"\"\"\n        put the final koji information in the buildroot under extra.osbs\n        \"\"\"\n        docker = output[1]['extra']['docker']\n\n        name = ''\n        for tag in docker['tags']:\n            for repo in docker['repositories']:\n                if tag in repo:\n                    iname = ImageName.parse(repo)\n                    name = iname.to_str(registry=False)\n                    break\n\n        buildroot['extra']['osbs']['koji'] = {\n            'build_name': name,\n            'builder_image_id': docker.get('digests', {})\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the metadata needed for importing the build", "response": "def get_metadata(self):\n        \"\"\"\n        Build the metadata needed for importing the build\n\n        :return: tuple, the metadata and the list of Output instances\n        \"\"\"\n        try:\n            metadata = get_build_json()[\"metadata\"]\n            self.build_id = metadata[\"name\"]\n        except KeyError:\n            self.log.error(\"No build metadata\")\n            raise\n\n        for image in self.workflow.tag_conf.unique_images:\n            self.pullspec_image = image\n            break\n\n        for image in self.workflow.tag_conf.primary_images:\n            # dash at first/last postition does not count\n            if '-' in image.tag[1:-1]:\n                self.pullspec_image = image\n                break\n\n        if not self.pullspec_image:\n            raise RuntimeError('Unable to determine pullspec_image')\n\n        metadata_version = 0\n\n        buildroot = self.get_buildroot(build_id=self.build_id)\n        output_files = self.get_output(buildroot['id'])\n\n        output = [output.metadata for output in output_files]\n        koji_metadata = {\n            'metadata_version': metadata_version,\n            'buildroots': [buildroot],\n            'output': output,\n        }\n        self.update_buildroot_koji(buildroot, output)\n\n        return koji_metadata, output_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects the worker node data and recreate the locally.", "response": "def get_worker_digests(self):\n        \"\"\"\n         If we are being called from an orchestrator build, collect the worker\n         node data and recreate the data locally.\n        \"\"\"\n        try:\n            builds = self.workflow.build_result.annotations['worker-builds']\n        except(TypeError, KeyError):\n            # This annotation is only set for the orchestrator build.\n            # It's not present, so this is a worker build.\n            return {}\n\n        worker_digests = {}\n\n        for plat, annotation in builds.items():\n            digests = annotation['digests']\n            self.log.debug(\"build %s has digests: %s\", plat, digests)\n\n            for digest in digests:\n                reg = registry_hostname(digest['registry'])\n                worker_digests.setdefault(reg, [])\n                worker_digests[reg].append(digest)\n\n        return worker_digests"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_remote_image_result(annotations=None, labels=None):\n        return BuildResult(image_id=BuildResult.REMOTE_IMAGE,\n                           annotations=annotations, labels=labels)", "response": "Instantiate BuildResult for image not built locally."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninspect base image :return: dict", "response": "def base_image_inspect(self):\n        \"\"\"\n        inspect base image\n\n        :return: dict\n        \"\"\"\n        if self._base_image_inspect is None:\n\n            if self.base_from_scratch:\n                self._base_image_inspect = {}\n            elif self.parents_pulled or self.custom_base_image:\n                try:\n                    self._base_image_inspect = self.tasker.inspect_image(self.base_image)\n\n                except docker.errors.NotFound:\n                    # If the base image cannot be found throw KeyError -\n                    # as this property should behave like a dict\n                    raise KeyError(\"Unprocessed base image Dockerfile cannot be inspected\")\n            else:\n                self._base_image_inspect =\\\n                    atomic_reactor.util.get_inspect_for_image(self.base_image,\n                                                              self.base_image.registry,\n                                                              self.base_image_insecure,\n                                                              self.base_image_dockercfg_path)\n\n            base_image_str = str(self.base_image)\n            if base_image_str not in self._parent_images_inspect:\n                self._parent_images_inspect[base_image_str] = self._base_image_inspect\n\n        return self._base_image_inspect"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parent_image_inspect(self, image):\n        image_name = ImageName.parse(image)\n        if image_name not in self._parent_images_inspect:\n            if self.parents_pulled:\n                self._parent_images_inspect[image_name] = self.tasker.inspect_image(image)\n            else:\n                self._parent_images_inspect[image_name] =\\\n                    atomic_reactor.util.get_inspect_for_image(image_name,\n                                                              image_name.registry,\n                                                              self.base_image_insecure,\n                                                              self.base_image_dockercfg_path)\n\n        return self._parent_images_inspect[image_name]", "response": "inspect parent image\n\n        :return: dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inspect_built_image(self):\n        logger.info(\"inspecting built image '%s'\", self.image_id)\n        self.ensure_is_built()\n        # dict with lots of data, see man docker-inspect\n        inspect_data = self.tasker.inspect_image(self.image_id)\n        return inspect_data", "response": "inspect built image\n\n        :return: dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_base_image_info(self):\n        if self.base_from_scratch:\n            return\n        logger.info(\"getting information about base image '%s'\", self.base_image)\n        image_info = self.tasker.get_image_info_by_image_name(self.base_image)\n        items_count = len(image_info)\n        if items_count == 1:\n            return image_info[0]\n        elif items_count <= 0:\n            logger.error(\"image '%s' not found\", self.base_image)\n            raise RuntimeError(\"image '%s' not found\" % self.base_image)\n        else:\n            logger.error(\"multiple (%d) images found for image '%s'\", items_count,\n                         self.base_image)\n            raise RuntimeError(\"multiple (%d) images found for image '%s'\" % (items_count,\n                                                                              self.base_image))", "response": "query docker about base image return dict of base image name and base image class"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_built_image_info(self):\n        logger.info(\"getting information about built image '%s'\", self.image)\n        image_info = self.tasker.get_image_info_by_image_name(self.image)\n        items_count = len(image_info)\n        if items_count == 1:\n            return image_info[0]\n        elif items_count <= 0:\n            logger.error(\"image '%s' not found\", self.image)\n            raise RuntimeError(\"image '%s' not found\" % self.image)\n        else:\n            logger.error(\"multiple (%d) images found for image '%s'\", items_count, self.image)\n            raise RuntimeError(\"multiple (%d) images found for image '%s'\" % (items_count,\n                                                                              self.image))", "response": "query docker about built image return dict of built image names and their associated local time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_image(self, df_dir_path, image, use_cache=False):\n        logger.debug(\"creating build image: df_dir_path = '%s', image = '%s'\", df_dir_path, image)\n\n        if not os.path.isdir(df_dir_path):\n            raise RuntimeError(\"Directory '%s' does not exist.\" % df_dir_path)\n\n        tmpdir = tempfile.mkdtemp()\n        df_tmpdir = os.path.join(tmpdir, 'df-%s' % uuid.uuid4())\n        git_tmpdir = os.path.join(tmpdir, 'git-%s' % uuid.uuid4())\n        os.mkdir(df_tmpdir)\n        logger.debug(\"tmp dir with dockerfile '%s' created\", df_tmpdir)\n        os.mkdir(git_tmpdir)\n        logger.debug(\"tmp dir with atomic-reactor '%s' created\", git_tmpdir)\n        try:\n            for f in glob(os.path.join(df_dir_path, '*')):\n                shutil.copy(f, df_tmpdir)\n                logger.debug(\"cp '%s' -> '%s'\", f, df_tmpdir)\n            logger.debug(\"df dir: %s\", os.listdir(df_tmpdir))\n            reactor_tarball = self.get_reactor_tarball_path(tmpdir=git_tmpdir)\n            reactor_tb_path = os.path.join(df_tmpdir, DOCKERFILE_REACTOR_TARBALL_NAME)\n            shutil.copy(reactor_tarball, reactor_tb_path)\n\n            image_name = ImageName.parse(image)\n            logs_gen = self.tasker.build_image_from_path(df_tmpdir, image_name,\n                                                         use_cache=use_cache)\n            wait_for_command(logs_gen)\n        finally:\n            shutil.rmtree(tmpdir)", "response": "create image from file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_reactor_tarball_path(self, tmpdir):\n        if self.reactor_tarball_path:\n            if not os.path.isfile(self.reactor_tarball_path):\n                logger.error(\"atomic-reactor sdist tarball does not exist: '%s'\",\n                             self.reactor_tarball_path)\n                raise RuntimeError(\"File does not exist: '%s'\" % self.reactor_tarball_path)\n            return self.reactor_tarball_path\n        elif self.reactor_local_path:\n            if not os.path.isdir(self.reactor_local_path):\n                logger.error(\"local atomic-reactor git clone does not exist: '%s'\",\n                             self.reactor_local_path)\n                raise RuntimeError(\"Local atomic-reactor git repo does not exist: '%s'\" %\n                                   self.reactor_local_path)\n            local_reactor_git_path = self.reactor_local_path\n        else:\n            if self.use_official_reactor_git:\n                self.reactor_remote_path = REACTOR_GIT_URL\n\n            g = LazyGit(self.reactor_remote_path, tmpdir=tmpdir)\n            local_reactor_git_path = g.git_path\n\n        cwd = os.getcwd()\n        os.chdir(local_reactor_git_path)\n        try:\n            logger.debug(\"executing sdist command in directory '%s'\", os.getcwd())\n            subprocess.check_call([\"python\", \"setup.py\", \"sdist\", \"--dist-dir\", tmpdir])\n        finally:\n            os.chdir(cwd)\n        candidates_list = glob(os.path.join(tmpdir, 'atomic-reactor-*.tar.gz'))\n        if len(candidates_list) == 1:\n            return candidates_list[0]\n        else:\n            logger.warning(\"len(atomic-reactor-*.tar.gz) != 1: '%s'\", candidates_list)\n            try:\n                return candidates_list[0]\n            except IndexError:\n                raise RuntimeError(\"No atomic-reactor tarball built.\")", "response": "generate atomic - reaction tarball"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a new image inside a new image", "response": "def build_inside(input_method, input_args=None, substitutions=None):\n    \"\"\"\n    use requested input plugin to load configuration and then initiate build\n    \"\"\"\n    def process_keyvals(keyvals):\n        \"\"\" [\"key=val\", \"x=y\"] -> {\"key\": \"val\", \"x\": \"y\"} \"\"\"\n        keyvals = keyvals or []\n        processed_keyvals = {}\n        for arg in keyvals:\n            key, value = arg.split(\"=\", 1)\n            processed_keyvals[key] = value\n        return processed_keyvals\n\n    main = __name__.split('.', 1)[0]\n    log_encoding = get_logging_encoding(main)\n    logger.info(\"log encoding: %s\", log_encoding)\n\n    if not input_method:\n        raise RuntimeError(\"No input method specified!\")\n\n    logger.debug(\"getting build json from input %s\", input_method)\n\n    cleaned_input_args = process_keyvals(input_args)\n    cleaned_input_args['substitutions'] = process_keyvals(substitutions)\n\n    input_runner = InputPluginsRunner([{'name': input_method,\n                                        'args': cleaned_input_args}])\n    build_json = input_runner.run()[input_method]\n\n    if isinstance(build_json, Exception):\n        raise RuntimeError(\"Input plugin raised exception: {}\".format(build_json))\n    logger.debug(\"build json: %s\", build_json)\n    if not build_json:\n        raise RuntimeError(\"No valid build json!\")\n    if not isinstance(build_json, dict):\n        raise RuntimeError(\"Input plugin did not return valid build json: {}\".format(build_json))\n\n    dbw = DockerBuildWorkflow(**build_json)\n    try:\n        build_result = dbw.build_docker_image()\n    except Exception as e:\n        logger.error('image build failed: %s', e)\n        raise\n    else:\n        if not build_result or build_result.is_failed():\n            raise RuntimeError(\"no image built\")\n        else:\n            logger.info(\"build has finished successfully \\\\o/\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        while True:  # make sure to run at least once before exiting\n            with self._lock:\n                self._update(self._data)\n            if self._done:\n                break\n            time.sleep(1)", "response": "Override this method to implement thread s functionality."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_docker_image(self):\n        exception_being_handled = False\n        self.builder = InsideBuilder(self.source, self.image)\n        # Make sure exit_runner is defined for finally block\n        exit_runner = None\n        try:\n            self.fs_watcher.start()\n            signal.signal(signal.SIGTERM, self.throw_canceled_build_exception)\n            prebuild_runner = PreBuildPluginsRunner(self.builder.tasker, self,\n                                                    self.prebuild_plugins_conf,\n                                                    plugin_files=self.plugin_files)\n            buildstep_runner = BuildStepPluginsRunner(self.builder.tasker, self,\n                                                      self.buildstep_plugins_conf,\n                                                      plugin_files=self.plugin_files)\n            prepublish_runner = PrePublishPluginsRunner(self.builder.tasker, self,\n                                                        self.prepublish_plugins_conf,\n                                                        plugin_files=self.plugin_files)\n            postbuild_runner = PostBuildPluginsRunner(self.builder.tasker, self,\n                                                      self.postbuild_plugins_conf,\n                                                      plugin_files=self.plugin_files)\n            exit_runner = ExitPluginsRunner(self.builder.tasker, self,\n                                            self.exit_plugins_conf,\n                                            keep_going=True,\n                                            plugin_files=self.plugin_files)\n            # time to run pre-build plugins, so they can access cloned repo\n            logger.info(\"running pre-build plugins\")\n            try:\n                prebuild_runner.run()\n            except PluginFailedException as ex:\n                logger.error(\"one or more prebuild plugins failed: %s\", ex)\n                raise\n            except AutoRebuildCanceledException as ex:\n                logger.info(str(ex))\n                self.autorebuild_canceled = True\n                raise\n\n            logger.info(\"running buildstep plugins\")\n            try:\n                self.build_result = buildstep_runner.run()\n\n                if self.build_result.is_failed():\n                    raise PluginFailedException(self.build_result.fail_reason)\n            except PluginFailedException as ex:\n                self.builder.is_built = False\n                logger.error('buildstep plugin failed: %s', ex)\n                raise\n\n            self.builder.is_built = True\n            if self.build_result.is_image_available():\n                self.builder.image_id = self.build_result.image_id\n\n            # run prepublish plugins\n            try:\n                prepublish_runner.run()\n            except PluginFailedException as ex:\n                logger.error(\"one or more prepublish plugins failed: %s\", ex)\n                raise\n\n            if self.build_result.is_image_available():\n                self.built_image_inspect = self.builder.inspect_built_image()\n                history = self.builder.tasker.d.history(self.builder.image_id)\n                diff_ids = self.built_image_inspect[INSPECT_ROOTFS][INSPECT_ROOTFS_LAYERS]\n\n                # diff_ids is ordered oldest first\n                # history is ordered newest first\n                # We want layer_sizes to be ordered oldest first\n                self.layer_sizes = [{\"diff_id\": diff_id, \"size\": layer['Size']}\n                                    for (diff_id, layer) in zip(diff_ids, reversed(history))]\n\n            try:\n                postbuild_runner.run()\n            except PluginFailedException as ex:\n                logger.error(\"one or more postbuild plugins failed: %s\", ex)\n                raise\n\n            return self.build_result\n        except Exception as ex:\n            logger.debug(\"caught exception (%r) so running exit plugins\", ex)\n            exception_being_handled = True\n            raise\n        finally:\n            # We need to make sure all exit plugins are executed\n            signal.signal(signal.SIGTERM, lambda *args: None)\n            if exit_runner is None:\n                exit_runner = ExitPluginsRunner(self.builder.tasker, self,\n                                                self.exit_plugins_conf,\n                                                keep_going=True,\n                                                plugin_files=self.plugin_files)\n            try:\n                exit_runner.run(keep_going=True)\n            except PluginFailedException as ex:\n                logger.error(\"one or more exit plugins failed: %s\", ex)\n\n                # raise exception only in case that there is no previous exception being already\n                # handled to prevent replacing original exceptions (root cause) with exceptions\n                # from exit plugins\n                if not exception_being_handled:\n                    raise ex\n            finally:\n                self.source.remove_tmpdir()\n                self.fs_watcher.finish()\n\n            signal.signal(signal.SIGTERM, signal.SIG_DFL)", "response": "build docker image\n\n        :return: BuildResult"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(workflow):\n    try:\n        workspace = workflow.plugin_workspace[ReactorConfigPlugin.key]\n        return workspace[WORKSPACE_CONF_KEY]\n    except KeyError:\n        # The plugin did not run or was not successful: use defaults\n        conf = ReactorConfig()\n        workspace = workflow.plugin_workspace.get(ReactorConfigPlugin.key, {})\n        workspace[WORKSPACE_CONF_KEY] = conf\n        workflow.plugin_workspace[ReactorConfigPlugin.key] = workspace\n        return conf", "response": "Returns the configuration object for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses and validate the plugin and store in workflow workspace", "response": "def run(self):\n        \"\"\"\n        Run the plugin\n\n        Parse and validate config.\n        Store in workflow workspace for later retrieval.\n        \"\"\"\n        if self.reactor_config_map:\n            self.log.info(\"reading config from REACTOR_CONFIG env variable\")\n            conf = read_yaml(self.reactor_config_map, 'schemas/config.json')\n        else:\n            config_filename = os.path.join(self.config_path, self.basename)\n            self.log.info(\"reading config from %s\", config_filename)\n            conf = read_yaml_from_file_path(config_filename, 'schemas/config.json')\n        reactor_conf = ReactorConfig(conf)\n        workspace = self.workflow.plugin_workspace.setdefault(self.key, {})\n        workspace[WORKSPACE_CONF_KEY] = reactor_conf\n\n        self.log.info(\"reading config content %s\", reactor_conf.conf)\n\n        # need to stash this on the workflow for access in a place that can't import this module\n        self.workflow.default_image_build_method = get_default_image_build_method(self.workflow)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n\n        workspace = self.workflow.plugin_workspace.get(self.key, {})\n        cf_maps_to_remove = workspace.get('cf_maps_to_remove', [])\n        for cm_key, osbs in cf_maps_to_remove:\n            try:\n                osbs.delete_config_map(cm_key)\n                self.log.debug(\"ConfigMap %s deleted\", cm_key)\n            except OsbsResponseException as ex:\n                self.log.warning(\"Failed to delete ConfigMap %s: %s\", cm_key, ex)", "response": "Delete all the config maps that are in the workspace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_build_input(self, image, args_path):\n        try:\n            with open(os.path.join(args_path, BUILD_JSON)) as json_args:\n                logger.debug(\"build input: image = '%s', args = '%s'\", image, json_args.read())\n        except (IOError, OSError) as ex:\n            logger.error(\"unable to open json arguments: %r\", ex)\n            raise RuntimeError(\"Unable to open json arguments: %r\" % ex)\n\n        if not self.tasker.image_exists(image):\n            logger.error(\"provided build image doesn't exist: '%s'\", image)\n            raise RuntimeError(\"Provided build image doesn't exist: '%s'\" % image)", "response": "Internal method validate provided args."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_image_dockerhost(self, build_image, json_args_path):\n        logger.info(\"building image '%s' in container using docker from host\", build_image)\n\n        self._check_build_input(build_image, json_args_path)\n        self._obtain_source_from_path_if_needed(json_args_path, CONTAINER_SHARE_PATH)\n\n        if not os.path.exists(DOCKER_SOCKET_PATH):\n            logger.error(\"looks like docker is not running because there is no socket at: %s\",\n                         DOCKER_SOCKET_PATH)\n            raise RuntimeError(\"docker socket not found: %s\" % DOCKER_SOCKET_PATH)\n\n        volume_bindings = {\n            DOCKER_SOCKET_PATH: {\n                'bind': DOCKER_SOCKET_PATH,\n            },\n            json_args_path: {\n                'bind': CONTAINER_SHARE_PATH,\n            },\n        }\n\n        if self._volume_bind_understands_mode():\n            volume_bindings[DOCKER_SOCKET_PATH]['mode'] = 'ro'\n            volume_bindings[json_args_path]['mode'] = 'rw,Z'\n        else:\n            volume_bindings[DOCKER_SOCKET_PATH]['ro'] = True\n            volume_bindings[json_args_path]['rw'] = True\n\n        with open(os.path.join(json_args_path, BUILD_JSON)) as fp:\n            logger.debug('build json mounted in container: %s', fp.read())\n\n        container_id = self.tasker.run(\n            ImageName.parse(build_image),\n            create_kwargs={'volumes': [DOCKER_SOCKET_PATH, json_args_path]},\n            volume_bindings=volume_bindings,\n            privileged=True,\n        )\n\n        return container_id", "response": "Build image inside privileged container using docker from host"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_image_from_path(self, path, image, use_cache=False, remove_im=True):\n        logger.info(\"building image '%s' from path '%s'\", image, path)\n        response = self.d.build(path=path, tag=image.to_str(),\n                                nocache=not use_cache, decode=True,\n                                rm=remove_im, forcerm=True, pull=False)  # returns generator\n        return response", "response": "build an image from a path and tag it with the name of the image"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_image_from_git(self, url, image, git_path=None, git_commit=None,\n                             copy_dockerfile_to=None,\n                             stream=False, use_cache=False):\n        \"\"\"\n        build image from provided url and tag it\n\n        this operation is asynchronous and you should consume returned generator in order to wait\n        for build to finish\n\n        :param url: str\n        :param image: ImageName, name of the resulting image\n        :param git_path: str, path to dockerfile within gitrepo\n        :param copy_dockerfile_to: str, copy dockerfile to provided path\n        :param stream: bool, True returns generator, False returns str\n        :param use_cache: bool, True if you want to use cache\n        :return: generator\n        \"\"\"\n        logger.info(\"building image '%s' from git repo '%s' specified as URL '%s'\",\n                    image, git_path, url)\n        logger.info(\"will copy Dockerfile to '%s'\", copy_dockerfile_to)\n        temp_dir = tempfile.mkdtemp()\n        response = None\n        try:\n            clone_git_repo(url, temp_dir, git_commit)\n            build_file_path, build_file_dir = figure_out_build_file(temp_dir, git_path)\n            if copy_dockerfile_to:  # TODO: pre build plugin\n                shutil.copyfile(build_file_path, copy_dockerfile_to)\n            response = self.build_image_from_path(build_file_dir, image, use_cache=use_cache)\n        finally:\n            try:\n                shutil.rmtree(temp_dir)\n            except (IOError, OSError) as ex:\n                # no idea why this is happening\n                logger.warning(\"Failed to remove dir '%s': %r\", temp_dir, ex)\n        logger.info(\"build finished\")\n        return response", "response": "build image from provided url and tag it with the given name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, image, command=None, create_kwargs=None, start_kwargs=None,\n            volume_bindings=None, privileged=None):\n        \"\"\"\n        create container from provided image and start it\n\n        for more info, see documentation of REST API calls:\n         * containers/{}/start\n         * container/create\n\n        :param image: ImageName or string, name or id of the image\n        :param command: str\n        :param create_kwargs: dict, kwargs for docker.create_container\n        :param start_kwargs: dict, kwargs for docker.start\n        :return: str, container id\n        \"\"\"\n        logger.info(\"creating container from image '%s' and running it\", image)\n        create_kwargs = create_kwargs or {}\n\n        if 'host_config' not in create_kwargs:\n            conf = {}\n            if volume_bindings is not None:\n                conf['binds'] = volume_bindings\n\n            if privileged is not None:\n                conf['privileged'] = privileged\n\n            create_kwargs['host_config'] = self.d.create_host_config(**conf)\n\n        start_kwargs = start_kwargs or {}\n        logger.debug(\"image = '%s', command = '%s', create_kwargs = '%s', start_kwargs = '%s'\",\n                     image, command, create_kwargs, start_kwargs)\n        if isinstance(image, ImageName):\n            image = image.to_str()\n        container_dict = self.d.create_container(image, command=command, **create_kwargs)\n        container_id = container_dict['Id']\n        logger.debug(\"container_id = '%s'\", container_id)\n        self.d.start(container_id, **start_kwargs)  # returns None\n        return container_id", "response": "create a container from provided image and start it"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef commit_container(self, container_id, image=None, message=None):\n        logger.info(\"committing container '%s'\", container_id)\n        logger.debug(\"container_id = '%s', image = '%s', message = '%s'\",\n                     container_id, image, message)\n        tag = None\n        if image:\n            tag = image.tag\n            image = image.to_str(tag=False)\n        response = self.d.commit(container_id, repository=image, tag=tag, message=message)\n        logger.debug(\"response = '%s'\", response)\n        try:\n            return response['Id']\n        except KeyError:\n            logger.error(\"ID missing from commit response\")\n            raise RuntimeError(\"ID missing from commit response\")", "response": "create image from provided container"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting info about an image by image_id", "response": "def get_image_info_by_image_id(self, image_id):\n        \"\"\"\n        using `docker images`, provide information about an image\n\n        :param image_id: str, hash of image to get info\n        :return: str or None\n        \"\"\"\n        logger.info(\"getting info about provided image specified by image_id '%s'\", image_id)\n        logger.debug(\"image_id = '%s'\", image_id)\n        # returns list of\n        # {u'Created': 1414577076,\n        #  u'Id': u'3ab9a7ed8a169ab89b09fb3e12a14a390d3c662703b65b4541c0c7bde0ee97eb',\n        #  u'ParentId': u'a79ad4dac406fcf85b9c7315fe08de5b620c1f7a12f45c8185c843f4b4a49c4e',\n        #  u'RepoTags': [u'buildroot-fedora:latest'],\n        #  u'Size': 0,\n        #  u'VirtualSize': 856564160}\n        images = self.d.images()\n        try:\n            image_dict = [i for i in images if i['Id'] == image_id][0]\n        except IndexError:\n            logger.info(\"image not found\")\n            return None\n        else:\n            return image_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets image info by name", "response": "def get_image_info_by_image_name(self, image, exact_tag=True):\n        \"\"\"\n        using `docker images`, provide information about an image\n\n        :param image: ImageName, name of image\n        :param exact_tag: bool, if false then return info for all images of the\n                          given name regardless what their tag is\n        :return: list of dicts\n        \"\"\"\n        logger.info(\"getting info about provided image specified by name '%s'\", image)\n        logger.debug(\"image_name = '%s'\", image)\n\n        # returns list of\n        # {u'Created': 1414577076,\n        #  u'Id': u'3ab9a7ed8a169ab89b09fb3e12a14a390d3c662703b65b4541c0c7bde0ee97eb',\n        #  u'ParentId': u'a79ad4dac406fcf85b9c7315fe08de5b620c1f7a12f45c8185c843f4b4a49c4e',\n        #  u'RepoTags': [u'buildroot-fedora:latest'],\n        #  u'Size': 0,\n        #  u'VirtualSize': 856564160}\n        images = self.d.images(name=image.to_str(tag=False))\n        if exact_tag:\n            # tag is specified, we are looking for the exact image\n            for found_image in images:\n                if image.to_str(explicit_tag=True) in found_image['RepoTags']:\n                    logger.debug(\"image '%s' found\", image)\n                    return [found_image]\n            images = []  # image not found\n\n        logger.debug(\"%d matching images found\", len(images))\n        return images"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pull_image(self, image, insecure=False, dockercfg_path=None):\n        logger.info(\"pulling image '%s' from registry\", image)\n        logger.debug(\"image = '%s', insecure = '%s'\", image, insecure)\n        tag = image.tag\n\n        if dockercfg_path:\n            self.login(registry=image.registry, docker_secret_path=dockercfg_path)\n        try:\n            command_result = self.retry_generator(self.d.pull,\n                                                  image.to_str(tag=False),\n                                                  tag=tag, insecure_registry=insecure,\n                                                  decode=True, stream=True)\n        except TypeError:\n            # because changing api is fun\n            command_result = self.retry_generator(self.d.pull,\n                                                  image.to_str(tag=False),\n                                                  tag=tag, decode=True, stream=True)\n\n        self.last_logs = command_result.logs\n        return image.to_str()", "response": "pull an image from the registry"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntagging provided image with specified name", "response": "def tag_image(self, image, target_image, force=False):\n        \"\"\"\n        tag provided image with specified image_name, registry and tag\n\n        :param image: str or ImageName, image to tag\n        :param target_image: ImageName, new name for the image\n        :param force: bool, force tag the image?\n        :return: str, image (reg.om/img:v1)\n        \"\"\"\n        logger.info(\"tagging image '%s' as '%s'\", image, target_image)\n        logger.debug(\"image = '%s', target_image_name = '%s'\", image, target_image)\n        if not isinstance(image, ImageName):\n            image = ImageName.parse(image)\n\n        if image != target_image:\n            response = self.d.tag(\n                image.to_str(),\n                target_image.to_str(tag=False),\n                tag=target_image.tag,\n                force=force)  # returns True/False\n            if not response:\n                logger.error(\"failed to tag image\")\n                raise RuntimeError(\"Failed to tag image '%s': target_image = '%s'\" %\n                                   image.to_str(), target_image)\n        else:\n            logger.debug('image already tagged correctly, nothing to do')\n        return target_image.to_str()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs-in to a docker registry", "response": "def login(self, registry, docker_secret_path):\n        \"\"\"\n        login to docker registry\n\n        :param registry: registry name\n        :param docker_secret_path: path to docker config directory\n        \"\"\"\n        logger.info(\"logging in: registry '%s', secret path '%s'\", registry, docker_secret_path)\n        # Docker-py needs username\n        dockercfg = Dockercfg(docker_secret_path)\n        credentials = dockercfg.get_credentials(registry)\n        unpacked_auth = dockercfg.unpack_auth_b64(registry)\n        username = credentials.get('username')\n        if unpacked_auth:\n            username = unpacked_auth.username\n        if not username:\n            raise RuntimeError(\"Failed to extract a username from '%s'\" % dockercfg)\n\n        logger.info(\"found username %s for registry %s\", username, registry)\n\n        response = self.d.login(registry=registry, username=username,\n                                dockercfg_path=dockercfg.json_secret_path)\n        if not response:\n            raise RuntimeError(\"Failed to login to '%s' with config '%s'\" % (registry, dockercfg))\n        if u'Status' in response and response[u'Status'] == u'Login Succeeded':\n            logger.info(\"login succeeded\")\n        else:\n            if not(isinstance(response, dict) and 'password' in response.keys()):\n                # for some reason docker-py returns the contents of the dockercfg - we shouldn't\n                # be displaying that\n                logger.debug(\"response: %r\", response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npush provided image to registry", "response": "def push_image(self, image, insecure=False):\n        \"\"\"\n        push provided image to registry\n\n        :param image: ImageName\n        :param insecure: bool, allow connecting to registry over plain http\n        :return: str, logs from push\n        \"\"\"\n        logger.info(\"pushing image '%s'\", image)\n        logger.debug(\"image: '%s', insecure: '%s'\", image, insecure)\n        try:\n            # push returns string composed of newline separated jsons; exactly what 'docker push'\n            # outputs\n            command_result = self.retry_generator(self.d.push,\n                                                  image.to_str(tag=False),\n                                                  tag=image.tag, insecure_registry=insecure,\n                                                  decode=True, stream=True)\n        except TypeError:\n            # because changing api is fun\n            command_result = self.retry_generator(self.d.push,\n                                                  image.to_str(tag=False),\n                                                  tag=image.tag, decode=True, stream=True)\n\n        self.last_logs = command_result.logs\n        return command_result.parsed_logs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntag provided image and push it to registry", "response": "def tag_and_push_image(self, image, target_image, insecure=False, force=False,\n                           dockercfg=None):\n        \"\"\"\n        tag provided image and push it to registry\n\n        :param image: str or ImageName, image id or name\n        :param target_image: ImageName, img\n        :param insecure: bool, allow connecting to registry over plain http\n        :param force: bool, force the tag?\n        :param dockercfg: path to docker config\n        :return: str, image (reg.com/img:v1)\n        \"\"\"\n        logger.info(\"tagging and pushing image '%s' as '%s'\", image, target_image)\n        logger.debug(\"image = '%s', target_image = '%s'\", image, target_image)\n        self.tag_image(image, target_image, force=force)\n        if dockercfg:\n            self.login(registry=target_image.registry, docker_secret_path=dockercfg)\n        return self.push_image(target_image, insecure=insecure)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninspecting an image and return detailed metadata about it", "response": "def inspect_image(self, image_id):\n        \"\"\"\n        return detailed metadata about provided image (see 'man docker-inspect')\n\n        :param image_id: str or ImageName, id or name of the image\n        :return: dict\n        \"\"\"\n        logger.info(\"inspecting image '%s'\", image_id)\n        logger.debug(\"image_id = '%s'\", image_id)\n        if isinstance(image_id, ImageName):\n            image_id = image_id.to_str()\n        image_metadata = self.d.inspect_image(image_id)\n        return image_metadata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving an image from the cache", "response": "def remove_image(self, image_id, force=False, noprune=False):\n        \"\"\"\n        remove provided image from filesystem\n\n        :param image_id: str or ImageName\n        :param noprune: bool, keep untagged parents?\n        :param force: bool, force remove -- just trash it no matter what\n        :return: None\n        \"\"\"\n        logger.info(\"removing image '%s' from filesystem\", image_id)\n        logger.debug(\"image_id = '%s'\", image_id)\n        if isinstance(image_id, ImageName):\n            image_id = image_id.to_str()\n        self.d.remove_image(image_id, force=force, noprune=noprune)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_container(self, container_id, force=False):\n        logger.info(\"removing container '%s' from filesystem\", container_id)\n        logger.debug(\"container_id = '%s'\", container_id)\n        self.d.remove_container(container_id, force=force)", "response": "remove container from filesystem"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the logs from a container", "response": "def logs(self, container_id, stderr=True, stream=True):\n        \"\"\"\n        acquire output (stdout, stderr) from provided container\n\n        :param container_id: str\n        :param stderr: True, False\n        :param stream: if True, return as generator\n        :return: either generator, or list of strings\n        \"\"\"\n        logger.info(\"getting stdout of container '%s'\", container_id)\n        logger.debug(\"container_id = '%s', stream = '%s'\", container_id, stream)\n        # returns bytes\n        response = self.d.logs(container_id, stdout=True, stderr=stderr, stream=stream)\n        if not stream:\n            if isinstance(response, bytes):\n                response = response.decode(\"utf-8\")  # py2 & 3 compat\n            response = [line for line in response.splitlines() if line]\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits for the container to finish", "response": "def wait(self, container_id):\n        \"\"\"\n        wait for container to finish the job (may run infinitely)\n\n        :param container_id: str\n        :return: int, exit code\n        \"\"\"\n        logger.info(\"waiting for container '%s' to finish\", container_id)\n        logger.debug(\"container = '%s'\", container_id)\n        response = self.d.wait(container_id)  # returns exit code as int\n        logger.debug(\"container finished with exit code %s\", response)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef image_exists(self, image_id):\n        logger.info(\"checking whether image '%s' exists\", image_id)\n        logger.debug(\"image_id = '%s'\", image_id)\n        try:\n            response = self.d.inspect_image(image_id)\n        except APIError as ex:\n            logger.warning(repr(ex))\n            response = False\n        else:\n            response = response is not None\n        logger.debug(\"image exists: %s\", response)\n        return response", "response": "Check if an image exists in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_volumes_for_container(self, container_id, skip_empty_source=True):\n        logger.info(\"listing volumes for container '%s'\", container_id)\n        inspect_output = self.d.inspect_container(container_id)\n        volumes = inspect_output['Mounts'] or []\n        volume_names = [x['Name'] for x in volumes]\n\n        if skip_empty_source:\n            # Don't show volumes which are not on the filesystem\n            volume_names = [x['Name'] for x in volumes if x['Source'] != \"\"]\n\n        logger.debug(\"volumes = %s\", volume_names)\n        return volume_names", "response": "get a list of volumes mounter in a container"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a specific volume by its name", "response": "def remove_volume(self, volume_name):\n        \"\"\"\n        remove a volume by its name\n\n        :param volume_name: str\n        :return None\n        \"\"\"\n        logger.info(\"removing volume '%s'\", volume_name)\n        try:\n            self.d.remove_volume(volume_name)\n        except APIError as ex:\n            if ex.response.status_code == requests.codes.CONFLICT:\n                logger.debug(\"ignoring a conflict when removing volume %s\", volume_name)\n            else:\n                raise ex"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget json with build config from environment variable BUILD_JSON_ENV", "response": "def run(self):\n        \"\"\"\n        get json with build config from environment variable\n        \"\"\"\n        env_name = self.env_name or BUILD_JSON_ENV\n        try:\n            build_cfg_json = os.environ[env_name]\n        except KeyError:\n            self.log.error(\"build config not found in env variable '%s'\", env_name)\n            return None\n        else:\n            try:\n                return self.substitute_configuration(json.loads(build_cfg_json))\n            except ValueError:\n                self.log.error(\"couldn't load build config: invalid json\")\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_image_build_conf(self):\n        target = self.koji_target\n\n        vcs_info = self.workflow.source.get_vcs_info()\n        ksurl = '{}#{}'.format(vcs_info.vcs_url, vcs_info.vcs_ref)\n\n        base_urls = []\n        for repo in self.repos:\n            for url in self.extract_base_url(repo):\n                # Imagefactory only supports $arch variable.\n                url = url.replace('$basearch', '$arch')\n                base_urls.append(url)\n\n        install_tree = base_urls[0] if base_urls else ''\n\n        repo = ','.join(base_urls)\n\n        kwargs = {\n            'target': target,\n            'ksurl': ksurl,\n            'install_tree': install_tree,\n            'repo': repo,\n        }\n        config_fp = StringIO(self.DEFAULT_IMAGE_BUILD_CONF.format(**kwargs))\n\n        config = ConfigParser()\n        config.readfp(config_fp)\n\n        self.update_config_from_dockerfile(config)\n\n        return config", "response": "Create a default image build config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_config_from_dockerfile(self, config):\n        labels = Labels(df_parser(self.workflow.builder.df_path).labels)\n        for config_key, label in (\n            ('name', Labels.LABEL_TYPE_COMPONENT),\n            ('version', Labels.LABEL_TYPE_VERSION),\n        ):\n            try:\n                _, value = labels.get_name_and_value(label)\n            except KeyError:\n                pass\n            else:\n                config.set('image-build', config_key, value)", "response": "Updates build config with values from the Dockerfile"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_plugins(self, plugin_class_name):\n        # imp.findmodule('atomic_reactor') doesn't work\n        plugins_dir = os.path.join(os.path.dirname(__file__), 'plugins')\n        logger.debug(\"loading plugins from dir '%s'\", plugins_dir)\n        files = [os.path.join(plugins_dir, f)\n                 for f in os.listdir(plugins_dir)\n                 if f.endswith(\".py\")]\n        if self.plugin_files:\n            logger.debug(\"loading additional plugins from files '%s'\", self.plugin_files)\n            files += self.plugin_files\n        plugin_class = globals()[plugin_class_name]\n        plugin_classes = {}\n        for f in files:\n            module_name = os.path.basename(f).rsplit('.', 1)[0]\n            # Do not reload plugins\n            if module_name in sys.modules:\n                f_module = sys.modules[module_name]\n            else:\n                try:\n                    logger.debug(\"load file '%s'\", f)\n                    f_module = imp.load_source(module_name, f)\n                except (IOError, OSError, ImportError, SyntaxError) as ex:\n                    logger.warning(\"can't load module '%s': %r\", f, ex)\n                    continue\n            for name in dir(f_module):\n                binding = getattr(f_module, name, None)\n                try:\n                    # if you try to compare binding and PostBuildPlugin, python won't match them\n                    # if you call this script directly b/c:\n                    # ! <class 'plugins.plugin_rpmqa.PostBuildRPMqaPlugin'> <= <class\n                    # '__main__.PostBuildPlugin'>\n                    # but\n                    # <class 'plugins.plugin_rpmqa.PostBuildRPMqaPlugin'> <= <class\n                    # 'atomic_reactor.plugin.PostBuildPlugin'>\n                    is_sub = issubclass(binding, plugin_class)\n                except TypeError:\n                    is_sub = False\n                if binding and is_sub and plugin_class.__name__ != binding.__name__:\n                    plugin_classes[binding.key] = binding\n        return plugin_classes", "response": "Load all available plugins and return a dict of bindings for the plugin_class_name and the plugin_class_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck requested plugins availability and handle missing plugins", "response": "def get_available_plugins(self):\n        \"\"\"\n        check requested plugins availability\n        and handle missing plugins\n\n        :return: list of namedtuples, runnable plugins data\n        \"\"\"\n        available_plugins = []\n        PluginData = namedtuple('PluginData', 'name, plugin_class, conf, is_allowed_to_fail')\n        for plugin_request in self.plugins_conf:\n            plugin_name = plugin_request['name']\n            try:\n                plugin_class = self.plugin_classes[plugin_name]\n            except KeyError:\n                if plugin_request.get('required', True):\n                    msg = (\"no such plugin: '%s', did you set \"\n                           \"the correct plugin type?\") % plugin_name\n                    exc = PluginFailedException(msg)\n                    self.on_plugin_failed(plugin_name, exc)\n                    logger.error(msg)\n                    raise exc\n                else:\n                    # This plugin is marked as not being required\n                    logger.warning(\"plugin '%s' requested but not available\",\n                                   plugin_name)\n                    continue\n            plugin_is_allowed_to_fail = plugin_request.get('is_allowed_to_fail',\n                                                           getattr(plugin_class,\n                                                                   \"is_allowed_to_fail\", True))\n            plugin_conf = plugin_request.get(\"args\", {})\n            plugin = PluginData(plugin_name,\n                                plugin_class,\n                                plugin_conf,\n                                plugin_is_allowed_to_fail)\n            available_plugins.append(plugin)\n        return available_plugins"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun all requested plugins and returns a list of all the results.", "response": "def run(self, keep_going=False, buildstep_phase=False):\n        \"\"\"\n        run all requested plugins\n\n        :param keep_going: bool, whether to keep going after unexpected\n                                 failure (only used for exit plugins)\n        :param buildstep_phase: bool, when True remaining plugins will\n                                not be executed after a plugin completes\n                                (only used for build-step plugins)\n        \"\"\"\n        failed_msgs = []\n        plugin_successful = False\n        plugin_response = None\n        available_plugins = self.available_plugins\n        for plugin in available_plugins:\n            plugin_successful = False\n\n            logger.debug(\"running plugin '%s'\", plugin.name)\n            start_time = datetime.datetime.now()\n\n            plugin_response = None\n            skip_response = False\n            try:\n                plugin_instance = self.create_instance_from_plugin(plugin.plugin_class,\n                                                                   plugin.conf)\n                self.save_plugin_timestamp(plugin.plugin_class.key, start_time)\n                plugin_response = plugin_instance.run()\n                plugin_successful = True\n                if buildstep_phase:\n                    assert isinstance(plugin_response, BuildResult)\n                    if plugin_response.is_failed():\n                        logger.error(\"Build step plugin %s failed: %s\",\n                                     plugin.plugin_class.key,\n                                     plugin_response.fail_reason)\n                        self.on_plugin_failed(plugin.plugin_class.key,\n                                              plugin_response.fail_reason)\n                        plugin_successful = False\n                        self.plugins_results[plugin.plugin_class.key] = plugin_response\n                        break\n\n            except AutoRebuildCanceledException as ex:\n                # if auto rebuild is canceled, then just reraise\n                # NOTE: We need to catch and reraise explicitly, so that the below except clause\n                #   doesn't catch this and make PluginFailedException out of it in the end\n                #   (calling methods would then need to parse exception message to see if\n                #   AutoRebuildCanceledException was raised here)\n                raise\n            except InappropriateBuildStepError:\n                logger.debug('Build step %s is not appropriate', plugin.plugin_class.key)\n                # don't put None, in results for InappropriateBuildStepError\n                skip_response = True\n                if not buildstep_phase:\n                    raise\n            except Exception as ex:\n                msg = \"plugin '%s' raised an exception: %r\" % (plugin.plugin_class.key, ex)\n                logger.debug(traceback.format_exc())\n                if not plugin.is_allowed_to_fail:\n                    self.on_plugin_failed(plugin.plugin_class.key, ex)\n\n                if plugin.is_allowed_to_fail or keep_going:\n                    logger.warning(msg)\n                    logger.info(\"error is not fatal, continuing...\")\n                    if not plugin.is_allowed_to_fail:\n                        failed_msgs.append(msg)\n                else:\n                    logger.error(msg)\n                    raise PluginFailedException(msg)\n\n                plugin_response = ex\n\n            try:\n                if start_time:\n                    finish_time = datetime.datetime.now()\n                    duration = finish_time - start_time\n                    seconds = duration.total_seconds()\n                    logger.debug(\"plugin '%s' finished in %ds\", plugin.name, seconds)\n                    self.save_plugin_duration(plugin.plugin_class.key, seconds)\n            except Exception:\n                logger.exception(\"failed to save plugin duration\")\n\n            if not skip_response:\n                self.plugins_results[plugin.plugin_class.key] = plugin_response\n\n            if plugin_successful and buildstep_phase:\n                logger.debug('stopping further execution of plugins '\n                             'after first successful plugin')\n                break\n\n        if len(failed_msgs) == 1:\n            raise PluginFailedException(failed_msgs[0])\n        elif len(failed_msgs) > 1:\n            raise PluginFailedException(\"Multiple plugins raised an exception: \" +\n                                        str(failed_msgs))\n\n        if not plugin_successful and buildstep_phase and not plugin_response:\n            self.on_plugin_failed(\"BuildStepPlugin\", \"No appropriate build step\")\n            raise PluginFailedException(\"No appropriate build step\")\n\n        return self.plugins_results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _translate_special_values(self, obj_to_translate):\n        translation_dict = {\n            'BUILT_IMAGE_ID': self.workflow.builder.image_id,\n            'BUILD_DOCKERFILE_PATH': self.workflow.builder.source.dockerfile_path,\n            'BUILD_SOURCE_PATH':  self.workflow.builder.source.path,\n        }\n\n        if self.workflow.builder.base_image:\n            translation_dict['BASE_IMAGE'] = self.workflow.builder.base_image.to_str()\n\n        if isinstance(obj_to_translate, dict):\n            # Recurse into dicts\n            translated_dict = copy.deepcopy(obj_to_translate)\n            for key, value in obj_to_translate.items():\n                translated_dict[key] = self._translate_special_values(value)\n\n            return translated_dict\n        elif isinstance(obj_to_translate, list):\n            # Iterate over lists\n            return [self._translate_special_values(elem)\n                    for elem in obj_to_translate]\n        else:\n            return translation_dict.get(obj_to_translate, obj_to_translate)", "response": "Translate special values to the runtime values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _should_send(self, rebuild, success, auto_canceled, manual_canceled):\n        should_send = False\n\n        should_send_mapping = {\n            self.MANUAL_SUCCESS: not rebuild and success,\n            self.MANUAL_FAIL: not rebuild and not success,\n            self.MANUAL_CANCELED: not rebuild and manual_canceled,\n            self.AUTO_SUCCESS: rebuild and success,\n            self.AUTO_FAIL: rebuild and not success,\n            self.AUTO_CANCELED: rebuild and auto_canceled\n        }\n\n        for state in self.send_on:\n            should_send |= should_send_mapping[state]\n        return should_send", "response": "Return True if any state in self. send_on meets given conditions thus meaning\n            that a notification mail should be sent."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders and return subject and body of the mail to send.", "response": "def _render_mail(self, rebuild, success, auto_canceled, manual_canceled):\n        \"\"\"Render and return subject and body of the mail to send.\"\"\"\n        subject_template = '%(endstate)s building image %(image_name)s'\n        body_template = '\\n'.join([\n            'Image Name: %(image_name)s',\n            'Repositories: %(repositories)s',\n            'Status: %(endstate)s',\n            'Submitted by: %(user)s',\n        ])\n\n        # Failed autorebuilds include logs as attachments.\n        # Koji integration stores logs in successful Koji Builds.\n        # Don't include logs in these cases.\n        if self.session and not rebuild:\n            body_template += '\\nLogs: %(logs)s'\n\n        endstate = None\n        if auto_canceled or manual_canceled:\n            endstate = 'Canceled'\n        else:\n            endstate = 'Succeeded' if success else 'Failed'\n\n        url = self._get_logs_url()\n\n        image_name, repos = self._get_image_name_and_repos()\n\n        repositories = ''\n        for repo in repos:\n            repositories += '\\n    ' + repo\n\n        formatting_dict = {\n            'repositories': repositories,\n            'image_name': image_name,\n            'endstate': endstate,\n            'user': '<autorebuild>' if rebuild else self.submitter,\n            'logs': url\n        }\n\n        vcs = self.workflow.source.get_vcs_info()\n        if vcs:\n            body_template = '\\n'.join([\n                body_template,\n                'Source url: %(vcs-url)s',\n                'Source ref: %(vcs-ref)s',\n            ])\n            formatting_dict['vcs-url'] = vcs.vcs_url\n            formatting_dict['vcs-ref'] = vcs.vcs_ref\n\n        log_files = None\n        if rebuild and endstate == 'Failed':\n            log_files = self._fetch_log_files()\n\n        return (subject_template % formatting_dict, body_template % formatting_dict, log_files)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _send_mail(self, receivers_list, subject, body, log_files=None):\n        if not receivers_list:\n            self.log.info('no valid addresses in requested addresses. Doing nothing')\n            return\n\n        self.log.info('sending notification to %s ...', receivers_list)\n\n        \"\"\"Actually sends the mail with `subject` and `body` and optionanl log_file attachements\n        to all members of `receivers_list`.\"\"\"\n        if log_files:\n            msg = MIMEMultipart()\n            msg.attach(MIMEText(body))\n            for entry in log_files:\n                log_mime = MIMEBase('application', \"octet-stream\")\n                log_file = entry[0]  # Output.file\n                log_file.seek(0)\n                log_mime.set_payload(log_file.read())\n                encoders.encode_base64(log_mime)\n                log_mime.add_header('Content-Disposition',\n                                    'attachment; filename=\"{}\"'.format(entry[1]['filename']))\n                msg.attach(log_mime)\n        else:\n            msg = MIMEText(body)\n        msg['Subject'] = subject\n        msg['From'] = self.from_address\n        msg['To'] = ', '.join([x.strip() for x in receivers_list])\n\n        s = None\n        try:\n            s = get_smtp_session(self.workflow, self.smtp_fallback)\n            s.sendmail(self.from_address, receivers_list, msg.as_string())\n        except (socket.gaierror, smtplib.SMTPException):\n            self.log.error('Error communicating with SMTP server')\n            raise\n        finally:\n            if s is not None:\n                s.quit()", "response": "Sends the mail with subject and body."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_platform_metadata(self, platform, build_annotations):\n\n        # retrieve all the workspace data\n        build_info = get_worker_build_info(self.workflow, platform)\n        osbs = build_info.osbs\n\n        kind = \"configmap/\"\n        cmlen = len(kind)\n        cm_key_tmp = build_annotations['metadata_fragment']\n        cm_frag_key = build_annotations['metadata_fragment_key']\n\n        if not cm_key_tmp or not cm_frag_key or cm_key_tmp[:cmlen] != kind:\n            msg = \"Bad ConfigMap annotations for platform {}\".format(platform)\n            self.log.warning(msg)\n            raise BadConfigMapError(msg)\n\n        # use the key to get the configmap data and then use the\n        # fragment_key to get the build metadata inside the configmap data\n        # save the worker_build metadata\n        cm_key = cm_key_tmp[cmlen:]\n        try:\n            cm_data = osbs.get_config_map(cm_key)\n        except Exception:\n            self.log.error(\"Failed to get ConfigMap for platform %s\",\n                           platform)\n            raise\n\n        metadata = cm_data.get_data_by_key(cm_frag_key)\n        defer_removal(self.workflow, cm_key, osbs)\n        return metadata", "response": "Get the metadata for the given platform."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_image_in_privileged_container(build_image, source, image, parent_registry=None,\n                                        target_registries=None, push_buildroot_to=None,\n                                        parent_registry_insecure=False,\n                                        target_registries_insecure=False,\n                                        dont_pull_base_image=False, **kwargs):\n    \"\"\"\n    build image from provided dockerfile (specified by `source`) in privileged container by\n    running another docker instance inside the container\n\n    :param build_image: str, image where target image should be built\n    :param source: dict, where/how to get source code to put in image\n    :param image: str, tag for built image ([registry/]image_name[:tag])\n    :param parent_registry: str, registry to pull base image from\n    :param target_registries: list of str, list of registries to push image to (might change in\n                              future)\n    :param push_buildroot_to: str, repository where buildroot should be pushed\n    :param parent_registry_insecure: bool, allow connecting to parent registry over plain http\n    :param target_registries_insecure: bool, allow connecting to target registries over plain http\n    :param dont_pull_base_image: bool, don't pull or update base image specified in dockerfile\n\n    :return: BuildResults\n    \"\"\"\n    build_json = _prepare_build_json(image, source, parent_registry, target_registries,\n                                     parent_registry_insecure, target_registries_insecure,\n                                     dont_pull_base_image, **kwargs)\n    m = PrivilegedBuildManager(build_image, build_json)\n    build_response = m.build()\n    if push_buildroot_to:\n        m.commit_buildroot()\n        m.push_buildroot(push_buildroot_to)\n    return build_response", "response": "build an image in a privileged container"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_image_here(source, image,\n                     parent_registry=None, target_registries=None, parent_registry_insecure=False,\n                     target_registries_insecure=False, dont_pull_base_image=False, **kwargs):\n    \"\"\"\n    build image from provided dockerfile (specified by `source`) in current environment\n\n    :param source: dict, where/how to get source code to put in image\n    :param image: str, tag for built image ([registry/]image_name[:tag])\n    :param parent_registry: str, registry to pull base image from\n    :param target_registries: list of str, list of registries to push image to (might change in\n                              future)\n    :param parent_registry_insecure: bool, allow connecting to parent registry over plain http\n    :param target_registries_insecure: bool, allow connecting to target registries over plain http\n    :param dont_pull_base_image: bool, don't pull or update base image specified in dockerfile\n\n    :return: BuildResults\n    \"\"\"\n    build_json = _prepare_build_json(image, source, parent_registry, target_registries,\n                                     parent_registry_insecure, target_registries_insecure,\n                                     dont_pull_base_image, **kwargs)\n    m = DockerBuildWorkflow(**build_json)\n    return m.build_docker_image()", "response": "build image from source dockerfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the output entry of the metadata.", "response": "def get_output(self, worker_metadatas):\n        \"\"\"\n        Build the output entry of the metadata.\n\n        :return: list, containing dicts of partial metadata\n        \"\"\"\n        outputs = []\n        has_pulp_pull = PLUGIN_PULP_PULL_KEY in self.workflow.exit_results\n        try:\n            pulp_sync_results = self.workflow.postbuild_results[PLUGIN_PULP_SYNC_KEY]\n            crane_registry = pulp_sync_results[0]\n        except (KeyError, IndexError):\n            crane_registry = None\n\n        for platform in worker_metadatas:\n            for instance in worker_metadatas[platform]['output']:\n                instance['buildroot_id'] = '{}-{}'.format(platform, instance['buildroot_id'])\n\n                if instance['type'] == 'docker-image':\n                    # update image ID with pulp_pull results;\n                    # necessary when using Pulp < 2.14. Only do this\n                    # when building for a single architecture -- if\n                    # building for many, we know Pulp has schema 2\n                    # support.\n                    if len(worker_metadatas) == 1 and has_pulp_pull:\n                        if self.workflow.builder.image_id is not None:\n                            instance['extra']['docker']['id'] = self.workflow.builder.image_id\n\n                    # update repositories to point to Crane\n                    if crane_registry:\n                        pulp_pullspecs = []\n                        docker = instance['extra']['docker']\n                        for pullspec in docker['repositories']:\n                            image = ImageName.parse(pullspec)\n                            image.registry = crane_registry.registry\n                            pulp_pullspecs.append(image.to_str())\n\n                        docker['repositories'] = pulp_pullspecs\n\n                outputs.append(instance)\n\n        return outputs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the buildroot entry of the metadata.", "response": "def get_buildroot(self, worker_metadatas):\n        \"\"\"\n        Build the buildroot entry of the metadata.\n\n        :return: list, containing dicts of partial metadata\n        \"\"\"\n        buildroots = []\n        for platform in sorted(worker_metadatas.keys()):\n            for instance in worker_metadatas[platform]['buildroots']:\n                instance['id'] = '{}-{}'.format(platform, instance['id'])\n                buildroots.append(instance)\n\n        return buildroots"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches Bearer token and retry.", "response": "def handle_401(self, response, repo, **kwargs):\n        \"\"\"Fetch Bearer token and retry.\"\"\"\n        if response.status_code != requests.codes.unauthorized:\n            return response\n\n        auth_info = response.headers.get('www-authenticate', '')\n\n        if 'bearer' not in auth_info.lower():\n            return response\n\n        self._token_cache[repo] = self._get_token(auth_info, repo)\n\n        # Consume content and release the original connection\n        # to allow our new request to reuse the same one.\n        # This pattern was inspired by the source code of requests.auth.HTTPDigestAuth\n        response.content\n        response.close()\n        retry_request = response.request.copy()\n        extract_cookies_to_jar(retry_request._cookies, response.request, response.raw)\n        retry_request.prepare_cookies(retry_request._cookies)\n\n        self._set_header(retry_request, repo)\n        retry_response = response.connection.send(retry_request, **kwargs)\n        retry_response.history.append(response)\n        retry_response.request = retry_request\n\n        return retry_response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_plugins_without_parameters(self):\n\n        # Compatibility code for dockerfile_content plugin\n        self.remove_plugin('prebuild_plugins', PLUGIN_DOCKERFILE_CONTENT_KEY,\n                           'dockerfile_content is deprecated, please remove from config')\n        if not self.reactor_env:\n            return\n        self.remove_koji_plugins()\n        self.remove_pulp_plugins()\n        if not self.get_value('odcs'):\n            self.remove_plugin('prebuild_plugins', PLUGIN_RESOLVE_COMPOSES_KEY,\n                               'no odcs available')\n        if not self.get_value('smtp'):\n            self.remove_plugin('exit_plugins', PLUGIN_SENDMAIL_KEY, 'no mailhost available')\n        if not self.get_value('sources_command'):\n            self.remove_plugin('prebuild_plugins', PLUGIN_DISTGIT_FETCH_KEY, 'no sources command')", "response": "Remove all plugins that are not available in the config."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        user_params = None\n        build_json = get_build_json()\n        git_url = os.environ['SOURCE_URI']\n        git_ref = os.environ.get('SOURCE_REF', None)\n        image = os.environ['OUTPUT_IMAGE']\n        self.target_registry = os.environ.get('OUTPUT_REGISTRY', None)\n        self.reactor_env = None\n\n        git_commit_depth = None\n        git_branch = None\n        try:\n            user_params = os.environ['USER_PARAMS']\n            user_data = self.validate_user_data(user_params)\n            git_commit_depth = user_data.get('git_commit_depth', None)\n            git_branch = user_data.get('git_branch', None)\n            self.plugins_json = self.get_plugins_with_user_data(user_params, user_data)\n            # if we get the USER_PARAMS, we'd better get the REACTOR_CONFIG too\n            reactor_config_map = os.environ['REACTOR_CONFIG']\n            self.reactor_env = read_yaml(reactor_config_map, 'schemas/config.json')\n        except KeyError:\n            try:\n                self.plugins_json = os.environ['ATOMIC_REACTOR_PLUGINS']\n            except KeyError:\n                raise RuntimeError(\"No plugin configuration found!\")\n\n        self.plugins_json = json.loads(self.plugins_json)\n        # validate json before performing any changes\n        read_yaml(json.dumps(self.plugins_json), 'schemas/plugins.json')\n\n        input_json = {\n            'source': {\n                'provider': 'git',\n                'uri': git_url,\n                'provider_params': {\n                    'git_commit': git_ref,\n                    'git_commit_depth': git_commit_depth,\n                    'git_branch': git_branch,\n                },\n            },\n            'image': image,\n            'openshift_build_selflink': build_json.get('metadata', {}).get('selfLink', None)\n        }\n        input_json.update(self.plugins_json)\n\n        self.log.debug(\"build json: %s\", input_json)\n\n        self.remove_plugins_without_parameters()\n        # make sure the final json is valid\n        read_yaml(json.dumps(self.plugins_json), 'schemas/plugins.json')\n\n        return input_json", "response": "This method is used to run the plugin and return the result of the plugin"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_signing_intent(self):\n\n        all_signing_intents = [\n            self.odcs_config.get_signing_intent_by_keys(compose_info.get('sigkeys', []))\n            for compose_info in self.composes_info\n        ]\n\n        # Because composes_info may contain composes that were passed as\n        # plugin parameters, add the parent signing intent to avoid the\n        # overall signing intent from surpassing parent's.\n        if self._parent_signing_intent:\n            all_signing_intents.append(self._parent_signing_intent)\n\n        # Calculate the least restrictive signing intent\n        signing_intent = min(all_signing_intents, key=lambda x: x['restrictiveness'])\n\n        self.log.info('Signing intent for build is %s', signing_intent['name'])\n        self.compose_config.set_signing_intent(signing_intent['name'])", "response": "Determine the correct signing intent for the build."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying enough information is available for requesting compose.", "response": "def validate_for_request(self):\n        \"\"\"Verify enough information is available for requesting compose.\"\"\"\n        if not self.use_packages and not self.modules and not self.pulp:\n            raise ValueError(\"Nothing to compose (no packages, modules, or enabled pulp repos)\")\n\n        if self.packages and not self.koji_tag:\n            raise ValueError('koji_tag is required when packages are used')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the image inside current environment", "response": "def run(self):\n        \"\"\"\n        build image inside current environment;\n        it's expected this may run within (privileged) docker container\n\n\n        Input:\n            df_dir\n            image\n\n        Output:\n            BuildResult\n            built_image_info\n            image_id\n        \"\"\"\n        builder = self.workflow.builder\n\n        allow_repo_dir_in_dockerignore(builder.df_dir)\n        logs_gen = self.tasker.build_image_from_path(builder.df_dir,\n                                                     builder.image)\n\n        self.log.debug('build is submitted, waiting for it to finish')\n        try:\n            command_result = wait_for_command(logs_gen)\n        except docker.errors.APIError as ex:\n            return BuildResult(logs=[], fail_reason=ex.explanation)\n\n        if command_result.is_failed():\n            return BuildResult(logs=command_result.logs,\n                               fail_reason=command_result.error)\n        else:\n            image_id = builder.get_built_image_info()['Id']\n            if ':' not in image_id:\n                # Older versions of the daemon do not include the prefix\n                image_id = 'sha256:{}'.format(image_id)\n\n            return BuildResult(logs=command_result.logs, image_id=image_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the scope of the image which is in the labels dict.", "response": "def get_scope(self, which, labels):\n        \"\"\"\n        :param which: str, description of the image this belongs to\n        :param labels: dict, labels on the image\n        \"\"\"\n\n        try:\n            scope_choice = labels[self.SCOPE_LABEL]\n        except (KeyError, TypeError):\n            self.log.debug(\"no distribution scope set for %s image\", which)\n            raise NothingToCheck\n\n        try:\n            scope = self.SCOPE_NAME.index(scope_choice)\n        except ValueError:\n            self.log.warning(\"invalid label %s=%s for %s image\",\n                             self.SCOPE_LABEL, scope_choice, which)\n            raise NothingToCheck\n\n        return scope"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets json with build config from path", "response": "def run(self):\n        \"\"\"\n        get json with build config from path\n        \"\"\"\n        path = self.path or CONTAINER_BUILD_JSON_PATH\n        try:\n            with open(path, 'r') as build_cfg_fd:\n                build_cfg_json = json.load(build_cfg_fd)\n        except ValueError:\n            self.log.error(\"couldn't decode json from file '%s'\", path)\n            return None\n        except IOError:\n            self.log.error(\"couldn't read json from file '%s'\", path)\n            return None\n        else:\n            return self.substitute_configuration(build_cfg_json)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if Dockerfile sets the operator manifest label", "response": "def has_operator_manifest(self):\n        \"\"\"\n        Check if Dockerfile sets the operator manifest label\n\n        :return: bool\n        \"\"\"\n        dockerfile = df_parser(self.workflow.builder.df_path, workflow=self.workflow)\n        labels = Labels(dockerfile.labels)\n        try:\n            _, operator_label = labels.get_name_and_value(Labels.LABEL_TYPE_OPERATOR_MANIFESTS)\n        except KeyError:\n            operator_label = 'false'\n        return operator_label.lower() == 'true'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the plugin should run or skip execution.", "response": "def should_run(self):\n        \"\"\"\n        Check if the plugin should run or skip execution.\n\n        :return: bool, False if plugin should skip execution\n        \"\"\"\n        if self.is_orchestrator():\n            self.log.warning(\"%s plugin set to run on orchestrator. Skipping\", self.key)\n            return False\n        if self.operator_manifests_extract_platform != self.platform:\n            self.log.info(\"Only platform [%s] will upload operators metadata. Skipping\",\n                          self.operator_manifests_extract_platform)\n            return False\n        if is_scratch_build():\n            self.log.info(\"Scratch build. Skipping\")\n            return False\n        if not self.has_operator_manifest():\n            self.log.info(\"Operator manifests label not set in Dockerfile. Skipping\")\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n        if not self.should_run():\n            return\n\n        manifests_archive_dir = tempfile.mkdtemp()\n        image = self.workflow.image\n        # As in flatpak_create_oci, we specify command to prevent possible docker daemon errors.\n        container_dict = self.tasker.d.create_container(image, command=['/bin/bash'])\n        container_id = container_dict['Id']\n        try:\n            bits, stat = self.tasker.d.get_archive(container_id,\n                                                   IMG_MANIFESTS_PATH)\n        except APIError as ex:\n            msg = ('Could not extract operator manifest files. '\n                   'Is there a %s path in the image?' % (IMG_MANIFESTS_PATH))\n            self.log.debug('Error while trying to extract %s from image: %s',\n                           IMG_MANIFESTS_PATH, ex)\n            self.log.error(msg)\n            raise RuntimeError('%s %s' % (msg, ex))\n        finally:\n            self.tasker.d.remove_container(container_id)\n\n        with tempfile.NamedTemporaryFile() as extracted_file:\n            for chunk in bits:\n                extracted_file.write(chunk)\n            extracted_file.flush()\n            tar_archive = tarfile.TarFile(extracted_file.name)\n\n        tar_archive.extractall(manifests_archive_dir)\n        manifests_path = os.path.join(manifests_archive_dir, MANIFESTS_DIR_NAME)\n\n        manifests_zipfile_path = os.path.join(manifests_archive_dir, OPERATOR_MANIFESTS_ARCHIVE)\n        with zipfile.ZipFile(manifests_zipfile_path, 'w') as archive:\n            for root, _, files in os.walk(manifests_path):\n                for f in files:\n                    filedir = os.path.relpath(root, manifests_path)\n                    filepath = os.path.join(filedir, f)\n                    archive.write(os.path.join(root, f), filepath)\n            manifest_files = archive.namelist()\n            if not manifest_files:\n                self.log.error('Empty operator manifests directory')\n                raise RuntimeError('Empty operator manifests directory')\n            self.log.debug(\"Archiving operator manifests: %s\", manifest_files)\n\n        shutil.rmtree(manifests_path)\n\n        return manifests_zipfile_path", "response": "This plugin extracts the operator manifest files from an image saves them as a zip archive and returns its path to the operator manifests zip file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the incoming event source stream and yield event chunks.", "response": "def _read(self):\n        \"\"\"Read the incoming event source stream and yield event chunks.\n\n        Unfortunately it is possible for some servers to decide to break an\n        event into multiple HTTP chunks in the response. It is thus necessary\n        to correctly stitch together consecutive response chunks and find the\n        SSE delimiter (empty new line) to yield full, correct event chunks.\"\"\"\n        data = b''\n        for chunk in self._event_source:\n            for line in chunk.splitlines(True):\n                data += line\n                if data.endswith((b'\\r\\r', b'\\n\\n', b'\\r\\n\\r\\n')):\n                    yield data\n                    data = b''\n        if data:\n            yield data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(wants, has):\n    if wants & has == 0:\n        return False\n    if wants & has < wants:\n        return False\n    return True", "response": "Check if a desired scope wants is part of an available scope has."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_names(scope):\n    return [\n        name\n        for (name, value) in SCOPE_NAME_DICT.iteritems()\n        if check(value, scope)\n    ]", "response": "Returns a list of scope names as defined in\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_int(*names, **kwargs):\n\n    return reduce(lambda prev, next: (prev | SCOPE_NAME_DICT.get(next, 0)),\n            names, kwargs.pop('default', 0))", "response": "Converts a list of scope names into an integer value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_data(self, request, key='params'):\n        return request.session.get('%s:%s' % (constants.SESSION_KEY, key))", "response": "Get the stored data from the session store."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cache_data(self, request, data, key='params'):\n        request.session['%s:%s' % (constants.SESSION_KEY, key)] = data", "response": "Stores the data in the session store."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_data(self, request):\n        for key in request.session.keys():\n            if key.startswith(constants.SESSION_KEY):\n                del request.session[key]", "response": "Clear all OAuth related data from the session store."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef authenticate(self, request):\n        for backend in self.authentication:\n            client = backend().authenticate(request)\n            if client is not None:\n                return client\n        return None", "response": "Authenticate a client against all the backends configured in\n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_client(self, request, data):\n        client = self.get_client(data.get('client_id'))\n\n        if client is None:\n            raise OAuthError({\n                'error': 'unauthorized_client',\n                'error_description': _(\"An unauthorized client tried to access\"\n                    \" your resources.\")\n            })\n\n        form = self.get_request_form(client, data)\n\n        if not form.is_valid():\n            raise OAuthError(form.errors)\n\n        return client, form.cleaned_data", "response": "Validate a client and return the client and cleaned_data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an error to be displayed to the resource owner if anything goes wrong.", "response": "def error_response(self, request, error, **kwargs):\n        \"\"\"\n        Return an error to be displayed to the resource owner if anything goes\n        awry. Errors can include invalid clients, authorization denials and\n        other edge cases such as a wrong ``redirect_uri`` in the authorization\n        request.\n\n        :param request: :attr:`django.http.HttpRequest`\n        :param error: ``dict``\n            The different types of errors are outlined in :rfc:`4.2.2.1`\n        \"\"\"\n        ctx = {}\n        ctx.update(error)\n\n        # If we got a malicious redirect_uri or client_id, remove all the\n        # cached data and tell the resource owner. We will *not* redirect back\n        # to the URL.\n\n        if error['error'] in ['redirect_uri', 'unauthorized_client']:\n            ctx.update(next='/')\n            return self.render_to_response(ctx, **kwargs)\n\n        ctx.update(next=self.get_redirect_url(request))\n\n        return self.render_to_response(ctx, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an error response to the client with default status code of 400", "response": "def error_response(self, error, mimetype='application/json', status=400,\n            **kwargs):\n        \"\"\"\n        Return an error response to the client with default status code of\n        *400* stating the error as outlined in :rfc:`5.2`.\n        \"\"\"\n        return HttpResponse(json.dumps(error), mimetype=mimetype,\n                status=status, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef access_token_response(self, access_token):\n\n        response_data = {\n            'access_token': access_token.token,\n            'token_type': constants.TOKEN_TYPE,\n            'expires_in': access_token.get_expire_delta(),\n            'scope': ' '.join(scope.names(access_token.scope)),\n        }\n\n        # Not all access_tokens are given a refresh_token\n        # (for example, public clients doing password auth)\n        try:\n            rt = access_token.refresh_token\n            response_data['refresh_token'] = rt.token\n        except ObjectDoesNotExist:\n            pass\n\n        return HttpResponse(\n            json.dumps(response_data), mimetype='application/json'\n        )", "response": "Returns a successful response after creating the access token"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef authorization_code(self, request, data, client):\n        grant = self.get_authorization_code_grant(request, request.POST,\n                client)\n        if constants.SINGLE_ACCESS_TOKEN:\n            at = self.get_access_token(request, grant.user, grant.scope, client)\n        else:\n            at = self.create_access_token(request, grant.user, grant.scope, client)\n            rt = self.create_refresh_token(request, grant.user, grant.scope, at,\n                    client)\n\n        self.invalidate_grant(grant)\n\n        return self.access_token_response(at)", "response": "Handle the authorization code grant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef refresh_token(self, request, data, client):\n        rt = self.get_refresh_token_grant(request, data, client)\n\n        # this must be called first in case we need to purge expired tokens\n        self.invalidate_refresh_token(rt)\n        self.invalidate_access_token(rt.access_token)\n\n        at = self.create_access_token(request, rt.user, rt.access_token.scope,\n                client)\n        rt = self.create_refresh_token(request, at.user, at.scope, at, client)\n\n        return self.access_token_response(at)", "response": "Handle a refresh_token grant."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles password grant types", "response": "def password(self, request, data, client):\n        \"\"\"\n        Handle ``grant_type=password`` requests as defined in :rfc:`4.3`.\n        \"\"\"\n\n        data = self.get_password_grant(request, data, client)\n        user = data.get('user')\n        scope = data.get('scope')\n\n        if constants.SINGLE_ACCESS_TOKEN:\n            at = self.get_access_token(request, user, scope, client)\n        else:\n            at = self.create_access_token(request, user, scope, client)\n            # Public clients don't get refresh tokens\n            if client.client_type != 1:\n                rt = self.create_refresh_token(request, user, scope, at, client)\n\n        return self.access_token_response(at)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(self, request):\n        if constants.ENFORCE_SECURE and not request.is_secure():\n            return self.error_response({\n                'error': 'invalid_request',\n                'error_description': _(\"A secure connection is required.\")})\n\n        if not 'grant_type' in request.POST:\n            return self.error_response({\n                'error': 'invalid_request',\n                'error_description': _(\"No 'grant_type' included in the \"\n                    \"request.\")})\n\n        grant_type = request.POST['grant_type']\n\n        if grant_type not in self.grant_types:\n            return self.error_response({'error': 'unsupported_grant_type'})\n\n        client = self.authenticate(request)\n\n        if client is None:\n            return self.error_response({'error': 'invalid_client'})\n\n        handler = self.get_handler(grant_type)\n\n        try:\n            return handler(request, request.POST, client)\n        except OAuthError, e:\n            return self.error_response(e.args[0])", "response": "As per :rfc:`3.2` the token endpoint *only* supports POST requests."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of seconds until this token expires.", "response": "def get_expire_delta(self, reference=None):\n        \"\"\"\n        Return the number of seconds until this token expires.\n        \"\"\"\n        if reference is None:\n            reference = now()\n        expiration = self.expires\n\n        if timezone:\n            if timezone.is_aware(reference) and timezone.is_naive(expiration):\n                # MySQL doesn't support timezone for datetime fields\n                # so we assume that the date was stored in the UTC timezone\n                expiration = timezone.make_aware(expiration, timezone.utc)\n            elif timezone.is_naive(reference) and timezone.is_aware(expiration):\n                reference = timezone.make_aware(reference, timezone.utc)\n\n        timedelta = expiration - reference\n        return timedelta.days*86400 + timedelta.seconds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride the default cleaning behaviour to exit early on errors", "response": "def _clean_fields(self):\n        \"\"\"\n        Overriding the default cleaning behaviour to exit early on errors\n        instead of validating each field.\n        \"\"\"\n        try:\n            super(OAuthForm, self)._clean_fields()\n        except OAuthValidationError, e:\n            self._errors.update(e.args[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlink to the OAuth2 draft.", "response": "def rfclink(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"Link to the OAuth2 draft.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    :param name: The role name used in the document.\n    :param rawtext: The entire markup snippet, with role.\n    :param text: The text marked with the role.\n    :param lineno: The line number where rawtext appears in the input.\n    :param inliner: The inliner instance that called us.\n    :param options: Directive options for customization.\n    :param content: The directive content for customization.\n    \"\"\"\n\n    node = nodes.reference(rawtext, \"Section \" + text, refuri=\"%s#section-%s\" % (base_url, text))\n\n    return [node], []"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates that the input is a list or tuple.", "response": "def validate(self, value):\n        \"\"\"\n        Validates that the input is a list or tuple.\n        \"\"\"\n        if self.required and not value:\n            raise OAuthValidationError({'error': 'invalid_request'})\n\n        # Validate that each value in the value list is in self.choices.\n        for val in value:\n            if not self.valid_value(val):\n                raise OAuthValidationError({\n                    'error': 'invalid_request',\n                    'error_description': _(\"'%s' is not a valid scope.\") % \\\n                            val})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the scope of the current context.", "response": "def clean_scope(self):\n        \"\"\"\n        The scope is assembled by combining all the set flags into a single\n        integer value which we can later check again for set bits.\n\n        If *no* scope is set, we return the default scope which is the first\n        defined scope in :attr:`provider.constants.SCOPES`.\n\n        \"\"\"\n        default = SCOPES[0][0]\n\n        flags = self.cleaned_data.get('scope', [])\n\n        return scope.to_int(default=default, *flags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the response_type is valid and returns it.", "response": "def clean_response_type(self):\n        \"\"\"\n        :rfc:`3.1.1` Lists of values are space delimited.\n        \"\"\"\n        response_type = self.cleaned_data.get('response_type')\n\n        if not response_type:\n            raise OAuthValidationError({'error': 'invalid_request',\n                'error_description': \"No 'response_type' supplied.\"})\n\n        types = response_type.split(\" \")\n\n        for type in types:\n            if type not in RESPONSE_TYPE_CHOICES:\n                raise OAuthValidationError({\n                    'error': 'unsupported_response_type',\n                    'error_description': u\"'%s' is not a supported response \"\n                        \"type.\" % type})\n\n        return response_type"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_redirect_uri(self):\n        redirect_uri = self.cleaned_data.get('redirect_uri')\n\n        if redirect_uri:\n            if not redirect_uri == self.client.redirect_uri:\n                raise OAuthValidationError({\n                    'error': 'invalid_request',\n                    'error_description': _(\"The requested redirect didn't \"\n                        \"match the client settings.\")})\n\n        return redirect_uri", "response": "Validate that the redirect_uri is the same as the client s redirect_uri."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean(self):\n        data = self.cleaned_data\n        want_scope = data.get('scope') or 0\n        refresh_token = data.get('refresh_token')\n        access_token = getattr(refresh_token, 'access_token', None) if \\\n            refresh_token else \\\n            None\n        has_scope = access_token.scope if access_token else 0\n\n        # Only check if we've actually got a scope in the data\n        # (read: All fields have been cleaned)\n        if want_scope is not 0 and not scope.check(want_scope, has_scope):\n            raise OAuthValidationError({'error': 'invalid_scope'})\n\n        return data", "response": "Check that the scope is less or equal to the previous scope!"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean(self):\n        data = self.cleaned_data\n        want_scope = data.get('scope') or 0\n        grant = data.get('grant')\n        has_scope = grant.scope if grant else 0\n\n        # Only check if we've actually got a scope in the data\n        # (read: All fields have been cleaned)\n        if want_scope is not 0 and not scope.check(want_scope, has_scope):\n            raise OAuthValidationError({'error': 'invalid_scope'})\n\n        return data", "response": "Check that the scope is less or equal to the scope allowed on the the\n        grant!"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef short_token():\n    hash = hashlib.sha1(shortuuid.uuid())\n    hash.update(settings.SECRET_KEY)\n    return hash.hexdigest()[::2]", "response": "Generate a short token that can be used as an application identifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize_instance(instance):\n    ret = dict([(k, v)\n                for k, v in instance.__dict__.items()\n                if not k.startswith('_')])\n    return json.loads(json.dumps(ret, cls=DjangoJSONEncoder))", "response": "Serialize a model to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deserialize_instance(model, data={}):\n    \"Translate raw data into a model instance.\"\n    ret = model()\n    for k, v in data.items():\n        if v is not None:\n            try:\n                f = model._meta.get_field(k)\n                if isinstance(f, DateTimeField):\n                    v = dateparse.parse_datetime(v)\n                elif isinstance(f, TimeField):\n                    v = dateparse.parse_time(v)\n                elif isinstance(f, DateField):\n                    v = dateparse.parse_date(v)\n            except FieldDoesNotExist:\n                pass\n        setattr(ret, k, v)\n    return ret", "response": "Translate raw data into a model instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_entities(tweet):\n\n    entity_key = \"entities\" if is_original_format(tweet) else \"twitter_entities\"\n    if get_tweet_type(tweet) == \"retweet\":\n        retweet_entities = tweet.get(entity_key, [])\n        all_entities = get_retweeted_tweet(tweet).get(entity_key,[]).copy()\n        # the only thing that the Retweet will have that the Retweeted Tweet\n        # won't have is the @-mention of the RTd user at the front (\"RT @someone:\")\n        # I'm going to add that in, so the the Retweet's entities are a superset\n        # of the RTd Tweet's entites\n        all_entities[\"user_mentions\"] = ([retweet_entities[\"user_mentions\"][0]] +\n            all_entities[\"user_mentions\"])\n        return all_entities\n    else:\n        return tweet.get(entity_key, [])", "response": "Get the entities of a Tweet in a new order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_media_entities(tweet):\n\n    ext_ents_key = \"extended_entities\" if is_original_format(tweet) else \"twitter_extended_entities\"\n    ext_ents = tweet.get(ext_ents_key)\n    media = ext_ents.get(\"media\", []) if ext_ents else []\n    return media", "response": "Get all the media entities contained within a tweet."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the https links to each media entity in the tweet.", "response": "def get_media_urls(tweet):\n    \"\"\"\n    Gets the https links to each media entity in the tweet.\n\n    Args:\n        tweet (Tweet or dict): tweet\n\n    Returns:\n        list: list of urls. Will be an empty list if there are no urls present.\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_entities import get_media_urls\n        >>> tweet = {'created_at': '2017-21-23T15:21:21.000Z',\n        ...          'entities': {'user_mentions': [{'id': 2382763597,\n        ...          'id_str': '2382763597',\n        ...          'indices': [14, 26],\n        ...          'name': 'Fiona',\n        ...          'screen_name': 'notFromShrek'}]},\n        ...          'extended_entities': {'media': [{'display_url': 'pic.twitter.com/something',\n        ...          'expanded_url': 'https://twitter.com/something',\n        ...          'id': 4242,\n        ...          'id_str': '4242',\n        ...          'indices': [88, 111],\n        ...          'media_url': 'http://pbs.twimg.com/media/something.jpg',\n        ...          'media_url_https': 'https://pbs.twimg.com/media/something.jpg',\n        ...          'sizes': {'large': {'h': 1065, 'resize': 'fit', 'w': 1600},\n        ...          'medium': {'h': 799, 'resize': 'fit', 'w': 1200},\n        ...          'small': {'h': 453, 'resize': 'fit', 'w': 680},\n        ...          'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n        ...          'type': 'photo',\n        ...          'url': 'https://t.co/something'},\n        ...          {'display_url': 'pic.twitter.com/something_else',\n        ...          'expanded_url': 'https://twitter.com/user/status/something/photo/1',\n        ...          'id': 4243,\n        ...          'id_str': '4243',\n        ...          'indices': [88, 111],\n        ...          'media_url': 'http://pbs.twimg.com/media/something_else.jpg',\n        ...          'media_url_https': 'https://pbs.twimg.com/media/something_else.jpg',\n        ...          'sizes': {'large': {'h': 1065, 'resize': 'fit', 'w': 1600},\n        ...          'medium': {'h': 799, 'resize': 'fit', 'w': 1200},\n        ...          'small': {'h': 453, 'resize': 'fit', 'w': 680},\n        ...          'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n        ...          'type': 'photo',\n        ...          'url': 'https://t.co/something_else'}]}\n        ...         }\n        >>> get_media_urls(tweet)\n        ['https://pbs.twimg.com/media/something.jpg', 'https://pbs.twimg.com/media/something_else.jpg']\n    \"\"\"\n\n    media = get_media_entities(tweet)\n    urls = [m.get(\"media_url_https\") for m in media] if media else []\n    return urls"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the user mentions in the Tweet as a list of dicts.", "response": "def get_user_mentions(tweet):\n    \"\"\"\n    Get the @-mentions in the Tweet as dictionaries.\n    Note that in the case of a quote-tweet, this does not return the users\n    mentioned in the quoted status. The recommended way to get that list would\n    be to use get_user_mentions on the quoted status.\n    Also note that in the caes of a quote-tweet, the list of @-mentioned users\n    does not include the user who authored the original (quoted) Tweet.\n\n    Args:\n        tweet (Tweet or dict): A Tweet object or dictionary\n\n    Returns:\n        list (list of dicts): 1 item per @ mention. Note that the fields here\n        aren't enforced by the parser, they are simply the fields as they\n        appear in a Tweet data payload.\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_entities import get_user_mentions\n        >>> original = {\"created_at\": \"Wed May 24 20:17:19 +0000 2017\",\n        ...             \"text\": \"RT @notFromShrek: Stuff! Words! ...\",\n        ...             \"entities\": {\"user_mentions\": [{\n        ...                              \"indices\": [2,12], #characters where the @ mention appears\n        ...                              \"id_str\": \"2382763597\", #id of @ mentioned user as a string\n        ...                              \"screen_name\": \"notFromShrek\", #screen_name of @d user\n        ...                              \"name\": \"Fiona\", #display name of @ mentioned user\n        ...                              \"id\": 2382763597 #id of @ mentioned user as an int\n        ...                            }]\n        ...                          },\n        ...             \"retweeted_status\": {\n        ...                 \"created_at\": \"Wed May 24 20:01:19 +0000 2017\",\n        ...                 \"text\": \"Stuff! Words! #Tweeting!\",\n        ...                 \"entities\": {\"user_mentions\": []}\n        ...                 }\n        ...             }\n        >>> get_user_mentions(original)\n        [{'indices': [2, 12], 'id_str': '2382763597', 'screen_name': 'notFromShrek', 'name': 'Fiona', 'id': 2382763597}]\n    \"\"\"\n    entities = get_entities(tweet)\n    user_mentions = entities.get(\"user_mentions\") if entities else None\n    return user_mentions if user_mentions else []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of hashtags in the Tweet", "response": "def get_hashtags(tweet):\n    \"\"\"\n    Get a list of hashtags in the Tweet\n    Note that in the case of a quote-tweet, this does not return the\n    hashtags in the quoted status.\n\n    Args:\n        tweet (Tweet or dict): A Tweet object or dictionary\n\n    Returns:\n        list (a list of strings): list of all of the hashtags in the Tweet\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_entities import get_hashtags\n        >>> original = {\"created_at\": \"Wed May 24 20:17:19 +0000 2017\",\n        ...            \"entities\": {\"hashtags\": [{\"text\":\"1hashtag\"}]}}\n        >>> get_hashtags(original)\n        ['1hashtag']\n\n        >>> activity = {\"postedTime\": \"2017-05-24T20:17:19.000Z\",\n        ...             \"verb\": \"post\",\n        ...             \"twitter_entities\": {\"hashtags\": [\n        ...                     {\"text\":\"1hashtag\"},\n        ...                     {\"text\": \"moreHashtags\"}]}}\n        >>> get_hashtags(activity)\n        ['1hashtag', 'moreHashtags']\n    \"\"\"\n    entities = get_entities(tweet)\n    hashtags = entities.get(\"hashtags\")\n    hashtags = [tag[\"text\"] for tag in hashtags] if hashtags else []\n    return hashtags"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the retweeted Tweet OR the quoted Tweet and return it as a dictionary", "response": "def get_embedded_tweet(tweet):\n    \"\"\"\n    Get the retweeted Tweet OR the quoted Tweet and return it as a dictionary\n\n    Args:\n        tweet (Tweet): A Tweet object (not simply a dict)\n\n    Returns:\n        dict (or None, if the Tweet is neither a quote tweet or a Retweet):\n        a dictionary representing the quote Tweet or the Retweet\n    \"\"\"\n    if tweet.retweeted_tweet is not None:\n        return tweet.retweeted_tweet\n    elif tweet.quoted_tweet is not None:\n        return tweet.quoted_tweet\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the bio text of the user who posted the Tweet", "response": "def get_bio(tweet):\n    \"\"\"\n    Get the bio text of the user who posted the Tweet\n\n    Args:\n        tweet (Tweet): A Tweet object (or a dictionary)\n\n    Returns:\n        str: the bio text of the user who posted the Tweet\n        In a payload the abscence of a bio seems to be represented by an\n        empty string or a None, this getter always returns a string (so, empty\n        string if no bio is available).\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_user import get_bio\n        >>> original_format_dict = {\n        ...             \"created_at\": \"Wed May 24 20:17:19 +0000 2017\",\n        ...             \"user\":\n        ...              {\"description\": \"Niche millenial content aggregator\"}\n        ...            }\n        >>> get_bio(original_format_dict)\n        'Niche millenial content aggregator'\n\n        >>> activity_streams_format_dict = {\n        ...             \"postedTime\": \"2017-05-24T20:17:19.000Z\",\n        ...             \"actor\":\n        ...              {\"summary\": \"Niche millenial content aggregator\"}\n        ...             }\n        >>> get_bio(activity_streams_format_dict)\n        'Niche millenial content aggregator'\n    \"\"\"\n\n    if is_original_format(tweet):\n        bio_or_none = tweet[\"user\"].get(\"description\", \"\")\n    else:\n        bio_or_none = tweet[\"actor\"].get(\"summary\", \"\")\n    if bio_or_none is None:\n        return \"\"\n    else:\n        return bio_or_none"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the Klout score of a Tweet", "response": "def get_klout_score(tweet):\n    \"\"\"\n    Warning: Klout is deprecated and is being removed from Tweet payloads May 2018. \\n\n    See https://developer.twitter.com/en/docs/tweets/enrichments/overview/klout \\n\n\n    Get the Klout score (int) (if it exists) of the user who posted the Tweet\n\n    Args:\n        tweet (Tweet): A Tweet object (or a dictionary)\n\n    Returns:\n        int: the Klout score (if it exists) of the user who posted the Tweet\n            else return None\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_user import get_klout_score\n        >>> original_format_dict = {\n        ...             \"created_at\": \"Wed May 24 20:17:19 +0000 2017\",\n        ...             \"user\":\n        ...              {\"derived\": {\"klout\": {\"score\": 12345}}}\n        ...            }\n        >>> get_klout_score(original_format_dict)\n        12345\n\n        >>> activity_streams_format_dict = {\n        ...             \"postedTime\": \"2017-05-24T20:17:19.000Z\",\n        ...             \"gnip\":{\"klout_score\": 12345}}\n        >>> get_klout_score(activity_streams_format_dict)\n        12345\n    \"\"\"\n    try:\n        if is_original_format(tweet):\n            score = tweet['user']['derived']['klout']['score']\n        else:\n            score = tweet['gnip']['klout_score']\n        return score\n    except KeyError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the Klout profile URL of the user", "response": "def get_klout_profile(tweet):\n    \"\"\"\n    Warning: Klout is deprecated and is being removed from Tweet payloads May 2018. \\n\n    See https://developer.twitter.com/en/docs/tweets/enrichments/overview/klout \\n\n    \n    Get the Klout profile URL of the user (str) (if it exists)\n\n    Args:\n        tweet (Tweet): A Tweet object (or a dictionary)\n\n    Returns:\n        str: the user's Klout profile URL (if it exists), else return None\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_user import get_klout_profile\n        >>> original_format_dict = {\n        ... \"created_at\": \"Wed May 24 20:17:19 +0000 2017\",\n        ... \"user\":\n        ...     {\"derived\": {\"klout\":\n        ...         {\"profile_url\":\n        ...             \"http://klout.com/topic/id/10000000000000016635\"}}}\n        ... }\n        >>> get_klout_profile(original_format_dict)\n        'http://klout.com/topic/id/10000000000000016635'\n\n        >>> activity_streams_format_dict = {\n        ... \"postedTime\": \"2017-05-24T20:17:19.000Z\",\n        ... \"gnip\":\n        ...     {\"klout_profile\": {\n        ...         \"link\": \"http://klout.com/topic/id/10000000000000016635\"}\n        ...     }\n        ... }\n        >>> get_klout_profile(activity_streams_format_dict)\n        'http://klout.com/topic/id/10000000000000016635'\n    \"\"\"\n    try:\n        if is_original_format(tweet):\n            profile = tweet['user']['derived']['klout']['profile_url']\n        else:\n            profile = tweet['gnip']['klout_profile']['link']\n        return profile\n    except KeyError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_klout_id(tweet):\n    try:\n        if is_original_format(tweet):\n            klout_id = tweet['user']['derived']['klout']['user_id']\n        else:\n            klout_id = tweet['gnip']['klout_profile']['klout_user_id']\n        return klout_id\n    except KeyError:\n        return None", "response": "Get the Klout ID of the user in the Tweet"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of dicts representing Klout topics for a given Tweet.", "response": "def get_klout_topics(tweet, topic_type='influence'):\n    \"\"\"\n    Warning: Klout is deprecated and is being removed from Tweet payloads May 2018. \\n\n    See https://developer.twitter.com/en/docs/tweets/enrichments/overview/klout \\n\n    \n    Get the user's chosen Klout topics (a list of dicts), if it exists.\n    Regardless of format or topic type, topic dicts will have the same keys:\n    \"url\", \"id\", \"name\", \"score\"\n\n    Args:\n        tweet (Tweet): A Tweet object\n        topic_type (str): Which type of Klout topic to return.\n                          Options are limited to 'influence' and 'interest'\n\n    Returns:\n        list: A list of dicts representing Klout topics, or if Klout topics \\\n        do not exist in the Tweet payload, return None. The list is sorted by\n        the \"score\" value.\n\n    Example:\n        >>> result = [{\n        ...     # the user's score for that topic\n        ...     \"score\": 0.54,\n        ...     # the Klout topic ID\n        ...     \"id\": \"10000000000000019376\",\n        ...     # the Klout topic URL\n        ...     \"url\": \"http://klout.com/topic/id/10000000000000019376\",\n        ...     # the Klout topic name\n        ...     \"name\": \"Emoji\"\n        ... },\n        ... {\n        ... \"score\": 0.43,\n        ... \"id\": \"9159\",\n        ... \"url\": \"http://klout.com/topic/id/9159\",\n        ... \"name\": \"Vegetables\"\n        ... }]\n    \"\"\"\n    try:\n        # check that the dict paths exist\n        if is_original_format(tweet):\n            topics = tweet['user']['derived']['klout']['{}_topics'.format(topic_type)]\n        else:\n            topics = tweet['gnip']['klout_profile']['topics']\n    except KeyError:\n        return None\n    # since we have topics, collect the right pieces\n    topics_list = []\n    if is_original_format(tweet):\n        for topic in topics:\n            # note: this is the same as the current structure of OF\n            # payloads, but is written out for consistency w/ AS payloads\n            this_topic = dict(url=topic['url'],\n                              id=topic['id'],\n                              name=topic['name'],\n                              score=topic['score'])\n            topics_list.append(this_topic)\n    else:\n        relevant_topics = [x for x in topics if x['topic_type'] == topic_type]\n        for topic in relevant_topics:\n            this_topic = dict(url=topic['link'],\n                              id=topic['id'],\n                              name=topic['displayName'],\n                              score=topic['score'])\n            topics_list.append(this_topic)\n    sorted_topics_list = sorted(topics_list, key=lambda x: x['score'])\n    return sorted_topics_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_original_format(tweet):\n    # deleted due to excess checking; it's a key lookup and does not need any\n    # operational optimization\n    if \"created_at\" in tweet:\n        original_format = True\n    elif \"postedTime\" in tweet:\n        original_format = False\n    else:\n        raise NotATweetError(\"This dict has neither 'created_at' or 'postedTime' as keys\")\n    return original_format", "response": "Simple checker to flag the format of a tweet in a key - based dict."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_keys(tweet, parent_key=''):\n    items = []\n    for k, v in tweet.items():\n        new_key = parent_key + \" \" + k\n        if isinstance(v, dict):\n            items.extend(get_all_keys(v, parent_key=new_key))\n        else:\n            items.append(new_key.strip(\" \"))\n    return items", "response": "Takes a tweet object and recursively returns a list of all keys contained in this level and all nexstted levels of the tweet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the keys present in a Tweet are present in a Tweet.", "response": "def key_validation_check(tweet_keys_list, superset_keys, minset_keys):\n    \"\"\"\n    Validates the keys present in a Tweet.\n\n    Args:\n        tweet_keys_list (list): the keys present in a tweet\n        superset_keys (set): the set of all possible keys for a tweet\n        minset_keys (set): the set of minimal keys expected in a tweet.\n\n    Returns:\n        0 if no errors\n\n    Raises:\n        UnexpectedFormatError on any mismatch of keys.\n    \"\"\"\n    # check for keys that must be present\n    tweet_keys = set(tweet_keys_list)\n    minset_overlap = tweet_keys & minset_keys\n    if minset_overlap != minset_keys:\n        raise UnexpectedFormatError(\"keys ({}) missing from Tweet (Public API data is not supported)\"\n                                    .format(minset_keys - tweet_keys))\n    # check for keys that could be present\n    unexpected_keys = tweet_keys - superset_keys\n    if len(unexpected_keys) > 0:\n        raise UnexpectedFormatError(\"Unexpected keys ({}) are in this Tweet\"\n                                    .format(unexpected_keys))\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_tweet(tweet, validation_checking=False):\n\n    if \"id\" not in tweet:\n        raise NotATweetError(\"This text has no 'id' key\")\n\n    original_format = is_original_format(tweet)\n\n    if original_format:\n        _check_original_format_tweet(tweet, validation_checking=validation_checking)\n    else:\n        _check_activity_streams_tweet(tweet, validation_checking=validation_checking)\n\n    return original_format", "response": "Checks if a tweet is valid and determines the type of format for the tweet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_full_text(tweet):\n    if is_original_format(tweet):\n        if tweet[\"truncated\"]:\n            return tweet[\"extended_tweet\"][\"full_text\"]\n        else:\n            return tweet[\"text\"]\n    else:\n        if \"long_object\" in tweet:\n            return tweet[\"long_object\"][\"body\"]\n        else:\n            return tweet[\"body\"]", "response": "Get the full text of a Tweet dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_lang(tweet):\n    if is_original_format(tweet):\n        lang_field = \"lang\"\n    else:\n        lang_field = \"twitter_lang\"\n    if tweet[lang_field] is not None and tweet[lang_field] != \"und\":\n            return tweet[lang_field]\n    else:\n        return None", "response": "Get the language that the Tweet is written in."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_poll_options(tweet):\n    if is_original_format(tweet):\n        try:\n            poll_options_text = []\n            for p in tweet[\"entities\"][\"polls\"]:\n                for o in p[\"options\"]:\n                    poll_options_text.append(o[\"text\"])\n            return poll_options_text\n        except KeyError:\n            return []\n\n    else:\n        raise NotAvailableError(\"Gnip activity-streams format does not\" +\n                                \" return poll options\")", "response": "Get the text in the options of a poll in a list of strings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the quoted or retweeted text in a TweetTree or TweetTreeTree.", "response": "def get_quote_or_rt_text(tweet):\n    \"\"\"\n    Get the quoted or retweeted text in a Tweet\n    (this is not the text entered by the posting user)\n    - tweet: empty string (there is no quoted or retweeted text)\n    - quote: only the text of the quoted Tweet\n    - retweet: the text of the retweet\n\n    Args:\n        tweet (Tweet or dict): A Tweet object or dictionary\n\n    Returns:\n        str: text of the retweeted-tweet or the quoted-tweet\n        (empty string if this is an original Tweet)\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_text import get_quote_or_rt_text\n        >>> # a quote tweet\n        >>> quote = {\"created_at\": \"Wed May 24 20:17:19 +0000 2017\",\n        ...          \"text\": \"adding my own commentary\",\n        ...          \"truncated\": False,\n        ...          \"quoted_status\": {\n        ...                 \"created_at\": \"Mon May 01 05:00:05 +0000 2017\",\n        ...                 \"truncated\": False,\n        ...                 \"text\": \"an interesting Tweet\"\n        ...                }\n        ...         }\n\n        >>> get_quote_or_rt_text(quote)\n        'an interesting Tweet'\n    \"\"\"\n    tweet_type = get_tweet_type(tweet)\n    if tweet_type == \"tweet\":\n        return \"\"\n    if tweet_type == \"quote\":\n        if is_original_format(tweet):\n            return get_full_text(tweet[\"quoted_status\"])\n        else:\n            return get_full_text(tweet[\"twitter_quoted_status\"])\n    if tweet_type == \"retweet\":\n        if is_original_format(tweet):\n            return get_full_text(tweet[\"retweeted_status\"])\n        else:\n            return get_full_text(tweet[\"object\"])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_text(tweet):\n    if is_original_format(tweet):\n        return \"\\n\".join(filter(None, [tweet.user_entered_text,\n                                       tweet.quote_or_rt_text,\n                                       \"\\n\".join(tweet.poll_options)]))\n    else:\n        return \"\\n\".join(filter(None, [tweet.user_entered_text,\n                                       tweet.quote_or_rt_text]))", "response": "Get all of the text of the Tweet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_links(text):\n    tco_link_regex = re.compile(\"https?://t.co/[A-z0-9].*\")\n    generic_link_regex = re.compile(\"(https?://)?(\\w*[.]\\w+)+([/?=&]+\\w+)*\")\n    remove_tco = re.sub(tco_link_regex, \" \", text)\n    remove_generic = re.sub(generic_link_regex, \" \", remove_tco)\n    return remove_generic", "response": "Helper function to remove the links from the input text"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_matching_rules(tweet):\n    if is_original_format(tweet):\n        rules = tweet.get(\"matching_rules\")\n    else:\n        gnip = tweet.get(\"gnip\")\n        rules = gnip.get(\"matching_rules\") if gnip else None\n    return rules", "response": "Returns the list of matching rules for a tweet with a gnip field enrichment."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the user s geo coordinates in the payload Returns None if unavaiable", "response": "def get_geo_coordinates(tweet):\n    \"\"\"\n    Get the user's geo coordinates, if they are included in the payload\n    (otherwise return None)\n\n    Args:\n        tweet (Tweet or dict): A Tweet object or dictionary\n\n    Returns:\n        dict: dictionary with the keys \"latitude\" and \"longitude\"\n              or, if unavaiable, None\n\n    Example:\n        >>> from tweet_parser.getter_methods.tweet_geo import get_geo_coordinates\n        >>> tweet_geo = {\"geo\": {\"coordinates\": [1,-1]}}\n        >>> get_geo_coordinates(tweet_geo)\n        {'latitude': 1, 'longitude': -1}\n\n        >>> tweet_no_geo = {\"geo\": {}}\n        >>> get_geo_coordinates(tweet_no_geo) #returns None\n    \"\"\"\n    if \"geo\" in tweet:\n        if tweet[\"geo\"] is not None:\n            if \"coordinates\" in tweet[\"geo\"]:\n                [lat, lon] = tweet[\"geo\"][\"coordinates\"]\n                return {\"latitude\": lat, \"longitude\": lon}\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets user s derived location data from the profile location enrichment", "response": "def get_profile_location(tweet):\n    \"\"\"\n    Get user's derived location data from the profile location enrichment\n    If unavailable, returns None.\n\n    Args:\n        tweet (Tweet or dict): Tweet object or dictionary\n\n    Returns:\n        dict: more information on the profile locations enrichment here:\n        http://support.gnip.com/enrichments/profile_geo.html\n\n    Example:\n        >>> result = {\"country\": \"US\",         # Two letter ISO-3166 country code\n        ...           \"locality\": \"Boulder\",   # The locality location (~ city)\n        ...           \"region\": \"Colorado\",    # The region location (~ state/province)\n        ...           \"sub_region\": \"Boulder\", # The sub-region location (~ county)\n        ...           \"full_name\": \"Boulder, Colorado, US\", # The full name (excluding sub-region)\n        ...           \"geo\":  [40,-105]        # lat/long value that coordinate that corresponds to\n        ...                            # the lowest granularity location for where the user\n        ...                            # who created the Tweet is from\n        ...  }\n\n    Caveats:\n        This only returns the first element of the 'locations' list.\n        I'm honestly not sure what circumstances would result in a list that\n        is more than one element long.\n    \"\"\"\n    if is_original_format(tweet):\n        try:\n            return tweet[\"user\"][\"derived\"][\"locations\"][0]\n        except KeyError:\n            return None\n    else:\n        try:\n            location = tweet[\"gnip\"][\"profileLocations\"][0]\n            reconstructed_original_format = {}\n            if location[\"address\"].get(\"country\", None) is not None:\n                reconstructed_original_format[\"country\"] = location[\"address\"][\"country\"]\n            if location[\"address\"].get(\"countryCode\", None) is not None:\n                reconstructed_original_format[\"country_code\"] = location[\"address\"][\"countryCode\"]\n            if location[\"address\"].get(\"locality\", None) is not None:\n                reconstructed_original_format[\"locality\"] = location[\"address\"][\"locality\"]\n            if location[\"address\"].get(\"region\", None) is not None:\n                reconstructed_original_format[\"region\"] = location[\"address\"][\"region\"]\n            if location[\"address\"].get(\"subRegion\", None) is not None:\n                reconstructed_original_format[\"sub_region\"] = location[\"address\"][\"subRegion\"]\n            if location.get(\"displayName\", None) is not None:\n                reconstructed_original_format[\"full_name\"] = location[\"displayName\"]\n            if location.get(\"geo\", None) is not None:\n                reconstructed_original_format[\"geo\"] = location[\"geo\"]\n            return reconstructed_original_format\n        except KeyError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tweet_links(tweet):\n    if is_original_format(tweet):\n        # get the urls from the Tweet\n        try:\n            tweet_urls = tweet[\"entities\"][\"urls\"]\n        except KeyError:\n            tweet_urls = []\n        # get the urls from the quote-tweet\n        if tweet.quoted_tweet is not None:\n            tweet_urls += tweet.quoted_tweet.tweet_links\n        # get the urls from the retweet\n        if tweet.retweeted_tweet is not None:\n            tweet_urls += tweet.retweeted_tweet.tweet_links\n        return tweet_urls\n    else:\n        # try to get normal urls\n        try:\n            tweet_urls = tweet[\"twitter_entities\"][\"urls\"]\n        except KeyError:\n            tweet_urls = []\n        # get the urls from the quote-tweet\n        if tweet.quoted_tweet is not None:\n            tweet_urls += tweet.quoted_tweet.tweet_links\n        # get the urls from the retweet\n        if tweet.retweeted_tweet is not None:\n            tweet_urls += tweet.retweeted_tweet.tweet_links\n        # otherwise, we're now going to combine the urls to try to\n        # to get the same format as the og format urls, try to get enriched urls\n        try:\n            gnip_tweet_urls = {x[\"url\"]: x for x in tweet[\"gnip\"][\"urls\"]}\n            gnip_tweet_exp_urls = {x[\"expanded_url\"]: x for x in tweet[\"gnip\"][\"urls\"]}\n        except KeyError:\n            return tweet_urls\n        key_mappings = {\"expanded_url\": \"url\",\n                        \"expanded_status\": \"status\",\n                        \"expanded_url_title\": \"title\",\n                        \"expanded_url_description\": \"description\"}\n        tweet_urls_expanded = []\n        for url in tweet_urls:\n            expanded_url = url\n            if url[\"url\"] in gnip_tweet_urls:\n                expanded_url[\"unwound\"] = {key_mappings[key]: value for key, value in gnip_tweet_urls[url[\"url\"]].items() if key != \"url\"}\n            elif url.get(\"expanded_url\", \"UNAVAILABLE\") in gnip_tweet_exp_urls:\n                expanded_url[\"unwound\"] = {key_mappings[key]: value for key, value in gnip_tweet_urls[url[\"expanded_url\"]].items() if key != \"url\"}\n            tweet_urls_expanded.append(expanded_url)\n        return tweet_urls_expanded", "response": "Returns a list of dictionaries containing the links that are included in the Tweet as unrolled or expanded_url information if it is available."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of the most unrolled urls available in the Tweet.", "response": "def get_most_unrolled_urls(tweet):\n    \"\"\"\n    For each url included in the Tweet \"urls\", get the most unrolled\n    version available. Only return 1 url string per url in tweet.tweet_links\n    In order of preference for \"most unrolled\"\n    (keys from the dict at tweet.tweet_links): \\n\n    1. `unwound`/`url` \\n\n    2. `expanded_url` \\n\n    3. `url`\n\n    Args:\n        tweet (Tweet): A Tweet object or dict\n\n    Returns:\n        list (list of strings): a list of the most unrolled url available\n    \"\"\"\n    unrolled_urls = []\n    for url in get_tweet_links(tweet):\n        if url.get(\"unwound\", {\"url\": None}).get(\"url\", None) is not None:\n            unrolled_urls.append(url[\"unwound\"][\"url\"])\n        elif url.get(\"expanded_url\", None) is not None:\n            unrolled_urls.append(url[\"expanded_url\"])\n        else:\n            unrolled_urls.append(url[\"url\"])\n    return unrolled_urls"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_generator(tweet):\n    if is_original_format(tweet):\n        if sys.version_info[0] == 3 and sys.version_info[1] >= 4:\n            parser = GeneratorHTMLParser(convert_charrefs=True)\n        else:\n            parser = GeneratorHTMLParser()\n        parser.feed(tweet[\"source\"])\n        return {\"link\": parser.generator_link,\n                \"name\": parser.generator_name}\n    else:\n        return {\"link\": tweet[\"generator\"][\"link\"],\n                \"name\": tweet[\"generator\"][\"displayName\"]}", "response": "Get information about the application that generated the Tweet"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lazy_property(fn):\n    attr_name = '_lazy_' + fn.__name__\n\n    @property\n    @wraps(fn)\n    def _lazy_property(self):\n        if not hasattr(self, attr_name):\n            setattr(self, attr_name, fn(self))\n        return getattr(self, attr_name)\n    return _lazy_property", "response": "Decorator that makes a property lazy - evaluated whilst preserving\n    docstrings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deprecated(reason):\n\n    if isinstance(reason, string_types):\n\n        # The @deprecated is used with a 'reason'.\n        #\n        # .. code-block:: python\n        #\n        #    @deprecated(\"please, use another function\")\n        #    def old_function(x, y):\n        #      pass\n\n        def decorator(func1):\n\n            if inspect.isclass(func1):\n                fmt1 = \"Call to deprecated class {name} ({reason}).\"\n            else:\n                fmt1 = \"Call to deprecated function {name} ({reason}).\"\n\n            @functools.wraps(func1)\n            def new_func1(*args, **kwargs):\n                #warnings.simplefilter('default', DeprecationWarning)\n                warnings.warn(\n                    fmt1.format(name=func1.__name__, reason=reason),\n                    category=FieldDeprecationWarning,\n                    stacklevel=2\n                )\n                #warnings.simplefilter('default', DeprecationWarning)\n                return func1(*args, **kwargs)\n\n            return new_func1\n\n        return decorator\n\n    elif inspect.isclass(reason) or inspect.isfunction(reason):\n\n        # The @deprecated is used without any 'reason'.\n        #\n        # .. code-block:: python\n        #\n        #    @deprecated\n        #    def old_function(x, y):\n        #      pass\n\n        func2 = reason\n\n        if inspect.isclass(func2):\n            fmt2 = \"Call to deprecated class {name}.\"\n        else:\n            fmt2 = \"Call to deprecated function {name}.\"\n\n        @functools.wraps(func2)\n        def new_func2(*args, **kwargs):\n            #warnings.simplefilter('default', DeprecationWarning)\n            warnings.warn(\n                fmt2.format(name=func2.__name__),\n                category=FieldDeprecationWarning,\n                stacklevel=2\n            )\n            #warnings.simplefilter('default', DeprecationWarning)\n            return func2(*args, **kwargs)\n\n        return new_func2\n\n    else:\n        raise TypeError(repr(type(reason)))", "response": "A function decorator which marks functions\n    as deprecated."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the quoted Tweet as a Tweet object", "response": "def quoted_tweet(self):\n        \"\"\"\n        The quoted Tweet as a Tweet object\n        If the Tweet is not a quote Tweet, return None\n        If the quoted Tweet payload cannot be loaded as a Tweet, this will\n        raise a \"NotATweetError\"\n\n        Returns:\n            Tweet: A Tweet representing the quoted status (or None)\n            (see tweet_embeds.get_quote_tweet, this is that value as a Tweet)\n\n        Raises:\n            NotATweetError: if quoted tweet is malformed\n        \"\"\"\n        quote_tweet = tweet_embeds.get_quoted_tweet(self)\n        if quote_tweet is not None:\n            try:\n                return Tweet(quote_tweet)\n            except NotATweetError as nate:\n                raise(NotATweetError(\"The quote-tweet payload appears malformed.\" +\n                                     \" Failed with '{}'\".format(nate)))\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the retweeted Tweet as a Tweet object", "response": "def retweeted_tweet(self):\n        \"\"\"\n        The retweeted Tweet as a Tweet object\n        If the Tweet is not a Retweet, return None\n        If the Retweet payload cannot be loaded as a Tweet, this will\n        raise a `NotATweetError`\n\n        Returns:\n            Tweet: A Tweet representing the retweeted status (or None)\n            (see tweet_embeds.get_retweet, this is that value as a Tweet)\n\n        Raises:\n            NotATweetError: if retweeted tweet is malformed\n        \"\"\"\n        retweet = tweet_embeds.get_retweeted_tweet(self)\n        if retweet is not None:\n            try:\n                return Tweet(retweet)\n            except NotATweetError as nate:\n                raise(NotATweetError(\"The retweet payload appears malformed.\" +\n                                     \" Failed with '{}'\".format(nate)))\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef embedded_tweet(self):\n        embedded_tweet = tweet_embeds.get_embedded_tweet(self)\n        if embedded_tweet is not None:\n            try:\n                return Tweet(embedded_tweet)\n            except NotATweetError as nate:\n                raise(NotATweetError(\"The embedded tweet payload {} appears malformed.\" +\n                                     \" Failed with '{}'\".format(embedded_tweet, nate)))\n        else:\n            return None", "response": "Get the retweeted Tweet OR the quoted Tweet and return it as a Tweet object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn whether this promoter is applicable for given conf", "response": "def is_applicable(cls, conf):\n        \"\"\"Return whether this promoter is applicable for given conf\"\"\"\n        return all((\n            URLPromoter.is_applicable(conf),\n            not cls.needs_firefox(conf),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns Xpath match for selector within html.", "response": "def xpath_selector(selector, html, select_all):\n    \"\"\"\n    Returns Xpath match for `selector` within `html`.\n\n    :param selector: XPath string\n    :param html: Unicode content\n    :param select_all: True to get all matches\n    \"\"\"\n    from defusedxml import lxml as dlxml\n    from lxml import etree\n    import re\n\n    # lxml requires argument to be bytes\n    # see https://github.com/kibitzr/kibitzr/issues/47\n    encoded = html.encode('utf-8')\n    root = dlxml.fromstring(encoded, parser=etree.HTMLParser())\n    xpath_results = root.xpath(selector)\n    if not xpath_results:\n        logger.warning('XPath selector not found: %r', selector)\n        return False, html\n\n    if isinstance(xpath_results, list):\n        if select_all is False:\n            xpath_results = xpath_results[0:1]\n    else:\n        xpath_results = [xpath_results]\n\n    # Serialize xpath_results\n    # see https://lxml.de/xpathxslt.html#xpath-return-values\n    results = []\n    for r in xpath_results:\n        # namespace declarations\n        if isinstance(r, tuple):\n            results.append(\"%s=\\\"%s\\\"\" % (r[0], r[1]))\n        # an element\n        elif hasattr(r, 'tag'):\n            results.append(\n                re.sub(r'\\s+', ' ',\n                       dlxml.tostring(r, method='html', encoding='unicode'))\n            )\n        else:\n            results.append(r)\n\n    return True, u\"\\n\".join(six.text_type(x).strip() for x in results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register():\n    registry = {\n        key: bake_html(key)\n        for key in ('css', 'css-all', 'tag', 'text')\n    }\n    registry['xpath'] = bake_parametrized(xpath_selector, select_all=False)\n    registry['xpath-all'] = bake_parametrized(xpath_selector, select_all=True)\n    return registry", "response": "Return dictionary of tranform factories that can be used to create a new node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reread(self):\n        logger.debug(\"Loading settings from %s\",\n                     os.path.abspath(self.filename))\n        conf = self.read_conf()\n        changed = self.creds.reread()\n        checks = self.parser.parse_checks(conf)\n        if self.checks != checks:\n            self.checks = checks\n            return True\n        else:\n            return changed", "response": "Read configuration file and substitute references into checks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads and parse the credentials file and set the current instance s credentials.", "response": "def reread(self):\n        \"\"\"\n        Read and parse credentials file.\n        If something goes wrong, log exception and continue.\n        \"\"\"\n        logger.debug(\"Loading credentials from %s\",\n                     os.path.abspath(self.creds_filename))\n        creds = {}\n        try:\n            with self.open_creds() as fp:\n                creds = yaml.safe_load(fp)\n        except IOError:\n            logger.info(\"No credentials file found at %s\",\n                        os.path.abspath(self.creds_filename))\n        except:\n            logger.exception(\"Error loading credentials file\")\n        if creds != self.creds:\n            self.creds = creds\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_checks(self, conf):\n        checks = conf.get('checks', conf.get('pages', []))\n        checks = list(self.unpack_batches(checks))\n        checks = list(self.unpack_templates(checks, conf.get('templates', {})))\n        self.inject_missing_names(checks)\n        for check in checks:\n            self.inject_scenarios(check, conf.get('scenarios', {}))\n            self.inject_notifiers(check, conf.get('notifiers', {}))\n            self.expand_schedule(check)\n        return checks", "response": "Unpack configuration from human - friendly form\n        to strict check definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the kibitzr. yml and kibitzr - creds. yml in current directory Create kibitzr. yml and kibitzr - creds. yml in current directory Create kibitzr - creds. yml in current directory if they do not exist.", "response": "def create_boilerplate():\n    \"\"\"\n    Create kibitzr.yml and kibitzr-creds.yml in current directory\n    if they do not exist.\n    \"\"\"\n    if not os.path.exists('kibitzr.yml'):\n        with open('kibitzr.yml', 'wt') as fp:\n            logger.info(\"Saving sample check in kibitzr.yml\")\n            fp.write(KIBITZR_YML)\n    else:\n        logger.info(\"kibitzr.yml already exists. Skipping\")\n    if not os.path.exists('kibitzr-creds.yml'):\n        with open('kibitzr-creds.yml', 'wt') as fp:\n            logger.info(\"Creating kibitzr-creds.yml\")\n            fp.write(KIBITZR_CREDS_YML)\n        os.chmod('kibitzr-creds.yml', stat.S_IRUSR | stat.S_IWUSR)\n    else:\n        logger.info(\"kibitzr-creds.yml already exists. Skipping\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload all kibitzr. before_start entry points and call each one with two arguments", "response": "def before_start(self, checkers):\n        \"\"\"\n        Loads entry points named kibitzr.before_start\n        and call each one with two arguments:\n\n            1. Application instance;\n            2. List of configured checkers\n        \"\"\"\n        for point in entrypoints.get_group_all(\"kibitzr.before_start\"):\n            entry = point.load()\n            entry(self, checkers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncleaning up the temporary directories of the current page.", "response": "def cleanup():\n    \"\"\"Must be called before exit\"\"\"\n    temp_dirs = []\n    for key in ('headless', 'headed'):\n        if FIREFOX_INSTANCE[key] is not None:\n            if FIREFOX_INSTANCE[key].profile:\n                temp_dirs.append(FIREFOX_INSTANCE[key].profile.profile_dir)\n            try:\n                FIREFOX_INSTANCE[key].quit()\n                FIREFOX_INSTANCE[key] = None\n            except:\n                logger.exception(\n                    \"Exception occurred in browser cleanup\"\n                )\n    for temp_dir in temp_dirs:\n        shutil.rmtree(temp_dir, ignore_errors=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_transform(self, rule):\n        try:\n            name, value = next(iter(rule.items()))\n        except AttributeError:\n            name, value = rule, None\n        try:\n            return self.REGISTRY[name](value, conf=self.conf)\n        except KeyError:\n            raise RuntimeError(\n                \"Unknown transform: %r\" % (name,)\n            )", "response": "Create single transform from rule."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_error(self, content):\n        error_policy = self.conf.get('error', 'notify')\n        if error_policy == 'ignore':\n            if content:\n                logger.error(\"Ignoring error in %s\",\n                             repr(content)[:60])\n            return None\n        elif error_policy == 'notify':\n            logger.debug(\"Notifying on error\")\n            return content\n        else:\n            logger.warning(\"Unknown error policy: %r\", error_policy)\n            logger.info(\"Defaulting to 'notify'\")\n            return content", "response": "Called when an error occurs in the content of a resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetcher_factory(conf):\n    global PROMOTERS\n    applicable = []\n    if not PROMOTERS:\n        PROMOTERS = load_promoters()\n    for promoter in PROMOTERS:\n        if promoter.is_applicable(conf):\n            applicable.append((promoter.PRIORITY, promoter))\n    if applicable:\n        best_match = sorted(applicable, reverse=True)[0][1]\n        return best_match(conf)\n    else:\n        raise ConfigurationError(\n            'No fetcher is applicable for \"{0}\"'.format(conf['name'])\n        )", "response": "Return initialized fetcher capable of processing given conf."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch URL and return HTML.", "response": "def fetch(self, conf):\n        \"\"\"\n        1. Fetch URL\n        2. Run automation.\n        3. Return HTML.\n        4. Close the tab.\n        \"\"\"\n        url = conf['url']\n        # If Firefox is broken, it will raise here, causing kibitzr restart:\n        self.driver.set_window_size(1366, 800)\n        self.driver.implicitly_wait(2)\n        self.driver.get(url)\n        try:\n            self._run_automation(conf)\n            html = self._get_html()\n        except:\n            logger.exception(\n                \"Exception occurred while fetching\"\n            )\n            return False, traceback.format_exc()\n        finally:\n            self._close_tab()\n        return True, html"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfill all inputs with provided Jinja2 templates.", "response": "def _fill_form(form):\n        \"\"\"\n        Fill all inputs with provided Jinja2 templates.\n        If no field had click key, submit last element.\n        \"\"\"\n        clicked = False\n        last_element = None\n        for field in form:\n            if field['text']:\n                field['element'].clear()\n                field['element'].send_keys(field['text'])\n            if field['click']:\n                field['element'].click()\n                clicked = True\n            last_element = field['element']\n        if last_element:\n            if not clicked:\n                last_element.submit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds elements defined in conf. form Return a list of dictionaries.", "response": "def _find_form(self, conf):\n        \"\"\"\n        Find elements defined in conf['form'].\n        Render all Jinja2 templates from field['value'].\n        Save all field['click'] triggers.\n        If all found, return them as a list of dictionaries.\n        If at least one was not found, return empty list.\n        \"\"\"\n        form = conf.get('form', [])\n        fields = []\n        creds = settings().creds\n        first_field = True\n        for field in form:\n            click = field.get('click')\n            text = self._parse_field_text(field, conf, creds)\n            selector_type, selector = self._parse_field_selector(field)\n            if selector:\n                element = self._find_element(\n                    selector,\n                    selector_type,\n                    check_displayed=first_field,\n                )\n                if element:\n                    fields.append({\n                        'element': element,\n                        'text': text,\n                        'click': click,\n                    })\n                else:\n                    logging.warning(\"Element {%s: %s} not found\",\n                                    selector_type, selector)\n            else:\n                logging.warning(\"Failed to parse selector %r\", field)\n            first_field = False\n        if len(fields) == len(form):\n            return fields\n        logging.info(\n            \"Skipped form filling because not all fields were found\"\n        )\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a field s selector.", "response": "def _parse_field_selector(field):\n        \"\"\"\n        Check field keys: css, xpath and id.\n        Return the first found along with it's value.\n        \"\"\"\n        for selector_type in ('css', 'xpath', 'id'):\n            selector = field.get(selector_type)\n            if selector:\n                return selector_type, selector\n        logger.warning(\"Form field does not define any selector \"\n                       \"(id, css and xpath available): %r\",\n                       field)\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_field_text(field, conf, creds):\n        value = field.get('value')\n        if value:\n            template = Template(value)\n            return template.render(\n                conf=conf,\n                creds=creds,\n            )\n        creds_path = field.get('creds')\n        if creds_path:\n            bread_crumbs = creds_path.split('.')\n            node = creds\n            for crumb in bread_crumbs:\n                node = node[crumb]\n            return node\n        return None", "response": "Parse the text of a field in the form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_element(self, selector, selector_type, check_displayed=False):\n        if selector_type == 'css':\n            elements = self.driver.find_elements_by_css_selector(selector)\n        elif selector_type == 'xpath':\n            elements = self.driver.find_elements_by_xpath(selector)\n        elif selector_type == 'id':\n            elements = self.driver.find_elements_by_css_selector('#' + selector)\n        else:\n            raise RuntimeError(\n                \"Unknown selector_type: %s for selector: %s\"\n                % (selector_type, selector)\n            )\n        for element in elements:\n            if check_displayed:\n                if not element.is_displayed() or sum(element.size.values()) <= 0:\n                    continue\n            return element", "response": "Find element by selector and type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _close_tab(self):\n        old_tab = self.driver.current_window_handle\n        self.driver.execute_script('''window.open(\"about:blank\", \"_blank\");''')\n        self.driver.switch_to.window(old_tab)\n        self.driver.close()\n        self.driver.switch_to.window(self.driver.window_handles[0])", "response": "Close the old one\n        and switch to the new one\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite changes in file and return a tuple of True and a tuple of False", "response": "def report_changes(self, content):\n        \"\"\"\n        1) Write changes in file,\n        2) Commit changes in git\n        3.1) If something changed, return tuple(True, changes)\n        3.2) If nothing changed, return tuple(False, None)\n\n        If style is \"verbose\", return changes in human-friendly format,\n        else use unified diff\n        \"\"\"\n        self.write(content)\n        if self.commit():\n            return True, self.reporter.report()\n        else:\n            return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave content on disk", "response": "def write(self, content):\n        \"\"\"Save content on disk\"\"\"\n        with io.open(self.target, 'w', encoding='utf-8') as fp:\n            fp.write(content)\n            if not content.endswith(u'\\n'):\n                fp.write(u'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef commit(self):\n        self.git.add('-A', '.')\n        try:\n            self.git.commit('-m', self.commit_msg)\n            return True\n        except sh.ErrorReturnCode_1:\n            return False", "response": "git commit and return whether there were changes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_repo_exists(self):\n        if not os.path.isdir(self.cwd):\n            os.makedirs(self.cwd)\n        if not os.path.isdir(os.path.join(self.cwd, \".git\")):\n            self.git.init()\n            self.git.config(\"user.email\", \"you@example.com\")\n            self.git.config(\"user.name\", \"Your Name\")", "response": "Create git repo if one does not exist yet"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef word(self):\n        try:\n            output = ensure_unicode(self.git.diff(\n                '--no-color',\n                '--word-diff=plain',\n                'HEAD~1:content',\n                'HEAD:content',\n            ).stdout)\n        except sh.ErrorReturnCode_128:\n            result = ensure_unicode(self.git.show(\n                \"HEAD:content\"\n            ).stdout)\n        else:\n            ago = ensure_unicode(self.git.log(\n                '-2',\n                '--pretty=format:last change was %cr',\n                'content'\n            ).stdout).splitlines()\n            lines = output.splitlines()\n            result = u'\\n'.join(\n                itertools.chain(\n                    itertools.islice(\n                        itertools.dropwhile(\n                            lambda x: not x.startswith('@@'),\n                            lines[1:],\n                        ),\n                        1,\n                        None,\n                    ),\n                    itertools.islice(ago, 1, None),\n                )\n            )\n        return result", "response": "Return last changes with word diff"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default(self):\n        output = ensure_unicode(self.git.log(\n            '-1',\n            '-p',\n            '--no-color',\n            '--format=%s',\n        ).stdout)\n        lines = output.splitlines()\n        return u'\\n'.join(\n            itertools.chain(\n                lines[:1],\n                itertools.islice(\n                    itertools.dropwhile(\n                        lambda x: not x.startswith('+++'),\n                        lines[1:],\n                    ),\n                    1,\n                    None,\n                ),\n            )\n        )", "response": "Return last changes in truncated unified diff format"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verbose(self):\n        try:\n            before = self.git.show('HEAD~1:content').strip()\n        except sh.ErrorReturnCode_128:\n            before = None\n        after = self.git.show('HEAD:content').strip()\n        if before is not None:\n            return (u'{subject}\\nNew value:\\n{after}\\n'\n                    u'Old value:\\n{before}\\n'\n                    .format(subject=self.subject,\n                            before=before,\n                            after=after))\n        else:\n            return u'\\n'.join([self.subject, after])", "response": "Return changes in human - friendly format #14"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef temp_file(self):\n        with tempfile.NamedTemporaryFile(suffix='.bat', delete=False) as fp:\n            try:\n                logger.debug(\"Saving code to %r\", fp.name)\n                fp.write(self.code.encode('utf-8'))\n                fp.close()\n                yield fp.name\n            finally:\n                os.remove(fp.name)", "response": "Create temporary file with code and yield its path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun kibitzr checks once and exit", "response": "def once(ctx, name):\n    \"\"\"Run kibitzr checks once and exit\"\"\"\n    from kibitzr.app import Application\n    app = Application()\n    sys.exit(app.run(once=True, log_level=ctx.obj['log_level'], names=name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if a given point is located inside a circle.", "response": "def point_in_circle(pt, center, radius):\n    '''\n    Returns true if a given point is located inside (or on the border) of a circle.\n    \n    >>> point_in_circle((0, 0), (0, 0), 1)\n    True\n    >>> point_in_circle((1, 0), (0, 0), 1)\n    True\n    >>> point_in_circle((1, 1), (0, 0), 1)\n    False\n    '''\n    d = np.linalg.norm(np.asarray(pt) - np.asarray(center))\n    return d <= radius"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the area of the intersection of a circle with a given radius and a distance.", "response": "def circle_intersection_area(r, R, d):\n    '''\n    Formula from: http://mathworld.wolfram.com/Circle-CircleIntersection.html\n    Does not make sense for negative r, R or d\n\n    >>> circle_intersection_area(0.0, 0.0, 0.0)\n    0.0\n    >>> circle_intersection_area(1.0, 1.0, 0.0)\n    3.1415...\n    >>> circle_intersection_area(1.0, 1.0, 1.0)\n    1.2283...\n    '''\n    if np.abs(d) < tol:\n        minR = np.min([r, R])\n        return np.pi * minR**2\n    if np.abs(r - 0) < tol or np.abs(R - 0) < tol:\n        return 0.0\n    d2, r2, R2 = float(d**2), float(r**2), float(R**2)\n    arg = (d2 + r2 - R2) / 2 / d / r\n    arg = np.max([np.min([arg, 1.0]), -1.0])  # Even with valid arguments, the above computation may result in things like -1.001\n    A = r2 * np.arccos(arg)\n    arg = (d2 + R2 - r2) / 2 / d / R\n    arg = np.max([np.min([arg, 1.0]), -1.0])\n    B = R2 * np.arccos(arg)\n    arg = (-d + r + R) * (d + r - R) * (d - r + R) * (d + r + R)\n    arg = np.max([arg, 0])\n    C = -0.5 * np.sqrt(arg)\n    return A + B + C"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef circle_line_intersection(center, r, a, b):\n    '''\n    Computes two intersection points between the circle centered at <center> and radius <r> and a line given by two points a and b.\n    If no intersection exists, or if a==b, None is returned. If one intersection exists, it is repeated in the answer.\n\n    >>> circle_line_intersection(np.array([0.0, 0.0]), 1, np.array([-1.0, 0.0]), np.array([1.0, 0.0]))\n    array([[ 1.,  0.],\n           [-1.,  0.]])\n    >>> abs(np.round(circle_line_intersection(np.array([1.0, 1.0]), np.sqrt(2), np.array([-1.0, 1.0]), np.array([1.0, -1.0])), 6))\n    array([[ 0.,  0.],\n           [ 0.,  0.]])\n    '''\n    s = b - a\n    # Quadratic eqn coefs\n    A = np.linalg.norm(s)**2\n    if abs(A) < tol:\n        return None\n    B = 2 * np.dot(a - center, s)\n    C = np.linalg.norm(a - center)**2 - r**2\n    disc = B**2 - 4 * A * C\n    if disc < 0.0:\n        return None\n    t1 = (-B + np.sqrt(disc)) / 2.0 / A\n    t2 = (-B - np.sqrt(disc)) / 2.0 / A\n    return np.array([a + t1 * s, a + t2 * s])", "response": "Computes two intersection points between the circle centered at center and radius r and a line given by two points a and b."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the distance of a circle in a region of a circle.", "response": "def find_distance_by_area(r, R, a, numeric_correction=0.0001):\n    '''\n    Solves circle_intersection_area(r, R, d) == a for d numerically (analytical solution seems to be too ugly to pursue).\n    Assumes that a < pi * min(r, R)**2, will fail otherwise.\n\n    The numeric correction parameter is used whenever the computed distance is exactly (R - r) (i.e. one circle must be inside another).\n    In this case the result returned is (R-r+correction). This helps later when we position the circles and need to ensure they intersect.\n\n    >>> find_distance_by_area(1, 1, 0, 0.0)\n    2.0\n    >>> round(find_distance_by_area(1, 1, 3.1415, 0.0), 4)\n    0.0\n    >>> d = find_distance_by_area(2, 3, 4, 0.0)\n    >>> d\n    3.37...\n    >>> round(circle_intersection_area(2, 3, d), 10)\n    4.0\n    >>> find_distance_by_area(1, 2, np.pi)\n    1.0001\n    '''\n    if r > R:\n        r, R = R, r\n    if np.abs(a) < tol:\n        return float(r + R)\n    if np.abs(min([r, R])**2 * np.pi - a) < tol:\n        return np.abs(R - r + numeric_correction)\n    return brentq(lambda x: circle_intersection_area(r, R, x) - a, R - r, R + r)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the coordinates of the intersection points of two circles A and B.", "response": "def circle_circle_intersection(C_a, r_a, C_b, r_b):\n    '''\n    Finds the coordinates of the intersection points of two circles A and B.\n    Circle center coordinates C_a and C_b, should be given as tuples (or 1x2 arrays).\n    Returns a 2x2 array result with result[0] being the first intersection point (to the right of the vector C_a -> C_b)\n    and result[1] being the second intersection point.\n\n    If there is a single intersection point, it is repeated in output.\n    If there are no intersection points or an infinite number of those, None is returned.\n\n    >>> circle_circle_intersection([0, 0], 1, [1, 0], 1) # Two intersection points\n    array([[ 0.5      , -0.866...],\n           [ 0.5      ,  0.866...]])\n    >>> circle_circle_intersection([0, 0], 1, [2, 0], 1) # Single intersection point (circles touch from outside)\n    array([[ 1.,  0.],\n           [ 1.,  0.]])\n    >>> circle_circle_intersection([0, 0], 1, [0.5, 0], 0.5) # Single intersection point (circles touch from inside)\n    array([[ 1.,  0.],\n           [ 1.,  0.]])\n    >>> circle_circle_intersection([0, 0], 1, [0, 0], 1) is None # Infinite number of intersections (circles coincide)\n    True\n    >>> circle_circle_intersection([0, 0], 1, [0, 0.1], 0.8) is None # No intersections (one circle inside another)\n    True\n    >>> circle_circle_intersection([0, 0], 1, [2.1, 0], 1) is None # No intersections (one circle outside another)\n    True\n    '''\n    C_a, C_b = np.asarray(C_a, float), np.asarray(C_b, float)\n    v_ab = C_b - C_a\n    d_ab = np.linalg.norm(v_ab)\n    if np.abs(d_ab) < tol:  # No intersection points or infinitely many of them (circle centers coincide)\n        return None\n    cos_gamma = (d_ab**2 + r_a**2 - r_b**2) / 2.0 / d_ab / r_a\n    \n    if abs(cos_gamma) > 1.0 + tol/10: # Allow for a tiny numeric tolerance here too (always better to be return something instead of None, if possible)\n        return None         # No intersection point (circles do not touch)\n    if (cos_gamma > 1.0):\n        cos_gamma = 1.0\n    if (cos_gamma < -1.0):\n        cos_gamma = -1.0\n    \n    sin_gamma = np.sqrt(1 - cos_gamma**2)\n    u = v_ab / d_ab\n    v = np.array([-u[1], u[0]])\n    pt1 = C_a + r_a * cos_gamma * u - r_a * sin_gamma * v\n    pt2 = C_a + r_a * cos_gamma * u + r_a * sin_gamma * v\n    return np.array([pt1, pt2])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes the center of mass of a single object.", "response": "def normalize_by_center_of_mass(coords, radii):\n    '''\n    Given coordinates of circle centers and radii, as two arrays,\n    returns new coordinates array, computed such that the center of mass of the\n    three circles is (0, 0).\n\n    >>> normalize_by_center_of_mass(np.array([[0.0, 0.0], [2.0, 0.0], [1.0, 3.0]]), np.array([1.0, 1.0, 1.0]))\n    array([[-1., -1.],\n           [ 1., -1.],\n           [ 0.,  2.]])\n    >>> normalize_by_center_of_mass(np.array([[0.0, 0.0], [2.0, 0.0], [1.0, 2.0]]), np.array([1.0, 1.0, np.sqrt(2.0)]))\n    array([[-1., -1.],\n           [ 1., -1.],\n           [ 0.,  1.]])\n    '''\n    # Now find the center of mass.\n    radii = radii**2\n    sum_r = np.sum(radii)\n    if sum_r < tol:\n        return coords\n    else:\n        return coords - np.dot(radii, coords) / np.sum(radii)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subtract_and_intersect_circle(self, center, radius):\n        '''Will throw a VennRegionException if the circle to be subtracted is completely inside and not touching the given region.'''\n        \n        # Check whether the target circle intersects us\n        center = np.asarray(center, float)\n        d = np.linalg.norm(center - self.center)\n        if d > (radius + self.radius - tol):\n            return [self, VennEmptyRegion()] # The circle does not intersect us\n        elif d < tol:\n            if radius > self.radius - tol:\n                # We are completely covered by that circle or we are the same circle\n                return [VennEmptyRegion(), self]\n            else:\n                # That other circle is inside us and smaller than us - we can't deal with it\n                raise VennRegionException(\"Invalid configuration of circular regions (holes are not supported).\")\n        else:\n            # We *must* intersect the other circle. If it is not the case, then it is inside us completely,\n            # and we'll complain.            \n            intersections = circle_circle_intersection(self.center, self.radius, center, radius)\n            \n            if intersections is None:\n                raise VennRegionException(\"Invalid configuration of circular regions (holes are not supported).\")\n            elif np.all(abs(intersections[0] - intersections[1]) < tol) and self.radius < radius:\n                # There is a single intersection point (i.e. we are touching the circle),\n                # the circle to be subtracted is not outside of us (this was checked before), and is larger than us.\n                # This is a particular corner case that is not dealt with correctly by the general-purpose code below and must\n                # be handled separately\n                return [VennEmptyRegion(), self]\n            else:\n                # Otherwise the subtracted region is a 2-arc-gon\n                # Before we need to convert the intersection points as angles wrt each circle.\n                a_1 = vector_angle_in_degrees(intersections[0] - self.center)\n                a_2 = vector_angle_in_degrees(intersections[1] - self.center)\n                b_1 = vector_angle_in_degrees(intersections[0] - center)\n                b_2 = vector_angle_in_degrees(intersections[1] - center)\n\n                # We must take care of the situation where the intersection points happen to be the same\n                if (abs(b_1 - b_2) < tol):\n                    b_1 = b_2 - tol/2\n                if (abs(a_1 - a_2) < tol):\n                    a_2 = a_1 + tol/2\n                \n                # The subtraction is a 2-arc-gon [(AB, B-), (BA, A+)]\n                s_arc1 = Arc(center, radius, b_1, b_2, False)\n                s_arc2 = Arc(self.center, self.radius, a_2, a_1, True)                \n                subtraction = VennArcgonRegion([s_arc1, s_arc2])\n                \n                # .. and the intersection is a 2-arc-gon [(AB, A+), (BA, B+)]\n                i_arc1 = Arc(self.center, self.radius, a_1, a_2, True)\n                i_arc2 = Arc(center, radius, b_2, b_1, True)\n                intersection = VennArcgonRegion([i_arc1, i_arc2])\n                return [subtraction, intersection]", "response": "This method subtracts the circle from the target region and returns a list of two elements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify(self):\n        '''\n        Verify the correctness of the region arcs. Throws an VennRegionException if verification fails\n        (or any other exception if it happens during verification).\n        '''\n        # Verify size of arcs list\n        if (len(self.arcs) < 2):\n            raise VennRegionException(\"At least two arcs needed in a poly-arc region\")\n        if (len(self.arcs) > 4):\n            raise VennRegionException(\"At most 4 arcs are supported currently for poly-arc regions\")\n        \n        TRIG_TOL = 100*tol  # We need to use looser tolerance level here because conversion to angles and back is prone to large errors.\n        # Verify connectedness of arcs\n        for i in range(len(self.arcs)):\n            if not np.all(self.arcs[i-1].end_point() - self.arcs[i].start_point() < TRIG_TOL):\n                raise VennRegionException(\"Arcs of an poly-arc-gon must be connected via endpoints\")\n        \n        # Verify that arcs do not cross-intersect except at endpoints\n        for i in range(len(self.arcs)-1):\n            for j in range(i+1, len(self.arcs)):\n                ips = self.arcs[i].intersect_arc(self.arcs[j])\n                for ip in ips:\n                    if not (np.all(abs(ip - self.arcs[i].start_point()) < TRIG_TOL) or np.all(abs(ip - self.arcs[i].end_point()) < TRIG_TOL)):\n                        raise VennRegionException(\"Arcs of a poly-arc-gon may only intersect at endpoints\")\n                \n                if len(ips) != 0 and (i - j) % len(self.arcs) > 1 and (j - i) % len(self.arcs) > 1:\n                    # Two non-consecutive arcs intersect. This is in general not good, but\n                    # may occasionally happen when all arcs inbetween have length 0.\n                    pass # raise VennRegionException(\"Non-consecutive arcs of a poly-arc-gon may not intersect\")\n        \n        # Verify that vertices are ordered so that at each point the direction along the polyarc changes towards the left.\n        # Note that this test only makes sense for polyarcs obtained using circle intersections & subtractions.\n        # A \"flower-like\" polyarc may have its vertices ordered counter-clockwise yet the direction would turn to the right at each of them.\n        for i in range(len(self.arcs)):\n            prev_arc = self.arcs[i-1]\n            cur_arc = self.arcs[i]\n            if box_product(prev_arc.direction_vector(prev_arc.to_angle), cur_arc.direction_vector(cur_arc.from_angle)) < -tol:\n                raise VennRegionException(\"Arcs must be ordered so that the direction at each vertex changes counter-clockwise\")", "response": "Verifies that the correctness of the arcs in the region is correct. Throws an exception if it fails during verification."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubtracting a circle from the region and returns a list of two elements where the circle is contained within the circle.", "response": "def subtract_and_intersect_circle(self, center, radius):\n        '''\n        Circle subtraction / intersection only supported by 2-gon regions, otherwise a VennRegionException is thrown.\n        In addition, such an exception will be thrown if the circle to be subtracted is completely within the region and forms a \"hole\".\n        \n        The result may be either a VennArcgonRegion or a VennMultipieceRegion (the latter happens when the circle \"splits\" a crescent in two).\n        '''\n        if len(self.arcs) != 2:\n            raise VennRegionException(\"Circle subtraction and intersection with poly-arc regions is currently only supported for 2-arc-gons.\")\n        \n        # In the following we consider the 2-arc-gon case.\n        # Before we do anything, we check for a special case, where the circle of interest is one of the two circles forming the arcs.\n        # In this case we can determine the answer quite easily.\n        matching_arcs = [a for a in self.arcs if a.lies_on_circle(center, radius)]\n        if len(matching_arcs) != 0:\n            # If the circle matches a positive arc, the result is [empty, self], otherwise [self, empty]\n            return [VennEmptyRegion(), self] if matching_arcs[0].direction else [self, VennEmptyRegion()]\n            \n        # Consider the intersection points of the circle with the arcs.\n        # If any of the intersection points corresponds exactly to any of the arc's endpoints, we will end up with\n        # a lot of messy special cases (as if the usual situation is not messy enough, eh).\n        # To avoid that, we cheat by slightly increasing the circle's radius until this is not the case any more.\n        center = np.asarray(center)\n        illegal_intersections = [a.start_point() for a in self.arcs]\n        while True:\n            valid = True\n            intersections = [a.intersect_circle(center, radius) for a in self.arcs]\n            for ints in intersections:\n                for pt in ints:\n                    for illegal_pt in illegal_intersections:\n                        if np.all(abs(pt - illegal_pt) < tol):\n                            valid = False\n            if valid:\n                break\n            else:\n                radius += tol\n                \n\n        # There must be an even number of those points in total.\n        # (If this is not the case, then we have an unfortunate case with weird numeric errors [TODO: find examples and deal with it?]).\n        # There are three possibilities with the following subcases:\n        #   I. No intersection points\n        #       a) The polyarc is completely within the circle.\n        #           result = [ empty, self ]\n        #       b) The polyarc is completely outside the circle.\n        #           result = [ self, empty ]\n        #   II. Four intersection points, two for each arc. Points x1, x2 for arc X and y1, y2 for arc Y, ordered along the arc.\n        #       a) The polyarc endpoints are both outside the circle.\n        #           result_subtraction = a combination of two 3-arc polyarcs:\n        #               1: {X - start to x1,\n        #                   x1 to y2 along circle (negative direction)),\n        #                   Y - y2 to end}\n        #               2: {Y start to y1,\n        #                   y1 to x2 along circle (negative direction)),\n        #                   X - x2 to end}\n        #       b) The polyarc endpoints are both inside the circle\n        #               same as above, but the \"along circle\" arc directions are flipped and subtraction/intersection parts are exchanged\n        #   III. Two intersection points\n        #       a) One arc, X, has two intersection points i & j, another arc, Y, has no intersection points\n        #           a.1) Polyarc endpoints are outside the circle\n        #               result_subtraction = {X from start to i, circle i to j (direction = negative), X j to end, Y}\n        #               result_intersection = {X i to j, circle j to i (direction = positive}\n        #           a.2) Polyarc endpoints are inside the circle\n        #               result_subtraction = {X i to j, circle j to i negative}\n        #               result_intersection = {X 0 to i, circle i to j positive, X j to end, Y}\n        #       b) Both arcs, X and Y, have one intersection point each. In this case one of the arc endpoints must be inside circle, another outside.\n        #          call the arc that starts with the outside point X, the other arc Y.\n        #           result_subtraction = {X start to intersection, intersection to intersection along circle (negative direction), Y from intersection to end}\n        #           result_intersection = {X intersection to end, Y start to intersecton, intersection to intersecion along circle (positive)}\n        center = np.asarray(center)\n        intersections = [a.intersect_circle(center, radius) for a in self.arcs]\n        \n        if len(intersections[0]) == 0 and len(intersections[1]) == 0:\n            # Case I\n            if point_in_circle(self.arcs[0].start_point(), center, radius):\n                # Case I.a)\n                return [VennEmptyRegion(), self]\n            else:\n                # Case I.b)\n                return [self, VennEmptyRegion()]\n        elif len(intersections[0]) == 2 and len(intersections[1]) == 2:\n            # Case II. a) or b)\n            case_II_a = not point_in_circle(self.arcs[0].start_point(), center, radius)\n            \n            a1 = self.arcs[0].subarc_between_points(None, intersections[0][0])\n            a2 = Arc(center, radius,\n                     vector_angle_in_degrees(intersections[0][0] - center),\n                     vector_angle_in_degrees(intersections[1][1] - center),\n                     not case_II_a)\n            a2.fix_360_to_0()\n            a3 = self.arcs[1].subarc_between_points(intersections[1][1], None)\n            piece1 = VennArcgonRegion([a1, a2, a3])\n            \n            b1 = self.arcs[1].subarc_between_points(None, intersections[1][0])\n            b2 = Arc(center, radius,\n                     vector_angle_in_degrees(intersections[1][0] - center),\n                     vector_angle_in_degrees(intersections[0][1] - center),\n                     not case_II_a)\n            b2.fix_360_to_0()\n            b3 = self.arcs[0].subarc_between_points(intersections[0][1], None)\n            piece2 = VennArcgonRegion([b1, b2, b3])\n            \n            subtraction = VennMultipieceRegion([piece1, piece2])\n            \n            c1 = self.arcs[0].subarc(a1.to_angle, b3.from_angle)\n            c2 = b2.reversed()\n            c3 = self.arcs[1].subarc(b1.to_angle, a3.from_angle)\n            c4 = a2.reversed()\n            intersection = VennArcgonRegion([c1, c2, c3, c4])\n            \n            return [subtraction, intersection] if case_II_a else [intersection, subtraction]\n        else:\n            # Case III. Yuck.\n            if len(intersections[0]) == 0 or len(intersections[1]) == 0:\n                # Case III.a)\n                x = 0 if len(intersections[0]) != 0 else 1\n                y = 1 - x\n                if len(intersections[x]) != 2:\n                    warnings.warn(\"Numeric precision error during polyarc intersection, case IIIa. Expect wrong results.\")\n                    intersections[x] = [intersections[x][0], intersections[x][0]]  # This way we'll at least produce some result, although it will probably be wrong\n                if not point_in_circle(self.arcs[0].start_point(), center, radius):\n                    # Case III.a.1)\n                    #   result_subtraction = {X from start to i, circle i to j (direction = negative), X j to end, Y}\n                    a1 = self.arcs[x].subarc_between_points(None, intersections[x][0])\n                    a2 = Arc(center, radius,\n                             vector_angle_in_degrees(intersections[x][0] - center),\n                             vector_angle_in_degrees(intersections[x][1] - center),\n                             False)\n                    a3 = self.arcs[x].subarc_between_points(intersections[x][1], None)\n                    a4 = self.arcs[y]\n                    subtraction = VennArcgonRegion([a1, a2, a3, a4])\n                    \n                    #   result_intersection = {X i to j, circle j to i (direction = positive)}\n                    b1 = self.arcs[x].subarc(a1.to_angle, a3.from_angle)\n                    b2 = a2.reversed()\n                    intersection = VennArcgonRegion([b1, b2])\n                    \n                    return [subtraction, intersection]\n                else:\n                    # Case III.a.2)\n                    #   result_subtraction = {X i to j, circle j to i negative}\n                    a1 = self.arcs[x].subarc_between_points(intersections[x][0], intersections[x][1])\n                    a2 = Arc(center, radius,\n                             vector_angle_in_degrees(intersections[x][1] - center),\n                             vector_angle_in_degrees(intersections[x][0] - center),\n                             False)\n                    subtraction = VennArcgonRegion([a1, a2])\n                    \n                    #   result_intersection = {X 0 to i, circle i to j positive, X j to end, Y}\n                    b1 = self.arcs[x].subarc(None, a1.from_angle)\n                    b2 = a2.reversed()\n                    b3 = self.arcs[x].subarc(a1.to_angle, None)\n                    b4 = self.arcs[y]\n                    intersection = VennArcgonRegion([b1, b2, b3, b4])\n                    \n                    return [subtraction, intersection]\n            else:\n                # Case III.b)\n                if len(intersections[0]) == 2 or len(intersections[1]) == 2:\n                    warnings.warn(\"Numeric precision error during polyarc intersection, case IIIb. Expect wrong results.\")\n                \n                # One of the arcs must start outside the circle, call it x\n                x = 0 if not point_in_circle(self.arcs[0].start_point(), center, radius) else 1\n                y = 1 - x\n                \n                a1 = self.arcs[x].subarc_between_points(None, intersections[x][0])\n                a2 = Arc(center, radius,\n                         vector_angle_in_degrees(intersections[x][0] - center),\n                         vector_angle_in_degrees(intersections[y][0] - center), False)\n                a3 = self.arcs[y].subarc_between_points(intersections[y][0], None)\n                subtraction = VennArcgonRegion([a1, a2, a3])\n                \n                b1 = self.arcs[x].subarc(a1.to_angle, None)\n                b2 = self.arcs[y].subarc(None, a3.from_angle)\n                b3 = a2.reversed()\n                intersection = VennArcgonRegion([b1, b2, b3])\n                return [subtraction, intersection]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the area of the patch.", "response": "def size(self):\n        '''Return the area of the patch.\n        \n        The area can be computed using the standard polygon area formula + signed segment areas of each arc.\n        '''\n        polygon_area = 0\n        for a in self.arcs:\n            polygon_area += box_product(a.start_point(), a.end_point())\n        polygon_area /= 2.0\n        return polygon_area + sum([a.sign * a.segment_area() for a in self.arcs])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef label_position(self):\n        '''\n        Find the largest region and position the label in that.\n        '''\n        reg_sizes = [(r.size(), r) for r in self.pieces]\n        reg_sizes.sort()\n        return reg_sizes[-1][1].label_position()", "response": "Find the position of the label in that."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_patch(self):\n        '''Currently only works if all the pieces are Arcgons.\n           In this case returns a multiple-piece path. Otherwise throws an exception.'''\n        paths = [p.make_patch().get_path() for p in self.pieces]\n        vertices = np.concatenate([p.vertices for p in paths])\n        codes = np.concatenate([p.codes for p in paths])\n        return PathPatch(Path(vertices, codes))", "response": "Makes a PathPatch object for all the pieces."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the venn area of a single venn diagram.", "response": "def compute_venn2_areas(diagram_areas, normalize_to=1.0):\n    '''\n    The list of venn areas is given as 3 values, corresponding to venn diagram areas in the following order:\n     (Ab, aB, AB)  (i.e. last element corresponds to the size of intersection A&B&C).\n    The return value is a list of areas (A, B, AB), such that the total area is normalized\n    to normalize_to. If total area was 0, returns (1e-06, 1e-06, 0.0)\n\n    Assumes all input values are nonnegative (to be more precise, all areas are passed through and abs() function)\n    >>> compute_venn2_areas((1, 1, 0))\n    (0.5, 0.5, 0.0)\n    >>> compute_venn2_areas((0, 0, 0))\n    (1e-06, 1e-06, 0.0)\n    >>> compute_venn2_areas((1, 1, 1), normalize_to=3)\n    (2.0, 2.0, 1.0)\n    >>> compute_venn2_areas((1, 2, 3), normalize_to=6)\n    (4.0, 5.0, 3.0)\n    '''\n    # Normalize input values to sum to 1\n    areas = np.array(np.abs(diagram_areas), float)\n    total_area = np.sum(areas)\n    if np.abs(total_area) < tol:\n        warnings.warn(\"Both circles have zero area\")\n        return (1e-06, 1e-06, 0.0)\n    else:\n        areas = areas / total_area * normalize_to\n        return (areas[0] + areas[2], areas[1] + areas[2], areas[2])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solve_venn2_circles(venn_areas):\n    '''\n    Given the list of \"venn areas\" (as output from compute_venn2_areas, i.e. [A, B, AB]),\n    finds the positions and radii of the two circles.\n    The return value is a tuple (coords, radii), where coords is a 2x2 array of coordinates and\n    radii is a 2x1 array of circle radii.\n\n    Assumes the input values to be nonnegative and not all zero.\n    In particular, the first two values must be positive.\n\n    >>> c, r = solve_venn2_circles((1, 1, 0))\n    >>> np.round(r, 3)\n    array([ 0.564,  0.564])\n    >>> c, r = solve_venn2_circles(compute_venn2_areas((1, 2, 3)))\n    >>> np.round(r, 3)\n    array([ 0.461,  0.515])\n    '''\n    (A_a, A_b, A_ab) = list(map(float, venn_areas))\n    r_a, r_b = np.sqrt(A_a / np.pi), np.sqrt(A_b / np.pi)\n    radii = np.array([r_a, r_b])\n    if A_ab > tol:\n        # Nonzero intersection\n        coords = np.zeros((2, 2))\n        coords[1][0] = find_distance_by_area(radii[0], radii[1], A_ab)\n    else:\n        # Zero intersection\n        coords = np.zeros((2, 2))\n        coords[1][0] = radii[0] + radii[1] + max(np.mean(radii) * 1.1, 0.2)   # The max here is needed for the case r_a = r_b = 0\n    coords = normalize_by_center_of_mass(coords, radii)\n    return (coords, radii)", "response": "Solve the two circles of a single area."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_venn2_regions(centers, radii):\n    '''\n    Returns a triple of VennRegion objects, describing the three regions of the diagram, corresponding to sets\n    (Ab, aB, AB)\n\n    >>> centers, radii = solve_venn2_circles((1, 1, 0.5))\n    >>> regions = compute_venn2_regions(centers, radii)\n    '''\n    A = VennCircleRegion(centers[0], radii[0])\n    B = VennCircleRegion(centers[1], radii[1])\n    Ab, AB = A.subtract_and_intersect_circle(B.center, B.radius)\n    aB, _ = B.subtract_and_intersect_circle(A.center, A.radius)\n    return (Ab, aB, AB)", "response": "Computes the three regions of the diagram."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_venn2_colors(set_colors):\n    '''\n    Given two base colors, computes combinations of colors corresponding to all regions of the venn diagram.\n    returns a list of 3 elements, providing colors for regions (10, 01, 11).\n\n    >>> compute_venn2_colors(('r', 'g'))\n    (array([ 1.,  0.,  0.]), array([ 0. ,  0.5,  0. ]), array([ 0.7 ,  0.35,  0.  ]))\n    '''\n    ccv = ColorConverter()\n    base_colors = [np.array(ccv.to_rgb(c)) for c in set_colors]\n    return (base_colors[0], base_colors[1], mix_colors(base_colors[0], base_colors[1]))", "response": "Given two base colors computes combinations of colors corresponding to all regions of the venn diagram."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_venn2_subsets(a, b):\n    '''\n    Given two set or Counter objects, computes the sizes of (a & ~b, b & ~a, a & b).\n    Returns the result as a tuple.\n\n    >>> compute_venn2_subsets(set([1,2,3,4]), set([2,3,4,5,6]))\n    (1, 2, 3)\n    >>> compute_venn2_subsets(Counter([1,2,3,4]), Counter([2,3,4,5,6]))\n    (1, 2, 3)\n    >>> compute_venn2_subsets(Counter([]), Counter([]))\n    (0, 0, 0)\n    >>> compute_venn2_subsets(set([]), set([]))\n    (0, 0, 0)\n    >>> compute_venn2_subsets(set([1]), set([]))\n    (1, 0, 0)\n    >>> compute_venn2_subsets(set([1]), set([1]))\n    (0, 0, 1)\n    >>> compute_venn2_subsets(Counter([1]), Counter([1]))\n    (0, 0, 1)\n    >>> compute_venn2_subsets(set([1,2]), set([1]))\n    (1, 0, 1)\n    >>> compute_venn2_subsets(Counter([1,1,2,2,2]), Counter([1,2,3,3]))\n    (3, 2, 2)\n    >>> compute_venn2_subsets(Counter([1,1,2]), Counter([1,2,2]))\n    (1, 1, 2)\n    >>> compute_venn2_subsets(Counter([1,1]), set([]))\n    Traceback (most recent call last):\n    ...\n    ValueError: Both arguments must be of the same type\n    '''\n    if not (type(a) == type(b)):\n        raise ValueError(\"Both arguments must be of the same type\")\n    set_size = len if type(a) != Counter else lambda x: sum(x.values())   # We cannot use len to compute the cardinality of a Counter\n    return (set_size(a - b), set_size(b - a), set_size(a & b))", "response": "Given two set or Counter objects a and b computes the sizes of the two sets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of three Circle patches for the Venn diagram.", "response": "def venn2_circles(subsets, normalize_to=1.0, alpha=1.0, color='black', linestyle='solid', linewidth=2.0, ax=None, **kwargs):\n    '''\n    Plots only the two circles for the corresponding Venn diagram.\n    Useful for debugging or enhancing the basic venn diagram.\n    parameters ``subsets``, ``normalize_to`` and ``ax`` are the same as in venn2()\n    ``kwargs`` are passed as-is to matplotlib.patches.Circle.\n    returns a list of three Circle patches.\n\n    >>> c = venn2_circles((1, 2, 3))\n    >>> c = venn2_circles({'10': 1, '01': 2, '11': 3}) # Same effect\n    >>> c = venn2_circles([set([1,2,3,4]), set([2,3,4,5,6])]) # Also same effect\n    '''\n    if isinstance(subsets, dict):\n        subsets = [subsets.get(t, 0) for t in ['10', '01', '11']]\n    elif len(subsets) == 2:\n        subsets = compute_venn2_subsets(*subsets)\n    areas = compute_venn2_areas(subsets, normalize_to)\n    centers, radii = solve_venn2_circles(areas)\n\n    if ax is None:\n        ax = gca()\n    prepare_venn_axes(ax, centers, radii)\n    result = []\n    for (c, r) in zip(centers, radii):\n        circle = Circle(c, r, alpha=alpha, edgecolor=color, facecolor='none', linestyle=linestyle, linewidth=linewidth, **kwargs)\n        ax.add_patch(circle)\n        result.append(circle)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef venn2(subsets, set_labels=('A', 'B'), set_colors=('r', 'g'), alpha=0.4, normalize_to=1.0, ax=None, subset_label_formatter=None):\n    '''Plots a 2-set area-weighted Venn diagram.\n    The subsets parameter can be one of the following:\n     - A list (or a tuple) containing two set objects.\n     - A dict, providing sizes of three diagram regions.\n       The regions are identified via two-letter binary codes ('10', '01', and '11'), hence a valid set could look like:\n       {'10': 10, '01': 20, '11': 40}. Unmentioned codes are considered to map to 0.\n     - A list (or a tuple) with three numbers, denoting the sizes of the regions in the following order:\n       (10, 01, 11)\n\n    ``set_labels`` parameter is a list of two strings - set labels. Set it to None to disable set labels.\n    The ``set_colors`` parameter should be a list of two elements, specifying the \"base colors\" of the two circles.\n    The color of circle intersection will be computed based on those.\n\n    The ``normalize_to`` parameter specifies the total (on-axes) area of the circles to be drawn. Sometimes tuning it (together\n    with the overall fiture size) may be useful to fit the text labels better.\n    The return value is a ``VennDiagram`` object, that keeps references to the ``Text`` and ``Patch`` objects used on the plot\n    and lets you know the centers and radii of the circles, if you need it.\n\n    The ``ax`` parameter specifies the axes on which the plot will be drawn (None means current axes).\n\n    The ``subset_label_formatter`` parameter is a function that can be passed to format the labels\n    that describe the size of each subset.\n\n    >>> from matplotlib_venn import *\n    >>> v = venn2(subsets={'10': 1, '01': 1, '11': 1}, set_labels = ('A', 'B'))\n    >>> c = venn2_circles(subsets=(1, 1, 1), linestyle='dashed')\n    >>> v.get_patch_by_id('10').set_alpha(1.0)\n    >>> v.get_patch_by_id('10').set_color('white')\n    >>> v.get_label_by_id('10').set_text('Unknown')\n    >>> v.get_label_by_id('A').set_text('Set A')\n\n    You can provide sets themselves rather than subset sizes:\n    >>> v = venn2(subsets=[set([1,2]), set([2,3,4,5])], set_labels = ('A', 'B'))\n    >>> c = venn2_circles(subsets=[set([1,2]), set([2,3,4,5])], linestyle='dashed')\n    >>> print(\"%0.2f\" % (v.get_circle_radius(1)/v.get_circle_radius(0)))\n    1.41\n    '''\n    if isinstance(subsets, dict):\n        subsets = [subsets.get(t, 0) for t in ['10', '01', '11']]\n    elif len(subsets) == 2:\n        subsets = compute_venn2_subsets(*subsets)\n\n    if subset_label_formatter is None:\n        subset_label_formatter = str\n\n    areas = compute_venn2_areas(subsets, normalize_to)\n    centers, radii = solve_venn2_circles(areas)\n    regions = compute_venn2_regions(centers, radii)\n    colors = compute_venn2_colors(set_colors)\n\n    if ax is None:\n        ax = gca()\n    prepare_venn_axes(ax, centers, radii)\n\n    # Create and add patches and subset labels\n    patches = [r.make_patch() for r in regions]\n    for (p, c) in zip(patches, colors):\n        if p is not None:\n            p.set_facecolor(c)\n            p.set_edgecolor('none')\n            p.set_alpha(alpha)\n            ax.add_patch(p)\n    label_positions = [r.label_position() for r in regions]\n    subset_labels = [ax.text(lbl[0], lbl[1], subset_label_formatter(s), va='center', ha='center') if lbl is not None else None for (lbl, s) in zip(label_positions, subsets)]\n\n    # Position set labels\n    if set_labels is not None:\n        padding = np.mean([r * 0.1 for r in radii])\n        label_positions = [centers[0] + np.array([0.0, - radii[0] - padding]),\n                           centers[1] + np.array([0.0, - radii[1] - padding])]\n        labels = [ax.text(pos[0], pos[1], txt, size='large', ha='right', va='top') for (pos, txt) in zip(label_positions, set_labels)]\n        labels[1].set_ha('left')\n    else:\n        labels = None\n    return VennDiagram(patches, subset_labels, labels, centers, radii)", "response": "Plots a 2 - set area - weighted Venn diagram."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef venn2_unweighted(subsets, set_labels=('A', 'B'), set_colors=('r', 'g'), alpha=0.4, normalize_to=1.0, subset_areas=(1, 1, 1), ax=None, subset_label_formatter=None):\n    '''\n    The version of venn2 without area-weighting.\n    It is implemented as a wrapper around venn2. Namely, venn2 is invoked as usual, but with all subset areas\n    set to 1. The subset labels are then replaced in the resulting diagram with the provided subset sizes.\n    \n    The parameters are all the same as that of venn2.\n    In addition there is a subset_areas parameter, which specifies the actual subset areas.\n    (it is (1, 1, 1) by default. You are free to change it, within reason).\n    '''\n    v = venn2(subset_areas, set_labels, set_colors, alpha, normalize_to, ax)\n    # Now rename the labels\n    if subset_label_formatter is None:\n        subset_label_formatter = str    \n    subset_ids = ['10', '01', '11']\n    if isinstance(subsets, dict):\n        subsets = [subsets.get(t, 0) for t in subset_ids]\n    elif len(subsets) == 2:\n        subsets = compute_venn2_subsets(*subsets)\n    for n, id in enumerate(subset_ids):\n        lbl = v.get_label_by_id(id)\n        if lbl is not None:\n            lbl.set_text(subset_label_formatter(subsets[n]))\n    return v", "response": "This function returns a version of venn2 with area - weighting."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef venn3_unweighted(subsets, set_labels=('A', 'B', 'C'), set_colors=('r', 'g', 'b'), alpha=0.4, normalize_to=1.0, subset_areas=(1, 1, 1, 1, 1, 1, 1), ax=None, subset_label_formatter=None):\n    '''\n    The version of venn3 without area-weighting.\n    It is implemented as a wrapper around venn3. Namely, venn3 is invoked as usual, but with all subset areas\n    set to 1. The subset labels are then replaced in the resulting diagram with the provided subset sizes.\n    \n    The parameters are all the same as that of venn2.\n    In addition there is a subset_areas parameter, which specifies the actual subset areas.\n    (it is (1, 1, 1, 1, 1, 1, 1) by default. You are free to change it, within reason).\n    '''\n    v = venn3(subset_areas, set_labels, set_colors, alpha, normalize_to, ax)\n    # Now rename the labels\n    if subset_label_formatter is None:\n        subset_label_formatter = str    \n    subset_ids = ['100', '010', '110', '001', '101', '011', '111']\n    if isinstance(subsets, dict):\n        subsets = [subsets.get(t, 0) for t in subset_ids]\n    elif len(subsets) == 3:\n        subsets = compute_venn3_subsets(*subsets)\n    for n, id in enumerate(subset_ids):\n        lbl = v.get_label_by_id(id)\n        if lbl is not None:\n            lbl.set_text(subset_label_formatter(subsets[n]))\n    return v", "response": "Returns a version of venn3 with area - weighting."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the list of venn areas given a list of diagrams.", "response": "def compute_venn3_areas(diagram_areas, normalize_to=1.0, _minimal_area=1e-6):\n    '''\n    The list of venn areas is given as 7 values, corresponding to venn diagram areas in the following order:\n     (Abc, aBc, ABc, abC, AbC, aBC, ABC)\n    (i.e. last element corresponds to the size of intersection A&B&C).\n    The return value is a list of areas (A_a, A_b, A_c, A_ab, A_bc, A_ac, A_abc),\n    such that the total area of all circles is normalized to normalize_to.\n    If the area of any circle is smaller than _minimal_area, makes it equal to _minimal_area.\n\n    Assumes all input values are nonnegative (to be more precise, all areas are passed through and abs() function)\n    >>> compute_venn3_areas((1, 1, 0, 1, 0, 0, 0))\n    (0.33..., 0.33..., 0.33..., 0.0, 0.0, 0.0, 0.0)\n    >>> compute_venn3_areas((0, 0, 0, 0, 0, 0, 0))\n    (1e-06, 1e-06, 1e-06, 0.0, 0.0, 0.0, 0.0)\n    >>> compute_venn3_areas((1, 1, 1, 1, 1, 1, 1), normalize_to=7)\n    (4.0, 4.0, 4.0, 2.0, 2.0, 2.0, 1.0)\n    >>> compute_venn3_areas((1, 2, 3, 4, 5, 6, 7), normalize_to=56/2)\n    (16.0, 18.0, 22.0, 10.0, 13.0, 12.0, 7.0)\n    '''\n    # Normalize input values to sum to 1\n    areas = np.array(np.abs(diagram_areas), float)\n    total_area = np.sum(areas)\n    if np.abs(total_area) < _minimal_area:\n        warnings.warn(\"All circles have zero area\")\n        return (1e-06, 1e-06, 1e-06, 0.0, 0.0, 0.0, 0.0)\n    else:\n        areas = areas / total_area * normalize_to\n        A_a = areas[0] + areas[2] + areas[4] + areas[6]\n        if A_a < _minimal_area:\n            warnings.warn(\"Circle A has zero area\")\n            A_a = _minimal_area\n        A_b = areas[1] + areas[2] + areas[5] + areas[6]\n        if A_b < _minimal_area:\n            warnings.warn(\"Circle B has zero area\")\n            A_b = _minimal_area\n        A_c = areas[3] + areas[4] + areas[5] + areas[6]\n        if A_c < _minimal_area:\n            warnings.warn(\"Circle C has zero area\")\n            A_c = _minimal_area\n\n        # Areas of the three intersections (ab, ac, bc)\n        A_ab, A_ac, A_bc = areas[2] + areas[6], areas[4] + areas[6], areas[5] + areas[6]\n\n        return (A_a, A_b, A_c, A_ab, A_bc, A_ac, areas[6])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsolves the three circles in a single 3 - dimensional manner.", "response": "def solve_venn3_circles(venn_areas):\n    '''\n    Given the list of \"venn areas\" (as output from compute_venn3_areas, i.e. [A, B, C, AB, BC, AC, ABC]),\n    finds the positions and radii of the three circles.\n    The return value is a tuple (coords, radii), where coords is a 3x2 array of coordinates and\n    radii is a 3x1 array of circle radii.\n\n    Assumes the input values to be nonnegative and not all zero.\n    In particular, the first three values must all be positive.\n\n    The overall match is only approximate (to be precise, what is matched are the areas of the circles and the\n    three pairwise intersections).\n\n    >>> c, r = solve_venn3_circles((1, 1, 1, 0, 0, 0, 0))\n    >>> np.round(r, 3)\n    array([ 0.564,  0.564,  0.564])\n    >>> c, r = solve_venn3_circles(compute_venn3_areas((1, 2, 40, 30, 4, 40, 4)))\n    >>> np.round(r, 3)\n    array([ 0.359,  0.476,  0.453])\n    '''\n    (A_a, A_b, A_c, A_ab, A_bc, A_ac, A_abc) = list(map(float, venn_areas))\n    r_a, r_b, r_c = np.sqrt(A_a / np.pi), np.sqrt(A_b / np.pi), np.sqrt(A_c / np.pi)\n    intersection_areas = [A_ab, A_bc, A_ac]\n    radii = np.array([r_a, r_b, r_c])\n\n    # Hypothetical distances between circle centers that assure\n    # that their pairwise intersection areas match the requirements.\n    dists = [find_distance_by_area(radii[i], radii[j], intersection_areas[i]) for (i, j) in [(0, 1), (1, 2), (2, 0)]]\n\n    # How many intersections have nonzero area?\n    num_nonzero = sum(np.array([A_ab, A_bc, A_ac]) > tol)\n\n    # Handle four separate cases:\n    #    1. All pairwise areas nonzero\n    #    2. Two pairwise areas nonzero\n    #    3. One pairwise area nonzero\n    #    4. All pairwise areas zero.\n\n    if num_nonzero == 3:\n        # The \"generic\" case, simply use dists to position circles at the vertices of a triangle.\n        # Before we need to ensure that resulting circles can be at all positioned on a triangle,\n        # use an ad-hoc fix.\n        for i in range(3):\n            i, j, k = (i, (i + 1) % 3, (i + 2) % 3)\n            if dists[i] > dists[j] + dists[k]:\n                a, b = (j, k) if dists[j] < dists[k] else (k, j)\n                dists[i] = dists[b] + dists[a]*0.8\n                warnings.warn(\"Bad circle positioning\")\n        coords = position_venn3_circles_generic(radii, dists)\n    elif num_nonzero == 2:\n        # One pair of circles is not intersecting.\n        # In this case we can position all three circles in a line\n        # The two circles that have no intersection will be on either sides.\n        for i in range(3):\n            if intersection_areas[i] < tol:\n                (left, right, middle) = (i, (i + 1) % 3, (i + 2) % 3)\n                coords = np.zeros((3, 2))\n                coords[middle][0] = dists[middle]\n                coords[right][0] = dists[middle] + dists[right]\n                # We want to avoid the situation where left & right still intersect\n                if coords[left][0] + radii[left] > coords[right][0] - radii[right]:\n                    mid = (coords[left][0] + radii[left] + coords[right][0] - radii[right]) / 2.0\n                    coords[left][0] = mid - radii[left] - 1e-5\n                    coords[right][0] = mid + radii[right] + 1e-5\n                break\n    elif num_nonzero == 1:\n        # Only one pair of circles is intersecting, and one circle is independent.\n        # Position all on a line first two intersecting, then the free one.\n        for i in range(3):\n            if intersection_areas[i] > tol:\n                (left, right, side) = (i, (i + 1) % 3, (i + 2) % 3)\n                coords = np.zeros((3, 2))\n                coords[right][0] = dists[left]\n                coords[side][0] = dists[left] + radii[right] + radii[side] * 1.1  # Pad by 10%\n                break\n    else:\n        # All circles are non-touching. Put them all in a sequence\n        coords = np.zeros((3, 2))\n        coords[1][0] = radii[0] + radii[1] * 1.1\n        coords[2][0] = radii[0] + radii[1] * 1.1 + radii[1] + radii[2] * 1.1\n\n    coords = normalize_by_center_of_mass(coords, radii)\n    return (coords, radii)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving radii and dists returns the coordinates of the center of the three circles in a 3x2 array.", "response": "def position_venn3_circles_generic(radii, dists):\n    '''\n    Given radii = (r_a, r_b, r_c) and distances between the circles = (d_ab, d_bc, d_ac),\n    finds the coordinates of the centers for the three circles so that they form a proper triangle.\n    The current positioning method puts the center of A and B on a horizontal line y==0,\n    and C just below.\n\n    Returns a 3x2 array with circle center coordinates in rows.\n\n    >>> position_venn3_circles_generic((1, 1, 1), (0, 0, 0))\n    array([[ 0.,  0.],\n           [ 0.,  0.],\n           [ 0., -0.]])\n    >>> position_venn3_circles_generic((1, 1, 1), (2, 2, 2))\n    array([[ 0.        ,  0.        ],\n           [ 2.        ,  0.        ],\n           [ 1.        , -1.73205081]])\n    '''\n    (d_ab, d_bc, d_ac) = dists\n    (r_a, r_b, r_c) = radii\n    coords = np.array([[0, 0], [d_ab, 0], [0, 0]], float)\n    C_x = (d_ac**2 - d_bc**2 + d_ab**2) / 2.0 / d_ab if np.abs(d_ab) > tol else 0.0\n    C_y = -np.sqrt(d_ac**2 - C_x**2)\n    coords[2, :] = C_x, C_y\n    return coords"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the 7 regions of a venn diagram.", "response": "def compute_venn3_regions(centers, radii):\n    '''\n    Given the 3x2 matrix with circle center coordinates, and a 3-element list (or array) with circle radii [as returned from solve_venn3_circles],\n    returns the 7 regions, comprising the venn diagram, as VennRegion objects.\n\n    Regions are returned in order (Abc, aBc, ABc, abC, AbC, aBC, ABC)\n\n    >>> centers, radii = solve_venn3_circles((1, 1, 1, 1, 1, 1, 1))\n    >>> regions = compute_venn3_regions(centers, radii)\n    '''\n    A = VennCircleRegion(centers[0], radii[0])\n    B = VennCircleRegion(centers[1], radii[1])\n    C = VennCircleRegion(centers[2], radii[2])\n    Ab, AB = A.subtract_and_intersect_circle(B.center, B.radius)\n    ABc, ABC = AB.subtract_and_intersect_circle(C.center, C.radius)\n    Abc, AbC = Ab.subtract_and_intersect_circle(C.center, C.radius)\n    aB, _ = B.subtract_and_intersect_circle(A.center, A.radius)\n    aBc, aBC = aB.subtract_and_intersect_circle(C.center, C.radius)\n    aC, _ = C.subtract_and_intersect_circle(A.center, A.radius)\n    abC, _ = aC.subtract_and_intersect_circle(B.center, B.radius)\n    return [Abc, aBc, ABc, abC, AbC, aBC, ABC]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving three base colors computes combinations of colors corresponding to all regions of the venn diagram.", "response": "def compute_venn3_colors(set_colors):\n    '''\n    Given three base colors, computes combinations of colors corresponding to all regions of the venn diagram.\n    returns a list of 7 elements, providing colors for regions (100, 010, 110, 001, 101, 011, 111).\n\n    >>> compute_venn3_colors(['r', 'g', 'b'])\n    (array([ 1.,  0.,  0.]),..., array([ 0.4,  0.2,  0.4]))\n    '''\n    ccv = ColorConverter()\n    base_colors = [np.array(ccv.to_rgb(c)) for c in set_colors]\n    return (base_colors[0], base_colors[1], mix_colors(base_colors[0], base_colors[1]), base_colors[2],\n            mix_colors(base_colors[0], base_colors[2]), mix_colors(base_colors[1], base_colors[2]), mix_colors(base_colors[0], base_colors[1], base_colors[2]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive three set or Counter objects a b and c computes the sizes of the three sets.", "response": "def compute_venn3_subsets(a, b, c):\n    '''\n    Given three set or Counter objects, computes the sizes of (a & ~b & ~c, ~a & b & ~c, a & b & ~c, ....),\n    as needed by the subsets parameter of venn3 and venn3_circles.\n    Returns the result as a tuple.\n\n    >>> compute_venn3_subsets(set([1,2,3]), set([2,3,4]), set([3,4,5,6]))\n    (1, 0, 1, 2, 0, 1, 1)\n    >>> compute_venn3_subsets(Counter([1,2,3]), Counter([2,3,4]), Counter([3,4,5,6]))\n    (1, 0, 1, 2, 0, 1, 1)\n    >>> compute_venn3_subsets(Counter([1,1,1]), Counter([1,1,1]), Counter([1,1,1,1]))\n    (0, 0, 0, 1, 0, 0, 3)\n    >>> compute_venn3_subsets(Counter([1,1,2,2,3,3]), Counter([2,2,3,3,4,4]), Counter([3,3,4,4,5,5,6,6]))\n    (2, 0, 2, 4, 0, 2, 2)\n    >>> compute_venn3_subsets(Counter([1,2,3]), Counter([2,2,3,3,4,4]), Counter([3,3,4,4,4,5,5,6]))\n    (1, 1, 1, 4, 0, 3, 1)\n    >>> compute_venn3_subsets(set([]), set([]), set([]))\n    (0, 0, 0, 0, 0, 0, 0)\n    >>> compute_venn3_subsets(set([1]), set([]), set([]))\n    (1, 0, 0, 0, 0, 0, 0)\n    >>> compute_venn3_subsets(set([]), set([1]), set([]))\n    (0, 1, 0, 0, 0, 0, 0)\n    >>> compute_venn3_subsets(set([]), set([]), set([1]))\n    (0, 0, 0, 1, 0, 0, 0)\n    >>> compute_venn3_subsets(Counter([]), Counter([]), Counter([1]))\n    (0, 0, 0, 1, 0, 0, 0)\n    >>> compute_venn3_subsets(set([1]), set([1]), set([1]))\n    (0, 0, 0, 0, 0, 0, 1)\n    >>> compute_venn3_subsets(set([1,3,5,7]), set([2,3,6,7]), set([4,5,6,7]))\n    (1, 1, 1, 1, 1, 1, 1)\n    >>> compute_venn3_subsets(Counter([1,3,5,7]), Counter([2,3,6,7]), Counter([4,5,6,7]))\n    (1, 1, 1, 1, 1, 1, 1)\n    >>> compute_venn3_subsets(Counter([1,3,5,7]), set([2,3,6,7]), set([4,5,6,7]))\n    Traceback (most recent call last):\n    ...\n    ValueError: All arguments must be of the same type\n    '''\n    if not (type(a) == type(b) == type(c)):\n        raise ValueError(\"All arguments must be of the same type\")\n    set_size = len if type(a) != Counter else lambda x: sum(x.values())   # We cannot use len to compute the cardinality of a Counter\n    return (set_size(a - (b | c)),  # TODO: This is certainly not the most efficient way to compute.\n        set_size(b - (a | c)),\n        set_size((a & b) - c),\n        set_size(c - (a | b)),\n        set_size((a & c) - b),\n        set_size((b & c) - a),\n        set_size(a & b & c))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef venn3_circles(subsets, normalize_to=1.0, alpha=1.0, color='black', linestyle='solid', linewidth=2.0, ax=None, **kwargs):\n    '''\n    Plots only the three circles for the corresponding Venn diagram.\n    Useful for debugging or enhancing the basic venn diagram.\n    parameters ``subsets``, ``normalize_to`` and ``ax`` are the same as in venn3()\n    kwargs are passed as-is to matplotlib.patches.Circle.\n    returns a list of three Circle patches.\n\n        >>> plot = venn3_circles({'001': 10, '100': 20, '010': 21, '110': 13, '011': 14})\n        >>> plot = venn3_circles([set(['A','B','C']), set(['A','D','E','F']), set(['D','G','H'])])\n    '''\n    # Prepare parameters\n    if isinstance(subsets, dict):\n        subsets = [subsets.get(t, 0) for t in ['100', '010', '110', '001', '101', '011', '111']]\n    elif len(subsets) == 3:\n        subsets = compute_venn3_subsets(*subsets)\n\n    areas = compute_venn3_areas(subsets, normalize_to)\n    centers, radii = solve_venn3_circles(areas)\n\n    if ax is None:\n        ax = gca()\n    prepare_venn_axes(ax, centers, radii)\n    result = []\n    for (c, r) in zip(centers, radii):\n        circle = Circle(c, r, alpha=alpha, edgecolor=color, facecolor='none', linestyle=linestyle, linewidth=linewidth, **kwargs)\n        ax.add_patch(circle)\n        result.append(circle)\n    return result", "response": "Returns a list of three Circle patches for the corresponding Venn diagram."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef venn3(subsets, set_labels=('A', 'B', 'C'), set_colors=('r', 'g', 'b'), alpha=0.4, normalize_to=1.0, ax=None, subset_label_formatter=None):\n    '''Plots a 3-set area-weighted Venn diagram.\n    The subsets parameter can be one of the following:\n     - A list (or a tuple), containing three set objects.\n     - A dict, providing sizes of seven diagram regions.\n       The regions are identified via three-letter binary codes ('100', '010', etc), hence a valid set could look like:\n       {'001': 10, '010': 20, '110':30, ...}. Unmentioned codes are considered to map to 0.\n     - A list (or a tuple) with 7 numbers, denoting the sizes of the regions in the following order:\n       (100, 010, 110, 001, 101, 011, 111).\n\n    ``set_labels`` parameter is a list of three strings - set labels. Set it to None to disable set labels.\n    The ``set_colors`` parameter should be a list of three elements, specifying the \"base colors\" of the three circles.\n    The colors of circle intersections will be computed based on those.\n\n    The ``normalize_to`` parameter specifies the total (on-axes) area of the circles to be drawn. Sometimes tuning it (together\n    with the overall fiture size) may be useful to fit the text labels better.\n    The return value is a ``VennDiagram`` object, that keeps references to the ``Text`` and ``Patch`` objects used on the plot\n    and lets you know the centers and radii of the circles, if you need it.\n\n    The ``ax`` parameter specifies the axes on which the plot will be drawn (None means current axes).\n\n    The ``subset_label_formatter`` parameter is a function that can be passed to format the labels\n    that describe the size of each subset.\n\n    Note: if some of the circles happen to have zero area, you will probably not get a nice picture.\n\n    >>> import matplotlib # (The first two lines prevent the doctest from falling when TCL not installed. Not really necessary in most cases)\n    >>> matplotlib.use('Agg')\n    >>> from matplotlib_venn import *\n    >>> v = venn3(subsets=(1, 1, 1, 1, 1, 1, 1), set_labels = ('A', 'B', 'C'))\n    >>> c = venn3_circles(subsets=(1, 1, 1, 1, 1, 1, 1), linestyle='dashed')\n    >>> v.get_patch_by_id('100').set_alpha(1.0)\n    >>> v.get_patch_by_id('100').set_color('white')\n    >>> v.get_label_by_id('100').set_text('Unknown')\n    >>> v.get_label_by_id('C').set_text('Set C')\n\n    You can provide sets themselves rather than subset sizes:\n    >>> v = venn3(subsets=[set([1,2]), set([2,3,4,5]), set([4,5,6,7,8,9,10,11])])\n    >>> print(\"%0.2f %0.2f %0.2f\" % (v.get_circle_radius(0), v.get_circle_radius(1)/v.get_circle_radius(0), v.get_circle_radius(2)/v.get_circle_radius(0)))\n    0.24 1.41 2.00\n    >>> c = venn3_circles(subsets=[set([1,2]), set([2,3,4,5]), set([4,5,6,7,8,9,10,11])])\n    '''\n    # Prepare parameters\n    if isinstance(subsets, dict):\n        subsets = [subsets.get(t, 0) for t in ['100', '010', '110', '001', '101', '011', '111']]\n    elif len(subsets) == 3:\n        subsets = compute_venn3_subsets(*subsets)\n\n    if subset_label_formatter is None:\n        subset_label_formatter = str\n\n    areas = compute_venn3_areas(subsets, normalize_to)\n    centers, radii = solve_venn3_circles(areas)\n    regions = compute_venn3_regions(centers, radii)\n    colors = compute_venn3_colors(set_colors)\n\n    # Remove regions that are too small from the diagram\n    MIN_REGION_SIZE = 1e-4\n    for i in range(len(regions)):\n        if regions[i].size() < MIN_REGION_SIZE and subsets[i] == 0:\n            regions[i] = VennEmptyRegion()\n\n    # There is a rare case (Issue #12) when the middle region is visually empty\n    # (the positioning of the circles does not let them intersect), yet the corresponding value is not 0.\n    # we address it separately here by positioning the label of that empty region in a custom way\n    if isinstance(regions[6], VennEmptyRegion) and subsets[6] > 0:\n        intersections = [circle_circle_intersection(centers[i], radii[i], centers[j], radii[j]) for (i, j) in [(0, 1), (1, 2), (2, 0)]]\n        middle_pos = np.mean([i[0] for i in intersections], 0)\n        regions[6] = VennEmptyRegion(middle_pos)\n\n    if ax is None:\n        ax = gca()\n    prepare_venn_axes(ax, centers, radii)\n\n    # Create and add patches and text\n    patches = [r.make_patch() for r in regions]\n    for (p, c) in zip(patches, colors):\n        if p is not None:\n            p.set_facecolor(c)\n            p.set_edgecolor('none')\n            p.set_alpha(alpha)\n            ax.add_patch(p)\n    label_positions = [r.label_position() for r in regions]\n    subset_labels = [ax.text(lbl[0], lbl[1], subset_label_formatter(s), va='center', ha='center') if lbl is not None else None for (lbl, s) in zip(label_positions, subsets)]\n\n    # Position labels\n    if set_labels is not None:\n        # There are two situations, when set C is not on the same line with sets A and B, and when the three are on the same line.\n        if abs(centers[2][1] - centers[0][1]) > tol:\n            # Three circles NOT on the same line\n            label_positions = [centers[0] + np.array([-radii[0] / 2, radii[0]]),\n                               centers[1] + np.array([radii[1] / 2, radii[1]]),\n                               centers[2] + np.array([0.0, -radii[2] * 1.1])]\n            labels = [ax.text(pos[0], pos[1], txt, size='large') for (pos, txt) in zip(label_positions, set_labels)]\n            labels[0].set_horizontalalignment('right')\n            labels[1].set_horizontalalignment('left')\n            labels[2].set_verticalalignment('top')\n            labels[2].set_horizontalalignment('center')\n        else:\n            padding = np.mean([r * 0.1 for r in radii])\n            # Three circles on the same line\n            label_positions = [centers[0] + np.array([0.0, - radii[0] - padding]),\n                               centers[1] + np.array([0.0, - radii[1] - padding]),\n                               centers[2] + np.array([0.0, - radii[2] - padding])]\n            labels = [ax.text(pos[0], pos[1], txt, size='large', ha='center', va='top') for (pos, txt) in zip(label_positions, set_labels)]\n    else:\n        labels = None\n    return VennDiagram(patches, subset_labels, labels, centers, radii)", "response": "Plots a 3 - set area - weighted Venn diagram."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef length_degrees(self):\n        '''Computes the length of the arc in degrees.\n        The length computation corresponds to what you would expect if you would draw the arc using matplotlib taking direction into account.\n        \n        >>> Arc((0,0), 1, 0, 0, True).length_degrees()\n        0.0\n        >>> Arc((0,0), 2, 0, 0, False).length_degrees()\n        0.0\n        \n        >>> Arc((0,0), 3, 0, 1, True).length_degrees()\n        1.0\n        >>> Arc((0,0), 4, 0, 1, False).length_degrees()\n        359.0\n\n        >>> Arc((0,0), 5, 0, 360, True).length_degrees()\n        360.0\n        >>> Arc((0,0), 6, 0, 360, False).length_degrees()\n        0.0\n        \n        >>> Arc((0,0), 7, 0, 361, True).length_degrees()\n        360.0\n        >>> Arc((0,0), 8, 0, 361, False).length_degrees()\n        359.0\n        \n        >>> Arc((0,0), 9, 10, -10, True).length_degrees()\n        340.0\n        >>> Arc((0,0), 10, 10, -10, False).length_degrees()\n        20.0\n        \n        >>> Arc((0,0), 1, 10, 5, True).length_degrees()\n        355.0\n        >>> Arc((0,0), 1, -10, -5, False).length_degrees()\n        355.0\n        >>> Arc((0,0), 1, 180, -180, True).length_degrees()\n        0.0\n        >>> Arc((0,0), 1, 180, -180, False).length_degrees()\n        360.0\n        >>> Arc((0,0), 1, -180, 180, True).length_degrees()\n        360.0\n        >>> Arc((0,0), 1, -180, 180, False).length_degrees()\n        0.0\n        >>> Arc((0,0), 1, 175, -175, True).length_degrees()\n        10.0\n        >>> Arc((0,0), 1, 175, -175, False).length_degrees()\n        350.0\n        '''\n        d_angle = self.sign * (self.to_angle - self.from_angle)\n        if (d_angle > 360):\n            return 360.0\n        elif (d_angle < 0):\n            return d_angle % 360.0\n        else:\n            return abs(d_angle)", "response": "Computes the length of the current arc in degrees."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the area of the corresponding arc segment.", "response": "def segment_area(self):\n        '''Returns the area of the corresponding arc segment.\n        \n        >>> Arc((0,0), 2, 0, 360, True).segment_area()\n        12.566...\n        >>> Arc((0,0), 2, 0, 180, True).segment_area()\n        6.283...\n        >>> Arc((0,0), 2, 0, 90, True).segment_area()\n        1.14159...\n        >>> Arc((0,0), 2, 0, 90, False).segment_area()\n        11.42477796...\n        >>> Arc((0,0), 2, 0, 0, False).segment_area()\n        0.0\n        >>> Arc((0, 9), 1, 89.99, 90, False).segment_area()\n        3.1415...\n        '''\n        theta = self.length_radians()\n        return self.radius**2 / 2 * (theta - np.sin(theta))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef angle_as_point(self, angle):\n        '''\n        Converts a given angle in degrees to the point coordinates on the arc's circle.\n        Inverse of point_to_angle.\n        \n        >>> Arc((1, 1), 1, 0, 0, True).angle_as_point(0)\n        array([ 2.,  1.])\n        >>> Arc((1, 1), 1, 0, 0, True).angle_as_point(90)\n        array([ 1.,  2.])\n        >>> Arc((1, 1), 1, 0, 0, True).angle_as_point(-270)\n        array([ 1.,  2.])\n        '''\n        angle_rad = angle * np.pi / 180.0\n        return self.center + self.radius * np.array([np.cos(angle_rad), np.sin(angle_rad)])", "response": "Converts a given angle in degrees to the point coordinates on the arc s circle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the midpoint of the arc as a 1x2 numpy array.", "response": "def mid_point(self):\n        '''\n        Returns the midpoint of the arc as a 1x2 numpy array.\n        '''\n        midpoint_angle = self.from_angle + self.sign*self.length_degrees() / 2\n        return self.angle_as_point(midpoint_angle)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if the parameters of this arc are within <tolerance > of the parameters of the other arc and the direction is the same.", "response": "def approximately_equal(self, arc, tolerance=tol):\n        '''\n        Returns true if the parameters of this arc are within <tolerance> of the parameters of the other arc, and the direction is the same.\n        Note that no angle simplification is performed (i.e. some arcs that might be equal in principle are not declared as such\n        by this method)\n        \n        >>> Arc((0, 0), 10, 20, 30, True).approximately_equal(Arc((tol/2, tol/2), 10+tol/2, 20-tol/2, 30-tol/2, True))\n        True\n        >>> Arc((0, 0), 10, 20, 30, True).approximately_equal(Arc((0, 0), 10, 20, 30, False))\n        False\n        >>> Arc((0, 0), 10, 20, 30, True).approximately_equal(Arc((0, 0+tol), 10, 20, 30, True))\n        False\n        '''\n        return self.direction == arc.direction \\\n                and np.all(abs(self.center - arc.center) < tolerance) and abs(self.radius - arc.radius) < tolerance \\\n                and abs(self.from_angle - arc.from_angle) < tolerance and abs(self.to_angle - arc.to_angle) < tolerance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn true if a point with the corresponding angle is within the arc.", "response": "def contains_angle_degrees(self, angle):\n        '''\n        Returns true, if a point with the corresponding angle (given in degrees) is within the arc.\n        Does no tolerance checks (i.e. if the arc is of length 0, you must provide angle == from_angle == to_angle to get a positive answer here)\n        \n        >>> a = Arc((0, 0), 1, 0, 0, True)\n        >>> assert a.contains_angle_degrees(0)\n        >>> assert a.contains_angle_degrees(360)\n        >>> assert not a.contains_angle_degrees(1)\n        \n        >>> a = Arc((0, 0), 1, 170, -170, True)\n        >>> assert not a.contains_angle_degrees(165)\n        >>> assert a.contains_angle_degrees(170)\n        >>> assert a.contains_angle_degrees(175)\n        >>> assert a.contains_angle_degrees(180)\n        >>> assert a.contains_angle_degrees(185)\n        >>> assert a.contains_angle_degrees(190)\n        >>> assert not a.contains_angle_degrees(195)\n        \n        >>> assert not a.contains_angle_degrees(-195)\n        >>> assert a.contains_angle_degrees(-190)\n        >>> assert a.contains_angle_degrees(-185)\n        >>> assert a.contains_angle_degrees(-180)\n        >>> assert a.contains_angle_degrees(-175)\n        >>> assert a.contains_angle_degrees(-170)\n        >>> assert not a.contains_angle_degrees(-165)\n        >>> assert a.contains_angle_degrees(-170 - 360)\n        >>> assert a.contains_angle_degrees(-190 - 360)\n        >>> assert a.contains_angle_degrees(170 + 360)\n        >>> assert not a.contains_angle_degrees(0)\n        >>> assert not a.contains_angle_degrees(100)\n        >>> assert not a.contains_angle_degrees(-100)\n        '''\n        _d = self.sign * (angle - self.from_angle) % 360.0\n        return (_d <= self.length_degrees())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef intersect_circle(self, center, radius):\n        '''\n        Given a circle, finds the intersection point(s) of the arc with the circle.\n        Returns a list of 2x1 numpy arrays. The list has length 0, 1 or 2, depending on how many intesection points there are.\n        If the circle touches the arc, it is reported as two intersection points (which are equal).\n        Points are ordered along the arc.\n        Intersection with the same circle as the arc's own (which means infinitely many points usually) is reported as no intersection at all.\n        \n        >>> a = Arc((0, 0), 1, -60, 60, True)\n        >>> a.intersect_circle((1, 0), 1)\n        [array([ 0.5..., -0.866...]), array([ 0.5...,  0.866...])]\n        >>> a.intersect_circle((0.9, 0), 1)\n        []\n        >>> a.intersect_circle((1,-0.1), 1)\n        [array([ 0.586...,  0.810...])]\n        >>> a.intersect_circle((1, 0.1), 1)\n        [array([ 0.586..., -0.810...])]\n        >>> a.intersect_circle((0, 0), 1)  # Infinitely many intersection points\n        []\n        >>> a.intersect_circle((2, 0), 1)  # Touching point, hence repeated twice\n        [array([ 1.,  0.]), array([ 1.,  0.])]\n        \n        >>> a = Arc((0, 0), 1, 60, -60, False) # Same arc, different direction\n        >>> a.intersect_circle((1, 0), 1)\n        [array([ 0.5...,  0.866...]), array([ 0.5..., -0.866...])]\n        \n        >>> a = Arc((0, 0), 1, 120, -120, True)\n        >>> a.intersect_circle((-1, 0), 1)\n        [array([-0.5...,  0.866...]), array([-0.5..., -0.866...])]\n        >>> a.intersect_circle((-0.9, 0), 1)\n        []\n        >>> a.intersect_circle((-1,-0.1), 1)\n        [array([-0.586...,  0.810...])]\n        >>> a.intersect_circle((-1, 0.1), 1)\n        [array([-0.586..., -0.810...])]\n        >>> a.intersect_circle((-2, 0), 1)\n        [array([-1.,  0.]), array([-1.,  0.])]\n        >>> a = Arc((0, 0), 1, -120, 120, False)\n        >>> a.intersect_circle((-1, 0), 1)\n        [array([-0.5..., -0.866...]), array([-0.5...,  0.866...])]\n        '''\n        intersections = circle_circle_intersection(self.center, self.radius, center, radius)\n        if intersections is None:\n            return []\n        \n        # Check whether the points lie on the arc and order them accordingly\n        _len = self.length_degrees()\n        isections = [[self.sign * (self.point_as_angle(pt) - self.from_angle) % 360.0, pt] for pt in intersections]\n        \n        # Try to find as many candidate intersections as possible (i.e. +- tol within arc limits)\n        # Unless arc's length is 360, interpret intersections just before the arc's starting point as belonging to the starting point.\n        if _len < 360.0 - tol:\n            for isec in isections:\n                if isec[0] > 360.0 - tol:\n                    isec[0] = 0.0\n        \n        isections = [(a, pt[0], pt[1]) for (a, pt) in isections if a < _len + tol or a > 360 - tol]\n        isections.sort()\n        return [np.array([b, c]) for (a, b, c) in isections]", "response": "Given a circle find the intersection point of the arc with the circle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intersect_arc(self, arc):\n        '''\n        Given an arc, finds the intersection point(s) of this arc with that.\n        Returns a list of 2x1 numpy arrays. The list has length 0, 1 or 2, depending on how many intesection points there are.\n        Points are ordered along the arc.\n        Intersection with the arc along the same circle (which means infinitely many points usually) is reported as no intersection at all.\n        \n        >>> a = Arc((0, 0), 1, -90, 90, True)\n        >>> a.intersect_arc(Arc((1, 0), 1, 90, 270, True))\n        [array([ 0.5      , -0.866...]), array([ 0.5      ,  0.866...])]\n        >>> a.intersect_arc(Arc((1, 0), 1, 90, 180, True))\n        [array([ 0.5      ,  0.866...])]\n        >>> a.intersect_arc(Arc((1, 0), 1, 121, 239, True))\n        []\n        >>> a.intersect_arc(Arc((1, 0), 1, 120-tol, 240+tol, True))     # Without -tol and +tol the results differ on different architectures due to rounding (see Debian #813782).\n        [array([ 0.5      , -0.866...]), array([ 0.5      ,  0.866...])]\n        '''\n        intersections = self.intersect_circle(arc.center, arc.radius)\n        isections = [pt for pt in intersections if arc.contains_angle_degrees(arc.point_as_angle(pt))]\n        return isections", "response": "Given an arc finds the intersection point of this arc with that arc."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a sub - arc from a given angle.", "response": "def subarc(self, from_angle=None, to_angle=None):\n        '''\n        Creates a sub-arc from a given angle (or beginning of this arc) to a given angle (or end of this arc).\n        Verifies that from_angle and to_angle are within the arc and properly ordered.\n        If from_angle is None, start of this arc is used instead.\n        If to_angle is None, end of this arc is used instead.\n        Angles are given in degrees.\n        \n        >>> a = Arc((0, 0), 1, 0, 360, True)\n        >>> a.subarc(None, None)\n        Arc([0.000, 0.000], 1.000,   0.000, 360.000,   True,   degrees=360.000)\n        >>> a.subarc(360, None)\n        Arc([0.000, 0.000], 1.000,   360.000, 360.000,   True,   degrees=0.000)\n        >>> a.subarc(0, None)\n        Arc([0.000, 0.000], 1.000,   0.000, 360.000,   True,   degrees=360.000)\n        >>> a.subarc(-10, None)\n        Arc([0.000, 0.000], 1.000,   350.000, 360.000,   True,   degrees=10.000)\n        >>> a.subarc(None, -10)\n        Arc([0.000, 0.000], 1.000,   0.000, 350.000,   True,   degrees=350.000)\n        >>> a.subarc(1, 359).subarc(2, 358).subarc()\n        Arc([0.000, 0.000], 1.000,   2.000, 358.000,   True,   degrees=356.000)\n        '''\n        \n        if from_angle is None:\n            from_angle = self.from_angle\n        if to_angle is None:\n            to_angle = self.to_angle\n        cur_length = self.length_degrees()\n        d_new_from = self.sign * (from_angle - self.from_angle)\n        if (d_new_from != 360.0):\n            d_new_from = d_new_from % 360.0\n        d_new_to = self.sign * (to_angle - self.from_angle)\n        if (d_new_to != 360.0):\n            d_new_to = d_new_to % 360.0\n        # Gracefully handle numeric precision issues for zero-length arcs\n        if abs(d_new_from - d_new_to) < tol:\n            d_new_from = d_new_to\n        if d_new_to < d_new_from:\n            raise ValueError(\"Subarc to-angle must be smaller than from-angle.\")\n        if d_new_to > cur_length + tol:\n            raise ValueError(\"Subarc to-angle must lie within the current arc.\")\n        return Arc(self.center, self.radius, self.from_angle + self.sign*d_new_from, self.from_angle + self.sign*d_new_to, self.direction)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive two points on the arc extract a sub - arc between those points.", "response": "def subarc_between_points(self, p_from=None, p_to=None):\n        '''\n        Given two points on the arc, extract a sub-arc between those points.\n        No check is made to verify the points are actually on the arc.\n        It is basically a wrapper around subarc(point_as_angle(p_from), point_as_angle(p_to)).\n        Either p_from or p_to may be None to denote first or last arc endpoints.\n        \n        >>> a = Arc((0, 0), 1, 0, 90, True)\n        >>> a.subarc_between_points((1, 0), (np.cos(np.pi/4), np.sin(np.pi/4)))\n        Arc([0.000, 0.000], 1.000,   0.000, 45.000,   True,   degrees=45.000)\n        >>> a.subarc_between_points(None, None)\n        Arc([0.000, 0.000], 1.000,   0.000, 90.000,   True,   degrees=90.000)\n        >>> a.subarc_between_points((np.cos(np.pi/4), np.sin(np.pi/4)))\n        Arc([0.000, 0.000], 1.000,   45.000, 90.000,   True,   degrees=45.000)\n        '''\n        a_from = self.point_as_angle(p_from) if p_from is not None else None\n        a_to = self.point_as_angle(p_to) if p_to is not None else None\n        return self.subarc(a_from, a_to)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a copy of this arc with the direction flipped.", "response": "def reversed(self):\n        '''\n        Returns a copy of this arc, with the direction flipped.\n        \n        >>> Arc((0, 0), 1, 0, 360, True).reversed()\n        Arc([0.000, 0.000], 1.000,   360.000, 0.000,   False,   degrees=360.000)\n        >>> Arc((0, 0), 1, 175, -175, True).reversed()\n        Arc([0.000, 0.000], 1.000,   -175.000, 175.000,   False,   degrees=10.000)\n        >>> Arc((0, 0), 1, 0, 370, True).reversed()\n        Arc([0.000, 0.000], 1.000,   370.000, 0.000,   False,   degrees=360.000)\n        '''\n        return Arc(self.center, self.radius, self.to_angle, self.from_angle, not self.direction)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a unit vector pointing in the arc s movement direction at a given angle.", "response": "def direction_vector(self, angle):\n        '''\n        Returns a unit vector, pointing in the arc's movement direction at a given (absolute) angle (in degrees).\n        No check is made whether angle lies within the arc's span (the results for angles outside of the arc's span )\n        Returns a 2x1 numpy array.\n        \n        >>> a = Arc((0, 0), 1, 0, 90, True)\n        >>> assert all(abs(a.direction_vector(0) - np.array([0.0, 1.0])) < tol)\n        >>> assert all(abs(a.direction_vector(45) - np.array([ -0.70710678, 0.70710678])) < 1e-6)\n        >>> assert all(abs(a.direction_vector(90) - np.array([-1.0, 0.0])) < tol)\n        >>> assert all(abs(a.direction_vector(135) - np.array([-0.70710678, -0.70710678])) < 1e-6)\n        >>> assert all(abs(a.direction_vector(-180) - np.array([0.0, -1.0])) < tol)\n        >>> assert all(abs(a.direction_vector(-90) - np.array([1.0, 0.0])) < tol)\n        >>> a = a.reversed()\n        >>> assert all(abs(a.direction_vector(0) - np.array([0.0, -1.0])) < tol)\n        >>> assert all(abs(a.direction_vector(45) - np.array([ 0.70710678, -0.70710678])) < 1e-6)\n        >>> assert all(abs(a.direction_vector(90) - np.array([1.0, 0.0])) < tol)\n        >>> assert all(abs(a.direction_vector(135) - np.array([0.70710678, 0.70710678])) < 1e-6)\n        >>> assert all(abs(a.direction_vector(-180) - np.array([0.0, 1.0])) < tol)\n        >>> assert all(abs(a.direction_vector(-90) - np.array([-1.0, 0.0])) < tol)\n        '''\n        a = angle + self.sign * 90\n        a = a * np.pi / 180.0\n        return np.array([np.cos(a), np.sin(a)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fix_360_to_0(self):\n        '''\n        Sometimes we have to create an arc using from_angle and to_angle computed numerically.\n        If from_angle == to_angle, it may sometimes happen that a tiny discrepancy will make from_angle > to_angle, and instead of\n        getting a 0-length arc we end up with a 360-degree arc.\n        Sometimes we know for sure that a 360-degree arc is not what we want, and in those cases\n        the problem is easy to fix. This helper method does that. It checks whether from_angle and to_angle are numerically similar,\n        and if so makes them equal.\n        \n        >>> a = Arc((0, 0), 1, 0, -tol/2, True)\n        >>> a\n        Arc([0.000, 0.000], 1.000,   0.000, -0.000,   True,   degrees=360.000)\n        >>> a.fix_360_to_0()\n        >>> a\n        Arc([0.000, 0.000], 1.000,   -0.000, -0.000,   True,   degrees=0.000)\n        '''\n        if abs(self.from_angle - self.to_angle) < tol:\n            self.from_angle = self.to_angle", "response": "This method fixes the from_angle and to_angle of a new Arc to 0 - length arc."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lies_on_circle(self, center, radius):\n        '''Tests whether the arc circle's center and radius match the given ones within <tol> tolerance.\n        \n        >>> a = Arc((0, 0), 1, 0, 0, False)\n        >>> a.lies_on_circle((tol/2, tol/2), 1+tol/2)\n        True\n        >>> a.lies_on_circle((tol/2, tol/2), 1-tol)\n        False\n        '''\n        return np.all(abs(np.asarray(center) - self.center) < tol) and abs(radius - self.radius) < tol", "response": "Tests whether the arc circle s center and radius match the given ones within <tol > tolerance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmix two colors to compute a mixed color.", "response": "def mix_colors(col1, col2, col3=None):\n    '''\n    Mixes two colors to compute a \"mixed\" color (for purposes of computing\n    colors of the intersection regions based on the colors of the sets.\n    Note that we do not simply compute averages of given colors as those seem\n    too dark for some default configurations. Thus, we lighten the combination up a bit.\n    \n    Inputs are (up to) three RGB triples of floats 0.0-1.0 given as numpy arrays.\n    \n    >>> mix_colors(np.array([1.0, 0., 0.]), np.array([1.0, 0., 0.])) # doctest: +NORMALIZE_WHITESPACE\n    array([ 1.,  0.,  0.])\n    >>> mix_colors(np.array([1.0, 1., 0.]), np.array([1.0, 0.9, 0.]), np.array([1.0, 0.8, 0.1])) # doctest: +NORMALIZE_WHITESPACE\n    array([ 1. ,  1. , 0.04])    \n    '''\n    if col3 is None:\n        mix_color = 0.7 * (col1 + col2)\n    else:\n        mix_color = 0.4 * (col1 + col2 + col3)\n    mix_color = np.min([mix_color, [1.0, 1.0, 1.0]], 0)    \n    return mix_color"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the axes object to suit venn plotting. I. e. hides ticks makes proper xlim and ylim.", "response": "def prepare_venn_axes(ax, centers, radii):\n    '''\n    Sets properties of the axis object to suit venn plotting. I.e. hides ticks, makes proper xlim/ylim.\n    '''\n    ax.set_aspect('equal')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    min_x = min([centers[i][0] - radii[i] for i in range(len(radii))])\n    max_x = max([centers[i][0] + radii[i] for i in range(len(radii))])\n    min_y = min([centers[i][1] - radii[i] for i in range(len(radii))])\n    max_y = max([centers[i][1] + radii[i] for i in range(len(radii))])\n    ax.set_xlim([min_x - 0.1, max_x + 0.1])\n    ax.set_ylim([min_y - 0.1, max_y + 0.1])\n    ax.set_axis_off()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a subset label by a region id.", "response": "def get_label_by_id(self, id):\n        '''\n        Returns a subset label by a \"region id\". \n        A region id is a string '10', '01' or '11' for 2-circle diagram or a \n        string like '001', '010', etc, for 3-circle diagram.\n        Alternatively, if the string 'A', 'B'  (or 'C' for 3-circle diagram) is given, the label of the\n        corresponding set is returned (or None).'''\n        if len(id) == 1:\n            return self.set_labels[self.id2idx[id]] if self.set_labels is not None else None\n        else:\n            return self.subset_labels[self.id2idx[id]]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hide_zeroes(self):\n        '''\n        Sometimes it makes sense to hide the labels for subsets whose size is zero.\n        This utility method does this.\n        '''\n        for v in self.subset_labels:\n            if v is not None and v.get_text() == '0':\n                v.set_visible(False)", "response": "Hide the zeroes in the subset labels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the parameters based on the difference between two dictionaries.", "response": "def calculate_dict_diff(old_params: dict, new_params: dict):\n    \"\"\"Return the parameters based on the difference.\n\n    If a parameter exists in `old_params` but not in `new_params` then\n    parameter will be set to an empty string.\n    \"\"\"\n    # Ignore all None values as those cannot be saved.\n    old_params = remove_None(old_params)\n    new_params = remove_None(new_params)\n    params_diff = {}\n    for key, value in old_params.items():\n        if key in new_params:\n            if value != new_params[key]:\n                params_diff[key] = new_params[key]\n        else:\n            params_diff[key] = ''\n    for key, value in new_params.items():\n        if key not in old_params:\n            params_diff[key] = value\n    return params_diff"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datum_to_value(self, instance, datum):\n        if datum is None:\n            return []\n        if not isinstance(datum, Sequence):\n            raise TypeError(\n                \"datum must be a sequence, not %s\" % type(datum).__name__)\n        # Get the class from the bound origin.\n        bound = getattr(instance._origin, \"FilesystemGroupDevices\")\n        return bound((\n            get_device_object(instance._origin, item)\n            for item in datum\n            ))", "response": "Convert a given MAAS - side datum into a Python - side value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that arg is a valid file.", "response": "def validate_file(parser, arg):\n    \"\"\"Validates that `arg` is a valid file.\"\"\"\n    if not os.path.isfile(arg):\n        parser.error(\"%s is not a file.\" % arg)\n    return arg"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters commands with the given parser.", "response": "def register(parser):\n    \"\"\"Register commands with the given parser.\"\"\"\n    cmd_machines.register(parser)\n    cmd_machine.register(parser)\n    cmd_allocate.register(parser)\n    cmd_deploy.register(parser)\n    cmd_commission.register(parser)\n    cmd_release.register(parser)\n    cmd_abort.register(parser)\n    cmd_mark_fixed.register(parser)\n    cmd_mark_broken.register(parser)\n    cmd_power_off.register(parser)\n    cmd_power_on.register(parser)\n    cmd_ssh.register(parser)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef perform_action(\n            self, action, machines, params, progress_title, success_title):\n        \"\"\"Perform the action on the set of machines.\"\"\"\n        if len(machines) == 0:\n            return 0\n        with utils.Spinner() as context:\n            return self._async_perform_action(\n                context, action, list(machines), params,\n                progress_title, success_title)", "response": "Perform the action on the set of machines."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a set of machines based on hostnames.", "response": "def get_machines(self, origin, hostnames):\n        \"\"\"Return a set of machines based on `hostnames`.\n\n        Any hostname that is not found will result in an error.\n        \"\"\"\n        hostnames = {\n            hostname: True\n            for hostname in hostnames\n        }\n        machines = origin.Machines.read(hostnames=hostnames)\n        machines = [\n            machine\n            for machine in machines\n            if hostnames.pop(machine.hostname, False)\n        ]\n        if len(hostnames) > 0:\n            raise CommandError(\n                \"Unable to find %s %s.\" % (\n                    \"machines\" if len(hostnames) > 1 else \"machine\",\n                    ','.join(hostnames)))\n        return machines"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the SSH arguments to the parser.", "response": "def add_ssh_options(self, parser):\n        \"\"\"Add the SSH arguments to the `parser`.\"\"\"\n        parser.add_argument(\n            \"--username\", metavar='USER', help=(\n                \"Username for the SSH connection.\"))\n        parser.add_argument(\n            \"--boot-only\", action=\"store_true\", help=(\n                \"Only use the IP addresses on the machine's boot interface.\"))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ip_addresses(self, machine, *, boot_only=False, discovered=False):\n        boot_ips = [\n            link.ip_address\n            for link in machine.boot_interface.links\n            if link.ip_address\n        ]\n        if boot_only:\n            if boot_ips:\n                return boot_ips\n            elif discovered:\n                return [\n                    link.ip_address\n                    for link in machine.boot_interface.discovered\n                    if link.ip_address\n                ]\n            else:\n                return []\n        else:\n            other_ips = [\n                link.ip_address\n                for interface in machine.interfaces\n                for link in interface.links\n                if (interface.id != machine.boot_interface.id and\n                    link.ip_address)\n            ]\n            ips = boot_ips + other_ips\n            if ips:\n                return ips\n            elif discovered:\n                return [\n                    link.ip_address\n                    for link in machine.boot_interface.discovered\n                    if link.ip_address\n                ] + [\n                    link.ip_address\n                    for interface in machine.interfaces\n                    for link in interface.discovered\n                    if (interface.id != machine.boot_interface.id and\n                        link.ip_address)\n                ]\n            else:\n                return []", "response": "Return all IP addresses for a machine."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _async_get_sshable_ips(self, ip_addresses):\n\n        async def _async_ping(ip_address):\n            try:\n                reader, writer = await asyncio.wait_for(\n                    asyncio.open_connection(ip_address, 22), timeout=5)\n            except (OSError, TimeoutError):\n                return None\n            try:\n                line = await reader.readline()\n            finally:\n                writer.close()\n            if line.startswith(b'SSH-'):\n                return ip_address\n\n        ssh_ips = await asyncio.gather(*[\n            _async_ping(ip_address)\n            for ip_address in ip_addresses\n        ])\n        return [\n            ip_address\n            for ip_address in ssh_ips\n            if ip_address is not None\n        ]", "response": "Return list of all IP addresses that could be pinged."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_ssh(self, *args):\n        ssh = subprocess.Popen(\n            args,\n            stdin=subprocess.DEVNULL,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL)\n        ssh.wait()\n        return ssh.returncode == 0", "response": "Check if SSH connection can be made to IP with username."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_deploy_options(self, options):\n        user_data = None\n        if options.user_data and options.b64_user_data:\n            raise CommandError(\n                \"Cannot provide both --user-data and --b64-user-data.\")\n        if options.b64_user_data:\n            user_data = options.b64_user_data\n        if options.user_data:\n            user_data = base64_file(options.user_data).decode(\"ascii\")\n        return utils.remove_None({\n            'distro_series': options.image,\n            'hwe_kernel': options.hwe_kernel,\n            'user_data': user_data,\n            'comment': options.comment,\n            'wait': False,\n        })", "response": "Return the deployment options based on command line options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling the user aborting mid deployment.", "response": "def _handle_abort(self, machine, allocated):\n        \"\"\"Handle the user aborting mid deployment.\"\"\"\n        abort = yes_or_no(\"Abort deployment?\")\n        if abort:\n            with utils.Spinner() as context:\n                if allocated:\n                    context.msg = colorized(\n                        \"{autoblue}Releasing{/autoblue} %s\") % (\n                            machine.hostname)\n                    machine.release()\n                    context.print(colorized(\n                        \"{autoblue}Released{/autoblue} %s\") % (\n                            machine.hostname))\n                else:\n                    context.msg = colorized(\n                        \"{autoblue}Aborting{/autoblue} %s\") % (\n                            machine.hostname)\n                    machine.abort()\n                    context.print(colorized(\n                        \"{autoblue}Aborted{/autoblue} %s\") % (\n                            machine.hostname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters profile commands with the given parser.", "response": "def register(parser):\n    \"\"\"Register profile commands with the given parser.\"\"\"\n    cmd_login.register(parser)\n    cmd_logout.register(parser)\n    cmd_switch.register(parser)\n    cmd_profiles.register(parser)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexplaining what to do next.", "response": "def print_whats_next(profile):\n        \"\"\"Explain what to do next.\"\"\"\n        what_next = [\n            \"{{autogreen}}Congratulations!{{/autogreen}} You are logged in \"\n            \"to the MAAS server at {{autoblue}}{profile.url}{{/autoblue}} \"\n            \"with the profile name {{autoblue}}{profile.name}{{/autoblue}}.\",\n            \"For help with the available commands, try:\",\n            \"  maas help\",\n            ]\n        for message in what_next:\n            message = message.format(profile=profile)\n            print(colorized(message))\n            print()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nobtains credentials from the user s input.", "response": "def obtain_credentials(credentials):\n    \"\"\"Prompt for credentials if possible.\n\n    If the credentials are \"-\" then read from stdin without interactive\n    prompting.\n    \"\"\"\n    if credentials == \"-\":\n        credentials = sys.stdin.readline().strip()\n    elif credentials is None:\n        credentials = try_getpass(\n            \"API key (leave empty for anonymous access): \")\n    # Ensure that the credentials have a valid form.\n    if credentials and not credentials.isspace():\n        return Credentials.parse(credentials)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a SessionAPI object for a given MAAS instance.", "response": "async def fromURL(\n            cls, url, *, credentials=None, insecure=False):\n        \"\"\"Return a `SessionAPI` for a given MAAS instance.\"\"\"\n        try:\n            description = await helpers.fetch_api_description(\n                url, insecure=insecure)\n        except helpers.RemoteError as error:\n            # For now just re-raise as SessionError.\n            raise SessionError(str(error))\n        else:\n            session = cls(description, credentials)\n            session.insecure = insecure\n            return session"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fromProfileName(cls, name):\n        with profiles.ProfileStore.open() as config:\n            return cls.fromProfile(config.load(name))", "response": "Return a SessionAPI object from a given configuration profile name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def login(\n            cls, url, *, username=None, password=None, insecure=False):\n        \"\"\"Make a `SessionAPI` by logging-in with a username and password.\n\n        :return: A tuple of ``profile`` and ``session``, where the former is\n            an unsaved `Profile` instance, and the latter is a `SessionAPI`\n            instance made using the profile.\n        \"\"\"\n        profile = await helpers.login(\n            url=url, username=username, password=password, insecure=insecure)\n        session = cls(profile.description, profile.credentials)\n        session.insecure = insecure\n        return profile, session", "response": "Make a SessionAPI by logging - in with a username and password."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def connect(\n            cls, url, *, apikey=None, insecure=False):\n        \"\"\"Make a `SessionAPI` by connecting with an apikey.\n\n        :return: A tuple of ``profile`` and ``session``, where the former is\n            an unsaved `Profile` instance, and the latter is a `SessionAPI`\n            instance made using the profile.\n        \"\"\"\n        profile = await helpers.connect(\n            url=url, apikey=apikey, insecure=insecure)\n        session = cls(profile.description, profile.credentials)\n        session.insecure = insecure\n        return profile, session", "response": "Make a SessionAPI by connecting with an apikey."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrebind the parameters into the URI.", "response": "def rebind(self, **params):\n        \"\"\"Rebind the parameters into the URI.\n\n        :return: A new `CallAPI` instance with the new parameters.\n        \"\"\"\n        new_params = self.__params.copy()\n        new_params.update(params)\n        return self.__class__(new_params, self.__action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nissues the call. :param data: Data to pass in the *body* of the request.", "response": "def call(self, **data):\n        \"\"\"Issue the call.\n\n        :param data: Data to pass in the *body* of the request.\n        \"\"\"\n        uri, body, headers = self.prepare(data)\n        return self.dispatch(uri, body, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare(self, data):\n        def expand(data):\n            for name, value in data.items():\n                if isinstance(value, Iterable):\n                    for value in value:\n                        yield name, value\n                else:\n                    yield name, value\n\n        # `data` must be an iterable yielding 2-tuples.\n        if self.action.method in (\"GET\", \"DELETE\"):\n            # MAAS does not expect an entity-body for GET or DELETE.\n            data = expand(data)\n        else:\n            # MAAS expects and entity-body for PUT and POST.\n            data = data.items()\n\n        # Bundle things up ready to throw over the wire.\n        uri, body, headers = utils.prepare_payload(\n            self.action.op, self.action.method, self.uri, data)\n\n        # Headers are returned as a list, but they must be a dict for\n        # the signing machinery.\n        headers = dict(headers)\n\n        # Sign request if credentials have been provided.\n        credentials = self.action.handler.session.credentials\n        if credentials is not None:\n            utils.sign(uri, headers, credentials)\n\n        return uri, body, headers", "response": "Prepare the payload for the call."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndispatch the call via HTTP.", "response": "async def dispatch(self, uri, body, headers):\n        \"\"\"Dispatch the call via HTTP.\n\n        This is used by `call` and can be overridden to use a different HTTP\n        library.\n        \"\"\"\n        insecure = self.action.handler.session.insecure\n        connector = aiohttp.TCPConnector(verify_ssl=(not insecure))\n        session = aiohttp.ClientSession(connector=connector)\n        async with session:\n            response = await session.request(\n                self.action.method, uri, data=body,\n                headers=_prefer_json(headers))\n            async with response:\n                # Fetch the raw body content.\n                content = await response.read()\n\n                # Debug output.\n                if self.action.handler.session.debug:\n                    print(response)\n\n                # 2xx status codes are all okay.\n                if response.status // 100 != 2:\n                    request = {\n                        \"body\": body,\n                        \"headers\": headers,\n                        \"method\": self.action.method,\n                        \"uri\": uri,\n                    }\n                    raise CallError(request, response, content, self)\n\n                # Decode from JSON if that's what it's declared as.\n                if response.content_type is None:\n                    data = await response.read()\n                elif response.content_type.endswith('/json'):\n                    data = await response.json()\n                else:\n                    data = await response.read()\n\n                if response.content_type is None:\n                    data = content\n                elif response.content_type.endswith('/json'):\n                    # JSON should always be UTF-8.\n                    data = json.loads(content.decode(\"utf-8\"))\n                else:\n                    data = content\n\n                return CallResult(response, content, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to MAAS at url using an API key.", "response": "async def connect(url, *, apikey=None, insecure=False):\n    \"\"\"Connect to MAAS at `url` using a previously obtained API key.\n\n    :param url: The URL of MAAS, e.g. http://maas.example.com:5240/MAAS/\n    :param apikey: The API key to use, e.g.\n        SkTvsyHhzkREvvdtNk:Ywn5FvXVupVPvNUhrN:cm3Q2f5naXYPYsrPDPfQy9Q9cUFaEgbM\n    :param insecure: Whether to check TLS certificates when using HTTPS.\n\n    :return: A client object.\n    \"\"\"\n    from .facade import Client  # Lazy.\n    from .viscera import Origin  # Lazy.\n    profile, origin = await Origin.connect(\n        url, apikey=apikey, insecure=insecure)\n    return Client(origin)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to MAAS at url with a user name and password.", "response": "async def login(url, *, username=None, password=None, insecure=False):\n    \"\"\"Connect to MAAS at `url` with a user name and password.\n\n    :param url: The URL of MAAS, e.g. http://maas.example.com:5240/MAAS/\n    :param username: The user name to use, e.g. fred.\n    :param password: The user's password.\n    :param insecure: Whether to check TLS certificates when using HTTPS.\n\n    :return: A client object.\n    \"\"\"\n    from .facade import Client  # Lazy.\n    from .viscera import Origin  # Lazy.\n    profile, origin = await Origin.login(\n        url, username=username, password=password, insecure=insecure)\n    return Client(origin)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def read(cls, *, hostnames: typing.Sequence[str] = None):\n        params = {}\n        if hostnames:\n            params[\"hostname\"] = [\n                normalize_hostname(hostname)\n                for hostname in hostnames\n            ]\n        data = await cls._handler.read(**params)\n        return cls(map(cls._object, data))", "response": "Return a list of all the related objects in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting to a Machine object.", "response": "def as_machine(self):\n        \"\"\"Convert to a `Machine` object.\n\n        `node_type` must be `NodeType.MACHINE`.\n        \"\"\"\n        if self.node_type != NodeType.MACHINE:\n            raise ValueError(\n                'Cannot convert to `Machine`, node_type is not a machine.')\n        return self._origin.Machine(self._data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting to a Device object.", "response": "def as_device(self):\n        \"\"\"Convert to a `Device` object.\n\n        `node_type` must be `NodeType.DEVICE`.\n        \"\"\"\n        if self.node_type != NodeType.DEVICE:\n            raise ValueError(\n                'Cannot convert to `Device`, node_type is not a device.')\n        return self._origin.Device(self._data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting to a rack controller object.", "response": "def as_rack_controller(self):\n        \"\"\"Convert to a `RackController` object.\n\n        `node_type` must be `NodeType.RACK_CONTROLLER` or\n        `NodeType.REGION_AND_RACK_CONTROLLER`.\n        \"\"\"\n        if self.node_type not in [\n                NodeType.RACK_CONTROLLER, NodeType.REGION_AND_RACK_CONTROLLER]:\n            raise ValueError(\n                'Cannot convert to `RackController`, node_type is not a '\n                'rack controller.')\n        return self._origin.RackController(self._data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_region_controller(self):\n        if self.node_type not in [\n                NodeType.REGION_CONTROLLER,\n                NodeType.REGION_AND_RACK_CONTROLLER]:\n            raise ValueError(\n                'Cannot convert to `RegionController`, node_type is not a '\n                'region controller.')\n        return self._origin.RegionController(self._data)", "response": "Convert to a RegionController object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the power parameters for this node.", "response": "async def get_power_parameters(self):\n        \"\"\"Get the power paramters for this node.\"\"\"\n        data = await self._handler.power_parameters(system_id=self.system_id)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def set_power(\n            self, power_type: str,\n            power_parameters: typing.Mapping[str, typing.Any] = {}):\n        \"\"\"Set the power type and power parameters for this node.\"\"\"\n        data = await self._handler.update(\n            system_id=self.system_id, power_type=power_type,\n            power_parameters=power_parameters)\n        self.power_type = data['power_type']", "response": "Set the power type and power parameters for this node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create(\n            cls, boot_source, os, release, *,\n            arches=None, subarches=None, labels=None):\n        \"\"\"Create a new `BootSourceSelection`.\"\"\"\n        if not isinstance(boot_source, BootSource):\n            raise TypeError(\n                \"boot_source must be a BootSource, not %s\"\n                % type(boot_source).__name__)\n        if arches is None:\n            arches = ['*']\n        if subarches is None:\n            subarches = ['*']\n        if labels is None:\n            labels = ['*']\n        data = await cls._handler.create(\n            boot_source_id=boot_source.id,\n            os=os, release=release, arches=arches, subarches=subarches,\n            labels=labels)\n        return cls._object(data, {\"boot_source_id\": boot_source.id})", "response": "Create a new BootSourceSelection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget BootSourceSelection by id.", "response": "async def read(cls, boot_source, id):\n        \"\"\"Get `BootSourceSelection` by `id`.\"\"\"\n        if isinstance(boot_source, int):\n            boot_source_id = boot_source\n        elif isinstance(boot_source, BootSource):\n            boot_source_id = boot_source.id\n        else:\n            raise TypeError(\n                \"boot_source must be a BootSource or int, not %s\"\n                % type(boot_source).__name__)\n        data = await cls._handler.read(boot_source_id=boot_source_id, id=id)\n        return cls(data, {\"boot_source_id\": boot_source_id})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def delete(self):\n        await self._handler.delete(\n            boot_source_id=self.boot_source.id, id=self.id)", "response": "Delete boot source selection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_size_and_sha265(content: io.IOBase, chunk_size: int):\n    size = 0\n    sha256 = hashlib.sha256()\n    content.seek(0, io.SEEK_SET)\n    while True:\n        buf = content.read(chunk_size)\n        length = len(buf)\n        size += length\n        sha256.update(buf)\n        if length != chunk_size:\n            break\n    return size, sha256.hexdigest()", "response": "Calculates the size and the sha2566 value of the content."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def create(\n            cls, name: str, architecture: str, content: io.IOBase, *,\n            title: str = \"\",\n            filetype: BootResourceFileType = BootResourceFileType.TGZ,\n            chunk_size=(1 << 22), progress_callback=None):\n        \"\"\"Create a `BootResource`.\n\n        Creates an uploaded boot resource with `content`. The `content` is\n        uploaded in chunks of `chunk_size`. `content` must be seekable as the\n        first pass through the `content` will calculate the size and sha256\n        value then the second pass will perform the actual upload.\n\n        :param name: Name of the boot resource. Must be in format 'os/release'.\n        :type name: `str`\n        :param architecture: Architecture of the boot resource. Must be in\n            format 'arch/subarch'.\n        :type architecture: `str`\n        :param content: Content of the boot resource.\n        :type content: `io.IOBase`\n        :param title: Title of the boot resource.\n        :type title: `str`\n        :param filetype: Type of file in content.\n        :type filetype: `str`\n        :param chunk_size: Size in bytes to upload to MAAS in chunks.\n            (Default is 4 MiB).\n        :type chunk_size: `int`\n        :param progress_callback: Called to inform the current progress of the\n            upload. One argument is passed with the progress as a precentage.\n            If the resource was already complete and no content\n            needed to be uploaded then this callback will never be called.\n        :type progress_callback: Callable\n        :returns: Create boot resource.\n        :rtype: `BootResource`.\n        \"\"\"\n        if '/' not in name:\n            raise ValueError(\n                \"name must be in format os/release; missing '/'\")\n        if '/' not in architecture:\n            raise ValueError(\n                \"architecture must be in format arch/subarch; missing '/'\")\n        if not content.readable():\n            raise ValueError(\"content must be readable\")\n        elif not content.seekable():\n            raise ValueError(\"content must be seekable\")\n        if chunk_size <= 0:\n            raise ValueError(\n                \"chunk_size must be greater than 0, not %d\" % chunk_size)\n\n        size, sha256 = calc_size_and_sha265(content, chunk_size)\n        resource = cls._object(await cls._handler.create(\n            name=name, architecture=architecture, title=title,\n            filetype=filetype.value, size=str(size), sha256=sha256))\n        newest_set = max(resource.sets, default=None)\n        assert newest_set is not None\n        resource_set = resource.sets[newest_set]\n        assert len(resource_set.files) == 1\n        rfile = list(resource_set.files.values())[0]\n        if rfile.complete:\n            # Already created and fully up-to-date.\n            return resource\n        else:\n            # Upload in chunks and reload boot resource.\n            await cls._upload_chunks(\n                rfile, content, chunk_size, progress_callback)\n            return cls._object.read(resource.id)", "response": "Create a boot resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload the content to rfile in chunks using chunk_size.", "response": "async def _upload_chunks(\n            cls, rfile: BootResourceFile, content: io.IOBase, chunk_size: int,\n            progress_callback=None):\n        \"\"\"Upload the `content` to `rfile` in chunks using `chunk_size`.\"\"\"\n        content.seek(0, io.SEEK_SET)\n        upload_uri = urlparse(\n            cls._handler.uri)._replace(path=rfile._data['upload_uri']).geturl()\n        uploaded_size = 0\n\n        insecure = cls._handler.session.insecure\n        connector = aiohttp.TCPConnector(verify_ssl=(not insecure))\n        session = aiohttp.ClientSession(connector=connector)\n\n        async with session:\n            while True:\n                buf = content.read(chunk_size)\n                length = len(buf)\n                if length > 0:\n                    uploaded_size += length\n                    await cls._put_chunk(session, upload_uri, buf)\n                    if progress_callback is not None:\n                        progress_callback(uploaded_size / rfile.size)\n                if length != chunk_size:\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload one chunk to upload_uri.", "response": "async def _put_chunk(\n            cls, session: aiohttp.ClientSession,\n            upload_uri: str, buf: bytes):\n        \"\"\"Upload one chunk to `upload_uri`.\"\"\"\n        # Build the correct headers.\n        headers = {\n            'Content-Type': 'application/octet-stream',\n            'Content-Length': '%s' % len(buf),\n        }\n        credentials = cls._handler.session.credentials\n        if credentials is not None:\n            utils.sign(upload_uri, headers, credentials)\n\n        # Perform upload of chunk.\n        async with await session.put(\n                upload_uri, data=buf, headers=headers) as response:\n            if response.status != 200:\n                content = await response.read()\n                request = {\n                    \"body\": buf,\n                    \"headers\": headers,\n                    \"method\": \"PUT\",\n                    \"uri\": upload_uri,\n                }\n                raise CallError(request, response, content, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget BootResource by id.", "response": "async def read(cls, id: int):\n        \"\"\"Get `BootResource` by `id`.\"\"\"\n        data = await cls._handler.read(id=id)\n        return cls(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget Bcache by id.", "response": "async def read(cls, node, id):\n        \"\"\"Get `Bcache` by `id`.\"\"\"\n        if isinstance(node, str):\n            system_id = node\n        elif isinstance(node, Node):\n            system_id = node.system_id\n        else:\n            raise TypeError(\n                \"node must be a Node or str, not %s\"\n                % type(node).__name__)\n        return cls(await cls._handler.read(system_id=system_id, id=id))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def read(cls, node):\n        if isinstance(node, str):\n            system_id = node\n        elif isinstance(node, Node):\n            system_id = node.system_id\n        else:\n            raise TypeError(\n                \"node must be a Node or str, not %s\"\n                % type(node).__name__)\n        data = await cls._handler.read(system_id=system_id)\n        return cls(\n            cls._object(\n                item, local_data={\"node_system_id\": system_id})\n            for item in data)", "response": "Get list of Bcache s for node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def create(\n            cls, node: Union[Node, str],\n            name: str,\n            backing_device: Union[BlockDevice, Partition],\n            cache_set: Union[BcacheCacheSet, int],\n            cache_mode: CacheMode, *,\n            uuid: str = None):\n        \"\"\"\n        Create a Bcache on a Node.\n\n        :param node: Node to create the interface on.\n        :type node: `Node` or `str`\n        :param name: Name of the Bcache.\n        :type name: `str`\n        :param backing_device: Either a block device or partition to create\n            the Bcache from.\n        :type backing_device: `BlockDevice` or `Partition`\n        :param cache_set: Bcache cache set to use in front of backing device.\n        :type cache_set: `BcacheCacheSet` or `int`\n        :param cache_mode: Caching mode to use for this device.\n        :type cache_mode: `CacheMode`\n        :type backing_device: `BlockDevice` or `Partition`\n        :param uuid: The UUID for the Bcache (optional).\n        :type uuid: `str`\n        \"\"\"\n        params = {\n            'name': name,\n        }\n        if isinstance(node, str):\n            params['system_id'] = node\n        elif isinstance(node, Node):\n            params['system_id'] = node.system_id\n        else:\n            raise TypeError(\n                'node must be a Node or str, not %s' % (\n                    type(node).__name__))\n\n        if isinstance(backing_device, BlockDevice):\n            params['backing_device'] = backing_device.id\n        elif isinstance(backing_device, Partition):\n            params['backing_partition'] = backing_device.id\n        else:\n            raise TypeError(\n                \"backing_device must be a BlockDevice or Partition, \"\n                \"not %s\" % type(backing_device).__name__)\n\n        if isinstance(cache_set, BcacheCacheSet):\n            params['cache_set'] = cache_set.id\n        elif isinstance(cache_set, int):\n            params['cache_set'] = cache_set\n        else:\n            raise TypeError(\n                \"cache_set must be a BcacheCacheSet or int, \"\n                \"not %s\" % type(cache_set).__name__)\n\n        if isinstance(cache_mode, CacheMode):\n            params['cache_mode'] = cache_mode.value\n        else:\n            raise TypeError(\n                \"cache_mode must be a CacheMode, \"\n                \"not %s\" % type(cache_mode).__name__)\n\n        if uuid is not None:\n            params['uuid'] = uuid\n        return cls._object(await cls._handler.create(**params))", "response": "Create a Bcache on a Node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the correct value for a fabric from arg.", "response": "def get_param_arg(param, idx, klass, arg, attr='id'):\n    \"\"\"Return the correct value for a fabric from `arg`.\"\"\"\n    if isinstance(arg, klass):\n        return getattr(arg, attr)\n    elif isinstance(arg, (int, str)):\n        return arg\n    else:\n        raise TypeError(\n            \"%s[%d] must be int, str, or %s, not %s\" % (\n                param, idx, klass.__name__, type(arg).__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new entry in the cache.", "response": "async def create(\n            cls, architecture: str, mac_addresses: typing.Sequence[str],\n            power_type: str,\n            power_parameters: typing.Mapping[str, typing.Any] = None, *,\n            subarchitecture: str = None, min_hwe_kernel: str = None,\n            hostname: str = None, domain: typing.Union[int, str] = None):\n        \"\"\"\n        Create a new machine.\n\n        :param architecture: The architecture of the machine (required).\n        :type architecture: `str`\n        :param mac_addresses: The MAC address of the machine (required).\n        :type mac_addresses: sequence of `str`\n        :param power_type: The power type of the machine (required).\n        :type power_type: `str`\n        :param power_parameters: The power parameters for the machine\n            (optional).\n        :type power_parameters: mapping of `str` to any value.\n        :param subarchitecture: The subarchitecture of the machine (optional).\n        :type subarchitecture: `str`\n        :param min_hwe_kernel: The minimal HWE kernel for the machine\n            (optional).\n        :param hostname: The hostname for the machine (optional).\n        :type hostname: `str`\n        :param domain: The domain for the machine (optional).\n        :type domain: `int` or `str`\n        \"\"\"\n        params = {\n            \"architecture\": architecture,\n            \"mac_addresses\": mac_addresses,\n            \"power_type\": power_type,\n        }\n        if power_parameters is not None:\n            params[\"power_parameters\"] = json.dumps(\n                power_parameters, sort_keys=True)\n        if subarchitecture is not None:\n            params[\"subarchitecture\"] = subarchitecture\n        if min_hwe_kernel is not None:\n            params[\"min_hwe_kernel\"] = min_hwe_kernel\n        if hostname is not None:\n            params[\"hostname\"] = hostname\n        if domain is not None:\n            params[\"domain\"] = domain\n        return cls._object(await cls._handler.create(**params))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nallocates a new instance of the class.", "response": "async def allocate(\n            cls, *,\n            hostname: str = None,\n            architectures: typing.Sequence[str] = None,\n            cpus: int = None,\n            fabrics: typing.Sequence[FabricParam] = None,\n            interfaces: typing.Sequence[InterfaceParam] = None,\n            memory: float = None,\n            pod: typing.Union[str, Pod] = None,\n            not_pod: typing.Union[str, Pod] = None,\n            pod_type: str = None,\n            not_pod_type: str = None,\n            storage: typing.Sequence[str] = None,\n            subnets: typing.Sequence[SubnetParam] = None,\n            tags: typing.Sequence[str] = None,\n            zone: typing.Union[str, Zone] = None,\n            not_fabrics: typing.Sequence[FabricParam] = None,\n            not_subnets: typing.Sequence[SubnetParam] = None,\n            not_tags: typing.Sequence[str] = None,\n            not_zones: typing.Sequence[ZoneParam] = None,\n            agent_name: str = None, comment: str = None,\n            bridge_all: bool = None, bridge_stp: bool = None,\n            bridge_fd: int = None, dry_run: bool = None, verbose: bool = None):\n        \"\"\"\n        Allocate a machine.\n\n        :param hostname: The hostname to match.\n        :type hostname: `str`\n        :param architectures: The architecture(s) to match.\n        :type architectures: sequence of `str`\n        :param cpus: The minimum number of CPUs to match.\n        :type cpus: `int`\n        :param fabrics: The connected fabrics to match.\n        :type fabrics: sequence of either `str`, `int`, or `Fabric`\n        :param interfaces: The interfaces to match.\n        :type interfaces: sequence of either `str`, `int`, or `Interface`\n        :param memory: The minimum amount of RAM to match in MiB.\n        :type memory: `int`\n        :param pod: The pod to allocate the machine from.\n        :type pod: `str`\n        :param not_pod: Pod the machine must not be located in.\n        :type not_pod: `str`\n        :param pod_type: The type of pod to allocate the machine from.\n        :type pod_type: `str`\n        :param not_pod_type: Pod type the machine must not be located in.\n        :type not_pod_type: `str`\n        :param subnets: The subnet(s) the desired machine must be linked to.\n        :type subnets: sequence of `str` or `int` or `Subnet`\n        :param storage: The storage contraint to match.\n        :type storage: `str`\n        :param tags: The tags to match, as a sequence.\n        :type tags: sequence of `str`\n        :param zone: The zone the desired machine must belong to.\n        :type zone: `str` or `Zone`\n        :param not_fabrics: The fabrics the machine must NOT be connected to.\n        :type not_fabrics: sequence of either `str`, `int`, or `Fabric`\n        :param not_subnets: The subnet(s) the desired machine must NOT be\n            linked to.\n        :type not_subnets: sequence of `str` or `int` or `Subnet`\n        :param not_zones: The zone(s) the desired machine must NOT in.\n        :type not_zones: sequence of `str` or `Zone`\n        :param agent_name: Agent name to attach to the acquire machine.\n        :type agent_name: `str`\n        :param comment: Comment for the allocate event placed on machine.\n        :type comment: `str`\n        :param bridge_all: Automatically create a bridge on all interfaces\n            on the allocated machine.\n        :type bridge_all: `bool`\n        :param bridge_stp: Turn spaning tree protocol on or off for the\n            bridges created with bridge_all.\n        :type bridge_stp: `bool`\n        :param bridge_fd: Set the forward delay in seconds on the bridges\n            created with bridge_all.\n        :type bridge_fd: `int`\n        :param dry_run: Don't actually acquire the machine just return the\n            machine that would have been acquired.\n        :type dry_run: `bool`\n        :param verbose: Indicate that the user would like additional verbosity\n            in the constraints_by_type field (each constraint will be prefixed\n            by `verbose_`, and contain the full data structure that indicates\n            which machine(s) matched).\n        :type verbose: `bool`\n        \"\"\"\n        params = remove_None({\n            'name': hostname,\n            'arch': architectures,\n            'cpu_count': str(cpus) if cpus else None,\n            'mem': str(memory) if memory else None,\n            'pod_type': pod_type,\n            'not_pod_type': not_pod_type,\n            'storage': storage,\n            'tags': tags,\n            'not_tags': not_tags,\n            'agent_name': agent_name,\n            'comment': comment,\n            'bridge_all': bridge_all,\n            'bridge_stp': bridge_stp,\n            'bridge_fd': bridge_fd,\n            'dry_run': dry_run,\n            'verbose': verbose,\n        })\n        if fabrics is not None:\n            params[\"fabrics\"] = [\n                get_param_arg('fabrics', idx, Fabric, fabric)\n                for idx, fabric in enumerate(fabrics)\n            ]\n        if interfaces is not None:\n            params[\"interfaces\"] = [\n                get_param_arg('interfaces', idx, Interface, nic)\n                for idx, nic in enumerate(interfaces)\n            ]\n        if pod is not None:\n            if isinstance(pod, Pod):\n                params[\"pod\"] = pod.name\n            elif isinstance(pod, str):\n                params[\"pod\"] = pod\n            else:\n                raise TypeError(\n                    \"pod must be a str or Pod, not %s\" % type(pod).__name__)\n        if not_pod is not None:\n            if isinstance(not_pod, Pod):\n                params[\"not_pod\"] = not_pod.name\n            elif isinstance(not_pod, str):\n                params[\"not_pod\"] = not_pod\n            else:\n                raise TypeError(\n                    \"not_pod must be a str or Pod, not %s\" %\n                    type(not_pod).__name__)\n        if subnets is not None:\n            params[\"subnets\"] = [\n                get_param_arg('subnets', idx, Subnet, subnet)\n                for idx, subnet in enumerate(subnets)\n            ]\n        if zone is not None:\n            if isinstance(zone, Zone):\n                params[\"zone\"] = zone.name\n            elif isinstance(zone, str):\n                params[\"zone\"] = zone\n            else:\n                raise TypeError(\n                    \"zone must be a str or Zone, not %s\" % type(zone).__name__)\n        if not_fabrics is not None:\n            params[\"not_fabrics\"] = [\n                get_param_arg('not_fabrics', idx, Fabric, fabric)\n                for idx, fabric in enumerate(not_fabrics)\n            ]\n        if not_subnets is not None:\n            params[\"not_subnets\"] = [\n                get_param_arg('not_subnets', idx, Subnet, subnet)\n                for idx, subnet in enumerate(not_subnets)\n            ]\n        if not_zones is not None:\n            params[\"not_in_zones\"] = [\n                get_param_arg('not_zones', idx, Zone, zone, attr='name')\n                for idx, zone in enumerate(not_zones)\n            ]\n        try:\n            data = await cls._handler.allocate(**params)\n        except CallError as error:\n            if error.status == HTTPStatus.CONFLICT:\n                message = \"No machine matching the given criteria was found.\"\n                raise MachineNotFound(message) from error\n            else:\n                raise\n        else:\n            return cls._object(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_power_parameters_for(\n            cls, system_ids: typing.Sequence[str]):\n        \"\"\"\n        Get a list of power parameters for specified systems.\n        *WARNING*: This method is considered 'alpha' and may be modified\n        in future.\n\n        :param system_ids: The system IDs to get power parameters for\n        \"\"\"\n        if len(system_ids) == 0:\n            return {}\n        data = await cls._handler.power_parameters(id=system_ids)\n        return data", "response": "Get a list of power parameters for the specified systems."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the machine in MAAS.", "response": "async def save(self):\n        \"\"\"Save the machine in MAAS.\"\"\"\n        orig_owner_data = self._orig_data['owner_data']\n        new_owner_data = dict(self._data['owner_data'])\n        self._changed_data.pop('owner_data', None)\n        await super(Machine, self).save()\n        params_diff = calculate_dict_diff(orig_owner_data, new_owner_data)\n        if len(params_diff) > 0:\n            params_diff['system_id'] = self.system_id\n            await self._handler.set_owner_data(**params_diff)\n            self._data['owner_data'] = self._data['owner_data']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def abort(self, *, comment: str = None):\n        params = {\n            \"system_id\": self.system_id\n        }\n        if comment:\n            params[\"comment\"] = comment\n        self._data = await self._handler.abort(**params)\n        return self", "response": "Abort the current action."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncommission this MAAS machine.", "response": "async def commission(\n            self, *, enable_ssh: bool = None, skip_networking: bool = None,\n            skip_storage: bool = None,\n            commissioning_scripts: typing.Sequence[str] = None,\n            testing_scripts: typing.Sequence[str] = None,\n            wait: bool = False, wait_interval: int = 5):\n        \"\"\"Commission this machine.\n\n        :param enable_ssh: Prevent the machine from powering off after running\n            commissioning scripts and enable your user to SSH into the machine.\n        :type enable_ssh: `bool`\n        :param skip_networking: Skip updating the MAAS interfaces for the\n            machine.\n        :type skip_networking: `bool`\n        :param skip_storage: Skip update the MAAS block devices for the\n            machine.\n        :type skip_storage: `bool`\n        :param commissioning_scripts: List of extra commisisoning scripts\n            to run. If the name of the commissioning scripts match a tag, then\n            all commissioning scripts with that tag will be used.\n        :type commissioning_scripts: sequence of `str`\n        :param testing_scripts: List of testing scripts to run after\n            commissioning. By default a small set of testing scripts will run\n            by default. Passing empty list will disable running any testing\n            scripts during commissioning. If the name of the testing scripts\n            match a tag, then all testing scripts with that tag will be used.\n        :type testing_scripts: sequence of `str`\n        :param wait: If specified, wait until the commissioning is complete.\n        :param wait_interval: How often to poll, defaults to 5 seconds\n        \"\"\"\n        params = {\"system_id\": self.system_id}\n        if enable_ssh is not None:\n            params[\"enable_ssh\"] = enable_ssh\n        if skip_networking is not None:\n            params[\"skip_networking\"] = skip_networking\n        if skip_storage is not None:\n            params[\"skip_storage\"] = skip_storage\n        if (commissioning_scripts is not None and\n                len(commissioning_scripts) > 0):\n            params[\"commissioning_scripts\"] = \",\".join(commissioning_scripts)\n        if testing_scripts is not None:\n            if len(testing_scripts) == 0 or testing_scripts == \"none\":\n                params[\"testing_scripts\"] = [\"none\"]\n            else:\n                params[\"testing_scripts\"] = \",\".join(testing_scripts)\n        self._data = await self._handler.commission(**params)\n        if not wait:\n            return self\n        else:\n            # Wait for the machine to be fully commissioned.\n            while self.status in [\n                    NodeStatus.COMMISSIONING, NodeStatus.TESTING]:\n                await asyncio.sleep(wait_interval)\n                self._data = await self._handler.read(system_id=self.system_id)\n            if self.status == NodeStatus.FAILED_COMMISSIONING:\n                msg = \"{hostname} failed to commission.\".format(\n                    hostname=self.hostname)\n                raise FailedCommissioning(msg, self)\n            if self.status == NodeStatus.FAILED_TESTING:\n                msg = \"{hostname} failed testing.\".format(\n                    hostname=self.hostname)\n                raise FailedTesting(msg, self)\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def deploy(\n            self, *, user_data: typing.Union[bytes, str] = None,\n            distro_series: str = None, hwe_kernel: str = None,\n            comment: str = None, wait: bool = False, wait_interval: int = 5):\n        \"\"\"Deploy this machine.\n\n        :param user_data: User-data to provide to the machine when booting. If\n            provided as a byte string, it will be base-64 encoded prior to\n            transmission. If provided as a Unicode string it will be assumed\n            to be already base-64 encoded.\n        :param distro_series: The OS to deploy.\n        :param hwe_kernel: The HWE kernel to deploy. Probably only relevant\n            when deploying Ubuntu.\n        :param comment: A comment for the event log.\n        :param wait: If specified, wait until the deploy is complete.\n        :param wait_interval: How often to poll, defaults to 5 seconds\n        \"\"\"\n        params = {\"system_id\": self.system_id}\n        if user_data is not None:\n            if isinstance(user_data, bytes):\n                params[\"user_data\"] = base64.encodebytes(user_data)\n            else:\n                # Already base-64 encoded. Convert to a byte string in\n                # preparation for multipart assembly.\n                params[\"user_data\"] = user_data.encode(\"ascii\")\n        if distro_series is not None:\n            params[\"distro_series\"] = distro_series\n        if hwe_kernel is not None:\n            params[\"hwe_kernel\"] = hwe_kernel\n        if comment is not None:\n            params[\"comment\"] = comment\n        self._data = await self._handler.deploy(**params)\n        if not wait:\n            return self\n        else:\n            # Wait for the machine to be fully deployed\n            while self.status == NodeStatus.DEPLOYING:\n                await asyncio.sleep(wait_interval)\n                self._data = await self._handler.read(system_id=self.system_id)\n            if self.status == NodeStatus.FAILED_DEPLOYMENT:\n                msg = \"{hostname} failed to deploy.\".format(\n                    hostname=self.hostname\n                )\n                raise FailedDeployment(msg, self)\n            return self", "response": "Deploy this node to the target system."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend this machine into rescue mode.", "response": "async def enter_rescue_mode(\n            self, wait: bool = False, wait_interval: int = 5):\n        \"\"\"\n        Send this machine into 'rescue mode'.\n\n        :param wait: If specified, wait until the deploy is complete.\n        :param wait_interval: How often to poll, defaults to 5 seconds\n        \"\"\"\n        try:\n            self._data = await self._handler.rescue_mode(\n                system_id=self.system_id)\n        except CallError as error:\n            if error.status == HTTPStatus.FORBIDDEN:\n                message = \"Not allowed to enter rescue mode\"\n                raise OperationNotAllowed(message) from error\n            else:\n                raise\n\n        if not wait:\n            return self\n        else:\n            # Wait for machine to finish entering rescue mode\n            while self.status == NodeStatus.ENTERING_RESCUE_MODE:\n                await asyncio.sleep(wait)\n                self._data = await self._handler.read(system_id=self.system_id)\n            if self.status == NodeStatus.FAILED_ENTERING_RESCUE_MODE:\n                msg = \"{hostname} failed to enter rescue mode.\".format(\n                    hostname=self.hostname\n                )\n                raise RescueModeFailure(msg, self)\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def exit_rescue_mode(\n            self, wait: bool = False, wait_interval: int = 5):\n        \"\"\"\n        Exit rescue mode.\n\n        :param wait: If specified, wait until the deploy is complete.\n        :param wait_interval: How often to poll, defaults to 5 seconds\n        \"\"\"\n        try:\n            self._data = await self._handler.exit_rescue_mode(\n                system_id=self.system_id\n            )\n        except CallError as error:\n            if error.status == HTTPStatus.FORBIDDEN:\n                message = \"Not allowed to exit rescue mode.\"\n                raise OperationNotAllowed(message) from error\n            else:\n                raise\n        if not wait:\n            return self\n        else:\n            # Wait for machine to finish exiting rescue mode\n            while self.status == NodeStatus.EXITING_RESCUE_MODE:\n                await asyncio.sleep(wait_interval)\n                self._data = await self._handler.read(system_id=self.system_id)\n            if self.status == NodeStatus.FAILED_EXITING_RESCUE_MODE:\n                msg = \"{hostname} failed to exit rescue mode.\".format(\n                    hostname=self.hostname\n                )\n                raise RescueModeFailure(msg, self)\n            return self", "response": "Exit rescue mode.\n\n        :param wait: If specified, wait until the deploy is complete.\n        :param wait_interval: How often to poll, defaults to 5 seconds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def get_details(self):\n        data = await self._handler.details(system_id=self.system_id)\n        return bson.decode_all(data)[0]", "response": "Get details of the machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def mark_broken(self, *, comment: str = None):\n        params = {\n            \"system_id\": self.system_id\n        }\n        if comment:\n            params[\"comment\"] = comment\n        self._data = await self._handler.mark_broken(**params)\n        return self", "response": "Mark the current state of the user as broken."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmarking the system as fixed.", "response": "async def mark_fixed(self, *, comment: str = None):\n        \"\"\"Mark fixes.\n\n        :param comment: Reason machine is fixed.\n        :type comment: `str`\n        \"\"\"\n        params = {\n            \"system_id\": self.system_id\n        }\n        if comment:\n            params[\"comment\"] = comment\n        self._data = await self._handler.mark_fixed(**params)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def release(\n            self, *, comment: str = None, erase: bool = None,\n            secure_erase: bool = None, quick_erase: bool = None,\n            wait: bool = False, wait_interval: int = 5):\n        \"\"\"\n        Release the machine.\n\n        :param comment: Reason machine was released.\n        :type comment: `str`\n        :param erase: Erase the disk when release.\n        :type erase: `bool`\n        :param secure_erase: Use the drive's secure erase feature if available.\n        :type secure_erase: `bool`\n        :param quick_erase: Wipe the just the beginning and end of the disk.\n            This is not secure.\n        :param wait: If specified, wait until the deploy is complete.\n        :type wait: `bool`\n        :param wait_interval: How often to poll, defaults to 5 seconds.\n        :type wait_interval: `int`\n        \"\"\"\n        params = remove_None({\n            \"system_id\": self.system_id,\n            \"comment\": comment,\n            \"erase\": erase,\n            \"secure_erase\": secure_erase,\n            \"quick_erase\": quick_erase,\n        })\n        self._data = await self._handler.release(**params)\n        if not wait:\n            return self\n        else:\n            # Wait for machine to be released\n            while self.status in [\n                    NodeStatus.RELEASING, NodeStatus.DISK_ERASING]:\n                await asyncio.sleep(wait_interval)\n                try:\n                    self._data = await self._handler.read(\n                        system_id=self.system_id)\n                except CallError as error:\n                    if error.status == HTTPStatus.NOT_FOUND:\n                        # Release must have been on a machine in a pod. This\n                        # machine no longer exists. Just return the machine\n                        # as it has been released.\n                        return self\n                    else:\n                        raise\n            if self.status == NodeStatus.FAILED_RELEASING:\n                msg = \"{hostname} failed to be released.\".format(\n                    hostname=self.hostname\n                )\n                raise FailedReleasing(msg, self)\n            elif self.status == NodeStatus.FAILED_DISK_ERASING:\n                msg = \"{hostname} failed to erase disk.\".format(\n                    hostname=self.hostname\n                )\n                raise FailedDiskErasing(msg, self)\n            return self", "response": "Release the machine.\n\n        :param comment: Reason machine was released.\n        :type comment: `str`\n        :param erase: Erase the disk when release.\n        :type erase: `bool`\n        :param secure_erase: Use the drive's secure erase feature if available.\n        :type secure_erase: `bool`\n        :param quick_erase: Wipe the just the beginning and end of the disk.\n            This is not secure.\n        :param wait: If specified, wait until the deploy is complete.\n        :type wait: `bool`\n        :param wait_interval: How often to poll, defaults to 5 seconds.\n        :type wait_interval: `int`"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npower on the object.", "response": "async def power_on(\n            self, comment: str = None,\n            wait: bool = False, wait_interval: int = 5):\n        \"\"\"\n        Power on.\n\n        :param comment: Reason machine was powered on.\n        :type comment: `str`\n        :param wait: If specified, wait until the machine is powered on.\n        :type wait: `bool`\n        :param wait_interval: How often to poll, defaults to 5 seconds.\n        :type wait_interval: `int`\n        \"\"\"\n        params = {\"system_id\": self.system_id}\n        if comment is not None:\n            params[\"comment\"] = comment\n        try:\n            self._data = await self._handler.power_on(**params)\n        except CallError as error:\n            if error.status == HTTPStatus.FORBIDDEN:\n                message = \"Not allowed to power on machine.\"\n                raise OperationNotAllowed(message) from error\n            else:\n                raise\n        if not wait or self.power_state == PowerState.UNKNOWN:\n            # Don't wait for a machine that always shows power state as\n            # unknown as the driver cannot query the power state.\n            return self\n        else:\n            # Wait for machine to be powered on.\n            while self.power_state == PowerState.OFF:\n                await asyncio.sleep(wait_interval)\n                self._data = await self._handler.read(system_id=self.system_id)\n            if self.power_state == PowerState.ERROR:\n                msg = \"{hostname} failed to power on.\".format(\n                    hostname=self.hostname\n                )\n                raise PowerError(msg, self)\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npower off the specified resource.", "response": "async def power_off(\n            self, stop_mode: PowerStopMode = PowerStopMode.HARD,\n            comment: str = None, wait: bool = False, wait_interval: int = 5):\n        \"\"\"\n        Power off.\n\n        :param stop_mode: How to perform the power off.\n        :type stop_mode: `PowerStopMode`\n        :param comment: Reason machine was powered on.\n        :type comment: `str`\n        :param wait: If specified, wait until the machine is powered on.\n        :type wait: `bool`\n        :param wait_interval: How often to poll, defaults to 5 seconds.\n        :type wait_interval: `int`\n        \"\"\"\n        params = {\"system_id\": self.system_id, 'stop_mode': stop_mode.value}\n        if comment is not None:\n            params[\"comment\"] = comment\n        try:\n            self._data = await self._handler.power_off(**params)\n        except CallError as error:\n            if error.status == HTTPStatus.FORBIDDEN:\n                message = \"Not allowed to power off machine.\"\n                raise OperationNotAllowed(message) from error\n            else:\n                raise\n        if not wait or self.power_state == PowerState.UNKNOWN:\n            # Don't wait for a machine that always shows power state as\n            # unknown as the driver cannot query the power state.\n            return self\n        else:\n            # Wait for machine to be powered off.\n            while self.power_state == PowerState.ON:\n                await asyncio.sleep(wait_interval)\n                self._data = await self._handler.read(system_id=self.system_id)\n            if self.power_state == PowerState.ERROR:\n                msg = \"{hostname} failed to power off.\".format(\n                    hostname=self.hostname\n                )\n                raise PowerError(msg, self)\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def query_power_state(self):\n        power_data = await self._handler.query_power_state(\n            system_id=self.system_id)\n        # Update the internal state of this object as well, since we have the\n        # updated power state from the BMC directly. MAAS server does this as\n        # well, just do it client side to make it nice for a developer.\n        self._data['power_state'] = power_data['state']\n        return PowerState(power_data['state'])", "response": "Query the machine s BMC for the current power state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def restore_default_configuration(self):\n        self._data = await self._handler.restore_default_configuration(\n            system_id=self.system_id)", "response": "Restore machine s default configuration to its initial state."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrestores machine s networking configuration to its initial state.", "response": "async def restore_networking_configuration(self):\n        \"\"\"\n        Restore machine's networking configuration to its initial state.\n        \"\"\"\n        self._data = await self._handler.restore_networking_configuration(\n            system_id=self.system_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def restore_storage_configuration(self):\n        self._data = await self._handler.restore_storage_configuration(\n            system_id=self.system_id)", "response": "Restore machine s storage configuration to its initial state."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def create(\n            cls, *, name: str = None, description: str = None):\n        \"\"\"\n        Create a `Space` in MAAS.\n\n        :param name: The name of the `Space` (optional, will be given a\n        default value if not specified).\n        :type name: `str`\n        :param description: A description of the `Space` (optional).\n        :type description: `str`\n        :returns: The created Space\n        :rtype: `Space`\n        \"\"\"\n        params = {}\n        if name is not None:\n            params[\"name\"] = name\n        if description is not None:\n            params[\"description\"] = description\n        return cls._object(await cls._handler.create(**params))", "response": "Create a new Space in MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_default(cls):\n        data = await cls._handler.read(id=cls._default_space_id)\n        return cls(data)", "response": "Get the default MAAS Space for the MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def read(cls, fabric: Union[Fabric, int], vid: int):\n        if isinstance(fabric, int):\n            fabric_id = fabric\n        elif isinstance(fabric, Fabric):\n            fabric_id = fabric.id\n        else:\n            raise TypeError(\n                \"fabric must be a Fabric or int, not %s\"\n                % type(fabric).__name__)\n        data = await cls._handler.read(fabric_id=fabric_id, vid=vid)\n        return cls(data, {\"fabric_id\": fabric_id})", "response": "Get the VLAN by vid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget list of VLANs for fabric.", "response": "async def read(cls, fabric: Union[Fabric, int]):\n        \"\"\"Get list of `Vlan`'s for `fabric`.\n\n        :param fabric: Fabric to get all VLAN's for.\n        :type fabric: `Fabric` or `int`\n        \"\"\"\n        if isinstance(fabric, int):\n            fabric_id = fabric\n        elif isinstance(fabric, Fabric):\n            fabric_id = fabric.id\n        else:\n            raise TypeError(\n                \"fabric must be a Fabric or int, not %s\"\n                % type(fabric).__name__)\n        data = await cls._handler.read(fabric_id=fabric_id)\n        return cls(\n            cls._object(\n                item, local_data={\"fabric_id\": fabric_id})\n            for item in data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def create(\n            cls, fabric: Union[Fabric, int], vid: int, *,\n            name: str = None, description: str = None, mtu: int = None,\n            relay_vlan: Union[Vlan, int] = None, dhcp_on: bool = False,\n            primary_rack: Union[RackController, str] = None,\n            secondary_rack: Union[RackController, str] = None,\n            space: Union[Space, int] = None):\n        \"\"\"\n        Create a `Vlan` in MAAS.\n\n        :param fabric: Fabric to create the VLAN on.\n        :type fabric: `Fabric` or `int`\n        :param vid: VID for the VLAN.\n        :type vid: `int`\n        :param name: The name of the VLAN (optional).\n        :type name: `str`\n        :param description: A description of the VLAN (optional).\n        :type description: `str`\n        :param mtu: The MTU for VLAN (optional, default of 1500 will be used).\n        :type mtu: `int`\n        :param relay_vlan: VLAN to relay this VLAN through.\n        :type relay_vlan: `Vlan` or `int`\n        :param dhcp_on: True turns the DHCP on, false keeps the DHCP off. True\n            requires that `primary_rack` is also set.\n        :type dhcp_on: `bool`\n        :param primary_rack: Primary rack controller to run the DCHP\n            service on.\n        :type primary_rack: `RackController` or `int`\n        :parma secondary_rack: Secondary rack controller to run the DHCP\n            service on. This will enable HA operation of the DHCP service.\n        :type secondary_rack: `RackController` or `int`\n        :returns: The created VLAN.\n        :rtype: `Vlan`\n        \"\"\"\n        params = {}\n        if isinstance(fabric, int):\n            params['fabric_id'] = fabric\n        elif isinstance(fabric, Fabric):\n            params['fabric_id'] = fabric.id\n        else:\n            raise TypeError(\n                \"fabric must be Fabric or int, not %s\" % (\n                    type(fabric).__class__))\n        params['vid'] = vid\n        if name is not None:\n            params['name'] = name\n        if description is not None:\n            params['description'] = description\n        if mtu is not None:\n            params['mtu'] = mtu\n        if relay_vlan is not None:\n            if isinstance(relay_vlan, int):\n                params['relay_vlan'] = relay_vlan\n            elif isinstance(relay_vlan, Vlan):\n                params['relay_vlan'] = relay_vlan.id\n            else:\n                raise TypeError(\n                    \"relay_vlan must be Vlan or int, not %s\" % (\n                        type(relay_vlan).__class__))\n        params['dhcp_on'] = dhcp_on\n        if primary_rack is not None:\n            if isinstance(primary_rack, str):\n                params['primary_rack'] = primary_rack\n            elif isinstance(primary_rack, RackController):\n                params['primary_rack'] = primary_rack.system_id\n            else:\n                raise TypeError(\n                    \"primary_rack must be RackController or str, not %s\" % (\n                        type(primary_rack).__class__))\n        if secondary_rack is not None:\n            if isinstance(secondary_rack, str):\n                params['secondary_rack'] = secondary_rack\n            elif isinstance(secondary_rack, RackController):\n                params['secondary_rack'] = secondary_rack.system_id\n            else:\n                raise TypeError(\n                    \"secondary_rack must be RackController or str, not %s\" % (\n                        type(secondary_rack).__class__))\n        if space is not None:\n            if isinstance(space, int):\n                params['space'] = space\n            elif isinstance(space, Space):\n                params['space'] = space.id\n            else:\n                raise TypeError(\n                    \"space must be Space or int, not %s\" % (\n                        type(space).__class__))\n        return cls._object(await cls._handler.create(**params))", "response": "Create a new VLAN in MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the default VLAN from the set.", "response": "def get_default(self):\n        \"\"\"Return the default VLAN from the set.\"\"\"\n        length = len(self)\n        if length == 0:\n            return None\n        elif length == 1:\n            return self[0]\n        else:\n            return sorted(self, key=attrgetter('id'))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_content_type(*names):\n    for name in names:\n        if name is not None:\n            mimetype, encoding = mimetypes.guess_type(name)\n            if mimetype is not None:\n                if isinstance(mimetype, bytes):\n                    return mimetype.decode(\"ascii\")\n                else:\n                    return mimetype\n    else:\n        return \"application/octet-stream\"", "response": "Return the MIME content type for the given file name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_payloads(name, content):\n    if content is None:\n        yield make_bytes_payload(name, b\"\")\n    elif isinstance(content, bool):\n        if content:\n            yield make_bytes_payload(name, b\"true\")\n        else:\n            yield make_bytes_payload(name, b\"false\")\n    elif isinstance(content, int):\n        yield make_bytes_payload(name, b\"%d\" % content)\n    elif isinstance(content, bytes):\n        yield make_bytes_payload(name, content)\n    elif isinstance(content, str):\n        yield make_string_payload(name, content)\n    elif isinstance(content, IOBase):\n        yield make_file_payload(name, content)\n    elif callable(content):\n        with content() as content:\n            for payload in make_payloads(name, content):\n                yield payload\n    elif isinstance(content, Iterable):\n        for part in content:\n            for payload in make_payloads(name, part):\n                yield payload\n    else:\n        raise AssertionError(\n            \"%r is unrecognised: %r\" % (name, content))", "response": "Generates the payloads for the given name and content."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode_multipart_data(data=(), files=()):\n    if isinstance(data, Mapping):\n        data = data.items()\n    if isinstance(files, Mapping):\n        files = files.items()\n    message = build_multipart_message(chain(data, files))\n    headers, body = encode_multipart_message(message)\n    return body, dict(headers)", "response": "Create a MIME multipart payload from data and files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_http_proxy(cls) -> typing.Optional[str]:\n        data = await cls.get_config(\"http_proxy\")\n        return None if data is None or data == \"\" else data", "response": "Proxy for APT and HTTP and HTTP traffic."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget kernel options from config file.", "response": "async def get_kernel_options(cls) -> typing.Optional[str]:\n        \"\"\"Kernel options.\n\n        Boot parameters to pass to the kernel by default.\n        \"\"\"\n        data = await cls.get_config(\"kernel_opts\")\n        return None if data is None or data == \"\" else data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def get_upstream_dns(cls) -> list:\n        data = await cls.get_config(\"upstream_dns\")\n        return [] if data is None else re.split(r'[,\\s]+', data)", "response": "Return the list of upstream DNS servers used to resolve domains not managed by this MAAS instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nenabling DNSSEC validation of upstream zones.", "response": "async def get_dnssec_validation(cls) -> DNSSEC:\n        \"\"\"Enable DNSSEC validation of upstream zones.\n\n        Only used when MAAS is running its own DNS server. This value is used\n        as the value of 'dnssec_validation' in the DNS server config.\n        \"\"\"\n        data = await cls.get_config(\"dnssec_validation\")\n        return cls.DNSSEC.lookup(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_windows_kms_host(cls) -> typing.Optional[str]:\n        data = await cls.get_config(\"windows_kms_host\")\n        return None if data is None or data == \"\" else data", "response": "Windows KMS activation host."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_default_storage_layout(cls) -> StorageLayout:\n        data = await cls.get_config(\"default_storage_layout\")\n        return cls.StorageLayout.lookup(data)", "response": "Default storage layout.\n\n        Storage layout that is applied to a node when it is deployed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def get_default_min_hwe_kernel(cls) -> typing.Optional[str]:\n        data = await cls.get_config(\"default_min_hwe_kernel\")\n        return None if data is None or data == \"\" else data", "response": "Default minimum kernel version."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets a configuration value in MAAS.", "response": "async def set_config(cls, name: str, value):\n        \"\"\"Set a configuration value in MAAS.\n\n        Consult your MAAS server for recognised settings. Alternatively, use\n        the pre-canned functions also defined on this object.\n        \"\"\"\n        return await cls._handler.set_config(name=[name], value=[value])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest helper for getting each value and sets it again.", "response": "async def _roundtrip(cls):\n        \"\"\"Testing helper: gets each value and sets it again.\"\"\"\n        getters = {\n            name[4:]: getattr(cls, name) for name in dir(cls)\n            if name.startswith(\"get_\") and name != \"get_config\"\n        }\n        setters = {\n            name[4:]: getattr(cls, name) for name in dir(cls)\n            if name.startswith(\"set_\") and name != \"set_config\"\n        }\n\n        for name, getter in getters.items():\n            print(\">>>\", name)\n            value = await getter()\n            print(\" ->\", repr(value))\n            print(\"   \", type(value))\n            setter = setters[name]\n            try:\n                await setter(value)\n            except CallError as error:\n                print(error)\n                print(error.content.decode(\"utf-8\", \"replace\"))\n            else:\n                value2 = await getter()\n                if value2 != value:\n                    print(\n                        \"!!! Round-trip failed:\", repr(value),\n                        \"-->\", repr(value2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create(\n            cls, node: Union[Node, str], name: str,\n            devices: Iterable[Union[BlockDevice, Partition]],\n            *, uuid: str = None):\n        \"\"\"\n        Create a volume group on a Node.\n\n        :param node: Node to create the interface on.\n        :type node: `Node` or `str`\n        :param name: Name of the volume group.\n        :type name: `str`\n        :param devices: Mixed list of block devices or partitions to create\n            the volume group from.\n        :type devices: iterable of mixed type of `BlockDevice` or `Partition`\n        :param uuid: The UUID for the volume group (optional).\n        :type uuid: `str`\n        \"\"\"\n        params = {\n            'name': name,\n        }\n        if isinstance(node, str):\n            params['system_id'] = node\n        elif isinstance(node, Node):\n            params['system_id'] = node.system_id\n        else:\n            raise TypeError(\n                'node must be a Node or str, not %s' % (\n                    type(node).__name__))\n\n        if len(devices) == 0:\n            raise ValueError(\"devices must contain at least one device.\")\n\n        block_devices = []\n        partitions = []\n        for idx, device in enumerate(devices):\n            if isinstance(device, BlockDevice):\n                block_devices.append(device.id)\n            elif isinstance(device, Partition):\n                partitions.append(device.id)\n            else:\n                raise TypeError(\n                    \"devices[%d] must be a BlockDevice or \"\n                    \"Partition, not %s\" % type(device).__name__)\n        if len(block_devices) > 0:\n            params['block_devices'] = block_devices\n        if len(partitions) > 0:\n            params['partitions'] = partitions\n        if uuid is not None:\n            params['uuid'] = uuid\n        return cls._object(await cls._handler.create(**params))", "response": "Create a new entry in the cache for the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the index for storing profiles.", "response": "def schema_create(conn):\n    \"\"\"Create the index for storing profiles.\n\n    This is idempotent; it can be called every time a database is opened to\n    make sure it's ready to use and up-to-date.\n\n    :param conn: A connection to an SQLite3 database.\n    \"\"\"\n    conn.execute(dedent(\"\"\"\\\n    CREATE TABLE IF NOT EXISTS profiles\n      (id INTEGER PRIMARY KEY,\n       name TEXT NOT NULL UNIQUE,\n       data BLOB NOT NULL,\n       selected BOOLEAN NOT NULL DEFAULT FALSE)\n    \"\"\"))\n    # Partial indexes are only available in >=3.8.0 and expressions in indexes\n    # are only available in >=3.9.0 (https://www.sqlite.org/partialindex.html\n    # & https://www.sqlite.org/expridx.html). Don't bother with any kind of\n    # index before that because it would complicate upgrades.\n    if sqlite3.sqlite_version_info >= (3, 9, 0):\n        # This index is for data integrity -- ensuring that only one profile\n        # is the default (\"selected\") profile -- and speed a distant second.\n        conn.execute(dedent(\"\"\"\\\n        CREATE UNIQUE INDEX IF NOT EXISTS\n          only_one_profile_selected ON profiles\n          (selected IS NOT NULL) WHERE selected\n        \"\"\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimporting profiles from another database.", "response": "def schema_import(conn, dbpath):\n    \"\"\"Import profiles from another database.\n\n    This does not overwrite existing profiles in the target database. Profiles\n    in the source database that share names with those in the target database\n    are ignored.\n\n    :param conn: A connection to an SQLite3 database into which to copy\n        profiles.\n    :param dbpath: The filesystem path to the source SQLite3 database.\n    \"\"\"\n    conn.execute(\n        \"ATTACH DATABASE ? AS source\", (str(dbpath),))\n    conn.execute(\n        \"INSERT OR IGNORE INTO profiles (name, data)\"\n        \" SELECT name, data FROM source.profiles\"\n        \" WHERE data IS NOT NULL\")\n    conn.commit()  # need to commit before detaching the other db\n    conn.execute(\n        \"DETACH DATABASE source\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace(self, **updates):\n        state = self.dump()\n        state.update(updates)\n        return self.__class__(**state)", "response": "Return a new profile with the given updates."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict of fields that can be used to recreate this profile.", "response": "def dump(self):\n        \"\"\"Return a dict of fields that can be used to recreate this profile.\n\n        For example::\n\n          >>> profile = Profile(name=\"foobar\", ...)\n          >>> profile == Profile(**profile.dump())\n          True\n\n        Use this value when persisting a profile.\n        \"\"\"\n        return dict(\n            self.other, name=self.name, url=self.url,\n            credentials=self.credentials, description=self.description,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the default profile.", "response": "def default(self) -> typing.Optional[Profile]:\n        \"\"\"The name of the default profile to use, or `None`.\"\"\"\n        found = self.database.execute(\n            \"SELECT name, data FROM profiles WHERE selected\"\n            \" ORDER BY name LIMIT 1\").fetchone()\n        if found is None:\n            return None\n        else:\n            state = json.loads(found[1])\n            state[\"name\"] = found[0]  # Belt-n-braces.\n            return Profile(**state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens a database file and return a context manager which opens it.", "response": "def open(cls, dbpath=Path(\"~/.maas.db\").expanduser(),\n             migrate_from=Path(\"~/.maascli.db\").expanduser()):\n        \"\"\"Load a profiles database.\n\n        Called without arguments this will open (and create) a database in the\n        user's home directory.\n\n        **Note** that this returns a context manager which will close the\n        database on exit, saving if the exit is clean.\n\n        :param dbpath: The path to the database file to create and open.\n        :param migrate_from: Path to a database file to migrate from.\n        \"\"\"\n        # Ensure we're working with a Path instance.\n        dbpath = Path(dbpath)\n        migrate_from = Path(migrate_from)\n        # See if we ought to do a one-time migration.\n        migrate = migrate_from.is_file() and not dbpath.exists()\n        # Initialise filename with restrictive permissions...\n        dbpath.touch(mode=0o600, exist_ok=True)\n        # Final check to see if it's safe to migrate.\n        migrate = migrate and not migrate_from.samefile(dbpath)\n        # before opening it with sqlite.\n        database = sqlite3.connect(str(dbpath))\n        try:\n            store = cls(database)\n            if migrate:\n                schema_import(database, migrate_from)\n                yield store\n            else:\n                yield store\n        except:  # noqa: E722\n            raise\n        else:\n            database.commit()\n        finally:\n            database.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_with_pager(output):\n    if sys.stdout.isatty():\n        try:\n            pager = subprocess.Popen(\n                ['less', '-F', '-r', '-S', '-X', '-K'],\n                stdin=subprocess.PIPE, stdout=sys.stdout)\n        except subprocess.CalledProcessError:\n            # Don't use the pager since starting it has failed.\n            print(output)\n            return\n        else:\n            pager.stdin.write(output.encode('utf-8'))\n            pager.stdin.close()\n            pager.wait()\n    else:\n        # Output directly to stdout since not in tty.\n        print(output)", "response": "Print the output to stdout using less when in a tty."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_profile_names_and_default() -> (\n        typing.Tuple[typing.Sequence[str], typing.Optional[Profile]]):\n    \"\"\"Return the list of profile names and the default profile object.\n\n    The list of names is sorted.\n    \"\"\"\n    with ProfileStore.open() as config:\n        return sorted(config), config.default", "response": "Return the list of profile names and the default profile object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and populate an argument parser.", "response": "def prepare_parser(program):\n    \"\"\"Create and populate an argument parser.\"\"\"\n    parser = ArgumentParser(\n        description=PROG_DESCRIPTION, prog=program,\n        formatter_class=HelpFormatter,\n        add_help=False)\n    parser.add_argument(\n        \"-h\", \"--help\", action=MinimalHelpAction, help=argparse.SUPPRESS)\n\n    # Register sub-commands.\n    submodules = (\n        \"nodes\", \"machines\", \"devices\", \"controllers\",\n        \"fabrics\", \"vlans\", \"subnets\", \"spaces\",\n        \"files\", \"tags\", \"users\",\n        \"profiles\", \"shell\",\n    )\n    cmd_help.register(parser)\n    for submodule in submodules:\n        module = import_module(\".\" + submodule, __name__)\n        module.register(parser)\n\n    # Register global options.\n    parser.add_argument(\n        '--debug', action='store_true', default=False,\n        help=argparse.SUPPRESS)\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_mortem(traceback):\n    try:\n        from ipdb import post_mortem\n    except ImportError:\n        from pdb import post_mortem\n\n    message = \"Entering post-mortem debugger. Type `help` for help.\"\n    redline = colorized(\"{autored}%s{/autored}\") % \"{0:=^{1}}\"\n\n    print()\n    print(redline.format(\" CRASH! \", len(message)))\n    print(message)\n    print(redline.format(\"\", len(message)))\n    print()\n\n    post_mortem(traceback)", "response": "Work with an exception in a post - mortem debugger."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nobtaining the subparser s object.", "response": "def subparsers(self):\n        \"\"\"Obtain the subparser's object.\"\"\"\n        try:\n            return self.__subparsers\n        except AttributeError:\n            parent = super(ArgumentParser, self)\n            self.__subparsers = parent.add_subparsers(title=\"drill down\")\n            self.__subparsers.metavar = \"COMMAND\"\n            return self.__subparsers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an argument group or return a pre - existing one.", "response": "def add_argument_group(self, title, description=None):\n        \"\"\"Add an argument group, or return a pre-existing one.\"\"\"\n        try:\n            groups = self.__groups\n        except AttributeError:\n            groups = self.__groups = {}\n\n        if title not in groups:\n            groups[title] = super().add_argument_group(\n                title=title, description=description)\n\n        return groups[title]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the formatted help text.", "response": "def print_minized_help(self, *, no_pager=False):\n        \"\"\"Return the formatted help text.\n\n        Override default ArgumentParser to just include the usage and the\n        description. The `help` action is used for provide more detail.\n        \"\"\"\n        formatter = self._get_formatter()\n        formatter.add_usage(\n            self.usage, self._actions,\n            self._mutually_exclusive_groups)\n        formatter.add_text(self.description)\n        if no_pager:\n            print(formatter.format_help())\n        else:\n            print_with_pager(formatter.format_help())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the preferred name as which this command will be known.", "response": "def name(cls):\n        \"\"\"Return the preferred name as which this command will be known.\"\"\"\n        name = cls.__name__.replace(\"_\", \"-\").lower()\n        name = name[4:] if name.startswith(\"cmd-\") else name\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters this command as a sub - parser of parser.", "response": "def register(cls, parser, name=None):\n        \"\"\"Register this command as a sub-parser of `parser`.\n\n        :type parser: An instance of `ArgumentParser`.\n        :return: The sub-parser created.\n        \"\"\"\n        help_title, help_body = utils.parse_docstring(cls)\n        command_parser = parser.subparsers.add_parser(\n            cls.name() if name is None else name, help=help_title,\n            description=help_title, epilog=help_body, add_help=False,\n            formatter_class=HelpFormatter)\n        command_parser.add_argument(\n            \"-h\", \"--help\", action=PagedHelpAction, help=argparse.SUPPRESS)\n        command_parser.set_defaults(execute=cls(command_parser))\n        return command_parser"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_all_commands(self, *, no_pager=False):\n        formatter = self.parent_parser._get_formatter()\n        command_names = sorted(self.parent_parser.subparsers.choices.keys())\n        max_name_len = max([len(name) for name in command_names]) + 1\n        commands = \"\"\n        for name in command_names:\n            command = self.parent_parser.subparsers.choices[name]\n            extra_padding = max_name_len - len(name)\n            command_line = '%s%s%s' % (\n                name, ' ' * extra_padding, command.description)\n            while len(command_line) > formatter._width:\n                lines = textwrap.wrap(command_line, formatter._width)\n                commands += \"%s\\n\" % lines[0]\n                if len(lines) > 1:\n                    lines[1] = (' ' * max_name_len) + lines[1]\n                    command_line = ' '.join(lines[1:])\n                else:\n                    command_line = None\n            if command_line:\n                commands += \"%s\\n\" % command_line\n        if no_pager:\n            print(commands[:-1])\n        else:\n            print_with_pager(commands[:-1])", "response": "Print help for all commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def query(\n            cls, *,\n            hostnames: typing.Iterable[str] = None,\n            domains: typing.Iterable[str] = None,\n            zones: typing.Iterable[str] = None,\n            macs: typing.Iterable[str] = None,\n            system_ids: typing.Iterable[str] = None,\n            agent_name: str = None,\n            level: typing.Union[Level, int, str] = None,\n            before: int = None,\n            after: int = None,\n            limit: int = None,\n            owner: typing.Union[User, str] = None):\n        \"\"\"Query MAAS for matching events.\"\"\"\n\n        if before is not None and after is not None:\n            raise ValueError(\"Specify either `before` or `after`, not both.\")\n\n        params = {}\n\n        if hostnames is not None:\n            params[\"hostname\"] = list(hostnames)\n        if domains is not None:\n            params[\"domain\"] = list(domains)\n        if zones is not None:\n            params[\"zone\"] = list(zones)\n        if macs is not None:\n            params[\"mac_address\"] = list(macs)\n        if system_ids is not None:\n            params[\"id\"] = list(system_ids)\n        if agent_name is not None:\n            params[\"agent_name\"] = [agent_name]\n        if level is not None:\n            level = Level.normalise(level)\n            params[\"level\"] = [level.name]\n        if before is not None:\n            params[\"before\"] = [\"{:d}\".format(before)]\n        if after is not None:\n            params[\"after\"] = [\"{:d}\".format(after)]\n        if limit is not None:\n            params[\"limit\"] = [\"{:d}\".format(limit)]\n        if owner is not None:\n            if isinstance(owner, User):\n                params[\"owner\"] = [owner.username]\n            elif isinstance(owner, str):\n                params[\"owner\"] = [owner]\n            else:\n                raise TypeError(\n                    \"owner must be either User or str, not %s\" % (\n                        type(owner).__name__))\n\n        data = await cls._handler.query(**params)\n        return cls(data)", "response": "Query MAAS for matching events."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def prev(self, count=None):\n        if self._prev_uri is None:\n            return self[:0]  # An empty slice of `self`.\n        else:\n            return self._fetch(self._prev_uri, count)", "response": "Load the previous page of events."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def next(self, count=None):\n        if self._next_uri is None:\n            return self[:0]  # An empty slice of `self`.\n        else:\n            return self._fetch(self._next_uri, count)", "response": "Load the next page of events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an SSH key in MAAS with the content in key.", "response": "async def create(cls, key: str):\n        \"\"\"\n        Create an SSH key in MAAS with the content in `key`.\n\n        :param key: The content of the SSH key\n        :type key: `str`\n        :returns: The created SSH key\n        :rtype: `SSHKey`\n        \"\"\"\n        return cls._object(await cls._handler.create(key=key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves this block device.", "response": "async def save(self):\n        \"\"\"Save this block device.\"\"\"\n        old_tags = list(self._orig_data['tags'])\n        new_tags = list(self.tags)\n        self._changed_data.pop('tags', None)\n        await super(BlockDevice, self).save()\n        for tag_name in new_tags:\n            if tag_name not in old_tags:\n                await self._handler.add_tag(\n                    system_id=self.node.system_id, id=self.id, tag=tag_name)\n            else:\n                old_tags.remove(tag_name)\n        for tag_name in old_tags:\n            await self._handler.remove_tag(\n                system_id=self.node.system_id, id=self.id, tag=tag_name)\n        self._orig_data['tags'] = new_tags\n        self._data['tags'] = list(new_tags)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting this node as boot disk.", "response": "async def set_as_boot_disk(self):\n        \"\"\"Set as boot disk for this node.\"\"\"\n        await self._handler.set_boot_disk(\n            system_id=self.node.system_id, id=self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting this block device.", "response": "async def format(self, fstype, *, uuid=None):\n        \"\"\"Format this block device.\"\"\"\n        self._data = await self._handler.format(\n            system_id=self.node.system_id, id=self.id,\n            fstype=fstype, uuid=uuid)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def unformat(self):\n        self._data = await self._handler.unformat(\n            system_id=self.node.system_id, id=self.id)", "response": "Unformat this block device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a physical block device on a Node.", "response": "async def create(\n            cls, node: Union[Node, str], name: str,\n            *, model: str = None, serial: str = None,\n            id_path: str = None, size: int = None, block_size: int = 512,\n            tags: Iterable[str] = None):\n        \"\"\"\n        Create a physical block device on a Node.\n\n        Either model and serial or id_path must be provided when creating a\n        `BlockDevice`. Size (bytes) is always required.\n\n        NOTE: It is recommended to use the MAAS commissioning process to\n        discover `BlockDevice`'s on a machine. Getting any of this information\n        incorrect can result on the machine failing to deploy.\n\n        :param node: Node to create the block device on.\n        :type node: `Node` or `str`\n        :param name: The name for the block device.\n        :type name: `str`\n        :param model: The model number for the block device.\n        :type model: `str`\n        :param serial: The serial number for the block device.\n        :type serial: `str`\n        :param id_path: Unique path that identifies the device no matter\n            the kernel the machine boots. Use only when the block device\n            does not have a model and serial number.\n        :type id_path: `str`\n        :param size: The size of the block device in bytes.\n        :type size: `int`\n        :param block_size: The block size of the block device in bytes.\n        :type block_size: `int`\n        :param tags: List of tags to add to the block device.\n        :type tags: sequence of `str`\n        \"\"\"\n        params = {}\n        if isinstance(node, str):\n            params['system_id'] = node\n        elif isinstance(node, Node):\n            params['system_id'] = node.system_id\n        else:\n            raise TypeError(\n                'node must be a Node or str, not %s' % (\n                    type(node).__name__))\n\n        if not size or size < 0:\n            raise ValueError(\"size must be provided and greater than zero.\")\n        if not block_size or block_size < 0:\n            raise ValueError(\n                \"block_size must be provided and greater than zero.\")\n        if model and not serial:\n            raise ValueError(\"serial must be provided when model is provided.\")\n        if not model and serial:\n            raise ValueError(\"model must be provided when serial is provided.\")\n        if not model and not serial and not id_path:\n            raise ValueError(\n                \"Either model/serial is provided or id_path must be provided.\")\n\n        params.update(remove_None({\n            'name': name,\n            'model': model,\n            'serial': serial,\n            'id_path': id_path,\n            'size': size,\n            'block_size': block_size,\n        }))\n        device = cls._object(await cls._handler.create(**params))\n        if tags:\n            device.tags = tags\n            await device.save()\n        return device"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(filename):\n    path = join(here, filename)\n    with open(path, \"r\") as fin:\n        return fin.read().strip()", "response": "Return the whitespace - stripped content of filename."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_subnets(self, vlan):\n        return vlan._origin.Subnets([\n            subnet\n            for subnet in self.subnets\n            if subnet.vlan.id == vlan.id\n        ])", "response": "Return the subnets for the vlan."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def create(cls, url, *, keyring_filename=None, keyring_data=None):\n        data = await cls._handler.create(\n            url=url, keyring_filename=coalesce(keyring_filename, \"\"),\n            keyring_data=coalesce(keyring_data, \"\"))\n        return cls._object(data)", "response": "Create a new BootSource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching the API description from the MAAS instance.", "response": "async def fetch_api_description(\n        url: typing.Union[str, ParseResult, SplitResult],\n        insecure: bool = False):\n    \"\"\"Fetch the API description from the remote MAAS instance.\"\"\"\n    url_describe = urljoin(_ensure_url_string(url), \"describe/\")\n    connector = aiohttp.TCPConnector(verify_ssl=(not insecure))\n    session = aiohttp.ClientSession(connector=connector)\n    async with session, session.get(url_describe) as response:\n        if response.status != HTTPStatus.OK:\n            raise RemoteError(\n                \"{0} -> {1.status} {1.reason}\".format(\n                    url, response))\n        elif response.content_type != \"application/json\":\n            raise RemoteError(\n                \"Expected application/json, got: %s\"\n                % response.content_type)\n        else:\n            return await response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert url to a string URL if it isn t one already.", "response": "def _ensure_url_string(url):\n    \"\"\"Convert `url` to a string URL if it isn't one already.\"\"\"\n    if isinstance(url, str):\n        return url\n    elif isinstance(url, (ParseResult, SplitResult)):\n        return url.geturl()\n    else:\n        raise TypeError(\n            \"Could not convert %r to a string URL.\" % (url,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nderives a stable human - readable name and identifier for a resource.", "response": "def derive_resource_name(name):\n    \"\"\"A stable, human-readable name and identifier for a resource.\"\"\"\n    if name.startswith(\"Anon\"):\n        name = name[4:]\n    if name.endswith(\"Handler\"):\n        name = name[:-7]\n    if name == \"Maas\":\n        name = \"MAAS\"\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to a MAAS instance with apikey.", "response": "async def connect(url, *, apikey=None, insecure=False):\n    \"\"\"Connect to a remote MAAS instance with `apikey`.\n\n    Returns a new :class:`Profile` which has NOT been saved. To connect AND\n    save a new profile::\n\n        profile = connect(url, apikey=apikey)\n        profile = profile.replace(name=\"mad-hatter\")\n\n        with profiles.ProfileStore.open() as config:\n            config.save(profile)\n            # Optionally, set it as the default.\n            config.default = profile.name\n\n    \"\"\"\n    url = api_url(url)\n    url = urlparse(url)\n\n    if url.username is not None:\n        raise ConnectError(\n            \"Cannot provide user-name explicitly in URL (%r) when connecting; \"\n            \"use login instead.\" % url.username)\n    if url.password is not None:\n        raise ConnectError(\n            \"Cannot provide password explicitly in URL (%r) when connecting; \"\n            \"use login instead.\" % url.username)\n\n    if apikey is None:\n        credentials = None  # Anonymous access.\n    else:\n        credentials = Credentials.parse(apikey)\n\n    description = await fetch_api_description(url, insecure)\n\n    # Return a new (unsaved) profile.\n    return Profile(\n        name=url.netloc, url=url.geturl(), credentials=credentials,\n        description=description)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def login(url, *, anonymous=False, username=None, password=None,\n                insecure=False):\n    \"\"\"Log-in to a remote MAAS instance.\n\n    Returns a new :class:`Profile` which has NOT been saved. To log-in AND\n    save a new profile::\n\n        profile = login(url, username=\"alice\", password=\"wonderland\")\n        profile = profile.replace(name=\"mad-hatter\")\n\n        with profiles.ProfileStore.open() as config:\n            config.save(profile)\n            # Optionally, set it as the default.\n            config.default = profile.name\n\n    :raise RemoteError: An unexpected error from the remote system.\n    :raise LoginError: An error related to logging-in.\n    :raise PasswordWithoutUsername: Password given, but not username.\n    :raise UsernameWithoutPassword: Username given, but not password.\n    :raise LoginNotSupported: Server does not support API client log-in.\n    \"\"\"\n    url = api_url(url)\n    url = urlparse(url)\n\n    if username is None:\n        username = url.username\n    else:\n        if url.username is None:\n            pass  # Anonymous access.\n        else:\n            raise LoginError(\n                \"User-name provided explicitly (%r) and in URL (%r); \"\n                \"provide only one.\" % (username, url.username))\n\n    if password is None:\n        password = url.password\n    else:\n        if url.password is None:\n            pass  # Anonymous access.\n        else:\n            raise LoginError(\n                \"Password provided explicitly (%r) and in URL (%r); \"\n                \"provide only one.\" % (password, url.password))\n\n    # Remove user-name and password from the URL.\n    userinfo, _, hostinfo = url.netloc.rpartition(\"@\")\n    url = url._replace(netloc=hostinfo)\n\n    if username is None:\n        if password:\n            raise PasswordWithoutUsername(\n                \"Password provided without user-name; specify user-name.\")\n        elif anonymous:\n            credentials = None\n        else:\n            credentials = await authenticate_with_macaroon(\n                url.geturl(), insecure=insecure)\n    else:\n        if password is None:\n            raise UsernameWithoutPassword(\n                \"User-name provided without password; specify password.\")\n        else:\n            credentials = await authenticate(\n                url.geturl(), username, password, insecure=insecure)\n\n    description = await fetch_api_description(url, insecure)\n    profile_name = username or url.netloc\n\n    # Return a new (unsaved) profile.\n    return Profile(\n        name=profile_name, url=url.geturl(), credentials=credentials,\n        description=description)", "response": "Log - in to a MAAS instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def authenticate_with_macaroon(url, insecure=False):\n    executor = futures.ThreadPoolExecutor(max_workers=1)\n\n    def get_token():\n        client = httpbakery.Client()\n        resp = client.request(\n            'POST', '{}/account/?op=create_authorisation_token'.format(url),\n            verify=not insecure)\n        if resp.status_code == HTTPStatus.UNAUTHORIZED:\n            # if the auteentication with Candid fails, an exception is raised\n            # above so we don't get here\n            raise MacaroonLoginNotSupported(\n                'Macaroon authentication not supported')\n        if resp.status_code != HTTPStatus.OK:\n            raise LoginError('Login failed: {}'.format(resp.text))\n        result = resp.json()\n        return '{consumer_key}:{token_key}:{token_secret}'.format(**result)\n\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(executor, get_token)", "response": "Authenticate via macaroons and generate and return new API keys."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nobtain a new API key by logging into MAAS.", "response": "async def authenticate(url, username, password, *, insecure=False):\n    \"\"\"Obtain a new API key by logging into MAAS.\n\n    :param url: URL for the MAAS API (i.e. ends with ``/api/x.y/``).\n    :param insecure: If true, don't verify SSL/TLS certificates.\n    :return: A `Credentials` instance.\n\n    :raise RemoteError: An unexpected error from the remote system.\n    :raise LoginNotSupported: Server does not support API client log-in.\n    \"\"\"\n    url_versn = urljoin(url, \"version/\")\n    url_authn = urljoin(url, \"../../accounts/authenticate/\")\n\n    def check_response_is_okay(response):\n        if response.status != HTTPStatus.OK:\n            raise RemoteError(\n                \"{0} -> {1.status} {1.reason}\".format(\n                    response.url_obj.human_repr(), response))\n\n    connector = aiohttp.TCPConnector(verify_ssl=(not insecure))\n    session = aiohttp.ClientSession(connector=connector)\n    async with session:\n        # Check that this server supports `authenticate-api`.\n        async with session.get(url_versn) as response:\n            check_response_is_okay(response)\n            version_info = await response.json()\n\n        if \"authenticate-api\" not in version_info[\"capabilities\"]:\n            raise LoginNotSupported(\n                \"Server does not support automated client log-in. \"\n                \"Please obtain an API token via the MAAS UI.\")\n\n        # POST to the `authenticate` endpoint.\n        data = {\n            \"username\": username, \"password\": password,\n            \"consumer\": \"%s@%s\" % (getuser(), gethostname())\n        }\n        async with session.post(url_authn, data=data) as response:\n            check_response_is_okay(response)\n            token_info = await response.json()\n\n        return Credentials(\n            token_info[\"consumer_key\"],\n            token_info[\"token_key\"],\n            token_info[\"token_secret\"],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def create(\n            cls, *, type: str,\n            power_address: str,\n            power_user: str = None,\n            power_pass: str = None,\n            name: str = None,\n            zone: typing.Union[str, Zone] = None,\n            tags: typing.Sequence[str] = None):\n        \"\"\"Create a `Pod` in MAAS.\n\n        :param type: Type of pod to create (rsd, virsh) (required).\n        :type name: `str`\n        :param power_address: Address for power control of the pod (required).\n        :type power_address: `str`\n        :param power_user: User for power control of the pod\n            (required for rsd).\n        :type power_user: `str`\n        :param power_pass: Password for power control of the pod\n            (required for rsd).\n        :type power_pass: `str`\n        :param name: Name for the pod (optional).\n        :type name: `str`\n        :param zone: Name of the zone for the pod (optional).\n        :type zone: `str` or `Zone`\n        :param tags: A tag or tags (separated by comma) for the pod.\n        :type tags: `str`\n        :returns: The created Pod.\n        :rtype: `Pod`\n        \"\"\"\n        params = remove_None({\n            'type': type,\n            'power_address': power_address,\n            'power_user': power_user,\n            'power_pass': power_pass,\n            'name': name,\n            'tags': tags,\n        })\n        if type == 'rsd' and power_user is None:\n            message = \"'power_user' is required for pod type `rsd`\"\n            raise OperationNotAllowed(message)\n        if type == 'rsd' and power_pass is None:\n            message = \"'power_pass' is required for pod type `rsd`\"\n            raise OperationNotAllowed(message)\n        if zone is not None:\n            if isinstance(zone, Zone):\n                params[\"zone\"] = zone.name\n            elif isinstance(zone, str):\n                params[\"zone\"] = zone\n            else:\n                raise TypeError(\n                    \"zone must be a str or Zone, not %s\" % type(zone).__name__)\n        return cls._object(await cls._handler.create(**params))", "response": "Create a new pod in MAAS."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def compose(\n            self, *, cores: int = None, memory: int = None,\n            cpu_speed: int = None, architecture: str = None,\n            storage: typing.Sequence[str] = None, hostname: str = None,\n            domain: typing.Union[int, str] = None,\n            zone: typing.Union[int, str, Zone] = None,\n            interfaces: typing.Sequence[str] = None):\n        \"\"\"Compose a machine from `Pod`.\n\n        All fields below are optional:\n\n        :param cores: Minimum number of CPU cores.\n        :type cores: `int`\n        :param memory: Minimum amount of memory (MiB).\n        :type memory: `int`\n        :param cpu_speed: Minimum amount of CPU speed (MHz).\n        :type cpu_speed: `int`\n        :param architecture: Architecture for the machine. Must be an\n            architecture that the pod supports.\n        :type architecture: `str`\n        :param storage: A list of storage constraint identifiers, in the form:\n            <label>:<size>(<tag>[,<tag>[,...])][,<label>:...]\n        :type storage: `str`\n        :param hostname: Hostname for the newly composed machine.\n        :type hostname: `str`\n        :param domain: The domain ID for the machine (optional).\n        :type domain: `int` or `str`\n        :param zone: The zone ID for the machine (optional).\n        :type zone: `int` or 'str' or `Zone`\n        :param interfaces: A labeled constraint map associating constraint\n            labels with interface properties that should be matched. Returned\n            nodes must have one or more interface matching the specified\n            constraints. The labeled constraint map must be in the format:\n            ``<label>:<key>=<value>[,<key2>=<value2>[,...]]``\n\n            Each key can be one of the following:\n\n            - id: Matches an interface with the specific id\n            - fabric: Matches an interface attached to the specified fabric.\n            - fabric_class: Matches an interface attached to a fabric\n              with the specified class.\n            - ip: Matches an interface with the specified IP address\n              assigned to it.\n            - mode: Matches an interface with the specified mode. (Currently,\n              the only supported mode is \"unconfigured\".)\n            - name: Matches an interface with the specified name.\n              (For example, \"eth0\".)\n            - hostname: Matches an interface attached to the node with\n              the specified hostname.\n            - subnet: Matches an interface attached to the specified subnet.\n            - space: Matches an interface attached to the specified space.\n            - subnet_cidr: Matches an interface attached to the specified\n              subnet CIDR. (For example, \"192.168.0.0/24\".)\n            - type: Matches an interface of the specified type. (Valid\n              types: \"physical\", \"vlan\", \"bond\", \"bridge\", or \"unknown\".)\n            - vlan: Matches an interface on the specified VLAN.\n            - vid: Matches an interface on a VLAN with the specified VID.\n            - tag: Matches an interface tagged with the specified tag.\n        :type interfaces: `str`\n        :returns: The created Machine\n        :rtype: `Machine`\n        \"\"\"\n        params = remove_None({\n            'cores': str(cores) if cores else None,\n            'memory': str(memory) if memory else None,\n            'cpu_speed': str(cpu_speed) if cpu_speed else None,\n            'architecture': architecture,\n            'storage': storage,\n            'hostname': hostname,\n            'domain': str(domain) if domain else None,\n            'interfaces': interfaces,\n        })\n        if zone is not None:\n            if isinstance(zone, Zone):\n                params[\"zone\"] = str(zone.id)\n            elif isinstance(zone, int):\n                params[\"zone\"] = str(zone)\n            elif isinstance(zone, str):\n                params[\"zone\"] = zone\n            else:\n                raise TypeError(\n                    \"zone must be an int, str or Zone, not %s\" %\n                    type(zone).__name__)\n        return await self._handler.compose(**params, id=self.id)", "response": "Compose a new machine from a Pod."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create(\n            cls, start_ip: str, end_ip: str, *,\n            type: IPRangeType = IPRangeType.RESERVED,\n            comment: str = None, subnet: Union[Subnet, int] = None):\n        \"\"\"\n        Create a `IPRange` in MAAS.\n\n        :param start_ip: First IP address in the range (required).\n        :type start_ip: `str`\n        :parma end_ip: Last IP address in the range (required).\n        :type end_ip: `str`\n        :param type: Type of IP address range (optional).\n        :type type: `IPRangeType`\n        :param comment: Reason for the IP address range (optional).\n        :type comment: `str`\n        :param subnet: Subnet the IP address range should be created on\n            (optional). By default MAAS will calculate the correct subnet\n            based on the `start_ip` and `end_ip`.\n        :type subnet: `Subnet` or `int`\n        :returns: The created IPRange\n        :rtype: `IPRange`\n        \"\"\"\n        if not isinstance(type, IPRangeType):\n            raise TypeError(\n                \"type must be an IPRangeType, not %s\"\n                % TYPE(type).__name__)\n\n        params = {\n            'start_ip': start_ip,\n            'end_ip': end_ip,\n            'type': type.value,\n        }\n        if comment is not None:\n            params[\"comment\"] = comment\n        if subnet is not None:\n            if isinstance(subnet, Subnet):\n                params[\"subnet\"] = subnet.id\n            elif isinstance(subnet, int):\n                params[\"subnet\"] = subnet\n            else:\n                raise TypeError(\n                    \"subnet must be Subnet or int, not %s\" % (\n                        TYPE(subnet).__class__))\n        return cls._object(await cls._handler.create(**params))", "response": "Create a new IPRange in MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the parents to send to the handler.", "response": "def gen_parents(parents):\n    \"\"\"Generate the parents to send to the handler.\"\"\"\n    for idx, parent in enumerate(parents):\n        if isinstance(parent, Interface):\n            parent = parent.id\n        elif isinstance(parent, int):\n            pass\n        else:\n            raise TypeError(\n                'parent[%d] must be an Interface or int, not %s' % (\n                    idx, type(parent).__name__))\n        yield parent"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the parent to send to the handler.", "response": "def get_parent(parent):\n    \"\"\"Get the parent to send to the handler.\"\"\"\n    if isinstance(parent, Interface):\n        return parent.id\n    elif isinstance(parent, int):\n        return parent\n    else:\n        raise TypeError(\n            \"parent must be an Interface or int, not %s\" % (\n                type(parent).__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting this interface link.", "response": "async def delete(self):\n        \"\"\"Delete this interface link.\"\"\"\n        interface = self._data['interface']\n        data = await interface._handler.unlink_subnet(\n            system_id=interface.node.system_id, id=interface.id, _id=self.id)\n        interface._data['links'] = list(data['links'])\n        interface._orig_data['links'] = copy.deepcopy(interface._data['links'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def set_as_default_gateway(self):\n        interface = self._data['interface']\n        await interface._handler.set_default_gateway(\n            system_id=interface.node.system_id, id=interface.id,\n            link_id=self.id)", "response": "Set this link as the default gateway for the node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a link on the given interface.", "response": "async def create(\n            cls, interface: Interface, mode: LinkMode,\n            subnet: Union[Subnet, int] = None, ip_address: str = None,\n            force: bool = False, default_gateway: bool = False):\n        \"\"\"\n        Create a link on `Interface` in MAAS.\n\n        :param interface: Interface to create the link on.\n        :type interface: `Interface`\n        :param mode: Mode of the link.\n        :type mode: `LinkMode`\n        :param subnet: The subnet to create the link on (optional).\n        :type subnet: `Subnet` or `int`\n        :param ip_address: The IP address to assign to the link.\n        :type ip_address: `str`\n        :param force: If True, allows `LinkMode.LINK_UP` to be created even if\n            other links already exist. Also allows the selection of any\n            subnet no matter the VLAN the subnet belongs to. Using this option\n            will cause all other interface links to be deleted (optional).\n        :type force: `bool`\n        :param default_gateway: If True, sets the gateway IP address for the\n            subnet as the default gateway for the node this interface belongs\n            to. Option can only be used with the `LinkMode.AUTO` and\n            `LinkMode.STATIC` modes.\n        :type default_gateway: `bool`\n\n        :returns: The created InterfaceLink.\n        :rtype: `InterfaceLink`\n        \"\"\"\n        if not isinstance(interface, Interface):\n            raise TypeError(\n                \"interface must be an Interface, not %s\"\n                % type(interface).__name__)\n        if not isinstance(mode, LinkMode):\n            raise TypeError(\n                \"mode must be a LinkMode, not %s\"\n                % type(mode).__name__)\n        if subnet is not None:\n            if isinstance(subnet, Subnet):\n                subnet = subnet.id\n            elif isinstance(subnet, int):\n                pass\n            else:\n                raise TypeError(\n                    \"subnet must be a Subnet or int, not %s\"\n                    % type(subnet).__name__)\n        if mode in [LinkMode.AUTO, LinkMode.STATIC]:\n            if subnet is None:\n                raise ValueError('subnet is required for %s' % mode)\n        if default_gateway and mode not in [LinkMode.AUTO, LinkMode.STATIC]:\n            raise ValueError('cannot set as default_gateway for %s' % mode)\n        params = {\n            'system_id': interface.node.system_id,\n            'id': interface.id,\n            'mode': mode.value,\n            'force': force,\n            'default_gateway': default_gateway,\n        }\n        if subnet is not None:\n            params['subnet'] = subnet\n        if ip_address is not None:\n            params['ip_address'] = ip_address\n        # The API doesn't return just the link it returns the whole interface.\n        # Store the link ids before the save to find the addition at the end.\n        link_ids = {\n            link.id\n            for link in interface.links\n        }\n        data = await interface._handler.link_subnet(**params)\n        # Update the links on the interface, except for the newly created link\n        # the `ManagedCreate` wrapper will add that to the interfaces link data\n        # automatically.\n        new_links = {\n            link['id']: link\n            for link in data['links']\n        }\n        links_diff = list(set(new_links.keys()) - link_ids)\n        new_link = new_links.pop(links_diff[0])\n        interface._data['links'] = list(new_links.values())\n        interface._orig_data['links'] = copy.deepcopy(interface._data['links'])\n        return cls._object(new_link)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def create(\n            cls, node: Union[Node, str],\n            interface_type: InterfaceType = InterfaceType.PHYSICAL, *,\n            name: str = None, mac_address: str = None,\n            tags: Iterable[str] = None, vlan: Union[Vlan, int] = None,\n            parent: Union[Interface, int] = None,\n            parents: Iterable[Union[Interface, int]] = None, mtu: int = None,\n            accept_ra: bool = None, autoconf: bool = None,\n            bond_mode: str = None, bond_miimon: int = None,\n            bond_downdelay: int = None, bond_updelay: int = None,\n            bond_lacp_rate: str = None, bond_xmit_hash_policy: str = None,\n            bridge_stp: bool = None, bridge_fd: int = None):\n        \"\"\"\n        Create a `Interface` in MAAS.\n\n        :param node: Node to create the interface on.\n        :type node: `Node` or `str`\n        :param interface_type: Type of interface to create (optional).\n        :type interface_type: `InterfaceType`\n        :param name: The name for the interface (optional).\n        :type name: `str`\n        :param tags: List of tags to add to the interface.\n        :type tags: sequence of `str`\n        :param mtu: The MTU for the interface (optional).\n        :type mtu: `int`\n        :param vlan: VLAN the interface is connected to (optional).\n        :type vlan: `Vlan` or `int`\n        :param accept_ra: True if the interface should accepted router\n            advertisements. (optional)\n        :type accept_ra: `bool`\n        :param autoconf: True if the interface should auto configure.\n        :type autoconf: `bool`\n\n        Following parameters specific to physical interface.\n\n        :param mac_address: The MAC address for the interface.\n        :type mac_address: `str`\n\n        Following parameters specific to a bond interface.\n\n        :param parents: Parent interfaces that make up the bond.\n        :type parents: sequence of `Interface` or `int`\n        :param mac_address: MAC address to use for the bond (optional).\n        :type mac_address: `str`\n        :param bond_mode: The operating mode of the bond (optional).\n        :type bond_mode: `str`\n        :param bond_miimon: The link monitoring freqeuncy in\n            milliseconds (optional).\n        :type bond_miimon: `int`\n        :param bond_downdelay: Specifies the time, in milliseconds, to wait\n            before disabling a slave after a link failure has been detected\n            (optional).\n        :type bond_downdelay: `int`\n        :param bond_updelay: Specifies the time, in milliseconds, to wait\n            before enabling a slave after a link recovery has been detected.\n        :type bond_updelay: `int`\n        :param bond_lacp_rate: Option specifying the rate in which we'll ask\n            our link partner to transmit LACPDU packets in 802.3ad\n            mode (optional).\n        :type bond_lacp_rate: `str`\n        :param bond_xmit_hash_policy: The transmit hash policy to use for\n            slave selection in balance-xor, 802.3ad, and tlb modes(optional).\n        :type bond_xmit_hash_policy: `str`\n\n        Following parameters specific to a VLAN interface.\n\n        :param parent: Parent interface for this VLAN interface.\n        :type parent: `Interface` or `int`\n\n        Following parameters specific to a Bridge interface.\n\n        :param parent: Parent interface for this bridge interface.\n        :type parent: `Interface` or `int`\n        :param mac_address: The MAC address for the interface (optional).\n        :type mac_address: `str`\n        :param bridge_stp: Turn spanning tree protocol on or off (optional).\n        :type bridge_stp: `bool`\n        :param bridge_fd: Set bridge forward delay to time seconds (optional).\n        :type bridge_fd: `int`\n\n        :returns: The created Interface.\n        :rtype: `Interface`\n        \"\"\"\n        params = {}\n        if isinstance(node, str):\n            params['system_id'] = node\n        elif isinstance(node, Node):\n            params['system_id'] = node.system_id\n        else:\n            raise TypeError(\n                'node must be a Node or str, not %s' % (\n                    type(node).__name__))\n\n        if name is not None:\n            params['name'] = name\n        if tags is not None:\n            params['tags'] = tags\n        if mtu is not None:\n            params['mtu'] = mtu\n        if vlan is not None:\n            if isinstance(vlan, Vlan):\n                params['vlan'] = vlan.id\n            elif isinstance(vlan, int):\n                params['vlan'] = vlan\n            else:\n                raise TypeError(\n                    'vlan must be a Vlan or int, not %s' % (\n                        type(vlan).__name__))\n        if accept_ra is not None:\n            params['accept_ra'] = accept_ra\n        if autoconf is not None:\n            params['autoconf'] = autoconf\n\n        handler = None\n        if not isinstance(interface_type, InterfaceType):\n            raise TypeError(\n                'interface_type must be an InterfaceType, not %s' % (\n                    type(interface_type).__name__))\n        if interface_type == InterfaceType.PHYSICAL:\n            handler = cls._handler.create_physical\n            if mac_address:\n                params['mac_address'] = mac_address\n            else:\n                raise ValueError(\n                    'mac_address required for physical interface')\n        elif interface_type == InterfaceType.BOND:\n            handler = cls._handler.create_bond\n            if parent is not None:\n                raise ValueError(\"use parents not parent for bond interface\")\n            if not isinstance(parents, Iterable):\n                raise TypeError(\n                    'parents must be a iterable, not %s' % (\n                        type(parents).__name__))\n            if len(parents) == 0:\n                raise ValueError(\n                    'at least one parent required for bond interface')\n            params['parents'] = list(gen_parents(parents))\n            if not name:\n                raise ValueError('name is required for bond interface')\n            if mac_address is not None:\n                params['mac_address'] = mac_address\n            if bond_mode is not None:\n                params['bond_mode'] = bond_mode\n            if bond_miimon is not None:\n                params['bond_miimon'] = bond_miimon\n            if bond_downdelay is not None:\n                params['bond_downdelay'] = bond_downdelay\n            if bond_updelay is not None:\n                params['bond_updelay'] = bond_updelay\n            if bond_lacp_rate is not None:\n                params['bond_lacp_rate'] = bond_lacp_rate\n            if bond_xmit_hash_policy is not None:\n                params['bond_xmit_hash_policy'] = bond_xmit_hash_policy\n        elif interface_type == InterfaceType.VLAN:\n            handler = cls._handler.create_vlan\n            if parents is not None:\n                raise ValueError(\"use parent not parents for VLAN interface\")\n            if parent is None:\n                raise ValueError(\"parent is required for VLAN interface\")\n            params['parent'] = get_parent(parent)\n            if vlan is None:\n                raise ValueError(\"vlan is required for VLAN interface\")\n        elif interface_type == InterfaceType.BRIDGE:\n            handler = cls._handler.create_bridge\n            if parents is not None:\n                raise ValueError(\"use parent not parents for bridge interface\")\n            if parent is None:\n                raise ValueError(\"parent is required for bridge interface\")\n            params['parent'] = get_parent(parent)\n            if not name:\n                raise ValueError('name is required for bridge interface')\n            if mac_address is not None:\n                params['mac_address'] = mac_address\n            if bridge_stp is not None:\n                params['bridge_stp'] = bridge_stp\n            if bridge_fd is not None:\n                params['bridge_fd'] = bridge_fd\n        else:\n            raise ValueError(\n                \"cannot create an interface of type: %s\" % interface_type)\n\n        return cls._object(await handler(**params))", "response": "Create a new interface in MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a BcacheCacheSet on a Node.", "response": "async def create(\n            cls, node: Union[Node, str],\n            cache_device: Union[BlockDevice, Partition]):\n        \"\"\"\n        Create a BcacheCacheSet on a Node.\n\n        :param node: Node to create the interface on.\n        :type node: `Node` or `str`\n        :param cache_device: Block device or partition to create\n            the cache set on.\n        :type cache_device: `BlockDevice` or `Partition`\n        \"\"\"\n        params = {}\n        if isinstance(node, str):\n            params['system_id'] = node\n        elif isinstance(node, Node):\n            params['system_id'] = node.system_id\n        else:\n            raise TypeError(\n                'node must be a Node or str, not %s' % (\n                    type(node).__name__))\n        if isinstance(cache_device, BlockDevice):\n            params['cache_device'] = cache_device.id\n        elif isinstance(cache_device, Partition):\n            params['cache_partition'] = cache_device.id\n        else:\n            raise TypeError(\n                'cache_device must be a BlockDevice or Partition, not %s' % (\n                    type(cache_device).__name__))\n\n        return cls._object(await cls._handler.create(**params))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _filter_rows(self, rows):\n        filtered_rows = []\n        for row in rows:\n            filtered_row = []\n            for idx, name in enumerate(self.columns.keys()):\n                if name in self.visible_columns:\n                    filtered_row.append(row[idx])\n            filtered_rows.append(filtered_row)\n        return filtered_rows", "response": "Filter rows based on the visible columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filter_rows(self, rows, visible_columns=None):\n        if visible_columns is None:\n            visible_columns = self.visible_columns\n        filtered_row = []\n        for idx, name in enumerate(self.columns.keys()):\n            if name in self.visible_columns:\n                filtered_row.append(rows[idx])\n        return filtered_row", "response": "Filter rows based on the visible columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def get_default(cls):\n        data = await cls._handler.read(id=cls._default_fabric_id)\n        return cls(data)", "response": "Get the default Fabric for the MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(cls, credentials) -> typing.Optional[\"Credentials\"]:\n        if credentials is None:\n            return None\n        elif isinstance(credentials, cls):\n            return credentials\n        elif isinstance(credentials, str):\n            if credentials == \"\":\n                return None\n            elif credentials.count(\":\") == 2:\n                return cls(*credentials.split(\":\"))\n            else:\n                raise ValueError(\n                    \"Malformed credentials. Expected 3 colon-separated \"\n                    \"parts, got %r.\" % (credentials, ))\n        else:\n            parts = list(credentials)\n            if len(parts) == 0:\n                return None\n            elif len(parts) == 3:\n                return cls(*parts)\n            else:\n                raise ValueError(\n                    \"Malformed credentials. Expected 3 parts, \"\n                    \"got %r.\" % (credentials, ))", "response": "Parse some given credentials and return a new instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dir_class(cls):\n    # Class attributes (including methods).\n    for name, value in vars_class(cls).items():\n        if isinstance(value, ObjectMethod):\n            if value.has_classmethod:\n                yield name\n        elif isinstance(value, Disabled):\n            pass  # Hide this; disabled.\n        else:\n            yield name\n    # Metaclass attributes.\n    for name, value in vars_class(type(cls)).items():\n        if name == \"mro\":\n            pass  # Hide this; not interesting.\n        elif isinstance(value, Disabled):\n            pass  # Hide this; disabled.\n        else:\n            yield name", "response": "Return a list of names available on cls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of names that bind to an ObjectMethod without a corresponding instance method.", "response": "def dir_instance(inst):\n    \"\"\"Return a list of names available on `inst`.\n\n    Eliminates names that bind to an `ObjectMethod` without a corresponding\n    instance method; see `ObjectMethod`.\n    \"\"\"\n    # Skip instance attributes; __slots__ is automatically defined, and\n    # descriptors are used to define attributes. Instead, go straight to class\n    # attributes (including methods).\n    for name, value in vars_class(type(inst)).items():\n        if isinstance(value, ObjectMethod):\n            if value.has_instancemethod:\n                yield name\n        elif isinstance(value, Disabled):\n            pass  # Hide this; disabled.\n        elif isinstance(value, (classmethod, staticmethod)):\n            pass  # Hide this; not interesting here.\n        else:\n            yield name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_pk_descriptor(descriptor, include_alt=False):\n    if descriptor.pk is True or type(descriptor.pk) is int:\n        return True\n    if include_alt:\n        return descriptor.alt_pk is True or type(descriptor.alt_pk) is int\n    else:\n        return False", "response": "Return true if descriptor is a primary key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pk_descriptors(cls):\n    pk_fields = {\n        name: descriptor\n        for name, descriptor in vars_class(cls).items()\n        if isinstance(descriptor, ObjectField) and is_pk_descriptor(descriptor)\n    }\n    alt_pk_fields = defaultdict(list)\n    for name, descriptor in vars_class(cls).items():\n        if isinstance(descriptor, ObjectField):\n            if descriptor.alt_pk is True:\n                alt_pk_fields[0].append((name, descriptor))\n            elif type(descriptor.alt_pk) is int:\n                alt_pk_fields[descriptor.alt_pk].append((name, descriptor))\n    if len(pk_fields) == 1:\n        return ((pk_fields.popitem(),), (alt_pk_fields[0],))\n    elif len(pk_fields) > 1:\n        unique_pk_fields = {\n            name: descriptor\n            for name, descriptor in pk_fields.items()\n            if descriptor.pk is True\n        }\n        if unique_pk_fields:\n            raise AttributeError(\n                \"more than one field is marked as unique primary key: %s\" % (\n                    ', '.join(sorted(pk_fields))))\n        pk_descriptors = tuple(sorted((\n            (name, descriptor)\n            for name, descriptor in pk_fields.items()\n        ), key=lambda item: item[1].pk))\n        alt_pk_descriptors = tuple(\n            alt_pk_fields[idx]\n            for idx, (name, descriptor) in enumerate(pk_descriptors)\n        )\n        return pk_descriptors, alt_pk_descriptors\n    else:\n        return tuple(), tuple()", "response": "Return tuple of attribute name and descriptor on the\n    cls that is defined as the primary keys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mapping_of(cls):\n    def mapper(data):\n        if not isinstance(data, Mapping):\n            raise TypeError(\n                \"data must be a mapping, not %s\"\n                % type(data).__name__)\n        return {\n            key: cls(value)\n            for key, value in data.items()\n        }\n    return mapper", "response": "Expects a mapping from some key to data for cls instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds subclasses of Object and ObjectSet in the given modules.", "response": "def find_objects(modules):\n    \"\"\"Find subclasses of `Object` and `ObjectSet` in the given modules.\n\n    :param modules: The full *names* of modules to include. These modules MUST\n        have been imported in advance.\n    \"\"\"\n    return {\n        subclass.__name__: subclass\n        for subclass in chain(\n            get_all_subclasses(Object),\n            get_all_subclasses(ObjectSet),\n        )\n        if subclass.__module__ in modules\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbinds this object to the given origin and handler.", "response": "def bind(cls, origin, handler, *, name=None):\n        \"\"\"Bind this object to the given origin and handler.\n\n        :param origin: An instance of `Origin`.\n        :param handler: An instance of `bones.HandlerAPI`.\n        :return: A subclass of this class.\n        \"\"\"\n        name = cls.__name__ if name is None else name\n        attrs = {\n            \"_origin\": origin, \"_handler\": handler,\n            \"__module__\": \"origin\",  # Could do better?\n        }\n        return type(name, (cls,), attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def refresh(self):\n        cls = type(self)\n        if hasattr(cls, 'read'):\n            descriptors, alt_descriptors = get_pk_descriptors(cls)\n            if len(descriptors) == 1:\n                try:\n                    obj = await cls.read(getattr(self, descriptors[0][0]))\n                except AttributeError:\n                    found = False\n                    for alt_name, _ in alt_descriptors[0]:\n                        if hasattr(self, alt_name):\n                            obj = await cls.read(getattr(self, alt_name))\n                            found = True\n                            break\n                    if not found:\n                        raise\n            elif len(descriptors) > 1:\n                args = []\n                for idx, (name, _) in enumerate(descriptors):\n                    try:\n                        args.append(getattr(self, name))\n                    except AttributeError:\n                        found = False\n                        for alt_name, _ in alt_descriptors[idx]:\n                            if hasattr(self, alt_name):\n                                args.append(getattr(self, alt_name))\n                                found = True\n                                break\n                        if not found:\n                            raise\n                obj = await cls.read(*args)\n            else:\n                raise AttributeError(\n                    \"unable to perform 'refresh' no primary key \"\n                    \"fields defined.\")\n            if type(obj) is cls:\n                self._data = obj._data\n                self._loaded = True\n            else:\n                raise TypeError(\n                    \"result of '%s.read' must be '%s', not '%s'\" % (\n                        cls.__name__, cls.__name__, type(obj).__name__))\n        else:\n            raise AttributeError(\n                \"'%s' object doesn't support refresh.\" % cls.__name__)", "response": "Refresh the object from MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the object in MAAS.", "response": "async def save(self):\n        \"\"\"Save the object in MAAS.\"\"\"\n        if hasattr(self._handler, \"update\"):\n            if self._changed_data:\n                update_data = dict(self._changed_data)\n                update_data.update({\n                    key: self._orig_data[key]\n                    for key in self._handler.params\n                })\n                self._data = await self._handler.update(**update_data)\n        else:\n            raise AttributeError(\n                \"'%s' object doesn't support save.\" % type(self).__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a custom ObjectSet that is managed by a related Object.", "response": "def Managed(cls, manager, field, items):\n        \"\"\"Create a custom `ObjectSet` that is managed by a related `Object.`\n\n        :param manager: The manager of the `ObjectSet`. This is the `Object`\n            that manages this set of objects.\n        :param field: The field on the `manager` that created this managed\n            `ObjectSet`.\n        :param items: The items in the `ObjectSet`.\n        \"\"\"\n        attrs = {\n            \"_manager\": manager,\n            \"_manager_field\": field,\n        }\n        if hasattr(cls, \"create\"):\n            attrs['create'] = ManagedCreate(cls)\n        cls = type(\n            \"%s.Managed#%s\" % (\n                cls.__name__, manager.__class__.__name__), (cls,), attrs)\n        return cls(items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Checked(cls, name, datum_to_value=None, value_to_datum=None, **other):\n        attrs = {}\n        if datum_to_value is not None:\n            @wraps(datum_to_value)\n            def datum_to_value_method(instance, datum):\n                return datum_to_value(datum)\n            attrs[\"datum_to_value\"] = staticmethod(datum_to_value_method)\n        if value_to_datum is not None:\n            @wraps(value_to_datum)\n            def value_to_datum_method(instance, value):\n                return value_to_datum(value)\n            attrs[\"value_to_datum\"] = staticmethod(value_to_datum_method)\n        cls = type(\"%s.Checked#%s\" % (cls.__name__, name), (cls,), attrs)\n        return cls(name, **other)", "response": "Create a custom object field that validates values and datums."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef datum_to_value(self, instance, datum):\n        datum = self.map_func(instance, datum)\n        if datum is None:\n            return None\n        local_data = None\n        if self.reverse is not None:\n            local_data = {}\n            if self.reverse is undefined:\n                local_data[instance.__class__.__name__.lower()] = instance\n            else:\n                local_data[self.reverse] = instance\n        # Get the class from the bound origin.\n        bound = getattr(instance._origin, self.cls)\n        return bound(datum, local_data=local_data)", "response": "Convert a given MAAS - side datum into a Python - side value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef value_to_datum(self, instance, value):\n        if value is None:\n            return None\n        bound = getattr(instance._origin, self.cls)\n        if type(value) is bound:\n            if self.use_data_setter:\n                # Using data setter, so just return the data for the object.\n                return value._data\n            else:\n                # Use the primary keys to set the value.\n                descriptors, alt_descriptors = get_pk_descriptors(bound)\n                if len(descriptors) == 1:\n                    return getattr(value, descriptors[0][0])\n                elif len(descriptors) > 1:\n                    return tuple(\n                        getattr(value, name)\n                        for name, _ in descriptors\n                    )\n                else:\n                    raise AttributeError(\n                        \"unable to perform set object no primary key \"\n                        \"fields defined for %s\" % self.cls)\n        else:\n            raise TypeError(\n                \"must be %s, not %s\" % (self.cls, type(value).__name__))", "response": "Convert a given Python - side value into a MAAS - side datum."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a given MAAS - side datum into a Python - side value.", "response": "def datum_to_value(self, instance, datum):\n        \"\"\"Convert a given MAAS-side datum to a Python-side value.\n\n        :param instance: The `Object` instance on which this field is\n            currently operating. This method should treat it as read-only, for\n            example to perform validation with regards to other fields.\n        :param datum: The MAAS-side datum to validate and convert into a\n            Python-side value.\n        :return: A set of `cls` from the given datum.\n        \"\"\"\n        if datum is None:\n            return []\n        if not isinstance(datum, Sequence):\n            raise TypeError(\n                \"datum must be a sequence, not %s\" % type(datum).__name__)\n        local_data = None\n        if self.reverse is not None:\n            local_data = {}\n            if self.reverse is undefined:\n                local_data[instance.__class__.__name__.lower()] = instance\n            else:\n                local_data[self.reverse] = instance\n        # Get the class from the bound origin.\n        bound = getattr(instance._origin, self.cls)\n        return bound.Managed(\n            instance, self,\n            (\n                bound._object(\n                    self.map_func(instance, item), local_data=local_data)\n                for item in datum\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an Origin for a given MAAS instance.", "response": "async def fromURL(cls, url, *, credentials=None, insecure=False):\n        \"\"\"Return an `Origin` for a given MAAS instance.\"\"\"\n        session = await bones.SessionAPI.fromURL(\n            url, credentials=credentials, insecure=insecure)\n        return cls(session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an Origin from a given configuration profile.", "response": "def fromProfile(cls, profile):\n        \"\"\"Return an `Origin` from a given configuration profile.\n\n        :see: `ProfileStore`.\n        \"\"\"\n        session = bones.SessionAPI.fromProfile(profile)\n        return cls(session)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an Origin object from a given configuration profile name.", "response": "def fromProfileName(cls, name):\n        \"\"\"Return an `Origin` from a given configuration profile name.\n\n        :see: `ProfileStore`.\n        \"\"\"\n        session = bones.SessionAPI.fromProfileName(name)\n        return cls(session)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking an Origin by logging - in with a username and password.", "response": "async def login(\n            cls, url, *, username=None, password=None, insecure=False):\n        \"\"\"Make an `Origin` by logging-in with a username and password.\n\n        :return: A tuple of ``profile`` and ``origin``, where the former is an\n            unsaved `Profile` instance, and the latter is an `Origin` instance\n            made using the profile.\n        \"\"\"\n        profile, session = await bones.SessionAPI.login(\n            url=url, username=username, password=password, insecure=insecure)\n        return profile, cls(session)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconnecting to an anatomy.", "response": "async def connect(\n            cls, url, *, apikey=None, insecure=False):\n        \"\"\"Make an `Origin` by connecting with an apikey.\n\n        :return: A tuple of ``profile`` and ``origin``, where the former is an\n            unsaved `Profile` instance, and the latter is an `Origin` instance\n            made using the profile.\n        \"\"\"\n        profile, session = await bones.SessionAPI.connect(\n            url=url, apikey=apikey, insecure=insecure)\n        return profile, cls(session)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeclares a method as a facade factory.", "response": "def facade(factory):\n    \"\"\"Declare a method as a facade factory.\"\"\"\n    wrapper = FacadeDescriptor(factory.__name__, factory)\n    return update_wrapper(wrapper, factory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets list of Partitions for node and block_device.", "response": "async def read(cls, node, block_device):\n        \"\"\"Get list of `Partitions`'s for `node` and `block_device`.\"\"\"\n        if isinstance(node, str):\n            system_id = node\n        elif isinstance(node, Node):\n            system_id = node.system_id\n        else:\n            raise TypeError(\n                \"node must be a Node or str, not %s\"\n                % type(node).__name__)\n        if isinstance(block_device, int):\n            block_device = block_device\n        elif isinstance(block_device, BlockDevice):\n            block_device = block_device.id\n        else:\n            raise TypeError(\n                \"node must be a Node or str, not %s\"\n                % type(block_device).__name__)\n        data = await cls._handler.read(\n            system_id=system_id, device_id=block_device)\n        return cls(\n            cls._object(item)\n            for item in data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new partition on a block device.", "response": "async def create(cls, block_device: BlockDevice, size: int):\n        \"\"\"\n        Create a partition on a block device.\n\n        :param block_device: BlockDevice to create the paritition on.\n        :type block_device: `BlockDevice`\n        :param size: The size of the partition in bytes.\n        :type size: `int`\n        \"\"\"\n        params = {}\n        if isinstance(block_device, BlockDevice):\n            params['system_id'] = block_device.node.system_id\n            params['device_id'] = block_device.id\n        else:\n            raise TypeError(\n                'block_device must be a BlockDevice, not %s' % (\n                    type(block_device).__name__))\n\n        if not size:\n            raise ValueError(\"size must be provided and greater than zero.\")\n        params['size'] = size\n        return cls._object(await cls._handler.create(**params))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef asynchronous(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        eventloop = get_event_loop()\n        result = func(*args, **kwargs)\n        if not eventloop.is_running():\n            while isawaitable(result):\n                result = eventloop.run_until_complete(result)\n        return result\n\n    return wrapper", "response": "A smart asynchronous version of the function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new Subnet in MAAS.", "response": "async def create(\n            cls, cidr: str, vlan: Union[Vlan, int] = None, *,\n            name: str = None, description: str = None,\n            gateway_ip: str = None, rdns_mode: RDNSMode = None,\n            dns_servers: Union[Sequence[str], str] = None,\n            managed: bool = None):\n        \"\"\"\n        Create a `Subnet` in MAAS.\n\n        :param cidr: The cidr of the `Subnet` (required).\n        :type cidr: `str`\n        :param vlan: The VLAN of the `Subnet` (required).\n        :type vlan: `Vlan`\n        :param name: The name of the `Subnet` (optional, will be given a\n        default value if not specified).\n        :type name: `str`\n        :param description: A description of the `Subnet` (optional).\n        :type description: `str`\n        :param gateway_ip: The gateway IP address for the `Subnet` (optional).\n        :type gateway_ip: `str`\n        :param rdns_mode: The reverse DNS mode for the `Subnet` (optional).\n        :type rdns_mode: `RDNSMode`\n        :param managed: Whether the `Subnet` is managed by MAAS (optional).\n        :type managed: `bool`\n        :returns: The created Subnet\n        :rtype: `Subnet`\n        \"\"\"\n        params = {\n            \"cidr\": cidr\n        }\n\n        if isinstance(vlan, int):\n            params[\"vlan\"] = vlan\n        elif isinstance(vlan, Vlan):\n            params[\"vlan\"] = vlan.id\n        else:\n            raise TypeError(\n                \"vlan must be Vlan or int, not %s\" % (\n                    type(vlan).__class__))\n        if name is not None:\n            params[\"name\"] = name\n        if description is not None:\n            params[\"description\"] = description\n        if gateway_ip is not None:\n            params[\"gateway_ip\"] = gateway_ip\n        if rdns_mode is not None:\n            params[\"rdns_mode\"] = rdns_mode\n        if isinstance(dns_servers, Sequence):\n            if len(dns_servers) > 0:\n                params[\"dns_servers\"] = \",\".join(dns_servers)\n        elif dns_servers is not None:\n            params[\"dns_servers\"] = dns_servers\n        if managed is not None:\n            params[\"managed\"] = managed\n        return cls._object(await cls._handler.create(**params))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_payload(op, method, uri, data):\n    query = [] if op is None else [(\"op\", op)]\n\n    def slurp(opener):\n        with opener() as fd:\n            return fd.read()\n\n    if method == \"GET\":\n        headers, body = [], None\n        query.extend(\n            (name, slurp(value) if callable(value) else value)\n            for name, value in data)\n    else:\n        # Even if data is empty, construct a multipart request body. Piston\n        # (server-side) sets `request.data` to `None` if there's no payload.\n        message = build_multipart_message(data)\n        headers, body = encode_multipart_message(message)\n\n    uri = urlparse(uri)._replace(query=urlencode(query)).geturl()\n    return uri, body, headers", "response": "Prepare the payload for the next request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sign(uri, headers, credentials):\n    consumer_key, token_key, token_secret = credentials\n    auth = OAuthSigner(token_key, token_secret, consumer_key, \"\")\n    auth.sign_request(uri, method=\"GET\", body=None, headers=headers)", "response": "Sign the URI and headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a Python docstring or the docstring found on thing.", "response": "def parse_docstring(thing):\n    \"\"\"Parse a Python docstring, or the docstring found on `thing`.\n\n    :return: a ``(title, body)`` tuple. As per docstring convention, title is\n        the docstring's first paragraph and body is the rest.\n    \"\"\"\n    assert not isinstance(thing, bytes)\n    doc = cleandoc(thing) if isinstance(thing, str) else getdoc(thing)\n    doc = empty if doc is None else doc\n    assert not isinstance(doc, bytes)\n    # Break the docstring into two parts: title and body.\n    parts = docstring_split(doc)\n    if len(parts) == 2:\n        title, body = parts[0], parts[1]\n    else:\n        title, body = parts[0], empty\n    # Remove line breaks from the title line.\n    title = remove_line_breaks(title)\n    # Normalise line-breaks on newline.\n    body = body.replace(\"\\r\\n\", newline).replace(\"\\r\", newline)\n    return docstring(title, body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef api_url(string):\n    url = urlparse(string)\n    url = url._replace(path=ensure_trailing_slash(url.path))\n    if re.search(\"/api/[0-9.]+/?$\", url.path) is None:\n        url = url._replace(path=url.path + \"api/2.0/\")\n    return url.geturl()", "response": "Ensure that string looks like a URL to the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dict of vars for the given class including all ancestors.", "response": "def vars_class(cls):\n    \"\"\"Return a dict of vars for the given class, including all ancestors.\n\n    This differs from the usual behaviour of `vars` which returns attributes\n    belonging to the given class and not its ancestors.\n    \"\"\"\n    return dict(chain.from_iterable(\n        vars(cls).items() for cls in reversed(cls.__mro__)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retries(timeout=30, intervals=1, time=time):\n    start = time()\n    end = start + timeout\n\n    if isinstance(intervals, Iterable):\n        intervals = iter(intervals)\n    else:\n        intervals = repeat(intervals)\n\n    return gen_retries(start, end, intervals, time)", "response": "Helper for retrying something between attempts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_retries(start, end, intervals, time=time):\n    for interval in intervals:\n        now = time()\n        if now < end:\n            wait = min(interval, end - now)\n            yield now - start, end - now, wait\n        else:\n            yield now - start, end - now, 0\n            break", "response": "Generator that yields tuples giving times in seconds until the next retry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all keys in params that have the value of None.", "response": "def remove_None(params: dict):\n    \"\"\"Remove all keys in `params` that have the value of `None`.\"\"\"\n    return {\n        key: value\n        for key, value in params.items()\n        if value is not None\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sign_request(self, url, method, body, headers):\n        # The use of PLAINTEXT here was copied from MAAS, but we should switch\n        # to HMAC once it works server-side.\n        client = oauth1.Client(\n            self.consumer_key, self.consumer_secret, self.token_key,\n            self.token_secret, signature_method=oauth1.SIGNATURE_PLAINTEXT,\n            realm=self.realm)\n        # To preserve API backward compatibility convert an empty string body\n        # to `None`. The old \"oauth\" library would treat the empty string as\n        # \"no body\", but \"oauthlib\" requires `None`.\n        body = None if body is None or len(body) == 0 else body\n        uri, signed_headers, body = client.sign(url, method, body, headers)\n        headers.update(signed_headers)", "response": "Sign a request.\n\n        :param url: The URL to which the request is to be sent.\n        :param headers: The headers in the request. These will be updated with\n            the signature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the current line of the spinner context.", "response": "def print(self, *args, **kwargs):\n        \"\"\"Print inside of the spinner context.\n\n        This must be used when inside of a spinner context to ensure that\n        the line printed doesn't overwrite an already existing spinner line.\n        \"\"\"\n        clear_len = max(len(self._prev_msg), len(self.msg)) + 4\n        self.spinner.stream.write(\"%s\\r\" % (' ' * clear_len))\n        print(*args, file=self.spinner.stream, flush=True, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def create(\n            cls, node: Union[Node, str],\n            level: Union[RaidLevel, str],\n            devices: Iterable[Union[BlockDevice, Partition]], *,\n            name: str = None, uuid: str = None,\n            spare_devices: Iterable[Union[BlockDevice, Partition]]):\n        \"\"\"\n        Create a RAID on a Node.\n\n        :param node: Node to create the interface on.\n        :type node: `Node` or `str`\n        :param level: RAID level.\n        :type level: `RaidLevel`\n        :param devices: Mixed list of block devices or partitions to create\n            the RAID from.\n        :type devices: iterable of mixed type of `BlockDevice` or `Partition`\n        :param name: Name of the RAID (optional).\n        :type name: `str`\n        :param uuid: The UUID for the RAID (optional).\n        :type uuid: `str`\n        :param spare_devices: Mixed list of block devices or partitions to add\n            as spare devices on the RAID.\n        :type spare_devices: iterable of mixed type of `BlockDevice` or\n            `Partition`\n        \"\"\"\n        if isinstance(level, RaidLevel):\n            level = level.value\n        params = {\n            'level': str(level),\n        }\n        if isinstance(node, str):\n            params['system_id'] = node\n        elif isinstance(node, Node):\n            params['system_id'] = node.system_id\n        else:\n            raise TypeError(\n                'node must be a Node or str, not %s' % (\n                    type(node).__name__))\n\n        if len(devices) == 0:\n            raise ValueError(\"devices must contain at least one device.\")\n\n        block_devices = []\n        partitions = []\n        for idx, device in enumerate(devices):\n            if isinstance(device, BlockDevice):\n                block_devices.append(device.id)\n            elif isinstance(device, Partition):\n                partitions.append(device.id)\n            else:\n                raise TypeError(\n                    \"devices[%d] must be a BlockDevice or \"\n                    \"Partition, not %s\" % type(device).__name__)\n        if len(block_devices) > 0:\n            params['block_devices'] = block_devices\n        if len(partitions) > 0:\n            params['partitions'] = partitions\n\n        spare_block_devices = []\n        spare_partitions = []\n        for idx, device in enumerate(spare_devices):\n            if isinstance(device, BlockDevice):\n                spare_block_devices.append(device.id)\n            elif isinstance(device, Partition):\n                spare_partitions.append(device.id)\n            else:\n                raise TypeError(\n                    \"spare_devices[%d] must be a BlockDevice or \"\n                    \"Partition, not %s\" % type(device).__name__)\n        if len(spare_block_devices) > 0:\n            params['spare_devices'] = spare_block_devices\n        if len(spare_partitions) > 0:\n            params['spare_partitions'] = spare_partitions\n\n        if name is not None:\n            params['name'] = name\n        if uuid is not None:\n            params['uuid'] = uuid\n        return cls._object(await cls._handler.create(**params))", "response": "Create a RAID on a Node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def create(cls, destination: Union[int, Subnet],\n                     source: Union[int, Subnet], gateway_ip: str, metric: int):\n        \"\"\"\n        Create a `StaticRoute` in MAAS.\n\n        :param name: The name of the `StaticRoute` (optional, will be given a\n        default value if not specified).\n        :type name: `str`\n        :param description: A description of the `StaticRoute` (optional).\n        :type description: `str`\n        :param class_type: The class type of the `StaticRoute` (optional).\n        :type class_type: `str`\n        :returns: The created StaticRoute\n        :rtype: `StaticRoute`\n        \"\"\"\n        params = {\n            \"gateway_ip\": gateway_ip,\n            \"metric\": metric,\n        }\n        if isinstance(source, Subnet):\n            params[\"source\"] = source.id\n        elif isinstance(source, int):\n            params[\"source\"] = source\n        if isinstance(destination, Subnet):\n            params[\"destination\"] = destination.id\n        elif isinstance(destination, int):\n            params[\"destination\"] = destination\n        return cls._object(await cls._handler.create(**params))", "response": "Create a StaticRoute in MAAS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_build_egg(self, req):\n\t\tfrom setuptools.command.easy_install import easy_install\n\t\tdist = Distribution({'script_args': ['easy_install']})\n\t\tdist.parse_config_files()\n\t\topts = dist.get_option_dict('easy_install')\n\t\tkeep = (\n\t\t\t'find_links', 'site_dirs', 'index_url', 'optimize',\n\t\t\t'site_dirs', 'allow_hosts'\n\t\t)\n\t\tfor key in list(opts):\n\t\t\tif key not in keep:\n\t\t\t\tdel opts[key]  # don't use any other settings\n\t\tif self.dependency_links:\n\t\t\tlinks = self.dependency_links[:]\n\t\t\tif 'find_links' in opts:\n\t\t\t\tlinks = opts['find_links'][1].split() + links\n\t\t\topts['find_links'] = ('setup', links)\n\t\tif self.allow_hosts:\n\t\t\topts['allow_hosts'] = ('test', self.allow_hosts)\n\t\tif self.index_url:\n\t\t\topts['index_url'] = ('test', self.index_url)\n\t\tinstall_dir_func = getattr(self, 'get_egg_cache_dir', _os.getcwd)\n\t\tinstall_dir = install_dir_func()\n\t\tcmd = easy_install(\n\t\t\tdist, args=[\"x\"], install_dir=install_dir,\n\t\t\texclude_scripts=True,\n\t\t\talways_copy=False, build_directory=None, editable=False,\n\t\t\tupgrade=False, multi_version=True, no_report=True, user=False\n\t\t)\n\t\tcmd.ensure_finalized()\n\t\treturn cmd.easy_install(req)", "response": "Specialized version of Distribution. fetch_build_egg that respects allow_hosts and index_url."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _assign_method(self, resource_class, method_type):\n        method_name = resource_class.get_method_name(\n            resource_class, method_type)\n        valid_status_codes = getattr(\n            resource_class.Meta,\n            'valid_status_codes',\n            DEFAULT_VALID_STATUS_CODES\n        )\n\n        # uid is now the first argument (after self)\n        @self._cache\n        def get(self, uid=None, method_type=method_type,\n                method_name=method_name,\n                valid_status_codes=valid_status_codes,\n                resource=resource_class, data=None, **kwargs):\n            uid = uid.lower() if isinstance(uid, str) else uid\n            return self.call_api(\n                method_type, method_name,\n                valid_status_codes, resource,\n                data, uid=uid, **kwargs)\n\n        # only GET method is used\n        setattr(\n            self, method_name,\n            types.MethodType(get, self)\n        )\n\n        # for easier listing of get methods\n        self._all_get_methods_names.append(method_name)", "response": "Assign a method to the cache object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _caching(self, disk_or_memory, cache_directory=None):\n        if disk_or_memory not in ('disk', 'memory'):\n            raise ValueError('Accepted values are \"disk\" or \"memory\"')\n\n        # Because of how BaseClient get methods are generated, they don't get a proper __name__.\n        # As such, it is hard to generate a specific cache directory name for each get method.\n        # Therefore, I decided to just generate a number for each folder, starting at zero.\n        # The same get methods get the same number every time because their order doesn't change.\n        # Also, variable is incremented inside a list because nonlocals are only python 3.0 and up.\n        get_methods_id = [0]\n\n        def memoize(func):\n            _global_cache_dir = ''\n\n            if disk_or_memory == 'disk':\n                if cache_directory:\n                    # Python 2 workaround\n                    if sys.version_info[0] == 2 and not isinstance(cache_directory, str):\n                        raise TypeError('expected str')\n\n                    _global_cache_dir = os.path.join(cache_directory, 'pokepy_cache')\n                    cache_dir = os.path.join(_global_cache_dir, str(get_methods_id[0]))\n                else:\n                    _global_cache_dir = appdirs.user_cache_dir('pokepy_cache', False,\n                                                               opinion=False)\n                    cache_dir = os.path.join(_global_cache_dir, str(get_methods_id[0]))\n\n                cache = FileCache('pokepy', flag='cs', app_cache_dir=cache_dir)\n                get_methods_id[0] += 1\n            else:  # 'memory'\n                cache = {}\n                _global_cache_dir = 'ram'\n\n            # global cache directory\n            # should only be set when setting the first get method\n            if not self._cache_location_global:\n                self._cache_location_global = _global_cache_dir\n\n            hits = [0]\n            misses = [0]\n\n            def cache_info():\n                return self._cache_info_(hits[0], misses[0], len(cache))\n\n            def cache_clear():\n                # global cache info\n                self._cache_hits_global -= hits[0]\n                self._cache_misses_global -= misses[0]\n                self._cache_len_global -= len(cache)\n                # local cache info\n                hits[0] = 0\n                misses[0] = 0\n\n                cache.clear()  # for disk-based cache, files are deleted but not the directories\n                if disk_or_memory == 'disk':\n                    cache.create()  # recreate cache file handles\n\n            def cache_location():\n                return 'ram' if disk_or_memory == 'memory' else cache.cache_dir\n\n            @functools.wraps(func)\n            def memoizer(*args, **kwargs):\n                # arguments to the get methods can be a value or uid=value\n                key = str(args[1]) if len(args) > 1 else str(kwargs.get(\"uid\"))\n\n                if key not in cache:\n                    # local and global cache info\n                    misses[0] += 1\n                    self._cache_misses_global += 1\n                    cache[key] = func(*args, **kwargs)\n                    self._cache_len_global += 1\n                else:\n                    self._cache_hits_global += 1  # global cache info\n                    hits[0] += 1  # local cache info\n                return cache[key]\n\n            memoizer.cache_info = cache_info\n            memoizer.cache_clear = cache_clear\n            memoizer.cache_location = cache_location\n            return memoizer\n\n        return memoize", "response": "Decorator that allows caching the outputs of BaseClient get methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the subresources of the resource object.", "response": "def set_subresources(self, **kwargs):\n        \"\"\"Same logic as the original except for the first 'if' clause.\"\"\"\n        for attribute_name, resource in self._subresource_map.items():\n            sub_attr = kwargs.get(attribute_name)\n            if sub_attr is None:\n                # Attribute was not found or is null\n                value = None\n            elif isinstance(sub_attr, list):\n                # A list of subresources is supported\n                value = [resource(**x) for x in sub_attr]\n            else:\n                # So is a single resource\n                value = resource(**sub_attr)\n            setattr(self, attribute_name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds base url based on config", "response": "def build_url_base(host, port, is_https):\n    \"\"\"\n    Make base of url based on config\n    \"\"\"\n    base = \"http\"\n    if is_https:\n        base += 's'\n\n    base += \"://\"\n    base += host\n\n    if port:\n        base += \":\"\n        base += str(port)\n\n    return base"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_about(self, element_to_query=None):\n\n        url = '%s/System/deviceInfo' % self._base\n        _LOGGING.info('url: %s', url)\n\n        response = requests.get(\n            url, auth=HTTPBasicAuth(self._username, self._password))\n\n        _LOGGING.debug('response: %s', response)\n        _LOGGING.debug(\"status_code %s\", response.status_code)\n\n        if response.status_code != 200:\n            log_response_errors(response)\n            return None\n\n        if element_to_query is None:\n            return response.text\n        else:\n            try:\n                tree = ElementTree.fromstring(response.text)\n\n                element_to_query = './/{%s}%s' % (\n                    self._xml_namespace, element_to_query)\n                result = tree.findall(element_to_query)\n                if len(result) > 0:\n                    _LOGGING.debug('element_to_query: %s result: %s',\n                                   element_to_query, result[0])\n\n                    return result[0].text.strip()\n                else:\n                    _LOGGING.error(\n                        'There was a problem finding element: %s',\n                        element_to_query)\n                    _LOGGING.error('Entire response: %s', response.text)\n\n            except AttributeError as attib_err:\n                _LOGGING.error('Entire response: %s', response.text)\n                _LOGGING.error(\n                    'There was a problem finding element:'\n                    ' %s AttributeError: %s', element_to_query, attib_err)\n                return\n        return", "response": "Get the value of a specific element from the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget current state of Motion Detection", "response": "def is_motion_detection_enabled(self):\n        \"\"\" Get current state of Motion Detection \"\"\"\n\n        response = requests.get(self.motion_url, auth=HTTPBasicAuth(\n            self._username, self._password))\n        _LOGGING.debug('Response: %s', response.text)\n\n        if response.status_code != 200:\n            _LOGGING.error(\n                \"There was an error connecting to %s\", self.motion_url)\n            _LOGGING.error(\"status_code %s\", response.status_code)\n            return\n\n        try:\n\n            tree = ElementTree.fromstring(response.text)\n            enabled_element = tree.findall(\n                './/{%s}enabled' % self._xml_namespace)\n            sensitivity_level_element = tree.findall(\n                './/{%s}sensitivityLevel' % self._xml_namespace)\n            if len(enabled_element) == 0:\n                _LOGGING.error(\"Problem getting motion detection status\")\n                return\n            if len(sensitivity_level_element) == 0:\n                _LOGGING.error(\"Problem getting sensitivityLevel status\")\n                return\n\n            result = enabled_element[0].text.strip()\n            _LOGGING.info(\n                'Current motion detection state? enabled: %s', result)\n\n            if int(sensitivity_level_element[0].text) == 0:\n                _LOGGING.warn(\n                    \"sensitivityLevel is 0.\")\n                sensitivity_level_element[0].text = str(\n                    self._sensitivity_level)\n                _LOGGING.info(\n                    \"sensitivityLevel now set to %s\", self._sensitivity_level)\n\n            if result == 'true':\n                # Save this for future switch off\n                self.xml_motion_detection_on = ElementTree.tostring(\n                    tree, encoding=XML_ENCODING)\n                enabled_element[0].text = 'false'\n                self.xml_motion_detection_off = ElementTree.tostring(\n                    tree, encoding=XML_ENCODING)\n                return True\n            else:\n                # Save this for future switch on\n                self.xml_motion_detection_off = ElementTree.tostring(\n                    tree, encoding=XML_ENCODING)\n                enabled_element[0].text = 'true'\n                self.xml_motion_detection_on = ElementTree.tostring(\n                    tree, encoding=XML_ENCODING)\n                return False\n\n        except AttributeError as attib_err:\n            _LOGGING.error(\n                'There was a problem parsing '\n                'camera motion detection state: %s', attib_err)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_motion_detection_xml(self, xml):\n\n        _LOGGING.debug('xml:')\n        _LOGGING.debug(\"%s\", xml)\n\n        headers = DEFAULT_HEADERS\n        headers['Content-Length'] = len(xml)\n        headers['Host'] = self._host\n        response = requests.put(self.motion_url, auth=HTTPBasicAuth(\n            self._username, self._password), data=xml, headers=headers)\n        _LOGGING.debug('request.headers:')\n        _LOGGING.debug('%s', response.request.headers)\n        _LOGGING.debug('Response:')\n        _LOGGING.debug('%s', response.text)\n\n        if response.status_code != 200:\n            _LOGGING.error(\n                \"There was an error connecting to %s\", self.motion_url)\n            _LOGGING.error(\"status_code %s\", response.status_code)\n            return\n\n        try:\n            tree = ElementTree.fromstring(response.text)\n            enabled_element = tree.findall(\n                './/{%s}statusString' % self._xml_namespace)\n            if len(enabled_element) == 0:\n                _LOGGING.error(\"Problem getting motion detection status\")\n                return\n\n            if enabled_element[0].text.strip() == 'OK':\n                _LOGGING.info('Updated successfully')\n\n        except AttributeError as attib_err:\n            _LOGGING.error(\n                'There was a problem parsing the response: %s', attib_err)\n            return", "response": "Put Motion Detection XML to the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlook up the DNS record for a domain and returns the result.", "response": "def lookup_domain(self, domain, lookup_record=None, **kw):\n        \"\"\"Looks up the DNS record for *domain* and returns:\n        \n        * None if it does not exist,\n        * The IP address if looking up the \"A\" record, or\n        * The list of hosts in the \"MX\" record.\n        \n        The return value, if treated as a boolean, says whether a domain exists.\n        \n        You can pass \"a\" or \"mx\" as the *lookup_record* parameter. Otherwise,\n        the *lookup_dns* parameter from the constructor is used.\n        \"a\" means verify that the domain exists.\n        \"mx\" means verify that the domain exists and specifies mail servers.\n        \"\"\"\n        import DNS\n        \n        lookup_record = lookup_record.lower() if lookup_record else self._lookup_dns\n        \n        if lookup_record not in ('a', 'mx'):\n            raise RuntimeError(\"Not a valid lookup_record value: \" + lookup_record)\n        \n        if lookup_record == \"a\":\n            request = DNS.Request(domain, **kw)\n            \n            try:\n                answers = request.req().answers\n            \n            except (DNS.Lib.PackError, UnicodeError):\n                # A part of the domain name is longer than 63.\n                return False\n            \n            if not answers:\n                return False\n            \n            result = answers[0]['data'] # This is an IP address\n            \n            if result in self.false_positive_ips: # pragma: no cover\n                return False\n            \n            return result\n        \n        try:\n            return DNS.mxlookup(domain)\n        \n        except UnicodeError:\n            pass\n        \n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef harvest(self, text):\n        for match in self.harvest_regex.finditer(text):\n            # TODO: optionally validate before yielding?\n            # TODO: keep a list of harvested but not validated?\n            yield match.group().replace('..', '.')", "response": "Iterator that yields the e - mail addresses contained in text."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef envelope(self):\n\t\tif not self.sender and not self.author:\n\t\t\traise ValueError(\"Unable to determine message sender; no author or sender defined.\")\n\n\t\treturn self.sender or self.author[0]", "response": "Returns the address of the envelope sender address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a date header string from a date value.", "response": "def _build_date_header_string(self, date_value):\n\t\t\"\"\"Gets the date_value (may be None, basestring, float or\n\t\tdatetime.datetime instance) and returns a valid date string as per\n\t\tRFC 2822.\"\"\"\n\t\tif isinstance(date_value, datetime):\n\t\t\tdate_value = time.mktime(date_value.timetuple())\n\t\tif not isinstance(date_value, basestring):\n\t\t\tdate_value = formatdate(date_value, localtime=True)\n\t\t# Encode it here to avoid this:\n\t\t# Date: =?utf-8?q?Sat=2C_01_Sep_2012_13=3A08=3A29_-0300?=\n\t\treturn native(date_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mime(self):\n\t\tauthor = self.author\n\t\tsender = self.sender\n\t\t\n\t\tif not author:\n\t\t\traise ValueError(\"You must specify an author.\")\n\n\t\tif not self.subject:\n\t\t\traise ValueError(\"You must specify a subject.\")\n\n\t\tif len(self.recipients) == 0:\n\t\t\traise ValueError(\"You must specify at least one recipient.\")\n\n\t\tif not self.plain:\n\t\t\traise ValueError(\"You must provide plain text content.\")\n\n\t\t# DISCUSS: Take the first author, or raise this error?\n\t\t# if len(author) > 1 and len(sender) == 0:\n\t\t#\t raise ValueError('If there are multiple authors of message, you must specify a sender!')\n\n\t\t# if len(sender) > 1:\n\t\t#\t raise ValueError('You must not specify more than one sender!')\n\n\t\tif not self._dirty and self._processed:\n\t\t\treturn self._mime\n\n\t\tself._processed = False\n\n\t\tplain = MIMEText(self._callable(self.plain), 'plain', self.encoding)\n\n\t\trich = None\n\t\tif self.rich:\n\t\t\trich = MIMEText(self._callable(self.rich), 'html', self.encoding)\n\n\t\tmessage = self._mime_document(plain, rich)\n\t\theaders = self._build_header_list(author, sender)\n\t\tself._add_headers_to_message(message, headers)\n\n\t\tself._mime = message\n\t\tself._processed = True\n\t\tself._dirty = False\n\n\t\treturn message", "response": "Produce the final MIME message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattaching a file to this message.", "response": "def attach(self, name, data=None, maintype=None, subtype=None,\n\t\tinline=False, filename=None, filename_charset='', filename_language='',\n\t\tencoding=None):\n\t\t\"\"\"Attach a file to this message.\n\n\t\t:param name: Path to the file to attach if data is None, or the name\n\t\t\t\t\t of the file if the ``data`` argument is given\n\t\t:param data: Contents of the file to attach, or None if the data is to\n\t\t\t\t\t be read from the file pointed to by the ``name`` argument\n\t\t:type data: bytes or a file-like object\n\t\t:param maintype: First part of the MIME type of the file -- will be\n\t\t\t\t\t\t automatically guessed if not given\n\t\t:param subtype: Second part of the MIME type of the file -- will be\n\t\t\t\t\t\tautomatically guessed if not given\n\t\t:param inline: Whether to set the Content-Disposition for the file to\n\t\t\t\t\t   \"inline\" (True) or \"attachment\" (False)\n\t\t:param filename: The file name of the attached file as seen\n\t\t\t\t\t\t\t\t\tby the user in his/her mail client.\n\t\t:param filename_charset: Charset used for the filename paramenter. Allows for \n\t\t\t\t\t\tattachment names with characters from UTF-8 or Latin 1. See RFC 2231.\n\t\t:param filename_language: Used to specify what language the filename is in. See RFC 2231.\n\t\t:param encoding: Value of the Content-Encoding MIME header (e.g. \"gzip\"\n\t\t\t\t\t\t in case of .tar.gz, but usually empty)\n\t\t\"\"\"\n\t\tself._dirty = True\n\n\t\tif not maintype:\n\t\t\tmaintype, guessed_encoding = guess_type(name)\n\t\t\tencoding = encoding or guessed_encoding\n\t\t\tif not maintype:\n\t\t\t\tmaintype, subtype = 'application', 'octet-stream'\n\t\t\telse:\n\t\t\t\tmaintype, _, subtype = maintype.partition('/')\n\n\t\tpart = MIMENonMultipart(maintype, subtype)\n\t\tpart.add_header('Content-Transfer-Encoding', 'base64')\n\n\t\tif encoding:\n\t\t\tpart.add_header('Content-Encoding', encoding)\n\n\t\tif data is None:\n\t\t\twith open(name, 'rb') as fp:\n\t\t\t\tvalue = fp.read()\n\t\t\tname = os.path.basename(name)\n\t\telif isinstance(data, bytes):\n\t\t\tvalue = data\n\t\telif hasattr(data, 'read'):\n\t\t\tvalue = data.read()\n\t\telse:\n\t\t\traise TypeError(\"Unable to read attachment contents\")\n\t\t\n\t\tpart.set_payload(base64.encodestring(value))\n\n\t\tif not filename:\n\t\t\tfilename = name\n\t\tfilename = os.path.basename(filename)\n\n\t\tif filename_charset or filename_language:\n\t\t\tif not filename_charset:\n\t\t\t\tfilename_charset = 'utf-8'\n\t\t\t# See https://docs.python.org/2/library/email.message.html#email.message.Message.add_header\n\t\t\t# for more information.\n\t\t\t# add_header() in the email module expects its arguments to be ASCII strings. Go ahead and handle\n\t\t\t# the case where these arguments come in as unicode strings, since encoding ASCII strings\n\t\t\t# as UTF-8 can't hurt.\n\t\t\tif sys.version_info < (3, 0):\n\t\t\t\tfilename=(filename_charset.encode('utf-8'), filename_language.encode('utf-8'), filename.encode('utf-8'))\n\t\t\telse:\n\t\t\t\tfilename=(filename_charset, filename_language, filename)\n\t\t\n\t\tif inline:\n\t\t\tif sys.version_info < (3, 0):\n\t\t\t\tpart.add_header('Content-Disposition'.encode('utf-8'), 'inline'.encode('utf-8'), filename=filename)\n\t\t\t\tpart.add_header('Content-ID'.encode('utf-8'), '<%s>'.encode('utf-8') % filename)\n\t\t\telse:\t\n\t\t\t\tpart.add_header('Content-Disposition', 'inline', filename=filename)\n\t\t\t\tpart.add_header('Content-ID', '<%s>' % filename)\n\t\t\tself.embedded.append(part)\n\t\telse:\n\t\t\tif sys.version_info < (3, 0):\n\t\t\t\tpart.add_header('Content-Disposition'.encode('utf-8'), 'attachment'.encode('utf-8'), filename=filename)\n\t\t\telse:\n\t\t\t\tpart.add_header('Content-Disposition', 'attachment', filename=filename)\n\t\t\tself.attachments.append(part)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattach an image file and prepare for HTML embedding.", "response": "def embed(self, name, data=None):\n\t\t\"\"\"Attach an image file and prepare for HTML embedding.\n\n\t\tThis method should only be used to embed images.\n\n\t\t:param name: Path to the image to embed if data is None, or the name\n\t\t\t\t\t of the file if the ``data`` argument is given\n\t\t:param data: Contents of the image to embed, or None if the data is to\n\t\t\t\t\t be read from the file pointed to by the ``name`` argument\n\t\t\"\"\"\n\t\tif data is None:\n\t\t\twith open(name, 'rb') as fp:\n\t\t\t\tdata = fp.read()\n\t\t\tname = os.path.basename(name)\n\t\telif isinstance(data, bytes):\n\t\t\tpass\n\t\telif hasattr(data, 'read'):\n\t\t\tdata = data.read()\n\t\telse:\n\t\t\traise TypeError(\"Unable to read image contents\")\n\n\t\tsubtype = imghdr.what(None, data)\n\t\tself.attach(name, data, 'image', subtype, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef startup(self):\n        \n        log.info(\"Immediate delivery manager starting.\")\n        \n        log.debug(\"Initializing transport queue.\")\n        self.transport.startup()\n        \n        log.info(\"Immediate delivery manager started.\")", "response": "Perform startup actions. This just chains down to the transport layer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef string_addresses(self, encoding=None):\n\t\t\n\t\tif not encoding:\n\t\t\tencoding = self.encoding\n\t\t\n\t\t# We need the punycode goodness.\n\t\treturn [Address(i.address).encode(encoding).decode(encoding) for i in self]", "response": "Return a list of string representations of the addresses suitable\n\tif usage in an SMTP transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites file to zip", "response": "def _write_to_zip(self, path, contents):\n        \"\"\" _write_to_zip: Write file to zip\n            Args:\n                path: (str) where in zip to write file\n                contents: (str) contents of file to write\n            Returns: None\n        \"\"\"\n        if isinstance(path, list):\n            path = os.path.sep.join(path)\n        self.zf.writestr(path, contents)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd folder to map", "response": "def _commit(self, path, title, source_id=None, description=None, author=None, language=None, license=None, copyright_holder=None, license_description=None, thumbnail=None):\n        \"\"\" _commit: Adds folder/file to map\n            Args:\n                path: (str) where in zip to write file\n                title: (str) content's title\n                source_id: (str) content's original id (optional)\n                description: (str) description of content (optional)\n                author (str): who created the content (optional)\n                language (str): language of content (optional)\n                license: (str) content's license (optional)\n                copyright_holder (str): holder of content's license (optional)\n                license_description (str): description of content's license (optional)\n                thumbnail (str):  path to thumbnail in zip (optional)\n            Returns: None\n        \"\"\"\n        node = self.map.get(path)\n        self.map.update({path: {\n            'title': title or node and node.get('path'),\n            'source_id': source_id or node and node.get('source_id'),\n            'description': description or node and node.get('description'),\n            'author': author or node and node.get('author'),\n            'language': language or node and node.get('language'),\n            'license': license or node and node.get('license'),\n            'copyright_holder': copyright_holder or node and node.get('copyright_holder'),\n            'license_description': license_description or node and node.get('license_description'),\n            'thumbnail': thumbnail or node and node.get('thumbnail'),\n        }})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_path(self, path):\n        paths = path.split('/')\n        current_path = paths[0]\n        for p in paths[1:]:\n            current_path = \"{}/{}\".format(current_path, p)\n            if not self.map.get(current_path): # Create any folders that might not exist yet\n                self._commit(current_path, p)", "response": "Go through the path and make sure that topics exist in the map."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the node metadata to the ZF file.", "response": "def _write_metadata(self):\n        \"\"\" _write_metadata: Writes node metadata to csv file\n            Args: None\n            Returns: None\n        \"\"\"\n        string_buffer = StringIO()\n        writer = csv.writer(string_buffer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(['Path *', 'Title *', 'Source ID', 'Description', 'Author', 'Language', 'License ID *', 'License Description', 'Copyright Holder', 'Thumbnail'])\n        for k in self.map:\n            node = self.map[k]\n            writer.writerow([k, node['title'], node['source_id'], node['description'], node['author'], node['language'], node['license'], node['license_description'], node['copyright_holder'], node['thumbnail']])\n\n        self.zf.writestr('Content.csv', string_buffer.getvalue())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a channel to the metadata file.", "response": "def add_channel(self, title, source_id, domain, language, description=None, thumbnail=None):\n        \"\"\" add_channel: Creates channel metadata\n            Args:\n                source_id: (str) channel's unique id\n                domain: (str) who is providing the content (e.g. learningequality.org)\n                title: (str): name of channel\n                language: (str): language code for channel (e.g. 'en')\n                description: (str) description of the channel (optional)\n                thumbnail: (str) path to thumbnail in zip (optional)\n            Returns: None\n        \"\"\"\n        string_buffer = StringIO()\n        writer = csv.writer(string_buffer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(['Title', 'Description', 'Domain', 'Source ID', 'Language', 'Thumbnail'])\n        writer.writerow([title, description, domain, source_id, language, thumbnail])\n        self.zf.writestr('Channel.csv', string_buffer.getvalue())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_folder(self, path, title, description=None, language=None, thumbnail=None, source_id=None, **node_data):\n        self._parse_path(path)\n        path = path if path.endswith(title) else \"{}/{}\".format(path, title)\n        self._commit(path, title, description=description, language=language, thumbnail=thumbnail, source_id=source_id)", "response": "Adds folder in csv\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_file(self, path, title, download_url, write_data=True, ext=None, license=None, copyright_holder=None, **node_data):\n        if write_data:\n            assert license, \"Files must have a license\"\n            copyright_holder = None if not copyright_holder or copyright_holder.strip() == '' else copyright_holder\n            assert license in NO_COPYRIGHT_HOLDER_REQUIRED or copyright_holder, \"Licenses must have a copyright holder if they are not public domain\"\n\n        self._parse_path(path)\n        if not ext:\n            _name, ext = os.path.splitext(download_url or \"\")\n            ext = ext.lower()  # normalize to lowercase extensions inside zip archive\n        filepath = \"{}/{}{}\".format(path, title, ext)\n        if download_url and filepath:\n            self._write_to_zip(filepath, read(download_url))\n            if write_data:\n                self._commit(filepath, title, license=license, copyright_holder=copyright_holder, **node_data)\n            return filepath", "response": "Adds a file to the archive and returns the path to the file in zip."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute_command_in_message(controller, cliargs, clioptions, message):\n    SUPPORTED_COMMANDS = ['start'] # , 'stop'] # TODO\n    print(message)\n\n    # args and options from SushiBar overrride command line args and options\n    args = cliargs\n    options = clioptions\n    if 'args' in message:\n        args.update(message['args'])\n    if 'options' in message:\n        options.update(message['options'])\n\n    if message['command'] == 'start':\n        if not controller.thread or not controller.thread.isAlive():\n            controller.thread = threading.Thread(\n                target=controller.chef.run,\n                args=(args, options),\n            )\n            controller.thread.start()\n        else:\n            config.LOGGER.info('Not starting because chef is already running.')\n    else:\n        config.LOGGER.info('Command not supported: %s' % message['command'])", "response": "Executes the command in message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a message to the remote server.", "response": "def send(self, data):\n        \"\"\"\n        This method keeps trying to send a message relying on the run method\n        to reopen the websocket in case it was closed.\n        \"\"\"\n        while not self.stopped():\n            try:\n                self.ws.send(data)\n                return\n            except websocket.WebSocketConnectionClosedException:\n                # config.LOGGER.debug('WebSocket closed, retrying send.') # TODO(investigate infinite loop)\n                time.sleep(0.1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a post request to create the channel run.", "response": "def __create_channel_run(self, channel, username, token):\n        \"\"\"Sends a post request to create the channel run.\"\"\"\n        data = {\n            'channel_id': channel.get_node_id().hex,\n            'chef_name': self.__get_chef_name(),\n            'ricecooker_version': __version__,\n            'started_by_user': username,\n            'started_by_user_token': token,\n            'content_server': config.DOMAIN,\n        }\n        try:\n            response = requests.post(\n                config.sushi_bar_channel_runs_url(),\n                data=data,\n                auth=AUTH)\n            response.raise_for_status()\n            return response.json()['run_id']\n        except Exception as e:\n            config.LOGGER.error('Error channel run: %s' % e)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self):\n        while True:\n            self.open_lock.acquire()\n            if self.stopped():\n                return\n            self.__open()\n            self.open_lock.release()", "response": "This override threading. Thread to open socket and wait for messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(path, loadjs=False, session=None, driver=None, timeout=60,\n        clear_cookies=True, loadjs_wait_time=3, loadjs_wait_for_callback=None):\n    \"\"\"Reads from source and returns contents\n    \n    Args:\n        path: (str) url or local path to download\n        loadjs: (boolean) indicates whether to load js (optional)\n        session: (requests.Session) session to use to download (optional)\n        driver: (selenium.webdriver) webdriver to use to download (optional)\n        timeout: (int) Maximum number of seconds to wait for the request to complete.\n        clear_cookies: (boolean) whether to clear cookies.\n        loadjs_wait_time: (int) if loading JS, seconds to wait after the\n            page has loaded before grabbing the page source\n        loadjs_wait_for_callback: (function<selenium.webdriver>) if loading\n            JS, a callback that will be invoked to determine when we can\n            grab the page source. The callback will be called with the\n            webdriver, and should return True when we're ready to grab the\n            page source. For example, pass in an argument like:\n            ``lambda driver: driver.find_element_by_id('list-container')``\n            to wait for the #list-container element to be present before rendering.\n    Returns: str content from file or page\n    \"\"\"\n    session = session or DOWNLOAD_SESSION\n\n    if clear_cookies:\n        session.cookies.clear()\n\n    try:\n        if loadjs:                                              # Wait until js loads then return contents\n            if PHANTOMJS_PATH:\n                driver = driver or webdriver.PhantomJS(executable_path=PHANTOMJS_PATH)\n            else:\n                driver = driver or webdriver.PhantomJS()\n            driver.get(path)\n            if loadjs_wait_for_callback:\n                selenium_ui.WebDriverWait(driver, 60).until(loadjs_wait_for_callback)\n            time.sleep(loadjs_wait_time)\n            return driver.page_source\n\n        else:                                                   # Read page contents from url\n            retry_count = 0\n            max_retries = 5\n            while True:\n                try:\n                    response = DOWNLOAD_SESSION.get(path, stream=True, timeout=timeout)\n                    break\n                except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout) as e:\n                    retry_count += 1\n                    print(\"Error with connection ('{msg}'); about to perform retry {count} of {trymax}.\"\n                        .format(msg=str(e), count=retry_count, trymax=max_retries))\n                    time.sleep(retry_count * 1)\n                    if retry_count >= max_retries:\n                        raise e\n\n            response.raise_for_status()\n            return response.content\n\n    except (requests.exceptions.MissingSchema, requests.exceptions.InvalidSchema):\n        with open(path, 'rb') as fobj:                          # If path is a local file path, try to open the file\n            return fobj.read()", "response": "Reads from source and returns contents of the page."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_static_assets(doc, destination, base_url,\n        request_fn=make_request, url_blacklist=[], js_middleware=None,\n        css_middleware=None, derive_filename=_derive_filename):\n    \"\"\"\n    Download all static assets referenced from an HTML page.\n    The goal is to easily create HTML5 apps! Downloads JS, CSS, images, and\n    audio clips.\n\n    Args:\n        doc: The HTML page source as a string or BeautifulSoup instance.\n        destination: The folder to download the static assets to!\n        base_url: The base URL where assets will be downloaded from.\n        request_fn: The function to be called to make requests, passed to\n            ricecooker.utils.html.download_file(). Pass in a custom one for custom\n            caching logic.\n        url_blacklist: A list of keywords of files to not include in downloading.\n            Will do substring matching, so e.g. 'acorn.js' will match\n            '/some/path/to/acorn.js'.\n        js_middleware: If specificed, JS content will be passed into this callback\n            which is expected to return JS content with any modifications.\n        css_middleware: If specificed, CSS content will be passed into this callback\n            which is expected to return CSS content with any modifications.\n\n    Return the modified page HTML with links rewritten to the locations of the\n    downloaded static files, as a BeautifulSoup object. (Call str() on it to\n    extract the raw HTML.)\n    \"\"\"\n    if not isinstance(doc, BeautifulSoup):\n        doc = BeautifulSoup(doc, \"html.parser\")\n\n    # Helper function to download all assets for a given CSS selector.\n    def download_assets(selector, attr, url_middleware=None,\n            content_middleware=None, node_filter=None):\n        nodes = doc.select(selector)\n\n        for i, node in enumerate(nodes):\n\n            if node_filter:\n                if not node_filter(node):\n                    src = node[attr]\n                    node[attr] = ''\n                    print('        Skipping node with src ', src)\n                    continue\n\n            if node[attr].startswith('data:'):\n                continue\n\n            url = urljoin(base_url, node[attr])\n\n            if _is_blacklisted(url, url_blacklist):\n                print('        Skipping downloading blacklisted url', url)\n                node[attr] = \"\"\n                continue\n\n            if url_middleware:\n                url = url_middleware(url)\n\n            filename = derive_filename(url)\n            node[attr] = filename\n\n            print(\"        Downloading\", url, \"to filename\", filename)\n            download_file(url, destination, request_fn=request_fn,\n                    filename=filename, middleware_callbacks=content_middleware)\n\n    def js_content_middleware(content, url, **kwargs):\n        if js_middleware:\n            content = js_middleware(content, url, **kwargs)\n\n        # Polyfill localStorage and document.cookie as iframes can't access\n        # them\n        return (content\n            .replace(\"localStorage\", \"_localStorage\")\n            .replace('document.cookie.split', '\"\".split')\n            .replace('document.cookie', 'window._document_cookie'))\n\n    def css_node_filter(node):\n        return \"stylesheet\" in node[\"rel\"]\n\n    def css_content_middleware(content, url, **kwargs):\n        if css_middleware:\n            content = css_middleware(content, url, **kwargs)\n\n        file_dir = os.path.dirname(urlparse(url).path)\n\n        # Download linked fonts and images\n        def repl(match):\n            src = match.group(1)\n            if src.startswith('//localhost'):\n                return 'url()'\n            # Don't download data: files\n            if src.startswith('data:'):\n                return match.group(0)\n            src_url = urljoin(base_url, os.path.join(file_dir, src))\n\n            if _is_blacklisted(src_url, url_blacklist):\n                print('        Skipping downloading blacklisted url', src_url)\n                return 'url()'\n\n            derived_filename = derive_filename(src_url)\n            download_file(src_url, destination, request_fn=request_fn,\n                    filename=derived_filename)\n            return 'url(\"%s\")' % derived_filename\n\n        return _CSS_URL_RE.sub(repl, content)\n\n    # Download all linked static assets.\n    download_assets(\"img[src]\", \"src\")  # Images\n    download_assets(\"link[href]\", \"href\",\n            content_middleware=css_content_middleware,\n            node_filter=css_node_filter)  # CSS\n    download_assets(\"script[src]\", \"src\",\n            content_middleware=js_content_middleware) # JS\n    download_assets(\"source[src]\", \"src\") # Potentially audio\n    download_assets(\"source[srcset]\", \"srcset\") # Potentially audio\n\n    # ... and also run the middleware on CSS/JS embedded in the page source to\n    # get linked files.\n    for node in doc.select('style'):\n        node.string = css_content_middleware(node.get_text(), url='')\n\n    for node in doc.select('script'):\n        if not node.attrs.get('src'):\n            node.string = js_content_middleware(node.get_text(), url='')\n\n    return doc", "response": "Download all static assets referenced from an HTML page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a channel path as obtained from a metadata provider into a rel_path.", "response": "def rel_path_from_chan_path(chan_path, channeldir, windows=False):\n    \"\"\"\n    Convert `chan_path` as obtained from a metadata provider into a `rel_path`\n    suitable for accessing the file from the current working directory, e.g.,\n    >>> rel_path_from_chan_path('Open Stax/Math', 'content/open_stax_zip/Open Stax')\n    'content/open_stax_zip/Open Stax/Math'\n    \"\"\"\n    if windows:\n        chan_path_list = chan_path.split('\\\\')\n    else:\n        chan_path_list = chan_path.split('/')\n    chan_path_list.pop(0)  # remove the channel root dir\n    rel_path = os.path.join(channeldir, *chan_path_list)\n    return rel_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a channel dict that contains a hierary of TopicNode dicts we use the to find the corresponding TopicNode.", "response": "def get_topic_for_path(channel, chan_path_tuple):\n    \"\"\"\n    Given channel (dict) that contains a hierary of TopicNode dicts, we use the\n    walk the path given in `chan_path_tuple` to find the corresponding TopicNode.\n    \"\"\"\n    assert chan_path_tuple[0] == channel['dirname'], 'Wrong channeldir'\n    chan_path_list = list(chan_path_tuple)\n    chan_path_list.pop(0)    # skip the channel name\n\n    if len(chan_path_list) == 0:\n        return channel\n\n    current = channel\n    for subtopic in chan_path_list:\n        current = list(filter(lambda d: 'dirname' in d and d['dirname'] == subtopic, current['children']))[0]\n    return current"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_filenames(filenames):\n    filenames_cleaned = []\n    for filename in filenames:\n        keep = True\n        for pattern in FILE_EXCLUDE_EXTENTIONS:\n            if filename.endswith(pattern):\n                keep = False\n        for pattern in FILE_SKIP_PATTENRS:   # This will reject exercises...\n            if pattern in filename:\n                keep = False\n        if keep:\n            filenames_cleaned.append(filename)\n    return filenames_cleaned", "response": "Filter filenames that contain EXTENTIONS and FILE_SKIP_PATTENRS."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_thumbnail_files(chan_path, filenames, metadata_provider):\n    thumbnail_files_to_skip = metadata_provider.get_thumbnail_paths()\n    filenames_cleaned = []\n    for filename in filenames:\n        keep = True\n        chan_filepath = os.path.join(chan_path, filename)\n        chan_filepath_tuple = path_to_tuple(chan_filepath)\n        if chan_filepath_tuple in thumbnail_files_to_skip:\n            keep = False\n        if keep:\n            filenames_cleaned.append(filename)\n    return filenames_cleaned", "response": "We don't want to create `ContentNode` from thumbnail files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkeeps only folders that don t contain patterns in DIR_EXCLUDE_PATTERNS.", "response": "def keep_folder(raw_path):\n    \"\"\"\n    Keep only folders that don't contain patterns in `DIR_EXCLUDE_PATTERNS`.\n    \"\"\"\n    keep = True\n    for pattern in DIR_EXCLUDE_PATTERNS:\n        if pattern in raw_path:\n            LOGGER.debug('rejecting', raw_path)\n            keep = False\n    return keep"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating `ContentNode`s from each file in this folder and the node to `channel` under the path `rel_path`.", "response": "def process_folder(channel, rel_path, filenames, metadata_provider):\n    \"\"\"\n    Create `ContentNode`s from each file in this folder and the node to `channel`\n    under the path `rel_path`.\n    \"\"\"\n    LOGGER.debug('IN process_folder ' + str(rel_path) + '     ' + str(filenames))\n    if not keep_folder(rel_path):\n        return\n\n    chan_path = chan_path_from_rel_path(rel_path, metadata_provider.channeldir)\n    chan_path_tuple = path_to_tuple(chan_path)\n    chan_path_list = list(chan_path_tuple)\n    LOGGER.debug('chan_path_list=' + str(chan_path_list))\n\n    # FIND THE CONTAINING NODE (channel or topic)\n    if len(chan_path_list) == 1:\n        # CASE CHANNEL ROOT: `rel_path` points to `channeldir`\n        # No need to create a topic node here since channel already exists\n        containing_node = channel  # attach content nodes in filenames directly to channel\n\n    else:\n        # CASE TOPIC FOLDER: `rel_path` points to a channelroot subfolder (a.k.a TopicNode)\n        dirname = chan_path_list.pop()  # name of the folder (used as ID for internal lookup)\n        topic_parent_node = get_topic_for_path(channel, chan_path_list)\n\n        # read topic metadata to get title and description for the TopicNode\n        topic_metadata = metadata_provider.get(chan_path_tuple)\n        thumbnail_chan_path =  topic_metadata.get('thumbnail_chan_path', None)\n        if thumbnail_chan_path:\n            thumbnail_rel_path = rel_path_from_chan_path(thumbnail_chan_path, metadata_provider.channeldir)\n        else:\n            thumbnail_rel_path = None\n        # create TopicNode for this folder\n        topic = dict(\n            kind=TOPIC_NODE,\n            dirname=dirname,\n            source_id='sourceid:' + rel_path,\n            title=topic_metadata.get('title', dirname),\n            description=topic_metadata.get('description', None),\n            author=topic_metadata.get('author', None),\n            language=topic_metadata.get('language', None),\n            license=topic_metadata.get('license', None),\n            thumbnail=thumbnail_rel_path,\n            children=[],\n        )\n        topic_parent_node['children'].append(topic)\n        containing_node = topic  # attach content nodes in filenames to the newly created topic\n\n    # filter filenames\n    filenames_cleaned = filter_filenames(filenames)\n    filenames_cleaned2 = filter_thumbnail_files(chan_path, filenames_cleaned, metadata_provider)\n\n    # PROCESS FILES\n    for filename in filenames_cleaned2:\n        chan_filepath = os.path.join(chan_path, filename)\n        chan_filepath_tuple = path_to_tuple(chan_filepath)\n        metadata = metadata_provider.get(chan_filepath_tuple)\n        node = make_content_node(metadata_provider.channeldir, rel_path, filename, metadata)\n        containing_node['children'].append(node)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_ricecooker_json_tree(args, options, metadata_provider, json_tree_path):\n    LOGGER.info('Starting to build the ricecooker_json_tree')\n\n    channeldir = args['channeldir']\n    if channeldir.endswith(os.path.sep):\n        channeldir.rstrip(os.path.sep)\n    channelparentdir, channeldirname = os.path.split(channeldir)\n    channelparentdir, channeldirname = os.path.split(channeldir)\n\n    # Ricecooker tree\n    channel_info = metadata_provider.get_channel_info()\n    thumbnail_chan_path =  channel_info.get('thumbnail_chan_path', None)\n    if thumbnail_chan_path:\n        thumbnail_rel_path = rel_path_from_chan_path(thumbnail_chan_path, metadata_provider.channeldir)\n    else:\n        thumbnail_rel_path = None\n\n    ricecooker_json_tree = dict(\n        dirname=channeldirname,\n        title=channel_info['title'],\n        description=channel_info['description'],\n        source_domain=channel_info['source_domain'],\n        source_id=channel_info['source_id'],\n        language=channel_info['language'],\n        thumbnail=thumbnail_rel_path,\n        children=[],\n    )\n    channeldir = args['channeldir']\n    content_folders = sorted(os.walk(channeldir))\n\n    # MAIN PROCESSING OF os.walk OUTPUT\n    ############################################################################\n    # TODO(ivan): figure out all the implications of the\n    # _ = content_folders.pop(0)  # Skip over channel folder because handled above\n    for rel_path, _subfolders, filenames in content_folders:\n        LOGGER.info('processing folder ' + str(rel_path))\n\n        # IMPLEMENTATION DETAIL:\n        #   - `filenames` contains real files in the `channeldir` folder\n        #   - `exercises_filenames` contains virtual files whose sole purpse is to set the\n        #     order of nodes within a given topic. Since alphabetical order is used to\n        #     walk the files in the `channeldir`, we must \"splice in\" the exercises here\n        if metadata_provider.has_exercises():\n            dir_chan_path = chan_path_from_rel_path(rel_path, metadata_provider.channeldir)\n            dir_path_tuple = path_to_tuple(dir_chan_path)\n            exercises_filenames = metadata_provider.get_exercises_for_dir(dir_path_tuple)\n            filenames.extend(exercises_filenames)\n\n        sorted_filenames = sorted(filenames)\n        process_folder(ricecooker_json_tree, rel_path, sorted_filenames, metadata_provider)\n\n    # Write out ricecooker_json_tree.json\n    write_tree_to_json_tree(json_tree_path, ricecooker_json_tree)\n    LOGGER.info('Folder hierarchy walk result stored in ' + json_tree_path)", "response": "Builds the ricecooker json tree from the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_content_node(channeldir, rel_path, filename, metadata):\n    file_key, file_ext = os.path.splitext(filename)\n    ext = file_ext[1:]\n    kind = None\n    if ext in content_kinds.MAPPING:\n        kind = content_kinds.MAPPING[ext]   # guess what kind based on file extension\n    elif 'questions' in metadata:\n        kind = content_kinds.EXERCISE\n    else:\n        raise ValueError('Could not find kind for extension ' + str(ext) + ' in content_kinds.MAPPING')\n\n    # Extract metadata fields\n    source_id = metadata.get('source_id', None)\n    if source_id is None:\n        source_id = metadata['chan_path']\n\n    filepath = os.path.join(rel_path, filename)\n    title = metadata['title']\n    description = metadata.get('description', None)\n    author = metadata.get('author', None)\n    lang =  metadata.get('language', None)\n    license_dict = metadata.get('license', None)\n    thumbnail_chan_path =  metadata.get('thumbnail_chan_path', None)\n    if thumbnail_chan_path:\n        thumbnail_rel_path = rel_path_from_chan_path(thumbnail_chan_path, channeldir)\n    else:\n        thumbnail_rel_path = None\n\n    if kind == VIDEO_NODE:\n        content_node = dict(\n            kind=VIDEO_NODE,\n            source_id=source_id,\n            title=title,\n            author=author,\n            description=description,\n            language=lang,\n            license=license_dict,\n            derive_thumbnail=True,  # video-specific option\n            thumbnail=thumbnail_rel_path,\n            files=[{'file_type':VIDEO_FILE, 'path':filepath, 'language':lang}], # ffmpeg_settings={\"crf\": 24},\n        )\n\n    elif kind == AUDIO_NODE:\n        content_node = dict(\n            kind=AUDIO_NODE,\n            source_id=source_id,\n            title=title,\n            author=author,\n            description=description,\n            language=lang,\n            license=license_dict,\n            thumbnail=thumbnail_rel_path,\n            files=[{'file_type':AUDIO_FILE, 'path':filepath, 'language':lang}],\n        )\n\n    elif kind == DOCUMENT_NODE:\n        content_node = dict(\n            kind=DOCUMENT_NODE,\n            source_id=source_id,\n            title=title,\n            author=author,\n            description=description,\n            language=lang,\n            license=license_dict,\n            thumbnail=thumbnail_rel_path,\n            files=[]\n        )\n        if ext == 'pdf':\n            pdf_file = {\n                'file_type':DOCUMENT_FILE,\n                'path':filepath,\n                'language':lang\n            }\n            content_node['files'].append(pdf_file)\n        elif ext == 'epub':\n            epub_file = {\n                'file_type':EPUB_FILE,\n                'path':filepath,\n                'language':lang\n            }\n            content_node['files'].append(epub_file)\n        else:\n            raise ValueError('Ext {} not supported for kind {}'.format(ext, kind))\n\n    elif kind == HTML5_NODE:\n        content_node = dict(\n            kind=HTML5_NODE,\n            source_id=source_id,\n            title=title,\n            author=author,\n            description=description,\n            language=lang,\n            license=license_dict,\n            thumbnail=thumbnail_rel_path,\n            files=[{'file_type':HTML5_FILE, 'path':filepath, 'language':lang}],\n        )\n\n    elif kind == EXERCISE_NODE:\n        content_node = dict(\n            kind=EXERCISE_NODE,\n            source_id=source_id,\n            title=title,\n            author=author,\n            description=description,\n            language=lang,\n            license=license_dict,\n            exercise_data=metadata['exercise_data'],\n            questions=metadata['questions'],\n            thumbnail=thumbnail_rel_path,\n            files=[],\n        )\n\n    else:\n        raise ValueError('Not implemented case for kind ' + str(kind))\n\n    return content_node", "response": "Create a ContentNode based on the file extension and metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the path to the storage directory for downloading content .", "response": "def get_storage_path(filename):\n    \"\"\" get_storage_path: returns path to storage directory for downloading content\n        Args: filename (str): Name of file to store\n        Returns: string path to file\n    \"\"\"\n    directory = os.path.join(STORAGE_DIRECTORY, filename[0], filename[1])\n    # Make storage directory for downloaded files if it doesn't already exist\n    if not os.path.exists(directory) :\n        os.makedirs(directory)\n\n    return os.path.join(directory, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the file mapping store for the current language.", "response": "def init_file_mapping_store():\n    \"\"\" init_file_mapping_store: creates log to keep track of downloaded files\n        Args: None\n        Returns: None\n    \"\"\"\n    # Make storage directory for restore files if it doesn't already exist\n    path = os.path.join(RESTORE_DIRECTORY, FILE_STORE_LOCATION)\n    if not os.path.exists(path):\n        os.makedirs(path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn path to directory for restoration points", "response": "def get_restore_path(filename):\n    \"\"\" get_restore_path: returns path to directory for restoration points\n        Args:\n            filename (str): Name of file to store\n        Returns: string path to file\n    \"\"\"\n    path = os.path.join(RESTORE_DIRECTORY, FILE_STORE_LOCATION)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return os.path.join(path, filename + '.pickle')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_channel_url(channel, staging=False):\n    return OPEN_CHANNEL_URL.format(domain=DOMAIN, channel_id=channel, access='staging' if staging or STAGE else 'edit')", "response": "Returns url to open channel"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, *path):\n        self.path = [self.channel_name]\n        self.path.extend(list(path))", "response": "Set path from root\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_predictable_zip(path):\n    # if path is a directory, recursively enumerate all the files under the directory\n    if os.path.isdir(path): \n        paths = []\n        for root, directories, filenames in os.walk(path):\n            paths += [os.path.join(root, filename)[len(path)+1:] for filename in filenames]\n        reader = lambda x: _read_file(os.path.join(path, x))\n    # otherwise, if it's a zip file, open it up and pull out the list of names\n    elif os.path.isfile(path) and os.path.splitext(path)[1] == \".zip\":\n        inputzip = zipfile.ZipFile(path)\n        paths = inputzip.namelist()\n        reader = lambda x: inputzip.read(x)\n    else:\n        raise Exception(\"The `path` must either point to a directory or to a zip file.\")\n\n    # create a temporary zip file path to write the output into\n    zippathfd, zippath = tempfile.mkstemp(suffix=\".zip\")\n\n    with zipfile.ZipFile(zippath, \"w\") as outputzip:\n        # loop over the file paths in sorted order, to ensure a predictable zip\n        for filepath in sorted(paths):\n            write_file_to_zip_with_neutral_metadata(outputzip, filepath, reader(filepath))\n        os.fdopen(zippathfd).close()\n    return zippath", "response": "Create a predictable zip file with predictable sort order and metadata so that MD5 will be consistent with the same content twice."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the string content into filename in the zip file.", "response": "def write_file_to_zip_with_neutral_metadata(zfile, filename, content):\n    \"\"\"\n    Write the string `content` to `filename` in the open ZipFile `zfile`.\n    Args:\n        zfile (ZipFile): open ZipFile to write the content into\n        filename (str): the file path within the zip file to write into\n        content (str): the content to write into the zip\n    Returns: None\n    \"\"\"\n    info = zipfile.ZipInfo(filename, date_time=(2015, 10, 21, 7, 28, 0))\n    info.compress_type = zipfile.ZIP_DEFLATED\n    info.comment = \"\".encode()\n    info.create_system = 0\n    zfile.writestr(info, content)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_abspath(path, content_dir=CONTENT_DIR):\n    if path:\n        file = re.search('content://(.+)', path)\n        if file:\n            return os.path.join(content_dir, file.group(1))\n    return path", "response": "Returns absolute path of path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nguess the file type of a file based on the kind of file.", "response": "def guess_file_type(kind, filepath=None, youtube_id=None, web_url=None, encoding=None):\n    \"\"\" guess_file_class: determines what file the content is\n        Args:\n            filepath (str): filepath of file to check\n        Returns: string indicating file's class\n    \"\"\"\n    if youtube_id:\n        return FileTypes.YOUTUBE_VIDEO_FILE\n    elif web_url:\n        return FileTypes.WEB_VIDEO_FILE\n    elif encoding:\n        return FileTypes.BASE64_FILE\n    else:\n        ext = os.path.splitext(filepath)[1][1:].lower()\n        if kind in FILE_TYPE_MAPPING and ext in FILE_TYPE_MAPPING[kind]:\n            return FILE_TYPE_MAPPING[kind][ext]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef guess_content_kind(path=None, web_video_data=None, questions=None):\n    # If there are any questions, return exercise\n    if questions and len(questions) > 0:\n        return content_kinds.EXERCISE\n\n    # See if any files match a content kind\n    if path:\n        ext = os.path.splitext(path)[1][1:].lower()\n        if ext in content_kinds.MAPPING:\n            return content_kinds.MAPPING[ext]\n        raise InvalidFormatException(\"Invalid file type: Allowed formats are {0}\".format([key for key, value in content_kinds.MAPPING.items()]))\n    elif web_video_data:\n        return content_kinds.VIDEO\n    else:\n        return content_kinds.TOPIC", "response": "Guesses what kind of content is the node s content is\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the tree from a source tree.", "response": "def _build_tree(node, sourcetree):\n    \"\"\"\n    Parse nodes given in `sourcetree` and add as children of `node`.\n    \"\"\"\n    for child_source_node in sourcetree:\n        try:\n            main_file = child_source_node['files'][0] if 'files' in child_source_node else {}\n            kind = guess_content_kind(path=main_file.get('path'), web_video_data=main_file.get('youtube_id') or main_file.get('web_url'), questions=child_source_node.get(\"questions\"))\n        except UnknownContentKindError:\n            continue\n\n        if kind == content_kinds.TOPIC:\n            child_node = nodes.TopicNode(\n                source_id=child_source_node[\"id\"],\n                title=child_source_node[\"title\"],\n                author=child_source_node.get(\"author\"),\n                description=child_source_node.get(\"description\"),\n                thumbnail=child_source_node.get(\"thumbnail\"),\n            )\n            node.add_child(child_node)\n\n            source_tree_children = child_source_node.get(\"children\", [])\n\n            _build_tree(child_node, source_tree_children)\n\n        elif kind == content_kinds.VIDEO:\n            child_node = nodes.VideoNode(\n                source_id=child_source_node[\"id\"],\n                title=child_source_node[\"title\"],\n                license=get_license(child_source_node.get(\"license\"), description=\"Description of license\", copyright_holder=child_source_node.get('copyright_holder')),\n                author=child_source_node.get(\"author\"),\n                description=child_source_node.get(\"description\"),\n                derive_thumbnail=True, # video-specific data\n                thumbnail=child_source_node.get('thumbnail'),\n            )\n            add_files(child_node, child_source_node.get(\"files\") or [])\n            node.add_child(child_node)\n\n        elif kind == content_kinds.AUDIO:\n            child_node = nodes.AudioNode(\n                source_id=child_source_node[\"id\"],\n                title=child_source_node[\"title\"],\n                license=child_source_node.get(\"license\"),\n                author=child_source_node.get(\"author\"),\n                description=child_source_node.get(\"description\"),\n                thumbnail=child_source_node.get(\"thumbnail\"),\n                copyright_holder=child_source_node.get(\"copyright_holder\"),\n            )\n            add_files(child_node, child_source_node.get(\"files\") or [])\n            node.add_child(child_node)\n\n        elif kind == content_kinds.DOCUMENT:\n            child_node = nodes.DocumentNode(\n                source_id=child_source_node[\"id\"],\n                title=child_source_node[\"title\"],\n                license=child_source_node.get(\"license\"),\n                author=child_source_node.get(\"author\"),\n                description=child_source_node.get(\"description\"),\n                thumbnail=child_source_node.get(\"thumbnail\"),\n                copyright_holder=child_source_node.get(\"copyright_holder\"),\n            )\n            add_files(child_node, child_source_node.get(\"files\") or [])\n            node.add_child(child_node)\n\n        elif kind == content_kinds.EXERCISE:\n            mastery_model = (child_source_node.get('mastery_model') and {\"mastery_model\": child_source_node['mastery_model']}) or {}\n            child_node = nodes.ExerciseNode(\n                source_id=child_source_node[\"id\"],\n                title=child_source_node[\"title\"],\n                license=child_source_node.get(\"license\"),\n                author=child_source_node.get(\"author\"),\n                description=child_source_node.get(\"description\"),\n                exercise_data=mastery_model,\n                thumbnail=child_source_node.get(\"thumbnail\"),\n                copyright_holder=child_source_node.get(\"copyright_holder\"),\n            )\n            add_files(child_node, child_source_node.get(\"files\") or [])\n            for q in child_source_node.get(\"questions\"):\n                question = create_question(q)\n                child_node.add_question(question)\n            node.add_child(child_node)\n\n        elif kind == content_kinds.HTML5:\n            child_node = nodes.HTML5AppNode(\n                source_id=child_source_node[\"id\"],\n                title=child_source_node[\"title\"],\n                license=child_source_node.get(\"license\"),\n                author=child_source_node.get(\"author\"),\n                description=child_source_node.get(\"description\"),\n                thumbnail=child_source_node.get(\"thumbnail\"),\n                copyright_holder=child_source_node.get(\"copyright_holder\"),\n            )\n            add_files(child_node, child_source_node.get(\"files\") or [])\n            node.add_child(child_node)\n\n        else:                   # unknown content file format\n            continue\n\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef construct_channel(self, *args, **kwargs):\n        channel = self.get_channel(*args, **kwargs)   # creates ChannelNode from data in self.channel_info\n        _build_tree(channel, SAMPLE_TREE)\n        raise_for_invalid_channel(channel)\n\n        return channel", "response": "Create a ChannelNode and build topic tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nputting data in format CC expects", "response": "def to_dict(self):\n        \"\"\" to_dict: puts data in format CC expects\n            Args: None\n            Returns: dict of node's data\n        \"\"\"\n        return {\n            \"assessment_id\": self.id.hex,\n            \"type\": self.question_type,\n            \"files\": [f.to_dict() for f in filter(lambda x: x and x.filename, self.files)],\n            \"question\": self.question,\n            \"hints\": json.dumps(self.hints, ensure_ascii=False),\n            \"answers\": json.dumps(self.answers, ensure_ascii=False),\n            \"raw_data\": self.raw_data,\n            \"source_url\": self.source_url,\n            \"randomize\": self.randomize,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse data that needs to have image strings processed Returns list of all downloaded files", "response": "def process_question(self):\n        \"\"\" process_question: Parse data that needs to have image strings processed\n            Args: None\n            Returns: list of all downloaded files\n        \"\"\"\n        # Process question\n        self.question, question_files = self.set_images(self.question)\n\n        # Process answers\n        answers = []\n        answer_files = []\n        answer_index = 0\n        for answer in self.answers:\n            processed_string, afiles = self.set_images(answer['answer'])\n            answers.append({\"answer\": processed_string, \"correct\": answer['correct'], \"order\": answer_index})\n            answer_index += 1\n            answer_files += afiles\n        self.answers = answers\n\n        # Process hints\n        hints = []\n        hint_files = []\n        hint_index = 0\n        for hint in self.hints:\n            processed_string, hfiles = self.set_images(hint)\n            hints.append({\"hint\": processed_string, \"order\": hint_index})\n            hint_index += 1\n            hint_files += hfiles\n        self.hints = hints\n\n        self.files += question_files + answer_files + hint_files\n        return [f.filename for f in self.files]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up image strings in place of image strings with downloaded image checksums", "response": "def set_images(self, text, parse_html=True):\n        \"\"\" set_images: Replace image strings with downloaded image checksums\n            Args:\n                text (str): text to parse for image strings\n            Returns:string with checksums in place of image strings and\n                list of files that were downloaded from string\n        \"\"\"\n        # Set up return values and regex\n        file_list = []\n        if parse_html:\n            processed_string = self.parse_html(text)\n        else:\n            processed_string = text\n        reg = re.compile(MARKDOWN_IMAGE_REGEX, flags=re.IGNORECASE)\n        matches = reg.findall(processed_string)\n\n        # Parse all matches\n        for match in matches:\n            file_result = self.set_image(match[1])\n            if file_result[0] != \"\":\n                replacement, new_files = file_result\n                processed_string = processed_string.replace(match[1], replacement)\n                file_list += new_files\n        return processed_string, file_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the assessment item s image at the given text.", "response": "def set_image(self, text):\n        \"\"\"\n        Save image resource at `text` (path or url) to storage, then return the\n        replacement string and the necessary exercicse image file object.\n        Args:\n          - text (str): path or url to parse as an exercise image resource\n        Returns: (new_text, files)\n          - `new_text` (str): replacement string for the original `text` string\n          - `files` (list): list of files that were downloaded from `text`\n        \"\"\"\n        # Make sure `text` hasn't already been processed\n        if exercises.CONTENT_STORAGE_PLACEHOLDER in text:\n            return text, []\n        # Strip `text` of whitespace\n        stripped_text = text.strip().replace('\\\\n', '')\n        # If `stripped_text` is a web+graphie: path, we need special processing\n        graphie_regex = re.compile(WEB_GRAPHIE_URL_REGEX, flags=re.IGNORECASE)\n        graphie_match = graphie_regex.match(stripped_text)\n        if graphie_match:\n            is_web_plus_graphie = True\n            graphie_rawpath = graphie_match.groupdict()['rawpath']\n            graphie_path = graphie_rawpath.replace(\"//\", \"https://\")\n            exercise_image_file = _ExerciseGraphieFile(graphie_path)\n        elif get_base64_encoding(stripped_text):\n            is_web_plus_graphie = False\n            exercise_image_file = _ExerciseBase64ImageFile(stripped_text)\n        else:\n            is_web_plus_graphie = False\n            exercise_image_file = _ExerciseImageFile(stripped_text)\n        # Setup link to assessment item\n        exercise_image_file.assessment_item = self\n        # Process file to make the replacement_str available\n        _filename = exercise_image_file.process_file()\n        # Get `new_text` = the replacement path for the image resource\n        new_text = exercises.CONTENT_STORAGE_FORMAT.format(exercise_image_file.get_replacement_str())\n        if is_web_plus_graphie:     # need to put back the `web+graphie:` prefix\n            new_text = \"web+graphie:\" + new_text\n        return new_text, [exercise_image_file]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self):\n        assert self.id is not None, \"Assumption Failed: Question must have an id\"\n        assert isinstance(self.question, str) or self.question is None, \"Assumption Failed: Question must be a string\"\n        assert isinstance(self.question_type, str), \"Assumption Failed: Question type must be a string\"\n        assert isinstance(self.answers, list), \"Assumption Failed: Answers must be a list\"\n        assert isinstance(self.hints, list), \"Assumption Failed: Hints must be a list\"\n        for a in self.answers:\n            assert isinstance(a, dict), \"Assumption Failed: Answer in answer list is not a dict\"\n        for h in self.hints:\n            assert isinstance(h, str), \"Assumption Failed: Hint in hints list is not a string\"\n        return True", "response": "Makes sure question is valid for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the Perseus Question is valid.", "response": "def validate(self):\n        \"\"\" validate: Makes sure perseus question is valid\n            Args: None\n            Returns: boolean indicating if perseus question is valid\n        \"\"\"\n        try:\n            assert self.question == \"\", \"Assumption Failed: Perseus question should not have a question\"\n            assert self.question_type == exercises.PERSEUS_QUESTION, \"Assumption Failed: Question should be perseus type\"\n            assert self.answers == [], \"Assumption Failed: Answer list should be empty for perseus question\"\n            assert self.hints == [], \"Assumption Failed: Hints list should be empty for perseus question\"\n            return super(PerseusQuestion, self).validate()\n        except AssertionError as ae:\n            raise InvalidQuestionException(\"Invalid question: {0}\".format(self.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_question(self):\n        image_files = []\n        question_data = json.loads(self.raw_data)\n\n        # process urls for widgets\n        self._recursive_url_find(question_data, image_files)\n\n        # Process question\n        if 'question' in question_data and 'images' in question_data['question']:\n            question_data['question']['images'], qfiles = self.process_image_field(question_data['question'])\n            image_files += qfiles\n\n        # Process hints\n        if 'hints' in question_data:\n            for hint in question_data['hints']:\n                if 'images' in hint:\n                    hint['images'], hfiles = self.process_image_field(hint)\n                    image_files += hfiles\n\n        # Process answers\n        if 'answers' in question_data:\n            for answer in question_data['answers']:\n                if 'images' in answer:\n                    answer['images'], afiles = self.process_image_field(answer)\n                    image_files += afiles\n\n        # Process raw data\n        self.raw_data = json.dumps(question_data, ensure_ascii=False)\n        # Assume no need for special HTML processing for Persues questions\n        # This avoids probelms with questions that contain < and > inequalities\n        # in formulas that get erroneously parsed as HTML tags\n        self.raw_data, data_files = super(PerseusQuestion, self).set_images(self.raw_data, parse_html=False)\n\n        # Combine all files processed\n        self.files = image_files + data_files\n\n        # Return all filenames\n        return [f.filename for f in self.files]", "response": "Process specific fields in question that need to have image strings that need to be parsed as HTML tags and add them to the list of files needed to render this question."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_image_field(self, data):\n        new_images_dict = copy.deepcopy(data['images'])\n        image_files = []\n\n        # STEP 1. Compile dict of {old_url-->new_url} image URL replacements\n        image_replacements = {}\n\n        # STEP 1A. get all images specified in data['images']\n        for old_url, image_settings in data['images'].items():\n            new_url, new_image_files = self.set_image(old_url)\n            image_files += new_image_files\n            new_images_dict[new_url] = new_images_dict.pop(old_url)\n            image_replacements[old_url] = new_url\n\n        # STEP 1B. look for additional `MARKDOWN_IMAGE_REGEX`-like link in `content` attr.\n        img_link_pat = re.compile(MARKDOWN_IMAGE_REGEX, flags=re.IGNORECASE)\n        img_link_matches = img_link_pat.findall(data['content'])\n        for match in img_link_matches:\n            old_url = match[1]\n            if old_url not in image_replacements.keys():\n                new_url, new_image_files = self.set_image(old_url)\n                image_files += new_image_files\n                image_replacements[old_url] = new_url\n\n        # Performd content replacent for all URLs in image_replacements\n        for old_url, new_url in image_replacements.items():\n            data['content'] = data['content'].replace(old_url, new_url)\n\n        return new_images_dict, image_files", "response": "Process perseus fields like questions and hints which look like a question and hints and returns a dictionary of all the fields that look like a question and hints."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate(self):\n        try:\n            assert self.question_type == exercises.MULTIPLE_SELECTION, \"Assumption Failed: Question should be multiple selection type\"\n            assert len(self.answers) > 0, \"Assumption Failed: Multiple selection question should have answers\"\n            for a in self.answers:\n                assert 'answer' in a and isinstance(a['answer'], str), \"Assumption Failed: Answer in answer list is not a string\"\n                assert 'correct' in a and isinstance(a['correct'], bool), \"Assumption Failed: Correct indicator is not a boolean in answer list\"\n            for h in self.hints:\n                assert isinstance(h, str), \"Assumption Failed: Hint in hint list is not a string\"\n            return super(MultipleSelectQuestion, self).validate()\n        except AssertionError as ae:\n            raise InvalidQuestionException(\"Invalid question: {0}\".format(self.__dict__))", "response": "Validate that the Multiple Select Question is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates that the single selection question is valid.", "response": "def validate(self):\n        \"\"\" validate: Makes sure single selection question is valid\n            Args: None\n            Returns: boolean indicating if single selection question is valid\n        \"\"\"\n        try:\n            assert self.question_type == exercises.SINGLE_SELECTION, \"Assumption Failed: Question should be single selection type\"\n            assert len(self.answers) > 0, \"Assumption Failed: Multiple selection question should have answers\"\n            correct_answers = 0\n            for a in self.answers:\n                assert 'answer' in a and isinstance(a['answer'], str), \"Assumption Failed: Answer in answer list is not a string\"\n                assert 'correct' in a and isinstance(a['correct'], bool), \"Assumption Failed: Correct indicator is not a boolean in answer list\"\n                correct_answers += 1 if a['correct'] else 0\n            assert correct_answers == 1, \"Assumption Failed: Single selection question should have only one correct answer\"\n            for h in self.hints:\n                assert isinstance(h, str), \"Assumption Failed: Hint in hints list is not a string\"\n            return super(SingleSelectQuestion, self).validate()\n        except AssertionError as ae:\n            raise InvalidQuestionException(\"Invalid question: {0}\".format(self.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self):\n        try:\n            assert self.question_type == exercises.INPUT_QUESTION, \"Assumption Failed: Question should be input answer type\"\n            assert len(self.answers) > 0, \"Assumption Failed: Multiple selection question should have answers\"\n            for a in self.answers:\n                assert 'answer' in a, \"Assumption Failed: Answers must have an answer field\"\n                try:\n                    float(a['answer'])\n                except ValueError:\n                    assert False, \"Assumption Failed: Answer {} must be numeric\".format(a['answer'])\n            for h in self.hints:\n                assert isinstance(h, str), \"Assumption Failed: Hint in hints list is not a string\"\n            return super(InputQuestion, self).validate()\n        except AssertionError as ae:\n            raise InvalidQuestionException(\"Invalid question: {0}\".format(self.__dict__))", "response": "Validate that the input question is valid for this instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_content_curation_token(args_token):\n    if args_token != \"#\":                               # retrieval methods 1, 2\n        if os.path.isfile(args_token):\n            with open(args_token, 'r') as fobj:\n                return fobj.read().strip()\n        else:\n            return args_token\n    else:                                               # retrieval strategies 3\n        token = get_env('STUDIO_TOKEN') or get_env('CONTENT_CURATION_TOKEN')\n        if token is not None:\n            return token                                # 3a\n        else:\n            return prompt_token(config.DOMAIN)", "response": "Get the content curation token through one of four possible strategies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprompting user to enter content curation server authentication token.", "response": "def prompt_token(domain):\n    \"\"\"\n    Prompt user to enter content curation server authentication token.\n    Args: domain (str): domain to authenticate user\n    Returns: token\n    \"\"\"\n    token = input(\"\\nEnter content curation server token ('q' to quit): \").lower()\n    if token == 'q':\n        sys.exit()\n    else:\n        return token.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tree_for_studio_id(self, studio_id):\n        channel_parent = {'children': []}  # this is like _ with children\n        def _build_subtree(parent, studio_id):\n            subtree = self.get_nodes_by_ids_complete(studio_id)\n            if 'children' in subtree:\n                children_refs = subtree['children']\n                subtree['children'] = []\n                for child_studio_id in children_refs:\n                    _build_subtree(subtree, child_studio_id)\n            parent['children'].append(subtree)\n        _build_subtree(channel_parent, studio_id)\n        channel = channel_parent['children'][0]\n        return channel", "response": "Returns the full json tree for a given studio_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading ricecooker json tree data from json file at srcpath.", "response": "def read_tree_from_json(srcpath):\n    \"\"\"\n    Load ricecooker json tree data from json file at `srcpath`.\n    \"\"\"\n    with open(srcpath) as infile:\n        json_tree = json.load(infile)\n        if json_tree is None:\n            raise ValueError('Could not find ricecooker json tree')\n    return json_tree"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_tree_to_json_tree(destpath, json_tree):\n    parent_dir, _ = os.path.split(destpath)\n    if not os.path.exists(parent_dir):\n        os.makedirs(parent_dir, exist_ok=True)\n    with open(destpath, 'w', encoding='utf8') as json_file:\n        json.dump(json_tree, json_file, indent=2, ensure_ascii=False)", "response": "Save contents of json_tree to json file at destpath."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_channel_node_from_json(json_tree):\n    channel = ChannelNode(\n        title=json_tree['title'],\n        description=json_tree['description'],\n        source_domain=json_tree['source_domain'],\n        source_id=json_tree['source_id'],\n        language=json_tree['language'],\n        thumbnail=json_tree.get('thumbnail', None),\n    )\n    return channel", "response": "Build a ChannelNode from json data provided in json_tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a tree from a JSON representation of a channel node.", "response": "def build_tree_from_json(parent_node, sourcetree):\n    \"\"\"\n    Recusively parse nodes in the list `sourcetree` and add them as children\n    to the `parent_node`. Usually called with `parent_node` being a `ChannelNode`.\n    \"\"\"\n    EXPECTED_NODE_TYPES = [TOPIC_NODE, VIDEO_NODE, AUDIO_NODE, EXERCISE_NODE,\n                           DOCUMENT_NODE, HTML5_NODE]\n\n    for source_node in sourcetree:\n        kind = source_node['kind']\n        if kind not in EXPECTED_NODE_TYPES:\n            LOGGER.critical('Unexpected node kind found: ' + kind)\n            raise NotImplementedError('Unexpected node kind found in json data.')\n\n        if kind == TOPIC_NODE:\n            child_node = nodes.TopicNode(\n                source_id=source_node.get('source_id', None),\n                title=source_node['title'],\n                description=source_node.get('description'),\n                author=source_node.get('author'),\n                aggregator=source_node.get('aggregator'),\n                provider=source_node.get('provider'),\n                # no role for topics (computed dynaically from descendants)\n                language=source_node.get('language'),\n                thumbnail=source_node.get('thumbnail'),\n            )\n            parent_node.add_child(child_node)\n            source_tree_children = source_node.get('children', [])\n            build_tree_from_json(child_node, source_tree_children)\n\n        elif kind == VIDEO_NODE:\n            child_node = nodes.VideoNode(\n                source_id=source_node['source_id'],\n                title=source_node['title'],\n                description=source_node.get('description'),\n                license=get_license(**source_node['license']),\n                author=source_node.get('author'),\n                aggregator=source_node.get('aggregator'),\n                provider=source_node.get('provider'),\n                role=source_node.get('role', roles.LEARNER),\n                language=source_node.get('language'),\n                derive_thumbnail=source_node.get('derive_thumbnail', True),  # video-specific option\n                thumbnail=source_node.get('thumbnail'),\n            )\n            add_files(child_node, source_node.get('files') or [])\n            parent_node.add_child(child_node)\n\n        elif kind == AUDIO_NODE:\n            child_node = nodes.AudioNode(\n                source_id=source_node['source_id'],\n                title=source_node['title'],\n                description=source_node.get('description'),\n                license=get_license(**source_node['license']),\n                author=source_node.get('author'),\n                aggregator=source_node.get('aggregator'),\n                provider=source_node.get('provider'),\n                role=source_node.get('role', roles.LEARNER),\n                language=source_node.get('language'),\n                thumbnail=source_node.get('thumbnail'),\n            )\n            add_files(child_node, source_node.get('files') or [])\n            parent_node.add_child(child_node)\n\n        elif kind == EXERCISE_NODE:\n            child_node = nodes.ExerciseNode(\n                source_id=source_node['source_id'],\n                title=source_node['title'],\n                description=source_node.get('description'),\n                license=get_license(**source_node['license']),\n                author=source_node.get('author'),\n                aggregator=source_node.get('aggregator'),\n                provider=source_node.get('provider'),\n                role=source_node.get('role', roles.LEARNER),\n                language=source_node.get('language'),\n                thumbnail=source_node.get('thumbnail'),\n                exercise_data=source_node.get('exercise_data'),\n                questions=[],\n            )\n            add_questions(child_node, source_node.get('questions') or [])\n            parent_node.add_child(child_node)\n\n        elif kind == DOCUMENT_NODE:\n            child_node = nodes.DocumentNode(\n                source_id=source_node['source_id'],\n                title=source_node['title'],\n                description=source_node.get('description'),\n                license=get_license(**source_node['license']),\n                author=source_node.get('author'),\n                aggregator=source_node.get('aggregator'),\n                provider=source_node.get('provider'),\n                role=source_node.get('role', roles.LEARNER),\n                language=source_node.get('language'),\n                thumbnail=source_node.get('thumbnail'),\n            )\n            add_files(child_node, source_node.get('files') or [])\n            parent_node.add_child(child_node)\n\n        elif kind == HTML5_NODE:\n            child_node = nodes.HTML5AppNode(\n                source_id=source_node['source_id'],\n                title=source_node['title'],\n                description=source_node.get('description'),\n                license=get_license(**source_node['license']),\n                author=source_node.get('author'),\n                aggregator=source_node.get('aggregator'),\n                provider=source_node.get('provider'),\n                role=source_node.get('role', roles.LEARNER),\n                language=source_node.get('language'),\n                thumbnail=source_node.get('thumbnail'),\n            )\n            add_files(child_node, source_node.get('files') or [])\n            parent_node.add_child(child_node)\n\n        else:\n            LOGGER.critical('Encountered an unknown kind: ' + str(source_node))\n            continue\n\n    return parent_node"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse all known command line arguments and also additional key = value options.", "response": "def parse_args_and_options(self):\n        \"\"\"\n        Parses all known command line args and also additional key=value options.\n        NOTE: this should be the only place cli args are parsed in order to have\n        a single consistent interface for all chef scripts.\n\n        Args: None, but implicitly depends on `self.arg_parser` and `sys.argv`\n        Returns:\n          tuple (`args`, `options`)\n            args (dict): chef command line arguments\n            options (dict): extra compatibility-mode options given on command line\n        \"\"\"\n        args_namespace, options_list = self.arg_parser.parse_known_args()\n        args = args_namespace.__dict__\n\n        # Make sure token is provided. There are four possible ways to specify:\n        #   --token=path to token-containing file\n        #   --token=140fefe...1f3\n        #   when --token is not specified on the command line, the default value is #\n        #     - try environment variable CONTENT_CURATION_TOKEN\n        #     - else prompt user\n        # If ALL of these fail, this call will raise and chef run will stop\n        args['token'] = get_content_curation_token(args['token'])\n\n        # Parse additional compatibility mode keyword arguments from `options_list`\n        options = {}\n        for preoption in options_list:\n            try:\n                option_key, option_value = preoption.split('=')\n                options.update({option_key.strip(): option_value.strip()})\n            except IndexError:\n                msg = \"Invalid option '{0}': use [key]=[value] format (no whitespace)\".format(preoption)\n                raise InvalidUsageException(msg)\n\n        # For compatibility mode, we check the chef script file exists and load it\n        if self.compatibility_mode:\n            try:\n                # Try to load the chef_script as a module\n                self.chef_module = SourceFileLoader(\"mod\", args['chef_script']).load_module()\n            except FileNotFoundError as e:\n                raise InvalidUsageException('Error: must specify `chef_module` for compatibility_mode')\n\n        return args, options"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_channel(self, **kwargs):\n        if self.compatibility_mode:\n            # For pre-sushibar scritps that do not implement `get_channel`,\n            # we must check it this function exists before calling it...\n            if hasattr(self.chef_module, 'get_channel'):\n                config.LOGGER.info(\"Calling get_channel... \")\n                # Create channel (using the function in the chef script)\n                channel = self.chef_module.get_channel(**kwargs)\n            # For chefs with a `create_channel` method instead of `get_channel`\n            if hasattr(self.chef_module, 'create_channel'):\n                config.LOGGER.info(\"Calling create_channel... \")\n                # Create channel (using the function in the chef script)\n                channel = self.chef_module.create_channel(**kwargs)\n            else:\n                channel = None  # since no channel info, SushiBar functionality will be disabled...\n            return channel\n\n        elif hasattr(self, 'channel_info'):\n            # If a sublass has an `channel_info` attribute (a dict) it doesn't need\n            # to define a `get_channel` method and instead rely on this code:\n            channel = ChannelNode(\n                source_domain=self.channel_info['CHANNEL_SOURCE_DOMAIN'],\n                source_id=self.channel_info['CHANNEL_SOURCE_ID'],\n                title=self.channel_info['CHANNEL_TITLE'],\n                thumbnail=self.channel_info.get('CHANNEL_THUMBNAIL'),\n                language=self.channel_info.get('CHANNEL_LANGUAGE'),\n                description=self.channel_info.get('CHANNEL_DESCRIPTION'),\n            )\n            return channel\n\n        else:\n            raise NotImplementedError('BaseChef must overrride the get_channel method')", "response": "Returns a ChannelNode instance from the chef script s get_channel method in compatibility mode... or..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a channel from a chef script.", "response": "def construct_channel(self, **kwargs):\n        \"\"\"\n        Calls chef script's construct_channel method. Used only in compatibility mode.\n        Args:\n            kwargs (dict): additional keyword arguments that `uploadchannel` received\n        Returns: channel populated from construct_channel method\n        \"\"\"\n        if self.compatibility_mode:\n            # Constuct channel (using function from imported chef script)\n            config.LOGGER.info(\"Populating channel... \")\n            channel = self.chef_module.construct_channel(**kwargs)\n            return channel\n        else:\n            raise NotImplementedError('Your chef class must overrride the construct_channel method')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef daemon_mode(self, args, options):\n        cws = ControlWebSocket(self, args, options)\n        cws.start()\n        if 'cmdsock' in args and args['cmdsock']:\n            lcs = LocalControlSocket(self, args, options)\n            lcs.start()\n            lcs.join()\n        cws.join()", "response": "Open a ControlWebSocket to SushiBar server and listend for remote commands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, args, options):\n        config.LOGGER.info('In SushiChef.run method. args=' + str(args) + 'options=' + str(options))\n        self.pre_run(args, options)\n        uploadchannel_wrapper(self, args, options)", "response": "This function calls pre_run and post_run methods to run all the steps."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_json_tree_path(self, *args, **kwargs):\n        json_tree_path = os.path.join(self.TREES_DATA_DIR, self.RICECOOKER_JSON_TREE)\n        return json_tree_path", "response": "Returns the path to the ricecooker json tree file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing the channel tree by adding TopicNodes and ContentNode children.", "response": "def construct_channel(self, **kwargs):\n        \"\"\"\n        Build the channel tree by adding TopicNodes and ContentNode children.\n        \"\"\"\n        channel = self.get_channel(**kwargs)\n        json_tree_path = self.get_json_tree_path(**kwargs)\n        json_tree = read_tree_from_json(json_tree_path)\n        build_tree_from_json(channel, json_tree['children'])\n        raise_for_invalid_channel(channel)\n        return channel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pre_run(self, args, options):\n        if 'generate' in args and args['generate']:\n            self.metadata_provider = CsvMetadataProvider(args['channeldir'],\n                                                    channelinfo=args['channelinfo'],\n                                                    contentinfo=args['contentinfo'],\n                                                    exercisesinfo=args['exercisesinfo'],\n                                                    questionsinfo=args['questionsinfo'],\n                                                    validate_and_cache=False)\n            self.metadata_provider.generate_templates(exercise_questions=True)\n            self.metadata_provider.generate_contentinfo_from_channeldir(args, options)\n            sys.exit(0)\n\n        elif 'importstudioid' in args and args['importstudioid']:\n            studio_id = args['importstudioid']\n            config.LOGGER.info(\"Calling with importstudioid... \" + studio_id)\n            self.metadata_provider = CsvMetadataProvider(args['channeldir'],\n                                                    channelinfo=args['channelinfo'],\n                                                    contentinfo=args['contentinfo'],\n                                                    exercisesinfo=args['exercisesinfo'],\n                                                    questionsinfo=args['questionsinfo'],\n                                                    validate_and_cache=False)\n            self.metadata_provider.generate_templates(exercise_questions=True)\n            self.metadata_provider.generate_exercises_from_importstudioid(args, options)\n            sys.exit(0)\n\n        if self.metadata_provider is None:\n            self._init_metadata_provider(args, options)\n        kwargs = {}   # combined dictionary of argparse args and extra options\n        kwargs.update(args)\n        kwargs.update(options)\n        json_tree_path = self.get_json_tree_path(**kwargs)\n        build_ricecooker_json_tree(args, options, self.metadata_provider, json_tree_path)", "response": "This function is called before the run method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_relative_url(url, filename=None, baseurl=None, subpath=None):\n\n    # ensure the provided subpath is a list\n    if isinstance(subpath, str):\n        subpath = subpath.strip(\"/\").split(\"/\")\n    elif subpath is None:\n        subpath = []\n\n    # if a base path was supplied, calculate the file's subpath relative to it\n    if baseurl:\n        baseurl = urllib.parse.urljoin(baseurl, \".\")  # ensure baseurl is normalized (to remove '/./' and '/../')\n        assert url.startswith(baseurl), \"URL must start with baseurl\"\n        subpath = subpath + url[len(baseurl):].strip(\"/\").split(\"/\")[:-1]\n\n    # if we don't have a filename, extract it from the URL\n    if not filename:\n        filename = unquote(urlparse(url).path.split(\"/\")[-1])\n\n    # calculate the url path to use to refer to this file, relative to destpath\n    relative_file_url = \"/\".join([\".\"] + subpath + [filename])\n\n    return relative_file_url, subpath, filename", "response": "Calculate the relative path for a URL relative to a base URL possibly also injecting in a subpath prefix."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads a file from a URL into a destination folder.", "response": "def download_file(url, destpath, filename=None, baseurl=None, subpath=None, middleware_callbacks=None, middleware_kwargs=None, request_fn=sess.get):\n    \"\"\"\n    Download a file from a URL, into a destination folder, with optional use of relative paths and middleware processors.\n\n    - If `filename` is set, that will be used as the name of the file when it's written to the destpath.\n    - If `baseurl` is specified, the file will be put into subdirectory of destpath per the url's path relative to the baseurl.\n    - If `subpath` is specified, it will be appended to destpath before deciding where to write the file.\n    - If `middleware_callbacks` is specified, the returned content will be passed through those function(s) before being returned.\n        - If `middleware_kwargs` are also specified, they will also be passed in to each function in middleware_callbacks.\n    \"\"\"\n\n    relative_file_url, subpath, filename = calculate_relative_url(url, filename=filename, baseurl=baseurl, subpath=subpath)\n\n    # ensure that the destination directory exists\n    fulldestpath = os.path.join(destpath, *subpath)\n    os.makedirs(fulldestpath, exist_ok=True)\n\n    # make the actual request to the URL\n    response = request_fn(url)\n    content = response.content\n\n    # if there are any middleware callbacks, apply them to the content\n    if middleware_callbacks:\n        content = content.decode()\n        if not isinstance(middleware_callbacks, list):\n            middleware_callbacks = [middleware_callbacks]\n        kwargs = {\n            \"url\": url,\n            \"destpath\": destpath,\n            \"filename\": filename,\n            \"baseurl\": baseurl,\n            \"subpath\": subpath,\n            \"fulldestpath\": fulldestpath,\n            \"response\": response,\n        }\n        kwargs.update(middleware_kwargs or {})\n        for callback in middleware_callbacks:\n            content = callback(content, **kwargs)\n\n    # ensure content is encoded, as we're doing a binary write\n    if isinstance(content, str):\n        content = content.encode()\n\n    # calculate the final destination for the file, and write the content out to there\n    dest = os.path.join(fulldestpath, filename)\n    with open(dest, \"wb\") as f:\n        f.write(content)\n\n    return relative_file_url, response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses the tree of files and returns a list of unique file names.", "response": "def process_tree(self, channel_node):\n        \"\"\"\n        Returns a list of all file names associated with a tree. Profiling suggests using a global list with `extend`\n        is faster than using a global set or deque.\n        :param channel_node: Root node of the channel being processed\n        :return: The list of unique file names in `channel_node`.\n        \"\"\"\n        file_names = []\n        self.process_tree_recur(file_names, channel_node)\n        return [x for x in set(file_names) if x]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the names of all the files associated with the sub-tree rooted by `node` to `file_names` in post-order. :param file_names: A global list containing all file names associated with a tree :param node: The root of the current sub-tree being processed :return: None.", "response": "def process_tree_recur(self, file_names, node):\n        \"\"\"\n        Adds the names of all the files associated with the sub-tree rooted by `node` to `file_names` in post-order.\n        :param file_names: A global list containing all file names associated with a tree\n        :param node: The root of the current sub-tree being processed\n        :return: None.\n        \"\"\"\n        # Process node's children\n        for child_node in node.children:\n            self.process_tree_recur(file_names, child_node)  # Call children first in case a tiled thumbnail is needed\n\n        file_names.extend(node.process_files())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting any files that failed during download process", "response": "def check_for_files_failed(self):\n        \"\"\" check_for_files_failed: print any files that failed during download process\n            Args: None\n            Returns: None\n        \"\"\"\n        if len(config.FAILED_FILES) > 0:\n            config.LOGGER.error(\"   {} file(s) have failed to download\".format(len(config.FAILED_FILES)))\n            for f in config.FAILED_FILES:\n                title = \"{0} {id}\".format(f.node.kind.capitalize(), id=f.node.source_id)\\\n                        if f.node else \"{0} {id}\".format(\"Question\", id=f.assessment_item.source_id)\n                file_identifier = f.__dict__\n                if hasattr(f, 'path') and f.path:\n                    file_identifier = f.path\n                elif hasattr(f, 'youtube_url') and f.youtube_url:\n                    file_identifier = f.youtube_url\n                config.LOGGER.warning(\"\\t{0}: {id} \\n\\t   {err}\".format(title, id=file_identifier, err=f.error))\n        else:\n            config.LOGGER.info(\"   All files were successfully downloaded\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_file_diff(self, files_to_diff):\n        file_diff_result = []\n        chunks = [files_to_diff[x:x+1000] for x in range(0, len(files_to_diff), 1000)]\n        file_count = 0\n        total_count = len(files_to_diff)\n        for chunk in chunks:\n            response = config.SESSION.post(config.file_diff_url(), data=json.dumps(chunk))\n            response.raise_for_status()\n            file_diff_result += json.loads(response._content.decode(\"utf-8\"))\n            file_count += len(chunk)\n            config.LOGGER.info(\"\\tGot file diff for {0} out of {1} files\".format(file_count, total_count))\n\n        return file_diff_result", "response": "Get list of files that do not exist on content curation server\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads files to server and stores the result in self. failed_uploads.", "response": "def upload_files(self, file_list):\n        \"\"\" upload_files: uploads files to server\n            Args:\n                file_list (str): list of files to upload\n            Returns: None\n        \"\"\"\n        counter = 0\n        files_to_upload = list(set(file_list) - set(self.uploaded_files)) # In case restoring from previous session\n        try:\n            for f in files_to_upload:\n                with open(config.get_storage_path(f), 'rb') as file_obj:\n                    response = config.SESSION.post(config.file_upload_url(), files={'file': file_obj})\n                    if response.status_code == 200:\n                        response.raise_for_status()\n                        self.uploaded_files.append(f)\n                        counter += 1\n                        config.LOGGER.info(\"\\tUploaded {0} ({count}/{total}) \".format(f, count=counter, total=len(files_to_upload)))\n                    else:\n                        self.failed_uploads[f] = response._content.decode('utf-8')\n        finally:\n            config.PROGRESS_MANAGER.set_uploading(self.uploaded_files)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattempts to upload failed files to server .", "response": "def reattempt_upload_fails(self):\n        \"\"\" reattempt_upload_fails: uploads failed files to server\n            Args: None\n            Returns: None\n        \"\"\"\n        if len(self.failed_uploads) > 0:\n            config.LOGGER.info(\"\\nReattempting to upload {0} file(s)...\".format(len(self.failed_uploads)))\n            current_fails = [k for k in self.failed_uploads]\n            self.failed_uploads = {}\n            self.upload_files(current_fails)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends processed channel data to server to create tree", "response": "def upload_tree(self):\n        \"\"\" upload_tree: sends processed channel data to server to create tree\n            Args: None\n            Returns: link to uploadedchannel\n        \"\"\"\n        from datetime import datetime\n        start_time = datetime.now()\n        root, channel_id = self.add_channel()\n        self.node_count_dict = {\"upload_count\": 0, \"total_count\": self.channel.count()}\n\n        config.LOGGER.info(\"\\tPreparing fields...\")\n        self.truncate_fields(self.channel)\n\n        self.add_nodes(root, self.channel)\n        if self.check_failed(print_warning=False):\n            failed = self.failed_node_builds\n            self.failed_node_builds = {}\n            self.reattempt_failed(failed)\n            self.check_failed()\n        channel_id, channel_link = self.commit_channel(channel_id)\n        end_time = datetime.now()\n        config.LOGGER.info(\"Upload time: {time}s\".format(time=(end_time - start_time).total_seconds()))\n        return channel_id, channel_link"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_channel(self):\n        config.LOGGER.info(\"   Creating channel {0}\".format(self.channel.title))\n        payload = {\n            \"channel_data\":self.channel.to_dict(),\n        }\n        response = config.SESSION.post(config.create_channel_url(), data=json.dumps(payload))\n        response.raise_for_status()\n        new_channel = json.loads(response._content.decode(\"utf-8\"))\n\n        return new_channel['root'], new_channel['channel_id']", "response": "Adds a new channel to the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_nodes(self, root_id, current_node, indent=1):\n        # if the current node has no children, no need to continue\n        if not current_node.children:\n            return\n\n        config.LOGGER.info(\"({count} of {total} uploaded) {indent}Processing {title} ({kind})\".format(\n            count=self.node_count_dict['upload_count'],\n            total=self.node_count_dict['total_count'],\n            indent=\"   \" * indent,\n            title=current_node.title,\n            kind=current_node.__class__.__name__)\n        )\n\n        # Send children in chunks to avoid gateway errors\n        try:\n            chunks = [current_node.children[x:x+10] for x in range(0, len(current_node.children), 10)]\n            for chunk in chunks:\n                payload_children = []\n\n                for child in chunk:\n                    failed = [f for f in child.files if f.is_primary and (not f.filename or self.failed_uploads.get(f.filename))]\n                    if any(failed):\n                        if not self.failed_node_builds.get(root_id):\n                            error_message = \"\"\n                            for fail in failed:\n                                reason = fail.filename + \": \" + self.failed_uploads.get(fail.filename) if fail.filename else \"File failed to download\"\n                                error_message = error_message + reason + \", \"\n                            self.failed_node_builds[root_id] = {'node': current_node, 'error': error_message[:-2]}\n                    else:\n                        payload_children.append(child.to_dict())\n                payload = {\n                    'root_id': root_id,\n                    'content_data': payload_children\n                }\n\n                # When iceqube is integrated, use this method to utilize upload file optimizations\n                # response = config.SESSION.post(config.add_nodes_from_file_url(), files={'file': json.dumps(payload)})\n\n                response = config.SESSION.post(config.add_nodes_url(), data=json.dumps(payload))\n                if response.status_code != 200:\n                    self.failed_node_builds[root_id] = {'node': current_node, 'error': response.reason}\n                else:\n                    response_json = json.loads(response._content.decode(\"utf-8\"))\n                    self.node_count_dict['upload_count'] += len(chunk)\n\n                    if response_json['root_ids'].get(child.get_node_id().hex):\n                        for child in chunk:\n                            self.add_nodes(response_json['root_ids'].get(child.get_node_id().hex), child, indent + 1)\n        except ConnectionError as ce:\n            self.failed_node_builds[root_id] = {'node': current_node, 'error': ce}", "response": "Adds processed nodes to the tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncommit the current state of a Kolibri Studio channel to Kolibri Studio.", "response": "def commit_channel(self, channel_id):\n        \"\"\" commit_channel: commits channel to Kolibri Studio\n            Args:\n                channel_id (str): channel's id on Kolibri Studio\n            Returns: channel id and link to uploadedchannel\n        \"\"\"\n        payload = {\n            \"channel_id\":channel_id,\n            \"stage\": config.STAGE,\n        }\n        response = config.SESSION.post(config.finish_channel_url(), data=json.dumps(payload))\n        if response.status_code != 200:\n            config.LOGGER.error(\"\\n\\nCould not activate channel: {}\\n\".format(response._content.decode('utf-8')))\n            if response.status_code == 403:\n                config.LOGGER.error(\"Channel can be viewed at {}\\n\\n\".format(config.open_channel_url(channel_id, staging=True)))\n                sys.exit()\n        response.raise_for_status()\n        new_channel = json.loads(response._content.decode(\"utf-8\"))\n        channel_link = config.open_channel_url(new_channel['new_channel'])\n        return channel_id, channel_link"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef publish(self, channel_id):\n        payload = {\n            \"channel_id\":channel_id,\n        }\n        response = config.SESSION.post(config.publish_channel_url(), data=json.dumps(payload))\n        response.raise_for_status()", "response": "publishes tree to Kolibri\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the filename from a url url = http://abc. com", "response": "def get_name_from_url(url):\n    \"\"\"\n    get the filename from a url \n    url = http://abc.com/xyz.txt\n    get_name_from_url(url) -> xyz.txt\n    \"\"\"\n\n    head, tail = ntpath.split(url)\n    params_index = tail.find(\"&\")\n    if params_index != -1:\n        tail = tail[:params_index]\n    params_index = tail.find(\"?\")\n    if params_index != -1:\n        tail = tail[:params_index]\n\n    basename = ntpath.basename(url)\n    params_b_index = basename.find(\"&\")\n    if params_b_index != -1:\n        basename = basename[:params_b_index]\n    return tail or basename"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a linear directory structure from a list of path levels names builds. chefdir. trees. test. chefdir. trees. test. chefdir builds. chefdir. trees. test. chefdir builds. chefdir builds. test. chefdir builds. test", "response": "def build_path(levels):\n    \"\"\"\n    make a linear directory structure from a list of path levels names\n    levels = [\"chefdir\", \"trees\", \"test\"]\n    builds ./chefdir/trees/test/\n    \"\"\"\n    path = os.path.join(*levels)\n    if not dir_exists(path):\n        os.makedirs(path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a session exists in the backup", "response": "def check_for_session(self, status=None):\n        \"\"\" check_for_session: see if session is in progress\n            Args:\n                status (str): step to check if last session reached (optional)\n            Returns: boolean indicating if session exists\n        \"\"\"\n        status = Status.LAST if status is None else status\n        return os.path.isfile(self.get_restore_path(status)) and os.path.getsize(self.get_restore_path(status)) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_restore_path(self, status=None):\n        status = self.get_status() if status is None else status\n        return config.get_restore_path(status.name.lower())", "response": "get path to restoration file for the current state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the current state of the current state of the current state to respective restoration file.", "response": "def __record_progress(self, next_step=None):\n        \"\"\" __record_progress: save progress to respective restoration file\n            Args: None\n            Returns: None\n        \"\"\"\n        config.SUSHI_BAR_CLIENT.report_progress(\n            self.get_status(), self.get_status().value/Status.DONE.value)\n        if next_step:\n            now = time.time()\n            config.SUSHI_BAR_CLIENT.report_stage(self.get_status(), now - self.timestamp)\n            self.timestamp = now\n            self.status = next_step\n        with open(self.get_restore_path(Status.LAST), 'wb') as handle, open(self.get_restore_path(), 'wb') as step_handle:\n            pickle.dump(self, handle)\n            pickle.dump(self, step_handle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_progress(self, resume_step):\n        resume_step = Status[resume_step]\n        progress_path = self.get_restore_path(resume_step)\n\n        # If progress is corrupted, revert to step before\n        while not self.check_for_session(resume_step):\n            config.LOGGER.error(\"Ricecooker has not reached {0} status. Reverting to earlier step...\".format(resume_step.name))\n            # All files are corrupted or absent, restart process\n            if resume_step.value - 1 < 0:\n                self.init_session()\n                return self\n            resume_step = Status(resume_step.value - 1)\n            progress_path = self.get_restore_path(resume_step)\n        config.LOGGER.error(\"Starting from status {0}\".format(resume_step.name))\n\n        # Load manager\n        with open(progress_path, 'rb') as handle:\n            manager = pickle.load(handle)\n            if isinstance(manager, RestoreManager):\n                return manager\n            else:\n                return self", "response": "Loads the manager with progress from the given step."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_session(self):\n        # Clear out previous session's restoration files\n        for status in Status:\n            path = self.get_restore_path(status)\n            if os.path.isfile(path):\n                os.remove(path)\n\n        self.__record_progress()\n        self.__record_progress(Status.CONSTRUCT_CHANNEL)", "response": "Sets the session to beginning"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrecording progress from constructed channel", "response": "def set_channel(self, channel):\n        \"\"\" set_channel: records progress from constructed channel\n            Args: channel (Channel): channel Ricecooker is creating\n            Returns: None\n        \"\"\"\n        self.channel = channel\n        self.__record_progress(Status.CREATE_TREE)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_tree(self, tree):\n        self.tree = tree\n        self.__record_progress(Status.DOWNLOAD_FILES)", "response": "Sets the tree to be used for the Ricecooker."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_files(self, files_downloaded, files_failed):\n        self.files_downloaded = files_downloaded\n        self.files_failed = files_failed\n        self.__record_progress(Status.GET_FILE_DIFF)", "response": "Sets the files that have been downloaded and failed to download."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_diff(self, file_diff):\n        self.file_diff = file_diff\n        self.__record_progress(Status.START_UPLOAD)", "response": "Sets the file_diff property of the object that was created on Kolibri Studio\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_uploading(self, files_uploaded):\n        self.files_uploaded = files_uploaded\n        self.__record_progress(Status.UPLOADING_FILES)", "response": "Sets the status of the files that have been successfully uploaded to the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_uploaded(self, files_uploaded):\n        self.files_uploaded = files_uploaded\n        self.__record_progress(Status.UPLOAD_CHANNEL)", "response": "Sets the list of files that have been successfully uploaded to the node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_done(self):\n        self.__record_progress(Status.DONE)\n\n        # Delete restoration point for last step to indicate process has been completed\n        os.remove(self.get_restore_path(Status.LAST))", "response": "Sets the status of the Ricecooker process to DONE."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract file extension from path or return default_ext.", "response": "def extract_path_ext(path, default_ext=None):\n    \"\"\"\n    Extract file extension (without dot) from `path` or return `default_ext` if\n    path does not contain a valid extension.\n    \"\"\"\n    ext = None\n    _, dotext = os.path.splitext(path)\n    if dotext:\n        ext = dotext[1:]\n    if not ext and default_ext:\n        ext = default_ext\n    if not ext:\n        raise ValueError('No extension in path {} and default_ext is None'.format(path))\n    return ext"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_key(action, path_or_id, settings=None, default=\" (default)\"):\n    settings = \" {}\".format(str(sorted(settings.items()))) if settings else default\n    return \"{}: {}{}\".format(action.upper(), path_or_id, settings)", "response": "Generate a key used for caching a single Youtube ID or video."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads file and returns filename", "response": "def download(path, default_ext=None):\n    \"\"\" download: downloads file\n        Args: None\n        Returns: filename\n    \"\"\"\n    key = \"DOWNLOAD:{}\".format(path)\n    if not config.UPDATE and FILECACHE.get(key):\n        return FILECACHE.get(key).decode('utf-8')\n\n    config.LOGGER.info(\"\\tDownloading {}\".format(path))\n\n    # Write file to temporary file\n    with tempfile.TemporaryFile() as tempf:\n        hash = write_and_get_hash(path, tempf)\n        tempf.seek(0)\n\n        # Get extension of file or default if none found\n        extension = os.path.splitext(path)[1][1:].lower()\n        if extension not in [key for key, value in file_formats.choices]:\n            if default_ext:\n                extension = default_ext\n            else:\n                raise IOError(\"No extension found: {}\".format(path))\n\n        filename = '{0}.{ext}'.format(hash.hexdigest(), ext=extension)\n\n        copy_file_to_storage(filename, tempf)\n\n        FILECACHE.set(key, bytes(filename, \"utf-8\"))\n\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading and convert video into mp4 file", "response": "def download_and_convert_video(path, default_ext=file_formats.MP4, ffmpeg_settings=None):\n    \"\"\" download: downloads file\n        Args: None\n        Returns: filename\n    \"\"\"\n    ffmpeg_settings = ffmpeg_settings or {}\n    key = \"DOWNLOAD:{}\".format(path)\n    if not config.UPDATE and FILECACHE.get(key):\n        return FILECACHE.get(key).decode('utf-8')\n\n    config.LOGGER.info(\"\\tDownloading {}\".format(path))\n\n    # Get extension of file or default if none found\n    extension = os.path.splitext(path)[1][1:].lower()\n    converted_path = None\n\n    # Convert video to temporary file\n    with tempfile.NamedTemporaryFile(suffix=\".{}\".format(extension), delete=False) as tempf:\n        # Write unsupported video to temporary file\n        write_and_get_hash(path, tempf)\n        tempf.seek(0)\n\n        # Compress video into mp4 file\n        path, _ext = os.path.splitext(tempf.name)\n        converted_path = \"{}.{}\".format(path, file_formats.MP4)\n        tempf.close()\n        compress_video(tempf.name, converted_path, overwrite=True, **ffmpeg_settings)\n        os.unlink(tempf.name)\n\n    # Write file to temporary file\n    with tempfile.TemporaryFile() as tempf:\n        hash = write_and_get_hash(converted_path, tempf)\n        tempf.seek(0)\n\n        filename = '{0}.{ext}'.format(hash.hexdigest(), ext=file_formats.MP4)\n\n        copy_file_to_storage(filename, tempf)\n        os.unlink(converted_path)\n\n        FILECACHE.set(key, bytes(filename, \"utf-8\"))\n\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_and_get_hash(path, write_to_file, hash=None):\n    hash = hash or hashlib.md5()\n    try:\n        # Access path\n        r = config.DOWNLOAD_SESSION.get(path, stream=True)\n        r.raise_for_status()\n        for chunk in r:\n            write_to_file.write(chunk)\n            hash.update(chunk)\n\n    except (MissingSchema, InvalidSchema):\n        # If path is a local file path, try to open the file (generate hash if none provided)\n        with open(path, 'rb') as fobj:\n            for chunk in iter(lambda: fobj.read(2097152), b\"\"):\n                write_to_file.write(chunk)\n                hash.update(chunk)\n\n    assert write_to_file.tell() > 0, \"File failed to write (corrupted).\"\n\n    return hash", "response": "Write file to file and return hash of file s contents."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to find a language object by the given language code.", "response": "def _get_language_with_alpha2_fallback(language_code):\n    \"\"\"\n    Lookup language code `language_code` (string) in the internal language codes,\n    and if that fails, try to map map `language_code` to the internal represention\n    using the `getlang_by_alpha2` helper method.\n    Returns either a le-utils Language object or None if both lookups fail.\n    \"\"\"\n    # 1. try to lookup `language` using internal representation\n    language_obj = languages.getlang(language_code)\n    # if language_obj not None, we know `language` is a valid language_id in the internal repr.\n    if language_obj is None:\n        # 2. try to match by two-letter ISO code\n        language_obj = languages.getlang_by_alpha2(language_code)\n    return language_obj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_youtube_subtitle_file_supported_language(language):\n    language_obj = _get_language_with_alpha2_fallback(language)\n    if language_obj is None:\n        config.LOGGER.warning(\"Found unsupported language code {}\".format(language))\n        return False\n    else:\n        return True", "response": "Check if the language code is a YouTube language code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset self. language to internal lang. repr. code from str or Language object.", "response": "def set_language(self, language):\n        \"\"\" Set self.language to internal lang. repr. code from str or Language object. \"\"\"\n        if isinstance(language, str):\n            language_obj = languages.getlang(language)\n            if language_obj:\n                self.language = language_obj.code\n            else:\n                raise TypeError(\"Language code {} not found\".format(language))\n        if isinstance(language, languages.Language):\n            self.language = language.code"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self):\n        assert self.path, \"{} must have a path\".format(self.__class__.__name__)\n        _, dotext = os.path.splitext(self.path)\n        ext = dotext.lstrip('.')\n        # don't validate for single-digit extension, or no extension\n        if len(ext) > 1:\n            assert ext in self.allowed_formats, \"{} must have one of the following\"\n            \"extensions: {} (instead, got '{}' from '{}')\".format(\n            self.__class__.__name__, self.allowed_formats, ext, self.path)", "response": "Validate that the current object is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self):\n        assert self.path, \"{} must have a path\".format(self.__class__.__name__)\n        ext = extract_path_ext(self.path, default_ext=self.default_ext)\n        if ext not in self.allowed_formats and ext not in CONVERTIBLE_FORMATS[format_presets.VIDEO_HIGH_RES]:\n            raise ValueError('Incompatible extension {} for VideoFile at {}'.format(ext, self.path))", "response": "Ensure that the VideoFile has at least one of the allowed extensions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self):\n        assert self.path, \"{} must have a path\".format(self.__class__.__name__)\n        ext = extract_path_ext(self.path, default_ext=self.subtitlesformat)\n        if ext not in self.allowed_formats and ext not in CONVERTIBLE_FORMATS[self.get_preset()]:\n            raise ValueError('Incompatible extension {} for SubtitleFile at {}'.format(ext, self.path))", "response": "Ensure that the path is one of the allowed extensions in self. allowed_formats."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download_and_transform_file(self, path):\n        key = \"DOWNLOAD:{}\".format(path)\n        if not config.UPDATE and FILECACHE.get(key):\n            return FILECACHE.get(key).decode('utf-8')\n\n        config.LOGGER.info(\"\\tDownloading {}\".format(path))\n\n        # Write file to temporary file\n        if self.subtitlesformat == file_formats.VTT:\n            with tempfile.TemporaryFile() as tempf:\n                hash = write_and_get_hash(path, tempf)\n                tempf.seek(0)\n\n                # Get extension of file or default if none found\n                filename = '{0}.{ext}'.format(hash.hexdigest(), ext=self.default_ext)\n                copy_file_to_storage(filename, tempf)\n                FILECACHE.set(key, bytes(filename, \"utf-8\"))\n        elif self.subtitlesformat == file_formats.SRT:\n            with tempfile.NamedTemporaryFile() as tempf_srt,\\\n                tempfile.NamedTemporaryFile() as tempf_vtt:\n                hash_srt = write_and_get_hash(path, tempf_srt)\n                tempf_srt.seek(0)\n                filename_tmp_vtt = os.path.join(\n                    \"/tmp\", '{0}.{ext}'.format(hash_srt.hexdigest(), ext=self.default_ext))\n                error_msg = srt2vtt(tempf_srt.name, filename_tmp_vtt)\n                \n                hash = write_and_get_hash(filename_tmp_vtt, tempf_vtt)\n                tempf_vtt.seek(0)\n                \n                filename = '{0}.{ext}'.format(hash.hexdigest(), ext=self.default_ext)\n                copy_file_to_storage(filename, tempf_vtt.name)\n                if error_msg is not None:\n                    config.LOGGER.error(\" An Error found in \" + path)\n                    config.LOGGER.error(error_msg)\n                else:\n                    FILECACHE.set(key, bytes(filename, \"utf-8\"))\n        return filename", "response": "Downloads and transforms the file at path to. vtt if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_file(self):\n        self.filename = self.convert_base64_to_file()\n        config.LOGGER.info(\"\\t--- Converted base64 image to {}\".format(self.filename))\n        return self.filename", "response": "Writes base64 encoding to file\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading a web + graphie file and return the filename", "response": "def process_file(self):\n        \"\"\" download: download a web+graphie file\n            Args: None\n            Returns: None\n        \"\"\"\n        try:\n            self.filename = self.generate_graphie_file()\n            config.LOGGER.info(\"\\t--- Generated graphie {}\".format(self.filename))\n            return self.filename\n        # Catch errors related to reading file path and handle silently\n        except (HTTPError, ConnectionError, InvalidURL, UnicodeDecodeError, UnicodeError, InvalidSchema, IOError) as err:\n            self.error = err\n            config.FAILED_FILES.append(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates ChannelNode and build topic tree.", "response": "def construct_channel(self, *args, **kwargs):\n        \"\"\"\n        Create ChannelNode and build topic tree.\n        \"\"\"\n        channel = self.get_channel(*args, **kwargs)  # creates ChannelNode from data in self.channel_info\n        city_topic = TopicNode(source_id=\"List_of_largest_cities\", title=\"Cities!\")\n        channel.add_child(city_topic)\n        add_subpages_from_wikipedia_list(city_topic, \"https://en.wikipedia.org/wiki/List_of_largest_cities\")\n\n        return channel"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_child(self, node):\n        assert isinstance(node, Node), \"Child node must be a subclass of Node\"\n        node.parent = self\n        self.children += [node]", "response": "Adds a child node to the node s children list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_file(self, file_to_add):\n        from .files import File\n        assert isinstance(file_to_add, File), \"Files being added must be instances of a subclass of File class\"\n        file_to_add.node = self\n        if file_to_add not in self.files:\n            self.files.append(file_to_add)", "response": "Adds a file to the node s files list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting node s thumbnail", "response": "def set_thumbnail(self, thumbnail):\n        \"\"\" set_thumbnail: Set node's thumbnail\n            Args: thumbnail (ThumbnailFile): file model to add to node\n            Returns: None\n        \"\"\"\n        self.thumbnail = thumbnail\n        if isinstance(self.thumbnail, str):\n            from .files import ThumbnailFile\n            self.thumbnail = ThumbnailFile(path=self.thumbnail)\n\n        if self.thumbnail:\n            self.add_file(self.thumbnail)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_thumbnail_preset(self):\n        if isinstance(self, ChannelNode):\n            return format_presets.CHANNEL_THUMBNAIL\n        elif isinstance(self, TopicNode):\n            return format_presets.TOPIC_THUMBNAIL\n        elif isinstance(self, VideoNode):\n            return format_presets.VIDEO_THUMBNAIL\n        elif isinstance(self, AudioNode):\n            return format_presets.AUDIO_THUMBNAIL\n        elif isinstance(self, DocumentNode):\n            return format_presets.DOCUMENT_THUMBNAIL\n        elif isinstance(self, ExerciseNode):\n            return format_presets.EXERCISE_THUMBNAIL\n        elif isinstance(self, HTML5AppNode):\n            return format_presets.HTML5_THUMBNAIL\n        else:\n            return None", "response": "Returns the format preset corresponding to this Node s type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_files(self):\n        file_names = []\n        for f in self.files:\n            file_names.append(f.process_file())\n\n        if not self.has_thumbnail() and config.THUMBNAILS:\n            file_names.append(self.derive_thumbnail())\n\n        # node_file = NodeFile(self.to_dict())\n        # self.hashed_file_name = node_file.process_file()\n        # file_names.append(self.hashed_file_name)\n\n        return file_names", "response": "Processes all the files associated with this Node creates and processes a NodeFile containing this Node s metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count(self):\n        total = len(self.children)\n        for child in self.children:\n            total += child.count()\n        return total", "response": "count - get number of nodes in tree\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the number of topics in tree", "response": "def get_topic_count(self):\n        \"\"\" get_topic_count: get number of topics in tree\n            Args: None\n            Returns: int\n        \"\"\"\n        total = 0\n        if self.kind == content_kinds.TOPIC or self.kind == \"Channel\":\n            total = 1\n            for child in self.children:\n                total += child.get_topic_count()\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_tree(self, indent=2):\n        config.LOGGER.info(\"{indent}{data}\".format(indent=\"   \" * indent, data=str(self)))\n        for child in self.children:\n            child.print_tree(indent + 1)", "response": "Prints out structure of tree\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate all nodes in this tree recusively.", "response": "def validate_tree(self):\n        \"\"\"\n        Validate all nodes in this tree recusively.\n          Args: None\n          Returns: boolean indicating if tree is valid\n        \"\"\"\n        self.validate()\n        for child in self.children:\n            assert child.validate_tree()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that the node is valid and if it is not raises an error", "response": "def validate(self):\n        \"\"\" validate: Makes sure node is valid\n            Args: None\n            Returns: boolean indicating if node is valid\n        \"\"\"\n        from .files import File\n\n        assert self.source_id is not None, \"Assumption Failed: Node must have a source_id\"\n        assert isinstance(self.title, str), \"Assumption Failed: Node title is not a string\"\n        assert isinstance(self.description, str) or self.description is None, \"Assumption Failed: Node description is not a string\"\n        assert isinstance(self.children, list), \"Assumption Failed: Node children is not a list\"\n        for f in self.files:\n            assert isinstance(f, File), \"Assumption Failed: files must be file class\"\n            f.validate()\n\n        source_ids = [c.source_id for c in self.children]\n        duplicates = set([x for x in source_ids if source_ids.count(x) > 1])\n        assert len(duplicates) == 0, \"Assumption Failed: Node must have unique source id among siblings ({} appears multiple times)\".format(duplicates)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_dict(self):\n        return {\n            \"id\": self.get_node_id().hex,\n            \"name\": self.title,\n            \"thumbnail\": self.thumbnail.filename if self.thumbnail else None,\n            \"language\" : self.language,\n            \"description\": self.description or \"\",\n            \"license\": self.license,\n            \"source_domain\": self.source_domain,\n            \"source_id\": self.source_id,\n            \"ricecooker_version\": __version__,\n        }", "response": "Returns a dict representation of the Kolibri - Studio channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that the channel is valid.", "response": "def validate(self):\n        \"\"\" validate: Makes sure channel is valid\n            Args: None\n            Returns: boolean indicating if channel is valid\n        \"\"\"\n        try:\n            assert isinstance(self.source_domain, str), \"Channel domain must be a string\"\n            assert self.language, \"Channel must have a language\"\n            return super(ChannelNode, self).validate()\n        except AssertionError as ae:\n            raise InvalidNodeException(\"Invalid channel ({}): {} - {}\".format(ae.args[0], self.title, self.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dict(self):\n        return {\n            \"title\": self.title,\n            \"language\" : self.language,\n            \"description\": self.description,\n            \"node_id\": self.get_node_id().hex,\n            \"content_id\": self.get_content_id().hex,\n            \"source_domain\": self.domain_ns.hex,\n            \"source_id\": self.source_id,\n            \"author\": self.author,\n            \"aggregator\": self.aggregator,\n            \"provider\": self.provider,\n            \"files\" : [f.to_dict() for f in self.files if f and f.filename], # Filter out failed downloads\n            \"tags\": self.tags,\n            \"kind\": self.kind,\n            \"license\": None,\n            \"license_description\": None,\n            \"copyright_holder\": \"\",\n            \"questions\": [],\n            \"extra_fields\": {},\n        }", "response": "Takes a Kolibri Studio object and returns a dictionary of channel data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self):\n        assert isinstance(self.author, str) , \"Assumption Failed: Author is not a string\"\n        assert isinstance(self.aggregator, str) , \"Assumption Failed: Aggregator is not a string\"\n        assert isinstance(self.provider, str) , \"Assumption Failed: Provider is not a string\"\n        assert isinstance(self.files, list), \"Assumption Failed: Files is not a list\"\n        assert isinstance(self.questions, list), \"Assumption Failed: Questions is not a list\"\n        assert isinstance(self.extra_fields, dict), \"Assumption Failed: Extra fields is not a dict\"\n        return super(TreeNode, self).validate()", "response": "Validate that the content node is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the topic is valid.", "response": "def validate(self):\n        \"\"\" validate: Makes sure topic is valid\n            Args: None\n            Returns: boolean indicating if topic is valid\n        \"\"\"\n        try:\n            assert self.kind == content_kinds.TOPIC, \"Assumption Failed: Node is supposed to be a topic\"\n            return super(TopicNode, self).validate()\n        except AssertionError as ae:\n            raise InvalidNodeException(\"Invalid node ({}): {} - {}\".format(ae.args[0], self.title, self.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self):\n        assert self.role in ROLES, \"Assumption Failed: Role must be one of the following {}\".format(ROLES)\n        assert isinstance(self.license, str) or isinstance(self.license, License), \"Assumption Failed: License is not a string or license object\"\n        self.license.validate()\n        # if self.required_file_format:\n        #     files_valid = False\n        #     #not any(f for f in self.files if isinstance(f, DownloadFile))\n        #     for f in self.files:\n        #         files_valid = files_valid or (f.path.endswith(self.required_file_format)\n        #     assert files_valid , \"Assumption Failed: Node should have at least one {} file\".format(self.required_file_format)\n        return super(ContentNode, self).validate()", "response": "Validate that the content node is valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_dict(self):\n        return {\n            \"title\": self.title,\n            \"language\" : self.language,\n            \"description\": self.description,\n            \"node_id\": self.get_node_id().hex,\n            \"content_id\": self.get_content_id().hex,\n            \"source_domain\": self.domain_ns.hex,\n            \"source_id\": self.source_id,\n            \"author\": self.author,\n            \"aggregator\": self.aggregator,\n            \"provider\": self.provider,\n            \"files\" : [f.to_dict() for f in filter(lambda x: x and x.filename, self.files)], # Filter out failed downloads\n            \"tags\": self.tags,\n            \"kind\": self.kind,\n            \"license\": self.license.license_id,\n            \"license_description\": self.license.description,\n            \"copyright_holder\": self.license.copyright_holder,\n            \"questions\": [question.to_dict() for question in self.questions],\n            \"extra_fields\": json.dumps(self.extra_fields),\n            \"role\": self.role,\n        }", "response": "puts data in format CC expects\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_files(self):\n        from .files import VideoFile, ExtractedVideoThumbnailFile, WebVideoFile\n\n        downloaded = super(VideoNode, self).process_files()\n\n        try:\n            # Extract thumbnail if one hasn't been provided and derive_thumbnail is set\n            if self.generate_thumbnail and not self.has_thumbnail():\n                videos = [f for f in self.files if isinstance(f, VideoFile) or isinstance(f, WebVideoFile)]\n                assert len(videos) > 0 and videos[0].filename, \"Cannot extract thumbnail (No videos found on node {0})\".format(self.source_id)\n\n                self.set_thumbnail(ExtractedVideoThumbnailFile(config.get_storage_path(videos[0].filename)))\n                downloaded.append(self.thumbnail.get_filename())\n\n        except AssertionError as ae:\n            config.LOGGER.warning(ae)\n\n        return downloaded", "response": "Download video s files\n            and set the thumbnail if needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate that video is valid.", "response": "def validate(self):\n        \"\"\" validate: Makes sure video is valid\n            Args: None\n            Returns: boolean indicating if video is valid\n        \"\"\"\n        from .files import VideoFile, WebVideoFile\n        try:\n            assert self.kind == content_kinds.VIDEO, \"Assumption Failed: Node should be a video\"\n            assert self.questions == [], \"Assumption Failed: Video should not have questions\"\n            assert len(self.files) > 0, \"Assumption Failed: Video must have at least one video file\"\n\n            # Check if there are any .mp4 files if there are video files (other video types don't have paths)\n            assert any(f for f in self.files if isinstance(f, VideoFile) or isinstance(f, WebVideoFile)), \"Assumption Failed: Video should have at least one .mp4 file\"\n\n            return super(VideoNode, self).validate()\n        except AssertionError as ae:\n            raise InvalidNodeException(\"Invalid node ({}): {} - {}\".format(ae.args[0], self.title, self.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that audio is valid.", "response": "def validate(self):\n        \"\"\" validate: Makes sure audio is valid\n            Args: None\n            Returns: boolean indicating if audio is valid\n        \"\"\"\n        from .files import AudioFile\n        try:\n            assert self.kind == content_kinds.AUDIO, \"Assumption Failed: Node should be audio\"\n            assert self.questions == [], \"Assumption Failed: Audio should not have questions\"\n            assert len(self.files) > 0, \"Assumption Failed: Audio should have at least one file\"\n            assert any(filter(lambda f: isinstance(f, AudioFile), self.files)), \"Assumption Failed: Audio should have at least one audio file\"\n            return super(AudioNode, self).validate()\n        except AssertionError as ae:\n            raise InvalidNodeException(\"Invalid node ({}): {} - {}\".format(ae.args[0], self.title, self.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate that the node is a document node.", "response": "def validate(self):\n        \"\"\" validate: Makes sure document node contains at least one EPUB or PDF\n            Args: None\n            Returns: boolean indicating if document is valid\n        \"\"\"\n        from .files import DocumentFile, EPubFile\n        try:\n            assert self.kind == content_kinds.DOCUMENT, \"Assumption Failed: Node should be a document\"\n            assert self.questions == [], \"Assumption Failed: Document should not have questions\"\n            assert len(self.files) > 0, \"Assumption Failed: Document should have at least one file\"\n            assert any(filter(lambda f: isinstance(f, DocumentFile) or isinstance(f, EPubFile), self.files)), \"Assumption Failed: Document should have at least one document file\"\n            return super(DocumentNode, self).validate()\n        except AssertionError as ae:\n            raise InvalidNodeException(\"Invalid node ({}): {} - {}\".format(ae.args[0], self.title, self.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self):\n        from .files import HTMLZipFile\n        try:\n            assert self.kind == content_kinds.HTML5, \"Assumption Failed: Node should be an HTML5 app\"\n            assert self.questions == [], \"Assumption Failed: HTML should not have questions\"\n            assert any(filter(lambda f: isinstance(f, HTMLZipFile), self.files)), \"Assumption Failed: HTML should have at least one html file\"\n            return super(HTML5AppNode, self).validate()\n\n        except AssertionError as ae:\n            raise InvalidNodeException(\"Invalid node ({}): {} - {}\".format(ae.args[0], self.title, self.__dict__))", "response": "Validate that the node is an HTML5 app."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing files in question fields and replaces image strings in the exercise.", "response": "def process_files(self):\n        \"\"\" process_files: goes through question fields and replaces image strings\n            Args: None\n            Returns: None\n        \"\"\"\n        config.LOGGER.info(\"\\t*** Processing images for exercise: {}\".format(self.title))\n        downloaded = super(ExerciseNode, self).process_files()\n        for question in self.questions:\n            downloaded += question.process_question()\n\n        self.process_exercise_data()\n\n        config.LOGGER.info(\"\\t*** Images for {} have been processed\".format(self.title))\n        return downloaded"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate(self):\n        try:\n            assert self.kind == content_kinds.EXERCISE, \"Assumption Failed: Node should be an exercise\"\n\n            # Check if questions are correct\n            assert any(self.questions), \"Assumption Failed: Exercise does not have a question\"\n            assert all(filter(lambda q: q.validate(), self.questions)), \"Assumption Failed: Exercise has invalid question\"\n            assert self.extra_fields['mastery_model'] in MASTERY_MODELS, \"Assumption Failed: Unrecognized mastery model {}\".format(self.extra_fields['mastery_model'])\n            return super(ExerciseNode, self).validate()\n        except AssertionError as ae:\n            raise InvalidNodeException(\"Invalid node ({}): {} - {}\".format(ae.args[0], self.title, self.__dict__))", "response": "Validate that the node is an exercise and that all questions in question are correct."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen the pdf file to read from.", "response": "def open(self, update=False):\n        \"\"\"\n        Opens pdf file to read from.\n        \"\"\"\n        filename = os.path.basename(self.source_path)\n        folder, _ext = os.path.splitext(filename)\n        self.path = os.path.sep.join([self.directory, folder, filename])\n        if not os.path.exists(os.path.dirname(self.path)):\n            os.makedirs(os.path.dirname(self.path))\n\n        # Download full pdf if it hasn't already been downloaded\n        if not os.path.isfile(self.path) or update:\n            with open(self.path, \"wb\") as fobj:\n                fobj.write(read(self.source_path))\n\n        self.file = open(self.path, 'rb')\n        self.pdf = CustomPDFReader(self.file)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_toc(self, subchapters=False):\n        self.check_path()\n        chapters = []\n        index = 0\n\n        for dest in self.pdf.getOutlines():\n\n            # Process chapters\n            if isinstance(dest, CustomDestination) and not isinstance(dest['/Page'], NullObject):\n                page_num = self.pdf.getDestinationPageNumber(dest)\n                chapter_pagerange = {\n                    \"title\": dest['/Title'].replace('\\xa0', ' '),\n                    \"page_start\": page_num if index != 0 else 0,\n                    \"page_end\": self.pdf.numPages,\n                }\n                if subchapters:\n                    chapter_pagerange[\"children\"] = []\n                chapters.append(chapter_pagerange)\n\n                if index > 0:\n                    # Go back to previous chapter and set page_end\n                    chapters[index - 1][\"page_end\"] = page_num\n                    if subchapters:\n                        previous_chapter = chapters[index - 1]\n                        if previous_chapter[\"children\"]:\n                            # Go back to previous subchapter and set page_end\n                            previous_chapter[\"children\"][-1][\"page_end\"] = page_num\n                index += 1\n\n            # Attach subchapters (lists) as children to last chapter\n            elif subchapters and isinstance(dest, list):\n                parent = chapters[index - 1]\n                subindex = 0\n                for subdest in dest:\n                    if isinstance(subdest, CustomDestination) and not isinstance(subdest['/Page'], NullObject):\n                        subpage_num = self.pdf.getDestinationPageNumber(subdest)\n                        parent['children'].append({\n                            \"title\": subdest['/Title'].replace('\\xa0', ' '),\n                            \"page_start\": subpage_num,\n                            \"page_end\": self.pdf.numPages\n                        })\n                        if subindex > 0:\n                            parent['children'][subindex - 1][\"page_end\"] = subpage_num\n                        subindex +=1\n\n        return chapters", "response": "Returns the table - of - contents information extracted from the PDF file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the page range to a PDF file.", "response": "def write_pagerange(self, pagerange, prefix=''):\n        \"\"\"\n        Save the subset of pages specified in `pagerange` (dict) as separate PDF.\n        e.g. pagerange = {'title':'First chapter', 'page_start':0, 'page_end':5}\n        \"\"\"\n        writer = PdfFileWriter()\n        slug = \"\".join([c for c in pagerange['title'].replace(\" \", \"-\") if c.isalnum() or c == \"-\"])\n        write_to_path = os.path.sep.join([self.directory, \"{}{}.pdf\".format(prefix, slug)])\n        for page in range(pagerange['page_start'], pagerange['page_end']):\n            writer.addPage(self.pdf.getPage(page))\n            writer.removeLinks()                   # must be done every page\n        with open(write_to_path, 'wb') as outfile:\n            writer.write(outfile)\n        return write_to_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting the PDF into individual chapters based on the page - range info.", "response": "def split_chapters(self, jsondata=None, prefix=''):\n        \"\"\"\n        Split the PDF doc into individual chapters based on the page-range info,\n        storing individual split PDFs in the output folder `self.directory`.\n        By default, we use the `self.get_toc()` to get the chapters page ranges.\n        Pass in the page range dict `jsondata` to customize split points.\n        \"\"\"\n        self.check_path()\n\n        toc = jsondata or self.get_toc()\n        chapters = []\n        for index, chpagerange in enumerate(toc):\n            newprefix = prefix + str(index) + '-'\n            write_to_path = self.write_pagerange(chpagerange, prefix=newprefix)\n            chapters.append({\"title\": chpagerange['title'], \"path\": write_to_path})\n        return chapters"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef path_to_tuple(path, windows=False):\n    if windows:\n        path_tup = tuple(path.split('\\\\'))\n    else:\n        path_tup = tuple(path.split('/'))\n    #\n    # Normalize UTF-8 encoding to consistent form so cache lookups will work, see\n    # https://docs.python.org/3.6/library/unicodedata.html#unicodedata.normalize\n    path_tup = tuple(normalize('NFD', part) for part in path_tup)\n    return path_tup", "response": "Split chan_path into individual parts and form a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the path to the metadata file named filename that is a sibling of channeldir.", "response": "def get_metadata_file_path(channeldir, filename):\n    \"\"\"\n    Return the path to the metadata file named `filename` that is a sibling of `channeldir`.\n    \"\"\"\n    channelparentdir, channeldirname = os.path.split(channeldir)\n    return os.path.join(channelparentdir, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_csv_lines(path):\n    csv_file = open(path, 'r')\n    csv_lines_raw = csv_file.readlines()\n    csv_lines_clean = [line for line in csv_lines_raw if len(line.strip()) > 0]\n    return csv_lines_clean", "response": "Reads a CSV file and returns list of rows."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _clean_dict(row):\n    row_cleaned = {}\n    for key, val in row.items():\n        if val is None or val == '':\n            row_cleaned[key] = None\n        else:\n            row_cleaned[key] = val\n    return row_cleaned", "response": "Transform empty strings values of dict row to None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cache_contentinfo(self):\n        csv_filename = get_metadata_file_path(self.channeldir, self.contentinfo)\n        csv_lines = _read_csv_lines(csv_filename)\n        dict_reader = csv.DictReader(csv_lines)\n        for row in dict_reader:\n            row_dict = self._map_content_row_to_dict(row)\n            path_tuple = path_to_tuple(row_dict['chan_path'], windows=self.winpaths)\n            self.contentcache[path_tuple] = row_dict\n\n        # Additional handling of data in Exercises.csv and ExerciseQuestions.txt\n        if self.has_exercises():\n            # A. Load exercise questions\n            questions_by_source_id = defaultdict(list)\n            csv_filename = get_metadata_file_path(self.channeldir, self.questionsinfo)\n            csv_lines = _read_csv_lines(csv_filename)\n            dict_reader = csv.DictReader(csv_lines)\n            for question_row in dict_reader:\n                question_dict = self._map_exercise_question_row_to_dict(question_row)\n                question_source_id = question_dict['source_id']\n                del question_dict['source_id']\n                questions_by_source_id[question_source_id].append(question_dict)\n\n            # B. Load exercises\n            csv_filename = get_metadata_file_path(self.channeldir, self.exercisesinfo)\n            csv_lines = _read_csv_lines(csv_filename)\n            dict_reader = csv.DictReader(csv_lines)\n            for exercise_row in dict_reader:\n                exercise_dict = self._map_exercise_row_to_dict(exercise_row)\n                path_tuple = path_to_tuple(exercise_dict['chan_path'], windows=self.winpaths)\n                question_source_id = exercise_dict['source_id']\n                exercise_dict['questions'] = questions_by_source_id[question_source_id]\n                # B1: exercises are standard content nodes, so add to contentcache\n                self.contentcache[path_tuple] = exercise_dict\n                # B2: add exercise to list of virtual filanames for current folder\n                dir_path_tuple = path_tuple[0:-1]\n                vfilename = path_tuple[-1]\n                self.exercise_filenames_in_dir[dir_path_tuple].append(vfilename)", "response": "This function is used to cache the content info for the current channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn metadata dict for path in path_tuple.", "response": "def get(self, path_tuple):\n        \"\"\"\n        Returns metadata dict for path in `path_tuple`.\n        \"\"\"\n        if path_tuple in self.contentcache:\n            metadata = self.contentcache[path_tuple]\n        else:\n            # TODO: make chef robust to missing metadata\n            # LOGGER.error(\n            LOGGER.warning('No metadata found for path_tuple ' + str(path_tuple))\n            metadata = dict(\n                filepath=os.path.sep.join(path_tuple),\n                title=os.path.sep.join(path_tuple)\n            )\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_channel_info(self):\n        csv_filename = get_metadata_file_path(channeldir=self.channeldir, filename=self.channelinfo)\n        csv_lines = _read_csv_lines(csv_filename)\n        dict_reader = csv.DictReader(csv_lines)\n        channel_csvs_list =  list(dict_reader)\n        channel_csv = channel_csvs_list[0]\n        if len(channel_csvs_list) > 1:\n            raise ValueError('Found multiple channel rows in ' + self.channelinfo)\n        channel_cleaned = _clean_dict(channel_csv)\n        channel_info = self._map_channel_row_to_dict(channel_cleaned)\n        return channel_info", "response": "Returns the first data row from Channel. csv\n        Returns the first data row from Channel. csv\n        Returns a dict of channel info"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts channel row to dict.", "response": "def _map_channel_row_to_dict(self, row):\n        \"\"\"\n        Convert dictionary keys from raw csv format (see CHANNEL_INFO_HEADER),\n        to ricecooker-like keys, e.g., ''Source ID' --> 'source_id'\n        \"\"\"\n        channel_cleaned = _clean_dict(row)\n        channel_dict = dict(\n            title=channel_cleaned[CHANNEL_TITLE_KEY],\n            description=channel_cleaned[CHANNEL_DESCRIPTION_KEY],\n            source_domain=channel_cleaned[CHANNEL_DOMAIN_KEY],\n            source_id=channel_cleaned[CHANNEL_SOURCEID_KEY],\n            language=channel_cleaned[CHANNEL_LANGUAGE_KEY],\n            thumbnail_chan_path=channel_cleaned[CHANNEL_THUMBNAIL_KEY]\n        )\n        return channel_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a single row from raw csv format to ricecooker - like format.", "response": "def _map_content_row_to_dict(self, row):\n        \"\"\"\n        Convert dictionary keys from raw csv format (see CONTENT_INFO_HEADER),\n        to ricecooker-like keys, e.g., 'Title *' --> 'title'\n        \"\"\"\n        row_cleaned = _clean_dict(row)\n        license_id = row_cleaned[CONTENT_LICENSE_ID_KEY]\n        if license_id:\n            license_dict = dict(\n                license_id=row_cleaned[CONTENT_LICENSE_ID_KEY],\n                description=row_cleaned.get(CONTENT_LICENSE_DESCRIPTION_KEY, None),\n                copyright_holder=row_cleaned.get(CONTENT_LICENSE_COPYRIGHT_HOLDER_KEY, None)\n            )\n        else:\n            license_dict = None\n        # row_dict represents either a topic node or a content node\n        row_dict = dict(\n            chan_path=row_cleaned[CONTENT_PATH_KEY],\n            title=row_cleaned[CONTENT_TITLE_KEY],\n            source_id=row_cleaned.get(CONTENT_SOURCEID_KEY, None),\n            description=row_cleaned.get(CONTENT_DESCRIPTION_KEY, None),\n            author=row_cleaned.get(CONTENT_AUTHOR_KEY, None),\n            language=row_cleaned.get(CONTENT_LANGUAGE_KEY, None),\n            license=license_dict,\n            thumbnail_chan_path=row_cleaned.get(CONTENT_THUMBNAIL_KEY, None)\n        )\n        return row_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting raw CSV Exercise format to ricecooker format.", "response": "def _map_exercise_row_to_dict(self, row):\n        \"\"\"\n        Convert dictionary keys from raw CSV Exercise format to ricecooker keys.\n        \"\"\"\n        row_cleaned = _clean_dict(row)\n        license_id = row_cleaned[CONTENT_LICENSE_ID_KEY]\n        if license_id:\n            license_dict = dict(\n                license_id=row_cleaned[CONTENT_LICENSE_ID_KEY],\n                description=row_cleaned.get(CONTENT_LICENSE_DESCRIPTION_KEY, None),\n                copyright_holder=row_cleaned.get(CONTENT_LICENSE_COPYRIGHT_HOLDER_KEY, None)\n            )\n        else:\n            license_dict = None\n\n        # Parse exercise_data\n        randomize_raw = row_cleaned.get(EXERCISE_RANDOMIZE_KEY, None)\n        if randomize_raw is None or randomize_raw.lower() in CSV_STR_TRUE_VALUES:\n            randomize = True\n        elif randomize_raw.lower() in CSV_STR_FALSE_VALUES:\n            randomize = False\n        else:\n            raise ValueError('Unrecognized value ' + randomize_raw + ' for randomzied key')\n        exercise_data = dict(\n            mastery_model=exercises.M_OF_N,\n            randomize=randomize,\n        )\n        m_value = row_cleaned.get(EXERCISE_M_KEY, None)\n        if m_value:\n            exercise_data['m'] = m_value\n        n_value = row_cleaned.get(EXERCISE_N_KEY, None)\n        if n_value:\n            exercise_data['n'] = n_value\n\n        exercise_dict = dict(\n            chan_path=row_cleaned[CONTENT_PATH_KEY],\n            title=row_cleaned[CONTENT_TITLE_KEY],\n            source_id=row_cleaned[EXERCISE_SOURCEID_KEY],\n            description=row_cleaned.get(CONTENT_DESCRIPTION_KEY, None),\n            author=row_cleaned.get(CONTENT_AUTHOR_KEY, None),\n            language=row_cleaned.get(CONTENT_LANGUAGE_KEY, None),\n            license=license_dict,\n            exercise_data=exercise_data,\n            thumbnail_chan_path=row_cleaned.get(CONTENT_THUMBNAIL_KEY, None)\n        )\n        return exercise_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _map_exercise_question_row_to_dict(self, row):\n        row_cleaned = _clean_dict(row)\n\n        # Parse answers\n        all_answers = []\n        ansA = row_cleaned[EXERCISE_QUESTIONS_OPTION_A_KEY]\n        all_answers.append(ansA)\n        ansB = row_cleaned.get(EXERCISE_QUESTIONS_OPTION_B_KEY, None)\n        if ansB:\n            all_answers.append(ansB)\n        ansC = row_cleaned.get(EXERCISE_QUESTIONS_OPTION_C_KEY, None)\n        if ansC:\n            all_answers.append(ansC)\n        ansD = row_cleaned.get(EXERCISE_QUESTIONS_OPTION_D_KEY, None)\n        if ansD:\n            all_answers.append(ansD)\n        ansE = row_cleaned.get(EXERCISE_QUESTIONS_OPTION_E_KEY, None)\n        if ansE:\n            all_answers.append(ansE)\n        more_answers_str = row_cleaned.get(EXERCISE_QUESTIONS_OPTION_FGHI_KEY, None)\n        if more_answers_str:\n            more_answers = more_answers_str.split(DEFAULT_EXTRA_ITEMS_SEPARATOR)\n            all_answers.extend([ans.strip() for ans in more_answers])\n\n        # Parse correct answers\n        correct_answers = []\n        correct_ans = row_cleaned[EXERCISE_QUESTIONS_CORRECT_ANSWER_KEY]\n        correct_answers.append(correct_ans)\n        correct_ans2 = row_cleaned.get(EXERCISE_QUESTIONS_CORRECT_ANSWER2_KEY, None)\n        if correct_ans2:\n            correct_answers.append(correct_ans2)\n        correct_ans3 = row_cleaned.get(EXERCISE_QUESTIONS_CORRECT_ANSWER3_KEY, None)\n        if correct_ans3:\n            correct_answers.append(correct_ans3)\n\n        # Parse hints\n        hints = []\n        hint1 = row_cleaned.get(EXERCISE_QUESTIONS_HINT_1_KEY, None)\n        if hint1:\n            hints.append(hint1)\n        hint2 = row_cleaned.get(EXERCISE_QUESTIONS_HINT_2_KEY, None)\n        if hint2:\n            hints.append(hint2)\n        hint3 = row_cleaned.get(EXERCISE_QUESTIONS_HINT_3_KEY, None)\n        if hint3:\n            hints.append(hint3)\n        hint4 = row_cleaned.get(EXERCISE_QUESTIONS_HINT_4_KEY, None)\n        if hint4:\n            hints.append(hint4)\n        hint5 = row_cleaned.get(EXERCISE_QUESTIONS_HINT_5_KEY, None)\n        if hint5:\n            hints.append(hint5)\n        more_hints_str = row_cleaned.get(EXERCISE_QUESTIONS_HINT_6789_KEY, None)\n        if more_hints_str:\n            more_hints = more_hints_str.split(DEFAULT_EXTRA_ITEMS_SEPARATOR)\n            hints.extend([hint.strip() for hint in more_hints])\n\n        # Build appropriate dictionary depending on question_type\n        question_type = row_cleaned[EXERCISE_QUESTIONS_TYPE_KEY]\n        if question_type == exercises.MULTIPLE_SELECTION:\n            question_dict = dict(\n                question_type=exercises.MULTIPLE_SELECTION,\n                source_id=row_cleaned[EXERCISE_SOURCEID_KEY],\n                id=row_cleaned[EXERCISE_QUESTIONS_QUESTIONID_KEY],\n                question=row_cleaned[EXERCISE_QUESTIONS_QUESTION_KEY],\n                correct_answers=correct_answers,\n                all_answers=all_answers,\n                hints=hints,\n            )\n        elif question_type == exercises.SINGLE_SELECTION:\n            question_dict = dict(\n                question_type=exercises.SINGLE_SELECTION,\n                source_id=row_cleaned[EXERCISE_SOURCEID_KEY],\n                id=row_cleaned[EXERCISE_QUESTIONS_QUESTIONID_KEY],\n                question=row_cleaned[EXERCISE_QUESTIONS_QUESTION_KEY],\n                correct_answer=correct_answers[0],\n                all_answers=all_answers,\n                hints=hints,\n            )\n        elif question_type == exercises.INPUT_QUESTION:\n            question_dict = dict(\n                question_type=exercises.INPUT_QUESTION,\n                source_id=row_cleaned[EXERCISE_SOURCEID_KEY],\n                id=row_cleaned[EXERCISE_QUESTIONS_QUESTIONID_KEY],\n                question=row_cleaned[EXERCISE_QUESTIONS_QUESTION_KEY],\n                answers=correct_answers,\n                hints=hints,\n            )\n        elif question_type == exercises.PERSEUS_QUESTION:\n            raise ValueError('Perseus questions not currently supported in CSV workflow.')\n\n        return question_dict", "response": "Map an entry from raw CSV Exercise Question format to ricecooker keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_headers(self):\n        super().validate()\n        self.validate_header(self.channeldir, self.channelinfo, CHANNEL_INFO_HEADER)\n        self.validate_header(self.channeldir, self.contentinfo, CONTENT_INFO_HEADER)\n        if self.has_exercises():\n            self.validate_header(self.channeldir, self.exercisesinfo, EXERCISE_INFO_HEADER)\n            self.validate_header(self.channeldir, self.questionsinfo, EXERCISE_QUESTIONS_INFO_HEADER)", "response": "Check if the CSV metadata files have the right format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if CSV metadata file filename has the expected header format.", "response": "def validate_header(self, channeldir, filename, expected_header):\n        \"\"\"\n        Check if CSV metadata file `filename` have the expected header format.\n        \"\"\"\n        expected = set(expected_header)\n        csv_filename = get_metadata_file_path(channeldir, filename)\n        csv_lines = _read_csv_lines(csv_filename)\n        dict_reader = csv.DictReader(csv_lines)\n        actual = set(dict_reader.fieldnames)\n        if not actual == expected:\n            raise ValueError('Unexpected CSV file header in ' + csv_filename \\\n                             + ' Expected header:' + str(expected))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_exercises_from_importstudioid(self, args, options):\n        print('Generating Exercises.csv and ExerciseQuestions.csv from a Studio channel')\n        self.studioapi = StudioApi(token=args['token'])\n        channel_dict = self.studioapi.get_tree_for_studio_id(args['importstudioid'])\n        json.dump(channel_dict, open('chefdata/studiotree.json', 'w'), indent=4, ensure_ascii=False, sort_keys=True)\n\n        soure_ids_seen = []\n        def _generate_source_id(subtree):\n            \"\"\"\n            Creates a Source ID form title and ensures it is unique withing channel.\n            \"\"\"\n            candidate = subtree['title'].replace(' ', '_')\n            if candidate not in soure_ids_seen:\n                source_id = candidate\n                soure_ids_seen.append(source_id)\n            else:\n                source_id = candidate + subtree['node_id'][0:7]\n                soure_ids_seen.append(source_id)\n            return source_id\n\n        def _write_subtree(path_tuple, subtree, is_root=False):\n            print('    '*len(path_tuple) + '  - ', subtree['title'])\n            kind = subtree['kind']\n\n            # TOPIC ############################################################\n            if kind == 'topic':\n\n                if is_root:\n                    self.write_topic_row_from_studio_dict(path_tuple, subtree, is_root=is_root)\n                    for child in subtree['children']:\n                        _write_subtree(path_tuple, child)\n                else:\n                    self.write_topic_row_from_studio_dict(path_tuple, subtree)\n                    for child in subtree['children']:\n                        _write_subtree(path_tuple+[subtree['title']], child)\n\n            # EXERCISE #########################################################\n            elif kind == 'exercise':\n                source_id = _generate_source_id(subtree)\n                self.write_exercice_row_from_studio_dict(path_tuple, subtree, source_id)\n                for question_dict in subtree['assessment_items']:\n                    self.write_question_row_from_question_dict(source_id, question_dict)\n\n            else:\n                print('skipping node', subtree['title'])\n\n        path_tuple = [ self.channeldir.split('/')[-1] ]\n        _write_subtree(path_tuple, channel_dict, is_root=True)", "response": "Generate ExerciseQuestions. csv and ExerciseQuestions. csv from a Studio channel"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_local_question_images(self, question_dict):\n        question_dict = question_dict.copy()\n        dest_path = 'exerciseimages/'\n\n        if not os.path.exists(dest_path):\n            os.mkdir(dest_path)\n\n        # helper method\n        def _process_string(string):\n            image_regex = re.compile(MARKDOWN_IMAGE_REGEX, flags=re.IGNORECASE)\n            contentstorage_prefix = '${\u2623 CONTENTSTORAGE}/'\n            studio_storage = 'https://studio.learningequality.org/content/storage/'\n\n            matches = image_regex.findall(string)\n\n            # Parse all matches\n            for match in matches:\n                file_result = match[1]\n                file_name = file_result.replace(contentstorage_prefix, '')\n                file_url = studio_storage + file_name[0] + '/' + file_name[1] + '/' + file_name\n                file_local_path = os.path.join(dest_path, file_name)\n                response = requests.get(file_url)\n                if response.status_code != 200:\n                    print('Failed for image ' + str(response.status_code) + ' >> ' + file_url)\n                    return string\n                with open(file_local_path, 'wb') as local_file:\n                    local_file.write(response.content)\n                    print('saved image file', file_local_path)\n                string = string.replace(file_result, file_local_path)\n\n            return string\n\n        # Process images in question\n        new_question = _process_string(question_dict['question'])\n        question_dict['question'] = new_question\n\n        # Process images in answers\n        answers = json.loads(question_dict['answers'])\n        new_answers = []\n        for ans in answers:\n            new_ans = ans.copy()\n            new_ans['answer'] = _process_string(new_ans['answer'])\n            new_answers.append(new_ans)\n        question_dict['answers'] = json.dumps(new_answers)\n\n        # TODO: process hint images\n\n        return question_dict", "response": "Process all mardown image links in question_dict and make local files under exerciseimages directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the Content. csv rows for each folder and file in self. channeldir.", "response": "def generate_contentinfo_from_channeldir(self, args, options):\n        \"\"\"\n        Create rows in Content.csv for each folder and file in `self.channeldir`.\n        \"\"\"\n        LOGGER.info('Generating Content.csv rows folders and file in channeldir')\n        file_path = get_metadata_file_path(self.channeldir, self.contentinfo)\n        with open(file_path, 'a') as csv_file:\n            csvwriter = csv.DictWriter(csv_file, CONTENT_INFO_HEADER)\n\n            channeldir = args['channeldir']\n            if channeldir.endswith(os.path.sep):\n                channeldir.rstrip(os.path.sep)\n\n            # MAIN PROCESSING OF os.walk OUTPUT\n            content_folders = sorted(os.walk(channeldir))\n            _ = content_folders.pop(0)           # Skip over channel root folder\n            for rel_path, _subfolders, filenames in content_folders:\n                LOGGER.info('processing folder ' + str(rel_path))\n                sorted_filenames = sorted(filenames)\n                self.generate_contentinfo_from_folder(csvwriter, rel_path, sorted_filenames)\n        LOGGER.info('Generted {} row for all folders and files in {}'.format(self.contentinfo, self.channeldir))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a topic node row in Content. csv for the folder at rel_path and add content node rows for all the files in the rel_path.", "response": "def generate_contentinfo_from_folder(self, csvwriter, rel_path, filenames):\n        \"\"\"\n        Create a topic node row in Content.csv for the folder at `rel_path` and\n        add content node rows for all the files in the `rel_path` folder.\n        \"\"\"\n        LOGGER.debug('IN process_folder ' + str(rel_path) + '     ' + str(filenames))\n        from ricecooker.utils.linecook import filter_filenames, filter_thumbnail_files, chan_path_from_rel_path\n\n        # WRITE TOPIC ROW\n        topicrow = self.channeldir_node_to_row( rel_path.split(os.path.sep) )\n        csvwriter.writerow(topicrow)\n\n        # WRITE CONTENT NODE ROWS\n        chan_path = chan_path_from_rel_path(rel_path, self.channeldir)\n        filenames_cleaned = filter_filenames(filenames)\n        # filenames_cleaned2 = filter_thumbnail_files(chan_path, filenames_cleaned, self)\n        for filename in filenames_cleaned:\n            path_tuple = rel_path.split(os.path.sep)\n            path_tuple.append(filename)\n            filerow = self.channeldir_node_to_row(path_tuple)\n            csvwriter.writerow(filerow)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef channeldir_node_to_row(self, path_tuple):\n        row = dict()\n        for key in CONTENT_INFO_HEADER:\n            row[key] = None\n        row[CONTENT_PATH_KEY] = \"/\".join(path_tuple)  # use / in .csv on Windows and UNIX\n        title = path_tuple[-1].replace('_', ' ')\n        for ext in content_kinds.MAPPING.keys():\n            if title.endswith(ext):\n                title = title.replace('.'+ext, '')\n        row[CONTENT_TITLE_KEY] = title\n        row[CONTENT_SOURCEID_KEY] = path_tuple[-1]\n        return row", "response": "Return a dict with keys corresponding to Content. csv columns."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_templates(self, exercise_questions=False):\n        self.generate_template(channeldir=self.channeldir,\n                               filename=self.channelinfo,\n                               header=CHANNEL_INFO_HEADER)\n        self.generate_template(channeldir=self.channeldir,\n                               filename=self.contentinfo,\n                               header=CONTENT_INFO_HEADER)\n        if exercise_questions:\n            self.generate_template(channeldir=self.channeldir,\n                                   filename=self.exercisesinfo,\n                                   header=EXERCISE_INFO_HEADER)\n            self.generate_template(channeldir=self.channeldir,\n                                   filename=self.questionsinfo,\n                                   header=EXERCISE_QUESTIONS_INFO_HEADER)", "response": "Generate empty. csv files with the right headers and place them in the\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a new empty template. csv file called filename as siblings of the channeldir with header fields specified in header.", "response": "def generate_template(self, channeldir, filename, header):\n        \"\"\"\n        Create empty template .csv file called `filename` as siblings of the\n        directory `channeldir` with header fields specified in `header`.\n        \"\"\"\n        file_path = get_metadata_file_path(channeldir, filename)\n        if not os.path.exists(file_path):\n            with open(file_path, 'w') as csv_file:\n                csvwriter = csv.DictWriter(csv_file, header)\n                csvwriter.writeheader()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self):\n        self.zf = zipfile.ZipFile(self.write_to_path, self.mode)", "response": "Opens the zip file to write to the to\n            file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing zipfile when done", "response": "def close(self):\n        \"\"\" close: Close zipfile when done\n            Args: None\n            Returns: None\n        \"\"\"\n        index_present = self.contains('index.html')\n        self.zf.close() # Make sure zipfile closes no matter what\n        if not index_present:\n            raise ReferenceError(\"Invalid Zip at {}: missing index.html file (use write_index_contents method)\".format(self.write_to_path))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_contents(self, filename, contents, directory=None):\n        filepath = \"{}/{}\".format(directory.rstrip(\"/\"), filename) if directory else filename\n        self._write_to_zipfile(filepath, contents)\n        return filepath", "response": "Writes contents to filename in zip\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a local file to the zip file", "response": "def write_file(self, filepath, filename=None, directory=None):\n        \"\"\" write_file: Write local file to zip\n            Args:\n                filepath: (str) location to local file\n                directory: (str) directory in zipfile to write file to (optional)\n            Returns: path to file in zip\n\n            Note: filepath must be a relative path\n        \"\"\"\n        arcname = None\n        if filename or directory:\n            directory = directory.rstrip(\"/\") + \"/\" if directory else \"\"\n            filename = filename or os.path.basename(filepath)\n            arcname = \"{}{}\".format(directory, filename)\n        self._copy_to_zipfile(filepath, arcname=arcname)\n        return arcname or filepath"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite contents from url to filename in zip", "response": "def write_url(self, url, filename, directory=None):\n        \"\"\" write_url: Write contents from url to filename in zip\n            Args:\n                url: (str) url to file to download\n                filename: (str) name of file in zip\n                directory: (str) directory in zipfile to write file to (optional)\n            Returns: path to file in zip\n        \"\"\"\n        return self.write_contents(filename, read(url), directory=directory)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uploadchannel_wrapper(chef, args, options):\n    try:\n        args_and_options = args.copy()\n        args_and_options.update(options)\n        uploadchannel(chef, **args_and_options)\n        config.SUSHI_BAR_CLIENT.report_stage('COMPLETED', 0)\n    except Exception as e:\n        if config.SUSHI_BAR_CLIENT:\n            config.SUSHI_BAR_CLIENT.report_stage('FAILURE', 0)\n        config.LOGGER.critical(str(e))\n        raise\n    finally:\n        if config.SUSHI_BAR_CLIENT:\n            config.SUSHI_BAR_CLIENT.close()\n        config.LOGGER.removeHandler(__logging_handler)", "response": "Wrapper for the SushiBar uploadchannel function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a channel to Kolibri Studio server", "response": "def uploadchannel(chef, update=False, thumbnails=False, download_attempts=3, resume=False, reset=False, step=Status.LAST.name, token=\"#\", prompt=False, publish=False,  debug=False, verbose=True, warn=False, quiet=False, compress=False, stage=False, **kwargs):\n    \"\"\" uploadchannel: Upload channel to Kolibri Studio server\n        Args:\n            chef (BaseChef or subclass): class that implements the construct_channel method\n            update (bool): indicates whether to re-download files (optional)\n            thumbnails (bool): indicates whether to automatically derive thumbnails from content (optional)\n            download_attempts (int): number of times to retry downloading files (optional)\n            resume (bool): indicates whether to resume last session automatically (optional)\n            step (str): step to resume process from (optional)\n            reset (bool): indicates whether to start session from beginning automatically (optional)\n            token (str): content server authorization token\n            prompt (bool): indicates whether to prompt user to open channel when done (optional)\n            publish (bool): indicates whether to automatically publish channel (optional)\n            debug (bool): indicates whether to print out debugging statements (optional)\n            verbose (bool): indicates whether to print out info statements (optional)\n            warnings (bool): indicates whether to print out warnings (optional)\n            quiet (bool): indicates whether to print out errors (optional)\n            compress (bool): indicates whether to compress larger files (optional)\n            stage (bool): indicates whether to stage rather than deploy channel (optional)\n            kwargs (dict): extra keyword args will be passed to get_channel and construct_channel (optional)\n        Returns: (str) link to access newly created channel\n    \"\"\"\n\n    # Set configuration settings\n    global __logging_handler\n\n    level = logging.NOTSET\n    if debug:\n        level = logging.DEBUG\n    elif warn:\n        level = logging.WARNING\n    elif quiet:\n        level = logging.ERROR\n    elif verbose:\n        level = logging.INFO\n\n\n    __logging_handler = logging.StreamHandler()\n    config.LOGGER.addHandler(__logging_handler)\n    logging.getLogger(\"requests\").setLevel(logging.WARNING)\n    logging.getLogger(\"cachecontrol.controller\").setLevel(logging.WARNING)\n    logging.getLogger(\"requests.packages\").setLevel(logging.WARNING)\n    config.LOGGER.setLevel(level)\n\n    config.UPDATE = update\n    config.COMPRESS = compress\n    config.THUMBNAILS = thumbnails\n    config.STAGE = stage\n    config.PUBLISH = publish\n\n    # Set max retries for downloading\n    config.DOWNLOAD_SESSION.mount('http://', requests.adapters.HTTPAdapter(max_retries=int(download_attempts)))\n    config.DOWNLOAD_SESSION.mount('https://', requests.adapters.HTTPAdapter(max_retries=int(download_attempts)))\n\n    # Get domain to upload to\n    config.init_file_mapping_store()\n\n    # Authenticate user and check current Ricecooker version\n    username, token = authenticate_user(token)\n    check_version_number()\n\n    # Setup Sushibar client based on channel info in `get_channel`\n    config.LOGGER.info(\"Running get_channel... \")\n    channel = chef.get_channel(**kwargs)\n    nomonitor = kwargs.get('nomonitor', False)\n    config.SUSHI_BAR_CLIENT = SushiBarClient(channel, username, token, nomonitor=nomonitor)\n\n    config.LOGGER.info(\"\\n\\n***** Starting channel build process *****\\n\\n\")\n\n    # Set up progress tracker\n    config.PROGRESS_MANAGER = RestoreManager()\n    if (reset or not config.PROGRESS_MANAGER.check_for_session()) and step.upper() != Status.DONE.name:\n        config.PROGRESS_MANAGER.init_session()\n    else:\n        if resume or prompt_yes_or_no('Previous session detected. Would you like to resume your last session?'):\n            config.LOGGER.info(\"Resuming your last session...\")\n            step = Status.LAST.name if step is None else step\n            config.PROGRESS_MANAGER = config.PROGRESS_MANAGER.load_progress(step.upper())\n        else:\n            config.PROGRESS_MANAGER.init_session()\n\n    # Construct channel if it hasn't been constructed already\n    if config.PROGRESS_MANAGER.get_status_val() <= Status.CONSTRUCT_CHANNEL.value:\n        config.LOGGER.info(\"Calling construct_channel... \")\n        channel = chef.construct_channel(**kwargs)\n        config.PROGRESS_MANAGER.set_channel(channel)\n    channel = config.PROGRESS_MANAGER.channel\n\n    # Set initial tree if it hasn't been set already\n    if config.PROGRESS_MANAGER.get_status_val() <= Status.CREATE_TREE.value:\n        config.PROGRESS_MANAGER.set_tree(create_initial_tree(channel))\n    tree = config.PROGRESS_MANAGER.tree\n\n    # Download files if they haven't been downloaded already\n    if config.PROGRESS_MANAGER.get_status_val() <= Status.DOWNLOAD_FILES.value:\n        config.LOGGER.info(\"Downloading files...\")\n        config.PROGRESS_MANAGER.set_files(*process_tree_files(tree))\n\n    # Set download manager in case steps were skipped\n    files_to_diff = config.PROGRESS_MANAGER.files_downloaded\n    config.FAILED_FILES = config.PROGRESS_MANAGER.files_failed\n\n    # Get file diff if it hasn't been generated already\n    if config.PROGRESS_MANAGER.get_status_val() <= Status.GET_FILE_DIFF.value:\n        config.LOGGER.info(\"Getting file diff...\")\n        config.PROGRESS_MANAGER.set_diff(get_file_diff(tree, files_to_diff))\n    file_diff = config.PROGRESS_MANAGER.file_diff\n\n    # Set which files have already been uploaded\n    tree.uploaded_files = config.PROGRESS_MANAGER.files_uploaded\n\n    # Upload files if they haven't been uploaded already\n    if config.PROGRESS_MANAGER.get_status_val() <= Status.UPLOADING_FILES.value:\n        config.LOGGER.info(\"Uploading files...\")\n        config.PROGRESS_MANAGER.set_uploaded(upload_files(tree, file_diff))\n\n    # Create channel on Kolibri Studio if it hasn't been created already\n    if config.PROGRESS_MANAGER.get_status_val() <= Status.UPLOAD_CHANNEL.value:\n        config.LOGGER.info(\"Creating channel...\")\n        config.PROGRESS_MANAGER.set_channel_created(*create_tree(tree))\n    channel_link = config.PROGRESS_MANAGER.channel_link\n    channel_id = config.PROGRESS_MANAGER.channel_id\n\n    # Publish tree if flag is set to True\n    if config.PUBLISH and config.PROGRESS_MANAGER.get_status_val() <= Status.PUBLISH_CHANNEL.value:\n        config.LOGGER.info(\"Publishing channel...\")\n        publish_tree(tree, channel_id)\n        config.PROGRESS_MANAGER.set_published()\n\n    # Open link on web browser (if specified) and return new link\n    config.LOGGER.info(\"\\n\\nDONE: Channel created at {0}\\n\".format(channel_link))\n    if prompt and prompt_yes_or_no('Would you like to open your channel now?'):\n        config.LOGGER.info(\"Opening channel... \")\n        webbrowser.open_new_tab(channel_link)\n\n    config.PROGRESS_MANAGER.set_done()\n    return channel_link"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nauthenticate user with given token.", "response": "def authenticate_user(token):\n    \"\"\"\n    Add the content curation Authorizatino `token` header to `config.SESSION`.\n    \"\"\"\n    config.SESSION.headers.update({\"Authorization\": \"Token {0}\".format(token)})\n    try:\n        response = config.SESSION.post(config.authentication_url())\n        response.raise_for_status()\n        user = json.loads(response._content.decode(\"utf-8\"))\n        config.LOGGER.info(\"Logged in with username {0}\".format(user['username']))\n        return user['username'], token\n    except HTTPError:\n        config.LOGGER.error(\"Invalid token: Credentials not found\")\n        sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prompt_yes_or_no(message):\n    user_input = input(\"{} [y/n]:\".format(message)).lower()\n    if user_input.startswith(\"y\"):\n        return True\n    elif user_input.startswith(\"n\"):\n        return False\n    else:\n        return prompt_yes_or_no(message)", "response": "Prompt user to reply with a yes or no response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_initial_tree(channel):\n    # Create channel manager with channel data\n    config.LOGGER.info(\"   Setting up initial channel structure... \")\n    tree = ChannelManager(channel)\n\n    # Make sure channel structure is valid\n    config.LOGGER.info(\"   Validating channel structure...\")\n    channel.print_tree()\n    tree.validate()\n    config.LOGGER.info(\"   Tree is valid\\n\")\n    return tree", "response": "Create initial tree structure for a single node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses the files from the nodes and returns the number of files to be downloaded and the number of failed files.", "response": "def process_tree_files(tree):\n    \"\"\" process_tree_files: Download files from nodes\n        Args:\n            tree (ChannelManager): manager to handle communication to Kolibri Studio\n        Returns: None\n    \"\"\"\n    # Fill in values necessary for next steps\n    config.LOGGER.info(\"Processing content...\")\n    files_to_diff = tree.process_tree(tree.channel)\n    config.SUSHI_BAR_CLIENT.report_statistics(files_to_diff, topic_count=tree.channel.get_topic_count())\n    tree.check_for_files_failed()\n    return files_to_diff, config.FAILED_FILES"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_file_diff(tree, files_to_diff):\n    # Determine which files have not yet been uploaded to the CC server\n    config.LOGGER.info(\"\\nChecking if files exist on Kolibri Studio...\")\n    file_diff = tree.get_file_diff(files_to_diff)\n    return file_diff", "response": "Download files from nodes\nAttributeNames and return a list of files that are not on Kolibri Studio\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_files(tree, file_diff):\n    # Upload new files to CC\n    config.LOGGER.info(\"\\nUploading {0} new file(s) to Kolibri Studio...\".format(len(file_diff)))\n    tree.upload_files(file_diff)\n    tree.reattempt_upload_fails()\n    return file_diff", "response": "Uploads new files to Kolibri Studio"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_tree(tree):\n    # Create tree\n    config.LOGGER.info(\"\\nCreating tree on Kolibri Studio...\")\n    channel_id, channel_link = tree.upload_tree()\n    # channel_id, channel_link = tree.upload_channel_structure()\n\n    return channel_link, channel_id", "response": "Create Kolibri Studio tree and link to it"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_ss_bonds(model, threshold=3.0):\n\n    # Taken from http://docs.python.org/library/itertools.html\n    # Python 2.4 does not include itertools.combinations\n\n    def combinations(iterable, r):\n        # combinations('ABCD', 2) --> AB AC AD BC BD CD\n        # combinations(range(4), 3) --> 012 013 023 123\n        pool = tuple(iterable)\n        n = len(pool)\n        if r > n:\n            return\n        indices = list(range(r))\n        yield tuple(pool[i] for i in indices)\n        while True:\n            for i in reversed(range(r)):\n                if indices[i] != i + n - r:\n                    break\n            else:\n                return\n            indices[i] += 1\n            for j in range(i + 1, r):\n                indices[j] = indices[j - 1] + 1\n            yield tuple(pool[i] for i in indices)\n\n    cysteines = [r for r in model.get_residues() if r.get_resname() == 'CYS']\n\n    pairs = combinations(cysteines, 2)  # Iterator with pairs\n\n    bridges = []\n    for cys_pair in pairs:\n        try:\n            if cys_pair[0]['SG'] - cys_pair[1]['SG'] < threshold:\n                bridges.append(cys_pair)\n        except KeyError:  # This will occur when a CYS residue is missing a SG atom for some reason\n            log.error('{}: no SG atom found for one or both of the cysteine residues {}'.format(model, cys_pair))\n            continue\n\n    infodict = {}\n    if bridges:\n        infodict = defaultdict(list)\n\n        for disulfide_bridge in bridges:\n            residue1 = disulfide_bridge[0]\n            residue2 = disulfide_bridge[1]\n            chain = residue1.get_parent().id\n            infodict[chain].append((residue1.get_full_id()[3], residue2.get_full_id()[3]))\n\n    return infodict", "response": "Searches S - S bonds for a given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndistances between the last atom of 2 residues", "response": "def residue_distances(res_1_num, res_1_chain, res_2_num, res_2_chain, model):\n    \"\"\"Distance between the last atom of 2 residues\"\"\"\n\n    res1 = model[res_1_chain][res_1_num].child_list[-1]\n    res2 = model[res_2_chain][res_2_num].child_list[-1]\n    distance = res1 - res2\n\n    return distance"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resname_in_proximity(resname, model, chains, resnums, threshold=5):\n    residues = [r for r in model.get_residues() if r.get_resname() == resname]\n\n    chains = ssbio.utils.force_list(chains)\n    resnums = ssbio.utils.force_list(resnums)\n\n    for chain in chains:\n        for resnum in resnums:\n            my_residue_last_atom = model[chain][resnum].child_list[-1]\n            for rz in residues:\n                distance = rz.child_list[-1] - my_residue_last_atom\n                if distance < threshold:\n                    # print(resnum, rz, distance)\n                    return True\n\n    return False", "response": "Returns True if a RESNAME is within the proximity of a list of residue numbers and their chains."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef within(resnum, angstroms, chain_id, model, use_ca=False, custom_coord=None):\n    # XTODO: documentation\n    # TODO: should have separate method for within a normal residue (can use \"resnum\" with a int) or a custom coord,\n    # where you don't need to specify resnum\n    atom_list = Selection.unfold_entities(model, 'A')\n    ns = NeighborSearch(atom_list)\n\n    if custom_coord:  # a list of XYZ coord\n        target_atom_coord = np.array(custom_coord, 'f')\n    else:\n        target_residue = model[chain_id][resnum]\n        if use_ca:\n            target_atom = target_residue['CA']\n        else:\n            target_atom = target_residue.child_list[-1]\n        target_atom_coord = np.array(target_atom.get_coord(), 'f')\n    neighbors = ns.search(target_atom_coord, angstroms)\n    residue_list = Selection.unfold_entities(neighbors, 'R')\n\n    return residue_list", "response": "Returns a list of Bio. PDB. Residue objects that are within the given set of angstroms."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a dictionary of a structure s sequences.", "response": "def get_structure_seqrecords(model):\n    \"\"\"Get a dictionary of a PDB file's sequences.\n\n    Special cases include:\n        - Insertion codes. In the case of residue numbers like \"15A\", \"15B\", both residues are written out. Example: 9LPR\n        - HETATMs. Currently written as an \"X\", or unknown amino acid.\n\n    Args:\n        model: Biopython Model object of a Structure\n\n    Returns:\n        list: List of SeqRecords\n\n    \"\"\"\n\n    structure_seq_records = []\n\n    # Loop over each chain of the PDB\n    for chain in model:\n        tracker = 0\n        chain_seq = ''\n        chain_resnums = []\n\n        # Loop over the residues\n        for res in chain.get_residues():\n            # NOTE: you can get the residue number too\n            res_id = res.id\n            res_num = res_id[1]\n            res_icode = res_id[2]\n\n            # Double check if the residue name is a standard residue\n            # If it is not a standard residue (ie. selenomethionine),\n            # it will be filled in with an X on the next iteration)\n            if Polypeptide.is_aa(res, standard=True):\n                end_tracker = res_num\n                res_aa_one = Polypeptide.three_to_one(res.get_resname())\n\n                # Tracker to fill in X's\n                if end_tracker != (tracker + 1):\n                    if res_icode != ' ':\n                        chain_seq += res_aa_one\n                        chain_resnums.append(res_num)\n                        tracker = end_tracker + 1\n                        continue\n                    else:\n                        multiplier = (end_tracker - tracker - 1)\n                        chain_seq += 'X' * multiplier\n                        # Residue numbers for unresolved or nonstandard residues are Infinite\n                        chain_resnums.extend([float(\"Inf\")] * multiplier)\n\n                chain_seq += res_aa_one\n                chain_resnums.append(res_num)\n                tracker = end_tracker\n\n            else:\n                continue\n\n        chain_seq_record = SeqRecord(Seq(chain_seq, IUPAC.protein), id=chain.get_id())\n        chain_seq_record.letter_annotations['structure_resnums'] = chain_resnums\n        structure_seq_records.append(chain_seq_record)\n\n    return structure_seq_records"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_structure_seqs(pdb_file, file_type):\n\n    # TODO: Please check out capitalization of chain IDs in mmcif files. example: 5afi - chain \"l\" is present but\n    # it seems like biopython capitalizes it to chain L\n\n    # Get the first model\n    my_structure = StructureIO(pdb_file)\n    model = my_structure.first_model\n\n    structure_seqs = {}\n\n    # Loop over each chain of the PDB\n    for chain in model:\n        chain_seq = ''\n        tracker = 0\n\n        # Loop over the residues\n        for res in chain.get_residues():\n            # NOTE: you can get the residue number too\n            # res_num = res.id[1]\n\n            # Double check if the residue name is a standard residue\n            # If it is not a standard residue (ie. selenomethionine),\n            # it will be filled in with an X on the next iteration)\n            if Polypeptide.is_aa(res, standard=True):\n                full_id = res.get_full_id()\n                end_tracker = full_id[3][1]\n                i_code = full_id[3][2]\n                aa = Polypeptide.three_to_one(res.get_resname())\n\n                # Tracker to fill in X's\n                if end_tracker != (tracker + 1):\n                    if i_code != ' ':\n                        chain_seq += aa\n                        tracker = end_tracker + 1\n                        continue\n                    else:\n                        chain_seq += 'X' * (end_tracker - tracker - 1)\n\n                chain_seq += aa\n                tracker = end_tracker\n\n            else:\n                continue\n\n        structure_seqs[chain.get_id()] = chain_seq\n\n    return structure_seqs", "response": "Get a dictionary of a PDB file s sequences."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match_structure_sequence(orig_seq, new_seq, match='X', fill_with='X', ignore_excess=False):\n    if len(orig_seq) == len(new_seq):\n        log.debug('Lengths already equal, nothing to fill in')\n        return new_seq\n\n    if not ignore_excess:\n        if len(orig_seq) < len(new_seq):\n            raise ValueError('Original sequence has a length less than the sequence provided to match to')\n    else:\n        log.debug('New sequence will be truncated to length of original sequence - information may be lost!')\n\n    if not isinstance(new_seq, str) and not isinstance(new_seq, tuple) and not isinstance(new_seq, list):\n        raise ValueError('Invalid sequence provided, must be string, tuple, or list')\n\n    orig_seq = ssbio.protein.sequence.utils.cast_to_str(orig_seq)\n    new_thing = deepcopy(new_seq)\n    if isinstance(new_seq, tuple):\n        new_thing = list(new_thing)\n\n    for i, s in enumerate(orig_seq):\n        if s == match:\n            if isinstance(new_thing, str):\n                new_thing = new_thing[:i] + fill_with + new_thing[i:]\n            if isinstance(new_thing, list):\n                new_thing.insert(i, fill_with)\n\n    new_thing = new_thing[:len(orig_seq)]\n\n    if isinstance(new_seq, tuple):\n        new_thing = tuple(new_thing)\n\n    return new_thing", "response": "Correct a sequence to match inserted X s in a structure sequence\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hse_output(pdb_file, file_type):\n    # Get the first model\n    my_structure = StructureIO(pdb_file)\n    model = my_structure.first_model\n\n    # Calculate HSEalpha\n    exp_ca = HSExposureCA(model)\n    # Calculate HSEbeta\n    exp_cb = HSExposureCB(model)\n    # Calculate classical coordination number\n    exp_fs = ExposureCN(model)\n\n    return", "response": "This function calculates the solvent exposure of an amino acid residue and returns the HSE representation of the residue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform_array(rot_mtx,vec_array):\n\n    '''transform_array( matrix, vector_array ) -> vector_array\n\n    '''\n\n    return map( lambda x,m=rot_mtx:transform(m,x), vec_array )", "response": "Transform a vector array by rotating it by rot_mtx."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntranslating a vector array into a new vector.", "response": "def translate_array(trans_vec,vec_array):\n\n    '''translate_array(trans_vec,vec_array) -> vec_array\n\n    Adds 'mult'*'trans_vec' to each element in vec_array, and returns\n    the translated vector.\n    '''\n\n    return map ( lambda x,m=trans_vec:add(m,x),vec_array )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying a fit result to an array of vectors", "response": "def fit_apply(fit_result,vec_array):\n    '''fit_apply(fir_result,vec_array) -> vec_array\n    \n    Applies a fit result to an array of vectors\n    '''\n\n    return map( lambda x,t1=fit_result[0],mt2=negate(fit_result[1]),\n        m=fit_result[2]: add(t1,transform(m,add(mt2,x))),vec_array)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(target_array, source_array):\n\n    '''fit(target_array, source_array) -> (t1, t2, rot_mtx, rmsd) [fit_result]\n\n    Calculates the translation vectors and rotation matrix required\n    to superimpose source_array onto target_array.  Original arrays are\n    not modified.  NOTE: Currently assumes 3-dimensional coordinates\n\n    t1,t2 are vectors from origin to centers of mass...\n    '''\n\n# Check dimensions of input arrays\n    if len(target_array) != len(source_array):\n        print (\"Error: arrays must be of same length for RMS fitting.\")\n        raise ValueError\n    if len(target_array[0]) != 3 or len(source_array[0]) != 3:\n        print (\"Error: arrays must be dimension 3 for RMS fitting.\")\n        raise ValueError\n    nvec = len(target_array)\n    ndim = 3\n    maxiter = 200\n    tol = 0.001\n\n# Calculate translation vectors (center-of-mass).\n\n    t1 = get_null()\n    t2 = get_null()\n    tvec1 = get_null()\n    tvec2 = get_null()\n\n    for i in range(nvec):\n        for j in range(ndim):\n            t1[j] = t1[j] + target_array[i][j]\n            t2[j] = t2[j] + source_array[i][j]\n    for j in range(ndim):\n        t1[j] = t1[j] / nvec\n        t2[j] = t2[j] / nvec\n\n# Calculate correlation matrix.\n\n    corr_mtx = []\n    for i in range(ndim):\n        temp_vec = []\n        for j in range(ndim):\n            temp_vec.append(0.0)\n        corr_mtx.append(temp_vec)\n\n    rot_mtx = []\n    for i in range(ndim):\n        temp_vec = []\n        for j in range(ndim):\n            temp_vec.append(0.0)\n        rot_mtx.append(temp_vec)\n    for i in range(ndim):\n        rot_mtx[i][i] = 1.\n\n    for i in range(nvec):\n        for j in range(ndim):\n            tvec1[j] = target_array[i][j] - t1[j]\n            tvec2[j] = source_array[i][j] - t2[j]\n        for j in range(ndim):\n            for k in range(ndim):\n                corr_mtx[j][k] = corr_mtx[j][k] + tvec2[j]*tvec1[k]\n\n# Main iteration scheme (hardwired for 3X3 matrix, but could be extended).\n\n    iters = 0\n    while (iters < maxiter):\n        iters = iters + 1\n        ix = (iters-1)%ndim\n        iy = iters%ndim\n        iz = (iters+1)%ndim\n        sig = corr_mtx[iz][iy] - corr_mtx[iy][iz]\n        gam = corr_mtx[iy][iy] + corr_mtx[iz][iz]\n\n        sg = (sig**2 + gam**2)**0.5\n        if sg != 0.0 and (abs(sig) > tol*abs(gam)):\n            sg = 1.0 / sg\n            for i in range(ndim):\n\n                bb = gam*corr_mtx[iy][i] + sig*corr_mtx[iz][i]\n                cc = gam*corr_mtx[iz][i] - sig*corr_mtx[iy][i]\n                corr_mtx[iy][i] = bb*sg\n                corr_mtx[iz][i] = cc*sg\n\n                bb = gam*rot_mtx[iy][i] + sig*rot_mtx[iz][i]\n                cc = gam*rot_mtx[iz][i] - sig*rot_mtx[iy][i]\n                rot_mtx[iy][i] = bb*sg\n                rot_mtx[iz][i] = cc*sg\n\n        else:\n# We have a converged rotation matrix.  Calculate RMS deviation.\n            vt1 = translate_array(negate(t1),target_array)\n            vt2 = translate_array(negate(t2),source_array)\n            vt3 = transform_array(rot_mtx,vt2)\n            rmsd = 0.0\n            for i in range(nvec):\n                rmsd = rmsd + distance_sq(vt1[i], vt3[i])\n            rmsd = math.sqrt(rmsd/nvec)\n            return(t1, t2, rot_mtx, rmsd)\n\n# Too many iterations; something wrong.\n    print (\"Error: Too many iterations in RMS fit.\")\n    raise ValueError", "response": "Fit a set of arrays to a set of target_array and source_array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve and download PFAM results from the HMMER search tool.", "response": "def manual_get_pfam_annotations(seq, outpath, searchtype='phmmer', force_rerun=False):\n    \"\"\"Retrieve and download PFAM results from the HMMER search tool.\n\n    Args:\n        seq:\n        outpath:\n        searchtype:\n        force_rerun:\n\n    Returns:\n\n    Todo:\n        * Document and test!\n\n    \"\"\"\n    if op.exists(outpath):\n        with open(outpath, 'r') as f:\n            json_results = json.loads(json.load(f))\n\n    else:\n        fseq = '>Seq\\n' + seq\n        if searchtype == 'phmmer':\n            parameters = {'seqdb': 'pdb', 'seq': fseq}\n        if searchtype == 'hmmscan':\n            parameters = {'hmmdb': 'pfam', 'seq': fseq}\n        enc_params = urllib.urlencode(parameters).encode('utf-8')\n        request = urllib2.Request('http://www.ebi.ac.uk/Tools/hmmer/search/{}'.format(searchtype), enc_params)\n        url = (urllib2.urlopen(request).geturl() + '?output=json')\n        request = str(url)\n        request_read = urlopen(request).read().decode(\"utf-8\")\n\n        with open(outpath, 'w') as f:\n            json.dump(request_read, f)\n\n        json_results = json.loads(request_read)\n\n    return json_results['results']['hits']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_ipynb():\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == 'ZMQInteractiveShell':  # Jupyter notebook or qtconsole?\n            return True\n        elif shell == 'TerminalInteractiveShell':  # Terminal running IPython?\n            return False\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False", "response": "Return True if the module is running in IPython kernel False if the module is running in IPython shell or other Python shell"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_df(df, fill_nan=True, drop_empty_columns=True):\n    if fill_nan:\n        df = df.fillna(value=np.nan)\n    if drop_empty_columns:\n        df = df.dropna(axis=1, how='all')\n    return df.sort_index()", "response": "Clean a pandas dataframe by fill_nan = True drop_empty_columns = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans a dictionary with values that contain single item iterators to single item ones", "response": "def clean_single_dict(indict, prepend_to_keys=None, remove_keys_containing=None):\n    \"\"\"Clean a dict with values that contain single item iterators to single items\n\n    Args:\n        indict (dict): Dictionary to be cleaned\n        prepend_to_keys (str): String to prepend to all keys\n        remove_keys_containing (str): Text to check for in keys to ignore\n\n    Returns:\n         dict: Cleaned dictionary\n\n    Examples:\n        >>> clean_single_dict(indict={'test1': [1], 'test2': ['H']})\n        {'test1': 1, 'test2': 'H'}\n\n        >>> clean_single_dict(indict={'test1': [1], 'test2': ['H']}, prepend_to_keys='struct_')\n        {'struct_test1': 1, 'struct_test2': 'H'}\n\n        >>> clean_single_dict(indict={'test1': [1], 'ignore': ['H']}, prepend_to_keys='struct_', remove_keys_containing='ignore')\n        {'struct_test1': 1}\n\n    \"\"\"\n    if not prepend_to_keys:\n        prepend_to_keys = ''\n\n    outdict = {}\n    for k, v in indict.items():\n        if remove_keys_containing:\n            if remove_keys_containing in k:\n                continue\n        outdict[prepend_to_keys + k] = v[0]\n\n    return outdict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a parameter to be used is None and if it is not then throw an error if it is not.", "response": "def double_check_attribute(object, setter, backup_attribute, custom_error_text=None):\n    \"\"\"Check if a parameter to be used is None, if it is, then check the specified backup attribute and throw\n    an error if it is also None.\n\n    Args:\n        object: The original object\n        setter: Any input object\n        backup_attribute (str): Attribute in <object> to be double checked\n        custom_error_text (str): If a custom string for the error should be raised\n\n    Raises:\n         ValueError: If both setter and backup_attribute are None\n\n    \"\"\"\n    if not setter:\n        next_checker = getattr(object, backup_attribute)\n        if not next_checker:\n            if custom_error_text:\n                raise ValueError(custom_error_text)\n            else:\n                raise ValueError('Attribute replacing \"{}\" must be specified'.format(backup_attribute))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_folder_and_path(filepath):\n    dirname = op.dirname(filepath)\n    filename = op.basename(filepath)\n    splitext = op.splitext(filename)\n    filename_without_extension = splitext[0]\n    extension = splitext[1]\n\n    return dirname, filename_without_extension, extension", "response": "Splits a file path into its folder filename and extension and returns a tuple of the folder filename without extension and extension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef outfile_maker(inname, outext='.out', outname='', outdir='', append_to_name=''):\n\n    # TODO: CHECK IF OUTNAME IS A VALID FILE NAME!\n    orig_dir, orig_name, orig_ext = split_folder_and_path(inname)\n\n    # If output filename not provided, default is to take name of inname\n    if not outname:\n        outname = orig_name\n\n    # Create new path in the same directory of old path if a new one isn't specified\n    if not outdir:\n        outdir = orig_dir\n\n    # Append additional stuff to the filename if specified\n    if append_to_name:\n        outname += append_to_name\n\n    # Join the output filename and output extension\n    final_outfile = op.join(outdir, '{}{}'.format(outname, outext))\n\n    return final_outfile", "response": "Create a default name for an output file based on the inname name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef force_rerun(flag, outfile):\n    # If flag is True, always run\n    if flag:\n        return True\n    # If flag is False but file doesn't exist, also run\n    elif not flag and not op.exists(outfile):\n        return True\n    # If flag is False but filesize of output is 0, also run\n    elif not flag and not is_non_zero_file(outfile):\n        return True\n    # Otherwise, do not run\n    else:\n        return False", "response": "Check if we should force rerunning of a command if an output file exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gunzip_file(infile, outfile=None, outdir=None, delete_original=False, force_rerun_flag=False):\n    if not outfile:\n        outfile = infile.replace('.gz', '')\n\n    if not outdir:\n        outdir = ''\n    else:\n        outdir = op.dirname(infile)\n    outfile = op.join(outdir, op.basename(outfile))\n\n    if force_rerun(flag=force_rerun_flag, outfile=outfile):\n        gz = gzip.open(infile, \"rb\")\n        decoded = gz.read()\n\n        with open(outfile, \"wb\") as new_file:\n            new_file.write(decoded)\n\n        gz.close()\n        log.debug('{}: file unzipped'.format(outfile))\n    else:\n        log.debug('{}: file already unzipped'.format(outfile))\n\n    if delete_original:\n        os.remove(infile)\n\n    return outfile", "response": "Decompress a gzip file and optionally set output values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request_file(link, outfile, force_rerun_flag=False):\n    if force_rerun(flag=force_rerun_flag, outfile=outfile):\n        req = requests.get(link)\n        if req.status_code == 200:\n            with open(outfile, 'w') as f:\n                f.write(req.text)\n            log.debug('Loaded and saved {} to {}'.format(link, outfile))\n        else:\n            log.error('{}: request error {}'.format(link, req.status_code))\n    return outfile", "response": "Download a file given a URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload a file in JSON format from a web request", "response": "def request_json(link, outfile, force_rerun_flag, outdir=None):\n    \"\"\"Download a file in JSON format from a web request\n\n    Args:\n        link: Link to web request\n        outfile: Name of output file\n        outdir: Directory of output file\n        force_rerun_flag: If true, redownload the file\n\n    Returns:\n        dict: contents of the JSON request\n\n    \"\"\"\n    if not outdir:\n        outdir = ''\n    outfile = op.join(outdir, outfile)\n\n    if force_rerun(flag=force_rerun_flag, outfile=outfile):\n        text_raw = requests.get(link)\n        my_dict = text_raw.json()\n        with open(outfile, 'w') as f:\n            json.dump(my_dict, f)\n\n        log.debug('Loaded and saved {} to {}'.format(link, outfile))\n    else:\n        with open(outfile, 'r') as f:\n            my_dict = json.load(f)\n        log.debug('Loaded {}'.format(outfile))\n\n    return my_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a shell command and return the result of the command.", "response": "def command_runner(shell_command, force_rerun_flag, outfile_checker, cwd=None, silent=False):\n    \"\"\"Run a shell command with subprocess, with additional options to check if output file exists and printing stdout.\n\n    Args:\n        shell_command (str): Command as it would be formatted in the command-line (ie. \"program -i test.in -o test.out\").\n        force_rerun_flag: If the program should be rerun even if the output file exists.\n        outfile_checker (str): Name out the output file which may have been generated. This does not specify what the outfile\n            will be, that should be done in the program's args or predetermined.\n        cwd (str): Path to working directory where command will be executed.\n        silent (bool): If program STDOUT should be printed to the current shell.\n\n    Returns:\n        bool: If the program ran successfully.\n\n    \"\"\"\n    program_and_args = shlex.split(shell_command)\n\n    # Check if program is installed\n    if not program_exists(program_and_args[0]):\n        raise OSError('{}: program not installed'.format(program_and_args[0]))\n\n    # Format outfile if working in cwd\n    if cwd:\n        # TODO: should this be done, or should user explicitly define whole outfile path?\n        outfile_checker = op.join(cwd, op.basename(outfile_checker))\n\n    # Check for force rerunning\n    if force_rerun(flag=force_rerun_flag, outfile=outfile_checker):\n        if silent:\n            command = subprocess.Popen(program_and_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n            out, err = command.communicate()\n            ret = command.returncode\n        else:\n            # Prints output\n            for path in execute(cmd=program_and_args, cwd=cwd):\n                print(path, end=\"\")\n\n        # TODO: check return code and log properly\n        log.debug('{}: Ran program, output to {}'.format(program_and_args[0], outfile_checker))\n    else:\n        log.debug('{}: Output already exists'.format(outfile_checker))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_head(d, N=5):\n    return {k: d[k] for k in list(d.keys())[:N]}", "response": "Return the first N key value pairs in a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rank_dated_files(pattern, dir, descending=True):\n    files = glob.glob(op.join(dir, pattern))\n    return sorted(files, reverse=descending)", "response": "Search a directory for files that match a pattern. Return an ordered list of these files by filename."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find(lst, a, case_sensitive=True):\n    a = force_list(a)\n\n    if not case_sensitive:\n        lst = [x.lower() for x in lst]\n        a = [y.lower() for y in a]\n\n    return [i for i, x in enumerate(lst) if x in a]", "response": "Return indices of a list which have elements that match an object or list of objects\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_list(lst, takeout, case_sensitive=True):\n    takeout = force_list(takeout)\n\n    if not case_sensitive:\n        lst = [x.lower() for x in lst]\n        takeout = [y.lower() for y in takeout]\n\n    return [x for x in lst if x not in takeout]", "response": "Returns a modified list removing items specified in takeout"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_list_by_indices(lst, indices):\n    return [x for i, x in enumerate(lst) if i in indices]", "response": "Return a modified list containing only the indices indicated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nforce a string representation of an object", "response": "def force_string(val=None):\n    \"\"\"Force a string representation of an object\n\n    Args:\n        val: object to parse into a string\n\n    Returns:\n        str: String representation\n\n    \"\"\"\n    if val is None:\n        return ''\n    if isinstance(val, list):\n        newval = [str(x) for x in val]\n        return ';'.join(newval)\n    if isinstance(val, str):\n        return val\n    else:\n        return str(val)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef force_list(val=None):\n    if val is None:\n        return []\n    if isinstance(val, pd.Series):\n        return val.tolist()\n    return val if isinstance(val, list) else [val]", "response": "Force a list representation of an object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_list_by_n(l, n):\n    n = max(1, n)\n    return list(l[i:i+n] for i in range(0, len(l), n))", "response": "Split a list into lists of size n."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a single list out of a list of lists and drop all duplicates.", "response": "def flatlist_dropdup(list_of_lists):\n    \"\"\"Make a single list out of a list of lists, and drop all duplicates.\n\n    Args:\n        list_of_lists: List of lists.\n\n    Returns:\n        list: List of single objects.\n\n    \"\"\"\n    return list(set([str(item) for sublist in list_of_lists for item in sublist]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates combinations of A and B.", "response": "def combinations(iterable, r):\n    \"\"\"Calculate combinations\n\n    >>> list(combinations('ABCD',2))\n    [['A', 'B'], ['A', 'C'], ['A', 'D'], ['B', 'C'], ['B', 'D'], ['C', 'D']]\n\n    >>> list(combinations(range(4), 3))\n    [[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]]\n\n    Args:\n        iterable: Any iterable object.\n        r: Size of combination.\n\n    Yields:\n        list: Combination of size r.\n\n    \"\"\"\n    pool = tuple(iterable)\n    n = len(pool)\n    if r > n:\n        return\n\n    indices = list(range(r))\n    yield list(pool[i] for i in indices)\n\n    while True:\n        for i in reversed(range(r)):\n            if indices[i] != i + n - r:\n                break\n        else:\n            return\n        indices[i] += 1\n        for j in range(i + 1, r):\n            indices[j] = indices[j - 1] + 1\n        yield list(pool[i] for i in indices)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef conv_to_float(indata, inf_str=''):\n    if indata.strip() == inf_str:\n        outdata = float('Inf')\n    else:\n        try:\n            outdata = float(indata)\n        except:\n            raise ValueError('Unable to convert {} to float'.format(indata))\n\n    return outdata", "response": "Try to convert an arbitrary string to a float."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap a number to a 0 to 1 scale", "response": "def remap( x, oMin, oMax, nMin, nMax ):\n    \"\"\"Map to a 0 to 1 scale\n        http://stackoverflow.com/questions/929103/convert-a-number-range-to-another-range-maintaining-ratio\n\n    \"\"\"\n\n    #range check\n    if oMin == oMax:\n        log.warning(\"Zero input range, unable to rescale\")\n        return x\n\n    if nMin == nMax:\n        log.warning(\"Zero output range, unable to rescale\")\n        return x\n\n    #check reversed input range\n    reverseInput = False\n    oldMin = min( oMin, oMax )\n    oldMax = max( oMin, oMax )\n    if not oldMin == oMin:\n        reverseInput = True\n\n    #check reversed output range\n    reverseOutput = False\n    newMin = min( nMin, nMax )\n    newMax = max( nMin, nMax )\n    if not newMin == nMin :\n        reverseOutput = True\n\n    portion = (x-oldMin)*(newMax-newMin)/(oldMax-oldMin)\n    if reverseInput:\n        portion = (oldMax-x)*(newMax-newMin)/(oldMax-oldMin)\n\n    result = portion + newMin\n    if reverseOutput:\n        result = newMax - portion\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scale_calculator(multiplier, elements, rescale=None):\n\n    # TODO: think about what happens when:\n    # TODO: 1. there is only one (or n) of each element, and rescale is set to seomthing. what is the original min/max to scale from?\n    # TODO: 2. can we normalize the scale based on other counts? (ie. other gene mutation frequencies)\n\n    if isinstance(elements, list):\n        unique_elements = list(set(elements))\n        scales = {}\n        for x in unique_elements:\n            count = elements.count(x)\n            scales[x] = multiplier * count\n    elif isinstance(elements, dict):\n        scales = {}\n        for k,count in elements.items():\n            scales[k] = multiplier * int(count)\n    else:\n        raise ValueError('Input list of elements or dictionary of elements & counts')\n\n    if not rescale:\n        return scales\n    else:\n        new_scales = {}\n        for k,v in scales.items():\n            new_scales[k] = remap(v, min(scales.values()), max(scales.values()), rescale[0], rescale[1])\n        return new_scales", "response": "Returns a dictionary of scales for each element in elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninputs a list of labeled tuples and return a dictionary of sequentially labeled regions.", "response": "def label_sequential_regions(inlist):\n    \"\"\"Input a list of labeled tuples and return a dictionary of sequentially labeled regions.\n\n    Args:\n        inlist (list): A list of tuples with the first number representing the index and the second the index label.\n\n    Returns:\n        dict: Dictionary of labeled regions.\n\n    Examples:\n\n        >>> label_sequential_regions([(1, 'O'), (2, 'O'), (3, 'O'), (4, 'M'), (5, 'M'), (6, 'I'), (7, 'M'), (8, 'O'), (9, 'O')])\n        {'O1': [1, 2, 3], 'M1': [4, 5], 'I1': [6], 'M2': [7], 'O2': [8, 9]}\n\n    \"\"\"\n    import more_itertools as mit\n\n    df = pd.DataFrame(inlist).set_index(0)\n\n    labeled = {}\n    for label in df[1].unique():\n        iterable = df[df[1] == label].index.tolist()\n        labeled.update({'{}{}'.format(label, i + 1): items for i, items in\n                        enumerate([list(group) for group in mit.consecutive_groups(iterable)])})\n\n    return labeled"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef seq(self):\n\n        if self.sequence_file:\n            file_to_load = copy(self.sequence_path)\n            log.debug('{}: reading sequence from sequence file {}'.format(self.id, file_to_load))\n            tmp_sr = SeqIO.read(file_to_load, 'fasta')\n            return tmp_sr.seq\n\n        else:\n            if not self._seq:\n                log.debug('{}: no sequence stored in memory'.format(self.id))\n            else:\n                log.debug('{}: reading sequence from memory'.format(self.id))\n\n            return self._seq", "response": "Dynamically loaded Seq object from the sequence file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef features(self):\n\n        if self.feature_file:\n            log.debug('{}: reading features from feature file {}'.format(self.id, self.feature_path))\n            with open(self.feature_path) as handle:\n                feats = list(GFF.parse(handle))\n                if len(feats) > 1:\n                    log.warning('Too many sequences in GFF')\n                else:\n                    return feats[0].features\n\n        else:\n            return self._features", "response": "Get the features stored in memory or in the GFF file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the sequence formatted as a string", "response": "def seq_str(self):\n        \"\"\"str: Get the sequence formatted as a string\"\"\"\n\n        if not self.seq:\n            return None\n\n        return ssbio.protein.sequence.utils.cast_to_str(self.seq)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sequence_path(self, fasta_path):\n        if not fasta_path:\n            self.sequence_dir = None\n            self.sequence_file = None\n\n        else:\n            if not op.exists(fasta_path):\n                raise OSError('{}: file does not exist'.format(fasta_path))\n\n            if not op.dirname(fasta_path):\n                self.sequence_dir = '.'\n            else:\n                self.sequence_dir = op.dirname(fasta_path)\n            self.sequence_file = op.basename(fasta_path)\n\n            tmp_sr = SeqIO.read(fasta_path, 'fasta')\n            if self.name == '<unknown name>':\n                self.name = tmp_sr.name\n            if self.description == '<unknown description>':\n                self.description = tmp_sr.description\n            if not self.dbxrefs:\n                self.dbxrefs = tmp_sr.dbxrefs\n            if not self.features:\n                self.features = tmp_sr.features\n            if not self.annotations:\n                self.annotations = tmp_sr.annotations\n            if not self.letter_annotations:\n                self.letter_annotations = tmp_sr.letter_annotations", "response": "Provide pointers to the paths of the sequences in the class attribute"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprovides pointers to the paths of the metadata file that are located in the metadata directory.", "response": "def metadata_path(self, m_path):\n        \"\"\"Provide pointers to the paths of the metadata file\n\n        Args:\n            m_path: Path to metadata file\n\n        \"\"\"\n        if not m_path:\n            self.metadata_dir = None\n            self.metadata_file = None\n\n        else:\n            if not op.exists(m_path):\n                raise OSError('{}: file does not exist!'.format(m_path))\n\n            if not op.dirname(m_path):\n                self.metadata_dir = '.'\n            else:\n                self.metadata_dir = op.dirname(m_path)\n            self.metadata_file = op.basename(m_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef feature_path(self, gff_path):\n        if not gff_path:\n            self.feature_dir = None\n            self.feature_file = None\n\n        else:\n            if not op.exists(gff_path):\n                raise OSError('{}: file does not exist!'.format(gff_path))\n\n            if not op.dirname(gff_path):\n                self.feature_dir = '.'\n            else:\n                self.feature_dir = op.dirname(gff_path)\n            self.feature_file = op.basename(gff_path)", "response": "Load a GFF file with information on a single sequence and store features in the features attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies features to memory and remove the association of the feature file.", "response": "def feature_path_unset(self):\n        \"\"\"Copy features to memory and remove the association of the feature file.\"\"\"\n        if not self.feature_file:\n            raise IOError('No feature file to unset')\n\n        with open(self.feature_path) as handle:\n            feats = list(GFF.parse(handle))\n            if len(feats) > 1:\n                log.warning('Too many sequences in GFF')\n            else:\n                tmp = feats[0].features\n\n        self.feature_dir = None\n        self.feature_file = None\n        self.features = tmp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of attributes that can be used to store the object in a Pandas DataFrame.", "response": "def get_dict(self, only_attributes=None, exclude_attributes=None, df_format=False):\n        \"\"\"Get a dictionary of this object's attributes. Optional format for storage in a Pandas DataFrame.\n\n        Args:\n            only_attributes (str, list): Attributes that should be returned. If not provided, all are returned.\n            exclude_attributes (str, list): Attributes that should be excluded.\n            df_format (bool): If dictionary values should be formatted for a dataframe\n                (everything possible is transformed into strings, int, or float -\n                if something can't be transformed it is excluded)\n\n        Returns:\n            dict: Dictionary of attributes\n\n        \"\"\"\n\n        # Choose attributes to return, return everything in the object if a list is not specified\n        if not only_attributes:\n            keys = list(self.__dict__.keys())\n        else:\n            keys = ssbio.utils.force_list(only_attributes)\n\n        # Remove keys you don't want returned\n        if exclude_attributes:\n            exclude_attributes = ssbio.utils.force_list(exclude_attributes)\n            for x in exclude_attributes:\n                if x in keys:\n                    keys.remove(x)\n\n        # Copy attributes into a new dictionary\n        df_dict = {}\n        for k, orig_v in self.__dict__.items():\n            if k in keys:\n                v = deepcopy(orig_v)\n                if df_format:\n                    if v and not isinstance(v, str) and not isinstance(v, int) and not isinstance(v,\n                                                                                                  float) and not isinstance(\n                            v, bool):\n                        try:\n                            df_dict[k] = ssbio.utils.force_string(deepcopy(v))\n                        except TypeError:\n                            log.warning('{}: excluding attribute from dict, cannot transform into string'.format(k))\n                    elif not v and not isinstance(v, int) and not isinstance(v, float):\n                        df_dict[k] = None\n                    else:\n                        df_dict[k] = deepcopy(v)\n                else:\n                    df_dict[k] = deepcopy(v)\n        return df_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest if the sequence is equal to another SeqProp s sequence", "response": "def equal_to(self, seq_prop):\n        \"\"\"Test if the sequence is equal to another SeqProp's sequence\n\n        Args:\n            seq_prop: SeqProp object\n\n        Returns:\n            bool: If the sequences are the same\n\n        \"\"\"\n        if not self.seq or not seq_prop or not seq_prop.seq:\n            return False\n\n        return self.seq == seq_prop.seq"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a FASTA file for the protein sequence.", "response": "def write_fasta_file(self, outfile, force_rerun=False):\n        \"\"\"Write a FASTA file for the protein sequence, ``seq`` will now load directly from this file.\n\n        Args:\n            outfile (str): Path to new FASTA file to be written to\n            force_rerun (bool): If an existing file should be overwritten\n\n        \"\"\"\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n            SeqIO.write(self, outfile, \"fasta\")\n\n        # The Seq as it will now be dynamically loaded from the file\n        self.sequence_path = outfile"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_gff_file(self, outfile, force_rerun=False):\n        if ssbio.utils.force_rerun(outfile=outfile, flag=force_rerun):\n            with open(outfile, \"w\") as out_handle:\n                GFF.write([self], out_handle)\n\n        self.feature_path = outfile", "response": "Write a GFF file for the protein features."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_point_feature(self, resnum, feat_type=None, feat_id=None, qualifiers=None):\n        if self.feature_file:\n            raise ValueError('Feature file associated with sequence, please remove file association to append '\n                             'additional features.')\n\n        if not feat_type:\n            feat_type = 'Manually added protein sequence single residue feature'\n        newfeat = SeqFeature(location=FeatureLocation(ExactPosition(resnum-1), ExactPosition(resnum)),\n                             type=feat_type,\n                             id=feat_id,\n                             qualifiers=qualifiers)\n\n        self.features.append(newfeat)", "response": "Adds a feature describing a single residue."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a feature describing a region of the protein sequence.", "response": "def add_region_feature(self, start_resnum, end_resnum, feat_type=None, feat_id=None, qualifiers=None):\n        \"\"\"Add a feature to the features list describing a region of the protein sequence.\n\n        Args:\n            start_resnum (int): Start residue number of the protein sequence feature\n            end_resnum (int): End residue number of the protein sequence feature\n            feat_type (str, optional): Optional description of the feature type (ie. 'binding domain')\n            feat_id (str, optional): Optional ID of the feature type (ie. 'TM1')\n\n        \"\"\"\n        if self.feature_file:\n            raise ValueError('Feature file associated with sequence, please remove file association to append '\n                             'additional features.')\n\n        if not feat_type:\n            feat_type = 'Manually added protein sequence region feature'\n        newfeat = SeqFeature(location=FeatureLocation(start_resnum-1, end_resnum),\n                             type=feat_type,\n                             id=feat_id,\n                             qualifiers=qualifiers)\n\n        self.features.append(newfeat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a subsequence as a new SeqProp object given a list of residue numbers", "response": "def get_subsequence(self, resnums, new_id=None, copy_letter_annotations=True):\n        \"\"\"Get a subsequence as a new SeqProp object given a list of residue numbers\"\"\"\n        # XTODO: documentation\n        biop_compound_list = []\n        for resnum in resnums:\n            # XTODO can be sped up by separating into ranges based on continuous resnums\n            feat = FeatureLocation(resnum - 1, resnum)\n            biop_compound_list.append(feat)\n\n        if len(biop_compound_list) == 0:\n            log.debug('Zero length subsequence')\n            return\n        elif len(biop_compound_list) == 1:\n            log.debug('Subsequence only one residue long')\n            sub_feature_location = biop_compound_list[0]\n        else:\n            sub_feature_location = CompoundLocation(biop_compound_list)\n\n        try:\n            sub_feature = sub_feature_location.extract(self)\n        except TypeError:\n            log.critical('SeqProp {}: unknown error when trying to get subsequence - please investigate! '\n                          'Try using a feature to extract a subsequence from the SeqProp'.format(self.id))\n            return\n\n        if not new_id:\n            new_id = '{}_subseq'.format(self.id)\n\n        new_sp = SeqProp(id=new_id, seq=sub_feature.seq)\n        if copy_letter_annotations:\n            new_sp.letter_annotations = sub_feature.letter_annotations\n        return new_sp"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a subsequence from a certain property.", "response": "def get_subsequence_from_property(self, property_key, property_value, condition,\n                                      return_resnums=False, copy_letter_annotations=True):\n        \"\"\"Get a subsequence as a new SeqProp object given a certain property you want to find in the\n        original SeqProp's letter_annotation\n\n        This can be used to do something like extract the subsequence of exposed residues, so you can can run\n        calculations on that subsequence. Useful if you have questions like \"are there any predicted surface exposed\n        cysteines in my protein sequence?\"\n\n        Example:\n            >>> sp = SeqProp(id='tester', seq='MQSLE')\n            >>> sp.letter_annotations['a_key'] = [2, 2, 3, 1, 0]\n            >>> pk = 'a_key'\n            >>> pv = 2\n            >>> cond = '<'\n            >>> new_sp = sp.get_subsequence_from_property(pk, pv, cond)\n            >>> new_sp.letter_annotations[pk]\n            [1, 0]\n            >>> new_sp\n            SeqProp(seq=Seq('LE', ExtendedIUPACProtein()), id='tester_a_key_<_2_extracted', name='<unknown name>', description='<unknown description>', dbxrefs=[])\n\n        Args:\n            property_key (str): Property key in the ``letter_annotations`` attribute that you want to filter using\n            property_value (str): Property value that you want to filter by\n            condition (str): ``<``, ``=``, ``>``, ``>=``, or ``<=`` to filter the values by\n\n        Returns:\n            SeqProp: New SeqProp object that you can run computations on or just extract its properties\n\n        \"\"\"\n\n        if property_key not in self.letter_annotations:\n            log.error('{}: {} not contained in the letter annotations'.format(self.id, property_key))\n            return\n\n        if condition == 'in':\n            subfeat_indices = list(locate(self.letter_annotations[property_key],\n                                          lambda x: x in property_value))\n        else:\n            subfeat_indices = list(\n                locate(self.letter_annotations[property_key], lambda x: ssbio.utils.check_condition(x, condition, property_value)))\n        subfeat_resnums = [x+1 for x in subfeat_indices]\n\n        new_sp = self.get_subsequence(resnums=subfeat_resnums, new_id='{}_{}_{}_{}_extracted'.format(self.id,\n                                                                                                    property_key,\n                                                                                                    condition,\n                                                                                                    property_value),\n                                      copy_letter_annotations=copy_letter_annotations)\n\n        if return_resnums:\n            return new_sp, subfeat_resnums\n        else:\n            return new_sp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_biopython_pepstats(self, clean_seq=False):\n\n        if self.seq:\n            if clean_seq:  # TODO: can make this a property of the SeqProp class\n                seq = self.seq_str.replace('X', '').replace('U', '')\n            else:\n                seq = self.seq_str\n\n            try:\n                pepstats = ssbio.protein.sequence.properties.residues.biopython_protein_analysis(seq)\n            except KeyError as e:\n                log.error('{}: unable to run ProteinAnalysis module, unknown amino acid {}'.format(self.id, e))\n                return\n            except ValueError as e:\n                log.error('{}: unable to run ProteinAnalysis module, {}'.format(self.id, e))\n                return\n            self.annotations.update(pepstats)\n        else:\n            raise ValueError('{}: no sequence available, unable to run ProteinAnalysis'.format(self.id))", "response": "Run Biopython s built in ProteinAnalysis module and store statistics in the annotations attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_emboss_pepstats(self):\n        if not self.sequence_file:\n            raise IOError('FASTA file needs to be written for EMBOSS pepstats to be run')\n        outfile = ssbio.protein.sequence.properties.residues.emboss_pepstats_on_fasta(infile=self.sequence_path)\n        pepstats = ssbio.protein.sequence.properties.residues.emboss_pepstats_parser(outfile)\n        self.annotations.update(pepstats)", "response": "Run the EMBOSS pepstats program on the sequence file and store the statistics in the annotations attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a property calculator given a sliding window size and window size", "response": "def get_sliding_window_properties(self, scale, window):\n        \"\"\"Run a property calculator given a sliding window size\n\n        Stores statistics in the ``letter_annotations`` attribute.\n\n        Todo:\n            - Add and document all scales available to set\n        \"\"\"\n        # XTODO: documentation\n        if self.seq:\n            # if clean_seq:  # TODO: can't do this because letter_annotations will complain about differing seqlen\n            #     seq = self.seq_str.replace('X', '').replace('U', '')\n            # else:\n            #     seq = self.seq_str\n\n            try:\n                prop = ssbio.protein.sequence.properties.residues.biopython_protein_scale(self.seq_str,\n                                                                                          scale=scale,\n                                                                                          window=window)\n            except KeyError as e:\n                log.error('{}: unable to run ProteinAnalysis module, unknown amino acid {}'.format(self.id, e))\n                return\n            except ValueError as e:\n                log.error('{}: unable to run ProteinAnalysis module, {}'.format(self.id, e))\n                return\n            self.letter_annotations['{}-window{}-biop'.format(scale, window)] = prop\n        else:\n            raise ValueError('{}: no sequence available, unable to run ProteinAnalysis'.format(self.id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blast_pdb(self, seq_ident_cutoff=0, evalue=0.0001, display_link=False,\n                  outdir=None, force_rerun=False):\n        \"\"\"BLAST this sequence to the PDB\"\"\"\n        if not outdir:\n            outdir = self.sequence_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        if not self.seq_str:\n            log.error('{}: no sequence loaded'.format(self.id))\n            return None\n\n        try:\n            blast_results = ssbio.databases.pdb_seq.blast_pdb(self.seq_str,\n                                                              outfile='{}_blast_pdb.xml'.format(custom_slugify(self.id)),\n                                                              outdir=outdir,\n                                                              force_rerun=force_rerun,\n                                                              evalue=evalue,\n                                                              seq_ident_cutoff=seq_ident_cutoff,\n                                                              link=display_link)\n        except requests.ConnectionError as e:\n            log.error('{}: BLAST request timed out'.format(self.id))\n            print(e)\n            return None\n\n        return blast_results", "response": "BLAST this sequence to the PDB"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves letter annotations for a residue or a range of residues", "response": "def get_residue_annotations(self, start_resnum, end_resnum=None):\n        \"\"\"Retrieve letter annotations for a residue or a range of residues\n\n        Args:\n            start_resnum (int): Residue number\n            end_resnum (int): Optional residue number, specify if a range is desired\n\n        Returns:\n            dict: Letter annotations for this residue or residues\n\n        \"\"\"\n        if not end_resnum:\n            end_resnum = start_resnum\n\n        # Create a new SeqFeature\n        f = SeqFeature(FeatureLocation(start_resnum - 1, end_resnum))\n\n        # Get sequence properties\n        return f.extract(self).letter_annotations"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_aggregation_propensity(self, email, password, cutoff_v=5, cutoff_n=5, run_amylmuts=False, outdir=None):\n        if not outdir:\n            outdir = self.sequence_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        import ssbio.protein.sequence.properties.aggregation_propensity as agg\n\n        agg_predictions = agg.AMYLPRED(email=email, password=password)\n        result = agg_predictions.get_aggregation_propensity(seq=self, outdir=outdir,\n                                                            cutoff_v=cutoff_v, cutoff_n=cutoff_n,\n                                                            run_amylmuts=run_amylmuts)\n        self.annotations['aggprop-amylpred'] = result", "response": "Run the AMYLPRED2 web server to calculate the aggregation propensity of this protein sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_kinetic_folding_rate(self, secstruct, at_temp=None):\n\n        import ssbio.protein.sequence.properties.kinetic_folding_rate as kfr\n\n        rate = kfr.get_foldrate(seq=self, secstruct=secstruct)\n        self.annotations['kinetic_folding_rate_37.0_C-foldrate'] = rate\n\n        if at_temp:\n            new_rate = kfr.get_foldrate_at_temp(ref_rate=rate, new_temp=at_temp)\n            self.annotations['kinetic_folding_rate_{}_C-foldrate'.format(at_temp)] = new_rate", "response": "Calculate the kinetic folding rate for this amino acid sequence and structural classficiation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the thermostability calculator using Dill or Oobatake methods.", "response": "def get_thermostability(self, at_temp):\n        \"\"\"Run the thermostability calculator using either the Dill or Oobatake methods.\n\n        Stores calculated (dG, Keq) tuple in the ``annotations`` attribute, under the key\n        `thermostability_<TEMP>-<METHOD_USED>`.\n\n        See :func:`ssbio.protein.sequence.properties.thermostability.get_dG_at_T` for instructions and details.\n\n        \"\"\"\n\n        import ssbio.protein.sequence.properties.thermostability as ts\n\n        dG = ts.get_dG_at_T(seq=self, temp=at_temp)\n        self.annotations['thermostability_{}_C-{}'.format(at_temp, dG[2].lower())] = (dG[0], dG[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef store_iupred_disorder_predictions(seqprop, iupred_path,\n                                          iupred_exec, prediction_type, force_rerun=False):\n        \"\"\"Scores above 0.5 indicate disorder\"\"\"\n        os.environ['IUPred_PATH'] = iupred_path\n        stored_key = 'disorder-{}-iupred'.format(prediction_type)\n        if stored_key not in seqprop.letter_annotations or force_rerun:\n            if not seqprop.sequence_file:\n                with tempfile.NamedTemporaryFile(delete=True) as f:\n                    SeqIO.write(seqprop, f.name, \"fasta\")\n                    command = '{} {} {}'.format(iupred_exec, f.name, prediction_type)\n                    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\n                    output = process.communicate()\n                    iupred = [float(x.split()[2]) for x in output[0].decode().split('\\n') if\n                              not x.startswith('#') and len(x) > 0]\n                    seqprop.letter_annotations[stored_key] = iupred\n            else:\n                command = '{} {} {}'.format(iupred_exec, seqprop.sequence_path, prediction_type)\n                process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\n                output = process.communicate()\n                iupred = [float(x.split()[2]) for x in output[0].decode().split('\\n') if\n                          not x.startswith('#') and len(x) > 0]\n                seqprop.letter_annotations[stored_key] = iupred", "response": "Store disorder predictions in the sequence file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the accpro20 output file and return the parsed FASTA records.", "response": "def read_accpro20(infile):\n    \"\"\"Read the accpro20 output (.acc20) and return the parsed FASTA records.\n\n    Keeps the spaces between the accessibility numbers.\n\n    Args:\n        infile: Path to .acc20 file\n\n    Returns:\n        dict: Dictionary of accessibilities with keys as the ID\n\n    \"\"\"\n    with open(infile) as f:\n        records = f.read().splitlines()\n\n    accpro20_dict = {}\n    for i, r in enumerate(records):\n        if i % 2 == 0:\n            # TODO: Double check how to parse FASTA IDs (can they have a space because that is what i split by)\n            # Key was originally records[i][1:]\n            accpro20_dict[records[i].split(' ')[0][1:]] = [int(x) for x in records[i + 1].split(' ')]\n\n    return accpro20_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_scratch(self, path_to_scratch, num_cores=1, outname=None, outdir=None, force_rerun=False):\n        if not outname:\n            outname = self.project_name\n        if not outdir:\n            outdir = ''\n\n        outname = op.join(outdir, outname)\n\n        self.out_sspro = '{}.ss'.format(outname)\n        self.out_sspro8 = '{}.ss8'.format(outname)\n        self.out_accpro = '{}.acc'.format(outname)\n        self.out_accpro20 = '{}.acc20'.format(outname)\n\n        # TODO: check for multiple output files in command_runner\n        ssbio.utils.command_runner(\n            shell_command='{} {} {} {}'.format(path_to_scratch, self.seq_file, outname, num_cores),\n            force_rerun_flag=force_rerun, outfile_checker='{}.ss'.format(outname))", "response": "Run the SCRATCH executable on the sequence_file that was loaded into the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sspro_results(self):\n        return ssbio.protein.sequence.utils.fasta.load_fasta_file_as_dict_of_seqs(self.out_sspro)", "response": "Parse the SSpro output file and return a dict of secondary structure compositions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sspro_summary(self):\n        summary = {}\n\n        records = ssbio.protein.sequence.utils.fasta.load_fasta_file(self.out_sspro)\n        for r in records:\n            seq_summary = {}\n            seq_summary['percent_H-sspro'] = r.seq.count('H')/float(len(r))\n            seq_summary['percent_E-sspro'] = r.seq.count('E')/float(len(r))\n            seq_summary['percent_C-sspro'] = r.seq.count('C')/float(len(r))\n\n            summary[r.id] = seq_summary\n\n        return summary", "response": "Parse the SSpro output file and return a summary of secondary structure composition."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the SSpro8 output file and return a dict of secondary structure compositions.", "response": "def sspro8_results(self):\n        \"\"\"Parse the SSpro8 output file and return a dict of secondary structure compositions.\n        \"\"\"\n        return ssbio.protein.sequence.utils.fasta.load_fasta_file_as_dict_of_seqs(self.out_sspro8)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sspro8_summary(self):\n        summary = {}\n\n        records = ssbio.protein.sequence.utils.fasta.load_fasta_file(self.out_sspro8)\n        for r in records:\n            seq_summary = {}\n            seq_summary['percent_H-sspro8'] = r.seq.count('H') / float(len(r))\n            seq_summary['percent_G-sspro8'] = r.seq.count('G') / float(len(r))\n            seq_summary['percent_I-sspro8'] = r.seq.count('I') / float(len(r))\n            seq_summary['percent_E-sspro8'] = r.seq.count('E') / float(len(r))\n            seq_summary['percent_B-sspro8'] = r.seq.count('B') / float(len(r))\n            seq_summary['percent_T-sspro8'] = r.seq.count('T') / float(len(r))\n            seq_summary['percent_S-sspro8'] = r.seq.count('S') / float(len(r))\n            seq_summary['percent_C-sspro8'] = r.seq.count('C') / float(len(r))\n\n            summary[r.id] = seq_summary\n\n        return summary", "response": "Parse the SSpro8 output file and return a summary of secondary structure composition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef accpro_results(self):\n        return ssbio.protein.sequence.utils.fasta.load_fasta_file_as_dict_of_seqs(self.out_accpro)", "response": "Parse the ACCpro output file and return a dict of secondary structure compositions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accpro20_summary(self, cutoff):\n        summary = {}\n\n        if cutoff < 1:\n            cutoff = 1 * 100\n\n        records = read_accpro20(self.out_accpro20)\n\n        for k,v in records.items():\n            seq_summary = {}\n\n            exposed = 0\n            buried = 0\n            for s in v:\n                if s > cutoff:\n                    exposed += 1\n                else:\n                    buried += 1\n\n            seq_summary['percent_exposed-accpro20'] = exposed / float(len(v))\n            seq_summary['percent_buried-accpro20'] = buried / float(len(v))\n\n            summary[k] = seq_summary\n\n        return summary", "response": "Parse the ACCpro output file and return a summary of percent exposed and buried residues based on a cutoff."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconsolidates function to load a GEM using COBRApy. Specify the file type being loaded.", "response": "def model_loader(gem_file_path, gem_file_type):\n    \"\"\"Consolidated function to load a GEM using COBRApy. Specify the file type being loaded.\n\n    Args:\n        gem_file_path (str): Path to model file\n        gem_file_type (str): GEM model type - ``sbml`` (or ``xml``), ``mat``, or ``json`` format\n\n    Returns:\n        COBRApy Model object.\n\n    \"\"\"\n\n    if gem_file_type.lower() == 'xml' or gem_file_type.lower() == 'sbml':\n        model = read_sbml_model(gem_file_path)\n    elif gem_file_type.lower() == 'mat':\n        model = load_matlab_model(gem_file_path)\n    elif gem_file_type.lower() == 'json':\n        model = load_json_model(gem_file_path)\n    else:\n        raise ValueError('File type must be \"sbml\", \"xml\", \"mat\", or \"json\".')\n\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the ID of a COBRApy Gene object matches a spontaneous ID regex.", "response": "def is_spontaneous(gene, custom_id=None):\n    \"\"\"Input a COBRApy Gene object and check if the ID matches a spontaneous ID regex.\n    \n    Args:\n        gene (Gene): COBRApy Gene\n        custom_id (str): Optional custom spontaneous ID if it does not match the regular expression ``[Ss](_|)0001``\n\n    Returns:\n        bool: If gene ID matches spontaneous ID\n\n    \"\"\"\n\n    spont = re.compile(\"[Ss](_|)0001\")\n    if spont.match(gene.id):\n        return True\n    elif gene.id == custom_id:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_out_spontaneous_genes(genes, custom_spont_id=None):\n    new_genes = DictList()\n    for gene in genes:\n        if not is_spontaneous(gene, custom_id=custom_spont_id):\n            new_genes.append(gene)\n\n    return new_genes", "response": "Returns a DictList of genes that are not spontaneous in a model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef true_num_genes(model, custom_spont_id=None):\n    true_num = 0\n    for gene in model.genes:\n        if not is_spontaneous(gene, custom_id=custom_spont_id):\n            true_num += 1\n    return true_num", "response": "Return the number of genes in a model ignoring spontaneously labeled genes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of reactions associated with a gene.", "response": "def true_num_reactions(model, custom_spont_id=None):\n    \"\"\"Return the number of reactions associated with a gene.\n\n    Args:\n        model (Model):\n        custom_spont_id (str): Optional custom spontaneous ID if it does not match the regular expression ``[Ss](_|)0001``\n\n    Returns:\n        int: Number of reactions associated with a gene\n\n    \"\"\"\n    true_num = 0\n    for rxn in model.reactions:\n        if len(rxn.genes) == 0:\n            continue\n        if len(rxn.genes) == 1 and is_spontaneous(list(rxn.genes)[0], custom_id=custom_spont_id):\n            continue\n        else:\n            true_num += 1\n    return true_num"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_fatcat(structure_path_1, structure_path_2, fatcat_sh, outdir='', silent=False, print_cmd=False, force_rerun=False):\n    filename1 = op.splitext(op.basename(structure_path_1))[0]\n    filename2 = op.splitext(op.basename(structure_path_2))[0]\n\n    if not op.exists(outdir):\n        os.mkdir(outdir)\n    outfile = op.join(outdir, filename1 + '__' + filename2 + '.xml')\n\n    # Run FATCAT on the structures, print the XML of the result to stdout\n    fatcat_cmd = '{} -file1 {} -file2 {} -outFile {}'.format(fatcat_sh, structure_path_1, structure_path_2, outfile)\n    if print_cmd:\n        print(fatcat_cmd)\n    ssbio.utils.command_runner(fatcat_cmd, force_rerun_flag=force_rerun, outfile_checker=outfile, silent=silent)\n\n    return outfile", "response": "Run FATCAT on two PDB files and return the path of the XML result file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_fatcat_all_by_all(list_of_structure_paths, fatcat_sh, outdir='', silent=True, force_rerun=False):\n    structure_ids = {x: i for i, x in enumerate(list_of_structure_paths)}\n\n    comps = itertools.combinations(list_of_structure_paths, 2)\n    tm_score_matrix = np.eye(len(list_of_structure_paths))\n\n    for pdb1, pdb2 in tqdm(comps):\n        fatcat_file = run_fatcat(pdb1, pdb2, fatcat_sh, outdir=outdir, silent=silent, force_rerun=force_rerun)\n        tm_score_matrix[structure_ids[pdb1], structure_ids[pdb2]] = parse_fatcat(fatcat_file)['tm_score']\n        tm_score_matrix[structure_ids[pdb2], structure_ids[pdb1]] = parse_fatcat(fatcat_file)['tm_score']\n\n    # Convert to dataframe with filenames\n    filenames = [op.splitext(op.basename(x))[0] for x in list_of_structure_paths]\n    tm_score_matrix_annotated = pd.DataFrame(data=tm_score_matrix, columns=filenames, index=filenames)\n\n    return tm_score_matrix_annotated", "response": "Run FATCAT on all pairs of structures and return a Pandas DataFrame with the TM - scores between all structures."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_fatcat(fatcat_xml):\n    fatcat_results = {}\n\n    # Parse output xml file\n    with open(fatcat_xml, 'r') as f:\n        soup = BeautifulSoup(f, 'lxml')\n\n    # Find the tmScore of the alignment\n    if soup.find('block'):\n        fatcat_results['tm_score'] = float(soup.find('afpchain')['tmscore'])\n\n    return fatcat_results", "response": "Parse a FATCAT XML result file and return a dictionary of the results"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _smcra_to_str(self, smcra, temp_dir='/tmp/'):\n        \n        temp_path = tempfile.mktemp( '.pdb', dir=temp_dir )\n        \n        io = PDBIO()\n        io.set_structure(smcra)\n        io.save(temp_path)\n        \n        f = open(temp_path, 'r')\n        string = f.read()\n        f.close()\n        \n        os.remove(temp_path)\n        \n        return string", "response": "Converts a SMCRA object to a PDB formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_alive(self):\n        \n        u = urllib.urlopen(\"http://wiws.cmbi.ru.nl/rest/TestEmpty/id/1crn/\")\n        x = xml.dom.minidom.parse(u)\n        self.alive = len(x.getElementsByTagName(\"TestEmptyResponse\"))\n\n        return self.alive", "response": "Test Function to check WHAT IF servers are up and running."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuploads a structure to the server.", "response": "def UploadPDB(self, structure):\n        \"\"\"\n        Uploads a structure to the Server.\n        Allowed input format: Bio.PDB Structure object, PDB formatted string\n        Returns id for future services.\n        \"\"\"\n        \n        if isinstance(structure, Entity): # SMCRA\n            s = self._smcra_to_str(structure)\n\n        elif isinstance(structure, str): # String\n            s = structure\n        \n        else:\n            raise ValueError('Unknown format. Use SMCRA object or string.')\n            \n        u = urllib.urlopen(\"http://www.cmbi.ru.nl/wiwsd/rest/UploadPDB\", s)\n        x = xml.dom.minidom.parse(u)\n        \n        id = x.getElementsByTagName(\"response\")[0].childNodes[0].data\n\n        return id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds Hydrogen Atoms to a Structure.", "response": "def PDBasXMLwithSymwithPolarH(self, id):\n        \"\"\"\n        Adds Hydrogen Atoms to a Structure.\n        \"\"\"\n        \n        print _WARNING\n        # Protonated Structure in XML Format\n        h_s_xml = urllib.urlopen(\"http://www.cmbi.ru.nl/wiwsd/rest/PDBasXMLwithSymwithPolarH/id/\" + id)\n        self.raw = h_s_xml\n        p = self.parser\n        h_s_smcra = p.read(h_s_xml, 'WHATIF_Output')\n        \n        return h_s_smcra"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_pdbs_for_gene(bigg_model, bigg_gene, cache_dir=tempfile.gettempdir(), force_rerun=False):\n    my_structures = []\n\n    # Download gene info\n    gene = ssbio.utils.request_json(link='http://bigg.ucsd.edu/api/v2/models/{}/genes/{}'.format(bigg_model, bigg_gene),\n                                    outfile='{}_{}.json'.format(bigg_model, bigg_gene),\n                                    outdir=cache_dir,\n                                    force_rerun_flag=force_rerun)\n\n    uniprots = []\n    if 'database_links' in gene:\n        if 'UniProt' in gene['database_links']:\n            uniprots = [x['id'] for x in gene['database_links']['UniProt']]\n        elif 'NCBI GI' in gene['database_links']:\n            uniprots = []\n            gis = [x['id'] for x in gene['database_links']['NCBI GI']]\n            gi_uniprots = bs_unip.mapping(fr='P_GI', to='ACC', query=gis).values()\n            uniprots.extend(gi_uniprots)\n            uniprots = ssbio.utils.flatlist_dropdup(uniprots)\n            uniprots = [x for x in uniprots if ssbio.databases.uniprot.is_valid_uniprot_id(x)]\n\n    if uniprots:\n        for u in uniprots:\n            get_best_structure = ssbio.databases.pdb.best_structures(uniprot_id=u, outdir=cache_dir)\n            if get_best_structure:\n                for best_structure in get_best_structure:\n                    my_structures.append((best_structure['pdb_id'], best_structure['chain_id']))\n\n    return my_structures", "response": "Get a rank - ordered list of PDB structures for a given BiGG Model and gene."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dssp_df_on_file(pdb_file, outfile=None, outdir=None, outext='_dssp.df', force_rerun=False):\n    # TODO: function unfinished\n    # Create the output file name\n    outfile = ssbio.utils.outfile_maker(inname=pdb_file, outname=outfile, outdir=outdir, outext=outext)\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        try:\n            d = dssp_dict_from_pdb_file(pdb_file)\n        except Exception('DSSP failed to produce an output'):\n            log.error('{}: unable to run DSSP'.format(pdb_file))\n            return pd.DataFrame()\n\n        appender = []\n        # TODO: WARNING: d is slightly different than when using function get_dssp_df\n        for k in d[1]:\n            to_append = []\n            y = d[0][k]\n            chain = k[0]\n            residue = k[1]\n            het = residue[0]\n            resnum = residue[1]\n            icode = residue[2]\n            to_append.extend([chain, resnum, icode])\n            to_append.extend(y)\n            appender.append(to_append)\n\n        cols = ['chain', 'resnum', 'icode',\n                'dssp_index', 'aa', 'ss', 'exposure_rsa', 'phi', 'psi',\n                'NH_O_1_relidx', 'NH_O_1_energy', 'O_NH_1_relidx',\n                'O_NH_1_energy', 'NH_O_2_relidx', 'NH_O_2_energy',\n                'O_NH_2_relidx', 'O_NH_2_energy']\n\n        df = pd.DataFrame.from_records(appender, columns=cols)\n\n        # Adding additional columns\n        df = df[df['aa'].isin(list(aa1))]\n        df['aa_three'] = df['aa'].apply(one_to_three)\n        df['max_acc'] = df['aa_three'].map(residue_max_acc['Sander'].get)\n        df[['exposure_rsa', 'max_acc']] = df[['exposure_rsa', 'max_acc']].astype(float)\n        df['exposure_asa'] = df['exposure_rsa'] * df['max_acc']\n\n        df.to_csv(outfile)\n    else:\n        log.debug('{}: already ran DSSP and force_rerun={}, loading results'.format(outfile, force_rerun))\n        df = pd.read_csv(outfile, index_col=0)\n\n    return df", "response": "Run DSSP directly on a structure file with the Biopython method Bio. PDB. DSSP. dssp_dict_from_pdb_file\nMimeType"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef secondary_structure_summary(dssp_df):\n    chains = dssp_df.chain.unique()\n\n    infodict = {}\n    for chain in chains:\n        expoinfo = defaultdict(int)\n        chain_df = dssp_df[dssp_df.chain == chain]\n        counts = chain_df.ss.value_counts()\n        total = float(len(chain_df))\n\n        for ss, count in iteritems(counts):\n            if ss == '-':\n                expoinfo['percent_C-dssp'] = count/total\n            if ss == 'H':\n                expoinfo['percent_H-dssp'] = count/total\n            if ss == 'B':\n                expoinfo['percent_B-dssp'] = count/total\n            if ss == 'E':\n                expoinfo['percent_E-dssp'] = count/total\n            if ss == 'G':\n                expoinfo['percent_G-dssp'] = count/total\n            if ss == 'I':\n                expoinfo['percent_I-dssp'] = count/total\n            if ss == 'T':\n                expoinfo['percent_T-dssp'] = count/total\n            if ss == 'S':\n                expoinfo['percent_S-dssp'] = count/total\n\n        # Filling in 0 percenters\n        for per in ['percent_C-dssp','percent_H-dssp','percent_B-dssp','percent_E-dssp',\n                    'percent_G-dssp','percent_I-dssp','percent_T-dssp','percent_S-dssp']:\n            if per not in expoinfo:\n                expoinfo[per] = 0.0\n\n        infodict[chain] = dict(expoinfo)\n\n    return infodict", "response": "Summarize the secondary structure content of the DSSP dataframe for each chain."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the percent of residues that are in the surface or buried basis. Returns a dictionary of this.", "response": "def calc_surface_buried(dssp_df):\n    '''Calculates the percent of residues that are in the surface or buried,\n    as well as if they are polar or nonpolar. Returns a dictionary of this.\n    '''\n    SN = 0\n    BN = 0\n    SP = 0\n    SNP = 0\n    SPo = 0\n    SNe = 0\n    BNP = 0\n    BP = 0\n    BPo = 0\n    BNe = 0\n    Total = 0\n\n    sbinfo = {}\n\n    df_min = dssp_df[['aa_three', 'exposure_asa']]\n\n    if len(df_min) == 0:\n        return sbinfo\n    else:\n        for i, r in df_min.iterrows():\n            res = r.aa_three\n            area = r.exposure_asa\n            if res in AAdict:\n                if AAdict[res] == 'nonpolar' and area > 3:\n                    SNP = SNP + 1\n                    SN = SN + 1\n                elif AAdict[res] == 'polar' and area > 3:\n                    SP = SP + 1\n                    SN = SN + 1\n                elif AAdict[res] == 'positive' and area > 3:\n                    SPo = SPo + 1\n                    SN = SN + 1\n                elif AAdict[res] == 'negative' and area > 3:\n                    SNe = SNe + 1\n                    SN = SN + 1\n                elif AAdict[res] == 'positive' and area <= 3:\n                    BPo = BPo + 1\n                    BN = BN + 1\n                elif AAdict[res] == 'negative' and area <= 3:\n                    BNe = BNe + 1\n                    BN = BN + 1\n                elif AAdict[res] == 'polar' and area <= 3:\n                    BP = BP + 1\n                    BN = BN + 1\n                elif AAdict[res] == 'nonpolar' and area <= 3:\n                    BNP = BNP + 1\n                    BN = BN + 1\n        Total = float(BN + SN)\n        pSNP = float(SNP) / Total\n        pSP = float(SP) / Total\n        pSPo = float(SPo) / Total\n        pSNe = float(SNe) / Total\n        pBNP = float(BNP) / Total\n        pBP = float(BP) / Total\n        pBPo = float(BPo) / Total\n        pBNe = float(BNe) / Total\n        pBN = float(BN) / Total\n        pSN = float(SN) / Total\n        sbinfo['ssb_per_S_NP'] = pSNP\n        sbinfo['ssb_per_S_P'] = pSP\n        sbinfo['ssb_per_S_pos'] = pSPo\n        sbinfo['ssb_per_S_neg'] = pSNe\n        sbinfo['ssb_per_B_NP'] = pBNP\n        sbinfo['ssb_per_B_P'] = pBP\n        sbinfo['ssb_per_B_pos'] = pBPo\n        sbinfo['ssb_per_B_neg'] = pBNe\n        sbinfo['ssb_per_S'] = pSN\n        sbinfo['ssb_per_B'] = pBN\n\n        return sbinfo"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the SASA of the current structure in a single BIF file.", "response": "def calc_sasa(dssp_df):\n    \"\"\"\n    Calculation of SASA utilizing the DSSP program.\n\n    DSSP must be installed for biopython to properly call it.\n    Install using apt-get on Ubuntu\n    or from: http://swift.cmbi.ru.nl/gv/dssp/\n\n    Input: PDB or CIF structure file\n    Output: SASA (integer) of structure\n    \"\"\"\n\n    infodict = {'ssb_sasa': dssp_df.exposure_asa.sum(),\n                'ssb_mean_rel_exposed': dssp_df.exposure_rsa.mean(),\n                'ssb_size': len(dssp_df)}\n\n    return infodict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a large dictionary of SASA secondary structure composition surface / buried composition. Values are computed using DSSP.", "response": "def all_dssp_props(filename, file_type):\n    '''Returns a large dictionary of SASA, secondary structure\n    composition, and surface/buried composition. Values are computed using DSSP.\n    Input: PDB or MMCIF filename\n    Output: Dictionary of values obtained from dssp\n    '''\n    t = dssp_dataframe(filename, file_type)\n    # print(t)\n    sasa = calc_sasa(t)\n    sstr = secondary_structure_summary(t)\n    subu = calc_surface_buried(t)\n\n    sasa.update(sstr)\n    sasa.update(subu)\n\n    return sasa"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefining the secondary structure class of a PDB file at the specific chain", "response": "def get_ss_class(pdb_file, dssp_file, chain):\n    \"\"\"Define the secondary structure class of a PDB file at the specific chain\n\n    Args:\n        pdb_file:\n        dssp_file:\n        chain:\n\n    Returns:\n\n    \"\"\"\n    prag = pr.parsePDB(pdb_file)\n    pr.parseDSSP(dssp_file, prag)\n    alpha, threeTen, beta = get_dssp_ss_content_multiplechains(prag, chain)\n\n    if alpha == 0 and beta > 0:\n        classification = 'all-beta'\n    elif beta == 0 and alpha > 0:\n        classification = 'all-alpha'\n    elif beta == 0 and alpha == 0:\n        classification = 'mixed'\n    elif float(alpha) / beta >= 20:\n        classification = 'all-alpha'\n    else:\n        classification = 'mixed'\n\n    return classification"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads relevant attributes and dbxrefs from a SeqRecord in a UniProt XML file.", "response": "def parse_uniprot_xml_metadata(sr):\n    \"\"\"Load relevant attributes and dbxrefs from a parsed UniProt XML file in a SeqRecord.\n\n    Returns:\n        dict: All parsed information \n\n    \"\"\"\n    # TODO: What about \"reviewed\" status? and EC number\n    xref_dbs_to_keep = ['GO', 'KEGG', 'PDB', 'PROSITE', 'Pfam', 'RefSeq']\n\n    infodict = {}\n\n    infodict['alt_uniprots'] = list(set(sr.annotations['accessions']).difference([sr.id]))\n    infodict['gene_name'] = None\n    if 'gene_name_primary' in sr.annotations:\n        infodict['gene_name'] = sr.annotations['gene_name_primary']\n    infodict['description'] = sr.description\n    infodict['taxonomy'] = None\n    if 'organism' in sr.annotations:\n        infodict['taxonomy'] = sr.annotations['organism']\n    infodict['seq_version'] = sr.annotations['sequence_version']\n    infodict['seq_date'] = sr.annotations['sequence_modified']\n    infodict['entry_version'] = sr.annotations['version']\n    infodict['entry_date'] = sr.annotations['modified']\n\n    tmp = defaultdict(list)\n    for xref in sr.dbxrefs:\n        database = xref.split(':', 1)[0]\n        xrefs = xref.split(':', 1)[-1]\n\n        if database in xref_dbs_to_keep:\n            if database == 'PDB':\n                tmp['pdbs'].append(xrefs)\n            else:\n                tmp[database.lower()].append(xrefs)\n\n    infodict.update(tmp)\n\n    return infodict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_valid_uniprot_id(instring):\n    valid_id = re.compile(\"[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9]([A-Z][A-Z0-9]{2}[0-9]){1,2}\")\n    if valid_id.match(str(instring)):\n        return True\n    else:\n        return False", "response": "Check if a string is a valid UniProt ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uniprot_reviewed_checker(uniprot_id):\n\n    query_string = 'id:' + uniprot_id\n\n    uni_rev_raw = StringIO(bsup.search(query_string, columns='id,reviewed', frmt='tab'))\n    uni_rev_df = pd.read_table(uni_rev_raw, sep='\\t', index_col=0)\n    uni_rev_df = uni_rev_df.fillna(False)\n    uni_rev_df = uni_rev_df[pd.notnull(uni_rev_df.Status)]\n\n    uni_rev_df = uni_rev_df.replace(to_replace=\"reviewed\", value=True)\n    uni_rev_df = uni_rev_df.replace(to_replace=\"unreviewed\", value=False)\n    uni_rev_dict_adder = uni_rev_df.to_dict()['Status']\n\n    return uni_rev_dict_adder[uniprot_id]", "response": "Check if a single UniProt ID is reviewed or not."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbatching check if uniprot IDs are reviewed or not", "response": "def uniprot_reviewed_checker_batch(uniprot_ids):\n    \"\"\"Batch check if uniprot IDs are reviewed or not\n\n    Args:\n        uniprot_ids: UniProt ID or list of UniProt IDs\n\n    Returns:\n        A dictionary of {UniProtID: Boolean}\n\n    \"\"\"\n    uniprot_ids = ssbio.utils.force_list(uniprot_ids)\n\n    invalid_ids = [i for i in uniprot_ids if not is_valid_uniprot_id(i)]\n    uniprot_ids = [i for i in uniprot_ids if is_valid_uniprot_id(i)]\n\n    if invalid_ids:\n        warnings.warn(\"Invalid UniProt IDs {} will be ignored\".format(invalid_ids))\n\n    # splitting query up into managable sizes (200 IDs each)\n    Nmax = 200\n    N, rest = divmod(len(uniprot_ids), Nmax)\n\n    uni_rev_dict = {}\n\n    if rest > 0:\n        N += 1\n    for i in range(0, N):\n        i1 = i * Nmax\n        i2 = (i + 1) * Nmax\n        if i2 > len(uniprot_ids):\n            i2 = len(uniprot_ids)\n\n        query = uniprot_ids[i1:i2]\n\n        query_string = ''\n        for x in query:\n            query_string += 'id:' + x + '+OR+'\n        query_string = query_string.strip('+OR+')\n\n        uni_rev_raw = StringIO(bsup.search(query_string, columns='id,reviewed', frmt='tab'))\n        uni_rev_df = pd.read_table(uni_rev_raw, sep='\\t', index_col=0)\n        uni_rev_df = uni_rev_df.fillna(False)\n\n        # no_metadata = uni_rev_df[pd.isnull(uni_rev_df.Status)].index.tolist()\n        # if no_metadata:\n        #     warnings.warn(\"Unable to retrieve metadata for {}.\".format(no_metadata))\n        uni_rev_df = uni_rev_df[pd.notnull(uni_rev_df.Status)]\n\n        uni_rev_df = uni_rev_df.replace(to_replace=\"reviewed\", value=True)\n        uni_rev_df = uni_rev_df.replace(to_replace=\"unreviewed\", value=False)\n        uni_rev_dict_adder = uni_rev_df.to_dict()['Status']\n        uni_rev_dict.update(uni_rev_dict_adder)\n\n    return uni_rev_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the EC number annotation for a UniProt ID.", "response": "def uniprot_ec(uniprot_id):\n    \"\"\"Retrieve the EC number annotation for a UniProt ID.\n\n    Args:\n        uniprot_id: Valid UniProt ID\n\n    Returns:\n\n    \"\"\"\n    r = requests.post('http://www.uniprot.org/uniprot/?query=%s&columns=ec&format=tab' % uniprot_id)\n    ec = r.content.decode('utf-8').splitlines()[1]\n\n    if len(ec) == 0:\n        ec = None\n\n    return ec"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a list of UniProt sites parsed from the feature file", "response": "def uniprot_sites(uniprot_id):\n    \"\"\"Retrieve a list of UniProt sites parsed from the feature file\n\n    Sites are defined here: http://www.uniprot.org/help/site and here: http://www.uniprot.org/help/function_section\n\n    Args:\n        uniprot_id: Valid UniProt ID\n\n    Returns:\n\n    \"\"\"\n\n    r = requests.post('http://www.uniprot.org/uniprot/%s.gff' % uniprot_id)\n    gff = StringIO(r.content.decode('utf-8'))\n\n    feats = list(GFF.parse(gff))\n    if len(feats) > 1:\n        log.warning('Too many sequences in GFF')\n    else:\n        return feats[0].features"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload a UniProt file for a UniProt ID", "response": "def download_uniprot_file(uniprot_id, filetype, outdir='', force_rerun=False):\n    \"\"\"Download a UniProt file for a UniProt ID/ACC\n\n    Args:\n        uniprot_id: Valid UniProt ID\n        filetype: txt, fasta, xml, rdf, or gff\n        outdir: Directory to download the file\n\n    Returns:\n        str: Absolute path to file\n\n    \"\"\"\n\n    my_file = '{}.{}'.format(uniprot_id, filetype)\n    url = 'http://www.uniprot.org/uniprot/{}'.format(my_file)\n    outfile = op.join(outdir, my_file)\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        urlretrieve(url, outfile)\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_uniprot_txt_file(infile):\n    uniprot_metadata_dict = {}\n\n    metadata = old_parse_uniprot_txt_file(infile)\n    metadata_keys = list(metadata.keys())\n\n    if metadata_keys:\n        metadata_key = metadata_keys[0]\n    else:\n        return uniprot_metadata_dict\n\n    uniprot_metadata_dict['seq_len'] = len(str(metadata[metadata_key]['sequence']))\n    uniprot_metadata_dict['reviewed'] = metadata[metadata_key]['is_reviewed']\n    uniprot_metadata_dict['seq_version'] = metadata[metadata_key]['sequence_version']\n    uniprot_metadata_dict['entry_version'] = metadata[metadata_key]['entry_version']\n    if 'gene' in metadata[metadata_key]:\n        uniprot_metadata_dict['gene_name'] = metadata[metadata_key]['gene']\n    if 'description' in metadata[metadata_key]:\n        uniprot_metadata_dict['description'] = metadata[metadata_key]['description']\n    if 'refseq' in metadata[metadata_key]:\n        uniprot_metadata_dict['refseq'] = metadata[metadata_key]['refseq']\n    if 'kegg' in metadata[metadata_key]:\n        uniprot_metadata_dict['kegg'] = metadata[metadata_key]['kegg']\n    if 'ec' in metadata[metadata_key]:\n        uniprot_metadata_dict['ec_number'] = metadata[metadata_key]['ec']\n    if 'pfam' in metadata[metadata_key]:\n        uniprot_metadata_dict['pfam'] = metadata[metadata_key]['pfam']\n    if 'pdbs' in metadata[metadata_key]:\n        uniprot_metadata_dict['pdbs'] = list(set(metadata[metadata_key]['pdbs']))\n    return uniprot_metadata_dict", "response": "Parse a raw UniProt metadata file and return a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef old_parse_uniprot_txt_file(infile):\n    with open(infile, 'r') as txt:\n        cache_txt = txt.read()\n        tag = None\n        uniprot_id = None\n        metadata_by_seqid = {}\n        for l in cache_txt.splitlines():\n            test_tag = l[:5].strip()\n            if test_tag and test_tag != tag:\n                tag = test_tag\n            line = l[5:].strip()\n            words = line.split()\n            if tag == \"ID\":\n                uniprot_id = words[0]\n                is_reviewed = words[1].startswith('Reviewed')\n                length = int(words[2])\n                metadata_by_seqid[uniprot_id] = {\n                    'id': uniprot_id,\n                    'is_reviewed': is_reviewed,\n                    'length': length,\n                    'sequence': '',\n                    'accs': [],\n                }\n                entry = metadata_by_seqid[uniprot_id]\n            if tag == \"DT\":\n                # DT   01-OCT-1996, integrated into UniProtKB/Swiss-Prot.\n                # DT   17-OCT-2006, sequence version 3.\n                # DT   22-JUL-2015, entry version 166.\n                comma_split = line.split(',')\n                if 'sequence version' in comma_split[1]:\n                    # print 'sequence_version', comma_split[0],\n                    # dateparse(comma_split[0]).date()\n                    entry['sequence_version'] = str(\n                        dateparse(comma_split[0]).date())\n                elif 'entry version' in comma_split[1]:\n                    # print 'entry_version', comma_split[0],\n                    # dateparse(comma_split[0]).date()\n                    entry['entry_version'] = str(\n                        dateparse(comma_split[0]).date())\n            if tag == \"SQ\":\n                if words[0] != \"SEQUENCE\":\n                    entry['sequence'] += ''.join(words)\n            if tag == \"AC\":\n                accs = [w.replace(\";\", \"\") for w in words]\n                entry['accs'].extend(accs)\n            if tag == \"DR\":\n                if len(words) > 0:\n                    if 'PDB' in words[0]:\n                        if 'pdb' not in entry:\n                            entry['pdb'] = words[1][:-1]\n                        if 'pdbs' not in entry:\n                            entry['pdbs'] = []\n                        entry['pdbs'].append(words[1][:-1])\n                    if 'RefSeq' in words[0]:\n                        if 'refseq' not in entry:\n                            entry['refseq'] = []\n                        ids = [w[:-1] for w in words[1:]]\n                        entry['refseq'].extend(ids)\n                    if 'KEGG' in words[0]:\n                        if 'kegg' not in entry:\n                            entry['kegg'] = []\n                        ids = [w[:-1] for w in words[1:]]\n                        ids = filter(lambda w: len(w) > 1, ids)\n                        entry['kegg'].extend(ids)\n                    if 'GO' in words[0]:\n                        if 'go' not in entry:\n                            entry['go'] = []\n                        entry['go'].append(' '.join(words[1:]))\n                    if 'Pfam' in words[0]:\n                        if 'pfam' not in entry:\n                            entry['pfam'] = []\n                        entry['pfam'].append(words[1][:-1])\n            if tag == \"GN\":\n                if 'gene' not in entry and len(words) > 0:\n                    pieces = words[0].split(\"=\")\n                    if len(pieces) > 1 and 'name' in pieces[0].lower():\n                        entry['gene'] = pieces[1].replace(\n                            ';', '').replace(',', '')\n            if tag == \"OS\":\n                # OS   Homo sapiens (Human).\n                if 'organism' not in entry:\n                    entry['organism'] = \"\"\n                entry['organism'] += line\n            if tag == \"DE\":\n                if 'descriptions' not in entry:\n                    entry['descriptions'] = []\n                entry['descriptions'].append(line)\n            if tag == \"CC\":\n                if 'comment' not in entry:\n                    entry['comment'] = ''\n                entry['comment'] += line + '\\n'\n\n        for entry in metadata_by_seqid.values():\n            descriptions = entry['descriptions']\n            for i in reversed(range(len(descriptions))):\n                description = descriptions[i]\n                if 'Short' in description or 'Full' in description or 'EC=' in description:\n                    if 'Short' in description or 'Full' in description:\n                        j = description.find('=')\n                        descriptions[i] = description[j + 1:].replace(';', '')\n                        if 'description' not in entry:\n                            entry['description'] = []\n                        entry['description'].append(descriptions[i])\n                    if 'EC=' in description:\n                        j = description.find('=')\n                        descriptions[i] = description[j + 1:].replace(';', '')\n                        if '{' in descriptions[i]:\n                            descriptions[i] = descriptions[i].split(' {')[0]\n                        if 'ec' not in entry:\n                            entry['ec'] = []\n                        entry['ec'].append(descriptions[i])\n                else:\n                    del descriptions[i]\n\n    return metadata_by_seqid", "response": "Parses the text of metadata retrieved from UniProt. org and returns a dictionary with the main UNIPROT ACC as keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the Seq object from the sequence file metadata file or in memory", "response": "def seq(self):\n        \"\"\"Seq: Get the Seq object from the sequence file, metadata file, or in memory\"\"\"\n\n        if self.sequence_file:\n            log.debug('{}: reading sequence from sequence file {}'.format(self.id, self.sequence_path))\n            tmp_sr = SeqIO.read(self.sequence_path, 'fasta')\n            return tmp_sr.seq\n\n        elif self.metadata_file:\n            log.debug('{}: reading sequence from metadata file {}'.format(self.id, self.metadata_path))\n            tmp_sr = SeqIO.read(self.metadata_path, 'uniprot-xml')\n            return tmp_sr.seq\n\n        else:\n            if not self._seq:\n                log.debug('{}: no sequence stored in memory'.format(self.id))\n            else:\n                log.debug('{}: reading sequence from memory'.format(self.id))\n\n            return self._seq"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the features from the feature file metadata file or in memory", "response": "def features(self):\n        \"\"\"list: Get the features from the feature file, metadata file, or in memory\"\"\"\n        if self.feature_file:\n            log.debug('{}: reading features from feature file {}'.format(self.id, self.feature_path))\n            with open(self.feature_path) as handle:\n                feats = list(GFF.parse(handle))\n                if len(feats) > 1:\n                    log.warning('Too many sequences in GFF')\n                else:\n                    return feats[0].features\n\n        elif self.metadata_file:\n            log.debug('{}: reading features from metadata file {}'.format(self.id, self.metadata_path))\n            tmp_sr = SeqIO.read(self.metadata_path, 'uniprot-xml')\n            return tmp_sr.features\n\n        else:\n            return self._features"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef metadata_path(self, m_path):\n        if not m_path:\n            self.metadata_dir = None\n            self.metadata_file = None\n\n        else:\n            if not op.exists(m_path):\n                raise OSError('{}: file does not exist!'.format(m_path))\n\n            if not op.dirname(m_path):\n                self.metadata_dir = '.'\n            else:\n                self.metadata_dir = op.dirname(m_path)\n            self.metadata_file = op.basename(m_path)\n\n            # Parse the metadata file using Biopython's built in SeqRecord parser\n            # Just updating IDs and stuff\n            tmp_sr = SeqIO.read(self.metadata_path, 'uniprot-xml')\n            parsed = parse_uniprot_xml_metadata(tmp_sr)\n            self.update(parsed, overwrite=True)", "response": "This function takes a path to the metadata file and updates the instance attributes with the contents of the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef metadata_path_unset(self):\n        if not self.metadata_file:\n            raise IOError('No metadata file to unset')\n\n        log.debug('{}: reading from metadata file {}'.format(self.id, self.metadata_path))\n        tmp_sr = SeqIO.read(self.metadata_path, 'uniprot-xml')\n        tmp_feats = tmp_sr.features\n\n        # TODO: should this be in separate unset functions?\n        self.metadata_dir = None\n        self.metadata_file = None\n        self.features = tmp_feats\n\n        if self.sequence_file:\n            tmp_sr = tmp_sr.seq\n            self.sequence_dir = None\n            self.sequence_file = None\n            self.seq = tmp_sr", "response": "Copy features to memory and remove the association of the metadata file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading and load the UniProt FASTA file", "response": "def download_seq_file(self, outdir, force_rerun=False):\n        \"\"\"Download and load the UniProt FASTA file\"\"\"\n\n        uniprot_fasta_file = download_uniprot_file(uniprot_id=self.id,\n                                                   filetype='fasta',\n                                                   outdir=outdir,\n                                                   force_rerun=force_rerun)\n\n        self.sequence_path = uniprot_fasta_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download_metadata_file(self, outdir, force_rerun=False):\n\n        uniprot_xml_file = download_uniprot_file(uniprot_id=self.id,\n                                                 outdir=outdir,\n                                                 filetype='xml',\n                                                 force_rerun=force_rerun)\n        self.metadata_path = uniprot_xml_file", "response": "Download and load the UniProt XML file for this UniProt session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_freesasa(infile, outfile, include_hetatms=True, outdir=None, force_rerun=False):\n    if not outdir:\n        outdir = ''\n\n    outfile = op.join(outdir, outfile)\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        if op.exists(outfile):\n            os.remove(outfile)\n        if include_hetatms:\n            shell_command = 'freesasa --format=rsa --hetatm {} -o {}'.format(infile, outfile)\n        else:\n            shell_command = 'freesasa --format=rsa {} -o {}'.format(infile, outfile)\n        command = subprocess.Popen(shell_command,\n                                   stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE,\n                                   shell=True)\n        out, err = command.communicate()\n\n    return outfile", "response": "Run freesasa on a PDB file and output using the NACCESS RSA format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_rsa_data(rsa_outfile, ignore_hets=True):\n\n    naccess_rel_dict = OrderedDict()\n\n    with open(rsa_outfile, 'r') as f:\n        for line in f:\n            if line.startswith('RES'):\n                res_name = line[4:7]\n                chain_id = line[8]\n                resseq = int(line[9:13])\n                icode = line[13]\n                res_id = (' ', resseq, icode)\n                all_atoms_abs = line[16:22].strip()\n                all_atoms_rel = line[23:28].strip()\n                side_chain_abs = line[29:35].strip()\n                side_chain_rel = line[36:41].strip()\n                main_chain_abs = line[42:48].strip()\n                main_chain_rel = line[49:54].strip()\n                non_polar_abs = line[55:61].strip()\n                non_polar_rel = line[62:67].strip()\n                all_polar_abs = line[68:74].strip()\n                all_polar_rel = line[75:80].strip()\n\n                if all_atoms_rel =='N/A' and main_chain_rel =='N/A' and all_polar_rel =='N/A' and non_polar_rel =='N/A' and side_chain_rel =='N/A' and ignore_hets:\n                    continue\n\n                naccess_rel_dict[(chain_id, res_id)] = {\n                    'res_name'      : res_name,\n                    'all_atoms_abs' : ssbio.utils.conv_to_float(all_atoms_abs, inf_str='N/A'),\n                    'all_atoms_rel' : ssbio.utils.conv_to_float(all_atoms_rel, inf_str='N/A'),\n                    'side_chain_abs': ssbio.utils.conv_to_float(side_chain_abs, inf_str='N/A'),\n                    'side_chain_rel': ssbio.utils.conv_to_float(side_chain_rel, inf_str='N/A'),\n                    'main_chain_abs': ssbio.utils.conv_to_float(main_chain_abs, inf_str='N/A'),\n                    'main_chain_rel': ssbio.utils.conv_to_float(main_chain_rel, inf_str='N/A'),\n                    'non_polar_abs' : ssbio.utils.conv_to_float(non_polar_abs, inf_str='N/A'),\n                    'non_polar_rel' : ssbio.utils.conv_to_float(non_polar_rel, inf_str='N/A'),\n                    'all_polar_abs' : ssbio.utils.conv_to_float(all_polar_abs, inf_str='N/A'),\n                    'all_polar_rel' : ssbio.utils.conv_to_float(all_polar_rel, inf_str='N/A')}\n\n    return naccess_rel_dict", "response": "Parse a NACCESS or Freesasa RSA output file and return a dictionary of per - residue values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the center of mass of an Entity.", "response": "def center_of_mass(entity, geometric=False):\n    \"\"\"\n    Returns gravitic [default] or geometric center of mass of an Entity.\n    Geometric assumes all masses are equal (geometric=True)\n    \"\"\"\n    \n    # Structure, Model, Chain, Residue\n    if isinstance(entity, Entity.Entity):\n        atom_list = entity.get_atoms()\n    # List of Residues, added 2018-03-17 by Nathan\n    elif hasattr(entity, '__iter__') and [x for x in entity if x.level == 'R']:\n        atom_list = []\n        for res in entity:\n            atom_list.extend(list(res.get_atoms()))\n    # List of Atoms\n    elif hasattr(entity, '__iter__') and [x for x in entity if x.level == 'A']:\n        atom_list = entity\n    else: # Some other weirdo object\n        raise ValueError(\"Center of Mass can only be calculated from the following objects:\\n\"\n                            \"Structure, Model, Chain, Residue, list of Atoms.\")\n    \n    masses = []\n    positions = [ [], [], [] ] # [ [X1, X2, ..] , [Y1, Y2, ...] , [Z1, Z2, ...] ]\n    \n    for atom in atom_list:\n        masses.append(atom.mass)\n        \n        for i, coord in enumerate(np.array(atom.coord).tolist()):\n            positions[i].append(coord)\n\n    # If there is a single atom with undefined mass complain loudly.\n    if 'ukn' in set(masses) and not geometric:\n        raise ValueError(\"Some Atoms don't have an element assigned.\\n\"\n                         \"Try adding them manually or calculate the geometrical center of mass instead.\")\n    \n    if geometric:\n        return [sum(coord_list)/len(masses) for coord_list in positions]\n    else:       \n        w_pos = [ [], [], [] ]\n        for atom_index, atom_mass in enumerate(masses):\n            w_pos[0].append(positions[0][atom_index]*atom_mass)\n            w_pos[1].append(positions[1][atom_index]*atom_mass)\n            w_pos[2].append(positions[2][atom_index]*atom_mass)\n\n        return [sum(coord_list)/sum(masses) for coord_list in w_pos]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves all attributes that start with df into a specified directory.", "response": "def save_dataframes(self, outdir, prefix='df_'):\n        \"\"\"Save all attributes that start with \"df\" into a specified directory.\n\n        Args:\n            outdir (str): Path to output directory\n            prefix (str): Prefix that dataframe attributes start with\n\n        \"\"\"\n        # Get list of attributes that start with \"df_\"\n        dfs = list(filter(lambda x: x.startswith(prefix), dir(self)))\n\n        counter = 0\n        for df in dfs:\n            outpath = ssbio.utils.outfile_maker(inname=df, outext='.csv', outdir=outdir)\n            my_df = getattr(self, df)\n            if not isinstance(my_df, pd.DataFrame):\n                raise TypeError('{}: object is not a Pandas DataFrame'.format(df))\n\n            if my_df.empty:\n                log.debug('{}: empty dataframe, not saving'.format(df))\n            else:\n                my_df.to_csv(outpath)\n                log.debug('{}: saved dataframe'.format(outpath))\n                counter += 1\n\n        log.debug('Saved {} dataframes at {}'.format(counter, outdir))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_bonding_network(self):\n        \n        self.bonds = {} # [Residue][Atom]: [ [missing],  [bonded] ]\n        self.selection = {} # [Residue]: 'nrml'/'nter'/'cter'\n        \n        missing = 0\n        \n        for residue in self.nh_structure.get_residues():\n            \n            bond_dict = self.bonds[residue] = {}\n            \n            atom_dict = residue.child_dict\n            atom_names = set(atom_dict.keys())\n            \n            # Pre-Populate Dictionary\n            \n            for name in atom_names:\n                bond_dict[name] = [ [],  [] ]\n            \n            # Define Template\n            if atom_names.intersection(self.C_TERMINAL_ATOMS):\n                selection = 'cter'\n            elif atom_names.intersection(self.N_TERMINAL_ATOMS):\n                selection = 'nter'\n            else:\n                selection = 'nrml'\n            \n            tmpl = self.tmpl[selection]\n            self.selection[residue] = selection # For place_hs\n            \n            # Iterate Template Bonds and record info\n            \n            if not tmpl.has_key(residue.resname):\n                raise ValueError(\"Unknown Residue Type: %s\" %residue.resname)\n            \n            template_bonds = tmpl[residue.resname]['bonds']\n            \n            for bond in template_bonds.keys():\n                \n                a1,  a2 = bond\n                \n                if a1 in atom_names and not a2 in atom_names:\n                    bond_dict[a1][0].append(a2)\n                    missing += 1\n                elif a1 not in atom_names and a2 in atom_names:\n                    bond_dict[a2][0].append(a1)\n                    missing += 1\n                else: # \n                    bond_dict[a1][1].append(atom_dict[a2])\n                    bond_dict[a2][1].append(atom_dict[a1])   \n        \n        return missing", "response": "Builds the bonding network for the current structure and the atom list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexclude the ss bonds that are present in the NH structure.", "response": "def _exclude_ss_bonded_cysteines(self):\n        \"\"\"\n        Pre-compute ss bonds to discard cystines for H-adding.\n        \"\"\"\n        \n        ss_bonds =  self.nh_structure.search_ss_bonds()\n        for cys_pair in ss_bonds:\n            cys1,  cys2 = cys_pair\n            \n            cys1.resname = 'CYX'\n            cys2.resname = 'CYX'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_secondary_anchors(self,  residue,  heavy_atom,  anchor):\n        \n        for secondary in self.bonds[residue][anchor.name][1]:\n            for tertiary in self.bonds[residue][secondary.name][1]:\n                if (tertiary.name != heavy_atom.name \n                    and tertiary.name != anchor.name):\n                    \n                    return (secondary,  tertiary)\n        \n        return None", "response": "Searches through the bond network for atoms bound to the anchor and returns a secondary and tertiary anchors. Returns None if no anchors are found."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_1(self,  hydrogen_name,  heavy_atom,  bonds):\n        \n        residue = heavy_atom.parent\n        ffld = self.ffld[self.selection[residue]]\n        bnd_len = self.bondfield.length\n        anchor = bonds[0]\n                \n        # If not linear\n        if self.bondfield.nonlinear.has_key(ffld[(  residue.resname,  \n                                                    heavy_atom.name)]['type']):\n            # returns tuple of two atoms\n            bonded = self._find_secondary_anchors(residue,  heavy_atom,  anchor)\n\n            if bonded:\n                # Phenolic hydrogens,  etc.\n                if self.bondfield.planer.has_key(ffld[( residue.resname,  \n                                                        anchor.name)]['type']):\n                    secondary_anchor = bonded[0]\n                   \n                    p0 = heavy_atom.coord - anchor.coord\n                    d2 = secondary_anchor.coord - anchor.coord\n                    p1 = normalize(cross_product(d2,  p0))\n                    p2 = normalize(cross_product(p0,  p1))                     \n                    vector = scale(p2,  TRI_TAN)\n                    vector = normalize(add(p0,  vector))\n                    \n                    hydrogen_coord = add(heavy_atom.coord, \n                                        scale(vector,  \n                                              bnd_len[(ffld[(residue.resname, \n                                                             heavy_atom.name)]['type'], \n                                              ffld[(residue.resname,  \n                                                    hydrogen_name)]['type'])]))\n                \n                else: # Ser,  Cys,  Thr hydroxyl hydrogens\n                    secondary_anchor = bonded[0]\n                    vector = anchor.coord - secondary_anchor.coord\n                    hydrogen_coord = add(heavy_atom.coord, \n                                        scale(vector,  \n                                              bnd_len[(ffld[(residue.resname,  heavy_atom.name)]['type'],  \n                                              ffld[(residue.resname,  hydrogen_name)]['type'])]    ))\n                    \n            elif len(bonds):\n                d2 = [1.0, 0, 0]\n                p0 = heavy_atom.coord - anchor.coord\n                p1 = normalize(cross_product(d2,  p0))\n                vector = scale(p1,  TET_TAN)\n                vector = normalize(add(p0,  vector))\n                \n                hydrogen_coord = add(heavy_atom.coord, \n                                     scale(  vector,  \n                                             bnd_len[(ffld[(residue.resname,  heavy_atom.name)]['type'],  \n                                             ffld[(residue.resname,  hydrogen_name)]['type'])]    ))\n            else:\n                hydrogen_coord = random_sphere(heavy_atom.coord, \n                                               bnd_len[ (ffld[(residue.resname,  heavy_atom.name)]['type'], \n                                                         ffld[(residue.resname,  hydrogen_name)]['type']) ])\n\n        elif len(bonds): # linear sum...amide,  tbu,  etc\n            vector = [0.0, 0.0, 0.0]\n            if heavy_atom.name == 'N': # Fix to get previous atom O from peptide bond. Ugly.\n                prev_res = list(residue.get_id())\n                prev_res[1] -= 1\n                prev_res = tuple(prev_res)\n                if residue.parent.child_dict.has_key(prev_res):\n                    prev_res = residue.parent.child_dict[prev_res]\n                    bonds.append(prev_res.child_dict['O'])\n            for b in bonds:\n                d = heavy_atom.coord - b.coord\n                vector = add(vector,  d)\n            vector = normalize(vector)\n            hydrogen_coord = add(heavy_atom.coord, \n                                 scale(vector, \n                                 bnd_len[(ffld[(residue.resname,  heavy_atom.name)]['type'], \n                                          ffld[(residue.resname,  hydrogen_name)]['type']) ]))\n        else:\n            hydrogen_coord = random_sphere(heavy_atom.coord, \n                                           bnd_len[ (ffld[(residue.resname,  heavy_atom.name)]['type'], \n                                                     ffld[(residue.resname,  hydrogen_name)]['type']) ])\n        \n        return hydrogen_coord", "response": "Adds the missing proton to a single protonated heavy atom."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a DOCK6 mol2 output file and return a Pandas DataFrame of the results.", "response": "def parse_results_mol2(mol2_outpath):\n    \"\"\"Parse a DOCK6 mol2 output file, return a Pandas DataFrame of the results.\n\n    Args:\n        mol2_outpath (str): Path to mol2 output file\n\n    Returns:\n        DataFrame: Pandas DataFrame of the results\n\n    \"\"\"\n    docked_ligands = pd.DataFrame()\n\n    lines = [line.strip() for line in open(mol2_outpath, 'r')]\n    props = {}\n\n    for i, line in enumerate(lines):\n        if line.startswith('########## Name:'):\n            ligand = line.strip().strip('##########').replace(' ', '').replace('\\t', '').split(':')[1]\n            line = lines[i + 1]\n            props = {}\n            props['Ligand'] = ligand\n        if line.startswith('##########'):\n            splitter = line.strip().strip('##########').replace(' ', '').replace('\\t', '').split(':')\n            props[splitter[0]] = float(splitter[1])\n        if line.startswith('@<TRIPOS>MOLECULE'):\n            if props:\n                docked_ligands = docked_ligands.append(props, ignore_index=True)\n\n    return docked_ligands"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dock_dir(self):\n        if self.root_dir:\n            return op.join(self.root_dir, self.id + '_DOCK')\n        else:\n            log.warning('Root directory not set')\n            return None", "response": "str - > DOCK folder"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprovides pointers to the paths of the structure file that are located at the given path.", "response": "def structure_path(self, path):\n        \"\"\"Provide pointers to the paths of the structure file\n\n        Args:\n            path: Path to structure file\n\n        \"\"\"\n        if not path:\n            self.structure_dir = None\n            self.structure_file = None\n\n        else:\n            if not op.exists(path):\n                raise OSError('{}: file does not exist!'.format(path))\n\n            if not op.dirname(path):\n                self.structure_dir = '.'\n            else:\n                self.structure_dir = op.dirname(path)\n            self.structure_file = op.basename(path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dockprep(self, force_rerun=False):\n        log.debug('{}: running dock preparation...'.format(self.id))\n\n        prep_mol2 = op.join(self.dock_dir, '{}_prep.mol2'.format(self.id))\n        prep_py = op.join(self.dock_dir, \"prep.py\")\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=prep_mol2):\n            with open(prep_py, \"w\") as f:\n                f.write('import chimera\\n')\n                f.write('from DockPrep import prep\\n')\n                f.write('models = chimera.openModels.list(modelTypes=[chimera.Molecule])\\n')\n                f.write('prep(models)\\n')\n                f.write('from WriteMol2 import writeMol2\\n')\n                f.write('writeMol2(models, \"{}\")\\n'.format(prep_mol2))\n\n            cmd = 'chimera --nogui {} {}'.format(self.structure_path, prep_py)\n            os.system(cmd)\n            os.remove(prep_py)\n            os.remove('{}c'.format(prep_py))\n\n        if ssbio.utils.is_non_zero_file(prep_mol2):\n            self.dockprep_path = prep_mol2\n            log.debug('{}: successful dockprep execution'.format(self.dockprep_path))\n        else:\n            log.critical('{}: dockprep failed to run on PDB file'.format(self.structure_path))", "response": "Prepare a single object in a new PDB file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nisolating the receptor by stripping everything except protein and no H.", "response": "def protein_only_and_noH(self, keep_ligands=None, force_rerun=False):\n        \"\"\"Isolate the receptor by stripping everything except protein and specified ligands.\n\n        Args:\n            keep_ligands (str, list): Ligand(s) to keep in PDB file\n            force_rerun (bool): If method should be rerun even if output file exists\n\n        \"\"\"\n        log.debug('{}: running protein receptor isolation...'.format(self.id))\n\n        if not self.dockprep_path:\n            return ValueError('Please run dockprep')\n\n        receptor_mol2 = op.join(self.dock_dir, '{}_receptor.mol2'.format(self.id))\n        receptor_noh = op.join(self.dock_dir, '{}_receptor_noH.pdb'.format(self.id))\n\n        prly_com = op.join(self.dock_dir, \"prly.com\")\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=receptor_noh):\n            with open(prly_com, \"w\") as f:\n                f.write('open {}\\n'.format(self.dockprep_path))\n\n                keep_str = 'delete ~protein'\n                if keep_ligands:\n                    keep_ligands = ssbio.utils.force_list(keep_ligands)\n                    for res in keep_ligands:\n                        keep_str += ' & ~:{} '.format(res)\n                keep_str = keep_str.strip() + '\\n'\n                f.write(keep_str)\n\n                f.write('write format mol2 0 {}\\n'.format(receptor_mol2))\n                f.write('delete element.H\\n')\n                f.write('write format pdb 0 {}\\n'.format(receptor_noh))\n\n            cmd = 'chimera --nogui {}'.format(prly_com)\n            os.system(cmd)\n            os.remove(prly_com)\n\n        if ssbio.utils.is_non_zero_file(receptor_mol2) and ssbio.utils.is_non_zero_file(receptor_noh):\n            self.receptormol2_path = receptor_mol2\n            self.receptorpdb_path = receptor_noh\n            log.debug('{}: successful receptor isolation (mol2)'.format(self.receptormol2_path))\n            log.debug('{}: successful receptor isolation (pdb)'.format(self.receptorpdb_path))\n        else:\n            log.critical('{}: protein_only_and_noH failed to run on dockprep file'.format(self.dockprep_path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dms_maker(self, force_rerun=False):\n        log.debug('{}: running surface representation maker...'.format(self.id))\n\n        if not self.receptorpdb_path:\n            return ValueError('Please run protein_only_and_noH')\n\n        dms = op.join(self.dock_dir, '{}_receptor.dms'.format(self.id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=dms):\n            cmd = 'dms {} -n -w 1.4 -o {}'.format(self.receptorpdb_path, dms)\n            os.system(cmd)\n\n        self.dms_path = dms\n\n        if ssbio.utils.is_non_zero_file(dms):\n            self.dms_path = dms\n            log.debug('{}: successful dms execution'.format(self.dms_path))\n        else:\n            log.critical('{}: dms_maker failed to run on receptor file'.format(self.receptorpdb_path))", "response": "Create the DMS file for the current receptor and store it in self. dms_path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sphgen(self, force_rerun=False):\n        log.debug('{}: running sphere generation...'.format(self.id))\n\n        if not self.dms_path:\n            return ValueError('Please run dms_maker')\n\n        sph = op.join(self.dock_dir, '{}_receptor.sph'.format(self.id))\n        insph = op.join(self.dock_dir, 'INSPH')\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=sph):\n            with open(insph, \"w\") as f:\n                f.write(\"{}\\n\".format(self.dms_path))\n                f.write(\"R\\n\")\n                f.write(\"X\\n\")\n                f.write(\"0.0\\n\")\n                f.write(\"4.0\\n\")\n                f.write(\"1.4\\n\")\n                f.write(\"{}\\n\".format(sph))\n\n            os.chdir(self.dock_dir)\n            cmd = \"sphgen_cpp\"\n            os.system(cmd)\n            os.remove(insph)\n\n        if ssbio.utils.is_non_zero_file(sph):\n            self.sphgen_path = sph\n            log.debug('{}: successful sphgen execution'.format(self.sphgen_path))\n        else:\n            log.critical('{}: sphgen_cpp failed to run on dms file'.format(self.dms_path))", "response": "Generate the sphere representation of the receptor from the surface representation of the receptor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef binding_site_mol2(self, residues, force_rerun=False):\n        log.debug('{}: running binding site isolation...'.format(self.id))\n\n        if not self.receptorpdb_path:\n            return ValueError('Please run protein_only_and_noH')\n\n        prefix = self.id + '_' + 'binding_residues'\n        mol2maker = op.join(self.dock_dir, '{}_make_mol2.py'.format(prefix))\n        outfile = op.join(self.dock_dir, '{}.mol2'.format(prefix))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n            with open(mol2maker, 'w') as mol2_maker:\n                mol2_maker.write('#! /usr/bin/env python\\n')\n                mol2_maker.write('from chimera import runCommand\\n')\n                mol2_maker.write('runCommand(\"open {}\")\\n'.format(self.receptorpdb_path))\n                mol2_maker.write('runCommand(\"delete ~:{}\")\\n'.format(residues))\n                mol2_maker.write('runCommand(\"write format mol2 resnum 0 {}\")\\n'.format(outfile))\n                mol2_maker.write('runCommand(\"close all\")')\n\n            cmd = 'chimera --nogui {}'.format(mol2maker)\n            os.system(cmd)\n            os.remove(mol2maker)\n            os.remove('{}c'.format(mol2maker))\n\n        if ssbio.utils.is_non_zero_file(outfile):\n            self.bindingsite_path = outfile\n            log.debug('{}: successful binding site isolation'.format(self.bindingsite_path))\n        else:\n            log.critical('{}: binding_site_mol2 failed to run on receptor file'.format(self.receptorpdb_path))", "response": "Create mol2 of only binding site residues from the receptor PDB file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nselecting spheres based on binding site residues.", "response": "def sphere_selector_using_residues(self, radius, force_rerun=False):\n        \"\"\"Select spheres based on binding site residues\n\n        Args:\n            radius (int, float): Radius around binding residues to dock to\n            force_rerun (bool): If method should be rerun even if output file exists\n\n        \"\"\"\n        log.debug('{}: running sphere selector...'.format(self.id))\n\n        if not self.sphgen_path or not self.bindingsite_path:\n            return ValueError('Please run sphgen and binding_site_mol2')\n\n        selsph = op.join(self.dock_dir, '{}_selsph_binding.sph'.format(self.id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=selsph):\n            cmd = \"sphere_selector {} {} {}\".format(self.sphgen_path, self.bindingsite_path, radius)\n            rename = \"mv selected_spheres.sph {}\".format(selsph)\n\n            os.system(cmd)\n            os.system(rename)\n\n        if ssbio.utils.is_non_zero_file(selsph):\n            self.sphsel_path = selsph\n            log.debug('{}: successful sphere selection'.format(self.sphsel_path))\n        else:\n            log.critical('{}: sphere_selector_using_residues failed to run on sph file'.format(self.sphgen_path))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate dummy PDB box around selected spheres.", "response": "def showbox(self, force_rerun=False):\n        \"\"\"Create the dummy PDB box around the selected spheres.\n\n        Args:\n            force_rerun (bool): If method should be rerun even if output file exists\n\n        \"\"\"\n        log.debug('{}: running box maker...'.format(self.id))\n\n        if not self.sphsel_path:\n            return ValueError('Please run sphere_selector_using_residues')\n\n        boxfile = op.join(self.dock_dir, \"{}_box.pdb\".format(self.id))\n        boxscript = op.join(self.dock_dir, \"{}_box.in\".format(self.id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=boxfile):\n            with open(boxscript, \"w\") as f:\n                f.write(\"Y\\n\")\n                f.write(\"0\\n\")\n                f.write(\"{}\\n\".format(op.basename(self.sphsel_path)))\n                f.write(\"1\\n\")\n                f.write(\"{}\".format(op.basename(boxfile)))\n\n            cmd = \"showbox < {}\".format(boxscript)\n            os.chdir(self.dock_dir)\n            os.system(cmd)\n\n        if ssbio.utils.is_non_zero_file(boxfile):\n            self.box_path = boxfile\n            log.debug('{}: successful box creation'.format(self.box_path))\n        else:\n            log.critical('{}: showbox failed to run on selected spheres file'.format(self.sphsel_path))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the scoring grid within the dummy box.", "response": "def grid(self, force_rerun=False):\n        \"\"\"Create the scoring grid within the dummy box.\n\n        Args:\n            force_rerun (bool): If method should be rerun even if output file exists\n\n        \"\"\"\n        log.debug('{}: running grid maker...'.format(self.id))\n\n        if not self.receptormol2_path or not self.box_path:\n            return ValueError('Please run protein_only_and_noH and showbox')\n\n        gridscript = op.join(self.dock_dir, \"{}_grid.in\".format(self.id))\n        out_name = op.join(self.dock_dir, \"{}_grid.out\".format(self.id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=out_name):\n            with open(gridscript, \"w\") as f:\n                grid_text = \"\"\"compute_grids                  yes\ngrid_spacing                   0.3\noutput_molecule                no\ncontact_score                  yes\ncontact_cutoff_distance        4.5\n\nenergy_score                   yes\nenergy_cutoff_distance         9999\n\natom_model                     all\nattractive_exponent            6\nrepulsive_exponent             12\ndistance_dielectric            yes\ndielectric_factor              4\n\nbump_filter                    yes\nbump_overlap                   0.75\n\nreceptor_file                  {}\nbox_file                       {}\nvdw_definition_file            {}\nscore_grid_prefix              {}_grid\n            \"\"\".format(op.basename(self.receptormol2_path), op.basename(self.box_path), self.amb_file, self.id)\n\n                f.write(grid_text)\n\n            os.chdir(self.dock_dir)\n            cmd = \"grid -i {} -o {}\".format(op.basename(gridscript), op.basename(out_name))\n            os.system(cmd)\n\n        if ssbio.utils.is_non_zero_file(out_name):\n            self.grid_path = out_name\n            log.debug('{}: successful grid creation'.format(self.grid_path))\n        else:\n            log.critical('{}: grid failed to run on receptor + box file'.format(self.box_path))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndocking a ligand to the protein. Args: ligand_path (str): Path to ligand (mol2 format) to dock to protein force_rerun (bool): If method should be rerun even if output file exists", "response": "def do_dock6_flexible(self, ligand_path, force_rerun=False):\n        \"\"\"Dock a ligand to the protein.\n\n        Args:\n            ligand_path (str): Path to ligand (mol2 format) to dock to protein\n            force_rerun (bool): If method should be rerun even if output file exists\n\n        \"\"\"\n        log.debug('{}: running DOCK6...'.format(self.id))\n\n        ligand_name = os.path.basename(ligand_path).split('.')[0]\n        in_name = op.join(self.dock_dir, \"{}_{}_flexdock.in\".format(self.id, ligand_name))\n        out_name = op.join(self.dock_dir, \"{}_{}_flexdock.out\".format(self.id, ligand_name))\n\n        conformers_out = op.join(self.dock_dir, '{}_{}_flexdock_conformers.mol2'.format(self.id, ligand_name))\n        scored_out = op.join(self.dock_dir, '{}_{}_flexdock_scored.mol2'.format(self.id, ligand_name))\n        ranked_out = op.join(self.dock_dir, '{}_{}_flexdock_ranked.mol2'.format(self.id, ligand_name))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=ranked_out):\n            with open(in_name, \"w\") as f:\n                dock_text = \"\"\"ligand_atom_file                                             {}\nlimit_max_ligands                                            no\nskip_molecule                                                no\nread_mol_solvation                                           no\ncalculate_rmsd                                               no\nuse_database_filter                                          no\norient_ligand                                                yes\nautomated_matching                                           yes\nreceptor_site_file                                           {}\nmax_orientations                                             500\ncritical_points                                              no\nchemical_matching                                            no\nuse_ligand_spheres                                           no\nuse_internal_energy                                          yes\ninternal_energy_rep_exp                                      12\nflexible_ligand                                              yes\nuser_specified_anchor                                        no\nlimit_max_anchors                                            no\nmin_anchor_size                                              5\npruning_use_clustering                                       yes\npruning_max_orients                                          100\npruning_clustering_cutoff                                    100\npruning_conformer_score_cutoff                               100\nuse_clash_overlap                                            no\nwrite_growth_tree                                            no\nbump_filter                                                  yes\nbump_grid_prefix                                             {}\nscore_molecules                                              yes\ncontact_score_primary                                        no\ncontact_score_secondary                                      no\ngrid_score_primary                                           yes\ngrid_score_secondary                                         no\ngrid_score_rep_rad_scale                                     1\ngrid_score_vdw_scale                                         1\ngrid_score_es_scale                                          1\ngrid_score_grid_prefix                                       {}\nmultigrid_score_secondary                                    no\ndock3.5_score_secondary                                      no\ncontinuous_score_secondary                                   no\ndescriptor_score_secondary                                   no\ngbsa_zou_score_secondary                                     no\ngbsa_hawkins_score_secondary                                 no\nSASA_descriptor_score_secondary                              no\namber_score_secondary                                        no\nminimize_ligand                                              yes\nminimize_anchor                                              yes\nminimize_flexible_growth                                     yes\nuse_advanced_simplex_parameters                              no\nsimplex_max_cycles                                           1\nsimplex_score_converge                                       0.1\nsimplex_cycle_converge                                       1.0\nsimplex_trans_step                                           1.0\nsimplex_rot_step                                             0.1\nsimplex_tors_step                                            10.0\nsimplex_anchor_max_iterations                                500\nsimplex_grow_max_iterations                                  500\nsimplex_grow_tors_premin_iterations                          0\nsimplex_random_seed                                          0\nsimplex_restraint_min                                        yes\nsimplex_coefficient_restraint                                10.0\natom_model                                                   all\nvdw_defn_file                                                {}\nflex_defn_file                                               {}\nflex_drive_file                                              {}\nligand_outfile_prefix                                        {}_{}_flexdock\nwrite_orientations                                           no\nnum_scored_conformers                                        20\nwrite_conformations                                          yes\ncluster_conformations                                        yes\nrank_ligands                                                 yes\n        \"\"\".format(ligand_path, op.basename(self.sphsel_path), op.splitext(op.basename(self.grid_path))[0],\n                   op.splitext(op.basename(self.grid_path))[0], self.amb_file, self.flex1_file, self.flex2_file,\n                   self.id, ligand_name)\n\n                f.write(dock_text)\n\n            os.chdir(self.dock_dir)\n            cmd = \"dock6 -i {} -o {} -v\".format(in_name, out_name)\n            os.system(cmd)\n\n        if ssbio.utils.is_non_zero_file(ranked_out):\n            self.dock_flexible_outfile = out_name\n            self.dock_flexible_conformers_result = conformers_out\n            self.dock_flexible_scored_result = scored_out\n            log.debug('{}: successful docking!'.format(self.dock_flexible_outfile))\n        else:\n            log.error('{}+{}: empty DOCK6 ranked file, execution error (or ligand failed to dock)'.format(self.id,\n                                                                                                          op.basename(ligand_path)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef auto_flexdock(self, binding_residues, radius, ligand_path=None, force_rerun=False):\n        log.debug('\\n{}: running DOCK6...\\n'\n                 '\\tBinding residues: {}\\n'\n                 '\\tBinding residues radius: {}\\n'\n                 '\\tLigand to dock: {}\\n'.format(self.id, binding_residues, radius, op.basename(ligand_path)))\n\n        self.dockprep(force_rerun=force_rerun)\n        self.protein_only_and_noH(force_rerun=force_rerun)\n        self.dms_maker(force_rerun=force_rerun)\n        self.sphgen(force_rerun=force_rerun)\n        self.binding_site_mol2(residues=binding_residues, force_rerun=force_rerun)\n        self.sphere_selector_using_residues(radius=radius, force_rerun=force_rerun)\n        self.showbox(force_rerun=force_rerun)\n        self.grid(force_rerun=force_rerun)\n\n        if ligand_path:\n            self.do_dock6_flexible(ligand_path=ligand_path, force_rerun=force_rerun)", "response": "Run DOCK6 on a PDB file given its binding residues and a radius around it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_metalpdb_info(metalpdb_lig_file):\n\n    pdb_metals = ['CU', 'ZN', 'MN', 'FE', 'MG', 'CO', 'SE', 'YB', 'SF4', 'FES', 'F3S', 'NI', 'FE2']\n\n    # Information to collect\n    coordination_number = 0\n    endogenous_ligands = []\n    exogenous_ligands = []\n\n    # Load the structure\n    ss = StructProp(ident='metalpdb', structure_path=metalpdb_lig_file, file_type='pdb')\n\n    # This lig file should just be for one chain\n    chain_id = op.basename(metalpdb_lig_file)[5]\n    metal_id = (op.basename(metalpdb_lig_file).split('_')[2], op.basename(metalpdb_lig_file).split('_')[3])\n\n    for r in ss.parse_structure().first_model.get_residues():\n        return_id = (r.get_id(), r.get_resname())\n        #         print(r.resname)\n        # Binding partners\n        ## check if residue is a normal one (not a HETATM, WAT, or the metal that is identified)\n        if r.get_id()[0] != ' ':\n            if not r.resname.strip() in pdb_metals and r.resname != 'HOH':\n                #                 print('appended', r.resname)\n                exogenous_ligands.append(return_id)\n        else:\n            endogenous_ligands.append(return_id)\n\n        # Coordination number\n        for a in r.get_atom():\n            if not a.element in pdb_metals:\n                coordination_number += 1\n\n    infodict = {metal_id: {'endogenous_ligands' : endogenous_ligands,\n                           'exogenous_ligands'  : exogenous_ligands,\n                           'coordination_number': coordination_number}}\n\n    return chain_id, infodict", "response": "Parse a MetalPDB. lig file and return a tuple of the chain ID it represents along with the parsed metal binding site information."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a global pairwise alignment between two sequences.", "response": "def pairwise_sequence_alignment(a_seq, b_seq, engine, a_seq_id=None, b_seq_id=None,\n                                gapopen=10, gapextend=0.5,\n                                outfile=None, outdir=None, force_rerun=False):\n    \"\"\"Run a global pairwise sequence alignment between two sequence strings.\n\n    Args:\n        a_seq (str, Seq, SeqRecord, SeqProp): Reference sequence\n        b_seq (str, Seq, SeqRecord, SeqProp): Sequence to be aligned to reference\n        engine (str): `biopython` or `needle` - which pairwise alignment program to use\n        a_seq_id (str): Reference sequence ID. If not set, is \"a_seq\"\n        b_seq_id (str): Sequence to be aligned ID. If not set, is \"b_seq\"\n        gapopen (int): Only for `needle` - Gap open penalty is the score taken away when a gap is created\n        gapextend (float): Only for `needle` - Gap extension penalty is added to the standard gap penalty for each \n            base or residue in the gap\n        outfile (str): Only for `needle` - name of output file. If not set, is {id_a}_{id_b}_align.txt\n        outdir (str): Only for `needle` - Path to output directory. Default is the current directory.\n        force_rerun (bool): Only for `needle` - Default False, set to True if you want to rerun the alignment \n            if outfile exists.\n\n    Returns:\n        MultipleSeqAlignment: Biopython object to represent an alignment\n\n    \"\"\"\n    engine = engine.lower()\n\n    if engine not in ['biopython', 'needle']:\n        raise ValueError('{}: invalid engine'.format(engine))\n\n    if not a_seq_id:\n        a_seq_id = 'a_seq'\n    if not b_seq_id:\n        b_seq_id = 'b_seq'\n\n    a_seq = ssbio.protein.sequence.utils.cast_to_str(a_seq)\n    b_seq = ssbio.protein.sequence.utils.cast_to_str(b_seq)\n\n    if engine == 'biopython':\n        # TODO: allow different matrices? needle uses blosum62 by default, how to change that?\n        # TODO: how to define gap open/extend when using matrix in biopython global alignment?\n        log.warning('Gap penalties not implemented in Biopython yet')\n\n        blosum62 = matlist.blosum62\n        alignments = pairwise2.align.globaldx(a_seq, b_seq, blosum62)  # TODO: add gap penalties\n        best_alignment = alignments[0]\n\n        a = ssbio.protein.sequence.utils.cast_to_seq_record(best_alignment[0], id=a_seq_id)\n        b = ssbio.protein.sequence.utils.cast_to_seq_record(best_alignment[1], id=b_seq_id)\n        alignment = MultipleSeqAlignment([a, b], annotations={\"score\": best_alignment[2],\n                                                              \"start\": best_alignment[3],\n                                                              \"end\"  : best_alignment[4]})\n        alignment.annotations['percent_identity'] = get_percent_identity(best_alignment[0], best_alignment[1]) * 100\n\n        return alignment\n\n    if engine == 'needle':\n        alignment_file = run_needle_alignment(seq_a=a_seq, seq_b=b_seq, gapopen=gapopen, gapextend=gapextend,\n                                              write_outfile=True,  # Has to be true, AlignIO parses files on disk\n                                              outdir=outdir, outfile=outfile, force_rerun=force_rerun)\n        log.debug('Needle alignment at {}'.format(alignment_file))\n\n        if not op.exists(alignment_file):\n            raise ValueError('{}: needle alignment file does not exist'.format(alignment_file))\n\n        # Use AlignIO to parse the needle alignment, alignments[0] is the first alignment (the only one in pairwise)\n        # alignments = list(AlignIO.parse(alignment_file, \"emboss\"))\n        # alignment = alignments[0]\n        alignment = needle_statistics_alignio(alignment_file)\n\n        # Rename the sequence IDs\n        alignment[0].id = a_seq_id\n        alignment[1].id = b_seq_id\n\n        # # Add needle statistics as annotations in the alignment object\n        # stats = needle_statistics(alignment_file)\n        # alignment_ids = list(stats.keys())\n        # if len(alignment_ids) > 1:\n        #     raise ValueError('Needle alignment file contains more than one pairwise alignment')\n        # needle_id = alignment_ids[0]\n        # alignment.annotations['percent_identity'] = stats[needle_id]['percent_identity']\n        # alignment.annotations['percent_similarity'] = stats[needle_id]['percent_similarity']\n        # alignment.annotations['percent_gaps'] = stats[needle_id]['percent_gaps']\n        # alignment.annotations['score'] = stats[needle_id]['score']\n\n        return alignment"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_needle_alignment(seq_a, seq_b, gapopen=10, gapextend=0.5, write_outfile=True,\n                         outdir=None, outfile=None, force_rerun=False):\n    \"\"\"Run the needle alignment program for two strings and return the raw alignment result.\n\n    More info:\n    EMBOSS needle: http://www.bioinformatics.nl/cgi-bin/emboss/help/needle\n    Biopython wrapper: http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc84\n    Using strings as input: https://www.biostars.org/p/91124/\n\n    Args:\n        id_a: ID of reference sequence\n        seq_a (str, Seq, SeqRecord): Reference sequence\n        id_b: ID of sequence to be aligned\n        seq_b (str, Seq, SeqRecord): String representation of sequence to be aligned\n        gapopen: Gap open penalty is the score taken away when a gap is created\n        gapextend: Gap extension penalty is added to the standard gap penalty for each base or residue in the gap\n        outdir (str, optional): Path to output directory. Default is the current directory.\n        outfile (str, optional): Name of output file. If not set, is {id_a}_{id_b}_align.txt\n        force_rerun (bool): Default False, set to True if you want to rerun the alignment if outfile exists.\n\n    Returns:\n        str: Raw alignment result of the needle alignment in srspair format.\n\n    \"\"\"\n    # TODO: check if needle is installed and raise error if not\n\n    if not outdir:\n        outdir = ''\n\n    # TODO: rewrite using utils functions - does not report error if needle is not installed currently\n    # TODO: rethink outdir/outfile, also if this should return the tempfile or just a file object or whatever\n    if write_outfile:\n        seq_a = ssbio.protein.sequence.utils.cast_to_str(seq_a)\n        seq_b = ssbio.protein.sequence.utils.cast_to_str(seq_b)\n\n        if not outfile:\n            outfile = op.join(tempfile.gettempdir(), 'temp_alignment.needle')\n        else:\n            outfile = op.join(outdir, outfile)\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n            cmd = 'needle -outfile=\"{}\" -asequence=asis::{} -bsequence=asis::{} -gapopen={} -gapextend={}'.format(\n                outfile, seq_a, seq_b, gapopen, gapextend)\n            command = subprocess.Popen(cmd,\n                                       stdout=subprocess.PIPE,\n                                       stderr=subprocess.PIPE,\n                                       shell=True)\n            out, err = command.communicate()\n\n        return outfile\n\n    else:\n        seq_a = ssbio.protein.sequence.utils.cast_to_str(seq_a)\n        seq_b = ssbio.protein.sequence.utils.cast_to_str(seq_b)\n\n        cmd = 'needle -auto -stdout -asequence=asis::{} -bsequence=asis::{} -gapopen={} -gapextend={}'.format(seq_a, seq_b, gapopen, gapextend)\n        command = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n        stdout = command.stdout.read()\n        return stdout", "response": "Run the needle alignment program for two strings and return the raw alignment result."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the needle alignment program for two fasta files and return the raw alignment result.", "response": "def run_needle_alignment_on_files(id_a, faa_a, id_b, faa_b, gapopen=10, gapextend=0.5,\n                                  outdir='', outfile='', force_rerun=False):\n    \"\"\"Run the needle alignment program for two fasta files and return the raw alignment result.\n\n    More info:\n    EMBOSS needle: http://www.bioinformatics.nl/cgi-bin/emboss/help/needle\n    Biopython wrapper: http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc84\n\n    Args:\n        id_a: ID of reference sequence\n        faa_a: File path to reference sequence\n        id_b: ID of sequence to be aligned\n        faa_b: File path to sequence to be aligned\n        gapopen: Gap open penalty is the score taken away when a gap is created\n        gapextend: Gap extension penalty is added to the standard gap penalty for each base or residue in the gap\n        outdir (str, optional): Path to output directory. Default is the current directory.\n        outfile (str, optional): Name of output file. If not set, is {id_a}_{id_b}_align.txt\n        force_rerun (bool): Default False, set to True if you want to rerun the alignment if outfile exists.\n\n    Returns:\n        str: Raw alignment result of the needle alignment in srspair format.\n\n    \"\"\"\n\n    # TODO: rewrite using utils functions so we can check for needle installation\n    # # If you don't want to save the output file, just run the alignment and return the raw results\n    # if not outfile and not outdir:\n    #     needle_cline = NeedleCommandline(asequence=faa_a, bsequence=faa_b,\n    #                                      gapopen=gapopen, gapextend=gapextend,\n    #                                      stdout=True, auto=True)\n    #     stdout, stderr = needle_cline()\n    #     raw_alignment_text = stdout.decode('utf-8')\n\n    # Make a default name if no outfile is set\n    if not outfile:\n        outfile = op.join(outdir, '{}_{}.needle'.format(id_a, id_b))\n    else:\n        outfile = op.join(outdir, outfile)\n\n    # Check if the outfile already exists\n    if op.exists(outfile) and not force_rerun:\n        return outfile\n    # If it doesn't exist, or force_rerun=True, run the alignment\n    else:\n        cmd = 'needle -outfile=\"{}\" -asequence=\"{}\" -bsequence=\"{}\" -gapopen={} -gapextend={}'.format(outfile,\n                                                                                                      faa_a,\n                                                                                                      faa_b,\n                                                                                                      gapopen,\n                                                                                                      gapextend)\n        command = subprocess.Popen(cmd,\n                                   stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE,\n                                   shell=True)\n        out, err = command.communicate()\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the percent identity between two alignment strings", "response": "def get_percent_identity(a_aln_seq, b_aln_seq):\n    \"\"\"Get the percent identity between two alignment strings\"\"\"\n\n    if len(a_aln_seq) != len(b_aln_seq):\n        raise ValueError('Sequence lengths not equal - was an alignment run?')\n\n    count = 0\n    gaps = 0\n    for n in range(0, len(a_aln_seq)):\n        if a_aln_seq[n] == b_aln_seq[n]:\n            if a_aln_seq[n] != \"-\":\n                count += 1\n            else:\n                gaps += 1\n\n    return count / float((len(a_aln_seq) - gaps))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_alignment_df(a_aln_seq, b_aln_seq, a_seq_id=None, b_seq_id=None):\n    if len(a_aln_seq) != len(b_aln_seq):\n        raise ValueError('Sequence lengths not equal - was an alignment run?')\n\n    if not a_seq_id:\n        a_seq_id = 'a_seq'\n    if not b_seq_id:\n        b_seq_id = 'b_seq'\n\n    a_aln_seq = ssbio.protein.sequence.utils.cast_to_str(a_aln_seq)\n    b_aln_seq = ssbio.protein.sequence.utils.cast_to_str(b_aln_seq)\n\n    a_idx = 1\n    b_idx = 1\n\n    appender = []\n\n    for i, (a,b) in enumerate(zip(a_aln_seq, b_aln_seq)):\n        to_append = {}\n\n        if a == b and a != '-' and b != '-':\n            aa_flag = 'match'\n        elif a != b and a == '-' and b != '-':\n            aa_flag = 'insertion'\n        elif a != b and a != '-' and b == '-':\n            aa_flag = 'deletion'\n        elif a != b and a != '-' and b == 'X':\n            aa_flag = 'unresolved'\n        elif a != b and b != '-' and a == 'X':\n            aa_flag = 'unresolved'\n        elif a != b and a != '-' and b != '-':\n            aa_flag = 'mutation'\n\n        to_append['id_a'] = a_seq_id\n        to_append['id_b'] = b_seq_id\n        to_append['type'] = aa_flag\n\n        if aa_flag == 'match' or aa_flag == 'unresolved' or aa_flag == 'mutation':\n            to_append['id_a_aa'] = a\n            to_append['id_a_pos'] = int(a_idx)\n            to_append['id_b_aa'] = b\n            to_append['id_b_pos'] = int(b_idx)\n            a_idx += 1\n            b_idx += 1\n\n        if aa_flag == 'deletion':\n            to_append['id_a_aa'] = a\n            to_append['id_a_pos'] = int(a_idx)\n            a_idx += 1\n\n        if aa_flag == 'insertion':\n            to_append['id_b_aa'] = b\n            to_append['id_b_pos'] = int(b_idx)\n            b_idx += 1\n\n        appender.append(to_append)\n\n    cols = ['id_a', 'id_b', 'type', 'id_a_aa', 'id_a_pos', 'id_b_aa', 'id_b_pos']\n    alignment_df = pd.DataFrame.from_records(appender, columns=cols)\n    alignment_df = alignment_df.fillna(value=np.nan)\n\n    return alignment_df", "response": "Summarize two alignment strings in a dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_alignment_df_from_file(alignment_file, a_seq_id=None, b_seq_id=None):\n    alignments = list(AlignIO.parse(alignment_file, \"emboss\"))\n    alignment_df = pd.DataFrame(columns=['id_a', 'id_b', 'type', 'id_a_aa', 'id_a_pos', 'id_b_aa', 'id_b_pos'])\n\n    for alignment in alignments:\n        if not a_seq_id:\n            a_seq_id = list(alignment)[0].id\n        a_seq = str(list(alignment)[0].seq)\n        if not b_seq_id:\n            b_seq_id = list(alignment)[1].id\n        b_seq = str(list(alignment)[1].seq)\n\n        df = get_alignment_df(a_seq, b_seq, a_seq_id, b_seq_id)\n        alignment_df = alignment_df.append(df).reset_index(drop=True)\n\n    return alignment_df", "response": "Get a Pandas DataFrame of the Needle alignment results. Contains all positions of the sequences."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of residue numbers that are mutated by the original sequence.", "response": "def get_mutations(aln_df):\n    \"\"\"Get a list of residue numbers (in the original sequence's numbering) that are mutated\n\n    Args:\n        aln_df (DataFrame): Alignment DataFrame\n        just_resnums: If only the residue numbers should be returned, instead of a list of tuples of\n            (original_residue, resnum, mutated_residue)\n\n    Returns:\n        list: Residue mutations\n\n    \"\"\"\n    mutation_df = aln_df[aln_df['type'] == 'mutation']\n    tuples = []\n    if not mutation_df.empty:\n        subset = mutation_df[['id_a_aa', 'id_a_pos', 'id_b_aa']]\n        subset['id_a_pos'] = subset['id_a_pos'].astype(int)\n        tuples = [tuple(x) for x in subset.values]\n    return tuples"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of residue numbers that are unresolved", "response": "def get_unresolved(aln_df):\n    \"\"\"Get a list of residue numbers (in the original sequence's numbering) that are unresolved\n\n    Args:\n        aln_df (DataFrame): Alignment DataFrame\n\n    Returns:\n        list: Residue numbers that are mutated\n\n    \"\"\"\n    unresolved_df = aln_df[aln_df['type'] == 'unresolved']\n    unresolved = []\n    if not unresolved_df.empty:\n        unresolved_df['id_a_pos'] = unresolved_df['id_a_pos'].astype(int)\n        unresolved = unresolved_df.id_a_pos.tolist()\n    return unresolved"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of tuples indicating the first and last residues of a deletion region.", "response": "def get_deletions(aln_df):\n    \"\"\"Get a list of tuples indicating the first and last residues of a deletion region, as well as the length of the deletion.\n\n    Examples:\n        # Deletion of residues 1 to 4, length 4\n        >>> test = {'id_a': {0: 'a', 1: 'a', 2: 'a', 3: 'a'}, 'id_a_aa': {0: 'M', 1: 'G', 2: 'I', 3: 'T'}, 'id_a_pos': {0: 1.0, 1: 2.0, 2: 3.0, 3: 4.0}, 'id_b': {0: 'b', 1: 'b', 2: 'b', 3: 'b'}, 'id_b_aa': {0: np.nan, 1: np.nan, 2: np.nan, 3: np.nan}, 'id_b_pos': {0: np.nan, 1: np.nan, 2: np.nan, 3: np.nan}, 'type': {0: 'deletion', 1: 'deletion', 2: 'deletion', 3: 'deletion'}}\n        >>> my_alignment = pd.DataFrame.from_dict(test)\n        >>> get_deletions(my_alignment)\n        [((1.0, 4.0), 4)]\n\n    Args:\n        aln_df (DataFrame): Alignment DataFrame\n\n    Returns:\n        list: A list of tuples with the format ((deletion_start_resnum, deletion_end_resnum), deletion_length)\n\n    \"\"\"\n\n    deletion_df = aln_df[aln_df['type'] == 'deletion']\n    if not deletion_df.empty:\n        deletion_df['id_a_pos'] = deletion_df['id_a_pos'].astype(int)\n    deletions = []\n\n    for k, g in groupby(deletion_df.index, key=lambda n, c=count(): n - next(c)):\n        tmp = list(g)\n        deletion_indices = (min(tmp), max(tmp))\n\n        deletion_start_ix = deletion_indices[0]\n        deletion_end_ix = deletion_indices[1]\n\n        deletion_length = deletion_end_ix - deletion_start_ix + 1\n\n        id_a_pos_deletion_start = aln_df.ix[deletion_start_ix].id_a_pos\n        id_a_pos_deletion_end = aln_df.ix[deletion_end_ix].id_a_pos\n\n        deletion_region = (id_a_pos_deletion_start, id_a_pos_deletion_end)\n\n        # Logging where the insertion is\n        log.debug('Deletion of length {} at residues {}'.format(deletion_length, deletion_region))\n\n        to_append = (deletion_region, deletion_length)\n        deletions.append(to_append)\n\n    return deletions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_insertions(aln_df):\n\n    insertion_df = aln_df[aln_df['type'] == 'insertion']\n    # if not insertion_df.empty: # don't need to do this for insertions\n    #     insertion_df['id_a_pos'] = insertion_df['id_a_pos'].astype(int)\n\n    insertions = []\n\n    for k, g in groupby(insertion_df.index, key=lambda n, c=count(): n - next(c)):\n        tmp = list(g)\n        insertion_indices = (min(tmp), max(tmp))\n\n        insertion_start = insertion_indices[0] - 1\n        insertion_end = insertion_indices[1] + 1\n\n        # Checking if insertion is at the beginning or end\n        if insertion_start < 0:\n            insertion_start = insertion_indices[0]\n            insertion_length = insertion_end - insertion_start\n        elif insertion_end >= len(aln_df):\n            insertion_end = insertion_indices[1]\n            insertion_length = insertion_end - insertion_start\n        else:\n            insertion_length = insertion_end - insertion_start - 1\n\n        id_a_pos_insertion_start = aln_df.ix[insertion_start].id_a_pos\n        id_a_pos_insertion_end = aln_df.ix[insertion_end].id_a_pos\n\n        # Checking if insertion is at the beginning or end\n        if np.isnan(id_a_pos_insertion_start) and id_a_pos_insertion_end == 1:\n            insertion_region = (-1, id_a_pos_insertion_end)\n        elif np.isnan(id_a_pos_insertion_end):\n            insertion_region = (id_a_pos_insertion_start, float('Inf'))\n        else:\n            insertion_region = (id_a_pos_insertion_start, id_a_pos_insertion_end)\n\n        # Logging where the insertion is\n        if insertion_region[0] == -1:\n            log.debug('Insertion of length {} at beginning'.format(insertion_length))\n        elif insertion_region[1] == float('Inf'):\n            log.debug('Insertion of length {} at end'.format(insertion_length))\n        else:\n            log.debug('Insertion of length {} at residues {}'.format(insertion_length, insertion_region))\n\n        to_append = (insertion_region, insertion_length)\n        insertions.append(to_append)\n\n    return insertions", "response": "Get a list of tuples indicating the first and last residues of an insertion region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map_resnum_a_to_resnum_b(resnums, a_aln, b_aln):\n    resnums = ssbio.utils.force_list(resnums)\n\n    aln_df = get_alignment_df(a_aln, b_aln)\n\n    maps = aln_df[aln_df.id_a_pos.isin(resnums)]\n\n    able_to_map_to_b = maps[pd.notnull(maps.id_b_pos)]\n    successful_map_from_a = able_to_map_to_b.id_a_pos.values.tolist()\n\n    mapping = dict([(int(a), int(b)) for a,b in zip(able_to_map_to_b.id_a_pos, able_to_map_to_b.id_b_pos)])\n\n    cant_map = list(set(resnums).difference(successful_map_from_a))\n    if len(cant_map) > 0:\n        log.warning('Unable to map residue numbers {} in first sequence to second'.format(cant_map))\n\n    return mapping", "response": "Map a residue number in a sequence to the corresponding residue number in an aligned sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a report of a pairwise alignment.", "response": "def pairwise_alignment_stats(reference_seq_aln, other_seq_aln):\n    \"\"\"Get a report of a pairwise alignment.\n\n    Args:\n        reference_seq_aln (str, Seq, SeqRecord): Reference sequence, alignment form\n        other_seq_aln (str, Seq, SeqRecord): Other sequence, alignment form\n\n    Returns:\n        dict: Dictionary of information on mutations, insertions, sequence identity, etc.\n\n    \"\"\"\n    if len(reference_seq_aln) != len(other_seq_aln):\n        raise ValueError('Sequence lengths not equal - was an alignment run?')\n\n    reference_seq_aln = ssbio.protein.sequence.utils.cast_to_str(reference_seq_aln)\n    other_seq_aln = ssbio.protein.sequence.utils.cast_to_str(other_seq_aln)\n\n    infodict = {}\n\n    # Percent identity to the reference sequence\n    stats_percent_ident = get_percent_identity(a_aln_seq=reference_seq_aln, b_aln_seq=other_seq_aln)\n    infodict['percent_identity'] = stats_percent_ident\n\n    # Other alignment results\n    aln_df = get_alignment_df(a_aln_seq=reference_seq_aln, b_aln_seq=other_seq_aln)\n    infodict['deletions'] = get_deletions(aln_df)\n    infodict['insertions'] = get_insertions(aln_df)\n    infodict['mutations'] = get_mutations(aln_df)\n    infodict['unresolved'] = get_unresolved(aln_df)\n\n    return infodict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef needle_statistics(infile):\n\n    alignments = list(AlignIO.parse(infile, \"emboss\"))\n    alignment_properties = defaultdict(dict)\n\n    with open(infile) as f:\n        line = f.readline()\n\n        for i in range(len(alignments)):\n            while line.rstrip() != \"#=======================================\":\n                line = f.readline()\n                if not line:\n                    raise StopIteration\n\n            while line[0] == \"#\":\n                # Read in the rest of this alignment header,\n                # try and discover the number of records expected and their length\n                parts = line[1:].split(\":\", 1)\n                key = parts[0].lower().strip()\n                if key == '1':\n                    a_id = parts[1].strip()\n                if key == '2':\n                    b_id = parts[1].strip()\n                if key == 'identity':\n                    ident_parse = parts[1].strip().replace('(','').replace(')','').replace('%','').split()\n                    ident_num = int(ident_parse[0].split('/')[0])\n                    ident_percent = float(ident_parse[1])\n                    alignment_properties[a_id + '_' + b_id]['identity'] = ident_num\n                    alignment_properties[a_id + '_' + b_id]['percent_identity'] = ident_percent\n                if key == 'similarity':\n                    sim_parse = parts[1].strip().replace('(','').replace(')','').replace('%','').split()\n                    sim_num = int(sim_parse[0].split('/')[0])\n                    sim_percent = float(sim_parse[1])\n                    alignment_properties[a_id + '_' + b_id]['similarity'] = sim_num\n                    alignment_properties[a_id + '_' + b_id]['percent_similarity'] = sim_percent\n                if key == 'gaps':\n                    gap_parse = parts[1].strip().replace('(','').replace(')','').replace('%','').split()\n                    gap_num = int(gap_parse[0].split('/')[0])\n                    gap_percent = float(gap_parse[1])\n                    alignment_properties[a_id + '_' + b_id]['gaps'] = gap_num\n                    alignment_properties[a_id + '_' + b_id]['percent_gaps'] = gap_percent\n                if key == 'score':\n                    score = float(parts[1].strip())\n                    alignment_properties[a_id + '_' + b_id]['score'] = score\n\n                # And read in another line...\n                line = f.readline()\n\n    return alignment_properties", "response": "Reads in a needle alignment file and spits out statistics of the alignment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef needle_statistics_alignio(infile):\n\n    alignments = list(AlignIO.parse(infile, \"emboss\"))\n\n    if len(alignments) > 1:\n        raise ValueError('Alignment file contains more than one pairwise alignment')\n\n    alignment = alignments[0]\n\n    with open(infile) as f:\n        line = f.readline()\n\n        for i in range(len(alignments)):\n            while line.rstrip() != \"#=======================================\":\n                line = f.readline()\n                if not line:\n                    raise StopIteration\n\n            while line[0] == \"#\":\n                # Read in the rest of this alignment header,\n                # try and discover the number of records expected and their length\n                parts = line[1:].split(\":\", 1)\n                key = parts[0].lower().strip()\n                if key == 'identity':\n                    ident_parse = parts[1].strip().replace('(','').replace(')','').replace('%','').split()\n                    ident_num = int(ident_parse[0].split('/')[0])\n                    ident_percent = float(ident_parse[1])\n                    alignment.annotations['identity'] = ident_num\n                    alignment.annotations['percent_identity'] = ident_percent\n                if key == 'similarity':\n                    sim_parse = parts[1].strip().replace('(','').replace(')','').replace('%','').split()\n                    sim_num = int(sim_parse[0].split('/')[0])\n                    sim_percent = float(sim_parse[1])\n                    alignment.annotations['similarity'] = sim_num\n                    alignment.annotations['percent_similarity'] = sim_percent\n                if key == 'gaps':\n                    gap_parse = parts[1].strip().replace('(','').replace(')','').replace('%','').split()\n                    gap_num = int(gap_parse[0].split('/')[0])\n                    gap_percent = float(gap_parse[1])\n                    alignment.annotations['gaps'] = gap_num\n                    alignment.annotations['percent_gaps'] = gap_percent\n                if key == 'score':\n                    score = float(parts[1].strip())\n                    alignment.annotations['score'] = score\n\n                # And read in another line...\n                line = f.readline()\n\n    return alignment", "response": "Reads in a needle alignment file and returns an AlignIO object with annotations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning FoldX RepairPDB on this PDB file.", "response": "def run_repair_pdb(self, silent=False, force_rerun=False):\n        \"\"\"Run FoldX RepairPDB on this PDB file.\n\n        Original command::\n\n            foldx --command=RepairPDB --pdb=4bxi.pdb\n\n        Args:\n            silent (bool): If FoldX output should be silenced from printing to the shell.\n            force_rerun (bool): If FoldX RepairPDB should be rerun even if a repaired file exists.\n\n        \"\"\"\n        # Create RepairPDB command\n        foldx_repair_pdb = 'foldx --command=RepairPDB --pdb={}'.format(self.pdb_file)\n\n        # Repaired PDB output file name\n        foldx_repair_outfile = '{}_Repair.pdb'.format(op.splitext(self.pdb_file)[0])\n\n        # Run RepairPDB\n        ssbio.utils.command_runner(shell_command=foldx_repair_pdb, force_rerun_flag=force_rerun, silent=silent,\n                                   outfile_checker=foldx_repair_outfile, cwd=self.foldx_dir)\n\n        # TODO: write stdout/stderr to log file somewhere!\n\n        self.repaired_pdb_outfile = foldx_repair_outfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_mutation_file(self, list_of_tuples):\n\n        self.mutation_infile = op.join(self.foldx_dir, 'individual_list.txt')\n\n        idx = 1\n\n        with open(self.mutation_infile, 'w') as f:\n            for mutant_group in list_of_tuples:\n                # Write the mutation string to the file\n                mutstring = ''.join(list(map(lambda x: '{}{}{}{};'.format(x[0], x[1], x[2], x[3]), mutant_group)))\n                f.write(mutstring + '\\n')\n\n                # Also keep track of the index being used for this mutation\n                self.mutation_index_to_group[idx] = mutant_group\n                idx += 1", "response": "Create the FoldX file in the self. foldx_dir where the file is generated for each of the individual list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_random_mutation_file(self, list_of_tuples, original_sequence,\n                                    randomize_resnums=False, randomize_resids=False,\n                                    skip_resnums=None):\n        \"\"\"Create the FoldX file 'individual_list.txt', but randomize the mutation numbers or residues that were input.\n\n        The randomize combinations can be a little confusing - this is what can happen:\n\n            - randomize_resnums=False, randomize_resids=False: no change, original mutations are carried out\n            - randomize_resnums=True, randomize_resids=False: mutations of resid X to resid Y will be carried out,\n                but on a different residue number where resid X is found\n            - randomize_resnums=False, randomize_resids=True: mutations of residue X# to a random residue will be\n                carried out\n            - randomize_resnums=True, randomize_resids=True: original mutations will be ignored, random mutation of\n                any residue will be carried out\n\n        Args:\n            list_of_tuples (list): A list of tuples indicating mutation groups to be randomized.\n            original_sequence (str, Seq, SeqRecord): Original amino acid sequence\n            randomize_resnums (bool): If residue numbers should be randomized\n            randomize_resids (bool): If residues themselves should be randomized\n            skip_resnums (list):\n\n        \"\"\"\n\n        import random\n\n        def find(s, ch):\n            return [i for i, ltr in enumerate(s) if ltr == ch]", "response": "Create the FoldX file individual_list. txt but randomize the mutation numbers or residues that were input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning FoldX BuildModel command with a mutant file input.", "response": "def run_build_model(self, num_runs=5, silent=False, force_rerun=False):\n        \"\"\"Run FoldX BuildModel command with a mutant file input.\n\n        Original command::\n\n            foldx --command=BuildModel --pdb=4bxi_Repair.pdb --mutant-file=individual_list.txt --numberOfRuns=5\n\n        Args:\n            num_runs (int):\n            silent (bool): If FoldX output should be silenced from printing to the shell.\n            force_rerun (bool): If FoldX BuildModel should be rerun even if the results file exists.\n\n        \"\"\"\n        # BuildModel output files\n        self.mutation_ddG_avg_outfile = 'Average_{}.fxout'.format(op.splitext(self.repaired_pdb_outfile)[0])\n        self.mutation_ddG_raw_outfile = 'Raw_{}.fxout'.format(op.splitext(self.repaired_pdb_outfile)[0])\n\n        # BuildModel command\n        foldx_build_model = 'foldx --command=BuildModel --pdb={} --mutant-file={} --numberOfRuns={}'.format(self.repaired_pdb_outfile,\n                                                                                                            op.basename(self.mutation_infile),\n                                                                                                            num_runs)\n\n        ssbio.utils.command_runner(shell_command=foldx_build_model, force_rerun_flag=force_rerun, silent=silent,\n                                   outfile_checker=self.mutation_ddG_avg_outfile, cwd=self.foldx_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ddG_results(self):\n        foldx_avg_df = self.df_mutation_ddG_avg\n\n        foldx_avg_ddG = {}\n        results = foldx_avg_df[['Pdb', 'total energy', 'SD']].T.to_dict().values()\n        for r in results:\n            ident = r['Pdb'].split('_')[-1]\n            ddG = r['total energy']\n            ddG_sd = r['SD']\n\n            foldx_avg_ddG[self.mutation_index_to_group[int(ident)]] = (ddG, ddG_sd)\n\n        return foldx_avg_ddG", "response": "Parse the results from BuildModel and get the delta G s."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncleaning a PDB file and return a new file with the cleaned structure.", "response": "def clean_pdb(pdb_file, out_suffix='_clean', outdir=None, force_rerun=False,\n              remove_atom_alt=True, keep_atom_alt_id='A', remove_atom_hydrogen=True, add_atom_occ=True,\n              remove_res_hetero=True, keep_chemicals=None, keep_res_only=None,\n              add_chain_id_if_empty='X', keep_chains=None):\n    \"\"\"Clean a PDB file.\n\n    Args:\n        pdb_file (str): Path to input PDB file\n        out_suffix (str): Suffix to append to original filename\n        outdir (str): Path to output directory\n        force_rerun (bool): If structure should be re-cleaned if a clean file exists already\n        remove_atom_alt (bool): Remove alternate positions\n        keep_atom_alt_id (str): If removing alternate positions, which alternate ID to keep\n        remove_atom_hydrogen (bool): Remove hydrogen atoms\n        add_atom_occ (bool): Add atom occupancy fields if not present\n        remove_res_hetero (bool): Remove all HETATMs\n        keep_chemicals (str, list): If removing HETATMs, keep specified chemical names\n        keep_res_only (str, list): Keep ONLY specified resnames, deletes everything else!\n        add_chain_id_if_empty (str): Add a chain ID if not present\n        keep_chains (str, list): Keep only these chains\n\n    Returns:\n        str: Path to cleaned PDB file\n\n    \"\"\"\n    outfile = ssbio.utils.outfile_maker(inname=pdb_file,\n                                        append_to_name=out_suffix,\n                                        outdir=outdir,\n                                        outext='.pdb')\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        my_pdb = StructureIO(pdb_file)\n        my_cleaner = CleanPDB(remove_atom_alt=remove_atom_alt,\n                              remove_atom_hydrogen=remove_atom_hydrogen,\n                              keep_atom_alt_id=keep_atom_alt_id,\n                              add_atom_occ=add_atom_occ,\n                              remove_res_hetero=remove_res_hetero,\n                              keep_res_only=keep_res_only,\n                              add_chain_id_if_empty=add_chain_id_if_empty,\n                              keep_chains=keep_chains,\n                              keep_chemicals=keep_chemicals)\n\n        my_clean_pdb = my_pdb.write_pdb(out_suffix=out_suffix,\n                                        out_dir=outdir,\n                                        custom_selection=my_cleaner,\n                                        force_rerun=force_rerun)\n\n        return my_clean_pdb\n    else:\n        return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning MSMS on a Biopython Structure Model and return a Pandas DataFrame.", "response": "def get_msms_df(model, pdb_id, outfile=None, outdir=None, outext='_msms.df', force_rerun=False):\n    \"\"\"Run MSMS (using Biopython) on a Biopython Structure Model.\n\n    Depths are in units Angstroms. 1A = 10^-10 m = 1nm. Returns a dictionary of::\n\n        {\n            chain_id:{\n                        resnum1_id: (res_depth, ca_depth),\n                        resnum2_id: (res_depth, ca_depth)\n                     }\n        }\n\n    Args:\n        model: Biopython Structure Model\n\n    Returns:\n        Pandas DataFrame: ResidueDepth property_dict, reformatted\n\n    \"\"\"\n    # XTODO: need to deal with temporary surface/vertex files in tmp directory when running on a large scale --\n    # XTODO: will run into inode limits! Also, some valuable information is in these MSMS output files that we should save.\n\n    # Create the output file name\n    outfile = ssbio.utils.outfile_maker(inname=pdb_id, outname=outfile, outdir=outdir, outext=outext)\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        # Run MSMS with Biopython\n        try:\n            rd = PDB.ResidueDepth(model)\n        except AssertionError:\n            log.error('{}: unable to run MSMS'.format(pdb_id))\n            return pd.DataFrame()\n\n        # Reorganize the results into a csv file\n        appender = []\n        for k in rd.property_keys:\n            x = rd.property_dict[k]\n            chain = k[0]\n            residue = k[1]\n            het = residue[0]\n            resnum = residue[1]\n            icode = residue[2]\n            resdepth = x[0]\n            cadepth = x[1]\n            appender.append((chain, resnum, icode, resdepth, cadepth))\n\n        df = pd.DataFrame.from_records(appender, columns=['chain', 'resnum', 'icode', 'res_depth', 'ca_depth'])\n        df.to_csv(outfile)\n    else:\n        log.debug('{}: already ran MSMS and force_rerun={}, loading results'.format(outfile, force_rerun))\n        df = pd.read_csv(outfile, index_col=0)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning MSMS on a file and return a Pandas DataFrame", "response": "def get_msms_df_on_file(pdb_file, outfile=None, outdir=None, outext='_msms.df', force_rerun=False):\n    \"\"\"Run MSMS (using Biopython) on a PDB file.\n\n    Saves a CSV file of:\n        chain: chain ID\n        resnum: residue number (PDB numbering)\n        icode: residue insertion code\n        res_depth: average depth of all atoms in a residue\n        ca_depth: depth of the alpha carbon atom\n\n    Depths are in units Angstroms. 1A = 10^-10 m = 1nm\n\n    Args:\n        pdb_file: Path to PDB file\n        outfile: Optional name of output file (without extension)\n        outdir: Optional output directory\n        outext: Optional extension for the output file\n        outext: Suffix appended to json results file\n        force_rerun: Rerun MSMS even if results exist already\n\n    Returns:\n        Pandas DataFrame: ResidueDepth property_dict, reformatted\n\n    \"\"\"\n    # Create the output file name\n    outfile = ssbio.utils.outfile_maker(inname=pdb_file, outname=outfile, outdir=outdir, outext=outext)\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        # Load the structure\n        my_structure = StructureIO(pdb_file)\n        model = my_structure.first_model\n        df = get_msms_df(model, pdb_id=op.splitext(op.basename(pdb_file))[0],\n                         outfile=outfile, outdir=outdir, outext=outext, force_rerun=force_rerun)\n    else:\n        log.debug('{}: already ran MSMS and force_rerun={}, loading results'.format(outfile, force_rerun))\n        df = pd.read_csv(outfile, index_col=0)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_bioassembly_info_from_file(self, biomol_num):\n\n        # current functionality is to take in a pre-assembled bioassembly file, parse it's MODELs and get info from that\n        pass\n        from Bio.PDB import PDBParser\n        p = PDBParser(PERMISSIVE=True, QUIET=True)\n\n        structure = StructureIO(self.structure_path, self.file_type)\n\n        structure = p.get_structure('tmp', infile)\n        bioass_to_chain_stoich = defaultdict(int)\n        for model in structure:\n            for chain in model:\n                bioass_to_chain_stoich[chain.id] += 1\n        return dict(bioass_to_chain_stoich)", "response": "Load metadata about a bioassembly from a structure file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_mmtf_header(infile):\n    infodict = {}\n\n    mmtf_decoder = mmtf.parse(infile)\n    infodict['date'] = mmtf_decoder.deposition_date\n    infodict['release_date'] = mmtf_decoder.release_date\n    try:\n        infodict['experimental_method'] = [x.decode() for x in mmtf_decoder.experimental_methods]\n    except AttributeError:\n        infodict['experimental_method'] = [x for x in mmtf_decoder.experimental_methods]\n    infodict['resolution'] = mmtf_decoder.resolution\n    infodict['description'] = mmtf_decoder.title\n\n    group_name_exclude = ['HOH']\n    chem_comp_type_exclude = ['l-peptide linking', 'peptide linking']\n    chemicals = list(set([mmtf_decoder.group_list[idx]['groupName'] for idx in mmtf_decoder.group_type_list if mmtf_decoder.group_list[idx]['chemCompType'].lower() not in chem_comp_type_exclude and mmtf_decoder.group_list[idx]['groupName'] not in group_name_exclude]))\n    infodict['chemicals'] = chemicals\n    return infodict", "response": "Parse an MMTF file and return basic header - like information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_mmcif_header(pdb_id, outdir='', force_rerun=False):\n    # TODO: keep an eye on https://github.com/biopython/biopython/pull/943 Biopython PR#493 for functionality of this\n    # method in biopython. extra file types have not been added to biopython download yet\n\n    pdb_id = pdb_id.lower()\n    file_type = 'cif'\n    folder = 'header'\n    outfile = op.join(outdir, '{}.header.{}'.format(pdb_id, file_type))\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        download_link = 'http://files.rcsb.org/{}/{}.{}'.format(folder, pdb_id, file_type)\n        urlretrieve(download_link, outfile)\n        log.debug('{}: saved header file'.format(outfile))\n    else:\n        log.debug('{}: header file already saved'.format(outfile))\n\n    return outfile", "response": "Download a mmCIF header file from the RCSB PDB by ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a couple important fields from the mmCIF file format with some manual curation of ligands.", "response": "def parse_mmcif_header(infile):\n    \"\"\"Parse a couple important fields from the mmCIF file format with some manual curation of ligands.\n\n    If you want full access to the mmCIF file just use the MMCIF2Dict class in Biopython.\n\n    Args:\n        infile: Path to mmCIF file\n\n    Returns:\n        dict: Dictionary of parsed header\n\n    \"\"\"\n    from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n\n    newdict = {}\n    try:\n        mmdict = MMCIF2Dict(infile)\n    except ValueError as e:\n        log.exception(e)\n        return newdict\n\n    chemical_ids_exclude = ['HOH']\n    chemical_types_exclude = ['l-peptide linking','peptide linking']\n\n    if '_struct.title' in mmdict:\n        newdict['pdb_title'] = mmdict['_struct.title']\n    else:\n        log.debug('{}: No title field'.format(infile))\n\n    if '_struct.pdbx_descriptor' in mmdict:\n        newdict['description'] = mmdict['_struct.pdbx_descriptor']\n    else:\n        log.debug('{}: no description field'.format(infile))\n\n    if '_pdbx_database_status.recvd_initial_deposition_date' in mmdict:\n        newdict['date'] = mmdict['_pdbx_database_status.recvd_initial_deposition_date']\n    elif '_database_PDB_rev.date' in mmdict:\n        newdict['date'] = mmdict['_database_PDB_rev.date']\n    else:\n        log.debug('{}: no date field'.format(infile))\n\n    if '_exptl.method' in mmdict:\n        newdict['experimental_method'] = mmdict['_exptl.method']\n    else:\n        log.debug('{}: no experimental method field'.format(infile))\n\n    # TODO: refactor how to get resolutions based on experimental method\n    if '_refine.ls_d_res_high' in mmdict:\n        try:\n            if isinstance(mmdict['_refine.ls_d_res_high'], list):\n                newdict['resolution'] = [float(x) for x in mmdict['_refine.ls_d_res_high']]\n            else:\n                newdict['resolution'] = float(mmdict['_refine.ls_d_res_high'])\n        except:\n            try:\n                newdict['resolution'] = float(mmdict['_em_3d_reconstruction.resolution'])\n            except:\n                log.debug('{}: no resolution field'.format(infile))\n    else:\n        log.debug('{}: no resolution field'.format(infile))\n\n    if '_chem_comp.id' in mmdict:\n        chemicals_filtered = ssbio.utils.filter_list_by_indices(mmdict['_chem_comp.id'],\n                                                            ssbio.utils.not_find(mmdict['_chem_comp.type'],\n                                                                           chemical_types_exclude,\n                                                                           case_sensitive=False))\n        chemicals_fitered = ssbio.utils.filter_list(chemicals_filtered, chemical_ids_exclude, case_sensitive=True)\n        newdict['chemicals'] = chemicals_fitered\n    else:\n        log.debug('{}: no chemical composition field'.format(infile))\n\n    if '_entity_src_gen.pdbx_gene_src_scientific_name' in mmdict:\n        newdict['taxonomy_name'] = mmdict['_entity_src_gen.pdbx_gene_src_scientific_name']\n    else:\n        log.debug('{}: no organism field'.format(infile))\n\n    return newdict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload the SIFTS XML file for a given PDB ID.", "response": "def download_sifts_xml(pdb_id, outdir='', force_rerun=False):\n    \"\"\"Download the SIFTS file for a PDB ID.\n\n    Args:\n        pdb_id (str): PDB ID\n        outdir (str): Output directory, current working directory if not specified.\n        force_rerun (bool): If the file should be downloaded again even if it exists\n\n    Returns:\n        str: Path to downloaded file\n\n    \"\"\"\n    baseURL = 'ftp://ftp.ebi.ac.uk/pub/databases/msd/sifts/xml/'\n    filename = '{}.xml.gz'.format(pdb_id.lower())\n\n    outfile = op.join(outdir, filename.split('.')[0] + '.sifts.xml')\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        response = urlopen(baseURL + filename)\n        with open(outfile, 'wb') as f:\n            f.write(gzip.decompress(response.read()))\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map_uniprot_resnum_to_pdb(uniprot_resnum, chain_id, sifts_file):\n    # Load the xml with lxml\n    parser = etree.XMLParser(ns_clean=True)\n    tree = etree.parse(sifts_file, parser)\n    root = tree.getroot()\n\n    my_pdb_resnum = None\n\n    # TODO: \"Engineered_Mutation is also a possible annotation, need to figure out what to do with that\n    my_pdb_annotation = False\n\n    # Find the right chain (entities in the xml doc)\n    ent = './/{http://www.ebi.ac.uk/pdbe/docs/sifts/eFamily.xsd}entity'\n    for chain in root.findall(ent):\n        # TODO: IMPORTANT - entityId is not the chain ID!!! it is just in alphabetical order!\n        if chain.attrib['entityId'] == chain_id:\n            # Find the \"crossRefDb\" tag that has the attributes dbSource=\"UniProt\" and  dbResNum=\"your_resnum_here\"\n            # Then match it to the crossRefDb dbResNum that has the attribute dbSource=\"PDBresnum\"\n\n            # Check if uniprot + resnum even exists in the sifts file (it won't if the pdb doesn't contain the residue)\n            ures = './/{http://www.ebi.ac.uk/pdbe/docs/sifts/eFamily.xsd}crossRefDb[@dbSource=\"UniProt\"][@dbResNum=\"%s\"]' % uniprot_resnum\n            my_uniprot_residue = chain.findall(ures)\n            if len(my_uniprot_residue) == 1:\n                # Get crossRefDb dbSource=\"PDB\"\n                parent = my_uniprot_residue[0].getparent()\n                pres = './/{http://www.ebi.ac.uk/pdbe/docs/sifts/eFamily.xsd}crossRefDb[@dbSource=\"PDB\"]'\n                my_pdb_residue = parent.findall(pres)\n                my_pdb_resnum = int(my_pdb_residue[0].attrib['dbResNum'])\n\n                # Get <residueDetail dbSource=\"PDBe\" property=\"Annotation\">\n                # Will be Not_Observed if it is not seen in the PDB\n                anno = './/{http://www.ebi.ac.uk/pdbe/docs/sifts/eFamily.xsd}residueDetail[@dbSource=\"PDBe\"][@property=\"Annotation\"]'\n                my_pdb_annotation = parent.findall(anno)\n                if len(my_pdb_annotation) == 1:\n                    my_pdb_annotation = my_pdb_annotation[0].text\n                    if my_pdb_annotation == 'Not_Observed':\n                        my_pdb_annotation = False\n                else:\n                    my_pdb_annotation = True\n            else:\n                return None, False\n\n    return my_pdb_resnum, my_pdb_annotation", "response": "Map a UniProt residue number to its corresponding PDB residue number."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef best_structures(uniprot_id, outname=None, outdir=None, seq_ident_cutoff=0.0, force_rerun=False):\n    outfile = ''\n\n    if not outdir:\n        outdir = ''\n\n    # if output dir is specified but not outname, use the uniprot\n    if not outname and outdir:\n        outname = uniprot_id\n\n    if outname:\n        outname = op.join(outdir, outname)\n        outfile = '{}.json'.format(outname)\n\n    # Load a possibly existing json file\n    if not ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        with open(outfile, 'r') as f:\n            raw_data = json.load(f)\n        log.debug('{}: loaded existing json file'.format(uniprot_id))\n\n    # Otherwise run the web request\n    else:\n        # TODO: add a checker for a cached file of uniprot -> PDBs - can be generated within gempro pipeline and stored\n        response = requests.get('https://www.ebi.ac.uk/pdbe/api/mappings/best_structures/{}'.format(uniprot_id),\n                                data={'key': 'value'})\n        if response.status_code == 404:\n            log.debug('{}: 404 returned, probably no structures available.'.format(uniprot_id))\n            raw_data = {uniprot_id: {}}\n        else:\n            log.debug('{}: Obtained best structures'.format(uniprot_id))\n            raw_data = response.json()\n\n        # Write the json file if specified\n        if outfile:\n            with open(outfile, 'w') as f:\n                json.dump(raw_data, f)\n            log.debug('{}: Saved json file of best structures'.format(uniprot_id))\n\n    data = dict(raw_data)[uniprot_id]\n\n    # Filter for sequence identity percentage\n    if seq_ident_cutoff != 0:\n        for result in data:\n            if result['coverage'] < seq_ident_cutoff:\n                data.remove(result)\n\n    return data", "response": "Use PDBe REST service to query for the best structures for a UniProt ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _property_table():\n    url = 'http://www.rcsb.org/pdb/rest/customReport.csv?pdbids=*&customReportColumns=structureId,resolution,experimentalTechnique,releaseDate&service=wsfile&format=csv'\n    r = requests.get(url)\n    p = pd.read_csv(StringIO(r.text)).set_index('structureId')\n    return p", "response": "Download the PDB -> resolution table directly from the RCSB PDB REST service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_resolution(pdb_id):\n\n    pdb_id = pdb_id.upper()\n    if pdb_id not in _property_table().index:\n        raise ValueError('PDB ID not in property table')\n    else:\n        resolution = _property_table().ix[pdb_id, 'resolution']\n        if pd.isnull(resolution):\n            log.debug('{}: no resolution available, probably not an X-ray crystal structure')\n            resolution = float('inf')\n\n    return resolution", "response": "Quick way to get the resolution of a PDB ID in Angstroms."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_release_date(pdb_id):\n\n    pdb_id = pdb_id.upper()\n    if pdb_id not in _property_table().index:\n        raise ValueError('PDB ID not in property table')\n    else:\n        release_date = _property_table().ix[pdb_id, 'releaseDate']\n        if pd.isnull(release_date):\n            log.debug('{}: no release date available')\n            release_date = None\n\n    return release_date", "response": "Quick way to get the release date of a given PDB ID using the REST service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the number of biological assemblies associated with a PDB entry.", "response": "def get_num_bioassemblies(pdb_id, cache=False, outdir=None, force_rerun=False):\n    \"\"\"Check if there are bioassemblies using the PDB REST API, and if there are, get the number of bioassemblies\n    available.\n\n    See: https://www.rcsb.org/pages/webservices/rest, section 'List biological assemblies'\n\n    Not all PDB entries have biological assemblies available and some have multiple. Details that are necessary to\n    recreate a biological assembly from the asymmetric unit can be accessed from the following requests.\n\n    - Number of biological assemblies associated with a PDB entry\n    - Access the transformation information needed to generate a biological assembly (nr=0 will return information\n      for the asymmetric unit, nr=1 will return information for the first assembly, etc.)\n\n    A query of https://www.rcsb.org/pdb/rest/bioassembly/nrbioassemblies?structureId=1hv4 returns this::\n\n        <nrBioAssemblies structureId=\"1HV4\" hasAssemblies=\"true\" count=\"2\"/>\n\n    Args:\n        pdb_id (str): PDB ID\n        cache (bool): If the XML file should be downloaded\n        outdir (str): If cache, then specify the output directory\n        force_rerun (bool): If cache, and if file exists, specify if API should be queried again\n\n    \"\"\"\n    parser = etree.XMLParser(ns_clean=True)\n\n    if not outdir:\n        outdir = os.getcwd()\n    outfile = op.join(outdir, '{}_nrbiomols.xml'.format(pdb_id))\n\n    if ssbio.utils.force_rerun(force_rerun, outfile):\n        page = 'https://www.rcsb.org/pdb/rest/bioassembly/nrbioassemblies?structureId={}'.format(pdb_id)\n        req = requests.get(page)\n\n        if req.status_code == 200:\n            response = req.text\n\n            # Save the XML file\n            if cache:\n                with open(outfile, 'w') as f:\n                    f.write(response)\n\n            # Parse the XML string\n            tree = etree.ElementTree(etree.fromstring(response, parser))\n            log.debug('Loaded bioassembly information from REST server')\n        else:\n            log.error('Request timed out')\n            req.raise_for_status()\n    else:\n        tree = etree.parse(outfile, parser)\n        log.debug('{}: Loaded existing XML results'.format(outfile))\n\n    r = tree.getroot()\n    has_biomols = r.get('hasAssemblies')\n    if has_biomols == 'true':\n        has_biomols = True\n    else:\n        has_biomols = False\n\n    if has_biomols:\n        num_biomols = r.get('count')\n    else:\n        num_biomols = 0\n\n    num_biomols = int(num_biomols)\n    return num_biomols"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bioassembly_info(pdb_id, biomol_num, cache=False, outdir=None, force_rerun=False):\n    parser = etree.XMLParser(ns_clean=True)", "response": "Get metadata about a biological assembly from the RCSB PDB s REST API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_structure(pdb_id, file_type, outdir='', only_header=False, force_rerun=False):\n    # method in biopython. extra file types have not been added to biopython download yet\n\n    pdb_id = pdb_id.lower()\n    file_type = file_type.lower()\n    file_types = ['pdb', 'pdb.gz', 'mmcif', 'cif', 'cif.gz', 'xml.gz', 'mmtf', 'mmtf.gz']\n    if file_type not in file_types:\n        raise ValueError('Invalid file type, must be either: pdb, pdb.gz, cif, cif.gz, xml.gz, mmtf, mmtf.gz')\n\n    if file_type == 'mmtf':\n        file_type = 'mmtf.gz'\n\n    if file_type.endswith('.gz'):\n        gzipped = True\n    else:\n        gzipped = False\n\n    if file_type == 'mmcif':\n        file_type = 'cif'\n\n    if only_header:\n        folder = 'header'\n        outfile = op.join(outdir, '{}.header.{}'.format(pdb_id, file_type))\n    else:\n        folder = 'download'\n        outfile = op.join(outdir, '{}.{}'.format(pdb_id, file_type))\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        if file_type == 'mmtf.gz' or file_type == 'mmtf':\n            mmtf_api = '1.0'\n            download_link = 'http://mmtf.rcsb.org/v{}/full/{}.mmtf.gz'.format(mmtf_api, pdb_id)\n        else:\n            download_link = 'http://files.rcsb.org/{}/{}.{}'.format(folder, pdb_id, file_type)\n\n        urlretrieve(download_link, outfile)\n\n        if gzipped:\n            outfile = ssbio.utils.gunzip_file(infile=outfile,\n                                              outfile=outfile.strip('.gz'),\n                                              outdir=outdir,\n                                              delete_original=False,\n                                              force_rerun_flag=force_rerun)\n\n        log.debug('{}: saved structure file'.format(outfile))\n    else:\n        if file_type == 'mmtf.gz':\n            outfile = op.join(outdir, '{}.{}'.format(pdb_id, 'mmtf'))\n        log.debug('{}: structure file already saved'.format(outfile))\n\n    return outfile", "response": "Download a structure from the RCSB PDB by ID. Specify the file type desired."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_structure_file(self, outdir, file_type=None, load_header_metadata=True, force_rerun=False):\n        ssbio.utils.double_check_attribute(object=self, setter=file_type, backup_attribute='file_type',\n                                           custom_error_text='Please set file type to be downloaded from the PDB: '\n                                                             'pdb, mmCif, xml, or mmtf')\n\n        # XTODO: check if outfile exists using ssbio.utils.force_rerun, pdblist seems to take long if it exists\n        # I know why - it's because we're renaming the ent to pdb. need to have mapping from file type to final extension\n        # Then check if file exists, if not then download again\n        p = PDBList()\n        with ssbio.utils.suppress_stdout():\n            structure_file = p.retrieve_pdb_file(pdb_code=self.id, pdir=outdir, file_format=file_type, overwrite=force_rerun)\n        if not op.exists(structure_file):\n            log.debug('{}: {} file not available'.format(self.id, file_type))\n            raise URLError('{}.{}: file not available to download'.format(self.id, file_type))\n        else:\n            log.debug('{}: {} file saved'.format(self.id, file_type))\n\n            # Rename .ent files to .pdb\n            if file_type == 'pdb':\n                new_name = structure_file.replace('pdb', '').replace('ent', 'pdb')\n                os.rename(structure_file, new_name)\n                structure_file = new_name\n\n            self.load_structure_path(structure_file, file_type)\n            if load_header_metadata and file_type == 'mmtf':\n                self.update(parse_mmtf_header(structure_file))\n            if load_header_metadata and file_type != 'mmtf':\n                self.update(parse_mmcif_header(download_mmcif_header(pdb_id=self.id, outdir=outdir, force_rerun=force_rerun)))", "response": "Download a structure file from the PDB file system and parse it into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sequence_checker(reference_seq_aln, structure_seq_aln,\n                     seq_ident_cutoff=0.5, allow_missing_on_termini=0.2,\n                     allow_mutants=False, allow_deletions=False,\n                     allow_insertions=False, allow_unresolved=False):\n    \"\"\"Report if a structure's sequence meets coverage checks to a reference sequence.\n\n    First aligns a sequence from a chain of a PDB structure to \"reference\" sequence.\n    Then creates a DataFrame of results to check for everything.\n\n    Args:\n        reference_seq_aln (str, Seq, SeqRecord): Reference sequence, alignment form\n        structure_seq_aln (str, Seq, SeqRecord): Structure sequence, alignment form\n        seq_ident_cutoff (float): Percent sequence identity cutoff, in decimal form\n        allow_missing_on_termini (float): Percentage of the total length of the reference sequence which will be ignored\n            when checking for modifications. Example: if 0.1, and reference sequence is 100 AA, then only residues\n            5 to 95 will be checked for modifications.\n        allow_mutants (bool): If mutations should be allowed or checked for\n        allow_deletions (bool): If deletions should be allowed or checked for\n        allow_insertions (bool): If insertions should be allowed or checked for\n        allow_unresolved (bool): If unresolved residues should be allowed or checked for\n\n    Returns:\n        bool: If the structure's sequence meets the quality checks.\n\n    \"\"\"\n    reference_seq_aln = ssbio.protein.sequence.utils.cast_to_str(reference_seq_aln)\n    structure_seq_aln = ssbio.protein.sequence.utils.cast_to_str(structure_seq_aln)\n    results = ssbio.protein.sequence.utils.alignment.pairwise_alignment_stats(reference_seq_aln=reference_seq_aln,\n                                                                              other_seq_aln=structure_seq_aln)\n\n    # Check percent identity cutoff\n    stats_percent_ident = results['percent_identity']\n    log.debug('{}: percent identity'.format(stats_percent_ident))\n    if stats_percent_ident < seq_ident_cutoff:\n        log.debug('Alignment does not meet percent identity cutoff')\n        return False\n    else:\n        log.debug('Alignment meets percent identity cutoff')\n\n    # Get cutoff stuff ready\n    ref_seq_len = len(reference_seq_aln.replace('-', ''))\n    allow_missing_on_termini /= 2\n    # If any differences appear before start, they are ignored\n    start = ref_seq_len - (ref_seq_len * (1 - allow_missing_on_termini))\n    # If any differences appear before end, they are ignored\n    end = ref_seq_len - (ref_seq_len * allow_missing_on_termini)\n\n    no_deletions_in_pdb = False\n    no_insertions_in_pdb = False\n    no_mutants_in_pdb = False\n    no_unresolved_in_pdb = False\n\n    # Check everything\n    if not allow_deletions:\n        # Get indices of the deletions\n        deletions = results['deletions']\n        # If there are no deletions, that's great\n        if len(deletions) == 0:\n            log.debug('No deletion regions')\n            no_deletions_in_pdb = True\n        else:\n            log.debug('{} deletion region(s)'.format(len(deletions)))\n            # If the deletion appears before or after the cutoff, that's also great\n            for deletion in deletions:\n                if deletion[0][1] < start or deletion[0][0] > end:\n                    no_deletions_in_pdb = True\n                    log.debug('Deletion region(s) are not within structure core')\n                else:\n                    no_deletions_in_pdb = False\n                    log.debug('Deletions within structure')\n                    log.debug('{} > {} or {} < {}'.format(deletion[0][1], start, deletion[0][0], end))\n                    break\n    else:\n        no_deletions_in_pdb = True\n\n    if not allow_insertions:\n        # Get indices of the insertions\n        insertions = results['insertions']\n        # If there are no insertions, that's great\n        if len(insertions) == 0:\n            log.debug('No insertion regions')\n            no_insertions_in_pdb = True\n        else:\n            log.debug('{} insertion region(s)'.format(len(insertions)))\n            # If the insertion appears before or after the cutoff, that's also great\n            for insertion in insertions:\n                if insertion[0][1] < start or insertion[0][0] > end:\n                    no_insertions_in_pdb = True\n                    log.debug('Insertion region(s) are not within structure core')\n                else:\n                    no_insertions_in_pdb = False\n                    log.debug('Insertion regions within structure')\n                    break\n    else:\n        no_insertions_in_pdb = True\n\n    if not allow_mutants:\n        # Get indices of the mutants\n        mutations_full = results['mutations']\n        mutations = [x[1] for x in mutations_full]\n        # If there are no mutants, that's great\n        if len(mutations) == 0:\n            log.debug('No point mutations')\n            no_mutants_in_pdb = True\n        else:\n            log.debug('{} point mutation(s)'.format(len(mutations)))\n            # If the mutant appears before or after the cutoff, that's also great\n            for mutation in mutations:\n                if mutation < start or mutation > end:\n                    no_mutants_in_pdb = True\n                    log.debug('Mutation region(s) are not within structure core')\n                else:\n                    no_mutants_in_pdb = False\n                    log.debug('Mutantion regions within structure')\n                    break\n    else:\n        no_mutants_in_pdb = True\n\n    if not allow_unresolved:\n        # Get indices of the unresolved residues\n        unresolved = results['unresolved']\n        # If there are no unresolved, that's great\n        if len(unresolved) == 0:\n            log.debug('No unresolved mutations')\n            no_unresolved_in_pdb = True\n        else:\n            log.debug('{} unresolved residue(s)'.format(len(unresolved)))\n            # If the unresolved residue appears before or after the cutoff, that's also great\n            for unr in unresolved:\n                if unr < start or unr > end:\n                    no_unresolved_in_pdb = True\n                    log.debug('Unresolved region(s) are not within structure core')\n                else:\n                    no_unresolved_in_pdb = False\n                    log.debug('Unresolved residues within structure')\n                    break\n    else:\n        no_unresolved_in_pdb = True\n\n    if no_deletions_in_pdb and no_insertions_in_pdb and no_mutants_in_pdb and no_unresolved_in_pdb:\n        return True\n    else:\n        return False", "response": "Report if a structure s sequence meets coverage checks."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses all PROCHECK files in a directory and returns a Pandas DataFrame of the results Insights", "response": "def parse_procheck(quality_directory):\n    \"\"\"Parses all PROCHECK files in a directory and returns a Pandas DataFrame of the results\n\n    Args:\n        quality_directory: path to directory with PROCHECK output (.sum files)\n\n    Returns:\n        Pandas DataFrame: Summary of PROCHECK results\n\n    \"\"\"\n\n    # TODO: save as dict instead, offer df as option\n    # TODO: parse for one file instead\n\n    procheck_summaries = glob.glob(os.path.join(quality_directory, '*.sum'))\n\n    if len(procheck_summaries) == 0:\n        return pd.DataFrame()\n\n    all_procheck = {}\n    for summ in procheck_summaries:\n        structure_id = os.path.basename(summ).split('.sum')[0]\n        procheck_dict = {}\n        with open(summ) as f_in:\n            lines = (line.rstrip() for line in f_in) # All lines including the blank ones\n            lines = (line for line in lines if line) # Non-blank lines\n            for line in lines:\n                if len(line.split()) > 1:\n                    if line.split()[1] == 'Ramachandran':\n                        procheck_dict['procheck_rama_favored'] = percentage_to_float(line.split()[3])\n                        procheck_dict['procheck_rama_allowed'] = percentage_to_float(line.split()[5])\n                        procheck_dict['procheck_rama_allowed_plus'] = percentage_to_float(line.split()[7])\n                        procheck_dict['procheck_rama_disallowed'] = percentage_to_float(line.split()[9])\n                    if line.split()[1] == 'G-factors':\n                        procheck_dict['procheck_gfac_dihedrals'] = line.split()[3]\n                        procheck_dict['procheck_gfac_covalent'] = line.split()[5]\n                        procheck_dict['procheck_gfac_overall'] = line.split()[7]\n        all_procheck[structure_id] = procheck_dict\n\n    DF_PROCHECK = pd.DataFrame.from_dict(all_procheck, orient='index')\n\n    return DF_PROCHECK"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a PSQS result file and returns a Pandas DataFrame of the results", "response": "def parse_psqs(psqs_results_file):\n    \"\"\"Parse a PSQS result file and returns a Pandas DataFrame of the results\n\n    Args:\n        psqs_results_file: Path to psqs results file\n\n    Returns:\n        Pandas DataFrame: Summary of PSQS results\n\n    \"\"\"\n\n    # TODO: generalize column names for all results, save as dict instead\n\n    psqs_results = pd.read_csv(psqs_results_file, sep='\\t', header=None)\n    psqs_results['pdb_file'] = psqs_results[0].apply(lambda x: str(x).strip('./').strip('.pdb'))\n    psqs_results = psqs_results.rename(columns = {1:'psqs_local', 2:'psqs_burial', 3:'psqs_contact', 4:'psqs_total'}).drop(0, axis=1)\n    psqs_results['u_pdb'] = psqs_results['pdb_file'].apply(lambda x: x.upper() if len(x)==4 else np.nan)\n    psqs_results['i_entry_name'] = psqs_results['pdb_file'].apply(lambda x: x.split('_model1')[0] if len(x)>4 else np.nan)\n    psqs_results = psqs_results[pd.notnull(psqs_results.psqs_total)]\n\n    return psqs_results"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload the entire set of DNA or protein sequences from NCBI and save it to a file.", "response": "def download_genome_sequence(genome_accession_or_id, seqtype, email, outdir='', outfile='', force_rerun=False):\n    \"\"\"Download the entire set of DNA or protein sequences from protein-encoding genes in a genome from NCBI.\n\n    Saves a FASTA file in the optional directory specified.\n\n    Args:\n        genome_accession_or_id (str): RefSeq complete genome ID (e.g. \"NC_000913\") or GenBank ID (e.g. \"U00096.3\")\n        seqtype (str): \"dna\" or \"protein\" - if you want the coding sequences in amino acid or DNA format.\n        email (str): mandatory email so NCBI knows who is accessing the execute_dir\n        outdir (str): optional output directory (default is the current directory)\n\n    Returns:\n        Path to downloaded FASTA file.\n\n    \"\"\"\n    if seqtype == 'dna':\n        extension = 'fna'\n        rettype = \"fasta_cds_na\"\n    elif seqtype == 'protein':\n        extension = 'faa'\n        rettype = \"fasta_cds_aa\"\n    else:\n        raise ValueError('seqtype must be \"dna\" or \"protein\"')\n\n    # path and filename parsing\n    if outfile:\n        outfile = op.join(outdir, '{}.{}'.format(outfile, extension))\n    else:\n        # if no outfile is specified, default is \"$GI.faa\"\n        outfile = op.join(outdir, '{}.{}'.format(genome_accession_or_id, extension))\n\n    if not force_rerun:\n        # return the path to the file if it was already downloaded\n        if op.exists(outfile):\n            log.debug('FASTA file already exists at {}'.format(outfile))\n            return outfile\n\n    Entrez.email = email\n\n    # convert to NCBI GI\n    convert_to_gi = Entrez.read(Entrez.esearch(db=\"nucleotide\", term=genome_accession_or_id, retmode=\"xml\"))\n    gi = convert_to_gi['IdList']\n\n    # download only the CDS protein FASTA\n    # See: https://www.ncbi.nlm.nih.gov/books/NBK25499/table/chapter4.T._valid_values_of__retmode_and/\n    record = Entrez.efetch(db=\"nucleotide\", id=gi, rettype=rettype, retmode=\"fasta\")\n\n    # save the FASTA file\n    with open(outfile, 'wt') as f:\n        f.write(record.read())\n\n    log.info('Saved FASTA file at {}'.format(outfile))\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sequence_dir(self):\n        if self.root_dir:\n            return op.join(self.protein_dir, 'sequences')\n        else:\n            log.debug('Root directory not set')\n            return None", "response": "str - Path where sequence related files are stored"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a dictionary of basic statistics describing this protein", "response": "def protein_statistics(self):\n        \"\"\"Get a dictionary of basic statistics describing this protein\"\"\"\n\n        # TODO: can i use get_dict here instead\n        d = {}\n\n        d['id'] = self.id\n        d['sequences'] = [x.id for x in self.sequences]\n        d['num_sequences'] = self.num_sequences\n        if self.representative_sequence:\n            d['representative_sequence'] = self.representative_sequence.id\n            d['repseq_gene_name'] = self.representative_sequence.gene_name\n            d['repseq_uniprot'] = self.representative_sequence.uniprot\n            d['repseq_description'] = self.representative_sequence.description\n        d['num_structures'] = self.num_structures\n        d['experimental_structures'] = [x.id for x in self.get_experimental_structures()]\n        d['num_experimental_structures'] = self.num_structures_experimental\n        d['homology_models'] = [x.id for x in self.get_homology_models()]\n        d['num_homology_models'] = self.num_structures_homology\n        if self.representative_structure:\n            d['representative_structure'] = self.representative_structure.id\n            d['representative_chain'] = self.representative_chain\n            d['representative_chain_seq_coverage'] = self.representative_chain_seq_coverage\n            d['repstruct_description'] = self.description\n            if self.representative_structure.is_experimental:\n                d['repstruct_resolution'] = self.representative_structure.resolution\n        d['num_sequence_alignments'] = len(self.sequence_alignments)\n        d['num_structure_alignments'] = len(self.structure_alignments)\n\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a DictList of all experimental structures in self. structures", "response": "def get_experimental_structures(self):\n        \"\"\"DictList: Return a DictList of all experimental structures in self.structures\"\"\"\n        # TODO: change to a property?\n        if self.representative_structure:\n            return DictList(x for x in self.structures if x.is_experimental and x.id != self.representative_structure.id)\n        else:\n            return DictList(x for x in self.structures if x.is_experimental)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a DictList of all homology models in self. structures", "response": "def get_homology_models(self):\n        \"\"\"DictList: Return a DictList of all homology models in self.structures\"\"\"\n        # TODO: change to a property?\n        if self.representative_structure:\n            return DictList(x for x in self.structures if not x.is_experimental and x.id != self.representative_structure.id)\n        else:\n            return DictList(x for x in self.structures if not x.is_experimental)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a DictList of only specified types in the sequences attribute.", "response": "def filter_sequences(self, seq_type):\n        \"\"\"Return a DictList of only specified types in the sequences attribute.\n\n        Args:\n            seq_type (SeqProp): Object type\n\n        Returns:\n            DictList: A filtered DictList of specified object type only\n\n        \"\"\"\n        return DictList(x for x in self.sequences if isinstance(x, seq_type))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a KEGG ID sequence and metadata file into the sequences attribute.", "response": "def load_kegg(self, kegg_id, kegg_organism_code=None, kegg_seq_file=None, kegg_metadata_file=None,\n                  set_as_representative=False, download=False, outdir=None, force_rerun=False):\n        \"\"\"Load a KEGG ID, sequence, and metadata files into the sequences attribute.\n\n        Args:\n            kegg_id (str): KEGG ID\n            kegg_organism_code (str): KEGG organism code to prepend to the kegg_id if not part of it already.\n                Example: ``eco:b1244``, ``eco`` is the organism code\n            kegg_seq_file (str): Path to KEGG FASTA file\n            kegg_metadata_file (str): Path to KEGG metadata file (raw KEGG format)\n            set_as_representative (bool): If this KEGG ID should be set as the representative sequence\n            download (bool): If the KEGG sequence and metadata files should be downloaded if not provided\n            outdir (str): Where the sequence and metadata files should be downloaded to\n            force_rerun (bool): If ID should be reloaded and files redownloaded\n\n        Returns:\n            KEGGProp: object contained in the sequences attribute\n\n        \"\"\"\n        if download:\n            if not outdir:\n                outdir = self.sequence_dir\n                if not outdir:\n                    raise ValueError('Output directory must be specified')\n\n        if kegg_organism_code:\n            kegg_id = kegg_organism_code + ':' + kegg_id\n\n        # If we have already loaded the KEGG ID\n        if self.sequences.has_id(kegg_id):\n            # Remove it if we want to force rerun things\n            if force_rerun:\n                existing = self.sequences.get_by_id(kegg_id)\n                self.sequences.remove(existing)\n            # Otherwise just get that KEGG object\n            else:\n                log.debug('{}: KEGG ID already present in list of sequences'.format(kegg_id))\n                kegg_prop = self.sequences.get_by_id(kegg_id)\n\n        # Check again (instead of else) in case we removed it if force rerun\n        if not self.sequences.has_id(kegg_id):\n            kegg_prop = KEGGProp(id=kegg_id, seq=None, fasta_path=kegg_seq_file, txt_path=kegg_metadata_file)\n            if download:\n                kegg_prop.download_seq_file(outdir, force_rerun)\n                kegg_prop.download_metadata_file(outdir, force_rerun)\n\n            # Check if KEGG sequence matches a potentially set representative sequence\n            # Do not add any info if a UniProt ID was already mapped though, we want to use that\n            if self.representative_sequence:\n                if not self.representative_sequence.uniprot:\n                    if kegg_prop.equal_to(self.representative_sequence):\n                        # Update the representative sequence field with KEGG metadata\n                        self.representative_sequence.update(kegg_prop.get_dict(), only_keys=['sequence_path',\n                                                                                             'metadata_path',\n                                                                                             'kegg',\n                                                                                             'description',\n                                                                                             'taxonomy',\n                                                                                             'id',\n                                                                                             'pdbs',\n                                                                                             'uniprot',\n                                                                                             'seq_record',\n                                                                                             'gene_name',\n                                                                                             'refseq'])\n                    else:\n                        # TODO: add option to use manual or kegg sequence if things do not match\n                        log.warning('{}: representative sequence does not match mapped KEGG sequence.'.format(self.id))\n\n            self.sequences.append(kegg_prop)\n\n        if set_as_representative:\n            self.representative_sequence = kegg_prop\n\n        return self.sequences.get_by_id(kegg_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a UniProt ID and associated sequence and metadata files into the sequences attribute.", "response": "def load_uniprot(self, uniprot_id, uniprot_seq_file=None, uniprot_xml_file=None, download=False, outdir=None,\n                     set_as_representative=False, force_rerun=False):\n        \"\"\"Load a UniProt ID and associated sequence/metadata files into the sequences attribute.\n\n        Sequence and metadata files can be provided, or alternatively downloaded with the download flag set to True.\n        Metadata files will be downloaded as XML files.\n\n        Args:\n            uniprot_id (str): UniProt ID/ACC\n            uniprot_seq_file (str): Path to FASTA file\n            uniprot_xml_file (str): Path to UniProt XML file\n            download (bool): If sequence and metadata files should be downloaded\n            outdir (str): Output directory for sequence and metadata files\n            set_as_representative (bool): If this sequence should be set as the representative one\n            force_rerun (bool): If files should be redownloaded and metadata reloaded\n\n        Returns:\n            UniProtProp: Sequence that was loaded into the ``sequences`` attribute\n\n        \"\"\"\n        if download:\n            if not outdir:\n                outdir = self.sequence_dir\n                if not outdir:\n                    raise ValueError('Output directory must be specified')\n\n        # If we have already loaded the UniProt ID\n        if self.sequences.has_id(uniprot_id):\n            # Remove it if we want to force rerun things\n            if force_rerun:\n                existing = self.sequences.get_by_id(uniprot_id)\n                self.sequences.remove(existing)\n            # Otherwise just get that UniProt object\n            else:\n                log.debug('{}: UniProt ID already present in list of sequences'.format(uniprot_id))\n                uniprot_prop = self.sequences.get_by_id(uniprot_id)\n\n        if not self.sequences.has_id(uniprot_id):\n            uniprot_prop = UniProtProp(id=uniprot_id,\n                                       seq=None,\n                                       fasta_path=uniprot_seq_file,\n                                       xml_path=uniprot_xml_file)\n            if download:\n                uniprot_prop.download_metadata_file(outdir=outdir, force_rerun=force_rerun)\n                uniprot_prop.download_seq_file(outdir=outdir, force_rerun=force_rerun)\n\n            # Also check if UniProt sequence matches a potentially set representative sequence\n            if self.representative_sequence:\n                # Test equality\n                if uniprot_prop.equal_to(self.representative_sequence):\n                    # Update the representative sequence field with UniProt metadata if it is the same\n                    self.representative_sequence.update(uniprot_prop.get_dict(), only_keys=['sequence_path',\n                                                                                            'metadata_path',\n                                                                                            'kegg',\n                                                                                            'description',\n                                                                                            'taxonomy',\n                                                                                            'id',\n                                                                                            'pdbs',\n                                                                                            'uniprot',\n                                                                                            'seq_record',\n                                                                                            'gene_name',\n                                                                                            'refseq'])\n                else:\n                    # TODO: add option to use manual or uniprot sequence if things do not match\n                    log.warning('{}: representative sequence does not match mapped UniProt sequence'.format(self.id))\n            self.sequences.append(uniprot_prop)\n\n        if set_as_representative:\n            self.representative_sequence = uniprot_prop\n\n        return self.sequences.get_by_id(uniprot_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_manual_sequence_file(self, ident, seq_file, copy_file=False, outdir=None, set_as_representative=False):\n        if copy_file:\n            if not outdir:\n                outdir = self.sequence_dir\n                if not outdir:\n                    raise ValueError('Output directory must be specified')\n            shutil.copy(seq_file, outdir)\n            seq_file = op.join(outdir, seq_file)\n\n        manual_sequence = SeqProp(id=ident, sequence_path=seq_file, seq=None)\n        self.sequences.append(manual_sequence)\n\n        if set_as_representative:\n            self.representative_sequence = manual_sequence\n\n        return self.sequences.get_by_id(ident)", "response": "Loads a manual sequence file and stores it in the sequences attribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_manual_sequence(self, seq, ident=None, write_fasta_file=False, outdir=None,\n                             set_as_representative=False, force_rewrite=False):\n        \"\"\"Load a manual sequence given as a string and optionally set it as the representative sequence.\n        Also store it in the sequences attribute.\n\n        Args:\n            seq (str, Seq, SeqRecord): Sequence string, Biopython Seq or SeqRecord object\n            ident (str): Optional identifier for the sequence, required if seq is a string. Also will override existing\n                IDs in Seq or SeqRecord objects if set.\n            write_fasta_file (bool): If this sequence should be written out to a FASTA file\n            outdir (str): Path to output directory\n            set_as_representative (bool): If this sequence should be set as the representative one\n            force_rewrite (bool): If the FASTA file should be overwritten if it already exists\n\n        Returns:\n            SeqProp: Sequence that was loaded into the ``sequences`` attribute\n\n        \"\"\"\n        if write_fasta_file:\n            if not outdir:\n                outdir = self.sequence_dir\n                if not outdir:\n                    raise ValueError('Output directory must be specified')\n            outfile = op.join(outdir, '{}.faa'.format(ident))\n        else:\n            outfile = None\n\n        if isinstance(seq, str) or isinstance(seq, Seq):\n            if not ident:\n                raise ValueError('ID must be specified if sequence is a string or Seq object')\n            manual_sequence = SeqProp(id=ident, seq=seq)\n        else:\n            if not ident:\n                # Use ID from SeqRecord ID if new ID not provided\n                ident = seq.id\n            else:\n                # Overwrite SeqRecord ID with new ID if provided\n                seq.id = ident\n            manual_sequence = SeqProp(id=ident, seq=seq, name=seq.name, description=seq.description)\n\n        if write_fasta_file:\n            manual_sequence.write_fasta_file(outfile=outfile, force_rerun=force_rewrite)\n        self.sequences.append(manual_sequence)\n\n        if set_as_representative:\n            self.representative_sequence = manual_sequence\n\n        return self.sequences.get_by_id(ident)", "response": "Loads a manual sequence into the sequences attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pairwise_align_sequences_to_representative(self, gapopen=10, gapextend=0.5, outdir=None,\n                                                   engine='needle', parse=True, force_rerun=False):\n        \"\"\"Pairwise all sequences in the sequences attribute to the representative sequence. Stores the alignments\n        in the ``sequence_alignments`` DictList attribute.\n\n        Args:\n            gapopen (int): Only for ``engine='needle'`` - Gap open penalty is the score taken away when a gap is created\n            gapextend (float): Only for ``engine='needle'`` - Gap extension penalty is added to the standard gap penalty\n                for each base or residue in the gap\n            outdir (str): Only for ``engine='needle'`` - Path to output directory. Default is the protein sequence\n                directory.\n            engine (str): ``biopython`` or ``needle`` - which pairwise alignment program to use.\n                ``needle`` is the standard EMBOSS tool to run pairwise alignments.\n                ``biopython`` is Biopython's implementation of needle. Results can differ!\n            parse (bool): Store locations of mutations, insertions, and deletions in the alignment object (as an\n                annotation)\n            force_rerun (bool): Only for ``engine='needle'`` - Default False, set to True if you want to rerun the\n                alignment if outfile exists.\n\n        \"\"\"\n\n        if not self.representative_sequence:\n            raise ValueError('{}: no representative sequence set'.format(self.id))\n\n        if not outdir:\n            outdir = self.sequence_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        for seq in self.sequences:\n            aln_id = '{}_{}'.format(self.id, seq.id)\n            outfile = '{}.needle'.format(aln_id)\n\n            if self.sequence_alignments.has_id(aln_id):\n                log.debug('{}: alignment already completed'.format(seq.id))\n                continue\n\n            if not seq.seq_str:\n                log.error('{}: no sequence stored, skipping alignment'.format(seq.id))\n                continue\n\n            # Don't need to compare sequence to itself\n            if seq.id == self.representative_sequence.id:\n                continue\n\n            aln = ssbio.protein.sequence.utils.alignment.pairwise_sequence_alignment(a_seq=self.representative_sequence.seq_str,\n                                                                                     a_seq_id=self.id,\n                                                                                     b_seq=seq.seq_str,\n                                                                                     b_seq_id=seq.id,\n                                                                                     gapopen=gapopen, gapextend=gapextend,\n                                                                                     engine=engine,\n                                                                                     outdir=outdir,\n                                                                                     outfile=outfile,\n                                                                                     force_rerun=force_rerun)\n            # Add an identifier to the MultipleSeqAlignment object for storage in a DictList\n            aln.id = aln_id\n            aln.annotations['a_seq'] = self.representative_sequence.id\n            aln.annotations['b_seq'] = seq.id\n\n            if parse:\n                aln_df = ssbio.protein.sequence.utils.alignment.get_alignment_df(a_aln_seq=str(list(aln)[0].seq),\n                                                                                 b_aln_seq=str(list(aln)[1].seq))\n                aln.annotations['ssbio_type'] = 'seqalign'\n                aln.annotations['mutations'] = ssbio.protein.sequence.utils.alignment.get_mutations(aln_df)\n                aln.annotations['deletions'] = ssbio.protein.sequence.utils.alignment.get_deletions(aln_df)\n                aln.annotations['insertions'] = ssbio.protein.sequence.utils.alignment.get_insertions(aln_df)\n\n            self.sequence_alignments.append(aln)", "response": "Pairwise all sequences in the sequences attribute to the representative sequence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pairwise_align_sequences_to_representative_parallelize(self, sc, gapopen=10, gapextend=0.5, outdir=None,\n                                                      engine='needle', parse=True, force_rerun=False):\n        \"\"\"Pairwise all sequences in the sequences attribute to the representative sequence. Stores the alignments\n        in the ``sequence_alignments`` DictList attribute.\n\n        Args:\n            sc (SparkContext): Configured spark context for parallelization\n            gapopen (int): Only for ``engine='needle'`` - Gap open penalty is the score taken away when a gap is created\n            gapextend (float): Only for ``engine='needle'`` - Gap extension penalty is added to the standard gap penalty\n                for each base or residue in the gap\n            outdir (str): Only for ``engine='needle'`` - Path to output directory. Default is the protein sequence\n                directory.\n            engine (str): ``biopython`` or ``needle`` - which pairwise alignment program to use.\n                ``needle`` is the standard EMBOSS tool to run pairwise alignments.\n                ``biopython`` is Biopython's implementation of needle. Results can differ!\n            parse (bool): Store locations of mutations, insertions, and deletions in the alignment object (as an\n                annotation)\n            force_rerun (bool): Only for ``engine='needle'`` - Default False, set to True if you want to rerun the\n                alignment if outfile exists.\n\n        \"\"\"\n\n        if not self.representative_sequence:\n            raise ValueError('{}: no representative sequence set'.format(self.id))\n\n        if not outdir:\n            outdir = self.sequence_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        def pairwise_sc(self, seqprop):\n            aln_id = '{}_{}'.format(self.id, seqprop.id)\n            outfile = '{}.needle'.format(aln_id)\n\n            aln = ssbio.protein.sequence.utils.alignment.pairwise_sequence_alignment(a_seq=self.representative_sequence.seq_str,\n                                                                                     a_seq_id=self.id,\n                                                                                     b_seq=seqprop.seq_str,\n                                                                                     b_seq_id=seqprop.id,\n                                                                                     gapopen=gapopen,\n                                                                                     gapextend=gapextend,\n                                                                                     engine=engine,\n                                                                                     outdir=outdir,\n                                                                                     outfile=outfile,\n                                                                                     force_rerun=force_rerun)\n            aln.id = aln_id\n            aln.annotations['a_seq'] = self.representative_sequence.id\n            aln.annotations['b_seq'] = seqprop.id\n\n            if parse:\n                aln_df = ssbio.protein.sequence.utils.alignment.get_alignment_df(a_aln_seq=str(list(aln)[0].seq),\n                                                                                 b_aln_seq=str(list(aln)[1].seq))\n                aln.annotations['ssbio_type'] = 'seqalign'\n                aln.annotations['mutations'] = ssbio.protein.sequence.utils.alignment.get_mutations(aln_df)\n                aln.annotations['deletions'] = ssbio.protein.sequence.utils.alignment.get_deletions(aln_df)\n                aln.annotations['insertions'] = ssbio.protein.sequence.utils.alignment.get_insertions(aln_df)\n\n            return aln\n\n        sequences_rdd = sc.parallelize(filter(lambda x: x.id != self.representative_sequence.id, self.sequences))\n        result = sequences_rdd.map(lambda x: pairwise_sc(self, x)).collect()\n\n        for r in result:\n            self.sequence_alignments.append(r)", "response": "Pairwise all sequences in the sequences attribute to the representative sequence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_all_sequences_file(self, outname, outdir=None):\n\n        if not outdir:\n            outdir = self.sequence_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        outfile = op.join(outdir, outname + '.faa')\n        SeqIO.write(self.sequences, outfile, \"fasta\")\n\n        log.info('{}: wrote all protein sequences to file'.format(outfile))\n        return outfile", "response": "Write all the stored sequences as a single FASTA file. By default sets IDs to model gene IDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sequence_properties(self, clean_seq=False, representative_only=True):\n        if representative_only:\n            # Check if a representative sequence was set\n            if not self.representative_sequence:\n                log.warning('{}: no representative sequence set, cannot get sequence properties'.format(self.id))\n                return\n\n            # Also need to check if a sequence has been stored\n            if not self.representative_sequence.seq:\n                log.warning('{}: representative sequence {} set, but no sequence stored. '\n                            'Cannot get sequence properties.'.format(self.id, self.representative_sequence.id))\n                return\n\n            self.representative_sequence.get_biopython_pepstats(clean_seq=clean_seq)\n            self.representative_sequence.get_emboss_pepstats()\n\n        if not representative_only:\n            for s in self.sequences:\n                # Need to check if a sequence has been stored\n                if not s.seq:\n                    log.warning('{}: no sequence stored. '\n                                'Cannot get sequence properties.'.format(s.id))\n                    continue\n\n                else:\n                    s.get_biopython_pepstats(clean_seq=clean_seq)\n                    s.get_emboss_pepstats()", "response": "Run Biopython ProteinAnalysis and EMBOSS pepstats to summarize basic statistics of the protein sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sequence_sliding_window_properties(self, scale, window, representative_only=True):\n        if representative_only:\n            # Check if a representative sequence was set\n            if not self.representative_sequence:\n                log.warning('{}: no representative sequence set, cannot get sequence properties'.format(self.id))\n                return\n\n            # Also need to check if a sequence has been stored\n            if not self.representative_sequence.seq:\n                log.warning('{}: representative sequence {} set, but no sequence stored. '\n                            'Cannot get sequence properties.'.format(self.id, self.representative_sequence.id))\n                return\n\n            self.representative_sequence.get_sliding_window_properties(scale=scale, window=window)\n\n        if not representative_only:\n            for s in self.sequences:\n                # Need to check if a sequence has been stored\n                if not s.seq:\n                    log.warning('{}: no sequence stored. '\n                                'Cannot get sequence properties.'.format(s.id))\n                    continue\n\n                else:\n                    s.get_sliding_window_properties(scale=scale, window=window)", "response": "Run Biopython ProteinAnalysis with a sliding window to calculate a given property."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prep_itasser_modeling(self, itasser_installation, itlib_folder, runtype, create_in_dir=None,\n                              execute_from_dir=None, print_exec=False, **kwargs):\n        \"\"\"Prepare to run I-TASSER homology modeling for the representative sequence.\n\n        Args:\n            itasser_installation (str): Path to I-TASSER folder, i.e. ``~/software/I-TASSER4.4``\n            itlib_folder (str): Path to ITLIB folder, i.e. ``~/software/ITLIB``\n            runtype: How you will be running I-TASSER - local, slurm, or torque\n            create_in_dir (str): Local directory where folders will be created\n            execute_from_dir (str): Optional path to execution directory - use this if you are copying the homology\n                models to another location such as a supercomputer for running\n            all_genes (bool): If all genes should be prepped, or only those without any mapped structures\n            print_exec (bool): If the execution statement should be printed to run modelling\n\n        Todo:\n            * Document kwargs - extra options for I-TASSER, SLURM or Torque execution\n            * Allow modeling of any sequence in sequences attribute, select by ID or provide SeqProp?\n\n        \"\"\"\n\n        if not create_in_dir:\n            if not self.structure_dir:\n                raise ValueError('Output directory must be specified')\n            self.homology_models_dir = op.join(self.structure_dir, 'homology_models')\n        else:\n            self.homology_models_dir = create_in_dir\n\n        ssbio.utils.make_dir(self.homology_models_dir)\n\n        if not execute_from_dir:\n            execute_from_dir = self.homology_models_dir\n\n        repseq = self.representative_sequence\n\n        itasser_kwargs = {'light': True,\n                          'java_home': None,\n                          'binding_site_pred': False,\n                          'ec_pred': False,\n                          'go_pred': False,\n                          'job_scheduler_header': None,\n                          'additional_options': None}\n\n        if kwargs:\n            itasser_kwargs.update(kwargs)\n\n        ITASSERPrep(ident=self.id, seq_str=repseq.seq_str, root_dir=self.homology_models_dir,\n                    itasser_path=itasser_installation, itlib_path=itlib_folder,\n                    runtype=runtype, print_exec=print_exec, execute_dir=execute_from_dir,\n                    java_home=itasser_kwargs['java_home'],\n                    light=itasser_kwargs['light'],\n                    binding_site_pred=itasser_kwargs['binding_site_pred'],\n                    ec_pred=itasser_kwargs['ec_pred'],\n                    go_pred=itasser_kwargs['go_pred'],\n                    job_scheduler_header=itasser_kwargs['job_scheduler_header'],\n                    additional_options=itasser_kwargs['additional_options'])\n\n        log.debug('Prepared I-TASSER modeling folder {}'.format(self.homology_models_dir))", "response": "Prepare I - TASSER homology modeling for the representative sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blast_representative_sequence_to_pdb(self, seq_ident_cutoff=0, evalue=0.0001, display_link=False,\n                                             outdir=None, force_rerun=False):\n        \"\"\"BLAST the representative protein sequence to the PDB. Saves a raw BLAST result file (XML file).\n\n        Args:\n            seq_ident_cutoff (float, optional): Cutoff results based on percent coverage (in decimal form)\n            evalue (float, optional): Cutoff for the E-value - filters for significant hits. 0.001 is liberal,\n                0.0001 is stringent (default).\n            display_link (bool, optional): Set to True if links to the HTML results should be displayed\n            outdir (str): Path to output directory of downloaded XML files, must be set if protein directory\n                was not initialized\n            force_rerun (bool, optional): If existing BLAST results should not be used, set to True. Default is False.\n\n        Returns:\n            list: List of new ``PDBProp`` objects added to the ``structures`` attribute\n\n        \"\"\"\n        # Check if a representative sequence was set\n        if not self.representative_sequence:\n            log.warning('{}: no representative sequence set, cannot BLAST'.format(self.id))\n            return None\n\n        # Also need to check if a sequence has been stored\n        if not self.representative_sequence.seq:\n            log.warning('{}: no representative sequence loaded, cannot BLAST'.format(self.id))\n            return None\n\n        # BLAST the sequence to the PDB\n        blast_results = self.representative_sequence.blast_pdb(seq_ident_cutoff=seq_ident_cutoff,\n                                                               evalue=evalue,\n                                                               display_link=display_link,\n                                                               outdir=outdir,\n                                                               force_rerun=force_rerun)\n\n        new_pdbs = []\n\n        # Add BLAST results to the list of structures\n        if blast_results:\n\n            # Filter for new BLASTed PDBs\n            pdbs = [x['hit_pdb'].lower() for x in blast_results]\n            new_pdbs = [y for y in pdbs if not self.structures.has_id(y)]\n            if new_pdbs:\n                log.debug('{}: adding {} PDBs from BLAST results'.format(self.id, len(new_pdbs)))\n            else:\n                already_have = [y for y in pdbs if self.structures.has_id(y)]\n                log.debug('{}: PDBs already contained in structures list'.format(';'.join(already_have)))\n            blast_results = [z for z in blast_results if z['hit_pdb'].lower() in new_pdbs]\n\n            for blast_result in blast_results:\n                pdb = blast_result['hit_pdb'].lower()\n                chains = blast_result['hit_pdb_chains']\n\n                for chain in chains:\n                    # load_pdb will append this protein to the list of structures\n                    new_pdb = self.load_pdb(pdb_id=pdb, mapped_chains=chain)\n                    new_pdb.add_chain_ids(chain)\n                    new_chain = new_pdb.chains.get_by_id(chain)\n\n                    # Store BLAST results within the chain\n                    new_chain.blast_results = blast_result\n\n        return new_pdbs", "response": "BLAST the representative sequence to the PDB."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a dataframe of PDB BLAST results", "response": "def df_pdb_blast(self):\n        \"\"\"DataFrame: Get a dataframe of PDB BLAST results\"\"\"\n        blast_results_pre_df = []\n\n        for p in self.get_experimental_structures():\n            for c in p.chains:\n                if hasattr(c, 'blast_results'):\n                    # Summary dataframe\n                    infodict = p.get_dict_with_chain(chain=c.id)['blast_results']\n                    infodict['pdb_id'] = p.id\n                    infodict['pdb_chain_id'] = c.id\n                    blast_results_pre_df.append(infodict)\n\n        cols = ['pdb_id', 'pdb_chain_id', 'hit_score', 'hit_evalue', 'hit_percent_similar',\n                'hit_percent_ident', 'hit_percent_gaps', 'hit_num_ident', 'hit_num_similar', 'hit_num_gaps']\n        df = pd.DataFrame.from_records(blast_results_pre_df, columns=cols).set_index('pdb_id')\n        return ssbio.utils.clean_df(df)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap the UniProt ID to a PDB ID using the PDBe Best Structures API.", "response": "def map_uniprot_to_pdb(self, seq_ident_cutoff=0.0, outdir=None, force_rerun=False):\n        \"\"\"Map the representative sequence's UniProt ID to PDB IDs using the PDBe \"Best Structures\" API.\n        Will save a JSON file of the results to the protein sequences folder.\n\n        The \"Best structures\" API is available at https://www.ebi.ac.uk/pdbe/api/doc/sifts.html\n        The list of PDB structures mapping to a UniProt accession sorted by coverage of the protein and,\n        if the same, resolution.\n\n        Args:\n            seq_ident_cutoff (float): Sequence identity cutoff in decimal form\n            outdir (str): Output directory to cache JSON results of search\n            force_rerun (bool): Force re-downloading of JSON results if they already exist\n\n        Returns:\n            list: A rank-ordered list of PDBProp objects that map to the UniProt ID\n\n        \"\"\"\n        if not self.representative_sequence:\n            log.error('{}: no representative sequence set, cannot use best structures API'.format(self.id))\n            return None\n\n        # Check if a UniProt ID is attached to the representative sequence\n        uniprot_id = self.representative_sequence.uniprot\n        if not uniprot_id:\n            log.error('{}: no representative UniProt ID set, cannot use best structures API'.format(self.id))\n            return None\n\n        if '-' in uniprot_id:\n            log.debug('{}: \"-\" detected in UniProt ID, isoform specific sequences are ignored with best structures API'.format(self.id))\n            uniprot_id = uniprot_id.split('-')[0]\n\n        if not outdir:\n            outdir = self.sequence_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        best_structures = ssbio.databases.pdb.best_structures(uniprot_id,\n                                                              outname='{}_best_structures'.format(custom_slugify(uniprot_id)),\n                                                              outdir=outdir,\n                                                              seq_ident_cutoff=seq_ident_cutoff,\n                                                              force_rerun=force_rerun)\n\n        new_pdbs = []\n        if best_structures:\n            rank = 1\n            for best_structure in best_structures:\n                currpdb = str(best_structure['pdb_id'].lower())\n                new_pdbs.append(currpdb)\n                currchain = str(best_structure['chain_id'])\n\n                # load_pdb will append this protein to the list\n                new_pdb = self.load_pdb(pdb_id=currpdb, mapped_chains=currchain)\n\n                # Also add this chain to the chains attribute so we can save the\n                # info we get from best_structures\n                new_pdb.add_chain_ids(currchain)\n\n                pdb_specific_keys = ['experimental_method', 'resolution']\n                chain_specific_keys = ['coverage', 'start', 'end', 'unp_start', 'unp_end']\n\n                new_pdb.update(best_structure, only_keys=pdb_specific_keys)\n                new_chain = new_pdb.chains.get_by_id(currchain)\n                new_chain.update(best_structure, only_keys=chain_specific_keys)\n                new_chain.update({'rank': rank})\n\n                rank += 1\n\n            log.debug('{}, {}: {} PDB/chain pairs mapped'.format(self.id, uniprot_id, len(best_structures)))\n        else:\n            log.debug('{}, {}: no PDB/chain pairs mapped'.format(self.id, uniprot_id))\n\n        return new_pdbs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a dataframe of UniProt - > best structure in PDB results", "response": "def df_pdb_ranking(self):\n        \"\"\"DataFrame: Get a dataframe of UniProt -> best structure in PDB results\"\"\"\n\n        best_structures_pre_df = []\n\n        chain_specific_keys = ['coverage', 'start', 'end', 'unp_start', 'unp_end', 'rank']\n\n        for p in self.get_experimental_structures():\n            for c in p.chains:\n                if hasattr(c, 'rank'):\n                    # Summary dataframe\n                    infodict = p.get_dict_with_chain(chain=c.id, df_format=True, chain_keys=chain_specific_keys)\n                    infodict['pdb_id'] = p.id\n                    infodict['pdb_chain_id'] = c.id\n                    infodict['uniprot'] = self.representative_sequence.uniprot\n                    best_structures_pre_df.append(infodict)\n\n        cols = ['uniprot', 'pdb_id', 'pdb_chain_id', 'experimental_method', 'resolution', 'coverage',\n                'taxonomy_name', 'start', 'end', 'unp_start', 'unp_end', 'rank']\n        df = pd.DataFrame.from_records(best_structures_pre_df, columns=cols).set_index(['pdb_id', 'pdb_chain_id'])\n        return ssbio.utils.clean_df(df)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a PDB ID and optional structure file into the structures attribute.", "response": "def load_pdb(self, pdb_id, mapped_chains=None, pdb_file=None, file_type=None, is_experimental=True,\n                 set_as_representative=False, representative_chain=None, force_rerun=False):\n        \"\"\"Load a structure ID and optional structure file into the structures attribute.\n\n        Args:\n            pdb_id (str): PDB ID\n            mapped_chains (str, list): Chain ID or list of IDs which you are interested in\n            pdb_file (str): Path to PDB file\n            file_type (str): Type of PDB file\n            is_experimental (bool): If this structure file is experimental\n            set_as_representative (bool): If this structure should be set as the representative structure\n            representative_chain (str): If ``set_as_representative`` is ``True``, provide the representative chain ID\n            force_rerun (bool): If the PDB should be reloaded if it is already in the list of structures\n\n        Returns:\n            PDBProp: The object that is now contained in the structures attribute\n\n        \"\"\"\n\n        if self.structures.has_id(pdb_id):\n            # Remove the structure if set to force rerun\n            if force_rerun:\n                existing = self.structures.get_by_id(pdb_id)\n                self.structures.remove(existing)\n            # Otherwise just retrieve it\n            else:\n                log.debug('{}: PDB ID already present in list of structures'.format(pdb_id))\n                pdb = self.structures.get_by_id(pdb_id)\n                if pdb_file:\n                    pdb.load_structure_path(pdb_file, file_type)\n                if mapped_chains:\n                    pdb.add_mapped_chain_ids(mapped_chains)\n\n        # Create a new StructProp entry\n        if not self.structures.has_id(pdb_id):\n            if is_experimental:\n                pdb = PDBProp(ident=pdb_id, mapped_chains=mapped_chains, structure_path=pdb_file, file_type=file_type)\n            else:\n                pdb = StructProp(ident=pdb_id, mapped_chains=mapped_chains, structure_path=pdb_file, file_type=file_type)\n            self.structures.append(pdb)\n\n        if set_as_representative:\n            # Parse structure so chains are stored before setting representative\n            pdb.parse_structure()\n            self._representative_structure_setter(structprop=pdb, keep_chain=representative_chain, force_rerun=force_rerun)\n\n        return self.structures.get_by_id(pdb_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the results folder from an I - TASSER run and copy relevant files over to the structure_dir attribute.", "response": "def load_itasser_folder(self, ident, itasser_folder, organize=False, outdir=None, organize_name=None,\n                            set_as_representative=False, representative_chain='X', force_rerun=False):\n        \"\"\"Load the results folder from an I-TASSER run (local, not from the website) and copy relevant files over to\n        the protein structures directory.\n\n        Args:\n            ident (str): I-TASSER ID\n            itasser_folder (str): Path to results folder\n            organize (bool): If select files from modeling should be copied to the Protein directory\n            outdir (str): Path to directory where files will be copied and organized to\n            organize_name (str): Basename of files to rename results to. If not provided, will use id attribute.\n            set_as_representative: If this structure should be set as the representative structure\n            representative_chain (str): If ``set_as_representative`` is ``True``, provide the representative chain ID\n            force_rerun (bool): If the PDB should be reloaded if it is already in the list of structures\n\n        Returns:\n            ITASSERProp: The object that is now contained in the structures attribute\n\n        \"\"\"\n        if organize:\n            if not outdir:\n                outdir = self.structure_dir\n                if not outdir:\n                    raise ValueError('Directory to copy results to must be specified')\n\n        if self.structures.has_id(ident):\n            if force_rerun:\n                existing = self.structures.get_by_id(ident)\n                self.structures.remove(existing)\n            else:\n                log.debug('{}: already present in list of structures'.format(ident))\n                itasser = self.structures.get_by_id(ident)\n\n        if not self.structures.has_id(ident):\n            itasser = ITASSERProp(ident, itasser_folder)\n            self.structures.append(itasser)\n\n        if set_as_representative:\n            self._representative_structure_setter(structprop=itasser, keep_chain=representative_chain, force_rerun=force_rerun)\n\n        if organize:\n            if itasser.structure_file:\n                # The name of the actual pdb file will be $GENEID_model1.pdb\n                if not organize_name:\n                    new_itasser_name = self.id + '_model1'\n                else:\n                    new_itasser_name = organize_name\n\n                # Additional results will be stored in a subdirectory\n                dest_itasser_extra_dir = op.join(outdir, '{}_itasser'.format(new_itasser_name))\n                ssbio.utils.make_dir(dest_itasser_extra_dir)\n\n                # Copy the model1.pdb and also create summary dataframes\n                itasser.copy_results(copy_to_dir=outdir, rename_model_to=new_itasser_name, force_rerun=force_rerun)\n\n        return self.structures.get_by_id(ident)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a dataframe of I - TASSER homology models for a given set of attributes.", "response": "def df_homology_models(self):\n        \"\"\"DataFrame: Get a dataframe of I-TASSER homology model results\"\"\"\n\n        # TODO: add definitions of column names\n        # model_date\tDate the model was created\n        # difficulty\tDifficulty level of the modeling run (easy, medium, hard)\n        # top_template_pdb\tTop template used to model the protein (PDB ID)\n        # top_template_chain\tChain of the top template used to model the protein\n        # c_score\tConfidence score of the homology model from [-5,2]\n        # tm_score\tStructural similarity score to best template\n        # tm_score_err\tStandard error of TM score\n        # rmsd\tRMSD to best template\n        # rmsd_err\tStandard error of RMSD\n        # top_bsite_site_num\tCluster which contained the consensus binding site\n        # top_bsite_c_score\tConfidence score of the consensus binding site prediction from [0,1]\n        # top_bsite_cluster_size\tNumber of predictions within this cluster\n        # top_bsite_binding_residues\tResidue numbers of top ranked binding site prediction\n        # top_bsite_binding_location_coords\tCartesian coordinates of top ranked binding site prediction\n        # top_bsite_pdb_ligand\tTop predicted ligand to bind\n        # top_bsite_ligand_cluster_counts\tNumber of predictions for all ligands\n        # top_bsite_algorithm\tAlgorithm used for top ranked binding site prediction\n        # top_bsite_c_score_method\tConfidence score of just the top algorithm's binding site prediction from [0,1]. >0.35 is somewhat reliable.\n        # top_bsite_pdb_template_id\tPDB ID of the template used to make the prediction\n        # top_bsite_pdb_template_chain\tChain of the PDB which has the ligand\n        # top_ec_ec_number\tTop predicted EC number\n        # top_ec_c_score\tConfidence score of the consensus EC number prediction from [0,1]\n        # top_ec_pdb_template_id\tTop PDB ID used as template for EC number prediction\n        # top_ec_pdb_template_chain\tChain of PDB ID used as template for EC number prediction\n        # top_ec_rmsd\tRMSD to best template\n        # top_ec_seq_coverage\tSequence coverage percentage to best template\n        # top_ec_seq_ident\tSequence identity percentage to best template\n        # top_ec_tm_score\tStructural similarity score to best template\n        # top_go_bp_go_id\tPredicted GO ID for biological process\n        # top_go_bp_go_term\tPredicted GO term for biological process\n        # top_go_bp_c_score\tConfidence score of the GO BP prediction from [0,1]\n        # top_go_cc_go_id\tPredicted GO ID for cellular compartment\n        # top_go_cc_go_term\tPredicted GO term for cellular compartment\n        # top_go_cc_c_score\tConfidence score of the GO CC prediction from [0,1]\n        # top_go_mf_go_id\tPredicted GO ID for molecular function\n        # top_go_mf_go_term\tPredicted GO term for molecular function\n        # top_go_mf_c_score\tConfidence score of the GO MF prediction from [0,1]\n\n        itasser_pre_df = []\n\n        df_cols = ['id', 'structure_file', 'model_date', 'difficulty',\n                   'top_template_pdb', 'top_template_chain', 'c_score',\n                   'tm_score', 'tm_score_err', 'rmsd', 'rmsd_err',\n                   'top_bsite_site_num', 'top_bsite_c_score', 'top_bsite_cluster_size', 'top_bsite_algorithm',\n                   'top_bsite_pdb_template_id', 'top_bsite_pdb_template_chain', 'top_bsite_pdb_ligand',\n                   'top_bsite_binding_location_coords', 'top_bsite_c_score_method', 'top_bsite_binding_residues',\n                   'top_bsite_ligand_cluster_counts',\n                   'top_ec_pdb_template_id', 'top_ec_pdb_template_chain', 'top_ec_tm_score', 'top_ec_rmsd',\n                   'top_ec_seq_ident', 'top_ec_seq_coverage', 'top_ec_c_score', 'top_ec_ec_number',\n                   'top_ec_binding_residues',\n                   'top_go_mf_go_id', 'top_go_mf_go_term', 'top_go_mf_c_score', 'top_go_bp_go_id', 'top_go_bp_go_term',\n                   'top_go_bp_c_score', 'top_go_cc_go_id', 'top_go_cc_go_term', 'top_go_cc_c_score']\n\n        for p in self.get_homology_models():\n            # Summary dataframe\n            new_homology_dict = p.get_dict(df_format=True, only_attributes=df_cols)\n            itasser_pre_df.append(new_homology_dict)\n\n        df = pd.DataFrame.from_records(itasser_pre_df, columns=df_cols).set_index('id')\n        return ssbio.utils.clean_df(df)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pdb_downloader_and_metadata(self, outdir=None, pdb_file_type=None, force_rerun=False):\n        if not outdir:\n            outdir = self.structure_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        if not pdb_file_type:\n            pdb_file_type = self.pdb_file_type\n\n        # Check if we have any PDBs\n        if self.num_structures_experimental == 0:\n            log.debug('{}: no structures available - nothing will be downloaded'.format(self.id))\n            return\n\n        downloaded_pdb_ids = []\n        # Download the PDBs\n        for s in self.get_experimental_structures():\n            log.debug('{}: downloading structure file from the PDB...'.format(s.id))\n            s.download_structure_file(outdir=outdir, file_type=pdb_file_type, force_rerun=force_rerun, load_header_metadata=True)\n            downloaded_pdb_ids.append(s.id)\n\n        return downloaded_pdb_ids", "response": "Download all mapped experimental structures to the protein structures directory and return a list of PDB IDs that were downloaded and the metadata for the header information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a dataframe of the PDB metadata for this experiment.", "response": "def df_pdb_metadata(self):\n        \"\"\"DataFrame: Get a dataframe of PDB metadata (PDBs have to be downloaded first)\"\"\"\n        if self.num_structures == 0:\n            log.error('No experimental PDB structures have been mapped to protein')\n            return pd.DataFrame()\n\n        pdb_pre_df = []\n\n        not_showing_info = []\n        for p in self.get_experimental_structures():\n            if not p.structure_file:\n                not_showing_info.append(p.id)\n                log.debug('{}: PDB file has not been downloaded, run \"pdb_downloader_and_metadata\" \\\n                    to download all structures'.format(p.id))\n                continue\n\n            infodict = p.get_dict(df_format=True)\n            infodict['pdb_id'] = p.id\n            pdb_pre_df.append(infodict)\n\n        log.warning('Not showing info for mapped PDB entries, as they have not been downloaded yet. Run pdb_downloader_and_metadata')\n\n        cols = ['pdb_id', 'pdb_title', 'description', 'experimental_method', 'mapped_chains',\n                'resolution', 'chemicals', 'date', 'taxonomy_name',\n                'structure_file']\n        df = pd.DataFrame.from_records(pdb_pre_df, columns=cols).set_index('pdb_id')\n        return ssbio.utils.clean_df(df)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef align_seqprop_to_structprop(self, seqprop, structprop, chains=None, outdir=None,\n                                    engine='needle', structure_already_parsed=False, parse=True, force_rerun=False,\n                                    **kwargs):\n        \"\"\"Run and store alignments of a SeqProp to chains in the ``mapped_chains`` attribute of a StructProp.\n\n        Alignments are stored in the sequence_alignments attribute, with the IDs formatted as\n        ``<SeqProp_ID>_<StructProp_ID>-<Chain_ID>``. Although it is more intuitive to align to individual ChainProps,\n        StructProps should be loaded as little as possible to reduce run times so the alignment is done to the entire\n        structure.\n\n        Args:\n            seqprop (SeqProp): SeqProp object with a loaded sequence\n            structprop (StructProp): StructProp object with a loaded structure\n            chains (str, list): Chain ID or IDs to map to. If not specified, ``mapped_chains`` attribute is inspected\n                for chains. If no chains there, all chains will be aligned to.\n            outdir (str): Directory to output sequence alignment files (only if running with needle)\n            engine (str): ``biopython`` or ``needle`` - which pairwise alignment program to use.\n                ``needle`` is the standard EMBOSS tool to run pairwise alignments.\n                ``biopython`` is Biopython's implementation of needle. Results can differ!\n            structure_already_parsed (bool): If the structure has already been parsed and the chain sequences are\n                stored. Temporary option until Hadoop sequence file is implemented to reduce number of times a\n                structure is parsed.\n            parse (bool): Store locations of mutations, insertions, and deletions in the alignment object (as an annotation)\n            force_rerun (bool): If alignments should be rerun\n            **kwargs: Other alignment options\n\n        Todo:\n            * Document **kwargs for alignment options\n\n        \"\"\"\n\n        if not outdir:\n            outdir = self.sequence_dir\n\n        if not structure_already_parsed:\n            # Parse the structure so chain sequences are stored\n            structprop.parse_structure()\n            # XTODO: remove and use the \"parsed\" attribute in a structprop instead\n\n        if chains:\n            chains_to_align_to = ssbio.utils.force_list(chains)\n        elif structprop.mapped_chains:\n            chains_to_align_to = structprop.mapped_chains\n        else:\n            log.warning('{}-{}: no chains specified in structure to align to, all chains will be aligned to'.format(seqprop.id,\n                                                                                                                    structprop.id))\n            chains_to_align_to = structprop.chains.list_attr('id')\n\n        for chain_id in chains_to_align_to:\n            full_structure_id = '{}-{}'.format(structprop.id, chain_id)\n            aln_id = '{}_{}'.format(seqprop.id, full_structure_id)\n            outfile = '{}.needle'.format(aln_id)\n\n            if self.sequence_alignments.has_id(aln_id) and not force_rerun:\n                log.debug('{}: alignment already completed, skipping'.format(aln_id))\n                continue\n\n            log.debug('{}: running pairwise alignment to structure {}, chain {}'.format(seqprop.id,\n                                                                                        structprop.id,\n                                                                                        chain_id))\n\n            # Check if the chain ID actually exists\n            if structprop.chains.has_id(chain_id):\n                chain_prop = structprop.chains.get_by_id(chain_id)\n                chain_seq_record = chain_prop.seq_record\n            else:\n                log.error('{}: chain not present in structure file!'.format(full_structure_id))\n                continue\n\n            # Check if the chain sequence was parsed\n            if not chain_seq_record:\n                log.error('{}: chain sequence not available, was structure parsed?'.format(full_structure_id))\n                continue\n\n            # Run the pairwise alignment\n            try:\n                aln = ssbio.protein.sequence.utils.alignment.pairwise_sequence_alignment(a_seq=seqprop,\n                                                                                         a_seq_id=seqprop.id,\n                                                                                         b_seq=chain_seq_record,\n                                                                                         b_seq_id=full_structure_id,\n                                                                                         engine=engine,\n                                                                                         outdir=outdir,\n                                                                                         outfile=outfile,\n                                                                                         force_rerun=force_rerun)\n            except ValueError:\n                log.error('{}: alignment failed to run, unable to check structure\\'s chain'.format(full_structure_id))\n                continue\n\n            # Add an identifier to the MultipleSeqAlignment object for storage in a DictList\n            aln.id = aln_id\n            # Add annotations to keep track of what was aligned\n            aln.annotations['a_seq'] = seqprop.id\n            aln.annotations['b_seq'] = full_structure_id\n            aln.annotations['structure_id'] = structprop.id\n            aln.annotations['chain_id'] = chain_id\n            aln.annotations['ssbio_type'] = 'structalign'\n\n            # Optionally parse for locations of mutations, deletions, and insertions\n            # Store mapping to chain index as letter annotations in the sequence\n            # Store locations in the alignment's annotations\n            if parse:\n                aln_df = ssbio.protein.sequence.utils.alignment.get_alignment_df(a_aln_seq=str(list(aln)[0].seq),\n                                                                                 b_aln_seq=str(list(aln)[1].seq))\n\n                chain_indices = aln_df[pd.notnull(aln_df.id_a_pos)].id_b_pos.tolist()\n                seqprop.letter_annotations['{}_chain_index'.format(aln_id)] = chain_indices\n\n                aln.annotations['mutations'] = ssbio.protein.sequence.utils.alignment.get_mutations(aln_df)\n                aln.annotations['deletions'] = ssbio.protein.sequence.utils.alignment.get_deletions(aln_df)\n                aln.annotations['insertions'] = ssbio.protein.sequence.utils.alignment.get_insertions(aln_df)\n\n            if force_rerun and self.sequence_alignments.has_id(aln.id):\n                self.sequence_alignments.remove(aln.id)\n            self.sequence_alignments.append(aln)", "response": "Run alignment of a SeqProp to a StructProp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the alignment stored in self. sequence_alignments given a seqprop + another seqprop", "response": "def _get_seqprop_to_seqprop_alignment(self, seqprop1, seqprop2):\n        \"\"\"Return the alignment stored in self.sequence_alignments given a seqprop + another seqprop\"\"\"\n        if isinstance(seqprop1, str):\n            seqprop1_id = seqprop1\n        else:\n            seqprop1_id = seqprop1.id\n        if isinstance(seqprop2, str):\n            seqprop2_id = seqprop2\n        else:\n            seqprop2_id = seqprop2.id\n\n        aln_id = '{}_{}'.format(seqprop1_id, seqprop2_id)\n\n        if self.sequence_alignments.has_id(aln_id):\n            alignment = self.sequence_alignments.get_by_id(aln_id)\n            return alignment\n        else:\n            raise ValueError('{}: sequence alignment not found, please run the alignment first'.format(aln_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_seqprop_to_seqprop_alignment_stats(self, seqprop1, seqprop2):\n        alignment = self._get_seqprop_to_seqprop_alignment(seqprop1=seqprop1, seqprop2=seqprop2)\n        return ssbio.protein.sequence.utils.alignment.pairwise_alignment_stats(reference_seq_aln=alignment[0],\n                                                                               other_seq_aln=alignment[1])", "response": "Get the alignment information for a sequence to a structure s chain."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmap a residue number in any SeqProp to another SeqProp using the pairwise alignment information.", "response": "def map_seqprop_resnums_to_seqprop_resnums(self, resnums, seqprop1, seqprop2):\n        \"\"\"Map a residue number in any SeqProp to another SeqProp using the pairwise alignment information.\n\n        Args:\n            resnums (int, list): Residue numbers in seqprop1\n            seqprop1 (SeqProp): SeqProp object the resnums match to\n            seqprop2 (SeqProp): SeqProp object you want to map the resnums to\n\n        Returns:\n            dict: Mapping of seqprop1 residue numbers to seqprop2 residue numbers. If mappings don't exist in this\n            dictionary, that means the residue number cannot be mapped according to alignment!\n\n        \"\"\"\n        resnums = ssbio.utils.force_list(resnums)\n\n        alignment = self._get_seqprop_to_seqprop_alignment(seqprop1=seqprop1, seqprop2=seqprop2)\n\n        mapped = ssbio.protein.sequence.utils.alignment.map_resnum_a_to_resnum_b(resnums=resnums,\n                                                                                 a_aln=alignment[0],\n                                                                                 b_aln=alignment[1])\n\n        return mapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_seqprop_to_structprop_alignment(self, seqprop, structprop, chain_id):\n        full_structure_id = '{}-{}'.format(structprop.id, chain_id)\n        aln_id = '{}_{}'.format(seqprop.id, full_structure_id)\n\n        if self.sequence_alignments.has_id(aln_id):\n            alignment = self.sequence_alignments.get_by_id(aln_id)\n            return alignment\n        else:\n            raise ValueError('{}: structure alignment not found, please run the alignment first'.format(aln_id))", "response": "Return the alignment stored in self. sequence_alignments given a seqprop structuprop and chain_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the alignment information for a sequence to a structure s chain.", "response": "def get_seqprop_to_structprop_alignment_stats(self, seqprop, structprop, chain_id):\n        \"\"\"Get the sequence alignment information for a sequence to a structure's chain.\"\"\"\n        alignment = self._get_seqprop_to_structprop_alignment(seqprop=seqprop, structprop=structprop, chain_id=chain_id)\n        return ssbio.protein.sequence.utils.alignment.pairwise_alignment_stats(reference_seq_aln=alignment[0],\n                                                                               other_seq_aln=alignment[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreports if a structure s chain meets the defined cutoffs for sequence quality.", "response": "def check_structure_chain_quality(self, seqprop, structprop, chain_id,\n                                      seq_ident_cutoff=0.5, allow_missing_on_termini=0.2,\n                                      allow_mutants=True, allow_deletions=False,\n                                      allow_insertions=False, allow_unresolved=True):\n        \"\"\"Report if a structure's chain meets the defined cutoffs for sequence quality.\"\"\"\n        alignment = self._get_seqprop_to_structprop_alignment(seqprop=seqprop, structprop=structprop, chain_id=chain_id)\n\n        # Compare sequence to structure's sequence using the alignment\n        chain_passes_quality_check = ssbio.protein.structure.properties.quality.sequence_checker(reference_seq_aln=alignment[0],\n                                                                   structure_seq_aln=alignment[1],\n                                                                   seq_ident_cutoff=seq_ident_cutoff,\n                                                                   allow_missing_on_termini=allow_missing_on_termini,\n                                                                   allow_mutants=allow_mutants,\n                                                                   allow_deletions=allow_deletions,\n                                                                   allow_insertions=allow_insertions,\n                                                                   allow_unresolved=allow_unresolved)\n        return chain_passes_quality_check"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_representative_chain(self, seqprop, structprop, chains_to_check=None,\n                                  seq_ident_cutoff=0.5, allow_missing_on_termini=0.2,\n                                  allow_mutants=True, allow_deletions=False,\n                                  allow_insertions=False, allow_unresolved=True):\n        \"\"\"Set and return the representative chain based on sequence quality checks to a reference sequence.\n\n        Args:\n            seqprop (SeqProp): SeqProp object to compare to chain sequences\n            structprop (StructProp): StructProp object with chains to compare to in the ``mapped_chains`` attribute. If\n                there are none present, ``chains_to_check`` can be specified, otherwise all chains are checked.\n            chains_to_check (str, list): Chain ID or IDs to check for sequence coverage quality\n            seq_ident_cutoff (float): Percent sequence identity cutoff, in decimal form\n            allow_missing_on_termini (float): Percentage of the total length of the reference sequence which will be\n                ignored when checking for modifications. Example: if 0.1, and reference sequence is 100 AA, then only\n                residues 5 to 95 will be checked for modifications.\n            allow_mutants (bool): If mutations should be allowed or checked for\n            allow_deletions (bool): If deletions should be allowed or checked for\n            allow_insertions (bool): If insertions should be allowed or checked for\n            allow_unresolved (bool): If unresolved residues should be allowed or checked for\n\n        Returns:\n            str: the best chain ID, if any\n\n        \"\"\"\n        if chains_to_check:\n            chains_to_check = ssbio.utils.force_list(chains_to_check)\n        elif structprop.mapped_chains:\n            chains_to_check = structprop.mapped_chains\n        else:\n            log.warning('{}-{}: no chains specified in structure to align to, all chains will be checked'.format(seqprop.id,\n                                                                                                                 structprop.id))\n            chains_to_check = structprop.chains.list_attr('id')\n\n        for chain_id in chains_to_check:\n            try:\n                # Compare sequence to structure's sequence using the alignment\n                found_good_chain = self.check_structure_chain_quality(seqprop=seqprop, structprop=structprop, chain_id=chain_id,\n                                                                      seq_ident_cutoff=seq_ident_cutoff,\n                                                                      allow_missing_on_termini=allow_missing_on_termini,\n                                                                      allow_mutants=allow_mutants,\n                                                                      allow_deletions=allow_deletions,\n                                                                      allow_insertions=allow_insertions,\n                                                                      allow_unresolved=allow_unresolved)\n            except ValueError:\n                log.error('{}-{}: unable to check chain \"{}\"'.format(seqprop.id, structprop.id, chain_id))\n                found_good_chain = False\n\n            # If found_good_chain = True, return chain ID\n            # If not, move on to the next potential chain\n            if found_good_chain:\n                stats = self.get_seqprop_to_structprop_alignment_stats(seqprop=seqprop, structprop=structprop, chain_id=chain_id)\n                self.representative_chain = chain_id\n                self.representative_chain_seq_coverage = stats['percent_identity']\n                return chain_id\n        else:\n            log.debug('{}: no chains meet quality checks'.format(structprop.id))\n            return None", "response": "Find the representative chain ID for a given sequence."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _map_seqprop_resnums_to_structprop_chain_index(self, resnums, seqprop=None, structprop=None, chain_id=None,\n                                                       use_representatives=False):\n        \"\"\"Map a residue number in any SeqProp to the mapping index in the StructProp + chain ID. This does not provide\n        a mapping to residue number, only a mapping to the index which then can be mapped to the structure resnum!\n\n        Args:\n            resnums (int, list): Residue numbers in the sequence\n            seqprop (SeqProp): SeqProp object\n            structprop (StructProp): StructProp object\n            chain_id (str): Chain ID to map to index\n            use_representatives (bool): If representative sequence/structure/chain should be used in mapping\n\n        Returns:\n            dict: Mapping of resnums to indices\n\n        \"\"\"\n        resnums = ssbio.utils.force_list(resnums)\n\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n            if not structprop:\n                raise ValueError('No representative structure set, please specify sequence, structure, and chain ID')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Please specify sequence, structure, and chain ID')\n\n        if self.representative_structure:\n            if structprop.id == self.representative_structure.id:\n                full_structure_id = '{}-{}'.format(structprop.id, chain_id).replace('REP-', '')\n            else:\n                full_structure_id = '{}-{}'.format(structprop.id, chain_id)\n        else:\n            full_structure_id = '{}-{}'.format(structprop.id, chain_id)\n\n        aln_id = '{}_{}'.format(seqprop.id, full_structure_id)\n        access_key = '{}_chain_index'.format(aln_id)\n        if access_key not in seqprop.letter_annotations:\n            raise KeyError('{}: structure mapping {} not available in sequence letter annotations. Was alignment parsed? '\n                           'Run ``align_seqprop_to_structprop`` with ``parse=True``.'.format(access_key, aln_id))\n        chain_index_mapping = seqprop.letter_annotations[access_key]\n\n        resnum_to_chain_index = {}\n        for x in resnums:\n            ix = chain_index_mapping[x - 1] - 1\n\n            if np.isnan(ix):\n                log.warning('{}-{}, {}: no equivalent residue found in structure sequence'.format(structprop.id,\n                                                                                                  chain_id,\n                                                                                                  x))\n            else:\n                resnum_to_chain_index[int(x)] = int(ix)\n\n        return resnum_to_chain_index", "response": "Map a residue number in any SeqProp to the index in the StructProp + chain ID. This function maps a residue number in any SeqProp to the mapping index in the StructProp + chain ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps a residue number in any SeqProp to the structure s residue number for a specified chain.", "response": "def map_seqprop_resnums_to_structprop_resnums(self, resnums, seqprop=None, structprop=None, chain_id=None,\n                                                  use_representatives=False):\n        \"\"\"Map a residue number in any SeqProp to the structure's residue number for a specified chain.\n\n        Args:\n            resnums (int, list): Residue numbers in the sequence\n            seqprop (SeqProp): SeqProp object\n            structprop (StructProp): StructProp object\n            chain_id (str): Chain ID to map to\n            use_representatives (bool): If the representative sequence and structure should be used. If True, seqprop,\n                structprop, and chain_id do not need to be defined.\n\n        Returns:\n            dict: Mapping of sequence residue numbers to structure residue numbers\n\n        \"\"\"\n        resnums = ssbio.utils.force_list(resnums)\n\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n            if not structprop:\n                raise ValueError('No representative structure set, please specify sequence, structure, and chain ID')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Please specify sequence, structure, and chain ID')\n\n        mapping_to_repchain_index = self._map_seqprop_resnums_to_structprop_chain_index(resnums=resnums,\n                                                                                        seqprop=seqprop,\n                                                                                        structprop=structprop,\n                                                                                        chain_id=chain_id,\n                                                                                        use_representatives=use_representatives)\n\n        chain = structprop.chains.get_by_id(chain_id)\n        chain_structure_resnum_mapping = chain.seq_record.letter_annotations['structure_resnums']\n\n        final_mapping = {}\n        for k, v in mapping_to_repchain_index.items():\n            k = int(k)\n            rn = chain_structure_resnum_mapping[v]\n\n            if rn == float('Inf'):\n                log.warning('{}-{}, {}: structure file does not contain coordinates for this residue'.format(structprop.id,\n                                                                                                             chain_id,\n                                                                                                             k))\n            else:\n                rn = int(rn)\n                final_mapping[k] = rn\n                index_of_structure_resnum = chain_structure_resnum_mapping.index(rn)\n\n                # Additionally report if residues are the same - they could be different in the structure though\n                format_data = {'seqprop_id'       : seqprop.id,\n                               'seqprop_resid'    : seqprop[k - 1],\n                               'seqprop_resnum'   : k,\n                               'structprop_id'    : structprop.id,\n                               'structprop_chid'  : chain_id,\n                               'structprop_resid' : chain.seq_record[index_of_structure_resnum],\n                               'structprop_resnum': rn}\n\n                if seqprop[k-1] != chain.seq_record[index_of_structure_resnum]:\n                    log.warning('Sequence {seqprop_id} residue {seqprop_resid}{seqprop_resnum} does not match to '\n                                'structure {structprop_id}-{structprop_chid} residue '\n                                '{structprop_resid}{structprop_resnum}. NOTE: this may be due to '\n                                'structural differences'.format(**format_data))\n                else:\n                    log.debug('Sequence {seqprop_id} residue {seqprop_resid}{seqprop_resnum} is mapped to '\n                              'structure {structprop_id}-{structprop_chid} residue '\n                              '{structprop_resid}{structprop_resnum}'.format(**format_data))\n\n        return final_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps a residue number in any StructProp + chain ID to any SeqProp s residue number.", "response": "def map_structprop_resnums_to_seqprop_resnums(self, resnums, structprop=None, chain_id=None, seqprop=None,\n                                                  use_representatives=False):\n        \"\"\"Map a residue number in any StructProp + chain ID to any SeqProp's residue number.\n\n        Args:\n            resnums (int, list): Residue numbers in the structure\n            structprop (StructProp): StructProp object\n            chain_id (str): Chain ID to map from\n            seqprop (SeqProp): SeqProp object\n            use_representatives (bool): If the representative sequence and structure should be used. If True, seqprop,\n                structprop, and chain_id do not need to be defined.\n\n        Returns:\n            dict: Mapping of structure residue numbers to sequence residue numbers\n\n        \"\"\"\n        resnums = ssbio.utils.force_list(resnums)\n\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n            if not structprop:\n                raise ValueError('No representative structure set, please specify sequence, structure, and chain ID')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Please specify sequence, structure, and chain ID')\n\n        if structprop.id == self.representative_structure.id:\n            full_structure_id = '{}-{}'.format(structprop.id, chain_id).replace('REP-', '')\n        else:\n            full_structure_id = '{}-{}'.format(structprop.id, chain_id)\n\n        aln_id = '{}_{}'.format(seqprop.id, full_structure_id)\n        access_key = '{}_chain_index'.format(aln_id)\n        if access_key not in seqprop.letter_annotations:\n            raise KeyError(\n                '{}: structure mapping {} not available in sequence letter annotations. Was alignment parsed? '\n                'Run ``align_seqprop_to_structprop`` with ``parse=True``.'.format(access_key, aln_id))\n\n        chain = structprop.chains.get_by_id(chain_id)\n        chain_structure_resnum_mapping = chain.seq_record.letter_annotations['structure_resnums']\n\n        final_mapping = {}\n        for resnum in resnums:\n            resnum = int(resnum)\n\n            resnum_index = chain_structure_resnum_mapping.index(resnum)\n            struct_res_singleaa = structprop.chains.get_by_id(chain_id).seq_record[resnum_index]\n\n            # if resnum not in seqprop.letter_annotations[access_key]:\n            #     log.warning('{}-{} -> {}: unable to map residue {} from structure to sequence, '\n            #                 'skipping'.format(structprop.id, chain_id, seqprop.id, resnum))\n            #     continue\n            what = seqprop.letter_annotations[access_key].index(resnum_index+1)\n            # TODO in progress...\n            seq_res_singleaa = seqprop[what]\n            sp_resnum = what + 1\n\n            final_mapping[resnum] = sp_resnum\n\n            # Additionally report if residues are the same - they could be different in the structure though\n            format_data = {'seqprop_id'       : seqprop.id,\n                           'seqprop_resid'    : seq_res_singleaa,\n                           'seqprop_resnum'   : sp_resnum,\n                           'structprop_id'    : structprop.id,\n                           'structprop_chid'  : chain_id,\n                           'structprop_resid' : struct_res_singleaa,\n                           'structprop_resnum': resnum}\n\n            if struct_res_singleaa != seq_res_singleaa:\n                log.warning('Sequence {seqprop_id} residue {seqprop_resid}{seqprop_resnum} does not match to '\n                            'structure {structprop_id}-{structprop_chid} residue '\n                            '{structprop_resid}{structprop_resnum}. NOTE: this may be due to '\n                            'structural differences'.format(**format_data))\n            else:\n                log.debug('Sequence {seqprop_id} residue {seqprop_resid}{seqprop_resnum} is mapped to '\n                          'structure {structprop_id}-{structprop_chid} residue '\n                          '{structprop_resid}{structprop_resnum}'.format(**format_data))\n\n        return final_mapping"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a subsequence from a given StructProp s letter_annotations attribute and return it", "response": "def get_seqprop_subsequence_from_structchain_property(self,\n                                                          property_key, property_value, condition,\n                                                          seqprop=None, structprop=None, chain_id=None,\n                                                          use_representatives=False,\n                                                          return_resnums=False):\n        \"\"\"Get a subsequence as a new SeqProp object given a certain property you want to find in the\n        given StructProp's chain's letter_annotation\n\n        This is similar to the :func:`ssbio.protein.sequence.seqprop.SeqProp.get_subsequence_from_property` method but instead of\n        filtering by the SeqProp's letter_annotation we use the StructProp annotation, and map back to the SeqProp.\n\n        Args:\n            seqprop (SeqRecord, SeqProp): SeqRecord or SeqProp object that has properties stored in its ``letter_annotations`` attribute\n            property_key (str): Property key in the ``letter_annotations`` attribute that you want to filter using\n            property_value (object): Property value that you want to filter by\n            condition (str): ``<``, ``=``, ``>``, ``>=``, or ``<=`` to filter the values by\n\n        Returns:\n            SeqProp: New SeqProp object that you can run computations on or just extract its properties\n\n        \"\"\"\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n            if not structprop:\n                raise ValueError('No representative structure set, please specify sequence, structure, and chain ID')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Please specify sequence, structure, and chain ID')\n\n        chain_prop = structprop.chains.get_by_id(chain_id)\n\n        # Get the subsequence from the structure\n        chain_subseq, subfeat_resnums = chain_prop.get_subsequence_from_property(property_key=property_key,\n                                                                                 property_value=property_value,\n                                                                                 condition=condition,\n                                                                                 return_resnums=True) or (None, [])\n        if not chain_subseq:\n            return\n\n        # Map subsequence feature resnums back to the seqprop\n        mapping_dict = self.map_structprop_resnums_to_seqprop_resnums(resnums=subfeat_resnums, structprop=structprop,\n                                                                      chain_id=chain_id,\n                                                                      seqprop=seqprop,\n                                                                      use_representatives=use_representatives)\n\n        sub_id = '{}-{}->{}_{}_{}_{}_extracted'.format(structprop.id, chain_id, seqprop.id,\n                                                       property_key, condition, property_value)\n        seqprop_resnums = [v for k,v in mapping_dict.items()]\n\n        new_sp = seqprop.get_subsequence(resnums=seqprop_resnums, new_id=sub_id, copy_letter_annotations=False)\n\n        if not new_sp:  # XTODO: investigate errors from subsequence extraction..\n            return\n\n        try:\n            new_sp.letter_annotations = chain_subseq.letter_annotations\n        except TypeError:\n            # If the length of the mapped sequence does not match, log a warning and don't store letter_annotations\n            log.warning('{}: cannot store structure letter annotations in subsequence, lengths do not match. '\n                        'Likely a deletion or insertion within the structure!'.format(sub_id))\n\n        if return_resnums:\n            return new_sp, seqprop_resnums\n        else:\n            return new_sp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the representative structure by cleaning it and copying over attributes of the original structure.", "response": "def _representative_structure_setter(self, structprop, keep_chain, clean=True, keep_chemicals=None,\n                                         out_suffix='_clean', outdir=None, force_rerun=False):\n        \"\"\"Set the representative structure by 1) cleaning it and 2) copying over attributes of the original structure.\n\n        The structure is copied because the chains stored may change, and cleaning it makes a new PDB file.\n\n        Args:\n            structprop (StructProp): StructProp object to set as representative\n            keep_chain (str): Chain ID to keep\n            clean (bool): If the PDB file should be cleaned (see ssbio.structure.utils.cleanpdb)\n            keep_chemicals (str, list): Keep specified chemical names\n            out_suffix (str): Suffix to append to clean PDB file\n            outdir (str): Path to output directory\n\n        Returns:\n            StructProp: representative structure\n\n        \"\"\"\n\n        # Set output directory for cleaned PDB file\n        if not outdir:\n            outdir = self.structure_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        # Create new ID for this representative structure, it cannot be the same as the original one\n        new_id = 'REP-{}'.format(structprop.id)\n\n        # Remove the previously set representative structure if set to force rerun\n        if self.structures.has_id(new_id):\n            if force_rerun:\n                existing = self.structures.get_by_id(new_id)\n                self.structures.remove(existing)\n\n        # If the structure is to be cleaned, and which chain to keep\n        if clean:\n            final_pdb = structprop.clean_structure(outdir=outdir, out_suffix=out_suffix,\n                                                   keep_chemicals=keep_chemicals, keep_chains=keep_chain,\n                                                   force_rerun=force_rerun)\n            log.debug('{}: cleaned structure and saved new file at {}'.format(structprop.id, final_pdb))\n        else:\n            final_pdb = structprop.structure_path\n\n        self.representative_structure = StructProp(ident=new_id, chains=keep_chain, mapped_chains=keep_chain,\n                                                   structure_path=final_pdb, file_type='pdb')\n        self.representative_chain = keep_chain\n\n        self.representative_structure.update(structprop.get_dict_with_chain(chain=keep_chain),\n                                             only_keys=self.__representative_structure_attributes,\n                                             overwrite=True)\n\n        # Save the original structure ID as an extra attribute\n        self.representative_structure.original_structure_id = structprop.id\n\n        # Also need to parse the clean structure and save its sequence..\n        self.representative_structure.parse_structure()\n\n        # And finally add it to the list of structures\n        self.structures.append(self.representative_structure)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_representative_structure(self, seq_outdir=None, struct_outdir=None, pdb_file_type=None,\n                                     engine='needle', always_use_homology=False, rez_cutoff=0.0,\n                                     seq_ident_cutoff=0.5, allow_missing_on_termini=0.2,\n                                     allow_mutants=True, allow_deletions=False,\n                                     allow_insertions=False, allow_unresolved=True,\n                                     clean=True, keep_chemicals=None, skip_large_structures=False,\n                                     force_rerun=False):\n        \"\"\"Set a representative structure from a structure in the structures attribute.\n\n        Each gene can have a combination of the following, which will be analyzed to set a representative structure.\n            * Homology model(s)\n            * Ranked PDBs\n            * BLASTed PDBs\n\n        If the ``always_use_homology`` flag is true, homology models are always set as representative when they exist.\n        If there are multiple homology models, we rank by the percent sequence coverage.\n\n        Args:\n            seq_outdir (str): Path to output directory of sequence alignment files, must be set if Protein directory\n                was not created initially\n            struct_outdir (str): Path to output directory of structure files, must be set if Protein directory\n                was not created initially\n            pdb_file_type (str): ``pdb``, ``mmCif``, ``xml``, ``mmtf`` - file type for files downloaded from the PDB\n            engine (str): ``biopython`` or ``needle`` - which pairwise alignment program to use.\n                ``needle`` is the standard EMBOSS tool to run pairwise alignments.\n                ``biopython`` is Biopython's implementation of needle. Results can differ!\n            always_use_homology (bool): If homology models should always be set as the representative structure\n            rez_cutoff (float): Resolution cutoff, in Angstroms (only if experimental structure)\n            seq_ident_cutoff (float): Percent sequence identity cutoff, in decimal form\n            allow_missing_on_termini (float): Percentage of the total length of the reference sequence which will be ignored\n                when checking for modifications. Example: if 0.1, and reference sequence is 100 AA, then only residues\n                5 to 95 will be checked for modifications.\n            allow_mutants (bool): If mutations should be allowed or checked for\n            allow_deletions (bool): If deletions should be allowed or checked for\n            allow_insertions (bool): If insertions should be allowed or checked for\n            allow_unresolved (bool): If unresolved residues should be allowed or checked for\n            clean (bool): If structure should be cleaned\n            keep_chemicals (str, list): Keep specified chemical names if structure is to be cleaned\n            skip_large_structures (bool): Default False -- currently, large structures can't be saved as a PDB file even\n                if you just want to save a single chain, so Biopython will throw an error when trying to do so. As an\n                alternative, if a large structure is selected as representative, the pipeline will currently point to it\n                and not clean it. If you don't want this to happen, set this to true.\n            force_rerun (bool): If sequence to structure alignment should be rerun\n\n        Returns:\n            StructProp: Representative structure from the list of structures. This is a not a map to the original\n            structure, it is copied and optionally cleaned from the original one.\n\n        Todo:\n            - Remedy large structure representative setting\n\n        \"\"\"\n        log.debug('{}: setting representative structure'.format(self.id))\n\n        if len(self.structures) == 0:\n            log.debug('{}: no structures available'.format(self.id))\n            return None\n\n        if not self.representative_sequence:\n            log.error('{}: no representative sequence to compare structures to'.format(self.id))\n            return None\n\n        if self.representative_structure and not force_rerun:\n            log.debug('{}: representative structure already set'.format(self.id))\n            return self.representative_structure\n\n        if self.representative_structure and force_rerun:\n            log.debug('{}: representative structure previously set, unsetting'.format(self.id))\n            self.representative_structure = None\n\n        if not pdb_file_type:\n            pdb_file_type = self.pdb_file_type\n\n        if not seq_outdir:\n            seq_outdir = self.sequence_dir\n            if not seq_outdir:\n                raise ValueError('Sequence output directory must be specified')\n\n        if not struct_outdir:\n            struct_outdir = self.structure_dir\n            if not struct_outdir:\n                raise ValueError('Structure output directory must be specified')\n\n        has_homology = False\n        has_pdb = False\n        use_homology = False\n        use_pdb = False\n\n        if self.num_structures_homology > 0:\n            has_homology = True\n        if self.num_structures_experimental > 0:\n            has_pdb = True\n\n        # If we mark to always use homology, use it if it exists\n        if always_use_homology:\n            if has_homology:\n                use_homology = True\n            elif has_pdb:\n                use_pdb = True\n        # If we don't always want to use homology, use PDB if it exists\n        else:\n            if has_homology and has_pdb:\n                use_pdb = True\n                use_homology = True\n            elif has_homology and not has_pdb:\n                use_homology = True\n            elif has_pdb and not has_homology:\n                use_pdb = True\n\n        if use_pdb:\n            # Put PDBs through QC/QA\n            all_pdbs = self.get_experimental_structures()\n            log.debug('{}: checking quality of {} experimental structures'.format(self.id, len(all_pdbs)))\n\n            for pdb in all_pdbs:\n                # Download the structure and parse it\n                # This will add all chains to the mapped_chains attribute if there are none\n                try:\n                    pdb.download_structure_file(outdir=struct_outdir, file_type=pdb_file_type, force_rerun=force_rerun, load_header_metadata=True)\n                except (requests.exceptions.HTTPError, URLError):\n                    log.error('{}: structure file could not be downloaded in {} format'.format(pdb, pdb_file_type))\n                    continue\n                    # TODO: add try/except to download cif file as fallback like below?\n\n                if rez_cutoff and pdb.resolution:\n                    if pdb.resolution > rez_cutoff:\n                        log.debug('{}: structure does not meet experimental resolution cutoff'.format(pdb, pdb_file_type))\n                        continue\n                # TODO: clean up these try/except things\n                try:\n                    self.align_seqprop_to_structprop(seqprop=self.representative_sequence,\n                                                     structprop=pdb,\n                                                     outdir=seq_outdir,\n                                                     engine=engine,\n                                                     parse=True,\n                                                     force_rerun=force_rerun)\n                except (PDBConstructionException, ExtraData, KeyError) as e:\n                    log.error('Protein {}, PDB {}: unable to parse structure file as {}. Falling back to mmCIF format.'.format(self.id, pdb, pdb_file_type))\n                    print(e)\n                    # Fall back to using mmCIF file if structure cannot be parsed\n                    try:\n                        pdb.download_structure_file(outdir=struct_outdir, file_type='mmCif',\n                                                    force_rerun=force_rerun, load_header_metadata=True)\n                    except (requests.exceptions.HTTPError, URLError):\n                        log.error('Protein {}, PDB {}: structure file could not be downloaded'.format(self.id, pdb))\n                        continue\n                    try:\n                        self.align_seqprop_to_structprop(seqprop=self.representative_sequence,\n                                                         structprop=pdb,\n                                                         outdir=seq_outdir,\n                                                         engine=engine,\n                                                         parse=True,\n                                                         force_rerun=force_rerun)\n                    except (PDBConstructionException, KeyError) as e:\n                        log.error('Protein {}, PDB {}: unable to parse structure file as {}.'.format(self.id, pdb, 'mmCif'))\n                        print(e)\n                        continue\n\n                best_chain = self.find_representative_chain(seqprop=self.representative_sequence,\n                                                            structprop=pdb,\n                                                            seq_ident_cutoff=seq_ident_cutoff,\n                                                            allow_missing_on_termini=allow_missing_on_termini,\n                                                            allow_mutants=allow_mutants, allow_deletions=allow_deletions,\n                                                            allow_insertions=allow_insertions, allow_unresolved=allow_unresolved)\n\n                if best_chain:\n                    try:\n                        self._representative_structure_setter(structprop=pdb,\n                                                              clean=clean,\n                                                              out_suffix='-{}_clean'.format(best_chain),\n                                                              keep_chain=best_chain,\n                                                              keep_chemicals=keep_chemicals,\n                                                              outdir=struct_outdir,\n                                                              force_rerun=force_rerun)\n                    except TypeError:\n                        if skip_large_structures == True:\n                            log.warning(\"{}: unable to save large PDB {}-{} in PDB file format, trying next \"\n                                        \"structure.\".format(self.id, pdb.id, best_chain))\n                            continue\n                        else:\n                            log.warning(\"{}: unable to save large PDB {}-{} in PDB file format, setting original \"\n                                        \"structure as representative. Set skip_large_structures=True if you don't \"\n                                        \"want this to happen\".format(self.id, pdb.id, best_chain))\n                            self.representative_structure = pdb\n                    except Exception as e:\n                        # Try force rerunning first if there exists a corrupt clean PDB file\n                        try:\n                            log.debug('{}: unknown error with {}, trying force_rerun first'.format(self.id, pdb.id))\n                            self._representative_structure_setter(structprop=pdb,\n                                                                  clean=clean,\n                                                                  out_suffix='-{}_clean'.format(best_chain),\n                                                                  keep_chain=best_chain,\n                                                                  keep_chemicals=keep_chemicals,\n                                                                  outdir=struct_outdir,\n                                                                  force_rerun=True)\n                        except Exception as e:\n                            # TODO: inspect causes of these errors - most common is Biopython PDBParser error\n                            logging.exception(\"{}: unknown error with PDB ID {}\".format(self.id, pdb.id))\n                            print(e)\n                            continue\n                    log.debug('{}-{}: set as representative structure'.format(pdb.id, best_chain))\n                    pdb.reset_chain_seq_records()\n                    return self.representative_structure\n                else:\n                    pdb.reset_chain_seq_records()\n            else:\n                log.debug('{}: no experimental structures meet cutoffs'.format(self.id))\n\n        # If we are to use homology, save its information in the representative structure field\n        if use_homology:\n            log.debug('{}: checking quality of homology models'.format(self.id))\n            all_models = self.get_homology_models()\n\n            # TODO: homology models are not ordered in any other way other than how they are loaded,\n            # rethink this for multiple homology models\n            for homology in all_models:\n                if not homology.structure_file:\n                    log.debug('{}: no homology structure file'.format(self.id))\n                    continue\n\n                self.align_seqprop_to_structprop(seqprop=self.representative_sequence,\n                                                 structprop=homology,\n                                                 outdir=seq_outdir,\n                                                 engine=engine,\n                                                 parse=True,\n                                                 force_rerun=force_rerun)\n                best_chain = self.find_representative_chain(seqprop=self.representative_sequence,\n                                                            structprop=homology,\n                                                            seq_ident_cutoff=seq_ident_cutoff,\n                                                            allow_missing_on_termini=allow_missing_on_termini,\n                                                            allow_mutants=allow_mutants,\n                                                            allow_deletions=allow_deletions,\n                                                            allow_insertions=allow_insertions,\n                                                            allow_unresolved=allow_unresolved)\n\n                if best_chain:\n                    # If chain ID is empty (some homology models are like that), use ID \"X\"\n                    if not best_chain.strip():\n                        best_chain = 'X'\n                    try:\n                        self._representative_structure_setter(structprop=homology,\n                                                              # new_id='{}-{}'.format(homology.id, best_chain), # 170906 Deprecated use of new_id\n                                                              clean=True,\n                                                              out_suffix='-{}_clean'.format(best_chain),\n                                                              keep_chain=best_chain,\n                                                              outdir=struct_outdir,\n                                                              force_rerun=force_rerun)\n                    except:\n                        # TODO: inspect causes of these errors - most common is Biopython PDBParser error\n                        logging.exception(\"Unknown error with homology model {}\".format(homology.id))\n                        continue\n                    log.debug('{}-{}: set as representative structure'.format(homology.id, best_chain))\n                    homology.reset_chain_seq_records()\n                    return self.representative_structure\n                else:\n                    homology.reset_chain_seq_records()\n\n        log.warning('{}: no structures meet quality checks'.format(self.id))\n        return None", "response": "Set a representative structure from a sequence alignment file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun DSSP on the structures and store annotations for each amino acid in the chain sequence at the corresponding sequence record.", "response": "def get_dssp_annotations(self, representative_only=True, force_rerun=False):\n        \"\"\"Run DSSP on structures and store calculations.\n\n        Annotations are stored in the protein structure's chain sequence at:\n        ``<chain_prop>.seq_record.letter_annotations['*-dssp']``\n\n        Args:\n            representative_only (bool): If analysis should only be run on the representative structure\n            force_rerun (bool): If calculations should be rerun even if an output file exists\n\n        Todo:\n            * Some errors arise from storing annotations for nonstandard amino acids, need to run DSSP separately for those\n\n        \"\"\"\n        if representative_only:\n            if self.representative_structure:\n                try:\n                    self.representative_structure.get_dssp_annotations(outdir=self.structure_dir, force_rerun=force_rerun)\n                except PDBException as e:\n                    log.error('{}: Biopython error, issue matching sequences with {}'.format(self.id, self.representative_structure))\n                    print(e)\n                except TypeError as e:\n                    log.error('{}: Biopython error, DSSP SeqRecord length mismatch with {}'.format(self.id, self.representative_structure))\n                    print(e)\n                except Exception as e:\n                    log.error('{}: DSSP failed to run on {}'.format(self.id, self.representative_structure))\n                    print(e)\n            else:\n                log.warning('{}: no representative structure set, cannot run DSSP'.format(self.id))\n        else:\n            for s in self.structures:\n                try:\n                    s.get_dssp_annotations(outdir=self.structure_dir)\n                except PDBException as e:\n                    log.error('{}: Biopython error, issue matching sequences with {}'.format(self.id, s.id))\n                    print(e)\n                except TypeError as e:\n                    log.error('{}: Biopython error, DSSP SeqRecord length mismatch with {}'.format(self.id, s.id))\n                    print(e)\n                except Exception as e:\n                    log.error('{}: DSSP failed to run on {}'.format(self.id, s.id))\n                    print(e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun MSMS on structures and store calculations.", "response": "def get_msms_annotations(self, representative_only=True, force_rerun=False):\n        \"\"\"Run MSMS on structures and store calculations.\n\n        Annotations are stored in the protein structure's chain sequence at:\n        ``<chain_prop>.seq_record.letter_annotations['*-msms']``\n\n        Args:\n            representative_only (bool): If analysis should only be run on the representative structure\n            force_rerun (bool): If calculations should be rerun even if an output file exists\n\n        \"\"\"\n        if representative_only:\n            if self.representative_structure:\n                try:\n                    self.representative_structure.get_msms_annotations(outdir=self.structure_dir, force_rerun=force_rerun)\n                except TypeError:\n                    log.error('{}: MSMS SeqRecord length mismatch with {}'.format(self.id, self.representative_structure))\n                except:\n                    log.error('{}: unknown MSMS error with {}'.format(self.id, self.representative_structure))\n            else:\n                log.warning('{}: no representative structure set, cannot run MSMS'.format(self.id))\n        else:\n            for s in self.structures:\n                try:\n                    s.get_msms_annotations(outdir=self.structure_dir)\n                except TypeError:\n                    log.error('{}: MSMS SeqRecord length mismatch with {}'.format(self.id, s.id))\n                except Exception as e:\n                    log.error('{}: unknown MSMS error with {}'.format(self.id, s.id))\n                    print(e)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_disulfide_bridges(self, representative_only=True):\n        if representative_only:\n            if self.representative_structure:\n                try:\n                    self.representative_structure.find_disulfide_bridges()\n                except KeyError:\n                    log.error('{}: unable to run disulfide bridge finder on {}'.format(self.id, self.representative_structure))\n            else:\n                log.warning('{}: no representative structure set, cannot run disulfide bridge finder'.format(self.id))\n        else:\n            for s in self.structures:\n                try:\n                    s.find_disulfide_bridges()\n                except KeyError:\n                    log.error('{}: unable to run disulfide bridge finder on {}'.format(self.id, s.id))", "response": "Run Biopython s disulfide bridge finder and store found bridges."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_residue_annotations(self, seq_resnum, seqprop=None, structprop=None, chain_id=None,\n                                use_representatives=False):\n        \"\"\"Get all residue-level annotations stored in the SeqProp ``letter_annotations`` field for a given residue number.\n\n        Uses the representative sequence, structure, and chain ID stored by default. If other properties from other\n        structures are desired, input the proper IDs. An alignment for the given sequence to the structure must\n        be present in the sequence_alignments list.\n\n        Args:\n            seq_resnum (int): Residue number in the sequence\n            seqprop (SeqProp): SeqProp object\n            structprop (StructProp): StructProp object\n            chain_id (str): ID of the structure's chain to get annotation from\n            use_representatives (bool): If the representative sequence/structure/chain IDs should be used\n\n        Returns:\n            dict: All available letter_annotations for this residue number\n\n        \"\"\"\n\n        if use_representatives:\n            if seqprop and structprop and chain_id:\n                raise ValueError('Overriding sequence, structure, and chain IDs with representatives. '\n                                 'Set use_representatives to False if custom IDs are to be used.')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Input sequence, structure, and chain to map between, or set use_representatives '\n                                 'to True.')\n\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n\n        # log.debug('Using sequence: {}, structure: {}, chain: {}'.format(seqprop.id, structprop.id, chain_id))\n\n        # Create a new SeqFeature\n        f = SeqFeature(FeatureLocation(seq_resnum-1, seq_resnum))\n\n        # Get sequence properties\n        seq_features = f.extract(seqprop)\n\n        # Store in dictionary to return, clean it up\n        all_info = ssbio.utils.clean_single_dict(indict=seq_features.letter_annotations,\n                                                 prepend_to_keys='seq_',\n                                                 remove_keys_containing='_chain_index')\n        all_info['seq_resnum'] = seq_resnum\n        all_info['seq_residue'] = str(seq_features.seq)\n\n        if structprop:\n            chain = structprop.chains.get_by_id(chain_id)\n\n            # Get structure properties\n            mapping_to_structure_resnum = self.map_seqprop_resnums_to_structprop_resnums(resnums=seq_resnum,\n                                                                                         seqprop=seqprop,\n                                                                                         structprop=structprop,\n                                                                                         chain_id=chain_id,\n                                                                                         use_representatives=use_representatives)\n\n            # Try finding the residue in the structure\n            if f.location.end.position in mapping_to_structure_resnum:\n                struct_resnum = mapping_to_structure_resnum[f.location.end.position]\n                struct_f = SeqFeature(FeatureLocation(struct_resnum-1, struct_resnum))\n\n                struct_seq_features = struct_f.extract(chain.seq_record)\n                struct_info = ssbio.utils.clean_single_dict(indict=struct_seq_features.letter_annotations,\n                                                            prepend_to_keys='struct_',\n                                                            remove_keys_containing='structure_resnums')\n                struct_info['struct_resnum'] = struct_resnum\n                struct_info['struct_residue'] = str(struct_seq_features.seq)\n                all_info.update(struct_info)\n\n                # Warn if residue differs from sequence\n                if seq_features.seq != struct_seq_features.seq:\n                    log.warning('Sequence residue ({}{}) does not match structure residue ({}{}). '\n                                'This may simply be due to differences in the structure'.format(seq_features.seq,\n                                                                                                seq_resnum,\n                                                                                                struct_seq_features.seq,\n                                                                                                struct_resnum))\n\n        return all_info", "response": "Get all the letter annotations for a given residue number."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sequence_mutation_summary(self, alignment_ids=None, alignment_type=None):\n        if alignment_ids:\n            ssbio.utils.force_list(alignment_ids)\n\n        if len(self.sequence_alignments) == 0:\n            log.error('{}: no sequence alignments'.format(self.id))\n            return {}, {}\n\n        fingerprint_counter = defaultdict(list)\n        single_counter = defaultdict(list)\n\n        for alignment in self.sequence_alignments:\n            # Ignore alignments if a list of identifiers is provided\n            if alignment_ids:\n                if alignment.id not in alignment_ids:\n                    continue\n            # Ignore alignments if type is specified\n            if alignment_type:\n                if alignment.annotations['ssbio_type'] != alignment_type:\n                    continue\n\n            other_sequence = alignment.annotations['b_seq']\n            mutations = alignment.annotations['mutations']\n\n            if mutations:\n                # Turn this list of mutations into a tuple so it can be a dictionary key\n                mutations = tuple(tuple(x) for x in mutations)\n                fingerprint_counter[mutations].append(other_sequence)\n\n                for m in mutations:\n                    single_counter[m].append(other_sequence)\n\n        return dict(single_counter), dict(fingerprint_counter)", "response": "Summarize all mutations found in the sequence_alignments attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd select features from the selected SeqProp object to an NGLWidget view object.", "response": "def add_features_to_nglview(self, view, seqprop=None, structprop=None, chain_id=None, use_representatives=False):\n        \"\"\"Add select features from the selected SeqProp object to an NGLWidget view object.\n\n        Currently parsing for:\n\n            * Single residue features (ie. metal binding sites)\n            * Disulfide bonds\n\n        Args:\n            view (NGLWidget): NGLWidget view object\n            seqprop (SeqProp): SeqProp object\n            structprop (StructProp): StructProp object\n            chain_id (str): ID of the structure's chain to get annotation from\n            use_representatives (bool): If the representative sequence/structure/chain IDs should be used\n\n        \"\"\"\n        if use_representatives:\n            if seqprop and structprop and chain_id:\n                raise ValueError('Overriding sequence, structure, and chain IDs with representatives. '\n                                 'Set use_representatives to False if custom IDs are to be used.')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Input sequence, structure, and chain to map between, or set use_representatives '\n                                 'to True.')\n\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n\n        # Parse and store chain seq if not already stored\n        if not structprop.chains.has_id(chain_id):\n            structprop.parse_structure()\n            if not structprop.chains.has_id(chain_id):\n                raise ValueError('Chain {} not present in structure {}'.format(chain_id, structprop.id))\n\n        if not seqprop.features:\n            log.warning('{}: no stored features'.format(seqprop.id))\n\n\n        # Loop through any stored features\n        for f in seqprop.features:\n\n            # Display disulfide bonds\n            if f.type.lower() == 'disulfide bond':\n                # TODO: double check if .start or .start + 1\n                disulfide = self.map_seqprop_resnums_to_structprop_resnums(resnums=[f.location.start + 1, f.location.end],\n                                                                           seqprop=seqprop,\n                                                                           structprop=structprop,\n                                                                           chain_id=chain_id,\n                                                                           use_representatives=use_representatives)\n                to_view = [str(x)+'.CA' for x in list(disulfide.values())]\n                view.add_distance(atom_pair=[to_view], color='black')\n                log.info('Disulfide bridge at residues {} & {}'.format(f.location.start + 1, f.location.end))\n\n            # Display DNA-binding regions\n            if f.type.lower() == 'dna-binding region' or f.type.lower() == 'nucleotide phosphate-binding region':\n                impres = self.map_seqprop_resnums_to_structprop_resnums(resnums=[f.location.start + 1,\n                                                                                 f.location.end],\n                                                                        seqprop=seqprop,\n                                                                        structprop=structprop,\n                                                                        chain_id=chain_id,\n                                                                        use_representatives=use_representatives)\n\n                # TODO: need to check if f.location.start was mapped and if not, try incrementing. or  input the list\n                # of resnums, not just the start and end\n                if f.location.start + 1 in impres and f.location.end in impres:\n                    mapped_start = impres[f.location.start + 1]\n                    mapped_end = impres[f.location.end]\n                    view.add_ball_and_stick(selection=':{} and ( {}-{} )'.format(chain_id,\n                                                                                 mapped_start,\n                                                                                 mapped_end), color='black')\n                    log.info('{} at sequence region {}-{}, structure residues {}-{}'.format(f.type,\n                                                                                            f.location.start,\n                                                                                            f.location.end,\n                                                                                            mapped_start,\n                                                                                            mapped_end))\n\n            # Display other single residues\n            if f.location.end - 1 == f.location.start:\n                if f.type.lower() == 'sequence variant' or f.type.lower() == 'mutagenesis site':\n                    continue\n                impres = self.map_seqprop_resnums_to_structprop_resnums(resnums=f.location.end,\n                                                                        seqprop=seqprop,\n                                                                        structprop=structprop,\n                                                                        chain_id=chain_id,\n                                                                        use_representatives=use_representatives)\n                if f.location.end in impres:\n                    impres_mapped = impres[f.location.end]\n                    view.add_ball_and_stick(selection=str(impres_mapped), color='black')\n                    view.add_label(selection=':{} and {}'.format(chain_id, impres_mapped), label_type='res', color='black')\n                    log.info('{} at sequence residue {}, structure residue {}'.format(f.type, f.location.end, impres_mapped))\n\n            # Display transmembrane regions\n            if f.type.lower() == 'transmembrane region':\n                impres = self.map_seqprop_resnums_to_structprop_resnums(resnums=[f.location.start + 1,\n                                                                                 f.location.end],\n                                                                        seqprop=seqprop,\n                                                                        structprop=structprop,\n                                                                        chain_id=chain_id,\n                                                                        use_representatives=use_representatives)\n\n                # TODO: need to check if f.location.start was mapped and if not, try incrementing. or  input the list\n                # of resnums, not just the start and end\n                if f.location.start + 1 in impres and f.location.end in impres:\n                    mapped_start = impres[f.location.start + 1]\n                    mapped_end = impres[f.location.end]\n                    view.add_cartoon(selection=':{} and ( {}-{} )'.format(chain_id,\n                                                                          mapped_start,\n                                                                          mapped_end),\n                                     aspectRatio=9,\n                                     color='black')\n                    log.info('{} at sequence region {}-{}, structure residues {}-{}'.format(f.type,\n                                                                                            f.location.start,\n                                                                                            f.location.end,\n                                                                                            mapped_start,\n                                                                                            mapped_end))\n\n            # Display topological domains\n            if f.type.lower() == 'topological domain':\n                impres = self.map_seqprop_resnums_to_structprop_resnums(resnums=[f.location.start + 1,\n                                                                                 f.location.end],\n                                                                        seqprop=seqprop,\n                                                                        structprop=structprop,\n                                                                        chain_id=chain_id,\n                                                                        use_representatives=use_representatives)\n\n                # TODO: need to check if f.location.start was mapped and if not, try incrementing. or  input the list\n                # of resnums, not just the start and end\n                if f.location.start + 1 in impres and f.location.end in impres:\n                    mapped_start = impres[f.location.start + 1]\n                    mapped_end = impres[f.location.end]\n                    view.add_cartoon(selection=':{} and ( {}-{} )'.format(chain_id,\n                                                                          mapped_start,\n                                                                          mapped_end),\n                                     aspectRatio=9,\n                                     color='green')\n                    log.info('{} at sequence region {}-{}, structure residues {}-{}'.format(f.type,\n                                                                                            f.location.start,\n                                                                                            f.location.end,\n                                                                                            mapped_start,\n                                                                                            mapped_end))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the residues that are mutated in the sequence_alignments attribute of the given object to the NGLWidget view object.", "response": "def add_mutations_to_nglview(self, view, alignment_type='seqalign', alignment_ids=None,\n                                 seqprop=None, structprop=None, chain_id=None, use_representatives=False,\n                                 grouped=False, color='red', unique_colors=True,\n                                 opacity_range=(0.8,1), scale_range=(1,5)):\n        \"\"\"Add representations to an NGLWidget view object for residues that are mutated in the\n        ``sequence_alignments`` attribute.\n\n        Args:\n            view (NGLWidget): NGLWidget view object\n            alignment_type (str): Specified alignment type contained in the ``annotation`` field of an alignment object,\n                ``seqalign`` or ``structalign`` are the current types.\n            alignment_ids (str, list): Specified alignment ID or IDs to use\n            seqprop (SeqProp): SeqProp object\n            structprop (StructProp): StructProp object\n            chain_id (str): ID of the structure's chain to get annotation from\n            use_representatives (bool): If the representative sequence/structure/chain IDs should be used\n            grouped (bool): If groups of mutations should be colored and sized together\n            color (str): Color of the mutations (overridden if unique_colors=True)\n            unique_colors (bool): If each mutation/mutation group should be colored uniquely\n            opacity_range (tuple): Min/max opacity values (mutations that show up more will be opaque)\n            scale_range (tuple): Min/max size values (mutations that show up more will be bigger)\n\n        \"\"\"\n        if use_representatives:\n            if seqprop and structprop and chain_id:\n                raise ValueError('Overriding sequence, structure, and chain IDs with representatives. '\n                                 'Set use_representatives to False if custom IDs are to be used.')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Input sequence, structure, and chain to map between, or set use_representatives '\n                                 'to True.')\n\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n\n        log.debug('Using sequence: {}, structure: {}, chain: {}'.format(seqprop.id, structprop.id, chain_id))\n\n        # Get the summary of mutations\n        single, fingerprint = self.sequence_mutation_summary(alignment_type=alignment_type, alignment_ids=alignment_ids)\n\n        # Map residues from sequence to structure\n        if not grouped:\n            single_lens = {k: len(v) for k, v in single.items()}\n            single_map_to_structure = {}\n\n            for k, v in single_lens.items():\n                resnum = int(k[1])\n                resnum_to_structure = self.map_seqprop_resnums_to_structprop_resnums(resnums=resnum,\n                                                                                     seqprop=seqprop,\n                                                                                     structprop=structprop,\n                                                                                     chain_id=chain_id,\n                                                                                     use_representatives=use_representatives)\n                if resnum not in resnum_to_structure:\n                    log.warning('{}: residue is not available in structure {}'.format(resnum, structprop.id))\n                    continue\n                new_key = resnum_to_structure[resnum]\n                single_map_to_structure[new_key] = v\n\n            structprop.add_scaled_residues_highlight_to_nglview(view=view,\n                                                                structure_resnums=single_map_to_structure,\n                                                                chain=chain_id,\n                                                                color=color,\n                                                                unique_colors=unique_colors,\n                                                                opacity_range=opacity_range,\n                                                                scale_range=scale_range)\n        else:\n            log.warning('Viewing mutation groups is currently in beta -- groups may overwrite each other')\n            fingerprint_lens = {k: len(v) for k, v in fingerprint.items()}\n            fingerprint_map_to_structure = {}\n\n            for k, v in fingerprint_lens.items():\n                k_list = [int(x[1]) for x in k]\n                resnums_to_structure = self.map_seqprop_resnums_to_structprop_resnums(resnums=k_list,\n                                                                                      seqprop=seqprop,\n                                                                                      structprop=structprop,\n                                                                                      chain_id=chain_id,\n                                                                                      use_representatives=use_representatives)\n                new_key = tuple(y for y in resnums_to_structure.values())\n                fingerprint_map_to_structure[new_key] = v\n\n            structprop.add_scaled_residues_highlight_to_nglview(view=view,\n                                                                structure_resnums=fingerprint_map_to_structure,\n                                                                chain=chain_id,\n                                                                color=color,\n                                                                unique_colors=unique_colors,\n                                                                opacity_range=opacity_range,\n                                                                scale_range=scale_range)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the given fingerprint to an NGLWidget view object.", "response": "def add_fingerprint_to_nglview(self, view, fingerprint,\n                                   seqprop=None, structprop=None, chain_id=None, use_representatives=False,\n                                   color='red', opacity_range=(0.8, 1), scale_range=(1, 5)):\n        \"\"\"Add representations to an NGLWidget view object for residues that are mutated in the\n        ``sequence_alignments`` attribute.\n\n        Args:\n            view (NGLWidget): NGLWidget view object\n            fingerprint (dict): Single mutation group from the ``sequence_mutation_summary`` function\n            seqprop (SeqProp): SeqProp object\n            structprop (StructProp): StructProp object\n            chain_id (str): ID of the structure's chain to get annotation from\n            use_representatives (bool): If the representative sequence/structure/chain IDs should be used\n            color (str): Color of the mutations (overridden if unique_colors=True)\n            opacity_range (tuple): Min/max opacity values (mutations that show up more will be opaque)\n            scale_range (tuple): Min/max size values (mutations that show up more will be bigger)\n\n        \"\"\"\n        if use_representatives:\n            if seqprop and structprop and chain_id:\n                raise ValueError('Overriding sequence, structure, and chain IDs with representatives. '\n                                 'Set use_representatives to False if custom IDs are to be used.')\n        else:\n            if not seqprop or not structprop or not chain_id:\n                raise ValueError('Input sequence, structure, and chain to map between, or set use_representatives '\n                                 'to True.')\n\n        if use_representatives:\n            seqprop = self.representative_sequence\n            structprop = self.representative_structure\n            chain_id = self.representative_chain\n\n        log.debug('Using sequence: {}, structure: {}, chain: {}'.format(seqprop.id, structprop.id, chain_id))\n\n        fingerprint_lens = {k: len(v) for k, v in fingerprint.items()}\n        fingerprint_map_to_structure = {}\n        for k, v in fingerprint_lens.items():\n            k_list = [int(x[1]) for x in k]\n            resnums_to_structure = self.map_seqprop_resnums_to_structprop_resnums(resnums=k_list,\n                                                                                  seqprop=seqprop,\n                                                                                  structprop=structprop,\n                                                                                  chain_id=chain_id,\n                                                                                  use_representatives=use_representatives)\n            new_key = tuple(y for y in resnums_to_structure.values())\n            fingerprint_map_to_structure[new_key] = v\n\n        structprop.add_scaled_residues_highlight_to_nglview(view=view,\n                                                            structure_resnums=fingerprint_map_to_structure,\n                                                            chain=chain_id,\n                                                            color=color,\n                                                            opacity_range=opacity_range,\n                                                            scale_range=scale_range)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget ALL PDBFlex entries for all mapped structures then stores the ones that match the repseq length", "response": "def get_all_pdbflex_info(self):\n        \"\"\"Gets ALL PDBFlex entries for all mapped structures, then stores the ones that match the repseq length\n\n        Ideas:\n            - maybe first check for quality of structure and then retrieve the pdbflex entry\n            - not sure which sequence is used in pdbflex\n\n        \"\"\"\n        # XTODO: documentation\n        log.debug('{}: representative sequence length'.format(self.representative_sequence.seq_len))\n\n        for s in self.get_experimental_structures():\n            log.debug('{};{}: chains matching protein {}'.format(s.id, s.mapped_chains, self.id))\n\n            s.download_structure_file(outdir=self.structure_dir, file_type='mmtf')\n            # s.parse_structure()\n\n            for c in s.mapped_chains:\n                # log.debug('{}: sequence length of chain {}'.format(len(s.chains.get_by_id(c).seq_record), c))\n\n                # Retrieve PDBFlex stats\n                stats = ssbio.databases.pdbflex.get_pdbflex_info(pdb_id=s.id, chain_id=c, outdir=self.structure_dir)\n                parent = stats['parentClusterID']\n\n                if parent:\n                    # Retrieve PDBFlex representative PDBs\n                    reps = ssbio.databases.pdbflex.get_pdbflex_representatives(pdb_id=s.id, chain_id=c, outdir=self.structure_dir)\n\n                    # Store general statistics in annotations\n                    parent_stats_key = 'structural_flexibility_stats_{}_parent-pdbflex'.format(parent)\n                    parent_reps_key = 'structural_flexibility_representatives_{}_parent-pdbflex'.format(parent)\n                    if parent_stats_key not in self.representative_sequence.annotations or parent_reps_key not in self.representative_sequence.annotations:\n                        self.representative_sequence.annotations[parent_stats_key] = stats\n                        self.representative_sequence.annotations[parent_reps_key] = reps\n                        log.debug('{}: stored PDB Flex stats in representative sequence for PDB parent {}'.format(\n                            self.representative_sequence.id,\n                            parent))\n                    else:\n                        log.info(\n                            '{}: existing PDB Flex stats already in representative sequence for PDB parent {}'.format(\n                                self.representative_sequence.id,\n                                parent))\n\n                    # Retrieve PDBFlex RMSDs\n                    rmsd = ssbio.databases.pdbflex.get_pdbflex_rmsd_profile(pdb_id=s.id, chain_id=c, outdir=self.structure_dir)\n                    log.info('{}: sequence length reported in PDB Flex'.format(len(rmsd['profile'])))\n\n                    # Store residue specific RMSDs in letter_annotations\n                    parent_key = 'rmsd_{}_parent-pdbflex'.format(parent)\n                    if parent_key not in self.representative_sequence.letter_annotations:\n                        self.representative_sequence.letter_annotations[parent_key] = rmsd['profile']\n                        log.info('{}: stored PDB Flex RMSD in representative sequence for PDB parent {}'.format(\n                            self.representative_sequence.id,\n                            parent))\n                    else:\n                        log.info(\n                            '{}: existing PDB Flex RMSD already in representative sequence for PDB parent {}'.format(\n                                self.representative_sequence.id,\n                                parent))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntranslating uniprot features into letter annotations.", "response": "def translate_features_to_letter_annotations(protein, more_sites=None):\n        \"\"\"Store select uniprot features (sites) as letter annotations with the key as the\n        type of site and the values as a list of booleans\"\"\"\n        from ssbio.databases.uniprot import longname_sites\n        from collections import defaultdict\n\n        sites = longname_sites  ## longname_sites = [\"active site\", \"binding site\", \"metal ion-binding site\", \"site\"]\n        sites.append('nucleotide phosphate-binding region')\n        sites.append('DNA-binding region')\n        sites.append('intramembrane region')\n        sites.append(\"transmembrane region\")\n        sites.append(\"catalyticResidue\")\n\n        ## ADD MORE IF YOU WANT\n        if more_sites:\n            more_sites = ssbio.utils.force_list(more_sites)\n            sites.extend(more_sites)\n\n        sites = list(set(sites))\n\n        for site in sites:\n            protein.representative_sequence.letter_annotations[site] = [False] * protein.representative_sequence.seq_len\n\n        to_store = defaultdict(list)\n        for f in protein.representative_sequence.features:\n            if f.type in sites:\n                to_store[f.type].append(f)\n\n        for site, feature in to_store.items():\n            try:\n                positions = [int(f.location.start) for f in feature]\n            except TypeError:\n                log.error('Protein {}, SeqProp {}: unable to translate feature {} into letter annotation'.format(protein.id, protein.representative_sequence.id, site))\n                continue\n            feat_letter_anno = []\n            for x in range(protein.representative_sequence.seq_len):\n                if x in positions:\n                    idx = positions.index(x)\n                    if 'description' in feature[idx].qualifiers:\n                        feat_letter_anno.append(feature[idx].qualifiers['description'])\n                    else:\n                        feat_letter_anno.append(True)\n                else:\n                    feat_letter_anno.append(False)\n            protein.representative_sequence.letter_annotations[site] = feat_letter_anno"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a subsequence from REPSEQ based on a property stored in REPSEQ. letter_annotations", "response": "def get_generic_subseq_2D(protein, cutoff, prop, condition):\n        \"\"\"Get a subsequence from REPSEQ based on a property stored in REPSEQ.letter_annotations\"\"\"\n        subseq, subseq_resnums = protein.representative_sequence.get_subsequence_from_property(property_key=prop,\n                                                                                               property_value=cutoff,\n                                                                                               condition=condition,\n                                                                                               return_resnums=True) or (\n                                     None, [])\n\n        return {'subseq_len': len(subseq_resnums), 'subseq': subseq, 'subseq_resnums': subseq_resnums}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_generic_subseq_3D(protein, cutoff, prop, condition):\n        if not protein.representative_structure:\n            log.error('{}: no representative structure, cannot search for subseq'.format(protein.id))\n            return {'subseq_len': 0, 'subseq': None, 'subseq_resnums': []}\n\n        subseq, subseq_resnums = protein.get_seqprop_subsequence_from_structchain_property(property_key=prop,\n                                                                                           property_value=cutoff,\n                                                                                           condition=condition,\n                                                                                           use_representatives=True,\n                                                                                           return_resnums=True) or (\n                                 None, [])\n\n        return {'subseq_len': len(subseq_resnums), 'subseq': subseq, 'subseq_resnums': subseq_resnums}", "response": "Get a subsequence from REPSEQ based on a property stored in REPSTRUCT. REPCHAIN. letter_annotations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_generic_subseq_within_2_5D(protein, prop_name, within, filter_resnums=None):\n        from ssbio.biopython.Bio.Struct.Geometry import center_of_mass\n        sites = list(set(protein.representative_sequence.letter_annotations[prop_name]))\n        sites.remove(False)\n\n        if len(sites) > 0:\n            log.debug(\n                '{} unique {} sites to find subsequence within {} angstroms of'.format(len(sites), prop_name, within))\n            ps = protein.representative_structure.parse_structure()\n\n            all_subseq_resnums = []\n            for site in sites:\n                site_binding_residues, site_binding_resnums = protein.representative_sequence.get_subsequence_from_property(\n                    property_key=prop_name,\n                    property_value=site, condition='=', return_resnums=True)\n                mapping_to_structure_resnums = protein.map_seqprop_resnums_to_structprop_resnums(\n                    resnums=site_binding_resnums,\n                    use_representatives=True)\n                all_residues = []\n                for resnum in mapping_to_structure_resnums:\n                    try:\n                        target_residue = ps.first_model[protein.representative_chain][resnum]\n                    except KeyError:\n                        log.error('Protein {}, RepStruct {}-{}: cannot find resnum {}, not including in list of '\n                                  'residues to find center of mass'.format(protein.id,\n                                                                           protein.representative_structure.id,\n                                                                           protein.representative_chain, resnum))\n                        continue\n                    all_residues.append(target_residue)\n                if not all_residues:\n                    log.error('{}->{}-{}: unable to map resnums to structure'.format(site_binding_resnums,\n                                                                                     protein.representative_structure.id,\n                                                                                     protein.representative_chain))\n                    continue\n                coords = center_of_mass(all_residues, geometric=True)\n                log.debug('{}: center of mass of site {}'.format(coords, site))\n                subseq, subseq_resnums = protein.representative_structure.get_seqprop_within(\n                    chain_id=protein.representative_chain,\n                    angstroms=within, resnum=None,\n                    custom_coord=coords, return_resnums=True)\n                log.debug('{}: structure resnums within {} of site {} (resnums {})'.format(subseq_resnums, within, site,\n                                                                                           site_binding_resnums))\n                all_subseq_resnums.extend(subseq_resnums)\n        else:\n            log.debug('No {} sites'.format(prop_name))\n            return {'subseq_len': 0, 'subseq': None, 'subseq_resnums': []}\n\n        # Combine and filter the site binding site residue numbers (if desired)\n        if filter_resnums:\n            all_subseq_resnums = list(set(all_subseq_resnums).intersection(filter_resnums))\n        else:\n            all_subseq_resnums = list(set(all_subseq_resnums))\n\n        # Get the chain subseq so we can save letter annotations later\n        chain_subseq = protein.get_representative_chain.get_subsequence(all_subseq_resnums)\n\n        # Now get the subsequence of the repseq and return it\n        mapping_dict = protein.map_structprop_resnums_to_seqprop_resnums(resnums=all_subseq_resnums,\n                                                                         structprop=protein.representative_structure,\n                                                                         chain_id=protein.representative_chain,\n                                                                         seqprop=protein.representative_sequence)\n        sub_id = '{}-{}->{}_within_{}_{}_extracted'.format(protein.representative_structure.id,\n                                                           protein.representative_chain,\n                                                           protein.representative_sequence.id,\n                                                           within, prop_name)\n        seqprop_resnums = [v for k, v in mapping_dict.items()]\n        new_sp = protein.representative_sequence.get_subsequence(resnums=seqprop_resnums, new_id=sub_id,\n                                                                 copy_letter_annotations=False)\n        try:\n            new_sp.letter_annotations = chain_subseq.letter_annotations\n        except (AttributeError, TypeError):\n            log.error('Protein {}, StructProp {}-{}: unable to copy chain subseq letter annotations'.format(protein.id,\n                                                                                                            protein.representative_structure.id,\n                                                                                                            protein.representative_chain))\n\n        return {'subseq_len': len(seqprop_resnums), 'subseq': new_sp, 'subseq_resnums': seqprop_resnums}", "response": "Get a generic subsequence from a REPSEQ based on a feature stored in REPSTRUCT and within the set distance in REPSTRUCT. REPCHAIN."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_combo_subseq_within_2_5D(protein, props, within, filter_resnums=None):\n        if not protein.representative_structure:\n            log.error('{}: no representative structure, cannot search for subseq'.format(protein.id))\n            return {'subseq_len': 0, 'subseq': None, 'subseq_resnums': []}\n\n        all_resnums = []\n        for prop in props:\n            tmp_results = protein.get_generic_subseq_within_2_5D(prop_name=prop, within=within,\n                                                         filter_resnums=filter_resnums)\n            all_resnums.extend(tmp_results['subseq_resnums'])\n        final_resnums = list(set(all_resnums))\n        sub_id = '{}-{}->{}_within_{}_{}_extracted'.format(protein.representative_structure.id,\n                                                           protein.representative_chain,\n                                                           protein.representative_sequence.id,\n                                                           within, props)\n        new_sp = protein.representative_sequence.get_subsequence(resnums=final_resnums, new_id=sub_id,\n                                                                 copy_letter_annotations=False)\n        return {'subseq_len': len(final_resnums), 'subseq': new_sp, 'subseq_resnums': final_resnums}", "response": "Get a subsequence from REPSEQ based on multiple features stored in REPSTRUCT. REPCHAIN and within the set distance in REPSTRUCT. REPCHAIN and within the set distance in REPSTRUCT. REPCHAIN."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_surface_subseq_3D(protein,\n                              depth_prop='RES_DEPTH-msms', depth_cutoff=2.5, depth_condition='<',\n                              acc_prop='RSA_ALL-freesasa_het', acc_cutoff=25, acc_condition='>'):\n        \"\"\"SURFACE 3D = NOTDEEP + ACC\"\"\"\n        empty = {'surface_3D': {'subseq_len'    : 0, 'subseq': None,\n                                   'subseq_resnums': []},\n                    'notdeep_3D': {'subseq_len'    : 0, 'subseq': None,\n                                   'subseq_resnums': []},\n                    'acc_3D'    : {'subseq_len'    : 0, 'subseq': None,\n                                   'subseq_resnums': []}}\n        if not protein.representative_structure:\n            log.error('{}: no representative structure, cannot search for subseq'.format(protein.id))\n            return empty\n\n        notdeep_subseq, notdeep_subseq_resnums = protein.get_seqprop_subsequence_from_structchain_property(\n            property_key=depth_prop,\n            property_value=depth_cutoff,\n            condition=depth_condition,\n            use_representatives=True,\n            return_resnums=True) or (None, [])\n\n        acc_subseq, acc_subseq_resnums = protein.get_seqprop_subsequence_from_structchain_property(\n            property_key=acc_prop,\n            property_value=acc_cutoff,\n            condition=acc_condition,\n            use_representatives=True,\n            return_resnums=True) or (None, [])\n\n        surface_subseq_resnums = list(set(notdeep_subseq_resnums).intersection(acc_subseq_resnums))\n        surface_subseq = protein.representative_sequence.get_subsequence(surface_subseq_resnums)\n\n        all_info = {'surface_3D': {'subseq_len'    : len(surface_subseq_resnums), 'subseq': surface_subseq,\n                                   'subseq_resnums': surface_subseq_resnums},\n                    'notdeep_3D': {'subseq_len'    : len(notdeep_subseq_resnums), 'subseq': notdeep_subseq,\n                                   'subseq_resnums': notdeep_subseq_resnums},\n                    'acc_3D'    : {'subseq_len'    : len(acc_subseq_resnums), 'subseq': acc_subseq,\n                                   'subseq_resnums': acc_subseq_resnums}}\n        return all_info", "response": "returns a dictionary of all subsequences in the 3D representation of the given structurechain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_disorder_subseq_3D(protein,\n                               pdbflex_keys_file,\n                               disorder_cutoff=2,\n                               disorder_condition='>'):\n        \"\"\"DISORDERED REGION 3D\"\"\"\n        with open(pdbflex_keys_file, 'r') as f:\n            pdbflex_keys = json.load(f)\n\n        if protein.id not in pdbflex_keys:\n            log.warning('{}: no PDBFlex info available'.format(protein.id))\n            final_repseq_sub, final_repseq_sub_resnums = (None, [])\n\n        else:\n            # Gather disordered regions for all mapped PDBFlex keys -- gets maximum disorder\n            # TODO: should add option to do the opposite (get consensus disorder)\n            repseq_sub_resnums_all = []\n            for disorder_prop in pdbflex_keys[protein.id]:\n                repseq_sub_raw, repseq_sub_resnums_raw = protein.representative_sequence.get_subsequence_from_property(\n                        property_key=disorder_prop,\n                        property_value=disorder_cutoff,\n                        condition=disorder_condition,\n                        return_resnums=True) or (None, [])\n                repseq_sub_resnums_all.extend(repseq_sub_resnums_raw)\n\n            final_repseq_sub_resnums = list(set(repseq_sub_resnums_all))\n            final_repseq_sub = protein.representative_sequence.get_subsequence(resnums=final_repseq_sub_resnums)\n\n        return {'subseq_len'    : len(final_repseq_sub_resnums), 'subseq': final_repseq_sub,\n                'subseq_resnums': final_repseq_sub_resnums}", "response": "Get the disordered subsequence for a given protein."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_disorder_predictions(self, iupred_path='/home/nathan/software/iupred/',\n                                          iupred_exec='iupred', disembl_cmd='/home/nathan/software/DisEMBL-1.4/DisEMBL.py',\n                                     representative_only=True):\n        \"\"\"Run Biopython ProteinAnalysis and EMBOSS pepstats to summarize basic statistics of the protein sequences.\n        Results are stored in the protein's respective SeqProp objects at ``.annotations``\n\n        Args:\n            representative_only (bool): If analysis should only be run on the representative sequence\n\n        \"\"\"\n        if representative_only:\n            # Check if a representative sequence was set\n            if not self.representative_sequence:\n                log.warning('{}: no representative sequence set, cannot get disorder properties'.format(self.id))\n                return\n\n            # Also need to check if a sequence has been stored\n            if not self.representative_sequence.seq:\n                log.warning('{}: representative sequence {} set, but no sequence stored. '\n                            'Cannot get disorder properties.'.format(self.id, self.representative_sequence.id))\n                return\n\n            # self.representative_sequence.store_iupred_disorder_predictions(iupred_path=iupred_path,\n            #                                                                iupred_exec=iupred_exec,\n            #                                                                prediction_type='long')\n            # self.representative_sequence.store_iupred_disorder_predictions(iupred_path=iupred_path,\n            #                                                                iupred_exec=iupred_exec,\n            #                                                                prediction_type='short')\n            self.representative_sequence.store_disembl_disorder_predictions(disembl_cmd=disembl_cmd)\n\n        if not representative_only:\n            for s in self.sequences:\n                # Need to check if a sequence has been stored\n                if not s.seq:\n                    log.warning('{}: no sequence stored. '\n                                'Cannot get disorder properties.'.format(s.id))\n                    continue\n\n                else:\n                    # s.store_iupred_disorder_predictions(iupred_path=iupred_path,\n                    #                                     iupred_exec=iupred_exec,\n                    #                                     prediction_type='long')\n                    # s.store_iupred_disorder_predictions(iupred_path=iupred_path,\n                    #                                     iupred_exec=iupred_exec,\n                    #                                     prediction_type='short')\n                    s.store_disembl_disorder_predictions(disembl_cmd=disembl_cmd)", "response": "Run Biopython ProteinAnalysis and EMBOSS pepstats to summarize basic statistics of the protein sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the main init. dat file which contains the modeling results", "response": "def parse_init_dat(infile):\n    \"\"\"Parse the main init.dat file which contains the modeling results\n\n    The first line of the file init.dat contains stuff like::\n    \n        \"120 easy  40   8\"\n    \n    The other lines look like this::\n    \n        \"     161   11.051   1  1guqA MUSTER\"\n    \n    and getting the first 10 gives you the top 10 templates used in modeling\n\n    Args:\n        infile (stt): Path to init.dat\n\n    Returns:\n        dict: Dictionary of parsed information\n\n    \"\"\"\n\n    # TODO: would be nice to get top 10 templates instead of just the top\n    init_dict = {}\n\n    log.debug('{}: reading file...'.format(infile))\n    with open(infile, 'r') as f:\n        # Get first 2 lines of file\n        head = [next(f).strip() for x in range(2)]\n\n    summary = head[0].split()\n    difficulty = summary[1]\n\n    top_template_info = head[1].split()\n    top_template_pdbchain = top_template_info[3]\n    top_template_pdb = top_template_pdbchain[:4]\n    top_template_chain = top_template_pdbchain[4:]\n\n    init_dict['difficulty'] = difficulty\n    init_dict['top_template_pdb'] = top_template_pdb\n    init_dict['top_template_chain'] = top_template_chain\n\n    return init_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_cscore(infile):\n\n    cscore_dict = {}\n\n    with open(infile, 'r') as f:\n        for ll in f.readlines():\n            # Look for the first line that starts with model1\n            if ll.lower().startswith('model1'):\n                l = ll.split()\n\n                cscore = l[1]\n                tmscore_full = l[2].split('+-')\n                tmscore = tmscore_full[0]\n                tmscore_err = tmscore_full[1]\n                rmsd_full = l[3].split('+-')\n                rmsd = rmsd_full[0]\n                rmsd_err = rmsd_full[1]\n\n                cscore_dict['c_score'] = float(cscore)\n                cscore_dict['tm_score'] = float(tmscore)\n                cscore_dict['tm_score_err'] = float(tmscore_err)\n                cscore_dict['rmsd'] = float(rmsd)\n                cscore_dict['rmsd_err'] = float(rmsd_err)\n\n    return cscore_dict", "response": "Parse the cscore file to return a dictionary of scores."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_coach_bsites_inf(infile):\n\n    bsites_results = []\n\n    with open(infile) as pp:\n        lines = list(filter(None, (line.rstrip() for line in pp)))\n\n    for i in range(len(lines) // 3):\n        bsites_site_dict = {}\n\n        line1 = lines[i * 3].split('\\t')\n        line2 = lines[i * 3 + 1].split('\\t')\n        line3 = lines[i * 3 + 2]\n\n        bsites_site_dict['site_num'] = line1[0]\n        bsites_site_dict['c_score'] = float(line1[1])\n        bsites_site_dict['cluster_size'] = line1[2]\n\n        bsites_site_dict['algorithm'] = line2[0]\n        bsites_site_dict['pdb_template_id'] = line2[1][:4]\n        bsites_site_dict['pdb_template_chain'] = line2[1][4]\n        bsites_site_dict['pdb_ligand'] = line2[2]\n        bsites_site_dict['binding_location_coords'] = tuple(float(x) for x in line2[3].split())\n\n        # TODO: what's the difference between this c-score and the cluster's c-score?\n        # how is the cluster's c-score computed? it's not the average c-score of all methods\n        # also why are some COFACTOR c-scores >1?\n        # 160411 - seems like the COFACTOR \"BS-score\" is being reported here, not its c-score...\n        tmp_split = line2[4].split(' :')\n        bsites_site_dict['c_score_method'] = tmp_split[0]\n        bsites_site_dict['binding_residues'] = tmp_split[1]\n\n        bsites_site_dict['ligand_cluster_counts'] = line3\n\n        bsites_results.append(bsites_site_dict)\n\n    return bsites_results", "response": "Parse the Bsites. inf output file of COACH and return a list of rank - ordered binding site predictions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the EC. dat output file of COACH and return a dataframe of predicted EC number and active residues.", "response": "def parse_coach_ec_df(infile):\n    \"\"\"Parse the EC.dat output file of COACH and return a dataframe of results\n\n    EC.dat contains the predicted EC number and active residues.\n    The columns are: PDB_ID, TM-score, RMSD, Sequence identity,\n    Coverage, Confidence score, EC number, and Active site residues\n\n    Args:\n        infile (str): Path to EC.dat\n\n    Returns:\n        DataFrame: Pandas DataFrame summarizing EC number predictions\n\n    \"\"\"\n\n    ec_df = pd.read_table(infile, delim_whitespace=True,\n                          names=['pdb_template', 'tm_score', 'rmsd', 'seq_ident', 'seq_coverage',\n                                 'c_score', 'ec_number', 'binding_residues'])\n\n    ec_df['pdb_template_id'] = ec_df['pdb_template'].apply(lambda x: x[:4])\n    ec_df['pdb_template_chain'] = ec_df['pdb_template'].apply(lambda x: x[4])\n\n    ec_df = ec_df[['pdb_template_id', 'pdb_template_chain', 'tm_score', 'rmsd',\n                   'seq_ident', 'seq_coverage', 'c_score', 'ec_number', 'binding_residues']]\n    ec_df['c_score'] = pd.to_numeric(ec_df.c_score, errors='coerce')\n\n    return ec_df"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a COACH GO output file from COACH and return a rank - ordered list of GO terms predictions", "response": "def parse_coach_go(infile):\n    \"\"\"Parse a GO output file from COACH and return a rank-ordered list of GO term predictions\n\n    The columns in all files are: GO terms, Confidence score, Name of GO terms. The files are:\n        \n        - GO_MF.dat - GO terms in 'molecular function'\n        - GO_BP.dat - GO terms in 'biological process'\n        - GO_CC.dat - GO terms in 'cellular component'\n\n    Args:\n        infile (str): Path to any COACH GO prediction file\n\n    Returns:\n        Pandas DataFrame: Organized dataframe of results, columns defined below\n            \n            - ``go_id``: GO term ID\n            - ``go_term``: GO term text\n            - ``c_score``: confidence score of the GO prediction\n\n    \"\"\"\n    go_list = []\n\n    with open(infile) as go_file:\n        for line in go_file.readlines():\n            go_dict = {}\n\n            go_split = line.split()\n            go_dict['go_id'] = go_split[0]\n            go_dict['c_score'] = go_split[1]\n            go_dict['go_term'] = ' '.join(go_split[2:])\n\n            go_list.append(go_dict)\n\n    return go_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy the raw information from I - TASSER modeling to a new folder.", "response": "def copy_results(self, copy_to_dir, rename_model_to=None, force_rerun=False):\n        \"\"\"Copy the raw information from I-TASSER modeling to a new folder.\n\n        Copies all files in the list _attrs_to_copy.\n\n        Args:\n            copy_to_dir (str): Directory to copy the minimal set of results per sequence.\n            rename_model_to (str): New file name (without extension)\n            force_rerun (bool): If existing models and results should be overwritten.\n\n        \"\"\"\n        # Save path to the structure and copy it if specified\n        if not rename_model_to:\n            rename_model_to = self.model_to_use\n\n        new_model_path = op.join(copy_to_dir, '{}.pdb'.format(rename_model_to))\n\n        if self.structure_path:\n            if ssbio.utils.force_rerun(flag=force_rerun, outfile=new_model_path):\n                # Clean and save it\n                custom_clean = CleanPDB()\n                my_pdb = StructureIO(self.structure_path)\n                new_model_path = my_pdb.write_pdb(custom_selection=custom_clean,\n                                                  custom_name=rename_model_to,\n                                                  out_dir=copy_to_dir,\n                                                  force_rerun=force_rerun)\n\n            # Update the structure_path to be the new clean file\n            self.load_structure_path(structure_path=new_model_path, file_type='pdb')\n\n            # Other modeling results - store in a new folder\n            dest_itasser_dir = op.join(copy_to_dir, '{}_itasser'.format(rename_model_to))\n            if not op.exists(dest_itasser_dir):\n                os.mkdir(dest_itasser_dir)\n\n            for attr in self._attrs_to_copy:\n                old_file_path = getattr(self, attr)\n                new_file_path = op.join(dest_itasser_dir, op.basename(old_file_path))\n                if ssbio.utils.force_rerun(flag=force_rerun, outfile=new_file_path):\n                    shutil.copy2(old_file_path, new_file_path)\n                    log.debug('{}: copied from {}'.format(new_file_path, old_file_path))\n                else:\n                    log.debug('{}: file already exists'.format(new_file_path))\n                setattr(self, attr, new_file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsummarizes the I - TASSER run in a dictionary containing modeling results and top predictions from COACH.", "response": "def get_dict(self, only_attributes=None, exclude_attributes=None, df_format=False):\n        \"\"\"Summarize the I-TASSER run in a dictionary containing modeling results and top predictions from COACH\n\n        Args:\n            only_attributes (str, list): Attributes that should be returned. If not provided, all are returned.\n            exclude_attributes (str, list): Attributes that should be excluded.\n            df_format (bool): If dictionary values should be formatted for a dataframe\n                (everything possible is transformed into strings, int, or float -\n                if something can't be transformed it is excluded)\n\n        Returns:\n            dict: Dictionary of attributes\n\n        \"\"\"\n\n        to_exclude = ['coach_bsites', 'coach_ec', 'coach_go_mf', 'coach_go_bp', 'coach_go_cc']\n        if not exclude_attributes:\n            excluder = to_exclude\n        else:\n            excluder = ssbio.utils.force_list(exclude_attributes)\n            excluder.extend(to_exclude)\n\n        summary_dict = StructProp.get_dict(self, only_attributes=only_attributes,\n                                           exclude_attributes=excluder,\n                                           df_format=df_format)\n\n        if self.coach_bsites:\n            tmp = {'top_bsite_' + k:v for k, v in self.coach_bsites[0].items()}\n            summary_dict.update(tmp)\n\n        if self.coach_ec:\n            tmp = {'top_ec_' + k: v for k, v in self.coach_ec[0].items()}\n            summary_dict.update(tmp)\n\n        if self.coach_go_mf:\n            tmp = {'top_go_mf_' + k: v for k, v in self.coach_go_mf[0].items()}\n            summary_dict.update(tmp)\n\n        if self.coach_go_bp:\n            tmp = {'top_go_bp_' + k: v for k, v in self.coach_go_bp[0].items()}\n            summary_dict.update(tmp)\n\n        if self.coach_go_cc:\n            tmp = {'top_go_cc_' + k: v for k, v in self.coach_go_cc[0].items()}\n            summary_dict.update(tmp)\n\n        return summary_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a COBRApy Model into the GEM - PRO project.", "response": "def load_cobra_model(self, model):\n        \"\"\"Load a COBRApy Model object into the GEM-PRO project.\n\n        Args:\n            model (Model): COBRApy ``Model`` object\n\n        \"\"\"\n        self.model = ModelPro(model)\n        for g in self.model.genes:\n            if self.genes_dir:\n                g.root_dir = self.genes_dir\n            g.protein.pdb_file_type = self.pdb_file_type\n        self.genes = self.model.genes\n\n        log.info('{}: loaded model'.format(model.id))\n        log.info('{}: number of reactions'.format(len(self.model.reactions)))\n        log.info('{}: number of reactions linked to a gene'.format(ssbio.core.modelpro.true_num_reactions(self.model)))\n        log.info('{}: number of genes (excluding spontaneous)'.format(ssbio.core.modelpro.true_num_genes(self.model,\n                                                                                                         custom_spont_id=self.custom_spont_id)))\n        log.info('{}: number of metabolites'.format(len(self.model.metabolites)))\n        log.warning('IMPORTANT: All Gene objects have been transformed into GenePro '\n                    'objects, and will be for any new ones')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef genes_with_structures(self):\n        return DictList(x for x in self.genes if x.protein.num_structures > 0)", "response": "Returns a DictList containing all genes with any mapped protein structures."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a DictList of all genes that have at least one experimental structure.", "response": "def genes_with_experimental_structures(self):\n        \"\"\"DictList: All genes that have at least one experimental structure.\"\"\"\n        return DictList(x for x in self.genes_with_structures if x.protein.num_structures_experimental > 0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef genes_with_a_representative_sequence(self):\n        return DictList(x for x in self.genes if x.protein.representative_sequence)", "response": "Returns a DictList of all genes with a representative sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef genes_with_a_representative_structure(self):\n        tmp = DictList(x for x in self.genes if x.protein.representative_structure)\n        return DictList(y for y in tmp if y.protein.representative_structure.structure_file)", "response": "DictList: All genes with a representative protein structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_gene_ids(self, genes_list):\n        orig_num_genes = len(self.genes)\n\n        for g in list(set(genes_list)):\n            if not self.genes.has_id(g):\n                new_gene = GenePro(id=g, pdb_file_type=self.pdb_file_type, root_dir=self.genes_dir)\n                if self.model:\n                    self.model.genes.append(new_gene)\n                else:\n                    self.genes.append(new_gene)\n\n        log.info('Added {} genes to GEM-PRO project'.format(len(self.genes)-orig_num_genes))", "response": "Add gene IDs manually into the GEM - PRO project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kegg_mapping_and_metadata(self, kegg_organism_code, custom_gene_mapping=None, outdir=None,\n                                  set_as_representative=False, force_rerun=False):\n        \"\"\"Map all genes in the model to KEGG IDs using the KEGG service.\n\n        Steps:\n            1. Download all metadata and sequence files in the sequences directory\n            2. Creates a KEGGProp object in the protein.sequences attribute\n            3. Returns a Pandas DataFrame of mapping results\n\n        Args:\n            kegg_organism_code (str): The three letter KEGG code of your organism\n            custom_gene_mapping (dict): If your model genes differ from the gene IDs you want to map,\n                custom_gene_mapping allows you to input a dictionary which maps model gene IDs to new ones.\n                Dictionary keys must match model gene IDs.\n            outdir (str): Path to output directory of downloaded files, must be set if GEM-PRO directories\n                were not created initially\n            set_as_representative (bool): If mapped KEGG IDs should be set as representative sequences\n            force_rerun (bool): If you want to overwrite any existing mappings and files\n\n        \"\"\"\n\n        # First map all of the organism's KEGG genes to UniProt\n        kegg_to_uniprot = ssbio.databases.kegg.map_kegg_all_genes(organism_code=kegg_organism_code, target_db='uniprot')\n\n        successfully_mapped_counter = 0\n\n        for g in tqdm(self.genes):\n            if custom_gene_mapping:\n                kegg_g = custom_gene_mapping[g.id]\n            else:\n                kegg_g = g.id\n\n            if kegg_g not in kegg_to_uniprot:\n                log.debug('{}: unable to map to KEGG'.format(g.id))\n                continue\n\n            # Download both FASTA and KEGG metadata files\n            kegg_prop = g.protein.load_kegg(kegg_id=kegg_g, kegg_organism_code=kegg_organism_code,\n                                            download=True, outdir=outdir, set_as_representative=set_as_representative,\n                                            force_rerun=force_rerun)\n\n            # Update potentially old UniProt ID\n            if kegg_g in kegg_to_uniprot.keys():\n                kegg_prop.uniprot = kegg_to_uniprot[kegg_g]\n                if g.protein.representative_sequence:\n                    if g.protein.representative_sequence.kegg == kegg_prop.kegg:\n                        g.protein.representative_sequence.uniprot = kegg_to_uniprot[kegg_g]\n\n            # Keep track of missing mappings - missing is defined by no available sequence\n            if kegg_prop.sequence_file:\n                successfully_mapped_counter += 1\n\n            log.debug('{}: loaded KEGG information for gene'.format(g.id))\n\n        log.info('{}/{}: number of genes mapped to KEGG'.format(successfully_mapped_counter, len(self.genes)))\n        log.info('Completed ID mapping --> KEGG. See the \"df_kegg_metadata\" attribute for a summary dataframe.')", "response": "Map all KEGG genes in the model to UniProt and download all metadata and sequences in the sequences attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kegg_mapping_and_metadata_parallelize(self, sc, kegg_organism_code, custom_gene_mapping=None, outdir=None,\n                                              set_as_representative=False, force_rerun=False):\n        \"\"\"Map all genes in the model to KEGG IDs using the KEGG service.\n\n        Steps:\n            1. Download all metadata and sequence files in the sequences directory\n            2. Creates a KEGGProp object in the protein.sequences attribute\n            3. Returns a Pandas DataFrame of mapping results\n\n        Args:\n            sc (SparkContext): Spark Context to parallelize this function\n            kegg_organism_code (str): The three letter KEGG code of your organism\n            custom_gene_mapping (dict): If your model genes differ from the gene IDs you want to map,\n                custom_gene_mapping allows you to input a dictionary which maps model gene IDs to new ones.\n                Dictionary keys must match model gene IDs.\n            outdir (str): Path to output directory of downloaded files, must be set if GEM-PRO directories\n                were not created initially\n            set_as_representative (bool): If mapped KEGG IDs should be set as representative sequences\n            force_rerun (bool): If you want to overwrite any existing mappings and files\n\n        \"\"\"\n\n        # First map all of the organism's KEGG genes to UniProt\n        kegg_to_uniprot = ssbio.databases.kegg.map_kegg_all_genes(organism_code=kegg_organism_code, target_db='uniprot')\n\n        # Parallelize the genes list\n        genes_rdd = sc.parallelize(self.genes)\n\n        # Write a sub-function to carry out the bulk of the original function\n        def gp_kegg_sc(g):\n            if custom_gene_mapping:\n                kegg_g = custom_gene_mapping[g.id]\n            else:\n                kegg_g = g.id\n\n            # Download both FASTA and KEGG metadata files\n            kegg_prop = g.protein.load_kegg(kegg_id=kegg_g, kegg_organism_code=kegg_organism_code,\n                                            download=True, outdir=outdir,\n                                            set_as_representative=set_as_representative,\n                                            force_rerun=force_rerun)\n\n            # Update potentially old UniProt ID\n            if kegg_g in kegg_to_uniprot.keys():\n                kegg_prop.uniprot = kegg_to_uniprot[kegg_g]\n                if g.protein.representative_sequence:\n                    if g.protein.representative_sequence.kegg == kegg_prop.kegg:\n                        g.protein.representative_sequence.uniprot = kegg_to_uniprot[kegg_g]\n\n            # Tracker for if it mapped successfully to KEGG\n            if kegg_prop.sequence_file:\n                success = True\n            else:\n                success = False\n\n            return g, success\n\n        # Run a map operation to execute the function on all items in the RDD\n        result = genes_rdd.map(gp_kegg_sc).collect()\n\n        # Copy the results over to the GEM-PRO object's genes using the GenePro function \"copy_modified_gene\"\n        # Also count how many genes mapped to KEGG\n        successfully_mapped_counter = 0\n        for modified_g, success in result:\n            original_gene = self.genes.get_by_id(modified_g.id)\n            original_gene.copy_modified_gene(modified_g)\n            if success:\n                successfully_mapped_counter += 1\n\n        log.info('{}/{}: number of genes mapped to KEGG'.format(successfully_mapped_counter, len(self.genes)))\n        log.info('Completed ID mapping --> KEGG. See the \"df_kegg_metadata\" attribute for a summary dataframe.')", "response": "Map all KEGG genes in the model to UniProt and create metadata files for each gene in the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists of genes with no mapping to KEGG.", "response": "def missing_kegg_mapping(self):\n        \"\"\"list: List of genes with no mapping to KEGG.\"\"\"\n        kegg_missing = []\n        for g in self.genes:\n            keggs = g.protein.filter_sequences(KEGGProp)\n            no_sequence_file_available = True\n            for k in keggs:\n                if k.sequence_file:\n                    no_sequence_file_available = False\n                    break\n            if no_sequence_file_available:\n                kegg_missing.append(g.id)\n        return list(set(kegg_missing))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap all genes in the model to UniProt IDs using UniProt mapping service.", "response": "def uniprot_mapping_and_metadata(self, model_gene_source, custom_gene_mapping=None, outdir=None,\n                                     set_as_representative=False, force_rerun=False):\n        \"\"\"Map all genes in the model to UniProt IDs using the UniProt mapping service.\n        Also download all metadata and sequences.\n\n        Args:\n            model_gene_source (str): the database source of your model gene IDs.\n                See: http://www.uniprot.org/help/api_idmapping\n                Common model gene sources are:\n\n                * Ensembl Genomes - ``ENSEMBLGENOME_ID`` (i.e. E. coli b-numbers)\n                * Entrez Gene (GeneID) - ``P_ENTREZGENEID``\n                * RefSeq Protein - ``P_REFSEQ_AC``\n\n            custom_gene_mapping (dict): If your model genes differ from the gene IDs you want to map,\n                custom_gene_mapping allows you to input a dictionary which maps model gene IDs to new ones.\n                Dictionary keys must match model genes.\n            outdir (str): Path to output directory of downloaded files, must be set if GEM-PRO directories\n                were not created initially\n            set_as_representative (bool): If mapped UniProt IDs should be set as representative sequences\n            force_rerun (bool): If you want to overwrite any existing mappings and files\n\n        \"\"\"\n\n        # Allow model gene --> custom ID mapping ({'TM_1012':'TM1012'})\n        if custom_gene_mapping:\n            genes_to_map = list(custom_gene_mapping.values())\n        else:\n            genes_to_map = [x.id for x in self.genes]\n\n        # Map all IDs first to available UniProts\n        genes_to_uniprots = bs_unip.mapping(fr=model_gene_source, to='ACC', query=genes_to_map)\n\n        successfully_mapped_counter = 0\n        for g in tqdm(self.genes):\n            if custom_gene_mapping and g.id in custom_gene_mapping.keys():\n                uniprot_gene = custom_gene_mapping[g.id]\n            else:\n                uniprot_gene = g.id\n\n            if uniprot_gene not in genes_to_uniprots:\n                log.debug('{}: unable to map to UniProt'.format(g.id))\n                continue\n\n            for mapped_uniprot in genes_to_uniprots[uniprot_gene]:\n                try:\n                    uniprot_prop = g.protein.load_uniprot(uniprot_id=mapped_uniprot, download=True, outdir=outdir,\n                                                          set_as_representative=set_as_representative,\n                                                          force_rerun=force_rerun)\n                except HTTPError as e:\n                    log.error('{}, {}: unable to complete web request'.format(g.id, mapped_uniprot))\n                    print(e)\n                    continue\n\n                if uniprot_prop.sequence_file or uniprot_prop.metadata_file:\n                    successfully_mapped_counter += 1\n\n        log.info('{}/{}: number of genes mapped to UniProt'.format(successfully_mapped_counter, len(self.genes)))\n        log.info('Completed ID mapping --> UniProt. See the \"df_uniprot_metadata\" attribute for a summary dataframe.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef manual_uniprot_mapping(self, gene_to_uniprot_dict, outdir=None, set_as_representative=True):\n        for g, u in tqdm(gene_to_uniprot_dict.items()):\n            g = str(g)\n            gene = self.genes.get_by_id(g)\n\n            try:\n                uniprot_prop = gene.protein.load_uniprot(uniprot_id=u,\n                                                         outdir=outdir, download=True,\n                                                         set_as_representative=set_as_representative)\n            except HTTPError as e:\n                log.error('{}, {}: unable to complete web request'.format(g, u))\n                print(e)\n                continue\n\n        log.info('Completed manual ID mapping --> UniProt. See the \"df_uniprot_metadata\" attribute for a summary dataframe.')", "response": "Read a dictionary of UniProt IDs and set them as representative sequences."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef df_uniprot_metadata(self):\n        uniprot_pre_df = []\n        df_cols = ['gene', 'uniprot', 'reviewed', 'gene_name', 'kegg', 'refseq', 'num_pdbs', 'pdbs', 'ec_number',\n                   'pfam', 'seq_len', 'description', 'entry_date', 'entry_version', 'seq_date', 'seq_version',\n                   'sequence_file', 'metadata_file']\n\n        for g in self.genes:\n            uniprot_mappings = g.protein.filter_sequences(UniProtProp)\n            for uniprot_prop in uniprot_mappings:\n                uniprot_dict = uniprot_prop.get_dict(df_format=True, only_attributes=df_cols)\n                uniprot_dict['gene'] = g.id\n                uniprot_pre_df.append(uniprot_dict)\n\n        df = pd.DataFrame.from_records(uniprot_pre_df, columns=df_cols).set_index('gene')\n        if df.empty:\n            log.warning('Empty dataframe')\n            return df\n        else:\n            return ssbio.utils.clean_df(df)", "response": "DataFrame of UniProt metadata per protein."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef missing_uniprot_mapping(self):\n        uniprot_missing = []\n        for g in self.genes:\n            ups = g.protein.filter_sequences(UniProtProp)\n\n            no_sequence_file_available = True\n            for u in ups:\n                if u.sequence_file or u.metadata_file:\n                    no_sequence_file_available = False\n                    break\n            if no_sequence_file_available:\n                uniprot_missing.append(g.id)\n\n        return list(set(uniprot_missing))", "response": "list of genes with no mapping to UniProt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef manual_seq_mapping(self, gene_to_seq_dict, outdir=None, write_fasta_files=True, set_as_representative=True):\n        if outdir:\n            outdir_set = True\n        else:\n            outdir_set = False\n\n        # Save the sequence information in individual FASTA files\n        for g, s in gene_to_seq_dict.items():\n            gene = self.genes.get_by_id(str(g))\n\n            if not outdir_set and write_fasta_files:\n                outdir = gene.protein.sequence_dir\n                if not outdir:\n                    raise ValueError('Output directory must be specified')\n\n            manual_info = gene.protein.load_manual_sequence(ident=g, seq=s, outdir=outdir,\n                                                            write_fasta_file=write_fasta_files,\n                                                            set_as_representative=set_as_representative)\n            log.debug('{}: loaded manually defined sequence information'.format(g))\n\n        log.info('Loaded in {} sequences'.format(len(gene_to_seq_dict)))", "response": "Read a dictionary of model gene IDs to their corresponding protein sequence strings. By default sets them as representative."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_representative_sequence(self, force_rerun=False):\n\n        # TODO: rethink use of multiple database sources - may lead to inconsistency with genome sources\n\n        successfully_mapped_counter = 0\n        for g in tqdm(self.genes):\n            repseq = g.protein.set_representative_sequence(force_rerun=force_rerun)\n\n            if repseq:\n                if repseq.sequence_file:\n                    successfully_mapped_counter += 1\n\n        log.info('{}/{}: number of genes with a representative sequence'.format(len(self.genes_with_a_representative_sequence),\n                                                                                len(self.genes)))\n        log.info('See the \"df_representative_sequences\" attribute for a summary dataframe.')", "response": "Set a single representative sequence for each protein in the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef df_representative_sequences(self):\n\n        seq_mapping_pre_df = []\n        df_cols = ['gene', 'uniprot', 'kegg', 'num_pdbs', 'pdbs', 'seq_len', 'sequence_file', 'metadata_file']\n\n        for g in self.genes_with_a_representative_sequence:\n            gene_dict = g.protein.representative_sequence.get_dict(df_format=True, only_attributes=df_cols)\n            gene_dict['gene'] = g.id\n            seq_mapping_pre_df.append(gene_dict)\n\n        df = pd.DataFrame.from_records(seq_mapping_pre_df, columns=df_cols).set_index('gene')\n        if df.empty:\n            log.warning('Empty dataframe')\n            return df\n        else:\n            return ssbio.utils.clean_df(df)", "response": "Pandas DataFrame of representative sequence information per protein."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists of genes with no mapping to a representative sequence", "response": "def missing_representative_sequence(self):\n        \"\"\"list: List of genes with no mapping to a representative sequence.\"\"\"\n        return [x.id for x in self.genes if not self.genes_with_a_representative_sequence.has_id(x.id)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_representative_sequences_file(self, outname, outdir=None, set_ids_from_model=True):\n\n        if not outdir:\n            outdir = self.data_dir\n            if not outdir:\n                raise ValueError('Output directory must be specified')\n\n        outfile = op.join(outdir, outname + '.faa')\n\n        tmp = []\n        for x in self.genes_with_a_representative_sequence:\n            repseq = x.protein.representative_sequence\n            copied_seq_record = copy(repseq)\n            if set_ids_from_model:\n                copied_seq_record.id = x.id\n            tmp.append(copied_seq_record)\n\n        SeqIO.write(tmp, outfile, \"fasta\")\n\n        log.info('{}: wrote all representative sequences to file'.format(outfile))\n        self.genome_path = outfile\n        return self.genome_path", "response": "Write all the model s representative sequences as a single FASTA file. By default sets IDs to model gene IDs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning Biopython ProteinAnalysis and EMBOSS pepstats to summarize basic statistics of all protein sequences.", "response": "def get_sequence_properties(self, clean_seq=False, representatives_only=True):\n        \"\"\"Run Biopython ProteinAnalysis and EMBOSS pepstats to summarize basic statistics of all protein sequences.\n        Results are stored in the protein's respective SeqProp objects at ``.annotations``\n\n        Args:\n            representative_only (bool): If analysis should only be run on the representative sequences\n\n        \"\"\"\n        for g in tqdm(self.genes):\n            g.protein.get_sequence_properties(clean_seq=clean_seq, representative_only=representatives_only)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns Biopython ProteinAnalysis and EMBOSS pepstats to summarize basic statistics of all protein sequences.", "response": "def get_sequence_sliding_window_properties(self, scale, window, representatives_only=True):\n        \"\"\"Run Biopython ProteinAnalysis and EMBOSS pepstats to summarize basic statistics of all protein sequences.\n        Results are stored in the protein's respective SeqProp objects at ``.annotations``\n\n        Args:\n            representative_only (bool): If analysis should only be run on the representative sequences\n\n        \"\"\"\n        for g in tqdm(self.genes):\n            g.protein.get_sequence_sliding_window_properties(scale=scale, window=window,\n                                                             representative_only=representatives_only)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun and parses the SCRATCH results to predict secondary structure and solvent accessibility.", "response": "def get_scratch_predictions(self, path_to_scratch, results_dir, scratch_basename='scratch', num_cores=1,\n                                exposed_buried_cutoff=25, custom_gene_mapping=None):\n        \"\"\"Run and parse ``SCRATCH`` results to predict secondary structure and solvent accessibility.\n        Annotations are stored in the protein's representative sequence at:\n\n            * ``.annotations``\n            * ``.letter_annotations``\n\n        Args:\n            path_to_scratch (str): Path to SCRATCH executable\n            results_dir (str): Path to SCRATCH results folder, which will have the files (scratch.ss, scratch.ss8,\n                scratch.acc, scratch.acc20)\n            scratch_basename (str): Basename of the SCRATCH results ('scratch' is default)\n            num_cores (int): Number of cores to use to parallelize SCRATCH run\n            exposed_buried_cutoff (int): Cutoff of exposed/buried for the acc20 predictions\n            custom_gene_mapping (dict): Default parsing of SCRATCH output files is to look for the model gene IDs. If\n                your output files contain IDs which differ from the model gene IDs, use this dictionary to map model\n                gene IDs to result file IDs. Dictionary keys must match model genes.\n\n        \"\"\"\n        if not self.genome_path:\n            # Write all sequences as one file\n            all_seqs = self.write_representative_sequences_file(outname=self.id)\n\n        # Runs SCRATCH or loads existing results in results_dir\n        scratch = SCRATCH(project_name=scratch_basename, seq_file=self.genome_path)\n        scratch.run_scratch(path_to_scratch=path_to_scratch, num_cores=num_cores, outdir=results_dir)\n        sspro_summary = scratch.sspro_summary()\n        sspro8_summary = scratch.sspro8_summary()\n        sspro_results = scratch.sspro_results()\n        sspro8_results = scratch.sspro8_results()\n        accpro_summary = scratch.accpro_summary()\n        accpro20_summary = scratch.accpro20_summary(exposed_buried_cutoff)\n        accpro_results = scratch.accpro_results()\n        accpro20_results = scratch.accpro20_results()\n        counter = 0\n\n        # Adding the scratch annotations to the representative_sequences letter_annotations\n        for g in tqdm(self.genes_with_a_representative_sequence):\n            if custom_gene_mapping:\n                g_id = custom_gene_mapping[g.id]\n            else:\n                g_id = g.id\n\n            if g_id in sspro_summary:\n                # Secondary structure\n                g.protein.representative_sequence.annotations.update(sspro_summary[g_id])\n                g.protein.representative_sequence.annotations.update(sspro8_summary[g_id])\n                try:\n                    g.protein.representative_sequence.letter_annotations['SS-sspro'] = sspro_results[g_id]\n                    g.protein.representative_sequence.letter_annotations['SS-sspro8'] = sspro8_results[g_id]\n                except TypeError:\n                    log.error('Gene {}, SeqProp {}: sequence length mismatch between SCRATCH results and representative '\n                              'sequence, unable to set letter annotation'.format(g_id, g.protein.representative_sequence.id))\n\n                # Solvent accessibility\n                g.protein.representative_sequence.annotations.update(accpro_summary[g_id])\n                g.protein.representative_sequence.annotations.update(accpro20_summary[g_id])\n                try:\n                    g.protein.representative_sequence.letter_annotations['RSA-accpro'] = accpro_results[g_id]\n                    g.protein.representative_sequence.letter_annotations['RSA-accpro20'] = accpro20_results[g_id]\n                except TypeError:\n                    log.error('Gene {}, SeqProp {}: sequence length mismatch between SCRATCH results and representative '\n                              'sequence, unable to set letter annotation'.format(g_id, g.protein.representative_sequence.id))\n\n                counter += 1\n            else:\n                log.error('{}: missing SCRATCH results'.format(g.id))\n\n        log.info('{}/{}: number of genes with SCRATCH predictions loaded'.format(counter, len(self.genes)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tmhmm_predictions(self, tmhmm_results, custom_gene_mapping=None):\n        # TODO: refactor to Protein class?\n        tmhmm_dict = ssbio.protein.sequence.properties.tmhmm.parse_tmhmm_long(tmhmm_results)\n\n        counter = 0\n        for g in tqdm(self.genes_with_a_representative_sequence):\n            if custom_gene_mapping:\n                g_id = custom_gene_mapping[g.id]\n            else:\n                g_id = g.id\n\n            if g_id in tmhmm_dict:\n                log.debug('{}: loading TMHMM results'.format(g.id))\n                if not tmhmm_dict[g_id]:\n                    log.error(\"{}: missing TMHMM results\".format(g.id))\n                g.protein.representative_sequence.annotations['num_tm_helix-tmhmm'] = tmhmm_dict[g_id]['num_tm_helices']\n                try:\n                    g.protein.representative_sequence.letter_annotations['TM-tmhmm'] = tmhmm_dict[g_id]['sequence']\n                    counter += 1\n                except TypeError:\n                    log.error('Gene {}, SeqProp {}: sequence length mismatch between TMHMM results and representative '\n                              'sequence, unable to set letter annotation'.format(g_id, g.protein.representative_sequence.id))\n            else:\n                log.error(\"{}: missing TMHMM results\".format(g.id))\n\n        log.info('{}/{}: number of genes with TMHMM predictions loaded'.format(counter, len(self.genes)))", "response": "Parse TMHMM results and store in the representative sequences file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nblast each representative protein sequence to the PDB. Saves raw BLAST results.", "response": "def blast_seqs_to_pdb(self, seq_ident_cutoff=0, evalue=0.0001, all_genes=False, display_link=False,\n                          outdir=None, force_rerun=False):\n        \"\"\"BLAST each representative protein sequence to the PDB. Saves raw BLAST results (XML files).\n\n        Args:\n            seq_ident_cutoff (float, optional): Cutoff results based on percent coverage (in decimal form)\n            evalue (float, optional): Cutoff for the E-value - filters for significant hits. 0.001 is liberal,\n                0.0001 is stringent (default).\n            all_genes (bool): If all genes should be BLASTed, or only those without any structures currently mapped\n            display_link (bool, optional): Set to True if links to the HTML results should be displayed\n            outdir (str): Path to output directory of downloaded files, must be set if GEM-PRO directories\n                were not created initially\n            force_rerun (bool, optional): If existing BLAST results should not be used, set to True. Default is False\n\n        \"\"\"\n        counter = 0\n\n        for g in tqdm(self.genes_with_a_representative_sequence):\n            # If all_genes=False, BLAST only genes without a uniprot -> pdb mapping\n            if g.protein.num_structures_experimental > 0 and not all_genes and not force_rerun:\n                log.debug('{}: skipping BLAST, {} experimental structures already mapped '\n                          'and all_genes flag is False'.format(g.id,\n                                                               g.protein.num_structures_experimental))\n                continue\n\n            # BLAST the sequence to the PDB\n            new_pdbs = g.protein.blast_representative_sequence_to_pdb(seq_ident_cutoff=seq_ident_cutoff,\n                                                                   evalue=evalue,\n                                                                   display_link=display_link,\n                                                                   outdir=outdir,\n                                                                   force_rerun=force_rerun)\n\n            if new_pdbs:\n                counter += 1\n                log.debug('{}: {} PDBs BLASTed'.format(g.id, len(new_pdbs)))\n            else:\n                log.debug('{}: no BLAST results'.format(g.id))\n\n        log.info('Completed sequence --> PDB BLAST. See the \"df_pdb_blast\" attribute for a summary dataframe.')\n        log.info('{}: number of genes with additional structures added from BLAST'.format(counter))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map_uniprot_to_pdb(self, seq_ident_cutoff=0.0, outdir=None, force_rerun=False):\n\n        # First get all UniProt IDs and check if they have PDBs\n        all_representative_uniprots = []\n        for g in self.genes_with_a_representative_sequence:\n            uniprot_id = g.protein.representative_sequence.uniprot\n            if uniprot_id:\n                # TODO: add warning or something for isoform ids?\n                if '-' in uniprot_id:\n                    uniprot_id = uniprot_id.split('-')[0]\n                all_representative_uniprots.append(uniprot_id)\n        log.info('Mapping UniProt IDs --> PDB IDs...')\n        uniprots_to_pdbs = bs_unip.mapping(fr='ACC', to='PDB_ID', query=all_representative_uniprots)\n\n        counter = 0\n        # Now run the best_structures API for all genes\n        for g in tqdm(self.genes_with_a_representative_sequence):\n            uniprot_id = g.protein.representative_sequence.uniprot\n            if uniprot_id:\n                if '-' in uniprot_id:\n                    uniprot_id = uniprot_id.split('-')[0]\n                if uniprot_id in uniprots_to_pdbs:\n                    best_structures = g.protein.map_uniprot_to_pdb(seq_ident_cutoff=seq_ident_cutoff, outdir=outdir, force_rerun=force_rerun)\n                    if best_structures:\n                        counter += 1\n                        log.debug('{}: {} PDBs mapped'.format(g.id, len(best_structures)))\n                else:\n                    log.debug('{}, {}: no PDBs available'.format(g.id, uniprot_id))\n\n        log.info('{}/{}: number of genes with at least one experimental structure'.format(len(self.genes_with_experimental_structures),\n                                                                                          len(self.genes)))\n        log.info('Completed UniProt --> best PDB mapping. See the \"df_pdb_ranking\" attribute for a summary dataframe.')", "response": "Map all representative sequences UniProt IDs to PDB IDs using the PDBe Best Structures API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a dataframe of UniProt - > best structure in PDB results", "response": "def df_pdb_ranking(self):\n        \"\"\"DataFrame: Get a dataframe of UniProt -> best structure in PDB results\"\"\"\n        df = pd.DataFrame()\n        for g in self.genes_with_experimental_structures:\n            protein_df = g.protein.df_pdb_ranking.copy().reset_index()\n            if not protein_df.empty:\n                protein_df['gene'] = g.id\n                df = df.append(protein_df)\n        if df.empty:\n            log.warning('Empty dataframe')\n            return df\n        else:\n            return ssbio.utils.clean_df(df.set_index('gene'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist of genes with no mapping to any experimental PDB structures.", "response": "def missing_pdb_structures(self):\n        \"\"\"list: List of genes with no mapping to any experimental PDB structure.\"\"\"\n        return [x.id for x in self.genes if not self.genes_with_experimental_structures.has_id(x.id)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying homology models from the GEM - PRO project to the GEM - PRO project.", "response": "def get_manual_homology_models(self, input_dict, outdir=None, clean=True, force_rerun=False):\n        \"\"\"Copy homology models to the GEM-PRO project.\n\n        Requires an input of a dictionary formatted like so::\n\n            {\n                model_gene: {\n                                homology_model_id1: {\n                                                        'model_file': '/path/to/homology/model.pdb',\n                                                        'file_type': 'pdb'\n                                                        'additional_info': info_value\n                                                    },\n                                homology_model_id2: {\n                                                        'model_file': '/path/to/homology/model.pdb'\n                                                        'file_type': 'pdb'\n                                                    }\n                            }\n            }\n\n        Args:\n            input_dict (dict): Dictionary of dictionaries of gene names to homology model IDs and other information\n            outdir (str): Path to output directory of downloaded files, must be set if GEM-PRO directories\n                were not created initially\n            clean (bool): If homology files should be cleaned and saved as a new PDB file\n            force_rerun (bool): If homology files should be copied again even if they exist in the GEM-PRO directory\n\n        \"\"\"\n        if outdir:\n            outdir_set = True\n        else:\n            outdir_set = False\n\n        counter = 0\n        for g in tqdm(self.genes):\n            if g.id not in input_dict:\n                continue\n\n            if not outdir_set:\n                outdir = g.protein.structure_dir\n                if not outdir:\n                    raise ValueError('Output directory must be specified')\n\n            for hid, hdict in input_dict[g.id].items():\n                if 'model_file' not in hdict or 'file_type' not in hdict:\n                    raise KeyError('\"model_file\" and \"file_type\" must be keys in the manual input dictionary.')\n\n                new_homology = g.protein.load_pdb(pdb_id=hid, pdb_file=hdict['model_file'],\n                                                  file_type=hdict['file_type'], is_experimental=False)\n\n                if clean:\n                    new_homology.load_structure_path(new_homology.clean_structure(outdir=outdir, force_rerun=force_rerun),\n                                                     hdict['file_type'])\n                else:\n                    copy_to = op.join(outdir, op.basename(hdict['model_file']))\n                    if ssbio.utils.force_rerun(force_rerun, copy_to):\n                        # Just copy the file to the structure directory and store the file name\n                        log.debug('{}: copying model from original directory to GEM-PRO directory'.format(op.basename(hdict['model_file'])))\n                        shutil.copy2(hdict['model_file'], outdir)\n                        new_homology.load_structure_path(copy_to, hdict['file_type'])\n                    else:\n                        log.debug('{}: homology model already copied to directory'.format(copy_to))\n                        new_homology.load_structure_path(copy_to, hdict['file_type'])\n\n                # TODO: need to better handle other info in the provided dictionary, if any\n                new_homology.update(hdict)\n\n                log.debug('{}: updated homology model information and copied model file.'.format(g.id))\n            counter += 1\n\n        log.info('Updated homology model information for {} genes.'.format(counter))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_itasser_models(self, homology_raw_dir, custom_itasser_name_mapping=None, outdir=None, force_rerun=False):\n        counter = 0\n\n        for g in tqdm(self.genes):\n            if custom_itasser_name_mapping and g.id in custom_itasser_name_mapping:\n                hom_id = custom_itasser_name_mapping[g.id]\n                if not op.exists(op.join(homology_raw_dir, hom_id)):\n                    hom_id = g.id\n            else:\n                hom_id = g.id\n\n            # The name of the actual pdb file will be $GENEID_model1.pdb\n            new_itasser_name = hom_id + '_model1'\n            orig_itasser_dir = op.join(homology_raw_dir, hom_id)\n\n            try:\n                itasser_prop = g.protein.load_itasser_folder(ident=hom_id, itasser_folder=orig_itasser_dir,\n                                                             organize=True, outdir=outdir,\n                                                             organize_name=new_itasser_name,\n                                                             force_rerun=force_rerun)\n            except OSError:\n                log.debug('{}: homology model folder unavailable'.format(g.id))\n                continue\n            except IOError:\n                log.debug('{}: homology model unavailable'.format(g.id))\n                continue\n\n            if itasser_prop.structure_file:\n                counter += 1\n            else:\n                log.debug('{}: homology model file unavailable, perhaps modelling did not finish'.format(g.id))\n\n        log.info('Completed copying of {} I-TASSER models to GEM-PRO directory. See the \"df_homology_models\" attribute for a summary dataframe.'.format(counter))", "response": "Copy generated I - TASSER models from a directory to the GEM - PRO directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists of genes with no homology models", "response": "def missing_homology_models(self):\n        \"\"\"list: List of genes with no mapping to any homology models.\"\"\"\n        return [x.id for x in self.genes if not self.genes_with_homology_models.has_id(x.id)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_representative_structure(self, seq_outdir=None, struct_outdir=None, pdb_file_type=None,\n                                     engine='needle', always_use_homology=False, rez_cutoff=0.0,\n                                     seq_ident_cutoff=0.5, allow_missing_on_termini=0.2,\n                                     allow_mutants=True, allow_deletions=False,\n                                     allow_insertions=False, allow_unresolved=True, skip_large_structures=False,\n                                     clean=True, force_rerun=False):\n        \"\"\"Set all representative structure for proteins from a structure in the structures attribute.\n\n        Each gene can have a combination of the following, which will be analyzed to set a representative structure.\n\n            * Homology model(s)\n            * Ranked PDBs\n            * BLASTed PDBs\n\n        If the ``always_use_homology`` flag is true, homology models are always set as representative when they exist.\n        If there are multiple homology models, we rank by the percent sequence coverage.\n\n        Args:\n            seq_outdir (str): Path to output directory of sequence alignment files, must be set if GEM-PRO directories\n                were not created initially\n            struct_outdir (str): Path to output directory of structure files, must be set if GEM-PRO directories\n                were not created initially\n            pdb_file_type (str): ``pdb``, ``mmCif``, ``xml``, ``mmtf`` - file type for files downloaded from the PDB\n            engine (str): ``biopython`` or ``needle`` - which pairwise alignment program to use.\n                ``needle`` is the standard EMBOSS tool to run pairwise alignments.\n                ``biopython`` is Biopython's implementation of needle. Results can differ!\n            always_use_homology (bool): If homology models should always be set as the representative structure\n            rez_cutoff (float): Resolution cutoff, in Angstroms (only if experimental structure)\n            seq_ident_cutoff (float): Percent sequence identity cutoff, in decimal form\n            allow_missing_on_termini (float): Percentage of the total length of the reference sequence which will be ignored\n                when checking for modifications. Example: if 0.1, and reference sequence is 100 AA, then only residues\n                5 to 95 will be checked for modifications.\n            allow_mutants (bool): If mutations should be allowed or checked for\n            allow_deletions (bool): If deletions should be allowed or checked for\n            allow_insertions (bool): If insertions should be allowed or checked for\n            allow_unresolved (bool): If unresolved residues should be allowed or checked for\n            skip_large_structures (bool): Default False -- currently, large structures can't be saved as a PDB file even\n                if you just want to save a single chain, so Biopython will throw an error when trying to do so. As an\n                alternative, if a large structure is selected as representative, the pipeline will currently point to it\n                and not clean it. If you don't want this to happen, set this to true.\n            clean (bool): If structures should be cleaned\n            force_rerun (bool): If sequence to structure alignment should be rerun\n\n        Todo:\n            - Remedy large structure representative setting\n\n        \"\"\"\n        for g in tqdm(self.genes):\n            repstruct = g.protein.set_representative_structure(seq_outdir=seq_outdir,\n                                                               struct_outdir=struct_outdir,\n                                                               pdb_file_type=pdb_file_type,\n                                                               engine=engine,\n                                                               rez_cutoff=rez_cutoff,\n                                                               seq_ident_cutoff=seq_ident_cutoff,\n                                                               always_use_homology=always_use_homology,\n                                                               allow_missing_on_termini=allow_missing_on_termini,\n                                                               allow_mutants=allow_mutants,\n                                                               allow_deletions=allow_deletions,\n                                                               allow_insertions=allow_insertions,\n                                                               allow_unresolved=allow_unresolved,\n                                                               skip_large_structures=skip_large_structures,\n                                                               clean=clean,\n                                                               force_rerun=force_rerun)\n\n        log.info('{}/{}: number of genes with a representative structure'.format(len(self.genes_with_a_representative_structure),\n                                                                                 len(self.genes)))\n        log.info('See the \"df_representative_structures\" attribute for a summary dataframe.')", "response": "Set all representative structures for a given sequence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef df_representative_structures(self):\n        rep_struct_pre_df = []\n        df_cols = ['gene', 'id', 'is_experimental', 'file_type', 'structure_file']\n\n        for g in self.genes_with_a_representative_structure:\n            repdict = g.protein.representative_structure.get_dict(df_format=True, only_attributes=df_cols)\n            repdict['gene'] = g.id\n            rep_struct_pre_df.append(repdict)\n\n        df = pd.DataFrame.from_records(rep_struct_pre_df, columns=df_cols).set_index('gene')\n        if df.empty:\n            log.warning('Empty dataframe')\n            return df\n        else:\n            return ssbio.utils.clean_df(df)", "response": "Get a dataframe of representative protein structure information."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef missing_representative_structure(self):\n        return [x.id for x in self.genes if not self.genes_with_a_representative_structure.has_id(x.id)]", "response": "list of genes with no mapping to a representative structure"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare I - TASSER homology modeling for genes without structures or all genes.", "response": "def prep_itasser_modeling(self, itasser_installation, itlib_folder, runtype, create_in_dir=None,\n                            execute_from_dir=None, all_genes=False, print_exec=False, **kwargs):\n        \"\"\"Prepare to run I-TASSER homology modeling for genes without structures, or all genes.\n\n        Args:\n            itasser_installation (str): Path to I-TASSER folder, i.e. ``~/software/I-TASSER4.4``\n            itlib_folder (str): Path to ITLIB folder, i.e. ``~/software/ITLIB``\n            runtype: How you will be running I-TASSER - local, slurm, or torque\n            create_in_dir (str): Local directory where folders will be created, if not provided default is the\n                GEM-PRO's ``data_dir``\n            execute_from_dir (str): Optional path to execution directory - use this if you are copying the homology\n                models to another location such as a supercomputer for running\n            all_genes (bool): If all genes should be prepped, or only those without any mapped structures\n            print_exec (bool): If the execution statement should be printed to run modelling\n\n        Todo:\n            * Document kwargs - extra options for I-TASSER, SLURM or Torque execution\n            * Allow modeling of any sequence in sequences attribute, select by ID or provide SeqProp?\n\n        \"\"\"\n        if not create_in_dir:\n            if not self.data_dir:\n                raise ValueError('Output directory must be specified')\n            self.homology_models_dir = op.join(self.data_dir, 'homology_models')\n        else:\n            self.homology_models_dir = create_in_dir\n\n        ssbio.utils.make_dir(self.homology_models_dir)\n\n        if not execute_from_dir:\n            execute_from_dir = self.homology_models_dir\n\n        counter = 0\n        for g in self.genes_with_a_representative_sequence:\n            repstruct = g.protein.representative_structure\n            if repstruct and not all_genes:\n                log.debug('{}: representative structure set, skipping homology modeling'.format(g.id))\n                continue\n\n            g.protein.prep_itasser_modeling(itasser_installation=itasser_installation,\n                                            itlib_folder=itlib_folder, runtype=runtype,\n                                            create_in_dir=self.homology_models_dir,\n                                            execute_from_dir=execute_from_dir,\n                                            print_exec=print_exec, **kwargs)\n            counter += 1\n\n        log.info('Prepared I-TASSER modeling folders for {} genes in folder {}'.format(counter,\n                                                                                       self.homology_models_dir))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pdb_downloader_and_metadata(self, outdir=None, pdb_file_type=None, force_rerun=False):\n\n        if not pdb_file_type:\n            pdb_file_type = self.pdb_file_type\n\n        counter = 0\n        for g in tqdm(self.genes):\n            pdbs = g.protein.pdb_downloader_and_metadata(outdir=outdir, pdb_file_type=pdb_file_type, force_rerun=force_rerun)\n\n            if pdbs:\n                counter += len(pdbs)\n\n        log.info('Updated PDB metadata dataframe. See the \"df_pdb_metadata\" attribute for a summary dataframe.')\n        log.info('Saved {} structures total'.format(counter))", "response": "Download ALL mapped experimental structures to each protein s structures directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a summary dataframe of all proteins in the project.", "response": "def df_proteins(self):\n        \"\"\"DataFrame: Get a summary dataframe of all proteins in the project.\"\"\"\n        pre_df = []\n        df_cols = ['gene', 'id', 'sequences', 'num_sequences', 'representative_sequence',\n                   'repseq_gene_name', 'repseq_uniprot', 'repseq_description',\n                   'num_structures', 'experimental_structures', 'num_experimental_structures',\n                   'homology_models', 'num_homology_models',\n                   'representative_structure', 'representative_chain', 'representative_chain_seq_coverage',\n                   'repstruct_description', 'repstruct_resolution',\n                   'num_sequence_alignments', 'num_structure_alignments']\n\n        for g in self.genes:\n            # Get per protein DataFrame\n            protein_dict = g.protein.protein_statistics\n            protein_dict['gene'] = g.id\n            pre_df.append(protein_dict)\n\n        df = pd.DataFrame.from_records(pre_df, columns=df_cols).set_index('gene')\n        if df.empty:\n            log.warning('Empty dataframe')\n            return df\n        else:\n            return ssbio.utils.clean_df(df)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_dssp_annotations_parallelize(self, sc, representatives_only=True, force_rerun=False):\n        genes_rdd = sc.parallelize(self.genes)\n\n        def get_dssp_annotation(g):\n            g.protein.get_dssp_annotations(representative_only=representatives_only, force_rerun=force_rerun)\n            return g\n\n        result = genes_rdd.map(get_dssp_annotation).collect()\n        for modified_g in result:\n            original_gene = self.genes.get_by_id(modified_g.id)\n            original_gene.copy_modified_gene(modified_g)", "response": "Run DSSP on structures and store calculations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_msms_annotations(self, representatives_only=True, force_rerun=False):\n        for g in tqdm(self.genes):\n            g.protein.get_msms_annotations(representative_only=representatives_only, force_rerun=force_rerun)", "response": "Run MSMS on structures and store calculations on the chain sequence at the corresponding ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_msms_annotations_parallelize(self, sc, representatives_only=True, force_rerun=False):\n        genes_rdd = sc.parallelize(self.genes)\n\n        def get_msms_annotation(g):\n            g.protein.get_msms_annotations(representative_only=representatives_only, force_rerun=force_rerun)\n            return g\n\n        result = genes_rdd.map(get_msms_annotation).collect()\n        for modified_g in result:\n            original_gene = self.genes.get_by_id(modified_g.id)\n            original_gene.copy_modified_gene(modified_g)", "response": "Run MSMS on structures and store annotations in the chain sequence at the corresponding location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_freesasa_annotations(self, include_hetatms=False, representatives_only=True, force_rerun=False):\n        for g in tqdm(self.genes):\n            g.protein.get_freesasa_annotations(include_hetatms=include_hetatms,\n                                               representative_only=representatives_only,\n                                               force_rerun=force_rerun)", "response": "Run freesasa on structures and store calculations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning freesasa on structures and store calculations.", "response": "def get_freesasa_annotations_parallelize(self, sc, include_hetatms=False,\n                                             representatives_only=True, force_rerun=False):\n        \"\"\"Run freesasa on structures and store calculations.\n\n        Annotations are stored in the protein structure's chain sequence at:\n        ``<chain_prop>.seq_record.letter_annotations['*-freesasa']``\n\n        Args:\n            include_hetatms (bool): If HETATMs should be included in calculations. Defaults to ``False``.\n            representative_only (bool): If analysis should only be run on the representative structure\n            force_rerun (bool): If calculations should be rerun even if an output file exists\n\n        \"\"\"\n        genes_rdd = sc.parallelize(self.genes)\n\n        def get_freesasa_annotation(g):\n            g.protein.get_freesasa_annotations(include_hetatms=include_hetatms,\n                                               representative_only=representatives_only,\n                                               force_rerun=force_rerun)\n            return g\n\n        result = genes_rdd.map(get_freesasa_annotation).collect()\n        for modified_g in result:\n            original_gene = self.genes.get_by_id(modified_g.id)\n            original_gene.copy_modified_gene(modified_g)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_disulfide_bridges(self, representatives_only=True):\n        for g in tqdm(self.genes):\n            g.protein.find_disulfide_bridges(representative_only=representatives_only)", "response": "Run Biopython s disulfide bridge finder and store found bridges."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun Biopython s disulfide bridge finder and store found bridges.", "response": "def find_disulfide_bridges_parallelize(self, sc, representatives_only=True):\n        \"\"\"Run Biopython's disulfide bridge finder and store found bridges.\n\n        Annotations are stored in the protein structure's chain sequence at:\n        ``<chain_prop>.seq_record.annotations['SSBOND-biopython']``\n\n        Args:\n            representative_only (bool): If analysis should only be run on the representative structure\n\n        \"\"\"\n        genes_rdd = sc.parallelize(self.genes)\n\n        def find_disulfide_bridges(g):\n            g.protein.find_disulfide_bridges(representative_only=representatives_only)\n            return g\n\n        result = genes_rdd.map(find_disulfide_bridges).collect()\n        for modified_g in result:\n            original_gene = self.genes.get_by_id(modified_g.id)\n            original_gene.copy_modified_gene(modified_g)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving all Proteins as pickle files and reset the attribute of each gene.", "response": "def save_protein_pickles_and_reset_protein(self):\n        \"\"\"Save all Proteins as pickle files -- currently development code for parallelization purposes. Also clears the\n        protein attribute in all genes!\"\"\"\n        self.gene_protein_pickles = {}\n        for g in tqdm(self.genes):\n            if g.protein.representative_sequence:\n                initproteinpickle = op.join(g.protein.protein_dir, '{}_protein.pckl'.format(g.id))\n                g.protein.save_pickle(initproteinpickle)\n                self.gene_protein_pickles[g.id] = initproteinpickle\n                g.reset_protein()\n            else:\n                g.reset_protein()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the oligomeric prediction in a SWISS - MODEL repository file and return a dictionary of the state of the current set of undecimated oligomeric.", "response": "def get_oligomeric_state(swiss_model_path):\n    \"\"\"Parse the oligomeric prediction in a SWISS-MODEL repository file\n\n    As of 2018-02-26, works on all E. coli models. Untested on other pre-made organism models.\n\n    Args:\n        swiss_model_path (str): Path to SWISS-MODEL PDB file\n\n    Returns:\n        dict: Information parsed about the oligomeric state\n\n    \"\"\"\n    oligo_info = {}\n    with open(swiss_model_path, 'r') as f:\n        for line in f:\n            if line.startswith('REMARK   3 MODEL INFORMATION'):\n                break\n        for i in range(10):\n            line = f.readline()\n            if 'ENGIN' in line:\n                oligo_info['ENGIN'] = line.rstrip().split(' ')[-1]\n            elif 'OSTAT' in line:\n                oligo_info['OSTAT'] = line.rstrip().split(' ')[-1]\n            elif 'OSRSN' in line:\n                oligo_info['OSRSN'] = line.rstrip().split(' ')[-1]\n            elif 'QSPRD' in line:\n                oligo_info['QSPRD'] = line.rstrip().split(' ')[-1]\n            elif 'GMQE' in line:\n                oligo_info['GMQE'] = line.rstrip().split(' ')[-1]\n            elif 'QMN4' in line:\n                oligo_info['QMN4'] = line.rstrip().split(' ')[-1]\n            elif 'MODT' in line:\n                oligo_info['MODT'] = line.rstrip().split(' ')[-1]\n    return oligo_info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef translate_ostat(ostat):\n    ostat_lower = ostat.strip().lower()\n    if ostat_lower == 'monomer':\n        return 1\n    elif ostat_lower == 'homo-dimer':\n        return 2\n    elif ostat_lower == 'homo-trimer':\n        return 3\n    elif ostat_lower == 'homo-tetramer':\n        return 4\n    elif ostat_lower == 'homo-pentamer':\n        return 5\n    elif ostat_lower == 'homo-hexamer':\n        return 6\n    elif ostat_lower == 'homo-heptamer':\n        return 7\n    elif ostat_lower == 'homo-octamer':\n        return 8\n    else:\n        num = int(ostat_lower.split('-')[1])\n        return num", "response": "Translate the OSTAT field to an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the INDEX_JSON file and reorganize it as a dictionary of lists.", "response": "def parse_metadata(self):\n        \"\"\"Parse the INDEX_JSON file and reorganize it as a dictionary of lists.\"\"\"\n\n        all_models = defaultdict(list)\n\n        with open(self.metadata_index_json) as f:\n            loaded = json.load(f)\n\n        for m in loaded['index']:\n            all_models[m['uniprot_ac']].append(m)\n\n        self.all_models = dict(all_models)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_models(self, uniprot_acc):\n        if uniprot_acc in self.all_models:\n            return self.all_models[uniprot_acc]\n        else:\n            log.error('{}: no SWISS-MODELs available'.format(uniprot_acc))\n            return None", "response": "Return all available models for a UniProt accession number."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the path to the homology model using information from the index dictionary.", "response": "def get_model_filepath(self, infodict):\n        \"\"\"Get the path to the homology model using information from the index dictionary for a single model.\n\n        Example: use self.get_models(UNIPROT_ID) to get all the models, which returns a list of dictionaries.\n            Use one of those dictionaries as input to this function to get the filepath to the model itself.\n\n        Args:\n            infodict (dict): Information about a model from get_models\n\n        Returns:\n            str: Path to homology model\n\n        \"\"\"\n        u = infodict['uniprot_ac']\n\n        original_filename = '{}_{}_{}_{}'.format(infodict['from'], infodict['to'],\n                                                 infodict['template'], infodict['coordinate_id'])\n        file_path = op.join(self.metadata_dir, u[:2], u[2:4], u[4:6],\n                            'swissmodel', '{}.pdb'.format(original_filename))\n\n        if op.exists(file_path):\n            return file_path\n        else:\n            log.warning('{}: no file {} found for model'.format(u, file_path))\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads all models available for a UniProt accession number.", "response": "def download_models(self, uniprot_acc, outdir='', force_rerun=False):\n        \"\"\"Download all models available for a UniProt accession number.\n\n        Args:\n            uniprot_acc (str): UniProt ACC/ID\n            outdir (str): Path to output directory, uses working directory if not set\n            force_rerun (bool): Force a redownload the models if they already exist\n\n        Returns:\n            list: Paths to the downloaded models\n\n        \"\"\"\n        downloaded = []\n        subset = self.get_models(uniprot_acc)\n\n        for entry in subset:\n            ident = '{}_{}_{}_{}'.format(uniprot_acc, entry['template'], entry['from'], entry['to'])\n            outfile = op.join(outdir, ident + '.pdb')\n\n            if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n                response = requests.get(entry['url'])\n\n                if response.status_code == 404:\n                    log.error('{}: 404 returned, no model available.'.format(ident))\n\n                else:\n                    with open(outfile, 'w') as f:\n                        f.write(response.text)\n\n                    log.debug('{}: downloaded homology model'.format(ident))\n                    downloaded.append(outfile)\n            else:\n                downloaded.append(outfile)\n\n        return downloaded"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef organize_models(self, outdir, force_rerun=False):\n        uniprot_to_swissmodel = defaultdict(list)\n        for u, models in self.all_models.items():\n            for m in models:\n                original_filename = '{}_{}_{}_{}'.format(m['from'], m['to'], m['template'], m['coordinate_id'])\n                file_path = op.join(self.metadata_dir,\n                                    u[:2], u[2:4], u[4:], 'swissmodel',\n                                    '{}.pdb'.format(original_filename))\n                if op.exists(file_path):\n                    new_filename = '{}_{}_{}_{}.pdb'.format(u, m['from'], m['to'], m['template'][:4])\n                    shutil.copy(file_path, op.join(outdir, new_filename))\n                    uniprot_to_swissmodel[u].append(new_filename)\n                else:\n                    log.warning('{}: no file {} found for model'.format(u, file_path))\n\n        return uniprot_to_swissmodel", "response": "Organize and rename SWISS - MODEL models to a single folder with a name containing template information."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates dH using Oobatake method in units cal / mol.", "response": "def calculate_oobatake_dH(seq, temp):\n    \"\"\"Get dH using Oobatake method in units cal/mol.\n\n    Args:\n        seq (str, Seq, SeqRecord): Amino acid sequence\n        temp (float): Temperature in degrees C\n\n    Returns:\n        float: dH in units cal/mol\n\n    \"\"\"\n\n    seq = ssbio.protein.sequence.utils.cast_to_str(seq)\n\n    dH = 0\n    temp += 273.15\n    T0 = 298.15\n    for aa in seq:\n        H0 = oobatake_dictionary[aa]['dH'] * 1000\n        dH += H0\n    return dH + _sum_of_dCp(seq) * (temp - T0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_oobatake_dS(seq, temp):\n\n    seq = ssbio.protein.sequence.utils.cast_to_str(seq)\n\n    dS = 0\n    temp += 273.15\n    T0 = 298.15\n    dCp_sum = _sum_of_dCp(seq)\n    for aa in seq:\n        S0 = oobatake_dictionary[aa]['dS']\n        dS += S0\n    return dS + dCp_sum * math.log(temp / T0)", "response": "Calculate dS using Oobatake method in units cal / mol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the free energy of unfolding using Oobatake method in units cal / mol.", "response": "def calculate_oobatake_dG(seq, temp):\n    \"\"\"Get free energy of unfolding (dG) using Oobatake method in units cal/mol.\n\n    Args:\n        seq (str, Seq, SeqRecord): Amino acid sequence\n        temp (float): Temperature in degrees C\n\n    Returns:\n        float: Free energy of unfolding dG (J/mol)\n\n    \"\"\"\n\n    dH = calculate_oobatake_dH(seq, temp)\n    dS = calculate_oobatake_dS(seq, temp)\n    dG = dH - (temp + 273.15) * dS\n\n    # 563.552 - a correction for N- and C-terminal group (approximated from 7 examples in the paper)\n    return dG - 563.552"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the free energy of unfolding in units J/mol using Dill method in units J/mol.", "response": "def calculate_dill_dG(seq_len, temp):\n    \"\"\"Get free energy of unfolding (dG) using Dill method in units J/mol.\n\n    Args:\n        seq_len (int): Length of amino acid sequence\n        temp (float): Temperature in degrees C\n\n    Returns:\n        float: Free energy of unfolding dG (J/mol)\n    \"\"\"\n    Th = 373.5  # This quantity affects the up-and-down of the dG vs temperature curve (dG values)\n    Ts = 385  # This quantity affects the left-and-right\n    temp += 273.15\n\n    dH = (4.0 * seq_len + 143) * 1000\n    dS = 13.27 * seq_len + 448\n    dCp = (0.049 * seq_len + 0.85) * 1000\n    dG = dH + dCp * (temp - Th) - temp * dS - temp * dCp * math.log(float(temp) / Ts)\n\n    return dG"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dG_at_T(seq, temp):\n    # R (molar gas constant) in calories\n    r_cal = scipy.constants.R / scipy.constants.calorie\n\n    seq = ssbio.protein.sequence.utils.cast_to_str(seq)\n\n    oobatake = {}\n    for t in range(20, 51):\n        oobatake[t] = calculate_oobatake_dG(seq, t)\n\n    stable = [i for i in oobatake.values() if i > 0]\n\n    if len(stable) == 0:\n        # If oobatake dG < 0 for all tempertures [20,50], use Dill dG\n        # and convert the number from J/mol to cal/mol\n        dG = 0.238846 * calculate_dill_dG(len(seq), temp)\n        method='Dill'\n    else:\n        dG = oobatake[temp]\n        method='Oobatake'\n\n    keq = math.exp(-1 * dG / (r_cal * (temp + 273.15)))\n\n    return dG, keq, method", "response": "Predicts the dG at temperature T using best predictions from Dill or Oobatake methods."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the PPM server from OPM to predict transmembrane residues.", "response": "def run_ppm_server(pdb_file, outfile, force_rerun=False):\n    \"\"\"Run the PPM server from OPM to predict transmembrane residues.\n\n    Args:\n        pdb_file (str): Path to PDB file\n        outfile (str): Path to output HTML results file\n        force_rerun (bool): Flag to rerun PPM if HTML results file already exists\n\n    Returns:\n        dict: Dictionary of information from the PPM run, including a link to download the membrane protein file\n\n    \"\"\"\n    if ssbio.utils.force_rerun(outfile=outfile, flag=force_rerun):\n        url = 'http://sunshine.phar.umich.edu/upload_file.php'\n        files = {'userfile': open(pdb_file, 'rb')}\n        r = requests.post(url, files=files)\n        info = r.text\n\n        # Save results in raw HTML format\n        with open(outfile, 'w') as f:\n            f.write(info)\n\n    else:\n        # Utilize existing saved results\n        with open(outfile, 'r') as f:\n            info = f.read()\n\n    # Clean up the HTML stuff\n    t = info.replace('\\n', '')\n    tt = t.replace('\\r', '')\n    ttt = tt.replace('\\t', '')\n\n    soup = BeautifulSoup(ttt, \"lxml\")\n    # Find all tables in the HTML code\n    tables = soup.find_all(\"table\", attrs={\"class\": \"data\"})\n\n    info_dict = {}\n\n    # There are multiple tables with information\n    table_index = 0\n    for t in tables:\n        data_index = 0\n\n        # \"row1\" contains data\n        for data in t.find_all('tr', attrs={\"class\": \"row1\"}):\n            data_list = list(data.strings)\n\n            if table_index == 0:\n                info_dict['Depth/Hydrophobic Thickness'] = data_list[0]\n                info_dict['deltaG_transfer'] = data_list[2]\n                info_dict['Tilt Angle'] = data_list[3]\n\n            if table_index == 1 and data_index == 0:\n                info_dict['Embedded_residues_Tilt'] = data_list[0]\n                info_dict['Embedded_residues'] = data_list[1]\n\n            if table_index == 1 and data_index == 1:\n                info_dict['Transmembrane_secondary_structure_segments_Tilt'] = data_list[0]\n                info_dict['Transmembrane_secondary_structure_segments'] = data_list[1]\n\n            if table_index == 2:\n                info_dict['Output Messages'] = data_list[1]\n\n            if table_index == 3:\n                baseurl = 'http://sunshine.phar.umich.edu/'\n                a = data.find('a', href=True)\n                download_url = baseurl + a['href'].replace('./', '')\n                info_dict['Output file download link'] = download_url\n\n            data_index += 1\n        table_index += 1\n\n    return info_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the complex folder name for the class.", "response": "def complex_dir(self):\n        \"\"\"str: Complex folder\"\"\"\n        if self.root_dir:\n            # Add a _complex suffix to the folder if it has the same name as its root folder\n            folder_name = self.id\n            if folder_name == op.basename(self.root_dir):\n                folder_name = self.id + '_complex'\n            return op.join(self.root_dir, folder_name)\n        else:\n            log.warning('Root directory not set')\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsubmit a string to CCTOP and return the job ID on the CCTOP server", "response": "def cctop_submit(seq_str):\n    \"\"\"Submit a protein sequence string to CCTOP and return the job ID.\n\n    Args:\n        seq_str (str): Protein sequence as a string\n\n    Returns:\n        dict: Job ID on the CCTOP server\n\n    \"\"\"\n    url = 'http://cctop.enzim.ttk.mta.hu/php/submit.php?sequence={}&tmFilter&signalPred'.format(seq_str)\n    r = requests.post(url)\n    jobid = r.text.split('ID: ')[1]\n\n    return jobid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cctop_check_status(jobid):\n    status = 'http://cctop.enzim.ttk.mta.hu/php/poll.php?jobId={}'.format(jobid)\n    status_text = requests.post(status)\n    return status_text.text", "response": "Check the status of a CCTOP job ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cctop_save_xml(jobid, outpath):\n    status = cctop_check_status(jobid=jobid)\n    if status == 'Finished':\n        result = 'http://cctop.enzim.ttk.mta.hu/php/result.php?jobId={}'.format(jobid)\n        result_text = requests.post(result)\n        with open(outpath, 'w') as f:\n            f.write(result_text.text)\n        return outpath\n    else:\n        raise ConnectionRefusedError('CCTOP job incomplete, status is \"{}\"'.format(status))", "response": "Save the CCTOP results file in XML format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_cctop_full(infile):\n    parser = etree.XMLParser(ns_clean=True)\n    with open(infile, 'r') as f:\n        tree = etree.fromstring(f.read(), parser)\n\n    all_info = []\n\n    if tree.find('Topology') is not None:\n        for r in tree.find('Topology').findall('Region'):\n            region_start = int(r.attrib['from'])\n            region_end = int(r.attrib['to'])\n            region = r.attrib['loc']\n            for i in range(region_start, region_end + 1):\n                all_info.append((i, region))\n\n    return all_info", "response": "Parse a CCTOP XML file and return a list of the consensus TM domains in the format described above\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_cctop_regions(infile):\n    parsed = parse_cctop_full(infile)\n    if not parsed:\n        return {}\n    return ssbio.utils.label_sequential_regions(parsed)", "response": "Parse a CCTOP XML file and return a dictionary of labeled TM domains."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_protein_feather_paths(protgroup, memornot, protgroup_dict, protein_feathers_dir, core_only_genes=None):\n    prots_memornot = protgroup_dict['localization'][memornot[0]][memornot[1]]\n\n    if protgroup[0] == 'localization':\n        if protgroup[2] != 'all':\n            if memornot[1] in ['membrane', 'inner_membrane', 'outer_membrane'] and protgroup[2] not in ['membrane', 'inner_membrane', 'outer_membrane']:\n                return []\n            if memornot[1] not in ['membrane', 'inner_membrane', 'outer_membrane'] and protgroup[2] in ['membrane', 'inner_membrane', 'outer_membrane']:\n                return []\n\n    prots_group = protgroup_dict[protgroup[0]][protgroup[1]][protgroup[2]]\n    prots_filtered = list(set(prots_group).intersection(prots_memornot))\n    if core_only_genes:\n        prots_filtered = list(set(prots_filtered).intersection(core_only_genes))\n    return [op.join(protein_feathers_dir, '{}_protein_strain_properties.fthr'.format(x)) for x in prots_filtered if op.exists(op.join(protein_feathers_dir, '{}_protein_strain_properties.fthr'.format(x)))]", "response": "Get a list of paths to feathers files for a given protein group and memornot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_observed_strains_and_df(observation, observation_dict):\n    observed_df = pd.DataFrame.from_records(observation_dict[observation], columns=['strain','phenotype']).set_index('strain')\n    observed_strains = observed_df.index.tolist()\n    return observed_strains, observed_df", "response": "get observed strains and df"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all the elements that are in the given subsequences", "response": "def get_interested_subsequences(subsequences):#, subseqdim):\n    \"\"\"\n    subsequences example: ['disorder_2D', 'ss_disorder_2D', ...]\n    filter_subseq_suffixes example: {'disorder_2D': ['aa_count_ord', 'aa_count_dis'], ... } -- defined above\n    \"\"\"\n    keep_indices = []\n    # if subseqdim == 'ALLD':\n    #     filter_subseq_suffixes = filter_subseq_suffixes\n    # elif subseqdim == '3D':\n    #     filter_subseq_suffixes = filter_subseq_3D_suffixes\n    # elif subseqdim == '2D':\n    #     filter_subseq_suffixes = filter_subseq_2D_suffixes\n    # else:\n    #     raise ValueError('ALLD, 3D, or 2D only!')\n\n    for subseq in subsequences:\n        if subseq == 'all':\n            keep_indices.extend([x for x in filter_subseq_suffixes[subseq]])\n        else:\n            keep_indices.extend([subseq + '_' + x for x in filter_subseq_suffixes[subseq]])\n    return keep_indices"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a feather of amino acid counts for a protein.", "response": "def load_feather(protein_feather, length_filter_pid=None, copynum_scale=False, copynum_df=None):\n    \"\"\"Load a feather of amino acid counts for a protein.\n\n    Args:\n        protein_feather (str): path to feather file\n        copynum_scale (bool): if counts should be multiplied by protein copy number\n        copynum_df (DataFrame): DataFrame of copy numbers\n\n    Returns:\n        DataFrame: of counts with some aggregated together\n\n    \"\"\"\n    protein_df = pd.read_feather(protein_feather).set_index('index')\n\n    # Combine counts for residue groups\n    from ssbio.protein.sequence.properties.residues import _aa_property_dict_one, EXTENDED_AA_PROPERTY_DICT_ONE\n    aggregators = {\n        'aa_count_bulk'    : {'residues': EXTENDED_AA_PROPERTY_DICT_ONE['Bulky'],\n                              'subseqs' : ['metal_2_5D', 'metal_3D']},\n        'aa_count_carb'    : {'residues': EXTENDED_AA_PROPERTY_DICT_ONE['Carbonylation susceptible'],\n                              'subseqs' : ['metal_2_5D', 'metal_3D', 'acc_2D', 'acc_3D', 'surface_3D']},\n        'aa_count_chrg'    : {'residues': _aa_property_dict_one['Charged'],\n                              'subseqs' : ['metal_2_5D', 'metal_3D', 'csa_2_5D', 'sites_2_5D', 'acc_2D', 'acc_3D',\n                                           'surface_3D']},\n        'aa_count_poschrg' : {'residues': _aa_property_dict_one['Basic'],\n                              'subseqs' : ['metal_2_5D', 'metal_3D', 'acc_2D', 'acc_3D', 'surface_3D']},\n        'aa_count_negchrg' : {'residues': _aa_property_dict_one['Acidic'],\n                              'subseqs' : ['metal_2_5D', 'metal_3D', 'acc_2D', 'acc_3D', 'surface_3D']},\n        'aa_count_tmstab'  : {'residues': EXTENDED_AA_PROPERTY_DICT_ONE['TM stabilizing'],\n                              'subseqs' : ['tm_2D', 'tm_3D']},\n        'aa_count_tmunstab': {'residues': EXTENDED_AA_PROPERTY_DICT_ONE['TM to Thr stabilizing'],\n                              'subseqs' : ['tm_2D', 'tm_3D']},\n        'aa_count_dis'     : {'residues': EXTENDED_AA_PROPERTY_DICT_ONE['Disorder promoting'],\n                              'subseqs' : ['disorder_2D', 'ss_disorder_2D', 'disorder_3D', 'ss_disorder_3D',\n                                           'dna_2_5D']},\n        'aa_count_ord'     : {'residues': EXTENDED_AA_PROPERTY_DICT_ONE['Order promoting'],\n                              'subseqs' : ['disorder_2D', 'ss_disorder_2D', 'disorder_3D', 'ss_disorder_3D',\n                                           'dna_2_5D']}}\n\n    # Do combination counts for all types of subsequences\n    for suffix, info in aggregators.items():\n        agg_residues = info['residues']\n        for prefix in info['subseqs']:\n            to_add_idxes = []\n            for agg_res in agg_residues:\n                to_add_idx = prefix + '_aa_count_' + agg_res\n                if to_add_idx in protein_df.index:\n                    to_add_idxes.append(to_add_idx)\n            subseq_agged_col = protein_df.loc[to_add_idxes, :].sum()  # Add each residue series\n            protein_df.loc[prefix + '_' + suffix] = subseq_agged_col  # Append to df\n\n    ## REMOVE OTHER STRAINS WITH DELETIONS (use float -- length_filter_pid=0.8 to get only strains with >80% length\n    ## alternative to atlas2.calculate_residue_counts_perstrain wt_pid_cutoff param -- works a little differently just considering length\n    if length_filter_pid:\n        keep_cols = protein_df.loc['aa_count_total'][protein_df.loc['aa_count_total'] > protein_df.at['aa_count_total', 'K12'] * length_filter_pid].index\n        protein_df = protein_df[keep_cols]\n\n    # Multiply by proteomics copy number?\n    if copynum_scale:\n        if not isinstance(copynum_df, pd.DataFrame):\n            raise ValueError('Please supply copy numbers')\n        protein_id = op.basename(protein_feather).split('_protein')[0]\n        if protein_id in copynum_df.index:\n            copynum = copynum_df.at[protein_id, 'copynum']\n            if copynum > 0:  # TODO: currently keeping one copy of proteins with 0, is that ok?\n                protein_df = protein_df * copynum\n\n    return protein_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_proteome_counts_impute_missing(prots_filtered_feathers, outpath, length_filter_pid=None,\n                                       copynum_scale=False, copynum_df=None,\n                                       force_rerun=False):\n    \"\"\"Get counts, uses the mean feature vector to fill in missing proteins for a strain\"\"\"\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outpath):\n        big_strain_counts_df = pd.DataFrame()\n        first = True\n        for feather in prots_filtered_feathers:\n            loaded = load_feather(protein_feather=feather, length_filter_pid=length_filter_pid,\n                                  copynum_scale=copynum_scale,\n                                  copynum_df=copynum_df)\n            if first:\n                big_strain_counts_df = pd.DataFrame(index=_all_counts, columns=loaded.columns)\n                first = False\n\n            new_columns = list(set(loaded.columns.tolist()).difference(big_strain_counts_df.columns))\n            if new_columns:\n                for col in new_columns:\n                    big_strain_counts_df[col] = big_strain_counts_df.mean(axis=1)\n\n            not_in_loaded = list(set(big_strain_counts_df.columns).difference(loaded.columns.tolist()))\n            if not_in_loaded:\n                for col in not_in_loaded:\n                    big_strain_counts_df[col] = big_strain_counts_df[col] + loaded.mean(axis=1)\n\n            big_strain_counts_df = big_strain_counts_df.add(loaded, fill_value=0)\n\n        if len(big_strain_counts_df) > 0:\n            big_strain_counts_df.astype(float).reset_index().to_feather(outpath)\n        return big_strain_counts_df\n    else:\n        return pd.read_feather(outpath).set_index('index')", "response": "Get counts for all proteins that are missing in a particular strain"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets counts and normalize by number of proteins providing percentages", "response": "def get_proteome_correct_percentages(prots_filtered_feathers, outpath, length_filter_pid=None,\n                                     copynum_scale=False, copynum_df=None,\n                                     force_rerun=False):\n    \"\"\"Get counts and normalize by number of proteins, providing percentages\"\"\"\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outpath):\n        prot_tracker = defaultdict(int)\n        big_strain_counts_df = pd.DataFrame()\n        first = True\n        for feather in prots_filtered_feathers:\n            loaded = load_feather(protein_feather=feather, length_filter_pid=length_filter_pid,\n                                  copynum_scale=copynum_scale,\n                                  copynum_df=copynum_df)\n\n            if first:\n                big_strain_counts_df = pd.DataFrame(columns=loaded.columns)\n                first = False\n            tmp_df = pd.DataFrame(columns=loaded.columns)\n            for strain in loaded.columns:\n                prot_tracker[strain] += 1\n                totals = list(filter(lambda x: x.endswith('total'), loaded[strain].index))\n                for t in totals:\n                    counts = t.rsplit('_', 1)[0]\n                    aa_counts = list(\n                        filter(lambda x: (x.startswith(counts) and x not in totals), loaded[strain].index))\n                    for aa_count in aa_counts:\n                        tmp_df.at[aa_count.replace('count', '%'), strain] = loaded[strain][aa_count] / \\\n                                                                            loaded[strain][t]\n            big_strain_counts_df = big_strain_counts_df.add(tmp_df, fill_value=0)\n\n        for c, total in prot_tracker.items():\n            big_strain_counts_df.loc[:, c] /= total\n\n        if len(big_strain_counts_df) > 0:\n            big_strain_counts_df.astype(float).reset_index().to_feather(outpath)\n        return big_strain_counts_df\n    else:\n        return pd.read_feather(outpath).set_index('index')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the skew statistics for all intra and inter distributions and returns them as two dictionaries reporting ( skew skew_pval for all groups", "response": "def compute_skew_stats(intra, inter):\n    \"\"\"Returns two dictionaries reporting (skew, skew_pval) for all groups\"\"\"\n    # Intra (within a group) stats\n    intra_skew = {}\n    for k, v in intra.items():\n        skew = st.skew(v)\n        try:\n            skew_zstat, skew_pval = st.skewtest(v)\n        except ValueError:  # if sample size too small\n            skew_zstat, skew_pval = (0, 1)\n        intra_skew[k] = (skew, skew_zstat, skew_pval)\n\n    # Inter (between groups) stats\n    inter_skew = {}\n    for k, v in inter.items():\n        # Inter skew stats\n        skew_sep = st.skew(v.flatten())\n        try:\n            skew_sep_zstat, skew_sep_pval = st.skewtest(v.flatten())\n        except ValueError:\n            skew_sep_zstat, skew_sep_pval = (0, 1)\n        inter_skew['-'.join(k)] = (skew_sep, skew_sep_zstat, skew_sep_pval)\n\n        # Significance of difference between intra and inter distributions\n        for intra_key in k:\n            try:\n                separation_zstat, separation_pval = mannwhitneyu(intra[intra_key],\n                                                                 v.flatten(),\n                                                                 alternative='less')\n            except ValueError:  # All numbers are identical in mannwhitneyu\n                separation_zstat, separation_pval = (0, 1)\n            inter_skew['{}<{}'.format(intra_key, '-'.join(k))] = (separation_zstat, separation_pval)\n\n    return intra_skew, inter_skew"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning all protein groups and subsequences in a single protein group.", "response": "def run_all2(protgroup, memornot, subsequences, base_outdir,\n            protgroup_dict, protein_feathers_dir, date, errfile, impute_counts=True,\n            cutoff_num_proteins=0, core_only_genes=None,\n            length_filter_pid=.8, remove_correlated_feats=True,\n            force_rerun_counts=False, force_rerun_percentages=False, force_rerun_pca=False):\n    \"\"\"run_all but ignoring observations before pca\"\"\"\n    import ssbio.utils\n\n    # Need to set multiprocessing limit for scipy/numpy stuff if parallelizing anything\n    import os\n    os.environ['OMP_NUM_THREADS'] = '1'\n\n    # First, filter down the protein group to the membrane/nonmembrane definition\n    prots_filtered_feathers = get_protein_feather_paths(protgroup=protgroup, memornot=memornot,\n                                                        protgroup_dict=protgroup_dict,\n                                                        protein_feathers_dir=protein_feathers_dir,\n                                                        core_only_genes=core_only_genes)\n    num_proteins = len(prots_filtered_feathers)\n    if num_proteins <= cutoff_num_proteins:\n        return\n\n    # Make output directories\n    protscale = 'proteome_unscaled'\n    outdir_d0 = ssbio.utils.make_dir(op.join(base_outdir, protscale))\n    outdir_d1 = ssbio.utils.make_dir(op.join(outdir_d0, '-'.join(memornot)))\n    outdir_final = ssbio.utils.make_dir(op.join(outdir_d1, '-'.join(protgroup)))\n\n    if impute_counts:\n        big_strain_counts_df = get_proteome_counts_impute_missing(prots_filtered_feathers=prots_filtered_feathers,\n                                                                  outpath=op.join(outdir_final,\n                                                                                  '{}-subsequence_proteome_IMP.fthr'.format(\n                                                                                      date)),\n                                                                  length_filter_pid=length_filter_pid,\n                                                                  force_rerun=force_rerun_counts)\n\n        big_strain_percents_df = get_proteome_percentages(counts_df=big_strain_counts_df,\n                                                          outpath=op.join(outdir_final,\n                                                                          '{}-subsequence_proteome_perc_IMP.fthr'.format(\n                                                                                  date)),\n                                                          force_rerun=force_rerun_percentages)\n        pca_pickle = op.join(outdir_final, '{}-subsequence_pca.pckl'.format(date))\n    # Divide by totals to get percentages in a new dataframe\n    else:\n        try:\n            big_strain_percents_df = get_proteome_correct_percentages(prots_filtered_feathers=prots_filtered_feathers,\n                                                                      outpath=op.join(outdir_final,\n                                                                                      '{}-subsequence_proteome_perc_AVG.fthr'.format(\n                                                                                              date)),\n                                                                      length_filter_pid=length_filter_pid,\n                                                                      force_rerun=force_rerun_percentages)\n            pca_pickle = op.join(outdir_final, '{}-subsequence_pca_AVG.pckl'.format(date))\n        except:\n            with open(errfile, \"a\") as myfile:\n                myfile.write('PERCENTAGES ERR: ' + '-'.join(memornot) + '\\t' + '-'.join(protgroup) + \"\\n\")\n            return\n\n\n\n    if ssbio.utils.force_rerun(flag=force_rerun_pca, outfile=pca_pickle):\n\n        # Then, get filters for rows of the loaded feathers for interested subsequences\n        keep_subsequences = get_interested_subsequences(subsequences=subsequences)\n\n        # Some numbers: number of features\n        num_feats = len(big_strain_percents_df)\n\n        # Make an unwieldy title\n        big_title = 'LOC={0}; PROTGROUP={1};\\n' \\\n                    'NUMPROTS={2}; NUMFEATS={3}'.format('-'.join(memornot),\n                                                                        '-'.join(protgroup),\n                                                                        num_proteins,\n                                                                        num_feats)\n\n        # Run PCA and make plots\n        runner = PCAMultiROS(features_df=big_strain_percents_df, observations_df=pd.DataFrame(), plot_title=big_title)\n        try:\n            runner.clean_data(keep_features=keep_subsequences, remove_correlated_feats=remove_correlated_feats)\n        except:\n            with open(errfile, \"a\") as myfile:\n                myfile.write(\n                    'CLEAN ERR: ' + '-'.join(memornot) + '\\t' + '-'.join(protgroup) + \"\\n\")\n            return\n        # try:\n        runner.run_pca()\n        # except:\n        #     with open(errfile, \"a\") as myfile:\n        #         myfile.write(\n        #             'PCA ERR: ' + '-'.join(memornot) + '\\t' + '-'.join(protgroup) + \"\\n\")\n        #     return\n        with open(pca_pickle, 'wb') as f:\n            pickle.dump(runner, f)\n    else:\n        with open(pca_pickle, 'rb') as f:\n            runner = pickle.load(f)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a plot showing contributions of properties to a PC", "response": "def make_contribplot(self, pc_to_look_at=1, sigadder=0.01, outpath=None, dpi=150, return_top_contribs=False):\n        \"\"\"Make a plot showing contributions of properties to a PC\"\"\"\n        cont = pd.DataFrame(self.pca.components_, columns=self.features_df.index, index=self.pc_names_list)\n        tmp_df = pd.DataFrame(cont.iloc[pc_to_look_at - 1]).reset_index().rename(columns={'index': 'Property'})\n        tmp_df['Contribution'] = tmp_df.iloc[:, 1] ** 2\n        tmp_df = tmp_df[tmp_df['Contribution'] > 1 / len(\n                cont.iloc[0]) + sigadder]  # Alter sigadder to just plot more/less significant contributors\n        tmp_df['Sign'] = np.where(tmp_df.iloc[:, 1] >= 0, 'Positive', 'Negative')\n        tmp_df = tmp_df.sort_values(by='Contribution', ascending=False)\n\n        fig, ax = plt.subplots(figsize=(30, 10))\n        sns.barplot(data=tmp_df, y='Property', x='Contribution', hue='Sign', dodge=False, ax=ax, hue_order=['Positive', 'Negative'],\n                        palette=sns.color_palette(\"coolwarm\", 2))\n\n        # Random formatting crap\n        self._change_height(ax, .6)  # Make bars thinner\n        ax.set_title('{} contributors'.format(self.pc_names_list[pc_to_look_at - 1]))\n        legend = plt.legend(loc=8, bbox_to_anchor=(1.2, .8), ncol=1, title='Sign', fontsize=10)\n        plt.setp(legend.get_title(), fontsize=12)\n        plt.gcf().subplots_adjust(left=.5, right=.65)\n        if outpath:\n            fig.savefig(outpath, dpi=dpi)\n        else:\n            plt.show()\n        plt.close()\n\n        if return_top_contribs:\n            return tmp_df.Property.values.tolist()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _change_height(self, ax, new_value):\n        for patch in ax.patches:\n            current_height = patch.get_height()\n            diff = current_height - new_value\n\n            # we change the bar height\n            patch.set_height(new_value)\n\n            # we recenter the bar\n            patch.set_y(patch.get_y() + diff * .5)", "response": "Change the height of the bar chart in horizontal bar chart thinner"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a dictionary of PC# - > K - S test stat for each PC.", "response": "def get_pca_ks_stats(self, maxrange=5):\n        \"\"\"Get a dictionary of PC#: K-S test stat for each \"\"\"\n        pc_to_phenotype_pairs = {}\n        num_components = self.principal_observations_df.shape[1]\n        if num_components < maxrange:\n            maxrange = num_components\n\n        phenotypes = self.principal_observations_df.phenotype.unique().tolist()\n        for i in range(0, maxrange):\n            phenotype_pair_to_ks = {}\n            for p1, p2 in combinations(phenotypes, 2):\n                p1_pc = self.principal_observations_df[self.principal_observations_df.phenotype == p1].iloc[:,i].as_matrix()\n                p2_pc = self.principal_observations_df[self.principal_observations_df.phenotype == p2].iloc[:,i].as_matrix()\n                phenotype_pair_to_ks[(p1, p2)] = ks_2samp(p1_pc, p2_pc)\n            pc_to_phenotype_pairs[i + 1] = phenotype_pair_to_ks\n\n        return pc_to_phenotype_pairs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_all_models_into_first_model(biop_structure):\n    from string import ascii_uppercase\n    idx = 1\n    first_model = biop_structure[0]\n\n    for m in biop_structure.get_models():\n        # Don't duplicate the original model\n        if first_model.id == m.id:\n            continue\n        for c in m.get_chains():\n            c.id = ascii_uppercase[idx]\n            first_model.add(c)\n        idx += 1", "response": "Merge all existing models into a single model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges all existing models into a single Biopython Structure object.", "response": "def merge_models(self, store_in_memory=False, outdir=None, outname=None, force_rerun=False):\n        \"\"\"Merge all existing models into a Structure's first_model attribute.\n\n        This directly modifies the Biopython Structure object. Chains IDs will start from A and increment for each new\n        chain (which is a Model that is converted).\n\n        Args:\n            store_in_memory (bool): If the modified Biopython Structure object should be stored in the attribute\n                ``structure``\n            outdir (str): If ``store_in_memory`` is False, the structure file has to be written somewhere so an output\n                directory must be specified here\n            outname (str): If ``store_in_memory`` is False, the structure file has to be written somewhere so an output\n                filename must be specified here (i.e. 4BXI_bio1)\n            force_rerun (bool): If merged file should be overwritten if it already exists\n\n        \"\"\"\n\n        if store_in_memory:\n            if self.structure:\n                parsed = copy(self.structure)\n            else:\n                parsed = self.parse_structure()\n            self.structure = merge_all_models_into_first_model(parsed)\n        else:\n            new_structure_path = write_merged_bioassembly(inpath=self.structure_path,\n                                                          outdir=outdir, outname=outname,\n                                                          force_rerun=force_rerun)\n            self.load_structure_path(new_structure_path, file_type='pdb')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave an ssbio object as a JSON file using json_tricks", "response": "def save_json(obj, outfile, allow_nan=True, compression=False):\n    \"\"\"Save an ssbio object as a JSON file using json_tricks\"\"\"\n    if compression:\n        with open(outfile, 'wb') as f:\n            dump(obj, f, allow_nan=allow_nan, compression=compression)\n    else:\n        with open(outfile, 'w') as f:\n            dump(obj, f, allow_nan=allow_nan, compression=compression)\n    log.info('Saved {} (id: {}) to {}'.format(type(obj), obj.id, outfile))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_json(file, new_root_dir=None, decompression=False):\n    if decompression:\n        with open(file, 'rb') as f:\n            my_object = load(f, decompression=decompression)\n    else:\n        with open(file, 'r') as f:\n            my_object = load(f, decompression=decompression)\n    if new_root_dir:\n        my_object.root_dir = new_root_dir\n\n    return my_object", "response": "Load a JSON file using json_tricks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the object as a pickle file", "response": "def save_pickle(obj, outfile, protocol=2):\n    \"\"\"Save the object as a pickle file\n\n    Args:\n        outfile (str): Filename\n        protocol (int): Pickle protocol to use. Default is 2 to remain compatible with Python 2\n\n    Returns:\n        str: Path to pickle file\n\n    \"\"\"\n    with open(outfile, 'wb') as f:\n        pickle.dump(obj, f, protocol=protocol)\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_pickle(file, encoding=None):\n    # TODO: test set encoding='latin1' for 2/3 incompatibility\n    if encoding:\n        with open(file, 'rb') as f:\n            return pickle.load(f, encoding=encoding)\n\n    with open(file, 'rb') as f:\n       return pickle.load(f)", "response": "Load a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a structure via PDBParser. Simplifies life..", "response": "def read( handle, id=None ):\n    \"\"\"\n    Reads a structure via PDBParser.\n    Simplifies life..\n    \"\"\"\n\n    from Bio.PDB import PDBParser\n    \n    if not id:\n        id = os.path.basename(handle).split('.')[0] # Get from filename\n    \n    p = PDBParser()\n    s = p.get_structure(id, handle)\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write( structure, name=None ):\n    \n    from Bio.PDB import PDBIO\n    \n    io = PDBIO()\n    \n    io.set_structure(structure)\n    \n    if not name:\n        s_name = structure.id\n    else:\n        s_name = name\n\n    name = \"%s.pdb\" %s_name\n    seed = 0\n        \n    while 1:\n        if os.path.exists(name):\n            name = \"%s_%s.pdb\" %(s_name, seed)\n            seed +=1\n        else:\n            break\n                \n    io.save(name)\n        \n    return name", "response": "Writes a Structure in PDB format through PDBIO."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload the PISA XML file for the specified list of PDB IDs.", "response": "def download_pisa_multimers_xml(pdb_ids, save_single_xml_files=True, outdir=None, force_rerun=False):\n    \"\"\"Download the PISA XML file for multimers.\n\n    See: http://www.ebi.ac.uk/pdbe/pisa/pi_download.html for more info\n\n    XML description of macromolecular assemblies:\n        http://www.ebi.ac.uk/pdbe/pisa/cgi-bin/multimers.pisa?pdbcodelist\n        where \"pdbcodelist\" is a comma-separated (strictly no spaces) list of PDB codes. The resulting file contain XML\n        output of assembly data, equivalent to that displayed in PISA assembly pages, for each of the specified PDB\n        entries.   NOTE: If a mass-download is intended, please minimize the number of retrievals by specifying as many\n        PDB codes in the URL as feasible (20-50 is a good range), and never send another URL request until the previous\n        one has been completed (meaning that the multimers.pisa file has been downloaded). Excessive requests will\n        silently die in the server queue.\n\n    Args:\n        pdb_ids (str, list): PDB ID or list of IDs\n        save_single_xml_files (bool): If single XML files should be saved per PDB ID. If False, if multiple PDB IDs are\n            provided, then a single, combined XML output file is downloaded\n        outdir (str): Directory to output PISA XML files\n        force_rerun (bool): Redownload files if they already exist\n\n    Returns:\n        list: of files downloaded\n\n    \"\"\"\n    if not outdir:\n        outdir = os.getcwd()\n\n    files = {}\n    pdb_ids = ssbio.utils.force_lower_list(sorted(pdb_ids))\n\n    # If we want to save single PISA XML files per PDB ID...\n    if save_single_xml_files:\n        # Check for existing PISA XML files\n        if not force_rerun:\n            existing_files = [op.basename(x) for x in glob.glob(op.join(outdir, '*_multimers.pisa.xml'))]\n            # Store the paths to these files to return\n            files = {v.split('_')[0]: op.join(outdir, v) for v in existing_files}\n            log.debug('Already downloaded PISA files for {}'.format(list(files.keys())))\n        else:\n            existing_files = []\n\n        # Filter PDB IDs based on existing file\n        pdb_ids = [x for x in pdb_ids if '{}_multimers.pisa.xml'.format(x) not in existing_files]\n\n        # Split the list into 50 to limit requests\n        split_list = ssbio.utils.split_list_by_n(pdb_ids, 40)\n\n        # Download PISA files\n        for l in split_list:\n            pdbs = ','.join(l)\n            all_pisa_link = 'http://www.ebi.ac.uk/pdbe/pisa/cgi-bin/multimers.pisa?{}'.format(pdbs)\n            r = requests.get(all_pisa_link)\n\n            # Parse PISA file and save individual XML files\n            parser = etree.XMLParser(ns_clean=True)\n            tree = etree.fromstring(r.text, parser)\n            for pdb in tree.findall('pdb_entry'):\n                filename = op.join(outdir, '{}_multimers.pisa.xml'.format(pdb.find('pdb_code').text))\n                add_root = etree.Element('pisa_multimers')\n                add_root.append(pdb)\n                with open(filename, 'wb') as f:\n                    f.write(etree.tostring(add_root))\n                files[pdb.find('pdb_code').text] = filename\n                log.debug('{}: downloaded PISA results'.format(pdb))\n\n    else:\n        split_list = ssbio.utils.split_list_by_n(pdb_ids, 40)\n\n        for l in split_list:\n            pdbs = ','.join(l)\n            all_pisa_link = 'http://www.ebi.ac.uk/pdbe/pisa/cgi-bin/multimers.pisa?{}'.format(pdbs)\n\n            filename = op.join(outdir, '{}_multimers.pisa.xml'.format(pdbs))\n\n            if ssbio.utils.force_rerun(flag=force_rerun, outfile=filename):\n                r = requests.get(all_pisa_link)\n                with open(filename, 'w') as f:\n                    f.write(r.text)\n                log.debug('Downloaded PISA results')\n            else:\n                log.debug('PISA results already downloaded')\n\n            for x in l:\n                files[x] = filename\n\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the PISA multimers XML output file and return a dictionary of PISA information.", "response": "def parse_pisa_multimers_xml(pisa_multimers_xml, download_structures=False, outdir=None, force_rerun=False):\n    \"\"\"Retrieve PISA information from an XML results file\n\n    See: http://www.ebi.ac.uk/pdbe/pisa/pi_download.html for more info\n\n    XML description of macromolecular assemblies:\n        http://www.ebi.ac.uk/pdbe/pisa/cgi-bin/multimers.pisa?pdbcodelist\n        where \"pdbcodelist\" is a comma-separated (strictly no spaces) list of PDB codes. The resulting file contain XML\n        output of assembly data, equivalent to that displayed in PISA assembly pages, for each of the specified PDB\n        entries.   NOTE: If a mass-download is intended, please minimize the number of retrievals by specifying as many\n        PDB codes in the URL as feasible (20-50 is a good range), and never send another URL request until the previous\n        one has been completed (meaning that the multimers.pisa file has been downloaded). Excessive requests will\n        silently die in the server queue.\n\n    Args:\n        pisa_multimers_xml (str): Path to PISA XML output file\n        download_structures (bool): If assembly files should be downloaded\n        outdir (str): Directory to output assembly files\n        force_rerun (bool): Redownload files if they already exist\n\n    Returns:\n        dict: of parsed PISA information\n\n    \"\"\"\n    if not outdir:\n        outdir = os.getcwd()\n\n    parser = etree.XMLParser(ns_clean=True)\n    tree = etree.parse(pisa_multimers_xml, parser)\n    root = tree.getroot()\n\n    pisa = defaultdict(dict)\n\n    for pdb in root.findall('pdb_entry'):\n        # Get the PDB ID\n        pdb_id = pdb.find('pdb_code').text\n\n        # Check the assembly status\n        status = pdb.find('status').text\n        errors = ['Entry not found', 'Overlapping structures', 'No symmetry operations']\n        if status in errors:\n            pisa[pdb_id]['status'] = status\n            continue\n\n        # Check monomer status\n        num_complexes = int(pdb.find('total_asm').text)\n        if num_complexes == 0:\n            pisa[pdb_id]['status'] = 'MONOMER'\n            continue\n\n        elif num_complexes > 0:\n            # All \"assembly sets\" (see PISA sets for more info)\n            sets = pdb.findall('asm_set')\n\n            for s in sets:\n                set_id = int(s.find('ser_no').text)\n\n                # All assemblies\n                assemblies = s.findall('assembly')\n\n                for cplx in assemblies:\n\n                    ############################################################################################\n                    # This part tells you the actual composition of the predicted complex (chains and ligands)\n                    parts = cplx.findall('molecule')\n\n                    chains = defaultdict(int)\n\n                    for part in parts:\n                        part_id = part.find('chain_id').text\n                        if part_id.startswith('['):\n                            part_id = 'LIG_' + part_id.split(']')[0].strip('[')\n                        chains[str(part_id)] += 1\n\n                    ligands = {}\n\n                    for key in deepcopy(chains).keys():\n                        if key.startswith('LIG_'):\n                            ligands[str(key.split('_')[1])] = chains.pop(key)\n                    ############################################################################################\n\n                    adder = {}\n\n                    cplx_id = int(cplx.find('id').text)\n                    cplx_composition = str(cplx.find('composition').text)\n                    d_g_diss = float(cplx.find('diss_energy').text)\n                    d_g_int = float(cplx.find('int_energy').text)\n                    pdb_biomol = int(cplx.find('R350').text)\n\n                    if d_g_diss >= 0:\n                        stable = True\n                    else:\n                        stable = False\n\n                    adder['cplx_composition'] = cplx_composition.strip()\n                    adder['cplx_chains'] = chains\n                    adder['cplx_ligands'] = ligands\n                    adder['stable'] = stable\n                    adder['d_g_diss'] = d_g_diss\n                    adder['d_g_int'] = d_g_int\n                    adder['pdb_biomol'] = pdb_biomol\n\n                    pisa[pdb_id][(set_id, cplx_id)] = adder\n\n                    if download_structures:\n                        ident = '{}:{},{}'.format(pdb_id, set_id, cplx_id)\n                        filename = op.join(outdir, ident + '.pdb')\n\n                        if ssbio.utils.force_rerun(flag=force_rerun, outfile=filename):\n                            download_structure_link = 'http://www.ebi.ac.uk/pdbe/pisa/cgi-bin/multimer.pdb?{}'.format(\n                                ident)\n                            r = requests.get(download_structure_link)\n\n                            with open(filename, 'w') as f:\n                                f.write(r.text)\n\n                            log.debug('{}: downloaded structure file'.format(ident))\n                        else:\n                            log.debug('{}: structure file already downloaded'.format(ident))\n\n                        pisa[pdb_id][(set_id, cplx_id)]['structure_file'] = filename\n\n    return pisa"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the gene folder of the class if it exists else None.", "response": "def gene_dir(self):\n        \"\"\"Gene folder\"\"\"\n        if self.root_dir:\n            return op.join(self.root_dir, self.id)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_modified_gene(self, modified_gene, ignore_model_attributes=True):\n        ignore = ['_model', '_reaction', '_functional', 'model', 'reaction', 'functional']\n        for attr in filter(lambda a: not a.startswith('__') and not isinstance(getattr(type(self), a, None), property) and not callable(getattr(self, a)),\n                           dir(modified_gene)):\n            if attr not in ignore and ignore_model_attributes:\n                setattr(self, attr, getattr(modified_gene, attr))", "response": "Copy attributes of a Gene object over to this Gene."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a structure file and provide pointers to its location", "response": "def load_structure_path(self, structure_path, file_type):\n        \"\"\"Load a structure file and provide pointers to its location\n\n        Args:\n            structure_path (str): Path to structure file\n            file_type (str): Type of structure file\n\n        \"\"\"\n\n        if not file_type:\n            raise ValueError('File type must be specified')\n\n        self.file_type = file_type\n        self.structure_dir = op.dirname(structure_path)\n        self.structure_file = op.basename(structure_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_structure(self, store_in_memory=False):\n        # TODO: perhaps add option to parse into ProDy object?\n        if not self.structure_file:\n            log.error('{}: no structure file, unable to parse'.format(self.id))\n            return None\n        else:\n            # Add Biopython structure object\n            structure = StructureIO(self.structure_path, self.file_type)\n\n            # Add all chains to self.chains as ChainProp objects\n            structure_chains = [x.id for x in structure.first_model.child_list]\n            self.add_chain_ids(structure_chains)\n            self.get_structure_seqs(structure.first_model)\n\n            # Also add all chains to self.mapped_chains ONLY if there are none specified\n            if not self.mapped_chains:\n                self.add_mapped_chain_ids(structure_chains)\n\n            if store_in_memory:\n                self.parsed = True\n                self.structure = structure\n\n            return structure", "response": "Read the 3D coordinates of a structure file and return it as a Biopython Structure object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean_structure(self, out_suffix='_clean', outdir=None, force_rerun=False,\n                        remove_atom_alt=True, keep_atom_alt_id='A',remove_atom_hydrogen=True,  add_atom_occ=True,\n                        remove_res_hetero=True, keep_chemicals=None, keep_res_only=None,\n                        add_chain_id_if_empty='X', keep_chains=None):\n        \"\"\"Clean the structure file associated with this structure, and save it as a new file. Returns the file path.\n\n        Args:\n            out_suffix (str): Suffix to append to original filename\n            outdir (str): Path to output directory\n            force_rerun (bool): If structure should be re-cleaned if a clean file exists already\n            remove_atom_alt (bool): Remove alternate positions\n            keep_atom_alt_id (str): If removing alternate positions, which alternate ID to keep\n            remove_atom_hydrogen (bool): Remove hydrogen atoms\n            add_atom_occ (bool): Add atom occupancy fields if not present\n            remove_res_hetero (bool): Remove all HETATMs\n            keep_chemicals (str, list): If removing HETATMs, keep specified chemical names\n            keep_res_only (str, list): Keep ONLY specified resnames, deletes everything else!\n            add_chain_id_if_empty (str): Add a chain ID if not present\n            keep_chains (str, list): Keep only these chains\n\n        Returns:\n            str: Path to cleaned PDB file\n\n        \"\"\"\n\n        if not self.structure_file:\n            log.error('{}: no structure file, unable to clean'.format(self.id))\n            return None\n\n        clean_pdb_file = ssbio.protein.structure.utils.cleanpdb.clean_pdb(self.structure_path, out_suffix=out_suffix,\n                                                                          outdir=outdir, force_rerun=force_rerun,\n                                                                          remove_atom_alt=remove_atom_alt,\n                                                                          remove_atom_hydrogen=remove_atom_hydrogen,\n                                                                          keep_atom_alt_id=keep_atom_alt_id,\n                                                                          add_atom_occ=add_atom_occ,\n                                                                          remove_res_hetero=remove_res_hetero,\n                                                                          keep_chemicals=keep_chemicals,\n                                                                          keep_res_only=keep_res_only,\n                                                                          add_chain_id_if_empty=add_chain_id_if_empty,\n                                                                          keep_chains=keep_chains)\n\n        return clean_pdb_file", "response": "Clean the structure file associated with this structure and save it as a new file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_mapped_chain_ids(self, mapped_chains):\n        mapped_chains = ssbio.utils.force_list(mapped_chains)\n\n        for c in mapped_chains:\n            if c not in self.mapped_chains:\n                self.mapped_chains.append(c)\n                log.debug('{}: added to list of mapped chains'.format(c))\n            else:\n                log.debug('{}: chain already in list of mapped chains, not adding'.format(c))", "response": "Add chains by ID into the mapped_chains attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding chains by ID into the chains attribute.", "response": "def add_chain_ids(self, chains):\n        \"\"\"Add chains by ID into the chains attribute\n\n        Args:\n            chains (str, list): Chain ID or list of IDs\n\n        \"\"\"\n        chains = ssbio.utils.force_list(chains)\n\n        for c in chains:\n            if self.chains.has_id(c):\n                log.debug('{}: chain already present'.format(c))\n            else:\n                chain_prop = ChainProp(ident=c, pdb_parent=self.id)\n                self.chains.append(chain_prop)\n                log.debug('{}: added to chains list'.format(c))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngathers chain sequences and store in their corresponding ChainProp objects in the chains attribute.", "response": "def get_structure_seqs(self, model):\n        \"\"\"Gather chain sequences and store in their corresponding ``ChainProp`` objects in the ``chains`` attribute.\n\n        Args:\n            model (Model): Biopython Model object of the structure you would like to parse\n\n        \"\"\"\n\n        # Don't overwrite existing ChainProp objects\n        dont_overwrite = []\n        chains = list(model.get_chains())\n        for x in chains:\n            if self.chains.has_id(x.id):\n                if self.chains.get_by_id(x.id).seq_record:\n                    dont_overwrite.append(x.id)\n        if len(dont_overwrite) == len(chains):\n            log.debug('Not writing structure sequences, already stored')\n            return\n\n        # Returns the structures sequences with Xs added\n        structure_seqs = ssbio.protein.structure.properties.residues.get_structure_seqrecords(model)\n        log.debug('{}: gathered chain sequences'.format(self.id))\n\n        # Associate with ChainProps\n        for seq_record in structure_seqs:\n            log.debug('{}: adding chain sequence to ChainProp'.format(seq_record.id))\n            my_chain = self.chains.get_by_id(seq_record.id)\n            my_chain.seq_record = seq_record"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dict_with_chain(self, chain, only_keys=None, chain_keys=None, exclude_attributes=None, df_format=False):\n\n        # Choose attributes to return, return everything in the object if a list is not specified\n        if not only_keys:\n            keys = list(self.__dict__.keys())\n        else:\n            keys = ssbio.utils.force_list(only_keys)\n\n        # Remove keys you don't want returned\n        if exclude_attributes:\n            exclude_attributes = ssbio.utils.force_list(exclude_attributes)\n            for x in exclude_attributes:\n                if x in keys:\n                    keys.remove(x)\n        else:\n            exclude_attributes = []\n\n        exclude_attributes.extend(['mapped_chains', 'chains'])\n\n        final_dict = {k: v for k, v in Object.get_dict(self, only_attributes=keys, exclude_attributes=exclude_attributes,\n                                                       df_format=df_format).items()}\n\n        chain_prop = self.chains.get_by_id(chain)\n        # Filter out keys that show up in StructProp\n        if not chain_keys:\n            chain_keys = [x for x in chain_prop.get_dict().keys() if x not in final_dict]\n\n        chain_dict = chain_prop.get_dict(only_attributes=chain_keys, df_format=df_format)\n        final_dict.update(chain_dict)\n\n        return final_dict", "response": "Get a dictionary of attributes of a specific chain."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds potential disulfide bridges for each chain and store in ChainProp.", "response": "def find_disulfide_bridges(self, threshold=3.0):\n        \"\"\"Run Biopython's search_ss_bonds to find potential disulfide bridges for each chain and store in ChainProp.\n\n        Will add a list of tuple pairs into the annotations field, looks like this::\n\n            [ ((' ', 79, ' '), (' ', 110, ' ')),\n              ((' ', 174, ' '), (' ', 180, ' ')),\n              ((' ', 369, ' '), (' ', 377, ' '))]\n\n        Where each pair is a pair of cysteine residues close together in space.\n\n        \"\"\"\n\n        if self.structure:\n            parsed = self.structure\n        else:\n            parsed = self.parse_structure()\n\n        if not parsed:\n            log.error('{}: unable to open structure to find S-S bridges'.format(self.id))\n            return\n\n        disulfide_bridges = ssbio.protein.structure.properties.residues.search_ss_bonds(parsed.first_model,\n                                                                                        threshold=threshold)\n        if not disulfide_bridges:\n            log.debug('{}: no disulfide bridges found'.format(self.id))\n\n        for chain, bridges in disulfide_bridges.items():\n            self.chains.get_by_id(chain).seq_record.annotations['SSBOND-biopython'] = disulfide_bridges[chain]\n            log.debug('{}: found {} disulfide bridges'.format(chain, len(bridges)))\n            log.debug('{}: stored disulfide bridges in the chain\\'s seq_record letter_annotations'.format(chain))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Polypeptide object of the amino acids within X angstroms of the specified chain + residue number.", "response": "def get_polypeptide_within(self, chain_id, resnum, angstroms, only_protein=True,\n                               use_ca=False, custom_coord=None, return_resnums=False):\n        \"\"\"Get a Polypeptide object of the amino acids within X angstroms of the specified chain + residue number.\n\n        Args:\n            resnum (int): Residue number of the structure\n            chain_id (str): Chain ID of the residue number\n            angstroms (float): Radius of the search sphere\n            only_protein (bool): If only protein atoms (no HETATMS) should be included in the returned sequence\n            use_ca (bool): If the alpha-carbon atom should be used for searching, default is False (last atom of residue used)\n            custom_coord (list): custom XYZ coord\n            return_resnums (bool): if list of resnums should be returned\n\n        Returns:\n            Bio.PDB.Polypeptide.Polypeptide: Biopython Polypeptide object\n\n        \"\"\"\n        # XTODO: documentation, unit test\n        if self.structure:\n            parsed = self.structure\n        else:\n            parsed = self.parse_structure()\n\n        residue_list = ssbio.protein.structure.properties.residues.within(resnum=resnum, chain_id=chain_id,\n                                                                          model=parsed.first_model,\n                                                                          angstroms=angstroms, use_ca=use_ca,\n                                                                          custom_coord=custom_coord)\n\n        if only_protein:\n            filtered_residue_list = [x for x in residue_list if x.id[0] == ' ']\n        else:\n            filtered_residue_list = residue_list\n\n        residue_list_combined = Polypeptide(filtered_residue_list)\n\n        if return_resnums:\n            resnums = [int(x.id[1]) for x in filtered_residue_list]\n            return residue_list_combined, resnums\n\n        return residue_list_combined"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_seqprop_within(self, chain_id, resnum, angstroms, only_protein=True,\n                           use_ca=False, custom_coord=None, return_resnums=False):\n        \"\"\"Get a SeqProp object of the amino acids within X angstroms of the specified chain + residue number.\n\n        Args:\n            resnum (int): Residue number of the structure\n            chain_id (str): Chain ID of the residue number\n            angstroms (float): Radius of the search sphere\n            only_protein (bool): If only protein atoms (no HETATMS) should be included in the returned sequence\n            use_ca (bool): If the alpha-carbon atom should be used for searching, default is False (last atom of residue used)\n\n        Returns:\n            SeqProp: Sequence that represents the amino acids in the vicinity of your residue number.\n\n        \"\"\"\n        # XTODO: change \"remove\" parameter to be clean_seq and to remove all non standard amino acids\n        # TODO: make return_resnums smarter\n        polypep, resnums = self.get_polypeptide_within(chain_id=chain_id, resnum=resnum, angstroms=angstroms, use_ca=use_ca,\n                                                      only_protein=only_protein, custom_coord=custom_coord,\n                                                      return_resnums=True)\n\n        # final_seq = polypep.get_sequence()\n        # seqprop = SeqProp(id='{}-{}_within_{}_of_{}'.format(self.id, chain_id, angstroms, resnum),\n        #                   seq=final_seq)\n\n        chain_subseq = self.chains.get_by_id(chain_id).get_subsequence(resnums)\n\n        if return_resnums:\n            return chain_subseq, resnums\n        else:\n            return chain_subseq", "response": "Get a SeqProp object of the amino acids within X angstroms of the specified chain + residue number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun DSSP on this structure and store the DSSP annotations in the corresponding ChainProp SeqRecords.", "response": "def get_dssp_annotations(self, outdir, force_rerun=False):\n        \"\"\"Run DSSP on this structure and store the DSSP annotations in the corresponding ChainProp SeqRecords\n\n        Calculations are stored in the ChainProp's ``letter_annotations`` at the following keys:\n\n            * ``SS-dssp``\n            * ``RSA-dssp``\n            * ``ASA-dssp``\n            * ``PHI-dssp``\n            * ``PSI-dssp``\n\n        Args:\n            outdir (str): Path to where DSSP dataframe will be stored.\n            force_rerun (bool): If DSSP results should be recalculated\n\n        TODO:\n            * Also parse global properties, like total accessible surface area. Don't think Biopython parses those?\n\n        \"\"\"\n        if self.structure:\n            parsed = self.structure\n        else:\n            parsed = self.parse_structure()\n\n        if not parsed:\n            log.error('{}: unable to open structure to run DSSP'.format(self.id))\n            return\n\n        log.debug('{}: running DSSP'.format(self.id))\n        dssp_results = ssbio.protein.structure.properties.dssp.get_dssp_df(model=parsed.first_model,\n                                                                           pdb_file=self.structure_path,\n                                                                           outdir=outdir,\n                                                                           force_rerun=force_rerun)\n\n        if dssp_results.empty:\n            log.error('{}: unable to run DSSP'.format(self.id))\n            return\n\n        chains = dssp_results.chain.unique()\n        dssp_summary = ssbio.protein.structure.properties.dssp.secondary_structure_summary(dssp_results)\n\n        for chain in chains:\n            ss = dssp_results[dssp_results.chain == chain].ss.tolist()\n            exposure_rsa = dssp_results[dssp_results.chain == chain].exposure_rsa.tolist()\n            exposure_asa = dssp_results[dssp_results.chain == chain].exposure_asa.tolist()\n            phi = dssp_results[dssp_results.chain == chain].phi.tolist()\n            psi = dssp_results[dssp_results.chain == chain].psi.tolist()\n\n            chain_prop = self.chains.get_by_id(chain)\n            chain_seq = chain_prop.seq_record\n\n            # Making sure the X's are filled in\n            ss = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain_seq,\n                                                                                      new_seq=ss,\n                                                                                      fill_with='-')\n            exposure_rsa = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain_seq,\n                                                                                                new_seq=exposure_rsa,\n                                                                                                fill_with=float('Inf'))\n            exposure_asa = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain_seq,\n                                                                                                new_seq=exposure_asa,\n                                                                                                fill_with=float('Inf'))\n            phi = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain_seq,\n                                                                                       new_seq=phi,\n                                                                                       fill_with=float('Inf'))\n            psi = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain_seq,\n                                                                                       new_seq=psi,\n                                                                                       fill_with=float('Inf'))\n\n            chain_prop.seq_record.annotations.update(dssp_summary[chain])\n\n            chain_prop.seq_record.letter_annotations['SS-dssp'] = ss\n            chain_prop.seq_record.letter_annotations['RSA-dssp'] = exposure_rsa\n            chain_prop.seq_record.letter_annotations['ASA-dssp'] = exposure_asa\n            chain_prop.seq_record.letter_annotations['PHI-dssp'] = phi\n            chain_prop.seq_record.letter_annotations['PSI-dssp'] = psi\n            log.debug('{}: stored DSSP annotations in chain seq_record letter_annotations'.format(chain))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_msms_annotations(self, outdir, force_rerun=False):\n        # Now can run on Biopython Model objects exclusively thanks to Biopython updates\n        # if self.file_type != 'pdb':\n        #     raise ValueError('{}: unable to run MSMS with \"{}\" file type. Please change file type to \"pdb\"'.format(self.id,\n        #                                                                                                     self.file_type))\n\n        if self.structure:\n            parsed = self.structure\n        else:\n            parsed = self.parse_structure()\n\n        if not parsed:\n            log.error('{}: unable to open structure to run MSMS'.format(self.id))\n            return\n\n        log.debug('{}: running MSMS'.format(self.id))\n        # PDB ID is currently set to the structure file so the output name is the same with _msms.df appended to it\n        msms_results = ssbio.protein.structure.properties.msms.get_msms_df(model=parsed.first_model, pdb_id=self.structure_path,\n                                                                           outdir=outdir, force_rerun=force_rerun)\n        if msms_results.empty:\n            log.error('{}: unable to run MSMS'.format(self.id))\n            return\n\n        chains = msms_results.chain.unique()\n\n        for chain in chains:\n            res_depths = msms_results[msms_results.chain == chain].res_depth.tolist()\n            ca_depths = msms_results[msms_results.chain == chain].ca_depth.tolist()\n\n            chain_prop = self.chains.get_by_id(chain)\n            chain_seq = chain_prop.seq_record\n\n            # Making sure the X's are filled in\n            res_depths = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain_seq,\n                                                                                              new_seq=res_depths,\n                                                                                              fill_with=float('Inf'))\n\n            ca_depths = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain_seq,\n                                                                                             new_seq=ca_depths,\n                                                                                             fill_with=float('Inf'))\n\n            chain_prop.seq_record.letter_annotations['RES_DEPTH-msms'] = res_depths\n            chain_prop.seq_record.letter_annotations['CA_DEPTH-msms'] = ca_depths\n            log.debug('{}: stored residue depths in chain seq_record letter_annotations'.format(chain))", "response": "Run MSMS on this structure and store the residue depths and ca depths in the corresponding ChainProp SeqRecords\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns freesasa on this structure and store the calculated properties in the corresponding ChainProps .", "response": "def get_freesasa_annotations(self, outdir, include_hetatms=False, force_rerun=False):\n        \"\"\"Run ``freesasa`` on this structure and store the calculated properties in the corresponding ChainProps\n        \"\"\"\n        if self.file_type != 'pdb':\n            log.error('{}: unable to run freesasa with \"{}\" file type. Please change file type to \"pdb\"'.format(self.id,\n                                                                                                                self.file_type))\n            return\n\n        # Parse the structure to store chain sequences\n        if self.structure:\n            parsed = self.structure\n        else:\n            parsed = self.parse_structure()\n\n        if not parsed:\n            log.error('{}: unable to open structure to run freesasa'.format(self.id))\n            return\n\n        # Set outfile name\n        log.debug('{}: running freesasa'.format(self.id))\n        if include_hetatms:\n            outfile = '{}.freesasa_het.rsa'.format(self.id)\n        else:\n            outfile = '{}.freesasa_nohet.rsa'.format(self.id)\n\n        # Run freesasa\n        result = fs.run_freesasa(infile=self.structure_path,\n                                 outfile=outfile,\n                                 include_hetatms=include_hetatms,\n                                 outdir=outdir,\n                                 force_rerun=force_rerun)\n\n        # Parse results\n        result_parsed = fs.parse_rsa_data(result)\n        prop_dict = defaultdict(lambda: defaultdict(list))\n        for k, v in result_parsed.items():\n            chain = k[0]\n            for prop, calc in v.items():\n                prop_dict[chain][prop].append(calc)\n\n        # Reorganize and store results\n        all_props = ['all_atoms_abs', 'all_atoms_rel', 'side_chain_abs', 'side_chain_rel', 'main_chain_abs',\n                     'main_chain_rel', 'non_polar_abs', 'non_polar_rel', 'all_polar_abs', 'all_polar_rel']\n        all_props_renamed = {'all_atoms_abs' : 'ASA_ALL-freesasa',\n                             'all_atoms_rel' : 'RSA_ALL-freesasa',\n                             'all_polar_abs' : 'ASA_POLAR-freesasa',\n                             'all_polar_rel' : 'RSA_POLAR-freesasa',\n                             'main_chain_abs': 'ASA_BACKBONE-freesasa',\n                             'main_chain_rel': 'RSA_BACKBONE-freesasa',\n                             'non_polar_abs' : 'ASA_NONPOLAR-freesasa',\n                             'non_polar_rel' : 'RSA_NONPOLAR-freesasa',\n                             'side_chain_abs': 'ASA_RESIDUE-freesasa',\n                             'side_chain_rel': 'RSA_RESIDUE-freesasa'}\n\n        ## Rename dictionary keys based on if HETATMs were included\n        if include_hetatms:\n            suffix = '_het'\n        else:\n            suffix = '_nohet'\n\n        for k, v in all_props_renamed.items():\n            all_props_renamed[k] = v + suffix\n\n        for chain in self.chains:\n            for prop in all_props:\n                prop_list = ssbio.protein.structure.properties.residues.match_structure_sequence(orig_seq=chain.seq_record,\n                                                                                                 new_seq=prop_dict[chain.id][prop],\n                                                                                                 fill_with=float('Inf'),\n                                                                                                 ignore_excess=True)\n                chain.seq_record.letter_annotations[all_props_renamed[prop]] = prop_list\n            log.debug('{}: stored freesasa calculations in chain seq_record letter_annotations'.format(chain))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses NGLviewer to display a structure in a Jupyter notebook.", "response": "def view_structure(self, only_chains=None, opacity=1.0, recolor=False, gui=False):\n        \"\"\"Use NGLviewer to display a structure in a Jupyter notebook\n\n        Args:\n            only_chains (str, list): Chain ID or IDs to display\n            opacity (float): Opacity of the structure\n            recolor (bool): If structure should be cleaned and recolored to silver\n            gui (bool): If the NGLview GUI should show up\n\n        Returns:\n            NGLviewer object\n\n        \"\"\"\n        # TODO: show_structure_file does not work for MMTF files - need to check for that and load accordingly\n\n        if ssbio.utils.is_ipynb():\n            import nglview as nv\n        else:\n            raise EnvironmentError('Unable to display structure - not running in a Jupyter notebook environment')\n\n        if not self.structure_file:\n            raise ValueError(\"Structure file not loaded\")\n\n        only_chains = ssbio.utils.force_list(only_chains)\n        to_show_chains = '( '\n        for c in only_chains:\n            to_show_chains += ':{} or'.format(c)\n        to_show_chains = to_show_chains.strip(' or ')\n        to_show_chains += ' )'\n\n        if self.file_type == 'mmtf' or self.file_type == 'mmtf.gz':\n            view = nv.NGLWidget()\n            view.add_component(self.structure_path)\n        else:\n            view = nv.show_structure_file(self.structure_path, gui=gui)\n\n        if recolor:\n            view.clear_representations()\n            if only_chains:\n                view.add_cartoon(selection='{} and (not hydrogen)'.format(to_show_chains), color='silver', opacity=opacity)\n            else:\n                view.add_cartoon(selection='protein', color='silver', opacity=opacity)\n        elif only_chains:\n            view.clear_representations()\n            view.add_cartoon(selection='{} and (not hydrogen)'.format(to_show_chains), color='silver', opacity=opacity)\n\n        return view"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a list of residue numbers to a NGLView object or a dictionary of residue numbers to counts. Size and opacity of added residues are scaled by counts.", "response": "def add_scaled_residues_highlight_to_nglview(self, view, structure_resnums, chain=None, color='red',\n                                                 unique_colors=False, opacity_range=(0.5,1), scale_range=(.7, 10),\n                                                 multiplier=None):\n        \"\"\"Add a list of residue numbers (which may contain repeating residues) to a view, or add a dictionary of\n            residue numbers to counts. Size and opacity of added residues are scaled by counts.\n\n        Args:\n            view (NGLWidget): NGLWidget view object\n            structure_resnums (int, list, dict): Residue number(s) to highlight, or a dictionary of residue number to\n                frequency count\n            chain (str, list): Chain ID or IDs of which residues are a part of. If not provided, all chains in the\n                mapped_chains attribute will be used. If that is also empty, and exception is raised.\n            color (str): Color to highlight residues with\n            unique_colors (bool): If each mutation should be colored uniquely (will override color argument)\n            opacity_range (tuple): Min/max opacity values (residues that have higher frequency counts will be opaque)\n            scale_range (tuple): Min/max size values (residues that have higher frequency counts will be bigger)\n\n        \"\"\"\n        # TODO: likely to move these functions to a separate nglview/utils folder since they are not coupled to the structure\n        # TODO: add color by letter_annotations!\n        if not chain:\n            chain = self.mapped_chains\n            if not chain:\n                raise ValueError('Please input chain ID to display residue on')\n        else:\n            chain = ssbio.utils.force_list(chain)\n\n        if isinstance(structure_resnums, dict):\n            if not multiplier:\n                multiplier = 1\n            opacity_dict = ssbio.utils.scale_calculator(multiplier, structure_resnums, rescale=opacity_range)\n            scale_dict = ssbio.utils.scale_calculator(multiplier, structure_resnums, rescale=scale_range)\n        else:\n            opacity_dict = {x: max(opacity_range) for x in ssbio.utils.force_list(structure_resnums)}\n            scale_dict = {x: max(scale_range) for x in ssbio.utils.force_list(structure_resnums)}\n\n        if isinstance(structure_resnums, list):\n            structure_resnums = list(set(structure_resnums))\n        elif isinstance(structure_resnums, dict):\n            structure_resnums = list(structure_resnums.keys())\n        elif isinstance(structure_resnums, int):\n            structure_resnums = ssbio.utils.force_list(structure_resnums)\n        else:\n            raise ValueError('Input must either be a list of residue numbers or a dictionary of residue numbers '\n                             'and their frequency.')\n\n        colors = sns.color_palette(\"hls\", len(structure_resnums)).as_hex()\n\n        to_show_chains = '( '\n        for c in chain:\n            to_show_chains += ':{} or'.format(c)\n        to_show_chains = to_show_chains.strip(' or ')\n        to_show_chains += ' )'\n\n        for i, x in enumerate(structure_resnums):\n            if isinstance(x, tuple):\n                to_show_res = '( '\n                for mut in x:\n                    to_show_res += '{} or '.format(mut)\n                to_show_res = to_show_res.strip(' or ')\n                to_show_res += ' )'\n            else:\n                to_show_res = x\n\n            log.info('Selection: {} and not hydrogen and {}'.format(to_show_chains, to_show_res))\n\n            if unique_colors:\n                view.add_ball_and_stick(selection='{} and not hydrogen and {}'.format(to_show_chains, to_show_res),\n                                        color=colors[i], opacity=opacity_dict[x], scale=scale_dict[x])\n            else:\n                view.add_ball_and_stick(selection='{} and not hydrogen and {}'.format(to_show_chains, to_show_res),\n                                        color=color, opacity=opacity_dict[x], scale=scale_dict[x])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the long format TMHMM and return a dictionary of { sequence_ID : TMHMM_prediction }.", "response": "def parse_tmhmm_long(tmhmm_results):\n    \"\"\"Parse the 'long' output format of TMHMM and return a dictionary of ``{sequence_ID: TMHMM_prediction}``.\n\n    Args:\n        tmhmm_results (str): Path to long format TMHMM output.\n\n    Returns:\n        dict: Dictionary of ``{sequence_ID: TMHMM_prediction}``\n\n    \"\"\"\n    with open(tmhmm_results) as f:\n        lines = f.read().splitlines()\n\n    infodict = defaultdict(dict)\n\n    for l in lines:\n        if 'Number of predicted TMHs:' in l:\n            gene = l.split(' Number')[0].strip('# ')\n            infodict[gene]['num_tm_helices'] = int(l.split(': ')[1])\n\n        if 'WARNING' in l:\n            log.warning('{}: no TMHMM predictions'.format(l))\n            continue\n\n        # TODO: POSSIBLE N-term signal sequence - parse this\n\n        # Look for the lines without #, these are the TM predicted regions\n        if '#' not in l:\n            stuff = l.split()\n            if stuff[1] == 'TMHMM2.0':\n\n                gene = stuff[0]\n                region = stuff[2]\n                region_start = stuff[3]\n                region_end = stuff[4]\n\n                if 'sequence' in infodict[gene]:\n                    tm_seq = infodict[gene]['sequence']\n                else:\n                    tm_seq = ''\n\n                if region == 'outside':\n                    info = 'O'\n                elif region == 'inside':\n                    info = 'I'\n                elif region == 'TMhelix':\n                    info = 'T'\n                else:\n                    log.error('{}: unknown region type'.format(info))\n                    info = '-'\n\n                for r in range(int(region_start), int(region_end) + 1):\n                    tm_seq += info\n\n                infodict[gene]['sequence'] = tm_seq\n\n    return infodict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef label_TM_tmhmm_residue_numbers_and_leaflets(tmhmm_seq):\n\n    TM_number_dict = {}\n    T_index = []\n    T_residue = []\n\n    residue_count = 1\n    for residue_label in tmhmm_seq:\n        if residue_label == 'T':\n            T_residue.append(residue_count)\n\n        residue_count = residue_count + 1\n    TM_number_dict.update({'T_residue': T_residue})\n\n    # finding the TM boundaries\n    T_residue_list = TM_number_dict['T_residue']\n\n    count = 0\n    max_count = len(T_residue_list) - 1\n    TM_helix_count = 0\n    TM_boundary_dict = {}\n\n    while count <= max_count:\n        # first residue = TM start\n        if count == 0:\n            TM_start = T_residue_list[count]\n            count = count + 1\n            continue\n        # Last residue = TM end\n        elif count == max_count:\n            TM_end = T_residue_list[count]\n            TM_helix_count = TM_helix_count + 1\n            TM_boundary_dict.update({'TM_helix_' + str(TM_helix_count): [TM_start, TM_end]})\n            break\n        # middle residues need to be start or end\n        elif T_residue_list[count] != T_residue_list[count + 1] - 1:\n            TM_end = T_residue_list[count]\n            TM_helix_count = TM_helix_count + 1\n            TM_boundary_dict.update({'TM_helix_' + str(TM_helix_count): [TM_start, TM_end]})\n            # new TM_start\n            TM_start = T_residue_list[count + 1]\n        count = count + 1\n    # assign leaflet to proper TM residues O or I\n    leaflet_dict = {}\n    for leaflet in ['O', 'I']:\n        leaflet_list = []\n        for TM_helix, TM_residues in TM_boundary_dict.items():\n            for residue_num in TM_residues:\n                tmhmm_seq_index = residue_num - 1\n                previous_residue = tmhmm_seq_index - 1\n                next_residue = tmhmm_seq_index + 1\n                # identify if the previous or next residue closest to the TM helix start/end is the proper leaflet\n                if tmhmm_seq[previous_residue] == leaflet or tmhmm_seq[next_residue] == leaflet:\n                    leaflet_list.append(residue_num)\n        leaflet_dict.update({'tmhmm_leaflet_' + leaflet: leaflet_list})\n\n    return TM_boundary_dict, leaflet_dict", "response": "This function returns the residue numbers of the TM - helices that cross the membrane and label them by leaflets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsubmit sequence and structural class to FOLD -RATE calculator.", "response": "def get_foldrate(seq, secstruct):\n    \"\"\"Submit sequence and structural class to FOLD-RATE calculator (http://www.iitm.ac.in/bioinfo/fold-rate/)\n    to calculate kinetic folding rate.\n\n    Args:\n        seq (str, Seq, SeqRecord): Amino acid sequence\n        secstruct (str): Structural class: `all-alpha``, ``all-beta``, ``mixed``, or ``unknown``\n\n    Returns:\n        float: Kinetic folding rate k_f\n\n    \"\"\"\n\n    seq = ssbio.protein.sequence.utils.cast_to_str(seq)\n\n    url = 'http://www.iitm.ac.in/bioinfo/cgi-bin/fold-rate/foldrateCalculator.pl'\n\n    values = {'sequence': seq, 'eqn': secstruct}\n    data = urlencode(values)\n    data = data.encode('ASCII')\n    response = urlopen(url, data)\n\n    result = str(response.read())\n    ind = str.find(result, 'The folding rate,')\n    result2 = result[ind:ind + 70]\n    ind1 = str.find(result2, '=')\n    ind2 = str.find(result2, '/sec')\n    rate = result2[ind1 + 2:ind2]\n\n    return rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_foldrate_at_temp(ref_rate, new_temp, ref_temp=37.0):\n\n    # Not much data available on this slope value, however its effect on growth rate in a model is very small\n    slope = 22000\n\n    # Get folding rate for the reference temperature\n    preFactor = float(ref_rate) + slope / (float(ref_temp) + 273.15)\n\n    # Calculate folding rate at desired temperature\n    rate = math.exp(preFactor - slope / (float(new_temp) + 273.15))\n\n    return rate", "response": "Scale the predicted kinetic folding rate of a protein to a given temperature"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing Biopython to calculate properties using a sliding window over a sequence given a specific scale to use.", "response": "def biopython_protein_scale(inseq, scale, custom_scale_dict=None, window=7):\n    \"\"\"Use Biopython to calculate properties using a sliding window over a sequence given a specific scale to use.\"\"\"\n\n    if scale == 'kd_hydrophobicity':\n        scale_dict = kd_hydrophobicity_one\n    elif scale == 'bulkiness':\n        scale_dict = bulkiness_one\n    elif scale == 'custom':\n        scale_dict = custom_scale_dict\n    else:\n        raise ValueError('Scale not available')\n\n    inseq = ssbio.protein.sequence.utils.cast_to_str(inseq)\n    analysed_seq = ProteinAnalysis(inseq)\n    result = analysed_seq.protein_scale(param_dict=scale_dict, window=window)\n\n    # Correct list length by prepending and appending \"inf\" (result needs to be same length as sequence)\n    for i in range(window // 2):\n        result.insert(0, float(\"Inf\"))\n        result.append(float(\"Inf\"))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef biopython_protein_analysis(inseq):\n\n    inseq = ssbio.protein.sequence.utils.cast_to_str(inseq)\n\n    analysed_seq = ProteinAnalysis(inseq)\n\n    info_dict = {}\n    info_dict['amino_acids_content-biop'] = analysed_seq.count_amino_acids()\n    info_dict['amino_acids_percent-biop'] = analysed_seq.get_amino_acids_percent()\n    info_dict['length-biop'] = analysed_seq.length\n    info_dict['monoisotopic-biop'] = analysed_seq.monoisotopic\n    info_dict['molecular_weight-biop'] = analysed_seq.molecular_weight()\n    info_dict['aromaticity-biop'] = analysed_seq.aromaticity()\n    info_dict['instability_index-biop'] = analysed_seq.instability_index()\n    # TODO: What is flexibility?\n    info_dict['flexibility-biop'] = analysed_seq.flexibility()\n    info_dict['isoelectric_point-biop'] = analysed_seq.isoelectric_point()\n\n    # grand average of hydrophobicity\n    info_dict['gravy-biop'] = analysed_seq.gravy()\n\n    # Separated secondary_structure_fraction into each definition\n    # info_dict['secondary_structure_fraction-biop'] = analysed_seq.secondary_structure_fraction()\n    info_dict['percent_helix_naive-biop'] = analysed_seq.secondary_structure_fraction()[0]\n    info_dict['percent_turn_naive-biop'] = analysed_seq.secondary_structure_fraction()[1]\n    info_dict['percent_strand_naive-biop'] = analysed_seq.secondary_structure_fraction()[2]\n\n    return info_dict", "response": "Utiize Biopython s ProteinAnalysis module to return a dictionary of properties of an amino acid string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef emboss_pepstats_on_fasta(infile, outfile='', outdir='', outext='.pepstats', force_rerun=False):\n\n    # Create the output file name\n    outfile = ssbio.utils.outfile_maker(inname=infile, outname=outfile, outdir=outdir, outext=outext)\n\n    # Run pepstats\n    program = 'pepstats'\n    pepstats_args = '-sequence=\"{}\" -outfile=\"{}\"'.format(infile, outfile)\n    cmd_string = '{} {}'.format(program, pepstats_args)\n    ssbio.utils.command_runner(cmd_string, force_rerun_flag=force_rerun, outfile_checker=outfile, silent=True)\n\n    return outfile", "response": "Run EMBOSS pepstats on a FASTA file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the pepstats file and return a dictionary of parsed information.", "response": "def emboss_pepstats_parser(infile):\n    \"\"\"Get dictionary of pepstats results.\n\n    Args:\n        infile: Path to pepstats outfile\n\n    Returns:\n        dict: Parsed information from pepstats\n\n    TODO:\n        Only currently parsing the bottom of the file for percentages of properties.\n\n    \"\"\"\n    with open(infile) as f:\n        lines = f.read().split('\\n')\n\n    info_dict = {}\n\n    for l in lines[38:47]:\n        info = l.split('\\t')\n        cleaninfo = list(filter(lambda x: x != '', info))\n        prop = cleaninfo[0]\n        num = cleaninfo[2]\n        percent = float(cleaninfo[-1]) / float(100)\n\n        info_dict['mol_percent_' + prop.lower() + '-pepstats'] = percent\n\n    return info_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_strain(self, strain_id, strain_genome_file):\n        # logging.disable(logging.WARNING)\n        strain_gp = GEMPRO(gem_name=strain_id, genome_path=strain_genome_file, write_protein_fasta_files=False)\n        # logging.disable(logging.NOTSET)\n\n        self.strains.append(strain_gp)\n        return self.strains.get_by_id(strain_id)", "response": "Load a strain as a new GEM - PRO by its ID and associated genome file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload sequences from PATRIC and load them as strains.", "response": "def download_patric_genomes(self, ids, force_rerun=False):\n        \"\"\"Download genome files from PATRIC given a list of PATRIC genome IDs and load them as strains.\n\n        Args:\n            ids (str, list): PATRIC ID or list of PATRIC IDs\n            force_rerun (bool): If genome files should be downloaded again even if they exist\n\n        \"\"\"\n        ids = ssbio.utils.force_list(ids)\n\n        counter = 0\n        log.info('Downloading sequences from PATRIC...')\n        for patric_id in tqdm(ids):\n            f = ssbio.databases.patric.download_coding_sequences(patric_id=patric_id, seqtype='protein',\n                                                                 outdir=self.sequences_by_organism_dir,\n                                                                 force_rerun=force_rerun)\n            if f:\n                self.load_strain(patric_id, f)\n                counter += 1\n                log.debug('{}: downloaded sequence'.format(patric_id))\n            else:\n                log.warning('{}: unable to download sequence'.format(patric_id))\n\n        log.info('Created {} new strain GEM-PROs, accessible at \"strains\" attribute'.format(counter))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_orthology_matrix(self, pid_cutoff=None, bitscore_cutoff=None, evalue_cutoff=None, filter_condition='OR',\n                             remove_strains_with_no_orthology=True,\n                             remove_strains_with_no_differences=False,\n                             remove_genes_not_in_base_model=True):\n        \"\"\"Create the orthology matrix by finding best bidirectional BLAST hits. Genes = rows, strains = columns\n\n        Runs run_makeblastdb, run_bidirectional_blast, and calculate_bbh for protein sequences.\n\n        Args:\n            pid_cutoff (float): Minimum percent identity between BLAST hits to filter for in the range [0, 100]\n            bitscore_cutoff (float): Minimum bitscore allowed between BLAST hits\n            evalue_cutoff (float): Maximum E-value allowed between BLAST hits\n            filter_condition (str): 'OR' or 'AND', how to combine cutoff filters. 'OR' gives more results since it\n            is less stringent, as you will be filtering for hits with (>80% PID or >30 bitscore or <0.0001 evalue).\n            remove_strains_with_no_orthology (bool): Remove strains which have no orthologous genes found\n            remove_strains_with_no_differences (bool): Remove strains which have all the same genes as the base model.\n                Default is False because since orthology is found using a PID cutoff, all genes may be present but\n                differences may be on the sequence level.\n            remove_genes_not_in_base_model (bool): Remove genes from the orthology matrix which are not present in our\n                base model. This happens if we use a genome file for our model that has other genes in it.\n\n        Returns:\n            DataFrame: Orthology matrix calculated from best bidirectional BLAST hits.\n\n        \"\"\"\n        # TODO: document and test other cutoffs\n\n        # Get the path to the reference genome\n        r_file = self.reference_gempro.genome_path\n\n        bbh_files = {}\n\n        log.info('Running bidirectional BLAST and finding best bidirectional hits (BBH)...')\n        for strain_gempro in tqdm(self.strains):\n            g_file = strain_gempro.genome_path\n\n            # Run bidirectional BLAST\n            log.debug('{} vs {}: Running bidirectional BLAST'.format(self.reference_gempro.id, strain_gempro.id))\n            r_vs_g, g_vs_r = ssbio.protein.sequence.utils.blast.run_bidirectional_blast(reference=r_file,\n                                                                                        other_genome=g_file,\n                                                                                        dbtype='prot',\n                                                                                        outdir=self.sequences_by_organism_dir)\n\n            # Using the BLAST files, find the BBH\n            log.debug('{} vs {}: Finding BBHs'.format(self.reference_gempro.id, strain_gempro.id))\n            bbh = ssbio.protein.sequence.utils.blast.calculate_bbh(blast_results_1=r_vs_g, blast_results_2=g_vs_r,\n                                                                   outdir=self.sequences_by_organism_dir)\n            bbh_files[strain_gempro.id] = bbh\n\n        # Make the orthologous genes matrix\n        log.info('Creating orthology matrix from BBHs...')\n        ortho_matrix = ssbio.protein.sequence.utils.blast.create_orthology_matrix(r_name=self.reference_gempro.id,\n                                                                                  genome_to_bbh_files=bbh_files,\n                                                                                  pid_cutoff=pid_cutoff,\n                                                                                  bitscore_cutoff=bitscore_cutoff,\n                                                                                  evalue_cutoff=evalue_cutoff,\n                                                                                  filter_condition=filter_condition,\n                                                                                  outname='{}_{}_orthology.csv'.format(self.reference_gempro.id, 'prot'),\n                                                                                  outdir=self.data_dir)\n\n        log.info('Saved orthology matrix at {}. See the \"df_orthology_matrix\" attribute.'.format(ortho_matrix))\n        self.df_orthology_matrix = pd.read_csv(ortho_matrix, index_col=0)\n\n        # Filter the matrix to genes only in our analysis, and also check for strains with no differences or no orthologous genes\n        self._filter_orthology_matrix(remove_strains_with_no_orthology=remove_strains_with_no_orthology,\n                                           remove_strains_with_no_differences=remove_strains_with_no_differences,\n                                           remove_genes_not_in_base_model=remove_genes_not_in_base_model)", "response": "Create the orthology matrix for the current species."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _filter_orthology_matrix(self,\n                                 remove_strains_with_no_orthology=True,\n                                 remove_strains_with_no_differences=False,\n                                 remove_genes_not_in_base_model=True):\n        \"\"\"Filters the orthology matrix by removing genes not in our base model, and also\n            removes strains from the analysis which have: 0 orthologous genes or no difference from the base strain.\n\n        Args:\n            remove_strains_with_no_orthology (bool): Remove strains which have no orthologous genes found\n            remove_strains_with_no_differences (bool): Remove strains which have all the same genes as the base model.\n                Default is False because since orthology is found using a PID cutoff, all genes may be present but\n                differences may be on the sequence level.\n            remove_genes_not_in_base_model (bool): Remove genes from the orthology matrix which are not present in our\n                base model. This happens if we use a genome file for our model that has other genes in it.\n\n        \"\"\"\n\n        if len(self.df_orthology_matrix) == 0:\n            raise RuntimeError('Empty orthology matrix')\n\n        initial_num_strains = len(self.strains)\n\n        # Adding names to the row and column of the orthology matrix\n        self.df_orthology_matrix = self.df_orthology_matrix.rename_axis('gene').rename_axis(\"strain\", axis=\"columns\")\n\n        # Gene filtering (of the orthology matrix)\n        if remove_genes_not_in_base_model:\n            # Check for gene IDs that are in the model and not in the orthology matrix\n            # This is probably because: the CDS FASTA file for the base strain did not contain the correct ID\n            # for the gene and consequently was not included in the orthology matrix\n            # Save these and report them\n            reference_strain_gene_ids = [x.id for x in self.reference_gempro.genes]\n            self.missing_in_orthology_matrix = [x for x in reference_strain_gene_ids if x not in self.df_orthology_matrix.index.tolist()]\n            self.missing_in_reference_strain = [y for y in self.df_orthology_matrix.index.tolist() if y not in reference_strain_gene_ids]\n\n            # Filter the matrix for genes within our base model only\n            self.df_orthology_matrix = self.df_orthology_matrix[self.df_orthology_matrix.index.isin(reference_strain_gene_ids)]\n            log.info('Filtered orthology matrix for genes present in base model')\n            log.warning('{} genes are in your base model but not your orthology matrix, see the attribute \"missing_in_orthology_matrix\"'.format(len(self.missing_in_orthology_matrix)))\n            log.warning('{} genes are in the orthology matrix but not your base model, see the attribute \"missing_in_reference_strain\"'.format(len(self.missing_in_reference_strain)))\n\n        # Strain filtering\n        for strain_gempro in self.strains.copy():\n            if remove_strains_with_no_orthology:\n                if strain_gempro.id not in self.df_orthology_matrix.columns:\n                    self.strains.remove(strain_gempro)\n                    log.info('{}: no orthologous genes found for this strain, removed from analysis.'.format(strain_gempro.id))\n                    continue\n                elif self.df_orthology_matrix[strain_gempro.id].isnull().all():\n                    self.strains.remove(strain_gempro)\n                    log.info('{}: no orthologous genes found for this strain, removed from analysis.'.format(strain_gempro.id))\n                    continue\n\n            if remove_strains_with_no_differences:\n                not_in_strain = self.df_orthology_matrix[pd.isnull(self.df_orthology_matrix[strain_gempro.id])][strain_gempro.id].index.tolist()\n                if len(not_in_strain) == 0:\n                    self.strains.remove(strain_gempro)\n                    log.info('{}: strain has no differences from the base, removed from analysis.')\n                    continue\n\n        log.info('{} strains to be analyzed, {} strains removed'.format(len(self.strains), initial_num_strains - len(self.strains)))", "response": "Filters the orthology matrix by removing strains which are not in the base model and remove strains which are not in the base model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmark genes as non - functional in a GEM - PRO model.", "response": "def _pare_down_model(self, strain_gempro, genes_to_remove):\n        \"\"\"Mark genes as non-functional in a GEM-PRO. If there is a COBRApy model associated with it, the\n            COBRApy method delete_model_genes is utilized to delete genes.\n\n        Args:\n            strain_gempro (GEMPRO): GEMPRO object\n            genes_to_remove (list): List of gene IDs to remove from the model\n\n        \"\"\"\n        # Filter out genes in genes_to_remove which do not show up in the model\n        strain_genes = [x.id for x in strain_gempro.genes]\n        genes_to_remove.extend(self.missing_in_orthology_matrix)\n        genes_to_remove = list(set(genes_to_remove).intersection(set(strain_genes)))\n\n        if len(genes_to_remove) == 0:\n            log.info('{}: no genes marked non-functional'.format(strain_gempro.id))\n            return\n        else:\n            log.debug('{}: {} genes to be marked non-functional'.format(strain_gempro.id, len(genes_to_remove)))\n\n        # If a COBRApy model exists, utilize the delete_model_genes method\n        if strain_gempro.model:\n            strain_gempro.model._trimmed = False\n            strain_gempro.model._trimmed_genes = []\n            strain_gempro.model._trimmed_reactions = {}\n\n            # Delete genes!\n            cobra.manipulation.delete_model_genes(strain_gempro.model, genes_to_remove)\n\n            if strain_gempro.model._trimmed:\n                log.info('{}: marked {} genes as non-functional, '\n                         'deactivating {} reactions'.format(strain_gempro.id, len(strain_gempro.model._trimmed_genes),\n                                                            len(strain_gempro.model._trimmed_reactions)))\n        # Otherwise, just mark the genes as non-functional\n        else:\n            for g in genes_to_remove:\n                strain_gempro.genes.get_by_id(g).functional = False\n            log.info('{}: marked {} genes as non-functional'.format(strain_gempro.id, len(genes_to_remove)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the strain sequences from the orthology matrix into the base model for comparisons and into the strain - specific model itself.", "response": "def _load_strain_sequences(self, strain_gempro):\n        \"\"\"Load strain sequences from the orthology matrix into the base model for comparisons, and into the\n        strain-specific model itself.\n\n        \"\"\"\n        if self._orthology_matrix_has_sequences:  # Load directly from the orthology matrix if it contains sequences\n            strain_sequences = self.df_orthology_matrix[strain_gempro.id].to_dict()\n        else:  # Otherwise load from the genome file if the orthology matrix contains gene IDs\n            # Load the genome FASTA file\n            log.debug('{}: loading strain genome CDS file'.format(strain_gempro.genome_path))\n            strain_sequences = SeqIO.index(strain_gempro.genome_path, 'fasta')\n\n        for strain_gene in strain_gempro.genes:\n            if strain_gene.functional:\n                if self._orthology_matrix_has_sequences:\n                    strain_gene_key = strain_gene.id\n                else:\n                    # Pull the gene ID of the strain from the orthology matrix\n                    strain_gene_key = self.df_orthology_matrix.loc[strain_gene.id, strain_gempro.id]\n                    log.debug('{}: original gene ID to be pulled from strain fasta file'.format(strain_gene_key))\n\n                # # Load into the base strain for comparisons\n                ref_gene = self.reference_gempro.genes.get_by_id(strain_gene.id)\n                new_id = '{}_{}'.format(strain_gene.id, strain_gempro.id)\n                if ref_gene.protein.sequences.has_id(new_id):\n                    log.debug('{}: sequence already loaded into reference model'.format(new_id))\n                    continue\n                ref_gene.protein.load_manual_sequence(seq=strain_sequences[strain_gene_key], ident=new_id,\n                                                      set_as_representative=False)\n                log.debug('{}: loaded sequence into reference model'.format(new_id))\n\n                # Load into the strain GEM-PRO\n                strain_gene.protein.load_manual_sequence(seq=strain_sequences[strain_gene_key], ident=new_id,\n                                                         set_as_representative=True)\n                log.debug('{}: loaded sequence into strain model'.format(new_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_strain_specific_models(self, save_models=False):\n\n        if len(self.df_orthology_matrix) == 0:\n            raise RuntimeError('Empty orthology matrix')\n\n        # Create an emptied copy of the reference GEM-PRO\n        for strain_gempro in tqdm(self.strains):\n            log.debug('{}: building strain specific model'.format(strain_gempro.id))\n\n            # For each genome, load the metabolic model or genes from the reference GEM-PRO\n            logging.disable(logging.WARNING)\n            if self._empty_reference_gempro.model:\n                strain_gempro.load_cobra_model(self._empty_reference_gempro.model)\n            elif self._empty_reference_gempro.genes:\n                strain_gempro.genes = [x.id for x in self._empty_reference_gempro.genes]\n            logging.disable(logging.NOTSET)\n\n            # Get a list of genes which do not have orthology in the strain\n            not_in_strain = self.df_orthology_matrix[pd.isnull(self.df_orthology_matrix[strain_gempro.id])][strain_gempro.id].index.tolist()\n\n            # Mark genes non-functional\n            self._pare_down_model(strain_gempro=strain_gempro, genes_to_remove=not_in_strain)\n\n            # Load sequences into the base and strain models\n            self._load_strain_sequences(strain_gempro=strain_gempro)\n\n            if save_models:\n                cobra.io.save_json_model(model=strain_gempro.model,\n                                         filename=op.join(self.model_dir, '{}.json'.format(strain_gempro.id)))\n                strain_gempro.save_pickle(op.join(self.model_dir, '{}_gp.pckl'.format(strain_gempro.id)))\n\n\n        log.info('Created {} new strain-specific models and loaded in sequences'.format(len(self.strains)))", "response": "Create and modify the strain specific models based on the orthologous genes matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align_orthologous_genes_pairwise(self, gapopen=10, gapextend=0.5):\n        for ref_gene in tqdm(self.reference_gempro.genes):\n            if len(ref_gene.protein.sequences) > 1:\n                alignment_dir = op.join(self.sequences_by_gene_dir, ref_gene.id)\n                if not op.exists(alignment_dir):\n                    os.mkdir(alignment_dir)\n                ref_gene.protein.pairwise_align_sequences_to_representative(gapopen=gapopen, gapextend=gapextend,\n                                                                            outdir=alignment_dir, parse=True)", "response": "Run a pairwise alignment for all orthologous genes to the base strain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_atlas_summary_df(self):\n        all_info = []\n        for g in self.reference_gempro.genes_with_a_representative_sequence:\n            info = {}\n            info['Gene_ID'] = g.id\n            info['Gene_name'] = g.name\n\n            # Protein object\n            p = g.protein\n            info['Protein_sequences'] = len(p.sequences)\n            info['Protein_structures'] = len(p.structures)\n\n            # SeqProp\n            rseq = p.representative_sequence\n            info['RepSeq_ID'] = rseq.id\n            info['RepSeq_sequence_length'] = rseq.seq_len\n            info['RepSeq_num_sequence_alignments'] = len([x for x in p.sequence_alignments if x.annotations['ssbio_type'] == 'seqalign'])\n            info['RepSeq_num_structure_alignments'] = len([x for x in p.sequence_alignments if x.annotations['ssbio_type'] == 'structalign'])\n\n            # SeqRecord annotations (properties calculated that summarize the whole sequence)\n            for annotation_name, annotation in rseq.annotations.items():\n                info['RepSeq_' + annotation_name] = annotation\n\n            # SeqRecord alignment annotations\n            all_num_mutations = []\n            all_num_deletions = []\n            all_len_deletions = []\n            all_num_insertions = []\n            all_len_insertions = []\n            all_percent_identity = []\n            all_percent_similarity = []\n            for aln in p.sequence_alignments:\n                # Gather the strain speicific stuff\n                if '{}_'.format(p.id) not in aln.annotations['b_seq']:\n                    continue\n                info[aln.annotations['b_seq'].split('{}_'.format(p.id))[1]] = aln.annotations['percent_identity']\n\n                # Gather the percent identities/similarities\n                all_percent_identity.append(aln.annotations['percent_identity'])\n                all_percent_similarity.append(aln.annotations['percent_similarity'])\n\n                # Gather the number of residues that are mutated (filter for different mutations of same residue)\n                num_mutations = len(list(set([x[1] for x in aln.annotations['mutations']])))\n                all_num_mutations.append(num_mutations)\n\n                # Gather the number of deletions as well as the length of the deletion\n                if not aln.annotations['deletions']:\n                    num_deletions = 0\n                    len_deletions = [0]\n                else:\n                    num_deletions = len(aln.annotations['deletions'])\n                    len_deletions = [x[1] for x in aln.annotations['deletions']]\n                all_num_deletions.append(num_deletions)\n                # Get the total length of the deletion for this one strain\n                avg_len_deletions = np.sum(len_deletions)\n                all_len_deletions.append(avg_len_deletions)\n\n                # Gather the number of insertions as well as the length of the insertion\n                if not aln.annotations['insertions']:\n                    num_insertions = 0\n                    len_insertions = [0]\n                else:\n                    num_insertions = len(aln.annotations['insertions'])\n                    len_insertions = [x[1] for x in aln.annotations['insertions']]\n                all_num_insertions.append(num_insertions)\n                # Get the total length of insertion for this one strain\n                avg_len_insertions = np.sum(len_insertions)\n                all_len_insertions.append(avg_len_insertions)\n\n            info['ATLAS_mean_num_mutations'] = np.mean(all_num_mutations)\n            info['ATLAS_mean_num_deletions'] = np.mean(all_num_deletions)\n            info['ATLAS_mean_len_deletions'] = np.mean(all_len_deletions)\n            info['ATLAS_mean_num_insertions'] = np.mean(all_num_insertions)\n            info['ATLAS_mean_len_insertions'] = np.mean(all_len_insertions)\n            info['ATLAS_mean_percent_identity'] = np.mean(all_percent_identity)\n            info['ATLAS_mean_percent_similarity'] = np.mean(all_percent_similarity)\n\n            # Other mutation analysis\n            single, fingerprint = p.sequence_mutation_summary()\n\n            # Mutations that show up in more than 10% of strains\n            singles = []\n            for k, v in single.items():\n                k = [str(x) for x in k]\n                if len(v) / len(p.sequence_alignments) >= 0.01:\n                    singles.append(''.join(k))  # len(v) is the number of strains which have this mutation\n            info['ATLAS_popular_mutations'] = ';'.join(singles)\n\n            # Mutation groups that show up in more than 10% of strains\n            allfingerprints = []\n            for k, v in fingerprint.items():\n                if len(v) / len(p.sequence_alignments) >= 0.01:\n                    fingerprints = []\n                    for m in k:\n                        y = [str(x) for x in m]\n                        fingerprints.append(''.join(y))\n                    allfingerprints.append('-'.join(fingerprints))\n            info['ATLAS_popular_mutation_groups'] = ';'.join(allfingerprints)\n\n            # StructProp\n            rstruct = p.representative_structure\n            if rstruct:\n                if rstruct.structure_file:\n                    info['RepStruct_ID'] = rstruct.id\n                    info['RepStruct_is_experimental'] = rstruct.is_experimental\n                    info['RepStruct_description'] = rstruct.description\n                    info['RepStruct_repseq_coverage'] = p.representative_chain_seq_coverage\n\n                    # ChainProp\n                    rchain = p.representative_chain\n                    info['RepChain_ID'] = rchain\n\n                    # ChainProp SeqRecord annotations\n                    rchain_sr = rstruct.chains.get_by_id(rchain).seq_record\n                    for annotation_name, annotation in rchain_sr.annotations.items():\n                        info['RepChain_' + annotation_name] = annotation\n\n            all_info.append(info)\n\n        cols = ['Gene_ID', 'Gene_name', 'Protein_sequences', 'Protein_structures',\n                'RepSeq_ID', 'RepSeq_sequence_length',\n                'RepSeq_num_sequence_alignments', 'RepSeq_num_structure_alignments',\n                'RepStruct_ID', 'RepChain_ID', 'RepStruct_description',\n                'RepStruct_is_experimental', 'RepStruct_repseq_coverage',\n                'ATLAS_mean_percent_identity', 'ATLAS_mean_percent_similarity', 'ATLAS_mean_num_mutations',\n                'ATLAS_popular_mutations', 'ATLAS_popular_mutation_groups', 'ATLAS_mean_num_deletions',\n                'ATLAS_mean_num_insertions', 'ATLAS_mean_len_deletions', 'ATLAS_mean_len_insertions',\n                'RepSeq_aromaticity', 'RepSeq_instability_index', 'RepSeq_isoelectric_point', 'RepSeq_molecular_weight',\n                'RepSeq_monoisotopic', 'RepSeq_num_tm_helix-tmhmm', 'RepSeq_percent_acidic', 'RepSeq_percent_aliphatic',\n                'RepSeq_percent_aromatic', 'RepSeq_percent_B-sspro8', 'RepSeq_percent_basic',\n                'RepSeq_percent_buried-accpro', 'RepSeq_percent_buried-accpro20', 'RepSeq_percent_C-sspro',\n                'RepSeq_percent_C-sspro8', 'RepSeq_percent_charged', 'RepSeq_percent_E-sspro',\n                'RepSeq_percent_E-sspro8', 'RepSeq_percent_exposed-accpro', 'RepSeq_percent_exposed-accpro20',\n                'RepSeq_percent_G-sspro8', 'RepSeq_percent_H-sspro', 'RepSeq_percent_H-sspro8',\n                'RepSeq_percent_helix_naive', 'RepSeq_percent_I-sspro8', 'RepSeq_percent_non-polar',\n                'RepSeq_percent_polar', 'RepSeq_percent_S-sspro8', 'RepSeq_percent_small',\n                'RepSeq_percent_strand_naive', 'RepSeq_percent_T-sspro8', 'RepSeq_percent_tiny',\n                'RepSeq_percent_turn_naive', 'RepChain_percent_B-dssp', 'RepChain_percent_C-dssp',\n                'RepChain_percent_E-dssp', 'RepChain_percent_G-dssp', 'RepChain_percent_H-dssp',\n                'RepChain_percent_I-dssp', 'RepChain_percent_S-dssp', 'RepChain_percent_T-dssp',\n                'RepChain_SSBOND-biopython']\n        cols.extend([x.id for x in self.strains])\n\n        df_atlas_summary = pd.DataFrame(all_info, columns=cols)\n        # Drop columns that don't have anything in them\n        df_atlas_summary.dropna(axis=1, how='all', inplace=True)\n\n        return df_atlas_summary", "response": "Create a single data frame which summarizes all genes per row."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_atlas_per_gene_mutation_df(self, gene_id):\n        # TODO: also count: number of unique mutations (have to consider position, amino acid change)\n        # TODO: keep track of strain with most mutations, least mutations\n        # TODO: keep track of strains that conserve the length of the protein, others that extend or truncate it\n        # need statistical test for that too (how long is \"extended\"/\"truncated\"?)\n        # TODO: number of strains with at least 1 mutations\n        # TODO: number of strains with <5% mutated, 5-10%, etc\n\n        g = self.reference_gempro.genes.get_by_id(gene_id)\n\n        single, fingerprint = g.protein.sequence_mutation_summary(alignment_type='seqalign')\n\n        structure_type_suffix = 'NA'\n        appender = []\n\n        for k, strains in single.items():\n            # Mutations in the strain\n            to_append = {}\n            orig_res = k[0]\n            resnum = int(k[1])\n            mutated_res = k[2]\n            num_strains_mutated = len(strains)\n            strain_ids = [str(x.split(g.id + '_')[1]) for x in strains]\n            to_append['ref_residue'] = orig_res\n            to_append['ref_resnum'] = resnum\n            to_append['strain_residue'] = mutated_res\n            to_append['num_strains_mutated'] = num_strains_mutated\n            to_append['strains_mutated'] = ';'.join(strain_ids)\n            to_append['at_disulfide_bridge'] = False\n\n            # Residue properties\n            origres_props = ssbio.protein.sequence.properties.residues.residue_biochemical_definition(orig_res)\n            mutres_props = ssbio.protein.sequence.properties.residues.residue_biochemical_definition(mutated_res)\n            to_append['ref_residue_prop'] = origres_props\n            to_append['strain_residue_prop'] = mutres_props\n\n            # Grantham score - score a mutation based on biochemical properties\n            grantham_s, grantham_txt = ssbio.protein.sequence.properties.residues.grantham_score(orig_res, mutated_res)\n            to_append['grantham_score'] = grantham_s\n            to_append['grantham_annotation'] = grantham_txt\n\n            # Get all per residue annotations - predicted from sequence and calculated from structure\n            to_append.update(g.protein.get_residue_annotations(seq_resnum=resnum, use_representatives=True))\n\n            # Check structure type\n            if g.protein.representative_structure:\n                if g.protein.representative_structure.is_experimental:\n                    to_append['structure_type'] = 'EXP'\n                else:\n                    to_append['structure_type'] = 'HOM'\n\n                # At disulfide bond?\n                repchain = g.protein.representative_chain\n                repchain_annotations = g.protein.representative_structure.chains.get_by_id(repchain).seq_record.annotations\n                if 'SSBOND-biopython' in repchain_annotations:\n                    structure_resnum = g.protein.map_seqprop_resnums_to_structprop_resnums(resnums=resnum,\n                                                                                           use_representatives=True)\n                    if resnum in structure_resnum:\n                        ssbonds = repchain_annotations['SSBOND-biopython']\n                        ssbonds_res = []\n                        for x in ssbonds:\n                            ssbonds_res.append(x[0])\n                            ssbonds_res.append(x[1])\n\n                        if structure_resnum in ssbonds_res:\n                            to_append['at_disulfide_bridge'] = True\n\n            appender.append(to_append)\n\n        if not appender:\n            return pd.DataFrame()\n\n        cols = ['ref_residue', 'ref_resnum', 'strain_residue', 'num_strains_mutated', 'strains_mutated',\n                'ref_residue_prop', 'strain_residue_prop', 'grantham_score', 'grantham_annotation',\n                'at_disulfide_bridge',\n                'seq_SS-sspro', 'seq_SS-sspro8', 'seq_RSA-accpro', 'seq_RSA-accpro20', 'seq_TM-tmhmm',\n                'struct_SS-dssp', 'struct_RSA-dssp', 'struct_ASA-dssp',\n                'struct_CA_DEPTH-msms', 'struct_RES_DEPTH-msms',\n                'struct_PHI-dssp', 'struct_PSI-dssp',\n                'struct_resnum', 'struct_residue'\n                'strains_mutated']\n\n        df_gene_summary = pd.DataFrame.from_records(appender, columns=cols)\n\n        # Drop columns that don't have anything in them\n        df_gene_summary.dropna(axis=1, how='all', inplace=True)\n\n        df_gene_summary.sort_values(by='ref_resnum', inplace=True)\n        df_gene_summary = df_gene_summary.set_index('ref_resnum')\n        return df_gene_summary", "response": "Create a dataframe which summarizes a gene and its mutations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a residue number or number to an NGLWidget view object.", "response": "def add_residues_highlight_to_nglview(view, structure_resnums, chain, res_color='red'):\n    \"\"\"Add a residue number or numbers to an NGLWidget view object.\n\n    Args:\n        view (NGLWidget): NGLWidget view object\n        structure_resnums (int, list): Residue number(s) to highlight, structure numbering\n        chain (str, list): Chain ID or IDs of which residues are a part of. If not provided, all chains in the\n            mapped_chains attribute will be used. If that is also empty, and exception is raised.\n        res_color (str): Color to highlight residues with\n\n    \"\"\"\n    chain = ssbio.utils.force_list(chain)\n\n    if isinstance(structure_resnums, list):\n        structure_resnums = list(set(structure_resnums))\n    elif isinstance(structure_resnums, int):\n        structure_resnums = ssbio.utils.force_list(structure_resnums)\n    else:\n        raise ValueError('Input must either be a residue number of a list of residue numbers')\n\n    to_show_chains = '( '\n    for c in chain:\n        to_show_chains += ':{} or'.format(c)\n    to_show_chains = to_show_chains.strip(' or ')\n    to_show_chains += ' )'\n\n    to_show_res = '( '\n    for m in structure_resnums:\n        to_show_res += '{} or '.format(m)\n    to_show_res = to_show_res.strip(' or ')\n    to_show_res += ' )'\n\n    log.info('Selection: {} and not hydrogen and {}'.format(to_show_chains, to_show_res))\n\n    view.add_ball_and_stick(selection='{} and not hydrogen and {}'.format(to_show_chains, to_show_res), color=res_color)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_features_to_nglview(view, structure_resnums, chain_id):\n    # Parse and store chain seq if not already stored\n    if not structprop.chains.has_id(chain_id):\n        structprop.parse_structure()\n        if not structprop.chains.has_id(chain_id):\n            raise ValueError('Chain {} not present in structure {}'.format(chain_id, structprop.id))\n\n    if not seqprop.features:\n        log.warning('{}: no stored features'.format(seqprop.id))\n\n    # Loop through any stored features\n    for f in seqprop.features:\n\n        # Display disulfide bonds\n        if f.type.lower() == 'disulfide bond':\n            # TODO: double check if .start or .start + 1\n            disulfide = map_seqprop_resnums_to_structprop_resnums(resnums=[f.location.start + 1, f.location.end],\n                                                                  seqprop=seqprop,\n                                                                  structprop=structprop,\n                                                                  chain_id=chain_id,\n                                                                  use_representatives=False)\n            to_view = [str(x)+'.CA' for x in list(disulfide.values())]\n            view.add_distance(atom_pair=[to_view], color='black')\n            log.info('Disulfide bridge at residues {} & {}'.format(f.location.start + 1, f.location.end))\n\n        # Display DNA-binding regions\n        if f.type.lower() == 'dna-binding region' or f.type.lower() == 'nucleotide phosphate-binding region':\n            impres = self.map_seqprop_resnums_to_structprop_resnums(resnums=[f.location.start + 1,\n                                                                             f.location.end],\n                                                                    seqprop=seqprop,\n                                                                    structprop=structprop,\n                                                                    chain_id=chain_id,\n                                                                    use_representatives=use_representatives)\n\n            # TODO: need to check if f.location.start was mapped and if not, try incrementing. or  input the list\n            # of resnums, not just the start and end\n            if f.location.start + 1 in impres and f.location.end in impres:\n                mapped_start = impres[f.location.start + 1]\n                mapped_end = impres[f.location.end]\n                view.add_ball_and_stick(selection=':{} and ( {}-{} )'.format(chain_id,\n                                                                             mapped_start,\n                                                                             mapped_end), color='black')\n                log.info('{} at sequence region {}-{}, structure residues {}-{}'.format(f.type,\n                                                                                        f.location.start,\n                                                                                        f.location.end,\n                                                                                        mapped_start,\n                                                                                        mapped_end))\n\n        # Display other single residues\n        if f.location.end - 1 == f.location.start:\n            if f.type.lower() == 'sequence variant' or f.type.lower() == 'mutagenesis site':\n                continue\n            impres = self.map_seqprop_resnums_to_structprop_resnums(resnums=f.location.end,\n                                                                    seqprop=seqprop,\n                                                                    structprop=structprop,\n                                                                    chain_id=chain_id,\n                                                                    use_representatives=use_representatives)\n            if f.location.end in impres:\n                impres_mapped = impres[f.location.end]\n                view.add_ball_and_stick(selection=str(impres_mapped), color='black')\n                view.add_label(selection=':{} and {}'.format(chain_id, impres_mapped), label_type='res', color='black')\n                log.info('{} at sequence residue {}, structure residue {}'.format(f.type, f.location.end, impres_mapped))", "response": "Add select features from the selected SeqProp object to an NGLWidget view object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading the KEGG flatfile for a KEGG gene ID and return the path.", "response": "def download_kegg_gene_metadata(gene_id, outdir=None, force_rerun=False):\n    \"\"\"Download the KEGG flatfile for a KEGG ID and return the path.\n\n    Args:\n        gene_id: KEGG gene ID (with organism code), i.e. \"eco:1244\"\n        outdir: optional output directory of metadata\n\n    Returns:\n        Path to metadata file\n\n    \"\"\"\n    if not outdir:\n        outdir = ''\n\n    # Replace colon with dash in the KEGG gene ID\n    outfile = op.join(outdir, '{}.kegg'.format(custom_slugify(gene_id)))\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        raw_text = bs_kegg.get(\"{}\".format(gene_id))\n        if raw_text == 404:\n            return\n\n        with io.open(outfile, mode='wt', encoding='utf-8') as f:\n            f.write(raw_text)\n\n        log.debug('{}: downloaded KEGG metadata file'.format(outfile))\n    else:\n        log.debug('{}: KEGG metadata file already exists'.format(outfile))\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_kegg_gene_metadata(infile):\n    metadata = defaultdict(str)\n\n    with open(infile) as mf:\n        kegg_parsed = bs_kegg.parse(mf.read())\n\n    # TODO: additional fields can be parsed\n\n    if 'DBLINKS' in kegg_parsed.keys():\n        if 'UniProt' in kegg_parsed['DBLINKS']:\n            unis = str(kegg_parsed['DBLINKS']['UniProt']).split(' ')\n            # TODO: losing other uniprot ids by doing this\n            if isinstance(unis, list):\n                metadata['uniprot'] = unis[0]\n            else:\n                metadata['uniprot'] = unis\n        if 'NCBI-ProteinID' in kegg_parsed['DBLINKS']:\n            metadata['refseq'] = str(kegg_parsed['DBLINKS']['NCBI-ProteinID'])\n    if 'STRUCTURE' in kegg_parsed.keys():\n        metadata['pdbs'] = str(kegg_parsed['STRUCTURE']['PDB']).split(' ')\n    else:\n        metadata['pdbs'] = None\n    if 'ORGANISM' in kegg_parsed.keys():\n        metadata['taxonomy'] = str(kegg_parsed['ORGANISM'])\n\n    return metadata", "response": "Parse the KEGG flatfile and return a dictionary of metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap all of an organism s gene IDs to the target database.", "response": "def map_kegg_all_genes(organism_code, target_db):\n    \"\"\"Map all of an organism's gene IDs to the target database.\n\n    This is faster than supplying a specific list of genes to map,\n    plus there seems to be a limit on the number you can map with a manual REST query anyway.\n\n    Args:\n        organism_code: the three letter KEGG code of your organism\n        target_db: ncbi-proteinid | ncbi-geneid | uniprot\n\n    Returns:\n        Dictionary of ID mapping\n\n    \"\"\"\n    mapping = bs_kegg.conv(target_db, organism_code)\n\n    # strip the organism code from the keys and the identifier in the values\n    new_mapping = {}\n    for k,v in mapping.items():\n        new_mapping[k.replace(organism_code + ':', '')] = str(v.split(':')[1])\n\n    return new_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprovide pointers to the paths of the metadata file that is located in the path.", "response": "def metadata_path(self, m_path):\n        \"\"\"Provide pointers to the paths of the metadata file\n\n        Args:\n            m_path: Path to metadata file\n\n        \"\"\"\n        if not m_path:\n            self.metadata_dir = None\n            self.metadata_file = None\n\n        else:\n            if not op.exists(m_path):\n                raise OSError('{}: file does not exist!'.format(m_path))\n\n            if not op.dirname(m_path):\n                self.metadata_dir = '.'\n            else:\n                self.metadata_dir = op.dirname(m_path)\n            self.metadata_file = op.basename(m_path)\n\n            # TODO: update using Biopython's built in SeqRecord parser\n            # Just updating IDs and stuff\n            self.update(parse_kegg_gene_metadata(self.metadata_path), overwrite=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake in a sequence string and prepares the folder for the I - TASSER run.", "response": "def prep_folder(self, seq):\n        \"\"\"Take in a sequence string and prepares the folder for the I-TASSER run.\"\"\"\n        itasser_dir = op.join(self.root_dir, self.id)\n\n        if not op.exists(itasser_dir):\n            os.makedirs(itasser_dir)\n\n        tmp = {self.id: seq}\n\n        fasta.write_fasta_file_from_dict(indict=tmp,\n                                         outname='seq',\n                                         outext='.fasta',\n                                         outdir=itasser_dir)\n        return itasser_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a BLAST database for a genome file.", "response": "def run_makeblastdb(infile, dbtype, outdir=''):\n    \"\"\"Make the BLAST database for a genome file.\n\n    Args:\n        infile (str): path to genome FASTA file\n        dbtype (str): \"nucl\" or \"prot\" - what format your genome files are in\n        outdir (str): path to directory to output database files (default is original folder)\n\n    Returns:\n        Paths to BLAST databases.\n\n    \"\"\"\n    # TODO: add force_rerun option\n    # TODO: rewrite using utils function command\n\n    # Output location\n    og_dir, name, ext = utils.split_folder_and_path(infile)\n    if not outdir:\n        outdir = og_dir\n    outfile_basename = op.join(outdir, name)\n\n    # Check if BLAST DB was already made\n    if dbtype == 'nucl':\n        outext = ['.nhr', '.nin', '.nsq']\n    elif dbtype == 'prot':\n        outext = ['.phr', '.pin', '.psq']\n    else:\n        raise ValueError('dbtype must be \"nucl\" or \"prot\"')\n    outfile_all = [outfile_basename + x for x in outext]\n    db_made = True\n    for f in outfile_all:\n        if not op.exists(f):\n            db_made = False\n\n    # Run makeblastdb if DB does not exist\n    if db_made:\n        log.debug('BLAST database already exists at {}'.format(outfile_basename))\n        return outfile_all\n    else:\n        retval = subprocess.call('makeblastdb -in {} -dbtype {} -out {}'.format(infile, dbtype, outfile_basename), shell=True)\n        if retval == 0:\n            log.debug('Made BLAST database at {}'.format(outfile_basename))\n            return outfile_all\n        else:\n            log.error('Error running makeblastdb, exit code {}'.format(retval))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_bidirectional_blast(reference, other_genome, dbtype, outdir=''):\n    # TODO: add force_rerun option\n\n    if dbtype == 'nucl':\n        command = 'blastn'\n    elif dbtype == 'prot':\n        command = 'blastp'\n    else:\n        raise ValueError('dbtype must be \"nucl\" or \"prot\"')\n\n    r_folder, r_name, r_ext = utils.split_folder_and_path(reference)\n    g_folder, g_name, g_ext = utils.split_folder_and_path(other_genome)\n\n    # make sure BLAST DBs have been made\n    run_makeblastdb(infile=reference, dbtype=dbtype, outdir=r_folder)\n    run_makeblastdb(infile=other_genome, dbtype=dbtype, outdir=g_folder)\n\n    # Reference vs genome\n    r_vs_g = r_name + '_vs_' + g_name + '_blast.out'\n    r_vs_g = op.join(outdir, r_vs_g)\n    if op.exists(r_vs_g) and os.stat(r_vs_g).st_size != 0:\n        log.debug('{} vs {} BLAST already run'.format(r_name, g_name))\n    else:\n        cmd = '{} -query {} -db {} -outfmt 6 -out {}'.format(command, reference, op.join(g_folder, g_name), r_vs_g)\n        log.debug('Running: {}'.format(cmd))\n        retval = subprocess.call(cmd, shell=True)\n        if retval == 0:\n            log.debug('BLASTed {} vs {}'.format(g_name, r_name))\n        else:\n            log.error('Error running {}, exit code {}'.format(command, retval))\n\n    # Genome vs reference\n    g_vs_r = g_name + '_vs_' + r_name + '_blast.out'\n    g_vs_r = op.join(outdir, g_vs_r)\n    if op.exists(g_vs_r) and os.stat(g_vs_r).st_size != 0:\n        log.debug('{} vs {} BLAST already run'.format(g_name, r_name))\n    else:\n        cmd = '{} -query {} -db {} -outfmt 6 -out {}'.format(command, other_genome, op.join(r_folder, r_name), g_vs_r)\n        log.debug('Running: {}'.format(cmd))\n        retval = subprocess.call(cmd, shell=True)\n        if retval == 0:\n            log.debug('BLASTed {} vs {}'.format(g_name, r_name))\n        else:\n            log.error('Error running {}, exit code {}'.format(command, retval))\n\n    return r_vs_g, g_vs_r", "response": "BLAST a genome against another genome and vice versa."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_run_bidirectional_blast(reference, other_genome, dbtype, outdir):\n    # TODO: add force_rerun option\n\n    if dbtype == 'nucl':\n        command = 'blastn'\n    elif dbtype == 'prot':\n        command = 'blastp'\n    else:\n        raise ValueError('dbtype must be \"nucl\" or \"prot\"')\n\n    r_folder, r_name, r_ext = utils.split_folder_and_path(reference)\n    g_folder, g_name, g_ext = utils.split_folder_and_path(other_genome)\n\n    # Reference vs genome\n    r_vs_g_name = r_name + '_vs_' + g_name\n    r_vs_g = r_vs_g_name + '_blast.out'\n    if op.exists(op.join(outdir, r_vs_g)) and os.stat(op.join(outdir, r_vs_g)).st_size != 0:\n        log.debug('{} vs {} BLAST already run'.format(r_name, g_name))\n    else:\n        cmd = '{} -query {} -db {} -outfmt 6 -out {}'.format(command, reference, g_name, r_vs_g)\n        utils.write_torque_script(command=cmd, err=r_vs_g_name, out=r_vs_g_name, name=r_vs_g_name,\n                                  outfile=op.join(outdir, r_vs_g_name) + '.sh',\n                                  walltime='00:15:00', queue='regular')\n\n    # Genome vs reference\n    g_vs_r_name = g_name + '_vs_' + r_name\n    g_vs_r = g_vs_r_name + '_blast.out'\n    if op.exists(op.join(outdir, g_vs_r)) and os.stat(op.join(outdir, g_vs_r)).st_size != 0:\n        log.debug('{} vs {} BLAST already run'.format(g_name, r_name))\n    else:\n        cmd = '{} -query {} -db {} -outfmt 6 -out {}'.format(command, other_genome, r_name, g_vs_r)\n        utils.write_torque_script(command=cmd, err=g_vs_r_name, out=g_vs_r_name, name=g_vs_r_name,\n                                  outfile=op.join(outdir, g_vs_r_name) + '.sh',\n                                  walltime='00:15:00', queue='regular')", "response": "Write torque submission files for running bidirectional blast on a server and print execution command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_bbh(blast_results_1, blast_results_2, r_name=None, g_name=None, outdir=''):\n    # TODO: add force_rerun option\n\n    cols = ['gene', 'subject', 'PID', 'alnLength', 'mismatchCount', 'gapOpenCount', 'queryStart', 'queryEnd',\n            'subjectStart', 'subjectEnd', 'eVal', 'bitScore']\n\n    if not r_name and not g_name:\n        r_name = op.basename(blast_results_1).split('_vs_')[0]\n        g_name = op.basename(blast_results_1).split('_vs_')[1].replace('_blast.out', '')\n\n        r_name2 = op.basename(blast_results_2).split('_vs_')[1].replace('_blast.out', '')\n        if r_name != r_name2:\n            log.warning('{} != {}'.format(r_name, r_name2))\n\n    outfile = op.join(outdir, '{}_vs_{}_bbh.csv'.format(r_name, g_name))\n    if op.exists(outfile) and os.stat(outfile).st_size != 0:\n        log.debug('{} vs {} BLAST BBHs already found at {}'.format(r_name, g_name, outfile))\n        return outfile\n\n    bbh1 = pd.read_csv(blast_results_1, sep='\\t', names=cols)\n    bbh2 = pd.read_csv(blast_results_2, sep='\\t', names=cols)\n\n    out = pd.DataFrame()\n    log.debug('Finding BBHs for {} vs. {}'.format(r_name, g_name))\n\n    for g in bbh1[pd.notnull(bbh1.gene)].gene.unique():\n        res = bbh1[bbh1.gene == g]\n        if len(res) == 0:\n            continue\n        best_hit = res.ix[res.PID.idxmax()].copy()\n        best_gene = best_hit.subject\n        res2 = bbh2[bbh2.gene == best_gene]\n        if len(res2) == 0:\n            continue\n        best_hit2 = res2.ix[res2.PID.idxmax()]\n        best_gene2 = best_hit2.subject\n        if g == best_gene2:\n            best_hit['BBH'] = '<=>'\n        else:\n            best_hit['BBH'] = '->'\n        out = pd.concat([out, pd.DataFrame(best_hit).transpose()])\n\n    out.to_csv(outfile)\n    log.debug('{} vs {} BLAST BBHs saved at {}'.format(r_name, g_name, outfile))\n    return outfile", "response": "Calculate the best bidirectional BLAST hits and save a dataframe of results."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_orthology_matrix(r_name, genome_to_bbh_files, pid_cutoff=None, bitscore_cutoff=None, evalue_cutoff=None,\n                            outname='', outdir='', force_rerun=False):\n    \"\"\"Create an orthology matrix using best bidirectional BLAST hits (BBH) outputs.\n\n    Args:\n        r_name (str): Name of the reference genome\n        genome_to_bbh_files (dict): Mapping of genome names to the BBH csv output from the\n            :func:`~ssbio.protein.sequence.utils.blast.calculate_bbh` method\n        pid_cutoff (float): Minimum percent identity between BLAST hits to filter for in the range [0, 100]\n        bitscore_cutoff (float): Minimum bitscore allowed between BLAST hits\n        evalue_cutoff (float): Maximum E-value allowed between BLAST hits\n        outname: Name of output file of orthology matrix\n        outdir: Path to output directory\n        force_rerun (bool): Force recreation of the orthology matrix even if the outfile exists\n\n    Returns:\n        str: Path to orthologous genes matrix.\n\n    \"\"\"\n    if outname:\n        outfile = op.join(outdir, outname)\n    else:\n        outfile = op.join(outdir, '{}_orthology.csv'.format(r_name))\n\n    if op.exists(outfile) and os.stat(outfile).st_size != 0 and not force_rerun:\n        log.info('{}: loaded existing orthology matrix'.format(outfile))\n        return outfile\n\n    if not pid_cutoff and not bitscore_cutoff and not evalue_cutoff:\n        log.warning('No cutoffs supplied, insignificant hits may be reported')\n\n    if not pid_cutoff:\n        pid_cutoff = 0\n    if not bitscore_cutoff:\n        bitscore_cutoff = 0\n    if not evalue_cutoff:\n        evalue_cutoff = float('Inf')\n\n    out = pd.DataFrame()\n    for g_name, bbh_path in genome_to_bbh_files.items():\n\n        df_bbh = pd.read_csv(bbh_path, index_col=0)\n        bidirectional = df_bbh[df_bbh.BBH == '<=>']\n\n        data = bidirectional[(bidirectional.PID > pid_cutoff) & (bidirectional.eVal < evalue_cutoff) & (bidirectional.bitScore > bitscore_cutoff)]\n\n        data.index = data.gene\n        data2 = data[['subject']]\n        if out.empty:\n            out = data2\n            out = out.rename(columns={'subject': g_name})\n        else:\n            out = pd.merge(out, data2, left_index=True, right_index=True, how='outer')\n            out = out.rename(columns={'subject': g_name})\n\n    out.to_csv(outfile)\n    log.debug('{} orthologous genes saved at {}'.format(r_name, outfile))\n    return outfile", "response": "Create an orthologous genes matrix from a set of genome names to BBH files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_protein(structure, filter_residues=True):\n\n    from ssbio.biopython.Bio.Struct.Protein import Protein\n    return Protein.from_structure(structure, filter_residues)", "response": "Returns a new structure object with the specified structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a new PDB file for the Structure s FIRST MODEL.", "response": "def write_pdb(self, custom_name='', out_suffix='', out_dir=None, custom_selection=None, force_rerun=False):\n        \"\"\"Write a new PDB file for the Structure's FIRST MODEL.\n\n        Set custom_selection to a PDB.Select class for custom SMCRA selections.\n\n        Args:\n            custom_name: Filename of the new file (without extension)\n            out_suffix: Optional string to append to new PDB file\n            out_dir: Optional directory to output the file\n            custom_selection: Optional custom selection class\n            force_rerun: If existing file should be overwritten\n\n        Returns:\n            out_file: filepath of new PDB file\n\n        \"\"\"\n        if not custom_selection:\n            custom_selection = ModelSelection([0])\n\n        # If no output directory, custom name, or suffix is specified, add a suffix \"_new\"\n        if not out_dir or not custom_name:\n            if not out_suffix:\n                out_suffix = '_new'\n\n        # Prepare the output file path\n        outfile = ssbio.utils.outfile_maker(inname=self.structure_file,\n                                            outname=custom_name,\n                                            append_to_name=out_suffix,\n                                            outdir=out_dir,\n                                            outext='.pdb')\n        try:\n            if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n                self.save(outfile, custom_selection)\n        except TypeError as e:\n            # If trying to save something that can't be saved as a PDB (example: 5iqr.cif), log an error and return None\n            # The error thrown by PDBIO.py is \"TypeError: %c requires int or char\"\n            log.error('{}: unable to save structure in PDB file format'.format(self.structure_file))\n            raise TypeError(e)\n\n        return outfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nblast the sequence in seq to available structures in the PDB.", "response": "def blast_pdb(seq, outfile='', outdir='', evalue=0.0001, seq_ident_cutoff=0.0, link=False, force_rerun=False):\n    \"\"\"Returns a list of BLAST hits of a sequence to available structures in the PDB.\n\n    Args:\n        seq (str): Your sequence, in string format\n        outfile (str): Name of output file\n        outdir (str, optional): Path to output directory. Default is the current directory.\n        evalue (float, optional): Cutoff for the E-value - filters for significant hits. 0.001 is liberal, 0.0001 is stringent (default).\n        seq_ident_cutoff (float, optional): Cutoff results based on percent coverage (in decimal form)\n        link (bool, optional): Set to True if a link to the HTML results should be displayed\n        force_rerun (bool, optional): If existing BLAST results should not be used, set to True. Default is False\n\n    Returns:\n        list: Rank ordered list of BLAST hits in dictionaries.\n\n    \"\"\"\n\n    if len(seq) < 12:\n        raise ValueError('Sequence must be at least 12 residues long.')\n    if link:\n        page = 'PDB results page: http://www.rcsb.org/pdb/rest/getBlastPDB1?sequence={}&eCutOff={}&maskLowComplexity=yes&matrix=BLOSUM62&outputFormat=HTML'.format(seq, evalue)\n        print(page)\n\n    parser = etree.XMLParser(ns_clean=True)\n\n    outfile = op.join(outdir, outfile)\n    if ssbio.utils.force_rerun(force_rerun, outfile):\n        # Load the BLAST XML results if force_rerun=True\n        page = 'http://www.rcsb.org/pdb/rest/getBlastPDB1?sequence={}&eCutOff={}&maskLowComplexity=yes&matrix=BLOSUM62&outputFormat=XML'.format(\n                        seq, evalue)\n        req = requests.get(page)\n        if req.status_code == 200:\n            response = req.text\n\n            # Save the XML file\n            if outfile:\n                with open(outfile, 'w') as f:\n                    f.write(response)\n\n            # Parse the XML string\n            tree = etree.ElementTree(etree.fromstring(response, parser))\n            log.debug('Loaded BLAST results from REST server')\n        else:\n            log.error('BLAST request timed out')\n            return []\n    else:\n        tree = etree.parse(outfile, parser)\n        log.debug('{}: Loaded existing BLAST XML results'.format(outfile))\n\n    # Get length of original sequence to calculate percentages\n    len_orig = float(len(seq))\n\n    root = tree.getroot()\n    hit_list = []\n\n    for hit in root.findall('BlastOutput_iterations/Iteration/Iteration_hits/Hit'):\n        info = {}\n\n        hitdef = hit.find('Hit_def')\n        if hitdef is not None:\n            info['hit_pdb'] = hitdef.text.split('|')[0].split(':')[0].lower()\n            info['hit_pdb_chains'] = hitdef.text.split('|')[0].split(':')[2].split(',')\n\n        # One PDB can align to different parts of the sequence\n        # Will just choose the top hit for this single PDB\n        hsp = hit.findall('Hit_hsps/Hsp')[0]\n\n        # Number of identical residues\n        hspi = hsp.find('Hsp_identity')\n        if hspi is not None:\n            info['hit_num_ident'] = int(hspi.text)\n            info['hit_percent_ident'] = int(hspi.text)/len_orig\n\n            if int(hspi.text)/len_orig < seq_ident_cutoff:\n                log.debug('{}: does not meet sequence identity cutoff'.format(hitdef.text.split('|')[0].split(':')[0]))\n                continue\n\n        # Number of similar residues (positive hits)\n        hspp = hsp.find('Hsp_positive')\n        if hspp is not None:\n            info['hit_num_similar'] = int(hspp.text)\n            info['hit_percent_similar'] = int(hspp.text) / len_orig\n\n        # Total number of gaps (unable to align in either query or subject)\n        hspg = hsp.find('Hsp_gaps')\n        if hspg is not None:\n            info['hit_num_gaps'] = int(hspg.text)\n            info['hit_percent_gaps'] = int(hspg.text) / len_orig\n\n        # E-value of BLAST\n        hspe = hsp.find('Hsp_evalue')\n        if hspe is not None:\n            info['hit_evalue'] = float(hspe.text)\n\n        # Score of BLAST\n        hsps = hsp.find('Hsp_score')\n        if hsps is not None:\n            info['hit_score'] = float(hsps.text)\n\n        hit_list.append(info)\n\n    log.debug(\"{}: Number of BLAST hits\".format(len(hit_list)))\n    return hit_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles a PDB Construction Error", "response": "def _handle_builder_exception(self, message, residue):\n        \"\"\"\n        Makes a PDB Construction Error a bit more verbose and informative\n        \"\"\"\n        \n        message = \"%s. Error when parsing residue %s:%s\" %(message, residue['number'], residue['name'])\n        \n        raise PDBConstructionException(message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse(self):\n        \n        atom_counter = 0\n        \n        structure_build = self.structure_builder\n        \n        residues = self._extract_residues()\n        \n        cur_model = None\n        cur_chain = None\n        structure_build.init_seg(' ') # There is never a SEGID present\n        \n        for r in residues:\n\n            # New model?\n            if cur_model != r['model']:\n                cur_model = r['model']\n                try:\n                    structure_build.init_model(cur_model)\n                except PDBConstructionException, message:\n                    self._handle_builder_exception(message, r) \n            \n            # New chain?\n            if cur_chain != r['chain']:\n                cur_chain = r['chain']\n                try:\n                    structure_build.init_chain(cur_chain)\n                except PDBConstructionException, message:\n                    self._handle_builder_exception(message, r)\n\n            # Create residue\n            if r['name'] in AA_LIST: # Get residue type crudely since there is no HETATM / ATOM\n                hetero_flag = ' '\n            elif r['name'] == 'WAT' or r['name'] == 'HOH':\n                hetero_flag = 'W'\n            else:\n                hetero_flag = 'H'\n            \n            # Some terminal atoms are added at residue 0. This residue has a small number of atoms.\n            # Protonated non-terminal glycine has 7 atoms. Any of these residues is smaller.\n            # HETATMs have only a couple of atoms (3 for water for example) and they are ok.\n            \n            if (len(r['atoms']) >= 7) or (hetero_flag != \" \"):\n                try:\n                    structure_build.init_residue(r['name'], hetero_flag, r['number'], r['icode'])\n                except PDBConstructionException, message:\n                    self._handle_builder_exception(message, r)\n            \n                # Create Atoms\n                for atom in r['atoms']:\n                    a = self._parse_atom(atom)\n\n                    if not sum(a['coord']): # e.g. HG of metal bound CYS coords are 0,0,0.\n                        continue\n                        \n                    try:\n                        atom_counter += 1\n                        # fullname = name; altloc is empty;\n                        structure_build.init_atom(a['name'], a['coord'], a['bfactor'], a['occupancy'], ' ',\n                                                    a['name'], atom_counter, a['element'],  hetero_flag)\n                    except PDBConstructionException, message:\n                        self._handle_builder_exception(message, r)      \n            \n            elif len(r['atoms']) < 7: # Terminal Residues\n                                \n                for atom in r['atoms']:\n                    \n                    a = self._parse_atom(atom)\n                    \n                    if not sum(a['coord']): # e.g. HG of metal bound CYS coords are 0,0,0.\n                        continue\n\n                    atom_counter += 1                    \n                    ter_atom = Atom(a['name'], a['coord'], a['bfactor'], a['occupancy'], ' ',\n                                    a['name'], atom_counter, a['element'], hetero_flag)\n                    \n                    if a['name'] in N_TERMINAL_ATOMS:\n                        \n                        inc_struct = self.structure_builder.get_structure()\n                        for model in inc_struct:\n                            for chain in model:\n                                if chain.id == r['chain']:\n                                    for residue in chain: # Find First residue matching name\n                                        if residue.resname == r['name']:\n                                            residue.add(ter_atom)\n                                            break\n                    \n                    elif a['name'] in C_TERMINAL_ATOMS:\n\n                        inc_struct = self.structure_builder.get_structure()\n                        c_ter = None\n                        for model in inc_struct:\n                            for chain in model:\n                                if chain.id == r['chain']:\n                                    for residue in chain: # Find Last residue matching name\n                                        if residue.resname == r['name']:\n                                            c_ter = residue\n                                    if c_ter:\n                                        c_ter.add(ter_atom)", "response": "Parse the atomic data of the XML file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield residues from the response.", "response": "def _extract_residues(self):\n        \"\"\"\n        WHAT IF puts terminal atoms in new residues at the end for some reason..\n        \"\"\"        \n        \n        r_list = self.handle.getElementsByTagName(\"response\")\n        \n        r_data = {}\n        \n        for r in r_list:\n            data = self._parse_residue(r)\n            res_id = (data['model'], data['chain'], data['number']) # (A, 1, 1), (A, 1, 2), ...\n            if not r_data.has_key(res_id):\n                r_data[res_id] = data\n            else: # Some atoms get repeated at the end with TER Hydrogens/oxygens\n                r_data[res_id]['atoms'] += data['atoms'] # Append Atoms\n        \n        for key in sorted(r_data.keys()):\n            yield r_data[key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the XML response of a residue into a dictionary of the appropriate type.", "response": "def _parse_residue(self, residue):\n        \"\"\"\n        Extracts Residue Name, Number, Chain, Model, Atoms.\n        I/O: xml object <response> / dictionary\n        \"\"\"\n        \n        # Filter Element Nodes\n        childs = [ child for child in residue.childNodes if child.nodeType == child.ELEMENT_NODE ]\n        \n        # Parse info out\n        resi = int(childs[0].firstChild.data.strip())\n        resn = childs[1].firstChild.data.strip()\n        icode = childs[3].firstChild.data\n        chain = childs[4].firstChild.data.strip()\n        model = int(childs[5].firstChild.data.strip())\n        atoms = childs[6:]\n        \n        # Output\n        \n        return {'name':  resn,  'number': resi,\n                'icode': icode, 'chain':  chain, \n                'model': model, 'atoms': atoms}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate residue counts for all STRAINS along with SUBSEQUENCES and save them to a feather file.", "response": "def calculate_residue_counts_perstrain(protein_pickle_path, outdir, pdbflex_keys_file, wt_pid_cutoff=None, force_rerun=False):\n    \"\"\"Writes out a feather file for a PROTEIN counting amino acid occurences for ALL STRAINS along with SUBSEQUENCES\"\"\"\n    from collections import defaultdict\n    from ssbio.protein.sequence.seqprop import SeqProp\n    from ssbio.protein.sequence.properties.residues import _aa_property_dict_one\n    log = logging.getLogger(__name__)\n\n    protein_id = op.splitext(op.basename(protein_pickle_path))[0].split('_')[0]\n\n    protein_df_outfile = op.join(outdir, '{}_protein_strain_properties.fthr'.format(protein_id))\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=protein_df_outfile):\n\n        protein = ssbio.io.load_pickle(protein_pickle_path)\n\n        # First calculate disorder cuz i forgot to\n        protein.get_all_disorder_predictions(representative_only=True)\n\n        # Then get all subsequences\n        all_protein_subseqs = protein.get_all_subsequences(pdbflex_keys_file=pdbflex_keys_file)\n        if not all_protein_subseqs:\n            log.error('{}: cannot run subsequence calculator'.format(protein.id))\n            return\n\n        # Each strain gets a dictionary\n        strain_to_infodict = defaultdict(dict)\n\n        for seqprop_to_analyze in protein.sequences:\n            if seqprop_to_analyze.id == protein.representative_sequence.id:\n                strain_id = 'K12'\n            elif type(seqprop_to_analyze) == SeqProp and seqprop_to_analyze.id != protein.id:  # This is to filter out other KEGGProps or UniProtProps\n                strain_id = seqprop_to_analyze.id.split('_', 1)[1]  # This split should work for all strains\n            else:\n                continue\n\n            ## Additional filtering for genes marked as orthologous but actually have large deletions or something\n            ## TODO: experiment with other cutoffs?\n            if wt_pid_cutoff:\n                aln = protein.sequence_alignments.get_by_id('{0}_{0}_{1}'.format(protein.id, seqprop_to_analyze.id))\n                if aln.annotations['percent_identity'] < wt_pid_cutoff:\n                    continue\n\n            ###### Calculate \"all\" properties ######\n            seqprop_to_analyze.get_biopython_pepstats()\n\n            # [ALL] aa_count\n            if 'amino_acids_percent-biop' not in seqprop_to_analyze.annotations:  # May not run if weird amino acids in the sequence\n                log.warning('Protein {}, sequence {}: skipping, unable to run Biopython ProteinAnalysis'.format(protein.id,\n                                                                                                            seqprop_to_analyze.id))\n                continue\n            strain_to_infodict[strain_id].update({'aa_count_{}'.format(k): v for k, v in seqprop_to_analyze.annotations['amino_acids_content-biop'].items()})\n\n            # [ALL] aa_count_total\n            strain_to_infodict[strain_id]['aa_count_total'] = seqprop_to_analyze.seq_len\n\n            ###### Calculate subsequence properties ######\n            for prop, propdict in all_protein_subseqs.items():\n                strain_to_infodict[strain_id].update(protein.get_subseq_props(property_dict=propdict, property_name=prop,\n                                                                              seqprop=seqprop_to_analyze))\n\n        protein_df = pd.DataFrame(strain_to_infodict)\n        protein_df.reset_index().to_feather(protein_df_outfile)\n\n    return protein_pickle_path, protein_df_outfile"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the orthology matrix for the current protein sequence.", "response": "def get_orthology_matrix(self, outfile, sc, outdir=None,\n                             pid_cutoff=None, bitscore_cutoff=None, evalue_cutoff=None,\n                             force_rerun=False):\n        \"\"\"Create the orthology matrix by finding best bidirectional BLAST hits. Genes = rows, strains = columns\n\n        Runs run_makeblastdb, run_bidirectional_blast, and calculate_bbh for protein sequences.\n\n        Args:\n            outfile (str): Filename with extension of the orthology matrix (ie. df_orthology.csv)\n            outdir (str): Path to output of orthology matrix, default is ATLAS data_dir\n            pid_cutoff (float): Minimum percent identity between BLAST hits to filter for in the range [0, 100]\n            bitscore_cutoff (float): Minimum bitscore allowed between BLAST hits\n            evalue_cutoff (float): Maximum E-value allowed between BLAST hits\n            filter_condition (str): 'OR' or 'AND', how to combine cutoff filters. 'OR' gives more results since it\n            is less stringent, as you will be filtering for hits with (>80% PID or >30 bitscore or <0.0001 evalue).\n            remove_strains_with_no_orthology (bool): Remove strains which have no orthologous genes found\n            remove_strains_with_no_differences (bool): Remove strains which have all the same genes as the base model.\n                Default is False because since orthology is found using a PID cutoff, all genes may be present but\n                differences may be on the sequence level.\n            remove_genes_not_in_base_model (bool): Remove genes from the orthology matrix which are not present in our\n                base model. This happens if we use a genome file for our model that has other genes in it.\n\n        Returns:\n            DataFrame: Orthology matrix calculated from best bidirectional BLAST hits.\n\n        \"\"\"\n        # Output directory for orthology matrix\n        if not outdir:\n            outdir = self.data_dir\n\n        ortho_matrix = op.join(outdir, outfile)\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=ortho_matrix):\n            if not sc:\n                raise ValueError('Please initialize SparkContext')\n            ################################################################################################################\n            # BIDIRECTIONAL BLAST\n            def run_bidirectional_blast(strain_id, strain_genome_path,\n                                        r_file=self.reference_gempro.genome_path,\n                                        outdir2=self.sequences_by_organism_dir):\n                import ssbio.protein.sequence.utils.blast\n\n                # Run bidirectional BLAST\n                r_vs_g, g_vs_r = ssbio.protein.sequence.utils.blast.run_bidirectional_blast(reference=r_file,\n                                                                                            other_genome=strain_genome_path,\n                                                                                            dbtype='prot',\n                                                                                            outdir=outdir2)\n\n                # Using the BLAST files, find the BBH\n                bbh = ssbio.protein.sequence.utils.blast.calculate_bbh(blast_results_1=r_vs_g, blast_results_2=g_vs_r,\n                                                                       outdir=outdir2)\n                return strain_id, bbh\n\n            log.info('Running bidirectional BLAST and finding best bidirectional hits (BBH)...')\n            for_strains_rdd = [(k, v['genome_path']) for k,v in self.strain_infodict.items()]\n            from random import shuffle\n            shuffle(for_strains_rdd)\n            strains_rdd = sc.parallelize(for_strains_rdd)\n            result = strains_rdd.map(lambda x: run_bidirectional_blast(strain_id=x[0], strain_genome_path=x[1])).collect()\n            bbh_files = dict(result)\n            ################################################################################################################\n\n            ################################################################################################################\n            # ORTHOLOGY MATRIX\n            log.info('Creating orthology matrix from BBHs...')\n            ortho_matrix = ssbio.protein.sequence.utils.blast.create_orthology_matrix(r_name=self.reference_gempro.id,\n                                                                                      genome_to_bbh_files=bbh_files,\n                                                                                      pid_cutoff=pid_cutoff,\n                                                                                      bitscore_cutoff=bitscore_cutoff,\n                                                                                      evalue_cutoff=evalue_cutoff,\n                                                                                      outname=outfile,\n                                                                                      outdir=outdir,\n                                                                                      force_rerun=force_rerun)\n\n        log.info('Orthology matrix at {}. See the \"df_orthology_matrix\" attribute.'.format(ortho_matrix))\n        self.df_orthology_matrix = pd.read_csv(ortho_matrix, index_col=0)\n        self.df_orthology_matrix = self.df_orthology_matrix.rename_axis('gene').rename_axis(\"strain\", axis=\"columns\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_genes_and_strains(self, remove_genes_not_in_reference_model=True,\n                                 remove_strains_with_no_orthology=True, remove_strains_with_no_differences=False,\n                                 custom_keep_strains=None, custom_keep_genes=None):\n        \"\"\"Filters the analysis by keeping a subset of strains or genes based on certain criteria.\n\n        Args:\n            remove_genes_not_in_reference_model (bool): Remove genes from reference model not in orthology matrix\n            remove_strains_with_no_orthology (bool): Remove strains which have no orthologous genes found\n            remove_strains_with_no_differences (bool): Remove strains which have all the same genes as the base model.\n                Default is False because since orthology is found using a PID cutoff, all genes may be present but\n                differences may be on the sequence level.\n            custom_keep_genes (list): List of gene IDs to keep in analysis\n            custom_keep_strains (list): List of strain IDs to keep in analysis\n\n        \"\"\"\n\n        if len(self.df_orthology_matrix) == 0:\n            raise RuntimeError('Empty orthology matrix, please calculate first!')\n\n        reference_strain_gene_ids = [x.id for x in self.reference_gempro.genes]\n        initial_num_genes = len(reference_strain_gene_ids)\n\n        initial_num_strains = len(self.strain_ids)\n\n        # Gene filtering\n        to_remove_genes = []\n        if custom_keep_genes:\n            to_remove_genes.extend([x for x in reference_strain_gene_ids if x not in custom_keep_genes])\n        if remove_genes_not_in_reference_model:\n            to_remove_genes.extend([x for x in reference_strain_gene_ids if x not in self.df_orthology_matrix.index.tolist()])\n\n        to_remove_genes = list(set(to_remove_genes))\n        if self.reference_gempro.model:\n            cobra.manipulation.delete_model_genes(self.reference_gempro.model, to_remove_genes)\n        else:\n            for g_id in to_remove_genes:\n                self.reference_gempro.genes.get_by_id(g_id).functional = False\n\n        # Create new orthology matrix with only our genes of interest\n        new_gene_subset = [x.id for x in self.reference_gempro.functional_genes]\n        tmp_new_orthology_matrix = self.df_orthology_matrix[self.df_orthology_matrix.index.isin(new_gene_subset)]\n\n        # Strain filtering\n        if custom_keep_strains or remove_strains_with_no_orthology or remove_strains_with_no_differences:\n            for strain_id in self.strain_ids:\n                if custom_keep_strains:\n                    if strain_id not in custom_keep_strains:\n                        self.strain_ids.remove(strain_id)\n                        continue\n\n                if remove_strains_with_no_orthology:\n                    if strain_id not in tmp_new_orthology_matrix.columns:\n                        self.strain_ids.remove(strain_id)\n                        log.info('{}: no orthologous genes found for this strain, removed from analysis.'.format(strain_id))\n                        continue\n                    elif tmp_new_orthology_matrix[strain_id].isnull().all():\n                        self.strain_ids.remove(strain_id)\n                        log.info('{}: no orthologous genes found for this strain, removed from analysis.'.format(strain_id))\n                        continue\n\n                if remove_strains_with_no_differences:\n                    not_in_strain = tmp_new_orthology_matrix[pd.isnull(tmp_new_orthology_matrix[strain_id])][strain_id].index.tolist()\n                    if len(not_in_strain) == 0:\n                        self.strain_ids.remove(strain_id)\n                        log.info('{}: strain has no differences from the base, removed from analysis.')\n                        continue\n\n        log.info('{} genes to be analyzed, originally {}'.format(len(self.reference_gempro.functional_genes), initial_num_genes))\n        log.info('{} strains to be analyzed, originally {}'.format(len(self.strain_ids), initial_num_strains))", "response": "Filters the analysis by keeping a subset of strains or genes based on certain criteria."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the strain functional genes to the json file", "response": "def _write_strain_functional_genes(self, strain_id, ref_functional_genes, orth_matrix, force_rerun=False):\n        \"\"\"Create strain functional genes json file\"\"\"\n        func_genes_path = op.join(self.model_dir, '{}_funcgenes.json'.format(strain_id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=func_genes_path):\n            gene_to_func = {k:True for k in ref_functional_genes}\n            # Get a list of genes which do not have orthology in the strain\n            genes_to_remove = orth_matrix[pd.isnull(orth_matrix[strain_id])][strain_id].index.tolist()\n\n            # Mark genes non-functional\n            genes_to_remove = list(set(genes_to_remove).intersection(set(ref_functional_genes)))\n\n            if len(genes_to_remove) > 0:\n                for g in genes_to_remove:\n                    gene_to_func[g] = False\n\n            with open(func_genes_path, 'w') as f:\n                json.dump(gene_to_func, f)\n        else:\n            with open(func_genes_path, 'r') as f:\n                gene_to_func = json.load(f)\n\n        return strain_id, gene_to_func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap function for _write_strain_functional_genes", "response": "def write_strain_functional_genes(self, force_rerun=False):\n        \"\"\"Wrapper function for _write_strain_functional_genes\"\"\"\n        if len(self.df_orthology_matrix) == 0:\n            raise RuntimeError('Empty orthology matrix, please calculate first!')\n        ref_functional_genes = [g.id for g in self.reference_gempro.functional_genes]\n        log.info('Building strain specific models...')\n        result = []\n        for s in tqdm(self.strain_ids):\n            result.append(self._write_strain_functional_genes(s, ref_functional_genes, self.df_orthology_matrix, force_rerun=force_rerun))\n\n        for strain_id, functional_genes in result:\n            self.strain_infodict[strain_id]['functional_genes'] = functional_genes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_strain_specific_model(self, strain_id, ref_functional_genes, orth_matrix, force_rerun=False):\n        gp_noseqs_path = op.join(self.model_dir, '{}_gp.pckl'.format(strain_id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=gp_noseqs_path):\n            logging.disable(logging.WARNING)\n\n            strain_gp = GEMPRO(gem_name=strain_id)\n\n            # if self.reference_gempro.model:\n            #     strain_gp.load_cobra_model(deepcopy(self.reference_gempro.model))\n            #     # Reset the GenePro attributes\n            #     for x in strain_gp.genes:\n            #         x.reset_protein()\n            # else:\n                # Otherwise, just copy the list of genes over and rename the IDs\n            strain_genes = [x for x in ref_functional_genes]\n            strain_gp.add_gene_ids(strain_genes)\n\n            logging.disable(logging.NOTSET)\n\n            # Get a list of genes which do not have orthology in the strain\n            genes_to_remove = orth_matrix[pd.isnull(orth_matrix[strain_id])][strain_id].index.tolist()\n\n            # Mark genes non-functional\n            strain_genes = [x.id for x in strain_gp.genes]\n            genes_to_remove = list(set(genes_to_remove).intersection(set(strain_genes)))\n\n            if len(genes_to_remove) > 0:\n                # If a COBRApy model exists, utilize the delete_model_genes method\n                # if strain_gp.model:\n                #     cobra.manipulation.delete_model_genes(strain_gp.model, genes_to_remove)\n                # # Otherwise, just mark the genes as non-functional\n                # else:\n                for g in genes_to_remove:\n                    strain_gp.genes.get_by_id(g).functional = False\n\n            strain_gp.save_pickle(outfile=gp_noseqs_path)\n\n        return strain_id, gp_noseqs_path", "response": "Create a strain GEMPRO and set functional genes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_strain_specific_models(self, joblib=False, cores=1, force_rerun=False):\n        if len(self.df_orthology_matrix) == 0:\n            raise RuntimeError('Empty orthology matrix, please calculate first!')\n        ref_functional_genes = [g.id for g in self.reference_gempro.functional_genes]\n        log.info('Building strain specific models...')\n        if joblib:\n            result = DictList(Parallel(n_jobs=cores)(delayed(self._build_strain_specific_model)(s, ref_functional_genes, self.df_orthology_matrix, force_rerun=force_rerun) for s in self.strain_ids))\n        # if sc:\n        #     strains_rdd = sc.parallelize(self.strain_ids)\n        #     result = strains_rdd.map(self._build_strain_specific_model).collect()\n        else:\n            result = []\n            for s in tqdm(self.strain_ids):\n                result.append(self._build_strain_specific_model(s, ref_functional_genes, self.df_orthology_matrix, force_rerun=force_rerun))\n\n        for strain_id, gp_noseqs_path in result:\n            self.strain_infodict[strain_id]['gp_noseqs_path'] = gp_noseqs_path", "response": "Wrapper function for _build_strain_specific_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads GEMPRO with functional genes defined load sequences to it save as new GEMPRO", "response": "def _load_sequences_to_strain(self, strain_id, force_rerun=False):\n        \"\"\"Load strain GEMPRO with functional genes defined, load sequences to it, save as new GEMPRO\"\"\"\n        gp_seqs_path = op.join(self.model_dir, '{}_gp_withseqs.pckl'.format(strain_id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=gp_seqs_path):\n            gp_noseqs = ssbio.io.load_pickle(self.strain_infodict[strain_id]['gp_noseqs_path'])\n            strain_sequences = SeqIO.index(self.strain_infodict[strain_id]['genome_path'], 'fasta')\n            for strain_gene in gp_noseqs.functional_genes:\n                # Pull the gene ID of the strain from the orthology matrix\n                strain_gene_key = self.df_orthology_matrix.at[strain_gene.id, strain_id]\n                # Load into the strain GEM-PRO\n                new_id = '{}_{}'.format(strain_gene.id, strain_id)\n                if strain_gene.protein.sequences.has_id(new_id):\n                    continue\n                strain_gene.protein.load_manual_sequence(seq=strain_sequences[strain_gene_key], ident=new_id,\n                                                         set_as_representative=True)\n            gp_noseqs.save_pickle(outfile=gp_seqs_path)\n\n        return strain_id, gp_seqs_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_sequences_to_strains(self, joblib=False, cores=1, force_rerun=False):\n        log.info('Loading sequences to strain GEM-PROs...')\n        if joblib:\n            result = DictList(Parallel(n_jobs=cores)(delayed(self._load_sequences_to_strain)(s, force_rerun) for s in self.strain_ids))\n        else:\n            result = []\n            for s in tqdm(self.strain_ids):\n                result.append(self._load_sequences_to_strain(s, force_rerun))\n\n        for strain_id, gp_seqs_path in result:\n            self.strain_infodict[strain_id]['gp_seqs_path'] = gp_seqs_path", "response": "Wrapper function for _load_sequences_to_strain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload orthologous strain sequences to reference Protein object save as new pickle", "response": "def _load_sequences_to_reference_gene(self, g_id, force_rerun=False):\n        \"\"\"Load orthologous strain sequences to reference Protein object, save as new pickle\"\"\"\n        protein_seqs_pickle_path = op.join(self.sequences_by_gene_dir, '{}_protein_withseqs.pckl'.format(g_id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=protein_seqs_pickle_path):\n            protein_pickle_path = self.gene_protein_pickles[g_id]\n            protein_pickle = ssbio.io.load_pickle(protein_pickle_path)\n\n            for strain, info in self.strain_infodict.items():\n                strain_sequences = SeqIO.index(info['genome_path'], 'fasta')\n                strain_gene_functional = info['functional_genes'][g_id]\n                if strain_gene_functional:\n                    # Pull the gene ID of the strain from the orthology matrix\n                    strain_gene_key = self.df_orthology_matrix.at[g_id, strain]\n                    new_id = '{}_{}'.format(g_id, strain)\n                    if protein_pickle.sequences.has_id(new_id):\n                        continue\n                    protein_pickle.load_manual_sequence(seq=strain_sequences[strain_gene_key],\n                                                        ident=new_id,\n                                                        set_as_representative=False)\n            protein_pickle.save_pickle(outfile=protein_seqs_pickle_path)\n\n        return g_id, protein_seqs_pickle_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store_disorder(self, sc=None, force_rerun=False):\n        log.info('Loading sequences to reference GEM-PRO...')\n        from random import shuffle\n        g_ids = [g.id for g in self.reference_gempro.functional_genes]\n        shuffle(g_ids)\n\n        def _store_disorder_sc(g_id, outdir=self.sequences_by_gene_dir,\n                               g_to_pickle=self.gene_protein_pickles, force_rerun=force_rerun):\n            \"\"\"Load orthologous strain sequences to reference Protein object, save as new pickle\"\"\"\n            import ssbio.utils\n            import ssbio.io\n            import os.path as op\n\n            protein_seqs_pickle_path = op.join(outdir, '{}_protein_withseqs_dis.pckl'.format(g_id))\n\n            if ssbio.utils.force_rerun(flag=force_rerun, outfile=protein_seqs_pickle_path):\n                protein_pickle_path = g_to_pickle[g_id]\n                protein_pickle = ssbio.io.load_pickle(protein_pickle_path)\n                protein_pickle.get_all_disorder_predictions(representative_only=False)\n                protein_pickle.save_pickle(outfile=protein_seqs_pickle_path)\n\n            return g_id, protein_seqs_pickle_path\n\n        if sc:\n            genes_rdd = sc.parallelize(g_ids)\n            result = genes_rdd.map(_store_disorder_sc).collect()\n        else:\n            result = []\n            for g in tqdm(g_ids):\n                result.append(self._load_sequences_to_reference_gene(g, force_rerun))\n\n        log.info('Storing paths to new Protein objects in self.gene_protein_pickles...')\n        for g_id, protein_pickle in result:\n            self.gene_protein_pickles[g_id] = protein_pickle", "response": "Store orthologous strain sequences to a new pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naligning orthologous strain sequences to representative Protein sequence save as new pickle", "response": "def _align_orthologous_gene_pairwise(self, g_id, gapopen=10, gapextend=0.5, engine='needle', parse=True, force_rerun=False):\n        \"\"\"Align orthologous strain sequences to representative Protein sequence, save as new pickle\"\"\"\n        protein_seqs_aln_pickle_path = op.join(self.sequences_by_gene_dir, '{}_protein_withseqs_dis_aln.pckl'.format(g_id))\n\n        if ssbio.utils.force_rerun(flag=force_rerun, outfile=protein_seqs_aln_pickle_path):\n            protein_seqs_pickle_path = self.gene_protein_pickles[g_id]\n            protein_pickle = ssbio.io.load_pickle(protein_seqs_pickle_path)\n\n            if not protein_pickle.representative_sequence:\n                log.error('{}: no representative sequence to align to'.format(g_id))\n                return\n\n            if len(protein_pickle.sequences) < 1:\n                log.error('{}: no other sequences to align to'.format(g_id))\n                return\n\n            alignment_dir = op.join(self.sequences_by_gene_dir, g_id)\n            ssbio.utils.make_dir(alignment_dir)\n            protein_pickle.pairwise_align_sequences_to_representative(gapopen=gapopen, gapextend=gapextend,\n                                                                      engine=engine, outdir=alignment_dir,\n                                                                      parse=parse, force_rerun=force_rerun)\n            protein_pickle.save_pickle(outfile=protein_seqs_aln_pickle_path)\n\n        return g_id, protein_seqs_aln_pickle_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\naligns orthologous strain sequences to representative Protein sequences", "response": "def align_orthologous_genes_pairwise(self, sc=None, joblib=False, cores=1, gapopen=10, gapextend=0.5,\n                                         engine='needle', parse=True, force_rerun=False):\n        \"\"\"Wrapper for _align_orthologous_gene_pairwise\"\"\"\n        log.info('Aligning sequences to reference GEM-PRO...')\n        from random import shuffle\n        g_ids = [g.id for g in self.reference_gempro.functional_genes]\n        shuffle(g_ids)\n\n        def _align_orthologous_gene_pairwise_sc(g_id, g_to_pickle=self.gene_protein_pickles,\n                                                gapopen=gapopen, gapextend=gapextend, engine=engine, parse=parse,\n                                                outdir=self.sequences_by_gene_dir,\n                                                force_rerun=force_rerun):\n            \"\"\"Align orthologous strain sequences to representative Protein sequence, save as new pickle\"\"\"\n            import ssbio.utils\n            import ssbio.io\n            import os.path as op\n\n            protein_seqs_aln_pickle_path = op.join(outdir, '{}_protein_withseqs_dis_aln.pckl'.format(g_id))\n\n            if ssbio.utils.force_rerun(flag=force_rerun, outfile=protein_seqs_aln_pickle_path):\n                protein_seqs_pickle_path = g_to_pickle[g_id]\n                protein_pickle = ssbio.io.load_pickle(protein_seqs_pickle_path)\n\n                if not protein_pickle.representative_sequence:\n                    return\n\n                if len(protein_pickle.sequences) < 1:\n                    return\n\n                alignment_dir = op.join(outdir, g_id)\n                ssbio.utils.make_dir(alignment_dir)\n                protein_pickle.pairwise_align_sequences_to_representative(gapopen=gapopen, gapextend=gapextend,\n                                                                          engine=engine, outdir=alignment_dir,\n                                                                          parse=parse, force_rerun=force_rerun)\n                protein_pickle.save_pickle(outfile=protein_seqs_aln_pickle_path)\n\n            return g_id, protein_seqs_aln_pickle_path\n\n        if sc:\n            genes_rdd = sc.parallelize(g_ids)\n            result_raw = genes_rdd.map(_align_orthologous_gene_pairwise_sc).collect()\n        else:\n            result_raw = []\n            for g in tqdm(g_ids):\n                result_raw.append(self._align_orthologous_gene_pairwise(g, gapopen=gapopen, gapextend=gapextend,\n                                                                    engine=engine, parse=parse,\n                                                                    force_rerun=force_rerun))\n\n        result = [x for x in result_raw if x is not None]\n        log.info('Storing paths to new Protein objects in self.gene_protein_pickles...')\n        for g_id, protein_pickle in result:\n            self.gene_protein_pickles[g_id] = protein_pickle"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncast a string representation of a Seq or SeqRecord to a string representation.", "response": "def cast_to_str(obj):\n    \"\"\"Return a string representation of a Seq or SeqRecord.\n\n    Args:\n        obj (str, Seq, SeqRecord): Biopython Seq or SeqRecord\n\n    Returns:\n        str: String representation of the sequence\n\n    \"\"\"\n\n    if isinstance(obj, str):\n        return obj\n    if isinstance(obj, Seq):\n        return str(obj)\n    if isinstance(obj, SeqRecord):\n        return str(obj.seq)\n    else:\n        raise ValueError('Must provide a string, Seq, or SeqRecord object.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncasting a string or SeqRecord object to a Seq object.", "response": "def cast_to_seq(obj, alphabet=IUPAC.extended_protein):\n    \"\"\"Return a Seq representation of a string or SeqRecord object.\n\n    Args:\n        obj (str, Seq, SeqRecord): Sequence string or Biopython SeqRecord object\n        alphabet: See Biopython SeqRecord docs\n\n    Returns:\n        Seq: Seq representation of the sequence\n\n    \"\"\"\n\n    if isinstance(obj, Seq):\n        return obj\n    if isinstance(obj, SeqRecord):\n        return obj.seq\n    if isinstance(obj, str):\n        obj = obj.upper()\n        return Seq(obj, alphabet)\n    else:\n        raise ValueError('Must provide a string, Seq, or SeqRecord object.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cast_to_seq_record(obj, alphabet=IUPAC.extended_protein, id=\"<unknown id>\", name=\"<unknown name>\",\n                       description=\"<unknown description>\", dbxrefs=None,\n                       features=None, annotations=None,\n                       letter_annotations=None):\n    \"\"\"Return a SeqRecord representation of a string or Seq object.\n\n    Args:\n        obj (str, Seq, SeqRecord): Sequence string or Biopython Seq object\n        alphabet: See Biopython SeqRecord docs\n        id: See Biopython SeqRecord docs\n        name: See Biopython SeqRecord docs\n        description: See Biopython SeqRecord docs\n        dbxrefs: See Biopython SeqRecord docs\n        features: See Biopython SeqRecord docs\n        annotations: See Biopython SeqRecord docs\n        letter_annotations: See Biopython SeqRecord docs\n\n    Returns:\n        SeqRecord: SeqRecord representation of the sequence\n\n    \"\"\"\n\n    if isinstance(obj, SeqRecord):\n        return obj\n    if isinstance(obj, Seq):\n        return SeqRecord(obj, id, name, description, dbxrefs, features, annotations, letter_annotations)\n    if isinstance(obj, str):\n        obj = obj.upper()\n        return SeqRecord(Seq(obj, alphabet), id, name, description, dbxrefs, features, annotations, letter_annotations)\n    else:\n        raise ValueError('Must provide a string, Seq, or SeqRecord object.')", "response": "Cast a string or Seq object to a SeqRecord."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_fasta_file(seq_records, outname, outdir=None, outext='.faa', force_rerun=False):\n\n    if not outdir:\n        outdir = ''\n    outfile = ssbio.utils.outfile_maker(inname='', outname=outname, outdir=outdir, outext=outext)\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        SeqIO.write(seq_records, outfile, \"fasta\")\n\n    return outfile", "response": "Write a FASTA file for a list of SeqRecords or a list of SeqRecords."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a FASTA file for a dictionary of IDs and sequence strings.", "response": "def write_fasta_file_from_dict(indict, outname, outdir=None, outext='.faa', force_rerun=False):\n    \"\"\"Write a FASTA file for a dictionary of IDs and their sequence strings.\n\n    Args:\n        indict: Input dictionary with keys as IDs and values as sequence strings\n        outname: Name of the output file which will have outext appended to it\n        outdir: Path to directory to output sequences to\n        outext: Extension of FASTA file, default \".faa\"\n        force_rerun: If file should be overwritten if it exists\n\n    Returns:\n        str: Path to output FASTA file.\n\n    \"\"\"\n\n    if not outdir:\n        outdir = ''\n    outfile = ssbio.utils.outfile_maker(inname='', outname=outname, outdir=outdir, outext=outext)\n\n    if ssbio.utils.force_rerun(flag=force_rerun, outfile=outfile):\n        seqs = []\n        for i, s in indict.items():\n            seq = ssbio.protein.sequence.utils.cast_to_seq_record(s, id=i)\n            seqs.append(seq)\n        SeqIO.write(seqs, outfile, \"fasta\")\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a sequence as a temporary FASTA file", "response": "def write_seq_as_temp_fasta(seq):\n    \"\"\"Write a sequence as a temporary FASTA file\n\n    Args:\n        seq (str, Seq, SeqRecord): Sequence string, Biopython Seq or SeqRecord object\n\n    Returns:\n        str: Path to temporary FASTA file (located in system temporary files directory)\n\n    \"\"\"\n    sr = ssbio.protein.sequence.utils.cast_to_seq_record(seq, id='tempfasta')\n    return write_fasta_file(seq_records=sr, outname='temp', outdir=tempfile.gettempdir(), force_rerun=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a FASTA file and return the sequences as a list of Biopython SeqRecords", "response": "def load_fasta_file(filename):\n    \"\"\"Load a FASTA file and return the sequences as a list of SeqRecords\n\n    Args:\n        filename (str): Path to the FASTA file to load\n\n    Returns:\n        list: list of all sequences in the FASTA file as Biopython SeqRecord objects\n\n    \"\"\"\n\n    with open(filename, \"r\") as handle:\n        records = list(SeqIO.parse(handle, \"fasta\"))\n    return records"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_fasta_file_as_dict_of_seqs(filename):\n\n    results = {}\n    records = load_fasta_file(filename)\n    for r in records:\n        results[r.id] = str(r.seq)\n\n    return results", "response": "Load a FASTA file and return the sequences as a dict of IDs to their sequence strings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a FASTA file and return the sequences as a dict of SeqRecords", "response": "def load_fasta_file_as_dict_of_seqrecords(filename):\n    \"\"\"Load a FASTA file and return the sequences as a dict of {ID: SeqRecord}\n\n    Args:\n        filename (str): Path to the FASTA file to load\n\n    Returns:\n        dict: Dictionary of IDs to their SeqRecords\n\n    \"\"\"\n\n    results = {}\n    records = load_fasta_file(filename)\n    for r in records:\n        results[r.id] = r\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fasta_files_equal(seq_file1, seq_file2):\n\n    # Load already set representative sequence\n    seq1 = SeqIO.read(open(seq_file1), 'fasta')\n\n    # Load kegg sequence\n    seq2 = SeqIO.read(open(seq_file2), 'fasta')\n\n    # Test equality\n    if str(seq1.seq) == str(seq2.seq):\n        return True\n    else:\n        return False", "response": "Check equality of a FASTA file to another FASTA file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CA_TRACE(residue):\n\n    cg_residue = deepcopy(residue)\n\n    for atom in cg_residue.child_dict.keys():\n        if cg_residue[atom].name != 'CA':\n            cg_residue.detach_child(cg_residue[atom].name)\n    \n    return cg_residue", "response": "Returns a copy of the given residue with the alpha carbon atom trace only."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new base - 3 - point ENCAD object for a given residue.", "response": "def ENCAD_3P(residue):\n    \"\"\"\n    Reduces complexity of protein residue to a 3 points coarse grained model:\n    CA, O, Bead in specific atom location.\n    \n    Based on Michael Levitt's ENCAD coarse graining model (2010).\n    \"\"\"\n    \n    conversion_table = {\n                        \"LEU\": [\"CG\"],\n                        \"ALA\": [\"CB\"],  \n                        \"GLY\": [],  \n                        \"VAL\": [\"CB\"],\n                        \"GLU\": [\"CD\"],\n                        \"ASP\": [\"CG\"],\n                        \"THR\": [\"CB\"],\n                        \"LYS\": [\"CD\"],\n                        \"ILE\": [\"CG1\"],\n                        \"ARG\": [\"NE\"],\n                        \"ASN\": [\"CG\"],\n                        \"PRO\": [\"CG\"],\n                        \"PHE\": [\"CG\"],\n                        \"GLN\": [\"CD\"],\n                        \"SER\": [\"CB\", \"OG\"],\n                        \"HIS\": [\"CG\"],\n                        \"MET\": [\"CG\"],\n                        \"TYR\": [\"CD1\", \"CD2\"],\n                        \"TRP\": [\"CD2\"],\n                        \"CYS\": [\"CB\", \"SG\"],\n                        }\n    \n    cg_residue = deepcopy(residue)\n    sc_atoms = []\n    \n    residue_name = cg_residue.resname\n    \n    if residue_name not in conversion_table:\n        raise ValueError(\"Residue %s not recognized\" %residue_name)\n        \n    for atom in cg_residue.child_dict.keys():\n        if cg_residue[atom].name in conversion_table[residue_name]:\n            sc_atoms.append(cg_residue[atom])\n            cg_residue.detach_child(cg_residue[atom].name)            \n        elif not cg_residue[atom].name in [\"CA\", \"O\"]:\n            cg_residue.detach_child(cg_residue[atom].name)    \n\n    # Side chain bead\n    if len(sc_atoms):\n        dummy = sorted(sc_atoms)[-1]\n        if len(sc_atoms) > 1: # Merge - C.o.M        \n            bead_coord = center_of_mass(sc_atoms, geometric=True)\n        else:\n            bead_coord = dummy.coord\n            \n        sc_bead = Atom( 'CMA', \n                        bead_coord, \n                        dummy.bfactor,\n                        dummy.occupancy,\n                        dummy.altloc,\n                        ' CMA',\n                        dummy.serial_number,\n                        dummy.element,\n                        dummy.hetatm)\n\n        cg_residue.add(sc_bead)\n        \n    return cg_residue"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a MARTINI coarse - grained model for a given residue.", "response": "def MARTINI(residue):\n    \"\"\"\n    Reduces complexity of protein residue to the MARTINI coarse grained model:\n    CA, O, Bead(s) in specific atom location.\n    \n    Reference:\n    Monticelli et al. The MARTINI coarse-grained force field: extension to proteins. \n    J. Chem. Theory Comput. (2008) vol. 4 (5) pp. 819-834\n    \"\"\"\n    \n    conversion_table = {\n                        \"LEU\": [\"CG\"],\n                        \"ALA\": [], # No Side Chain  \n                        \"GLY\": [],  \n                        \"VAL\": [\"CB\"],\n                        \"GLU\": [\"CB\"],\n                        \"ASP\": [\"CG\"],\n                        \"THR\": [\"CB\"],\n                        \"LYS\": [\"CG\", \"CE\"],\n                        \"ILE\": [\"CD1\"],\n                        \"ARG\": [\"CG\", \"NE\"],\n                        \"ASN\": [\"CG\"],\n                        \"PRO\": [\"CG\"],\n                        \"PHE\": [\"CG\", \"CE1\", \"CE2\"],\n                        \"GLN\": [\"CB\"],\n                        \"SER\": [\"CB\"],\n                        \"HIS\": [\"CB\", \"ND1\", \"NE2\"],\n                        \"MET\": [\"CG\"],\n                        \"TYR\": [\"CG\", \"CE1\", \"CE2\"],\n                        \"TRP\": [\"CB\", \"CD1\", \"CD2\", \"CE2\"],\n                        \"CYS\": [\"SG\"],\n                        }\n                        \n    cg_residue = deepcopy(residue)\n    \n    residue_name = cg_residue.resname\n    \n    if residue_name not in conversion_table:\n        raise ValueError(\"Residue %s not recognized\" %residue_name)\n    \n    for atom in cg_residue.child_dict.keys():\n        if cg_residue[atom].name in conversion_table[residue_name]:\n            # S1, S2, S3, or S4\n            name = \"S%s\" % (int(conversion_table[residue_name].index(cg_residue[atom].name)) + 1)\n            cg_residue[atom].name = name\n            cg_residue[atom].fullname = \" %s \" %name\n            cg_residue[atom].id = name\n            cg_residue[atom].occupancy = int(cg_residue[atom].occupancy)\n            cg_residue[atom].bfactor = int(cg_residue[atom].bfactor)\n            cg_residue[atom].coord = [ int(xyz) for xyz in cg_residue[atom].coord ]\n            \n        elif cg_residue[atom].name == \"CA\":\n            cg_residue[atom].name = \"BB\"\n            cg_residue[atom].fullname = \" BB \"\n            cg_residue[atom].id = \"BB\"\n            cg_residue[atom].occupancy = int(cg_residue[atom].occupancy)\n            cg_residue[atom].bfactor = int(cg_residue[atom].bfactor)\n            cg_residue[atom].coord = [ int(xyz) for xyz in cg_residue[atom].coord ]                        \n        else:\n            cg_residue.detach_child(cg_residue[atom].name)\n            \n            \n    return cg_residue"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads all coding sequences from NCBI.", "response": "def download_coding_sequences(patric_id, seqtype, outdir='', outfile='', force_rerun=False):\n    \"\"\"Download the entire set of DNA or protein sequences from protein-encoding genes in a genome from NCBI.\n\n    Saves a FASTA file in the optional directory specified.\n\n    Args:\n        genome_accession_or_id (str): PATRIC ID\n        seqtype (str): \"dna\" or \"protein\" - if you want the coding sequences in DNA or amino acid formats.\n        outdir (str): optional output directory (default is the current directory)\n        outfile (str): optional custom name for file\n        force_rerun (bool): if you want to redownload existing files\n\n    Returns:\n        Path to downloaded FASTA file.\n\n    \"\"\"\n    if seqtype == 'dna':\n        extension = 'ffn'\n    elif seqtype == 'protein':\n        extension = 'faa'\n    else:\n        raise ValueError('seqtype must be \"dna\" or \"protein\"')\n\n    # TODO: use utils functions here\n    # path and filename parsing\n    if outfile:\n        outfile = op.join(outdir, '{}.{}'.format(outfile, extension))\n    else:\n        # if no outfile is specified, default is \"$GI.PATRIC.faa\"\n        outfile = op.join(outdir, '{}.PATRIC.{}'.format(patric_id, extension))\n\n    if not force_rerun:\n        # return the path to the file if it was already downloaded\n        if op.exists(outfile) and os.stat(outfile).st_size != 0:\n            log.debug('FASTA file already exists at {}'.format(outfile))\n            return outfile\n\n    try:\n        ftp = ftplib.FTP('ftp.patricbrc.org')\n        ftp.login()\n        ftp.cwd(\"/patric2/patric3/genomes/{0}/\".format(patric_id))\n        with open(outfile, \"wb\") as gFile:\n            ftp.retrbinary('RETR {0}.PATRIC.{1}'.format(patric_id, extension), gFile.write)\n        ftp.quit()\n    # TODO: check exceptions\n    except:\n        return None\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a protein - specific structure as a protein - specific structure.", "response": "def from_structure(cls, original, filter_residues):\n        \"\"\"\n        Loads structure as a protein, exposing\n        protein-specific methods.\n        \"\"\"\n        P = cls(original.id)\n        P.full_id = original.full_id\n        \n        for child in original.child_dict.values():\n            copycat = deepcopy(child)\n            P.add(copycat)\n        \n        # Discriminate non-residues (is_aa function)\n        remove_list = []\n        if filter_residues:\n            for model in P:\n                for chain in model:\n                    for residue in chain:\n                        if residue.get_id()[0] != ' ' or not is_aa(residue):\n                            remove_list.append(residue)\n            \n            for residue in remove_list:\n                residue.parent.detach_child(residue.id)\n            \n            for chain in P.get_chains(): # Remove empty chains\n                if not len(chain.child_list):\n                    model.detach_child(chain.id)\n                    \n        P.header = deepcopy(original.header)\n        P.xtra = deepcopy(original.xtra)\n        \n        return P"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_ss_bonds(self, threshold=3.0):\n        \n        # Taken from http://docs.python.org/library/itertools.html\n        # Python 2.4 does not include itertools.combinations\n\n        def combinations(iterable, r):\n            # combinations('ABCD', 2) --> AB AC AD BC BD CD\n            # combinations(range(4), 3) --> 012 013 023 123\n            pool = tuple(iterable)\n            n = len(pool)\n            if r > n:\n                return\n            indices = range(r)\n            yield tuple(pool[i] for i in indices)\n            while True:\n                for i in reversed(range(r)):\n                    if indices[i] != i + n - r:\n                        break\n                else:\n                    return\n                indices[i] += 1\n                for j in range(i+1, r):\n                    indices[j] = indices[j-1] + 1\n                yield tuple(pool[i] for i in indices)\n\n        \n        \n        model = self.child_list[0]\n        cysteines = [r for r in model.get_residues() if r.get_resname() == 'CYS']\n\n        pairs = combinations(cysteines, 2) # Iterator with pairs\n\n        for cys_pair in pairs:\n            if cys_pair[0]['SG'] - cys_pair[1]['SG'] < threshold:\n                yield cys_pair", "response": "Searches the S - S bonds of the structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck for missing atoms based on a template.", "response": "def check_missing_atoms(self, template=None, ha_only=True):\n        \"\"\"\n        Checks for missing atoms based on a template.\n        Default: Searches for missing heavy atoms (not Hydrogen) based on Bio.Struct.protein_residues\n        \n        Arguments:\n          - template, dictionary, keys are residue names, values list of atom names.\n          - ha_only, boolean, default True, restrict check to heavy atoms.\n          \n        Returns a dictionary of tuples with the missing atoms per residue.\n        \"\"\"\n        \n        missing_atoms = {}\n        \n        if not template:\n            import protein_residues\n            template = protein_residues.normal # Don't care for terminal residues here..\n            \n        for residue in self.get_residues():\n            \n            if not template.has_key(residue.resname):\n                # Maybe add this as a warning instead of exception?\n                raise ValueError('Residue name (%s) not in the template' %residue.resname )\n            \n            if ha_only:\n                heavy_atoms = [ atom for atom in template[residue.resname]['atoms'].keys() \n                                if atom[0] != 'H' and not (atom[0].isdigit() and atom[1] == 'H')]\n                reference_set = set(heavy_atoms)\n            else:\n                reference_set = set(template[residue.resname]['atoms'].keys())\n            \n            structure_set = set(residue.child_dict.keys())\n            \n            diff = reference_set.difference(structure_set)\n            \n            if diff:\n                residue_uniq_id = (residue.parent.id, residue.resname, residue.get_id()[1]) # Chain, Name, Number\n                missing_atoms[residue_uniq_id] = list(diff)\n        \n        return missing_atoms"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreduce the protein structure complexity to a few (pseudo-)atoms per residue. Parameters: - cg_type: CA_TRACE (Ca-only) [Default] ENCAD_3P (CA, O, SC Beads) MARTINI (CA, O, SC Beads) Returns a new structure object.", "response": "def coarse_grain(self, cg_type=\"CA_TRACE\"):\n        \"\"\" \n            Reduces the protein structure complexity to a few (pseudo-)atoms per residue.\n\n            Parameters:\n              - cg_type:      CA_TRACE (Ca-only) [Default]\n                              ENCAD_3P (CA, O, SC Beads)\n                              MARTINI (CA, O, SC Beads)\n            \n            Returns a new structure object.\n        \"\"\"\n        \n        # Import CG Types\n        \n        import CG_Models\n        \n        CG_Library = {  \"CA_TRACE\": CG_Models.CA_TRACE,\n                        \"ENCAD_3P\": CG_Models.ENCAD_3P,\n                        \"MARTINI\":  CG_Models.MARTINI   }\n        \n        CG_Method = CG_Library[cg_type]\n        \n        # Creates a brand new structure object\n        from Bio.PDB.StructureBuilder import StructureBuilder\n        \n        structure_builder=StructureBuilder()\n        \n        cg_id = \"CG_\" + self.id\n        structure_builder.init_structure(cg_id)\n        structure_builder.init_seg(' ') # Empty SEGID\n\n        for model in self:\n            structure_builder.init_model(model.id)\n            \n            for chain in model:\n                structure_builder.init_chain(chain.id)\n                cur_chain = structure_builder.chain\n\n                for residue in chain:\n                    cg_residue = CG_Method(residue)\n                    cur_chain.add(cg_residue)\n        \n        cg_structure = structure_builder.get_structure()\n        \n        return cg_structure"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all the sequences that share the target protein and chain.", "response": "def find_seq_homologues(self, return_raw=False):\n        \"\"\"\n        Uses NCBI BLAST to look for structures deposited in the PDB database\n        that share __sequence__ homology with the target protein/chain.\n        Bridges to Bio.BLAST.NCBIWWW.\n        \"\"\"\n        \n        \n        # Get sequence from structure/chain\n        # We could use Bio.PDB.Polypeptide?\n        \n        from Bio.SCOP.Raf import to_one_letter_code\n        \n        s = self\n        seq_iter = s.get_residues()\n        seq_str = ''\n\n        for aa in seq_iter:\n            if aa.resname in to_one_letter_code:\n                seq_str += to_one_letter_code[aa.resname]\n        \n        # Use BLAST to find homologous sequences with associated\n        # structures in the PDB database.\n        # Perhaps include local BLAST?\n        \n        from Bio.Blast.NCBIWWW import qblast\n\n        # Adapt for short query sequences if needed\n        # From http://www.ncbi.nlm.nih.gov/blast/producttable.shtml#shortp\n        \n        if len(seq_str) < 15:\n            word_size = 2\n            expect = 20000\n            matrix_name = 'PAM30'\n            filter = None\n        else:\n            word_size = 3\n            expect = 10.0\n            matrix_name = 'BLOSUM62'\n            filter = 'SEG'\n             \n        query_result = qblast(  \"blastp\", \"pdb\", seq_str, \n                                word_size, expect, matrix_name, filter)\n        \n        if return_raw:\n           return query_result \n            \n        # Parse BLAST result to yield results\n        # PDBID : (E-Value, Identity, Positives, Gaps, Alignment)\n        \n        from Bio.Blast import NCBIXML\n        \n        blast_records = NCBIXML.read(query_result)\n        \n        results = []\n        \n        for alignment in blast_records.alignments:\n            for hsps in alignment.hsps:\n                id_perc = \"%s/%s\" %(hsps.identities, alignment.length)\n                pos_perc = \"%s/%s\" %(hsps.positives, alignment.length)\n                gaps_perc = \"%s/%s\" %(hsps.gaps, alignment.length)\n\n                pdb_id = alignment.title.split('|')[3]\n                e_value = hsps.expect\n                \n                results.append( ( pdb_id,\n                                \"%2.5e\" %e_value,\n                                id_perc, \n                                pos_perc, \n                                gaps_perc,\n                                '\\n'.join([hsps.query, hsps.match, hsps.sbjct]) ) )\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the AMYLPRED2 web server for a protein sequence and get the consensus result for aggregation propensity.", "response": "def get_aggregation_propensity(self, seq, outdir, cutoff_v=5, cutoff_n=5, run_amylmuts=False):\n        \"\"\"Run the AMYLPRED2 web server for a protein sequence and get the consensus result for aggregation propensity.\n\n        Args:\n            seq (str, Seq, SeqRecord): Amino acid sequence\n            outdir (str): Directory to where output files should be saved\n            cutoff_v (int): The minimal number of methods that agree on a residue being a aggregation-prone residue\n            cutoff_n (int): The minimal number of consecutive residues to be considered as a 'stretch' of \n                aggregation-prone region\n            run_amylmuts (bool): If AMYLMUTS method should be run, default False. AMYLMUTS is optional as it is the most\n                time consuming and generates a slightly different result every submission.\n\n        Returns:\n            int: Aggregation propensity - the number of aggregation-prone segments on an unfolded protein sequence\n\n        \"\"\"\n\n        seq = ssbio.protein.sequence.utils.cast_to_str(seq)\n\n        results = self.run_amylpred2(seq=seq, outdir=outdir, run_amylmuts=run_amylmuts)\n        agg_index, agg_conf = self.parse_for_consensus_aggregation(N=len(seq), results=results, cutoff_v=cutoff_v, cutoff_n=cutoff_n)\n\n        return agg_index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_amylpred2(self, seq, outdir, run_amylmuts=False):\n        outdir_amylpred = op.join(outdir, 'AMYLPRED2_results')\n        if not op.exists(outdir_amylpred):\n            os.mkdir(outdir_amylpred)\n\n        url = \"http://aias.biol.uoa.gr/AMYLPRED2/login.php\"\n        cj = CookieJar()\n        opener = build_opener(HTTPCookieProcessor(cj))\n        formdata = {\"email\": self.email, \"password\": self.password}\n        data_encoded = urlencode(formdata)\n        data_encoded = data_encoded.encode('ASCII')\n        response = opener.open(url, data_encoded)\n\n        # AMYLMUTS is most time consuming and generates a slightly different result every submission\n        methods = ['AGGRESCAN', 'NETCSSP', 'PAFIG', 'APD', 'AMYLPATTERN',\n                   'SECSTR', 'BSC', 'WALTZ', 'CONFENERGY', 'TANGO']\n\n        if run_amylmuts:\n            methods.append('AMYLMUTS')\n\n        output = {}\n        timeCounts = 0\n\n        for met in methods:\n            # First check if there is an existing results file\n            existing_results = glob.glob(op.join(outdir_amylpred, '*_{}.txt'.format(met)))\n            if existing_results:\n                results_file = existing_results[0]\n            else:\n                values = {'seq_data': seq, 'method': met}\n                data = urlencode(values)\n                data = data.encode('ASCII')\n                url_input = \"http://aias.biol.uoa.gr/cgi-bin/AMYLPRED2/amylpred2.pl\"\n                response = opener.open(url_input, data)\n                result = str(response.read())\n                ind = str.find(result, 'Job ID')\n                result2 = result[ind:ind + 50]\n                ind1 = str.find(result2, ':')\n                ind2 = str.find(result2, '<BR>')\n                job_id = result2[ind1 + 2:ind2]\n\n                # Waiting for the calculation to complete\n                url_result = 'http://aias.biol.uoa.gr/AMYLPRED2/tmp/' + job_id + '.txt'\n                print(url_result)\n                print(\"Waiting for %s results\" % met, end='.')\n                while True:\n                    result = urlopen(url_result).read()\n                    if not result:\n                        time.sleep(1)\n                        timeCounts += 1\n                        print('.', end='')\n                    else:\n                        response = requests.get(url_result)\n                        break\n                results_file = op.join(outdir_amylpred, \"{}_{}.txt\".format(url_result.split('/')[-1].strip('.txt'), met))\n                with open(results_file, \"wb\") as handle:\n                    for data in response.iter_content():\n                        handle.write(data)\n                print(\"\")\n\n            method, hits = self.parse_method_results(results_file, met)\n            # if method.lower() == met.lower():\n            output[met] = hits\n            # elif method == 'Beta-strand contiguity' and met == 'BSC':\n            # output[met]=hits\n            # elif method == 'Hexapeptide Conf. Energy' and met == 'CONFENERGY':\n        if timeCounts != 0:\n            print(\"Time spent: %d seconds\" % timeCounts)\n        return output", "response": "Run all methods on the AMYLPRED2 web server for a amino acid sequence and gather results."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the output of a AMYLPRED2 method result file.", "response": "def parse_method_results(self, results_file, met):\n        \"\"\"Parse the output of a AMYLPRED2 result file.\"\"\"\n\n        result = str(open(results_file).read())\n        ind_s = str.find(result, 'HITS')\n        ind_e = str.find(result, '**NOTE')\n        tmp = result[ind_s + 10:ind_e].strip(\" \")\n        hits_resid = []\n        method = None\n        if \":\" in tmp:\n            method = tmp.split(\":\")[0]\n            hits = tmp.split(\":\")[1]\n            if \"-\" in hits:\n                for ele in hits.split(\",\"):\n                    ele = ele.replace('\\\\r\\\\n\\\\r\\\\n', '')\n                    res_s = ele.split(\"-\")[0]\n                    res_e = ele.split(\"-\")[1]\n                    for i in range(int(res_s), int(res_e) + 1):\n                        hits_resid.append(i)\n        if method:\n            return method, hits_resid\n        else:\n            return met, hits_resid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a video into a numpy array", "response": "def _load_video(video_filename):\n    \"\"\"Load a video into a numpy array\"\"\"\n    video_filename = str(video_filename)\n    print(\"Loading \" + video_filename)\n    if not os.path.isfile(video_filename):\n        raise Exception(\"File Not Found: %s\" % video_filename)\n    # noinspection PyArgumentList\n    capture = cv2.VideoCapture(video_filename)\n    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    width, height = get_capture_dimensions(capture)\n    fps = int(capture.get(cv2.CAP_PROP_FPS))\n    x = 0\n    vid_frames = numpy.zeros((frame_count, height, width, 3), dtype='uint8')\n    while capture.isOpened():\n        ret, frame = capture.read()\n        if not ret:\n            break\n\n        vid_frames[x] = frame\n        x += 1\n    capture.release()\n\n    return vid_frames, fps"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_capture_dimensions(capture):\n    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    return width, height", "response": "Get the dimensions of a capture"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving a video to disk", "response": "def save_video(video, fps, save_filename='media/output.avi'):\n    \"\"\"Save a video to disk\"\"\"\n    # fourcc = cv2.CAP_PROP_FOURCC('M', 'J', 'P', 'G')\n    print(save_filename)\n    video = float_to_uint8(video)\n    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n    writer = cv2.VideoWriter(save_filename, fourcc, fps, (video.shape[2], video.shape[1]), 1)\n    for x in range(0, video.shape[0]):\n        res = cv2.convertScaleAbs(video[x])\n        writer.write(res)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_frequencies(vid_data, fps, bounds=None):\n    averages = []\n\n    if bounds:\n        for x in range(1, vid_data.shape[0] - 1):\n            averages.append(vid_data[x, bounds[2]:bounds[3], bounds[0]:bounds[1], :].sum())\n    else:\n        for x in range(1, vid_data.shape[0] - 1):\n            averages.append(vid_data[x, :, :, :].sum())\n\n    averages = averages - min(averages)\n\n    charts_x = 1\n    charts_y = 2\n    pyplot.figure(figsize=(20, 10))\n    pyplot.subplots_adjust(hspace=.7)\n\n    pyplot.subplot(charts_y, charts_x, 1)\n    pyplot.title(\"Pixel Average\")\n    pyplot.xlabel(\"Time\")\n    pyplot.ylabel(\"Brightness\")\n    pyplot.plot(averages)\n\n    freqs = scipy.fftpack.fftfreq(len(averages), d=1.0 / fps)\n    fft = abs(scipy.fftpack.fft(averages))\n    idx = np.argsort(freqs)\n\n    pyplot.subplot(charts_y, charts_x, 2)\n    pyplot.title(\"FFT\")\n    pyplot.xlabel(\"Freq (Hz)\")\n    freqs = freqs[idx]\n    fft = fft[idx]\n\n    freqs = freqs[len(freqs) // 2 + 1:]\n    fft = fft[len(fft) // 2 + 1:]\n    pyplot.plot(freqs, abs(fft))\n\n    pyplot.show()", "response": "Graph the average value of the video as well as the frequency strength"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a gaussian representation of a video", "response": "def gaussian_video(video, shrink_multiple):\n    \"\"\"Create a gaussian representation of a video\"\"\"\n    vid_data = None\n    for x in range(0, video.shape[0]):\n        frame = video[x]\n        gauss_copy = np.ndarray(shape=frame.shape, dtype=\"float\")\n        gauss_copy[:] = frame\n        for i in range(shrink_multiple):\n            gauss_copy = cv2.pyrDown(gauss_copy)\n\n        if x == 0:\n            vid_data = np.zeros((video.shape[0], gauss_copy.shape[0], gauss_copy.shape[1], 3))\n        vid_data[x] = gauss_copy\n    return vid_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef combine_pyramid_and_save(g_video, orig_video, enlarge_multiple, fps, save_filename='media/output.avi'):\n    width, height = get_frame_dimensions(orig_video[0])\n    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n    print(\"Outputting to %s\" % save_filename)\n    writer = cv2.VideoWriter(save_filename, fourcc, fps, (width, height), 1)\n    for x in range(0, g_video.shape[0]):\n        img = np.ndarray(shape=g_video[x].shape, dtype='float')\n        img[:] = g_video[x]\n        for i in range(enlarge_multiple):\n            img = cv2.pyrUp(img)\n\n        img[:height, :width] = img[:height, :width] + orig_video[x]\n        res = cv2.convertScaleAbs(img[:height, :width])\n        writer.write(res)", "response": "Combine a gaussian video representation with the original and save to file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of messages by given session id.", "response": "def messages(self, uid=0, **kwargs):\n        \"\"\"\n        Fetch messages by given session id.\n        An useful synonym for \"messages/search\" command with provided `sessionId` parameter.\n        Returns a list of :class:`Message` objects and a pager dict.\n\n        :Example:\n\n        messages = client.sessions.messages(1901001)\n\n        :param int uid:   The unique id of the Session. Required.\n        :param int page:  Fetch specified results page. Default=1\n        :param int limit: How many results on page. Default=10\n        \"\"\"\n        messages = Messages(self.base_uri, self.auth)\n        return self.get_subresource_instances(uid, instance=messages,\n                                              resource=\"messages\", params=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of messages and a pager dict.", "response": "def list(self, search=False, **kwargs):\n        \"\"\"\n        Returns a list of :class:`Message` objects and a pager dict.\n\n        :Example:\n\n        messages, pager = client.messages.list()\n\n        :param bool search:     If True then search messages using `ids`, `sessionId`, and/or `query`. Default=False\n        :param int  page:       Fetch specified results page. Default=1\n        :param int  limit:      How many results on page. Default=10\n        :param str  ids:        Find message by ID(s). Using with `search`=True.\n        :param str  sessionId:  Find messages by session ID. Using with `search`=True.\n        :param str  query:      Find messages by specified search query. Using with `search`=True.\n        \"\"\"\n        kwargs[\"search\"] = search\n        return self.get_instances(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, from_=None, **kwargs):\n\n        if \"dummy\" in kwargs and kwargs[\"dummy\"]:\n            response, instance = self.request(\"POST\", self.uri, data=kwargs)\n            return instance\n        if from_:\n            kwargs[\"from\"] = from_\n        return self.create_instance(kwargs)", "response": "Creates a new outbound message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef price(self, from_=None, **kwargs):\n        if from_:\n            kwargs[\"from\"] = from_\n        uri = \"%s/%s\" % (self.uri, \"price\")\n        response, instance = self.request(\"GET\", uri, params=kwargs)\n        return instance", "response": "Check pricing for a new outbound message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new Token by given username and password.", "response": "def create(self, username, password):\n        \"\"\"\n        Create Token by given username and password.\n        Authenticate user by given username and password.\n        Returning a :class:`Token` object contains username and token that you should pass to the all requests\n        (in X-TM-Username and X-TM-Key, respectively).\n\n        :Example:\n\n        token = client.tokens.create(username=\"my_username\", password=\"my_password\")\n\n        :param username: Account username or email. Required.\n        :param password: Account password. Required.\n        \"\"\"\n        data = dict(username=username, password=password)\n        response, instance = self.request(\"POST\", self.uri, data=data)\n        return self.load_instance(instance)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrefresh access token. Only non-expired tokens can be renewed. :Example: token = client.tokens.refresh()", "response": "def refresh(self):\n        \"\"\"\n        Refresh access token. Only non-expired tokens can be renewed.\n\n        :Example:\n\n        token = client.tokens.refresh()\n        \"\"\"\n        uri = \"%s/%s\" % (self.uri, \"refresh\")\n        response, instance = self.request(\"GET\", uri)\n        return response.ok"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self):\n        response, instance = self.request(\"GET\", self.uri)\n        return self.load_instance(instance)", "response": "Get current user info."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, **kwargs):\n        response, instance = self.request(\"PUT\", self.uri, data=kwargs)\n        return response.status == 201", "response": "Update an existing User s attributes. Returns True if success."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_invite(self, **kwargs):\n        resp, _ = self.request(\"POST\", self.uri, data=kwargs)\n        return resp.status == 204", "response": "Invite new subaccount. Returns True if success."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch messages from chat with specified phone number.", "response": "def by_phone(self, phone, **kwargs):\n        \"\"\"\n        Fetch messages from chat with specified phone number.\n\n        :Example:\n\n        chat = client.chats.by_phone(phone=\"447624800500\")\n\n        :param str phone: Phone number in E.164 format.\n        :param int page:  Fetch specified results page. Default=1\n        :param int limit: How many results on page. Default=10\n        \"\"\"\n        chat_messages = ChatMessages(self.base_uri, self.auth)\n        return self.get_subresource_instances(uid=phone, instance=chat_messages, params=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all messaging stats.", "response": "def list(self, **kwargs):\n        \"\"\"\n        Returns a list of :class:`MessagingStat` objects (messaging statistics).\n\n        :Example:\n\n        stats = client.stats_messaging.list()\n\n        :param str by:    Group results by specified period: `off`, `day`, `month` or `year`. Default is `off`.\n        :param str start: Start date in unix timestamp format. Default is 7 days ago.\n        :param str end:   End date in unix timestamp format. Default is now.\n        \"\"\"\n        response, instances = self.request(\"GET\", self.uri, params=kwargs)\n        return [self.load_instance(r) for r in instances]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the TextMagic credentials from the current environment", "response": "def get_credentials(env=None):\n    \"\"\"\n    Gets the TextMagic credentials from current environment\n\n    :param env: environment\n    :return: username, token\n    \"\"\"\n    environ = env or os.environ\n    try:\n        username = environ[\"TEXTMAGIC_USERNAME\"]\n        token = environ[\"TEXTMAGIC_AUTH_TOKEN\"]\n        return username, token\n    except KeyError:\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lists(self, uid=0, **kwargs):\n        lists = Lists(self.base_uri, self.auth)\n        return self.get_subresource_instances(uid, instance=lists,\n                                              resource=\"lists\", params=kwargs)", "response": "Returns a list of lists which Contact belongs to and a pager dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contacts(self, uid=0, **kwargs):\n        contacts = Contacts(self.base_uri, self.auth)\n        return self.get_subresource_instances(uid, instance=contacts,\n                                              resource=\"contacts\", params=kwargs)", "response": "Fetches user contacts by given group id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put_contacts(self, uid, **kwargs):\n        return self.update_subresource_instance(uid,\n                                                body=kwargs,\n                                                subresource=None,\n                                                slug=\"contacts\")", "response": "Assign contacts to the specified list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting contacts from the specified list.", "response": "def delete_contacts(self, uid, **kwargs):\n        \"\"\"\n        Unassign contacts from the specified list.\n        If contacts assign only to the specified list, then delete permanently.\n        Returns True if success.\n\n        :Example:\n\n        client.lists.delete_contacts(uid=1901010, contacts=\"1723812,1239912\")\n\n        :param int uid:      The unique id of the List. Required.\n        :param str contacts: Contact ID(s), separated by comma. Required.\n        \"\"\"\n        uri = \"%s/%s/contacts\" % (self.uri, uid)\n        response, instance = self.request(\"DELETE\", uri, data=kwargs)\n        return response.status == 204"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef available(self, **kwargs):\n        uri = \"%s/%s\" % (self.uri, \"available\")\n        response, instance = self.request(\"GET\", uri, params=kwargs)\n        return instance", "response": "Find available dedicated numbers to buy. Returns dictionary like this :"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all available sender settings which could be used in POST messages method.", "response": "def allowed(self, **kwargs):\n        \"\"\"\n        Get all available sender settings which could be used in \"from\" parameter of POST messages method.\n        Returns :class:`Source` object.\n\n        :Example:\n\n        allowed = client.sources.allowed()\n\n        :param country:  Return sender settings available in specified country only. Optional.\n        \"\"\"\n        resp, instance = self.request(\"GET\", self.uri, params=kwargs)\n        return self.load_instance(instance)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cert_file():\n    try:\n        current_path = os.path.realpath(__file__)\n        ca_cert_path = os.path.join(current_path, \"..\", \"..\", \"..\",\n                                    \"conf\", \"cacert.pem\")\n        return os.path.abspath(ca_cert_path)\n    except Exception:\n        return None", "response": "Get the certificates file for https"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a HTTP request to the specified URL.", "response": "def make_request(method, url, params=None, data=None, headers=None, auth=None):\n    \"\"\"\n    Make an HTTP request.\n\n    :param str   method:  \"POST\", \"GET\", \"PUT\" or \"DELETE\"\n    :param str   url:     URL to process request.\n    :param dict  params:  Params to add to the URL.\n    :param dict  data:    Form data.\n    :param dict  headers: Headers to request.\n    :param tuple auth:    Username and token.\n    \"\"\"\n\n    http = httplib2.Http(\n        ca_certs=get_cert_file(),\n    )\n\n    if auth is not None:\n        http.add_credentials(auth[0], auth[1])\n\n    if params is not None:\n        enc_params = urlencode(params, doseq=True)\n        if urlparse(url).query:\n            url = '%s&%s' % (url, enc_params)\n        else:\n            url = '%s?%s' % (url, enc_params)\n        params[\"_format\"] = \"json\"\n\n    def encode_atom(atom):\n        if isinstance(atom, (integer_types, binary_type)):\n            return atom\n        elif isinstance(atom, string_types):\n            return atom.encode('utf-8')\n        else:\n            raise ValueError('list elements should be an integer, '\n                             'binary, or string')\n\n    if data is not None:\n        udata = {}\n        for k, v in iteritems(data):\n            key = k.encode('utf-8')\n            if isinstance(v, (list, tuple, set)):\n                udata[key] = [encode_atom(x) for x in v]\n            elif isinstance(v, (integer_types, binary_type, string_types)):\n                udata[key] = encode_atom(v)\n            else:\n                raise ValueError('data should be an integer, '\n                                 'binary, or string, or sequence ')\n        data = udata\n        data = urlencode(udata, doseq=True)\n\n    response, content = http.request(uri=url, method=method,\n                                     headers=headers, body=data)\n\n    return Response(response, content, url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_tm_request(method, uri, **kwargs):\n    headers = kwargs.get(\"headers\", {})\n\n    user_agent = \"textmagic-python/%s (Python %s)\" % (\n        __version__,\n        platform.python_version()\n    )\n    headers[\"User-agent\"] = user_agent\n    headers[\"Accept-Charset\"] = \"utf-8\"\n    if \"Accept-Language\" not in headers:\n        headers[\"Accept-Language\"] = \"en-us\"\n\n    if (method == \"POST\" or method == \"PUT\") and \"Content-Type\" not in headers:\n        headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n\n    kwargs[\"headers\"] = headers\n\n    if \"Accept\" not in headers:\n        headers[\"Accept\"] = \"application/json\"\n\n    headers[\"X-TM-Username\"], headers[\"X-TM-Key\"] = kwargs[\"auth\"]\n\n    response = make_request(method, uri, **kwargs)\n\n    if not response.ok:\n        try:\n            resp_body = json.loads(response.content)\n            message = resp_body[\"message\"]\n            errors = resp_body[\"errors\"]\n        except:\n            message = response.content\n            errors = None\n\n        raise TextmagicRestException(status=response.status, method=method, uri=response.url,\n                                     msg=message, errors=errors)\n\n    return response", "response": "Make a request to TextMagic REST APIv2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend an HTTP request to the TextMagic API.", "response": "def request(self, method, uri, **kwargs):\n        \"\"\"\n        Send an HTTP request.\n\n        :param str method: \"POST\", \"GET\", \"PUT\" or \"DELETE\"\n        :param str uri:    URI to process request.\n        :raises :class:`TextmagicRestException`\n        \"\"\"\n\n        response = make_tm_request(method, uri, auth=self.auth, **kwargs)\n\n        if method == \"DELETE\" or response.status == 204:\n            return response, {}\n        else:\n            return response, json.loads(response.content.decode(\"utf-8\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate an existing resource with the given uid.", "response": "def update_instance(self, uid, body):\n        \"\"\"\n        Update an Model via a PUT request\n\n        :param str  uid:  String identifier for the list resource\n        :param dict body: Dictionary of items to PUT\n        \"\"\"\n        uri = \"%s/%s\" % (self.uri, uid)\n        response, instance = self.request(\"PUT\", uri, data=body)\n        return self.load_instance(instance)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_instance(self, uid):\n        uri = \"%s/%s\" % (self.uri, uid)\n        response, instance = self.request(\"DELETE\", uri)\n        return response.status == 204", "response": "Delete an objectModel via a DELETE request"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_value(self, uid, **kwargs):\n        contacts = Contacts(self.base_uri, self.auth)\n        return self.update_subresource_instance(uid, body=kwargs,\n                                                subresource=contacts,\n                                                slug=\"update\")", "response": "Updates the value of a custom field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ping(self):\n        uri = \"%s/%s\" % (self.base_uri, \"ping\")\n        resp, data = self.request(\"GET\", uri)\n        return data", "response": "Ping the server for a set of unique identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshare bounds checking and buffer creation.", "response": "def _get_buffer(self, index):\n        \"\"\"Shared bounds checking and buffer creation.\"\"\"\n        if not 0 <= index < self.count:\n            raise IndexError()\n        size = struct.calcsize(self.format)\n        # We create the buffer every time instead of keeping the buffer (which is 32 bytes at least)\n        # around forever.\n        buf = bytearray(size + 1)\n        buf[0] = self.first_register + size * index\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modelserializer_factory(model, serializer=serializers.ModelSerializer, fields=None, exclude=None, **kwargs):\n\n    # TODO: decide if we need cache and what to do with parameters tha can be different\n    serializer_class = serializer_cache.get(model, None)\n\n    if serializer_class:\n        return serializer_class\n\n    def _get_declared_fields(attrs):\n        fields = [(field_name, attrs.pop(field_name))\n                  for field_name, obj in list(attrs.items())\n                  if isinstance(obj, serializers.Field)]\n        fields.sort(key=lambda x: x[1]._creation_counter)\n        return OrderedDict(fields)\n\n    meta_attrs = {'model': model}\n    if fields is not None:\n        meta_attrs['fields'] = fields\n    if exclude is not None:\n        meta_attrs['exclude'] = exclude\n    if fields is None and exclude is None:\n        meta_attrs['fields'] = '__all__'\n\n    parent = (object, )\n    Meta = type(str('Meta'), parent, meta_attrs)\n    class_name = model.__name__ + str('Serializer')\n\n    serializer_class_attrs = {\n        'Meta': Meta,\n        '_get_declared_fields': _get_declared_fields(kwargs),\n    }\n    serializer_class = type(serializer)(class_name, (serializer,), serializer_class_attrs)\n    serializer_cache[model] = serializer_class\n    return serializer_class", "response": "Generate serializer for a model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget serializer for a node.", "response": "def get_serializer(instance, plugin=None, model=None, *args, **kwargs):\n    \"\"\"\n    :param instance: model instance or queryset\n    :param plugin: plugin instance that is used to get serializer for\n    :param model: plugin model we build serializer for\n    :param kwargs: kwargs like many and other\n    :return:\n    \"\"\"\n    serializer_class = get_serializer_class(plugin=plugin, model=model)\n    if 'read_only' not in kwargs:\n        kwargs['read_only'] = True\n    return serializer_class(instance, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_inlines(self, obj):\n        plugin = obj.get_plugin_class()\n        inlines = getattr(plugin, 'inlines', [])\n        data = {}\n        for inline in inlines:\n            for related_object in obj._meta.related_objects:\n                if getattr(related_object, 'related_model', None) == inline.model:\n                    name = related_object.name\n                    serializer = get_serializer(\n                        getattr(obj, name).all(), model=inline.model, many=True, context=self.context)\n                    data[name] = serializer.data\n                    break\n        return data", "response": "This method returns data for all inlines listed in inlines parameter of plugin\n        prepare and return together with parent object\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_children(self, obj):\n        data = []\n        plugin = obj.get_plugin_class()\n        if not(getattr(plugin, 'allow_children', False) and getattr(plugin, 'child_classes', None)):\n            return data\n        children = obj.get_descendants().order_by('placeholder', 'path')\n        children = [obj] + list(children)\n        children = list(downcast_plugins(children))\n        children[0].parent_id = None\n        children = list(build_plugin_tree(children))\n\n        def get_plugin_data(child_plugin):\n            serializer = get_serializer(child_plugin, model=child_plugin._meta.model, context=self.context)\n            plugin_data = serializer.data\n            plugin_data['inlines'] = self.get_inlines(child_plugin)\n            if child_plugin.child_plugin_instances:\n                plugin_data['children'] = []\n                for plug in child_plugin.child_plugin_instances:\n                    plugin_data['children'].append(get_plugin_data(plug))\n            return plugin_data\n\n        for child in children[0].child_plugin_instances or []:\n            data.append(get_plugin_data(child))\n        return data", "response": "This method returns the children of the object and all its descendants."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nunzip file located at src_path into dest_path", "response": "def _unzip_file(self, src_path, dest_path, filename):\n        \"\"\"unzips file located at src_path into destination_path\"\"\"\n        self.logger.info(\"unzipping file...\")\n\n        # construct full path (including file name) for unzipping\n        unzip_path = os.path.join(dest_path, filename)\n        utils.ensure_directory_exists(unzip_path)\n\n        # extract data\n        with zipfile.ZipFile(src_path, \"r\") as z:\n            z.extractall(unzip_path)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch url of the current dataset.", "response": "def get_dataset_url(self, tournament=1):\n        \"\"\"Fetch url of the current dataset.\n\n        Args:\n            tournament (int, optional): ID of the tournament, defaults to 1\n\n        Returns:\n            str: url of the current dataset\n\n        Example:\n            >>> NumerAPI().get_dataset_url()\n            https://numerai-datasets.s3.amazonaws.com/t1/104/numerai_datasets.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYNVLTPMU6QILOHA%2F20180424%2Fus-west-1%2Fs3%2Faws4_request&X-Amz-Date=20180424T084911Z&X-Amz-Expires=900&X-Amz-SignedHeaders=host&X-Amz-Signature=83863db44689c9907da6d3c8ac28160cd5e2d17aa90f12c7eee6811810e4b8d3\n        \"\"\"\n        query = \"\"\"\n            query($tournament: Int!) {\n                dataset(tournament: $tournament)\n            }\"\"\"\n        arguments = {'tournament': tournament}\n        url = self.raw_query(query, arguments)['data']['dataset']\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download_current_dataset(self, dest_path=\".\", dest_filename=None,\n                                 unzip=True, tournament=1):\n        \"\"\"Download dataset for the current active round.\n\n        Args:\n            dest_path (str, optional): destination folder, defaults to `.`\n            dest_filename (str, optional): desired filename of dataset file,\n                defaults to `numerai_dataset_<round number>.zip`\n            unzip (bool, optional): indication of whether the training data\n                should be unzipped, defaults to `True`\n            tournament (int, optional): ID of the tournament, defaults to 1\n\n        Returns:\n            str: Path to the downloaded dataset\n\n        Example:\n            >>> NumerAPI().download_current_dataset()\n            ./numerai_dataset_104.zip\n        \"\"\"\n        # set up download path\n        if dest_filename is None:\n            round_number = self.get_current_round()\n            dest_filename = \"numerai_dataset_{0}.zip\".format(round_number)\n        else:\n            # ensure it ends with \".zip\"\n            if unzip and not dest_filename.endswith(\".zip\"):\n                dest_filename += \".zip\"\n        dataset_path = os.path.join(dest_path, dest_filename)\n\n        if os.path.exists(dataset_path):\n            self.logger.info(\"target file already exists\")\n            return dataset_path\n\n        # create parent folder if necessary\n        utils.ensure_directory_exists(dest_path)\n\n        url = self.get_dataset_url(tournament)\n        utils.download_file(url, dataset_path, self.show_progress_bars)\n\n        # unzip dataset\n        if unzip:\n            # remove the \".zip\" in the end\n            dataset_name = dest_filename[:-4]\n            self._unzip_file(dataset_path, dest_path, dataset_name)\n\n        return dataset_path", "response": "Download the current dataset for the current active round."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raw_query(self, query, variables=None, authorization=False):\n        body = {'query': query,\n                'variables': variables}\n        headers = {'Content-type': 'application/json',\n                   'Accept': 'application/json'}\n        if authorization:\n            if self.token:\n                public_id, secret_key = self.token\n                headers['Authorization'] = \\\n                    'Token {}${}'.format(public_id, secret_key)\n            else:\n                raise ValueError(\"API keys required for this action.\")\n\n        result = utils.post_with_err_handling(\n            API_TOURNAMENT_URL, body, headers)\n\n        if result and \"errors\" in result:\n            err = self._handle_call_error(result['errors'])\n            # fail!\n            raise ValueError(err)\n\n        return result", "response": "Send a raw request to the Numerai s GraphQL API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_leaderboard(self, round_num=0, tournament=1):\n        msg = \"getting leaderboard for tournament {} round {}\"\n        self.logger.info(msg.format(tournament, round_num))\n        query = '''\n            query($number: Int!\n                  $tournament: Int!) {\n              rounds(number: $number\n                     tournament: $tournament) {\n                leaderboard {\n                  consistency\n                  concordance {\n                    pending\n                    value\n                  }\n                  liveLogloss\n                  liveAuroc\n                  submissionId\n                  username\n                  validationLogloss\n                  validationAuroc\n                  paymentGeneral {\n                    nmrAmount\n                    usdAmount\n                  }\n                  paymentStaking {\n                    nmrAmount\n                    usdAmount\n                  }\n                  stakeResolution {\n                    destroyed\n                    paid\n                    successful\n                  }\n                }\n              }\n            }\n        '''\n        arguments = {'number': round_num, 'tournament': tournament}\n        result = self.raw_query(query, arguments)['data']['rounds'][0]\n        # happens for non-existent tournament IDs\n        if result is None:\n            return None\n        leaderboard = result['leaderboard']\n        # parse to correct data types\n        for item in leaderboard:\n            for p in [\"paymentGeneral\", \"paymentStaking\"]:\n                utils.replace(item[p], \"nmrAmount\", utils.parse_float_string)\n                utils.replace(item[p], \"usdAmount\", utils.parse_float_string)\n            utils.replace(item['stakeResolution'], \"paid\",\n                          utils.parse_float_string)\n        return leaderboard", "response": "Returns the leaderboard for the given round."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_staking_leaderboard(self, round_num=0, tournament=1):\n        msg = \"getting stakes for tournament {} round {}\"\n        self.logger.info(msg.format(tournament, round_num))\n        query = '''\n            query($number: Int!\n                  $tournament: Int!) {\n              rounds(number: $number\n                     tournament: $tournament) {\n                leaderboard {\n                  consistency\n                  liveLogloss\n                  liveAuroc\n                  username\n                  validationLogloss\n                  validationAuroc\n                  stake {\n                    insertedAt\n                    soc\n                    confidence\n                    value\n                    txHash\n                  }\n                }\n              }\n            }\n        '''\n        arguments = {'number': round_num, 'tournament': tournament}\n        result = self.raw_query(query, arguments)['data']['rounds'][0]\n        if result is None:\n            return None\n        stakes = result['leaderboard']\n        # filter those with actual stakes\n        stakes = [item for item in stakes if item[\"stake\"] is not None]\n        # convert strings to python objects\n        for s in stakes:\n            utils.replace(s[\"stake\"], \"insertedAt\",\n                          utils.parse_datetime_string)\n            utils.replace(s[\"stake\"], \"confidence\", utils.parse_float_string)\n            utils.replace(s[\"stake\"], \"soc\", utils.parse_float_string)\n            utils.replace(s[\"stake\"], \"value\", utils.parse_float_string)\n        return stakes", "response": "Returns the leaderboard of the staking competition for the given tournament."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_nmr_prize_pool(self, round_num=0, tournament=1):\n        tournaments = self.get_competitions(tournament)\n        tournaments.sort(key=lambda t: t['number'])\n        if round_num == 0:\n            t = tournaments[-1]\n        else:\n            tournaments = [t for t in tournaments if t['number'] == round_num]\n            if len(tournaments) == 0:\n                raise ValueError(\"invalid round number\")\n            t = tournaments[0]\n        return t['prizePoolNmr']", "response": "Get NMR prize pool for the given round and tournament."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the staking cutoff for the given round and tournament.", "response": "def get_staking_cutoff(self, round_num=0, tournament=1):\n        \"\"\"Compute staking cutoff for the given round and tournament.\n\n        Args:\n            round_num (int, optional): The round you are interested in,\n                defaults to current round.\n            tournament (int, optional): ID of the tournament, defaults to 1\n\n        Returns:\n            decimal.Decimal: cutoff probability\n\n        Raises:\n            ValueError: in case of missing prize pool information\n        \"\"\"\n        query = '''\n            query($number: Int!\n                  $tournament: Int!) {\n              rounds(number: $number\n                     tournament: $tournament) {\n                selection {\n                  outcome\n                  pCutoff\n                  bCutoff\n                }\n              }\n            }\n        '''\n        arguments = {'number': round_num, 'tournament': tournament}\n        result = self.raw_query(query, arguments)\n        result = result['data']['rounds'][0]['selection']\n        key = 'bCutoff' if round_num >= 154 or round_num == 0 else 'pCutoff'\n        return utils.parse_float_string(result[key])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving information about all competitions of a specific tournament.", "response": "def get_competitions(self, tournament=1):\n        \"\"\"Retrieves information about all competitions\n\n        Args:\n            tournament (int, optional): ID of the tournament, defaults to 1\n\n        Returns:\n            list of dicts: list of rounds\n\n            Each round's dict contains the following items:\n\n                * datasetId (`str`)\n                * number (`int`)\n                * openTime (`datetime`)\n                * resolveTime (`datetime`)\n                * participants (`int`): number of participants\n                * prizePoolNmr (`decimal.Decimal`)\n                * prizePoolUsd (`decimal.Decimal`)\n                * resolvedGeneral (`bool`)\n                * resolvedStaking (`bool`)\n                * ruleset (`string`)\n\n        Example:\n            >>> NumerAPI().get_competitions()\n            [\n             {'datasetId': '59a70840ca11173c8b2906ac',\n              'number': 71,\n              'openTime': datetime.datetime(2017, 8, 31, 0, 0),\n              'resolveTime': datetime.datetime(2017, 9, 27, 21, 0),\n              'participants': 1287,\n              'prizePoolNmr': Decimal('0.00'),\n              'prizePoolUsd': Decimal('6000.00'),\n              'resolvedGeneral': True,\n              'resolvedStaking': True,\n              'ruleset': 'p_auction'\n             },\n              ..\n            ]\n        \"\"\"\n        self.logger.info(\"getting rounds...\")\n\n        query = '''\n            query($tournament: Int!) {\n              rounds(tournament: $tournament) {\n                number\n                resolveTime\n                datasetId\n                openTime\n                resolvedGeneral\n                resolvedStaking\n                participants\n                prizePoolNmr\n                prizePoolUsd\n                ruleset\n              }\n            }\n        '''\n        arguments = {'tournament': tournament}\n        result = self.raw_query(query, arguments)\n        rounds = result['data']['rounds']\n        # convert datetime strings to datetime.datetime objects\n        for r in rounds:\n            utils.replace(r, \"openTime\", utils.parse_datetime_string)\n            utils.replace(r, \"resolveTime\", utils.parse_datetime_string)\n            utils.replace(r, \"prizePoolNmr\", utils.parse_float_string)\n            utils.replace(r, \"prizePoolUsd\", utils.parse_float_string)\n        return rounds"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_current_round(self, tournament=1):\n        # zero is an alias for the current round!\n        query = '''\n            query($tournament: Int!) {\n              rounds(tournament: $tournament\n                     number: 0) {\n                number\n              }\n            }\n        '''\n        arguments = {'tournament': tournament}\n        data = self.raw_query(query, arguments)['data']['rounds'][0]\n        if data is None:\n            return None\n        round_num = data[\"number\"]\n        return round_num", "response": "Get the number of the current active round."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tournaments(self, only_active=True):\n        query = \"\"\"\n            query {\n              tournaments {\n                id\n                name\n                tournament\n                active\n            }\n        }\n        \"\"\"\n        data = self.raw_query(query)['data']['tournaments']\n        if only_active:\n            data = [d for d in data if d['active']]\n        return data", "response": "Get all tournaments in a specific language."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user_activities(self, username, tournament=1):\n        query = '''\n            query($tournament: Int!\n                  $username: String!) {\n              userActivities(tournament: $tournament\n                     username: $username) {\n                resolved\n                roundNumber\n                tournament\n                submission {\n                    concordance\n                    consistency\n                    date\n                    liveLogloss\n                    liveAuroc\n                    validationLogloss\n                    validationAuroc\n                }\n                stake {\n                    confidence\n                    date\n                    nmrEarned\n                    staked\n                    usdEarned\n                    value\n                    burned\n                }\n              }\n            }\n        '''\n        arguments = {'tournament': tournament, 'username': username}\n        data = self.raw_query(query, arguments)['data']['userActivities']\n        # filter rounds with no activity\n        data = [item for item in data\n                if item['submission']['date'] is not None]\n        for item in data:\n            # remove stakes with all values set to None\n            if item['stake']['date'] is None:\n                del item['stake']\n            # parse\n            else:\n                utils.replace(item['stake'], \"date\",\n                              utils.parse_datetime_string)\n                for col in ['confidence', 'value', 'nmrEarned', 'usdEarned']:\n                    utils.replace(item['stake'], col, utils.parse_float_string)\n        # parse\n        for item in data:\n            utils.replace(item['submission'], \"date\",\n                          utils.parse_datetime_string)\n        return data", "response": "Get the user activities."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_submission_filenames(self, tournament=None, round_num=None):\n        query = '''\n          query {\n            user {\n              submissions {\n                filename\n                selected\n                round {\n                   tournament\n                   number\n                }\n              }\n            }\n          }\n        '''\n        data = self.raw_query(query, authorization=True)['data']['user']\n\n        filenames = [{\"round_num\": item['round']['number'],\n                      \"tournament\": item['round']['tournament'],\n                      \"filename\": item['filename']}\n                     for item in data['submissions'] if item['selected']]\n\n        if round_num is not None:\n            filenames = [f for f in filenames if f['round_num'] == round_num]\n        if tournament is not None:\n            filenames = [f for f in filenames if f['tournament'] == tournament]\n        filenames.sort(key=lambda f: (f['round_num'], f['tournament']))\n        return filenames", "response": "Get filenames of the submission of the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the overall ranking of a specific item in the nature of the overall item.", "response": "def get_rankings(self, limit=50, offset=0):\n        \"\"\"Get the overall ranking\n\n        Args:\n            limit (int): number of items to return (optional, defaults to 50)\n            offset (int): number of items to skip (optional, defaults to 0)\n\n        Returns:\n            list of dicts: list of ranking items\n\n            Each dict contains the following items:\n\n                * id (`str`)\n                * username (`str`)\n                * nmrBurned (`decimal.Decimal`)\n                * nmrPaid (`decimal.Decimal`)\n                * nmrStaked (`decimal.Decimal`)\n                * rep (`int`)\n                * stakeCount (`int`)\n                * usdEarned (`decimal.Decimal`)\n\n        Example:\n            >>> numerapi.NumerAPI().get_rankings(1)\n            [{'username': 'glasperlenspiel',\n              'usdEarned': Decimal('16347.12'),\n              'stakeCount': 41,\n              'rep': 14,\n              'nmrStaked': Decimal('250.000000000000000000'),\n              'nmrPaid': Decimal('16061.37'),\n              'nmrBurned': Decimal('295.400000000000000000'),\n              'id': 'bbee4f0e-f238-4d8a-8f1b-5eb384cdcbfc'}]\n        \"\"\"\n        query = '''\n            query($limit: Int!\n                  $offset: Int!) {\n              rankings(limit: $limit\n                       offset: $offset) {\n                username\n                id\n                nmrBurned\n                nmrPaid\n                nmrStaked\n                rep\n                stakeCount\n                usdEarned\n              }\n            }\n        '''\n        arguments = {'limit': limit, 'offset': offset}\n        data = self.raw_query(query, arguments)['data']['rankings']\n        for item in data:\n            for p in [\"nmrBurned\", \"nmrPaid\", \"nmrStaked\", \"usdEarned\"]:\n                utils.replace(item, p, utils.parse_float_string)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_submission_ids(self, tournament=1):\n        query = \"\"\"\n            query($tournament: Int!) {\n              rounds(tournament: $tournament\n                     number: 0) {\n                leaderboard {\n                  username\n                  submissionId\n                }\n            }\n        }\n        \"\"\"\n        arguments = {'tournament': tournament}\n        data = self.raw_query(query, arguments)['data']['rounds'][0]\n        if data is None:\n            return None\n        mapping = {item['username']: item['submissionId']\n                   for item in data['leaderboard']}\n        return mapping", "response": "Get dict with username - > submission_id mapping."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all user information.", "response": "def get_user(self):\n        \"\"\"Get all information about you!\n\n        Returns:\n            dict: user information including the following fields:\n\n                * assignedEthAddress (`str`)\n                * availableNmr (`decimal.Decimal`)\n                * availableUsd (`decimal.Decimal`)\n                * banned (`bool`)\n                * email (`str`)\n                * id (`str`)\n                * insertedAt (`datetime`)\n                * mfaEnabled (`bool`)\n                * status (`str`)\n                * username (`str`)\n                * country (`str)\n                * phoneNumber (`str`)\n                * apiTokens (`list`) each with the following fields:\n                 * name (`str`)\n                 * public_id (`str`)\n                 * scopes (`list of str`)\n\n        Example:\n            >>> api = NumerAPI(secret_key=\"..\", public_id=\"..\")\n            >>> api.get_user()\n            {'apiTokens': [\n                    {'name': 'tokenname',\n                     'public_id': 'BLABLA',\n                     'scopes': ['upload_submission', 'stake', ..]\n                     }, ..],\n             'assignedEthAddress': '0x0000000000000000000000000001',\n             'availableNmr': Decimal('99.01'),\n             'availableUsd': Decimal('9.47'),\n             'banned': False,\n             'email': 'username@example.com',\n             'phoneNumber': '0123456',\n             'country': 'US',\n             'id': '1234-ABC..',\n             'insertedAt': datetime.datetime(2018, 1, 1, 2, 16, 48),\n             'mfaEnabled': False,\n             'status': 'VERIFIED',\n             'username': 'cool username'\n             }\n        \"\"\"\n        query = \"\"\"\n          query {\n            user {\n              username\n              banned\n              assignedEthAddress\n              availableNmr\n              availableUsd\n              email\n              id\n              mfaEnabled\n              status\n              country\n              phoneNumber\n              insertedAt\n              apiTokens {\n                name\n                public_id\n                scopes\n              }\n            }\n          }\n        \"\"\"\n        data = self.raw_query(query, authorization=True)['data']['user']\n        # convert strings to python objects\n        utils.replace(data, \"insertedAt\", utils.parse_datetime_string)\n        utils.replace(data, \"availableUsd\", utils.parse_float_string)\n        utils.replace(data, \"availableNmr\", utils.parse_float_string)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all your payments.", "response": "def get_payments(self):\n        \"\"\"Get all your payments.\n\n        Returns:\n            list of dicts: payments\n\n            For each payout in the list, a dict contains the following items:\n\n                * nmrAmount (`decimal.Decimal`)\n                * usdAmount (`decimal.Decimal`)\n                * tournament (`str`)\n                * round (`dict`)\n                 * number (`int`)\n                 * openTime (`datetime`)\n                 * resolveTime (`datetime`)\n                 * resolvedGeneral (`bool`)\n                 * resolvedStaking (`bool`)\n\n        Example:\n            >>> api = NumerAPI(secret_key=\"..\", public_id=\"..\")\n            >>> api.get_payments()\n            [{'nmrAmount': Decimal('0.00'),\n              'round': {'number': 84,\n               'openTime': datetime.datetime(2017, 12, 2, 18, 0, tzinfo=tzutc()),\n               'resolveTime': datetime.datetime(2018, 1, 1, 18, 0, tzinfo=tzutc()),\n               'resolvedGeneral': True,\n               'resolvedStaking': True},\n              'tournament': 'staking',\n              'usdAmount': Decimal('17.44')},\n              ...\n             ]\n        \"\"\"\n        query = \"\"\"\n          query {\n            user {\n              payments {\n                nmrAmount\n                round {\n                  number\n                  openTime\n                  resolveTime\n                  resolvedGeneral\n                  resolvedStaking\n                }\n                tournament\n                usdAmount\n              }\n            }\n          }\n        \"\"\"\n        data = self.raw_query(query, authorization=True)['data']\n        payments = data['user']['payments']\n        # convert strings to python objects\n        for p in payments:\n            utils.replace(p['round'], \"openTime\", utils.parse_datetime_string)\n            utils.replace(p['round'], \"resolveTime\",\n                          utils.parse_datetime_string)\n            utils.replace(p, \"usdAmount\", utils.parse_float_string)\n            utils.replace(p, \"nmrAmount\", utils.parse_float_string)\n        return payments"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_transactions(self):\n        query = \"\"\"\n          query {\n            user {\n              nmrDeposits {\n                from\n                posted\n                status\n                to\n                txHash\n                value\n              }\n              nmrWithdrawals {\n                from\n                posted\n                status\n                to\n                txHash\n                value\n              }\n              usdWithdrawals {\n                ethAmount\n                confirmTime\n                from\n                posted\n                sendTime\n                status\n                to\n                txHash\n                usdAmount\n              }\n            }\n          }\n        \"\"\"\n        txs = self.raw_query(query, authorization=True)['data']['user']\n        # convert strings to python objects\n        for t in txs['usdWithdrawals']:\n            utils.replace(t, \"confirmTime\", utils.parse_datetime_string)\n            utils.replace(t, \"sendTime\", utils.parse_datetime_string)\n            utils.replace(t, \"usdAmount\", utils.parse_float_string)\n        for t in txs[\"nmrWithdrawals\"]:\n            utils.replace(t, \"value\", utils.parse_float_string)\n        for t in txs[\"nmrDeposits\"]:\n            utils.replace(t, \"value\", utils.parse_float_string)\n        return txs", "response": "Get all your deposits and withdrawals."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all your stakes.", "response": "def get_stakes(self):\n        \"\"\"List all your stakes.\n\n        Returns:\n            list of dicts: stakes\n\n            Each stake is a dict with the following fields:\n\n                * confidence (`decimal.Decimal`)\n                * roundNumber (`int`)\n                * tournamentId (`int`)\n                * soc (`decimal.Decimal`)\n                * insertedAt (`datetime`)\n                * staker (`str`): NMR adress used for staking\n                * status (`str`)\n                * txHash (`str`)\n                * value (`decimal.Decimal`)\n\n        Example:\n\n            >>> api = NumerAPI(secret_key=\"..\", public_id=\"..\")\n            >>> api.get_stakes()\n            [{'confidence': Decimal('0.053'),\n              'insertedAt': datetime.datetime(2017, 9, 26, 8, 18, 36, 709000, tzinfo=tzutc()),\n              'roundNumber': 74,\n              'soc': Decimal('56.60'),\n              'staker': '0x0000000000000000000000000000000000003f9e',\n              'status': 'confirmed',\n              'tournamentId': 1,\n              'txHash': '0x1cbb985629552a0f57b98a1e30a5e7f101a992121db318cef02e02aaf0e91c95',\n              'value': Decimal('3.00')},\n              ..\n             ]\n        \"\"\"\n        query = \"\"\"\n          query {\n            user {\n              stakeTxs {\n                confidence\n                insertedAt\n                roundNumber\n                tournamentId\n                soc\n                staker\n                status\n                txHash\n                value\n              }\n            }\n          }\n        \"\"\"\n        data = self.raw_query(query, authorization=True)['data']\n        stakes = data['user']['stakeTxs']\n        # convert strings to python objects\n        for s in stakes:\n            utils.replace(s, \"insertedAt\", utils.parse_datetime_string)\n            utils.replace(s, \"soc\", utils.parse_float_string)\n            utils.replace(s, \"confidence\", utils.parse_float_string)\n            utils.replace(s, \"value\", utils.parse_float_string)\n        return stakes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the status of the last submission associated with the account.", "response": "def submission_status(self, submission_id=None):\n        \"\"\"submission status of the last submission associated with the account.\n\n        Args:\n            submission_id (str): submission of interest, defaults to the last\n                submission done with the account\n\n        Returns:\n            dict: submission status with the following content:\n\n                * concordance (`dict`):\n                 * pending (`bool`)\n                 * value (`bool`): whether the submission is concordant\n                * consistency (`float`): consistency of the submission\n                * validationLogloss (`float`)\n                * validationAuroc (`float`)\n\n        Example:\n            >>> api = NumerAPI(secret_key=\"..\", public_id=\"..\")\n            >>> api.upload_predictions()\n            >>> api.submission_status()\n            {'concordance': {'pending': False, 'value': True},\n             'consistency': 91.66666666666666,\n             'validationLogloss': 0.691733023121,\n             'validationAuroc': 0.52}\n        \"\"\"\n        if submission_id is None:\n            submission_id = self.submission_id\n\n        if submission_id is None:\n            raise ValueError('You need to submit something first or provide\\\n                              a submission ID')\n\n        query = '''\n            query($submission_id: String!) {\n              submissions(id: $submission_id) {\n                concordance {\n                  pending\n                  value\n                }\n                consistency\n                validationLogloss\n                validationAuroc\n              }\n            }\n            '''\n        variable = {'submission_id': submission_id}\n        data = self.raw_query(query, variable, authorization=True)\n        status = data['data']['submissions'][0]\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upload_predictions(self, file_path, tournament=1):\n        self.logger.info(\"uploading predictions...\")\n\n        auth_query = '''\n            query($filename: String!\n                  $tournament: Int!) {\n                submission_upload_auth(filename: $filename\n                                       tournament: $tournament) {\n                    filename\n                    url\n                }\n            }\n            '''\n        arguments = {'filename': os.path.basename(file_path),\n                     'tournament': tournament}\n        submission_resp = self.raw_query(auth_query, arguments,\n                                         authorization=True)\n        submission_auth = submission_resp['data']['submission_upload_auth']\n        with open(file_path, 'rb') as fh:\n            requests.put(submission_auth['url'], data=fh.read())\n        create_query = '''\n            mutation($filename: String!\n                     $tournament: Int!) {\n                create_submission(filename: $filename\n                                  tournament: $tournament) {\n                    id\n                }\n            }\n            '''\n        arguments = {'filename': submission_auth['filename'],\n                     'tournament': tournament}\n        create = self.raw_query(create_query, arguments, authorization=True)\n        self.submission_id = create['data']['create_submission']['id']\n        return self.submission_id", "response": "Uploads predictions from file to numer API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparticipating in the staking competition.", "response": "def stake(self, confidence, value, tournament=1):\n        \"\"\"Participate in the staking competition.\n\n        Args:\n            confidence (float or str): your confidence (C) value\n            value (float or str): amount of NMR you are willing to stake\n            tournament (int): ID of the tournament (optional, defaults to 1)\n\n        Returns:\n            dict: stake information with the following content:\n\n              * insertedAt (`datetime`)\n              * status (`str`)\n              * txHash (`str`)\n              * value (`decimal.Decimal`)\n              * from (`str`)\n\n        Example:\n            >>> api = NumerAPI(secret_key=\"..\", public_id=\"..\")\n            >>> api.stake(0.1, 10)\n            {'stake':\n              {'from': None,\n               'insertedAt': None,\n               'status': None,\n               'txHash': '0x76519...2341ca0',\n               'value': '10'}\n             }\n        \"\"\"\n\n        query = '''\n          mutation($code: String\n            $confidence: String!\n            $password: String\n            $round: Int!\n            $value: String!\n            $tournament: Int!) {\n              stake(code: $code\n                    confidence: $confidence\n                    password: $password\n                    round: $round\n                    value: $value\n                    tournament: $tournament) {\n                insertedAt\n                status\n                txHash\n                value\n                from\n              }\n        }\n        '''\n        arguments = {'code': 'somecode',\n                     'confidence': str(confidence),\n                     'password': \"somepassword\",\n                     'round': self.get_current_round(),\n                     'value': str(value),\n                     'tournament': tournament}\n        result = self.raw_query(query, arguments, authorization=True)\n        stake = result['data']\n        utils.replace(stake, \"value\", utils.parse_float_string)\n        utils.replace(stake, \"insertedAt\", utils.parse_datetime_string)\n        return stake"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_new_round(self, hours=24, tournament=1):\n        query = '''\n            query($tournament: Int!) {\n              rounds(tournament: $tournament\n                     number: 0) {\n                number\n                openTime\n              }\n            }\n        '''\n        arguments = {'tournament': tournament}\n        raw = self.raw_query(query, arguments)['data']['rounds'][0]\n        if raw is None:\n            return False\n        open_time = utils.parse_datetime_string(raw['openTime'])\n        now = datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\n        is_new_round = open_time > now - datetime.timedelta(hours=hours)\n        return is_new_round", "response": "Check if a new round has started within the last 24 hours."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_submission_successful(self, submission_id=None):\n        status = self.submission_status(submission_id)\n        # need to cast to bool to not return None in some cases.\n        success = bool(status[\"concordance\"][\"value\"])\n        return success", "response": "Checks if the last submission passes all checks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tournament_number2name(self, number):\n        tournaments = self.get_tournaments()\n        d = {t['tournament']: t['name'] for t in tournaments}\n        return d.get(number, None)", "response": "Translate tournament number to tournament name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntranslate tournament name to tournament number.", "response": "def tournament_name2number(self, name):\n        \"\"\"Translate tournament name to tournament number.\n\n        Args:\n            name (str): tournament name to translate\n\n        Returns:\n            number (int): number of the tournament or `None` if unknown.\n\n        Examples:\n            >>> NumerAPI().tournament_name2number('delta')\n            4\n            >>> NumerAPI().tournament_name2number('foo')\n            None\n        \"\"\"\n        tournaments = self.get_tournaments()\n        d = {t['name']: t['tournament'] for t in tournaments}\n        return d.get(name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads dataset for the current active round.", "response": "def download_dataset(tournament, unzip):\n    \"\"\"Download dataset for the current active round.\"\"\"\n    click.echo(napi.download_current_dataset(tournament=tournament, unzip=unzip))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the leaderboard for the given round.", "response": "def leaderboard(round_num=0, tournament=1):\n    \"\"\"Retrieves the leaderboard for the given round.\"\"\"\n    click.echo(prettify(napi.get_leaderboard(tournament=tournament,\n                                             round_num=round_num)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the staking leaderboard for the given round.", "response": "def staking_leaderboard(round_num=0, tournament=1):\n    \"\"\"Retrieves the staking competition leaderboard for the given round.\"\"\"\n    click.echo(prettify(napi.get_staking_leaderboard(tournament=tournament,\n                                                     round_num=round_num)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rankings(limit=20, offset=0):\n    click.echo(prettify(napi.get_rankings(limit=limit, offset=offset)))", "response": "Get the overall rankings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_activities(username, tournament=1):\n    click.echo(prettify(napi.get_user_activities(username, tournament)))", "response": "Get user activities (works for all users!)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef submission_filenames(round_num=None, tournament=None):\n    click.echo(prettify(\n        napi.get_submission_filenames(tournament, round_num)))", "response": "Get filenames of your submissions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the staking cutoff for the given round and tournament.", "response": "def staking_cutoff(round_num, tournament):\n    \"\"\"Compute staking cutoff for the given round and tournament.\"\"\"\n    click.echo(prettify(napi.get_staking_cutoff(round_num, tournament)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_new_round(hours=24, tournament=1):\n    click.echo(int(napi.check_new_round(hours=hours, tournament=tournament)))", "response": "Check if a new round has started within the last hours."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparticipate in the staking competition.", "response": "def stake(confidence, value, tournament):\n    \"\"\"Participate in the staking competition.\"\"\"\n    click.echo(napi.stake(confidence, value, tournament))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall javascript and exporting server extensions in Jupyter notebook.", "response": "def install_bootstrapped_files(nb_path=None, server_config=True, DEBUG=False):\n    \"\"\"\n    Installs javascript and exporting server extensions in Jupyter notebook.\n\n    Args:\n        nb_path (string): Path to notebook module.\n        server_config (boolean): Install exporting server extensions.\n        DEBUG (boolean): Verbose mode.\n    \"\"\"\n\n    install_path = None\n    print('Starting hide_code.js install...')\n    current_dir = path.abspath(path.dirname(__file__))\n    config_dirs = j_path.jupyter_config_path()\n    notebook_module_path = Utils.get_notebook_module_dir()\n\n    # check for config directory with a \"custom\" folder\n    # TODO update this logic to check if custom.js file exists\n    for dir in config_dirs:\n        custom_dir = path.join(dir, \"custom\")\n        if path.isdir(custom_dir):\n            install_path = custom_dir\n            break\n\n    # last ditch effort in case jupyter config directories don't contain custom/custom.js\n    if install_path == None:\n        print(\"No config directories contain \\\"custom\\\" folder. Trying Jupyter notebook module path...\")\n        install_path = path.join(notebook_module_path, \"static\", \"custom\")\n\n    if nb_path != None:\n        install_path = nb_path\n        print(\"Using argument supplied path: \" + install_path)\n\n    if DEBUG:\n        print(install_path)\n\n    # copy js into static/custom directory in Jupyter/iPython directory\n    if path.isdir(install_path):\n        shutil.copyfile(path.join(current_dir, \"hide_code.js\"), path.join(install_path, \"hide_code.js\"))\n        print('Copying hide_code.js to ' + install_path)\n\n        # add require to end of custom.js to auto-load on notebook startup\n        print(\"Attempting to configure custom.js to auto-load hide_code.js...\")\n        try:\n            with open(path.join(current_dir, \"auto-load.txt\")) as auto:\n                auto_load_txt = auto.read();\n                auto_loaded = False\n\n                # check if auto-load.txt is already in custom.js\n                with open(path.join(install_path, \"custom.js\"), 'r') as customJS:\n                    if auto_load_txt in customJS.read():\n                        auto_loaded = True\n                        print(\"Custom.js already configured to auto-load hide_code.js.\")\n\n                if not auto_loaded:  # append auto load require to end of custom.js\n                    with open(path.join(install_path, \"custom.js\"), 'a') as customJS:\n                        customJS.write(auto_load_txt)\n                        print(\"Configured custom.js to auto-load hide_code.js.\")\n        except:\n            print(\"Custom.js not in custom directory.\")\n    else:\n        print('Unable to install into ' + install_path)\n        print('Directory doesn\\'t exist.')\n        print('Make sure Jupyter is installed.')\n\n    if server_config:\n        print(\"Attempting to configure auto-loading for hide_code export handlers.\")\n        try:\n            # Activate the Python server extension\n            server_cm = ConfigManager(config_dir=j_path.jupyter_config_dir())\n            cfg = server_cm.get('jupyter_notebook_config')\n            server_extensions = (cfg.setdefault('NotebookApp', {})\n                                 .setdefault('server_extensions', [])\n                                 )\n            extension = 'hide_code.hide_code'\n            if extension not in server_extensions:\n                cfg['NotebookApp']['server_extensions'] += [extension]\n                server_cm.update('jupyter_notebook_config', cfg)\n                print('Configured jupyter to auto-load hide_code export handlers.')\n            else:\n                print(\"Jupyter already configured to auto-load export handlers.\")\n        except:\n            print('Unable to install server extension.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ipynb_file_name(params):\n    global notebook_dir\n    p = notebook_dir + [param.replace('/', '') for param in params if param is not None]\n    return path.join(*p)", "response": "Returns the name of the notebook file based on route parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the Tb from the radiance using the Planck function", "response": "def radiance2tb(rad, wavelength):\n    \"\"\"\n    Get the Tb from the radiance using the Planck function\n\n    rad:\n        Radiance in SI units\n    wavelength:\n        Wavelength in SI units (meter)\n    \"\"\"\n    from pyspectral.blackbody import blackbody_rad2temp as rad2temp\n    return rad2temp(wavelength, rad)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the relative spectral responses from file and store them in self. rsr.", "response": "def _get_rsr(self):\n        \"\"\"\n        Get the relative spectral responses from file, find the bandname, and\n        convert to the requested wave-spave (wavelength or wave number)\n\n        \"\"\"\n        sensor = RelativeSpectralResponse(self.platform_name, self.instrument)\n\n        if self.wavespace == WAVE_NUMBER:\n            LOG.debug(\"Converting to wavenumber...\")\n            self.rsr, info = convert2wavenumber(sensor.rsr)\n        else:\n            self.rsr = sensor.rsr\n            info = {'unit': sensor.unit, 'si_scale': sensor.si_scale}\n\n        self._wave_unit = info['unit']\n        self._wave_si_scale = info['si_scale']\n\n        if isinstance(self.band, str):\n            self.bandname = BANDNAMES.get(self.instrument, BANDNAMES['generic']).get(self.band, self.band)\n        elif isinstance(self.band, Number):\n            self.bandwavelength = self.band\n            self.bandname = get_bandname_from_wavelength(self.instrument, self.band, self.rsr)\n\n        self.wavelength_or_wavenumber = (self.rsr[self.bandname][self.detector][self.wavespace] *\n                                         self._wave_si_scale)\n        self.response = self.rsr[self.bandname][self.detector]['response']\n        # Get the integral of the spectral response curve:\n        self.rsr_integral = np.trapz(self.response, self.wavelength_or_wavenumber)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the satellite name used in the rsr - reader from the platform name and number of the satellite.", "response": "def _getsatname(self):\n        \"\"\"\n        Get the satellite name used in the rsr-reader, from the platform\n        and number\n\n        \"\"\"\n        if self.platform_name.startswith(\"Meteosat\"):\n            return self.platform_name\n        else:\n            raise NotImplementedError(\n                'Platform {0} not yet supported...'.format(self.platform_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tb2radiance(self, tb_, **kwargs):\n        lut = kwargs.get('lut', None)\n        normalized = kwargs.get('normalized', True)\n\n        if self.wavespace == WAVE_NUMBER:\n            if normalized:\n                unit = 'W/m^2 sr^-1 (m^-1)^-1'\n            else:\n                unit = 'W/m^2 sr^-1'\n            scale = 1.0\n        else:\n            if normalized:\n                unit = 'W/m^2 sr^-1 m^-1'\n            else:\n                unit = 'W/m^2 sr^-1'\n            scale = 1.0\n\n        if lut:\n            ntb = (tb_ * self.tb_scale).astype('int16')\n            start = int(lut['tb'][0] * self.tb_scale)\n            retv = {}\n            bounds = 0, lut['radiance'].shape[0] - 1\n            index = np.clip(ntb - start, bounds[0], bounds[1])\n            retv['radiance'] = lut['radiance'][index]\n            if retv['radiance'].ravel().shape[0] == 1:\n                retv['radiance'] = retv['radiance'][0]\n            retv['unit'] = unit\n            retv['scale'] = scale\n            return retv\n\n        planck = self.blackbody_function(self.wavelength_or_wavenumber, tb_) * self.response\n        if normalized:\n            radiance = integrate.trapz(planck, self.wavelength_or_wavenumber) / self.rsr_integral\n        else:\n            radiance = integrate.trapz(planck, self.wavelength_or_wavenumber)\n\n        return {'radiance': radiance,\n                'unit': unit,\n                'scale': scale}", "response": "This function returns the radiance value for a specific brightness temperature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a Tb to radiance look - up table", "response": "def make_tb2rad_lut(self, filepath, normalized=True):\n        \"\"\"Generate a Tb to radiance look-up table\"\"\"\n        tb_ = np.arange(TB_MIN, TB_MAX, self.tb_resolution)\n        retv = self.tb2radiance(tb_, normalized=normalized)\n        rad = retv['radiance']\n        np.savez(filepath, tb=tb_, radiance=rad)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef radiance2tb(self, rad):\n        return radiance2tb(rad, self.rsr[self.bandname][self.detector]['central_wavelength'] * 1e-6)", "response": "Get the Tb from the radiance using the Planck function and the central wavelength of the band."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef radiance2tb(self, rad):\n        #\n        # Tb = C2 * \u03bdc/{\u03b1 * log[C1*\u03bdc**3 / L + 1]} - \u03b2/\u03b1\n        #\n        # C1 = 2 * h * c**2 and C2 = hc/k\n        #\n        c_1 = 2 * H_PLANCK * C_SPEED ** 2\n        c_2 = H_PLANCK * C_SPEED / K_BOLTZMANN\n\n        vc_ = SEVIRI[self.bandname][self.platform_name][0]\n        # Multiply by 100 to get SI units!\n        vc_ *= 100.0\n        alpha = SEVIRI[self.bandname][self.platform_name][1]\n        beta = SEVIRI[self.bandname][self.platform_name][2]\n\n        tb_ = c_2 * vc_ / \\\n            (alpha * np.log(c_1 * vc_ ** 3 / rad + 1)) - beta / alpha\n\n        return tb_", "response": "Get the Tb from the radiance using the simple non - linear regression\n        method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tb2radiance(self, tb_, **kwargs):\n        # L = C1 * \u03bdc**3 / (exp (C2 \u03bdc / [\u03b1Tb + \u03b2]) \u2212 1)\n        #\n        # C1 = 2 * h * c**2 and C2 = hc/k\n        #\n        lut = kwargs.get('lut', None)\n        normalized = kwargs.get('normalized', True)\n\n        if lut is not None:\n            raise NotImplementedError('Using a tb-radiance LUT is not yet supported')\n        if not normalized:\n            raise NotImplementedError('Deriving the band integrated radiance is not supported')\n\n        c_1 = 2 * H_PLANCK * C_SPEED ** 2\n        c_2 = H_PLANCK * C_SPEED / K_BOLTZMANN\n\n        vc_ = SEVIRI[self.bandname][self.platform_name][0]\n        # Multiply by 100 to get SI units!\n        vc_ *= 100.0\n        alpha = SEVIRI[self.bandname][self.platform_name][1]\n        beta = SEVIRI[self.bandname][self.platform_name][2]\n\n        radiance = c_1 * vc_ ** 3 / \\\n            (np.exp(c_2 * vc_ / (alpha * tb_ + beta)) - 1)\n\n        unit = 'W/m^2 sr^-1 (m^-1)^-1'\n        scale = 1.0\n        return {'radiance': radiance,\n                'unit': unit,\n                'scale': scale}", "response": "Get the radiance from the Tb using the simple non - linear regression\n        method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_rsr_data_version(self):\n\n        rsr_data_version_path = os.path.join(self.rsr_dir, RSR_DATA_VERSION_FILENAME)\n        if not os.path.exists(rsr_data_version_path):\n            return \"v0.0.0\"\n\n        with open(rsr_data_version_path, 'r') as fpt:\n            # Get the version from the file\n            return fpt.readline().strip()", "response": "Check the version of the RSR data file in the RSR directory and return the string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking and fix instrument name if needed", "response": "def _check_instrument(self):\n        \"\"\"Check and try fix instrument name if needed\"\"\"\n        instr = INSTRUMENTS.get(self.platform_name, self.instrument.lower())\n        if instr != self.instrument.lower():\n            self.instrument = instr\n            LOG.warning(\"Inconsistent instrument/satellite input - \" +\n                        \"instrument set to %s\", self.instrument)\n\n        self.instrument = self.instrument.lower().replace('/', '')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the rsr filname from platform and instrument names and download if not available.", "response": "def _get_filename(self):\n        \"\"\"Get the rsr filname from platform and instrument names, and download if not\n           available.\n\n        \"\"\"\n        self.filename = expanduser(\n            os.path.join(self.rsr_dir, 'rsr_{0}_{1}.h5'.format(self.instrument,\n                                                               self.platform_name)))\n\n        LOG.debug('Filename: %s', str(self.filename))\n        if not os.path.exists(self.filename) or not os.path.isfile(self.filename):\n            LOG.warning(\"No rsr file %s on disk\", self.filename)\n\n            if self._rsr_data_version_uptodate:\n                LOG.info(\"RSR data up to date, so seems there is no support for this platform and sensor\")\n            else:\n                # Try download from the internet!\n                if self.do_download:\n                    LOG.info(\"Will download from internet...\")\n                    download_rsr()\n                    if self._get_rsr_data_version() == RSR_DATA_VERSION:\n                        self._rsr_data_version_uptodate = True\n\n        if not self._rsr_data_version_uptodate:\n            LOG.warning(\"rsr data may not be up to date: %s\", self.filename)\n            if self.do_download:\n                LOG.info(\"Will download from internet...\")\n                download_rsr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the internally formatet hdf5 relative spectral response data", "response": "def load(self):\n        \"\"\"Read the internally formatet hdf5 relative spectral response data\"\"\"\n        import h5py\n\n        no_detectors_message = False\n        with h5py.File(self.filename, 'r') as h5f:\n            self.band_names = [b.decode('utf-8') for b in h5f.attrs['band_names'].tolist()]\n            self.description = h5f.attrs['description'].decode('utf-8')\n            if not self.platform_name:\n                try:\n                    self.platform_name = h5f.attrs['platform_name'].decode('utf-8')\n                except KeyError:\n                    LOG.warning(\"No platform_name in HDF5 file\")\n                    try:\n                        self.platform_name = h5f.attrs[\n                            'platform'] + '-' + h5f.attrs['satnum']\n                    except KeyError:\n                        LOG.warning(\n                            \"Unable to determine platform name from HDF5 file content\")\n                        self.platform_name = None\n\n            if not self.instrument:\n                try:\n                    self.instrument = h5f.attrs['sensor'].decode('utf-8')\n                except KeyError:\n                    LOG.warning(\"No sensor name specified in HDF5 file\")\n                    self.instrument = INSTRUMENTS.get(self.platform_name)\n\n            for bandname in self.band_names:\n                self.rsr[bandname] = {}\n                try:\n                    num_of_det = h5f[bandname].attrs['number_of_detectors']\n                except KeyError:\n                    if not no_detectors_message:\n                        LOG.debug(\"No detectors found - assume only one...\")\n                    num_of_det = 1\n                    no_detectors_message = True\n\n                for i in range(1, num_of_det + 1):\n                    dname = 'det-{0:d}'.format(i)\n                    self.rsr[bandname][dname] = {}\n                    try:\n                        resp = h5f[bandname][dname]['response'][:]\n                    except KeyError:\n                        resp = h5f[bandname]['response'][:]\n\n                    self.rsr[bandname][dname]['response'] = resp\n\n                    try:\n                        wvl = (h5f[bandname][dname]['wavelength'][:] *\n                               h5f[bandname][dname][\n                                   'wavelength'].attrs['scale'])\n                    except KeyError:\n                        wvl = (h5f[bandname]['wavelength'][:] *\n                               h5f[bandname]['wavelength'].attrs['scale'])\n\n                    # The wavelength is given in micro meters!\n                    self.rsr[bandname][dname]['wavelength'] = wvl * 1e6\n\n                    try:\n                        central_wvl = h5f[bandname][\n                            dname].attrs['central_wavelength']\n                    except KeyError:\n                        central_wvl = h5f[bandname].attrs['central_wavelength']\n\n                    self.rsr[bandname][dname][\n                        'central_wavelength'] = central_wvl"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef integral(self, bandname):\n        intg = {}\n        for det in self.rsr[bandname].keys():\n            wvl = self.rsr[bandname][det]['wavelength']\n            resp = self.rsr[bandname][det]['response']\n            intg[det] = np.trapz(resp, wvl)\n        return intg", "response": "Calculate the integral of the spectral response function for each detector."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert spectral response functions from wavelength to wavenumber", "response": "def convert(self):\n        \"\"\"Convert spectral response functions from wavelength to wavenumber\"\"\"\n\n        from pyspectral.utils import (convert2wavenumber, get_central_wave)\n        if self._wavespace == WAVE_LENGTH:\n            rsr, info = convert2wavenumber(self.rsr)\n            for band in rsr.keys():\n                for det in rsr[band].keys():\n                    self.rsr[band][det][WAVE_NUMBER] = rsr[\n                        band][det][WAVE_NUMBER]\n                    self.rsr[band][det]['response'] = rsr[\n                        band][det]['response']\n                    self.unit = info['unit']\n                    self.si_scale = info['si_scale']\n            self._wavespace = WAVE_NUMBER\n            for band in rsr.keys():\n                for det in rsr[band].keys():\n                    self.rsr[band][det]['central_wavenumber'] = \\\n                        get_central_wave(self.rsr[band][det][WAVE_NUMBER], self.rsr[band][det]['response'])\n                    del self.rsr[band][det][WAVE_LENGTH]\n        else:\n            errmsg = \"Conversion from {wn} to {wl} not supported yet\".format(wn=WAVE_NUMBER, wl=WAVE_LENGTH)\n            raise NotImplementedError(errmsg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_options_from_config(self):\n\n        options = get_config()\n        self.output_dir = options.get('rsr_dir', './')\n        self.path = options[self.platform_name + '-' + self.instrument]['path']\n        self.options = options", "response": "Get configuration settings from configuration file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_bandfilenames(self):\n        for band in self.bandnames:\n            LOG.debug(\"Band = %s\", str(band))\n            self.filenames[band] = os.path.join(self.path,\n                                                self.options[self.platform_name + '-' +\n                                                             self.instrument][band])\n            LOG.debug(self.filenames[band])\n            if not os.path.exists(self.filenames[band]):\n                LOG.warning(\"Couldn't find an existing file for this band: %s\",\n                            str(self.filenames[band]))", "response": "Get the instrument rsr filenames"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blackbody_rad2temp(wavelength, radiance):\n\n    mask = False\n    if np.isscalar(radiance):\n        rad = np.array([radiance, ], dtype='float64')\n    else:\n        rad = np.array(radiance, dtype='float64')\n        if np.ma.is_masked(radiance):\n            mask = radiance.mask\n\n    rad = np.ma.masked_array(rad, mask=mask)\n    rad = np.ma.masked_less_equal(rad, 0)\n\n    if np.isscalar(wavelength):\n        wvl = np.array([wavelength, ], dtype='float64')\n    else:\n        wvl = np.array(wavelength, dtype='float64')\n\n    const1 = H_PLANCK * C_SPEED / K_BOLTZMANN\n    const2 = 2 * H_PLANCK * C_SPEED**2\n    res = const1 / (wvl * np.log(np.divide(const2, rad * wvl**5) + 1.0))\n\n    shape = rad.shape\n    resshape = res.shape\n\n    if wvl.shape[0] == 1:\n        if rad.shape[0] == 1:\n            return res[0]\n        else:\n            return res[::].reshape(shape)\n    else:\n        if rad.shape[0] == 1:\n            return res[0, :]\n        else:\n            if len(shape) == 1:\n                return np.reshape(res, (shape[0], resshape[1]))\n            else:\n                return np.reshape(res, (shape[0], shape[1], resshape[1]))", "response": "Derive brightness temperatures from radiance using the Planck\n    function. Wavelength space assumes SI units as input and returns a Kelvin array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blackbody_wn_rad2temp(wavenumber, radiance):\n\n    if np.isscalar(radiance):\n        rad = np.array([radiance, ], dtype='float64')\n    else:\n        rad = np.array(radiance, dtype='float64')\n    if np.isscalar(wavenumber):\n        wavnum = np.array([wavenumber, ], dtype='float64')\n    else:\n        wavnum = np.array(wavenumber, dtype='float64')\n\n    const1 = H_PLANCK * C_SPEED / K_BOLTZMANN\n    const2 = 2 * H_PLANCK * C_SPEED**2\n    res = const1 * wavnum / np.log(np.divide(const2 * wavnum**3, rad) + 1.0)\n\n    shape = rad.shape\n    resshape = res.shape\n\n    if wavnum.shape[0] == 1:\n        if rad.shape[0] == 1:\n            return res[0]\n        else:\n            return res[::].reshape(shape)\n    else:\n        if rad.shape[0] == 1:\n            return res[0, :]\n        else:\n            if len(shape) == 1:\n                return np.reshape(res, (shape[0], resshape[1]))\n            else:\n                return np.reshape(res, (shape[0], shape[1], resshape[1]))", "response": "Derive brightness temperatures from radiance using the Planck \n    function. Wavenumber space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_central_wave(wavl, resp):\n    return np.trapz(resp * wavl, wavl) / np.trapz(resp, wavl)", "response": "Calculate the central wavelength or the central wavenumber depending on what is input\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_seviri_file(seviri, platform_name):\n    import h5py\n\n    filename = os.path.join(seviri.output_dir,\n                            \"rsr_seviri_{0}.h5\".format(platform_name))\n\n    sat_name = platform_name\n    with h5py.File(filename, \"w\") as h5f:\n\n        h5f.attrs['description'] = 'Relative Spectral Responses for SEVIRI'\n        h5f.attrs['platform_name'] = platform_name\n        bandlist = [str(key) for key in seviri.rsr.keys()]\n        h5f.attrs['band_names'] = bandlist\n\n        for key in seviri.rsr.keys():\n            grp = h5f.create_group(key)\n            if isinstance(seviri.central_wavelength[key][sat_name], dict):\n                grp.attrs['central_wavelength'] = \\\n                    seviri.central_wavelength[key][sat_name]['95']\n            else:\n                grp.attrs['central_wavelength'] = \\\n                    seviri.central_wavelength[key][sat_name]\n            arr = seviri.rsr[key]['wavelength']\n            dset = grp.create_dataset('wavelength', arr.shape, dtype='f')\n            dset.attrs['unit'] = 'm'\n            dset.attrs['scale'] = 1e-06\n            dset[...] = arr\n            try:\n                arr = seviri.rsr[key][sat_name]['95']\n            except ValueError:\n                arr = seviri.rsr[key][sat_name]\n            dset = grp.create_dataset('response', arr.shape, dtype='f')\n            dset[...] = arr\n\n    return", "response": "Generate the pyspectral internal common format relative response for one SEVIRI"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    sev_obj = Seviri()\n\n    for satnum in [8, 9, 10, 11]:\n        generate_seviri_file(sev_obj, 'Meteosat-{0:d}'.format(satnum))\n        print(\"Meteosat-{0:d} done...\".format(satnum))", "response": "Generate Seviri files for all the meteosats"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the SEVIRI rsr data from a file.", "response": "def _load(self, filename=None):\n        \"\"\"Read the SEVIRI rsr data\"\"\"\n        if not filename:\n            filename = self.seviri_path\n\n        wb_ = open_workbook(filename)\n\n        self.rsr = {}\n        sheet_names = []\n        for sheet in wb_.sheets():\n            if sheet.name in ['Info', 'Requirements']:\n                continue\n            ch_name = sheet.name.strip()\n            sheet_names.append(sheet.name.strip())\n\n            self.rsr[ch_name] = {'wavelength': None,\n                                 'Meteosat-8': None,\n                                 'Meteosat-9': None,\n                                 'Meteosat-10': None,\n                                 'Meteosat-11': None}\n\n            if ch_name.startswith('HRV'):\n                wvl = np.array(\n                    sheet.col_values(0, start_rowx=37, end_rowx=137))\n                # TODO: Add the 'extended' responses as well!\n                met8 = np.array(\n                    sheet.col_values(1, start_rowx=37, end_rowx=137))\n                met9 = np.array(\n                    sheet.col_values(3, start_rowx=37, end_rowx=137))\n                met10 = np.array(\n                    sheet.col_values(5, start_rowx=37, end_rowx=137))\n                met11 = np.array(\n                    sheet.col_values(6, start_rowx=37, end_rowx=137))\n                self.rsr[ch_name]['Meteosat-8'] = met8\n                self.rsr[ch_name]['Meteosat-9'] = met9\n                self.rsr[ch_name]['Meteosat-10'] = met10\n                self.rsr[ch_name]['Meteosat-11'] = met11\n            elif ch_name.startswith('IR'):\n                wvl = np.array(\n                    sheet.col_values(0, start_rowx=13, end_rowx=113))\n                met8_95 = np.array(\n                    sheet.col_values(1, start_rowx=13, end_rowx=113))\n                met9_95 = np.array(\n                    sheet.col_values(3, start_rowx=13, end_rowx=113))\n                met10_95 = np.array(\n                    sheet.col_values(5, start_rowx=13, end_rowx=113))\n                met11_95 = np.array(\n                    sheet.col_values(7, start_rowx=13, end_rowx=113))\n                met8_85 = np.array(\n                    sheet.col_values(2, start_rowx=13, end_rowx=113))\n                met9_85 = np.array(\n                    sheet.col_values(4, start_rowx=13, end_rowx=113))\n                met10_85 = np.array(\n                    sheet.col_values(6, start_rowx=13, end_rowx=113))\n                met11_85 = np.array(\n                    sheet.col_values(8, start_rowx=13, end_rowx=113))\n                self.rsr[ch_name]['Meteosat-8'] = {'95': met8_95,\n                                                   '85': met8_85}\n                self.rsr[ch_name]['Meteosat-9'] = {'95': met9_95,\n                                                   '85': met9_85}\n                self.rsr[ch_name]['Meteosat-10'] = {'95': met10_95,\n                                                    '85': met10_85}\n                self.rsr[ch_name]['Meteosat-11'] = {'95': met11_95,\n                                                    '85': met11_85}\n            else:\n                wvl = np.array(\n                    sheet.col_values(0, start_rowx=12, end_rowx=112))\n                met8 = np.array(\n                    sheet.col_values(1, start_rowx=12, end_rowx=112))\n                met9 = np.array(\n                    sheet.col_values(2, start_rowx=12, end_rowx=112))\n                met10 = np.array(\n                    sheet.col_values(3, start_rowx=12, end_rowx=112))\n                met11 = np.array(\n                    sheet.col_values(4, start_rowx=12, end_rowx=112))\n                self.rsr[ch_name]['Meteosat-8'] = met8\n                self.rsr[ch_name]['Meteosat-9'] = met9\n                self.rsr[ch_name]['Meteosat-10'] = met10\n                self.rsr[ch_name]['Meteosat-11'] = met11\n\n            self.rsr[ch_name]['wavelength'] = wvl"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting from wavelengths to wavenumber", "response": "def convert2wavenumber(self):\n        \"\"\"Convert from wavelengths to wavenumber\"\"\"\n        for chname in self.rsr.keys():\n            elems = [k for k in self.rsr[chname].keys()]\n            for sat in elems:\n                if sat == \"wavelength\":\n                    LOG.debug(\"Get the wavenumber from the wavelength: sat=%s chname=%s\", sat, chname)\n                    wnum = 1. / (1e-4 * self.rsr[chname][sat][:])  # microns to cm\n                    self.rsr[chname]['wavenumber'] = wnum[::-1]\n                else:\n                    if type(self.rsr[chname][sat]) is dict:\n                        for name in self.rsr[chname][sat].keys():\n                            resp = self.rsr[chname][sat][name][:]\n                            self.rsr[chname][sat][name] = resp[::-1]\n                    else:\n                        resp = self.rsr[chname][sat][:]\n                        self.rsr[chname][sat] = resp[::-1]\n\n        for chname in self.rsr.keys():\n            del self.rsr[chname]['wavelength']\n\n        self.unit = 'cm-1'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the central wavenumbers or central wavelengths of all channels depending on the given wavespace", "response": "def get_centrals(self):\n        \"\"\"Get the central wavenumbers or central wavelengths of all channels,\n        depending on the given 'wavespace'\n        \"\"\"\n        result = {}\n        for chname in self.rsr.keys():\n            result[chname] = {}\n            if self.wavespace == \"wavelength\":\n                x__ = self.rsr[chname][\"wavelength\"]\n            else:\n                x__ = self.rsr[chname][\"wavenumber\"]\n            for sat in self.rsr[chname].keys():\n                if sat in [\"wavelength\", \"wavenumber\"]:\n                    continue\n                if type(self.rsr[chname][sat]) is dict:\n                    result[chname][sat] = {}\n                    for name in self.rsr[chname][sat].keys():\n                        resp = self.rsr[chname][sat][name]\n                        result[chname][sat][name] = get_central_wave(x__, resp)\n                else:\n                    resp = self.rsr[chname][sat]\n                    result[chname][sat] = get_central_wave(x__, resp)\n\n        if self.wavespace == \"wavelength\":\n            self.central_wavelength = result\n        else:\n            self.central_wavenumber = result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the ABI relative spectral responses", "response": "def _load(self, scale=1.0):\n        \"\"\"Load the ABI relative spectral responses\n        \"\"\"\n\n        LOG.debug(\"File: %s\", str(self.requested_band_filename))\n\n        data = np.genfromtxt(self.requested_band_filename,\n                             unpack=True,\n                             names=['wavelength',\n                                    'wavenumber',\n                                    'response'],\n                             skip_header=2)\n\n        wvl = data['wavelength'] * scale\n        resp = data['response']\n\n        self.rsr = {'wavelength': wvl, 'response': resp}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert2wavenumber(self):\n        self.wavenumber = 1. / (1e-4 * self.wavelength[::-1])\n        self.irradiance = (self.irradiance[::-1] *\n                           self.wavelength[::-1] * self.wavelength[::-1] * 0.1)\n        self.wavelength = None", "response": "Convert from wavelengths to wavenumber."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load(self):\n        self.wavelength, self.irradiance = \\\n            np.genfromtxt(self.filename, unpack=True)", "response": "Read the tabulated spectral irradiance data from file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the solar constant of the class", "response": "def solar_constant(self):\n        \"\"\"Calculate the solar constant\"\"\"\n        if self.wavenumber is not None:\n            return np.trapz(self.irradiance, self.wavenumber)\n        elif self.wavelength is not None:\n            return np.trapz(self.irradiance, self.wavelength)\n        else:\n            raise TypeError('Neither wavelengths nor wavenumbers available!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inband_solarflux(self, rsr, scale=1.0, **options):\n        return self._band_calculations(rsr, True, scale, **options)", "response": "Derive the inband solar flux for a given instrument relative\n        spectral response valid for an earth - sun distance of one AU."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nderives the inband solar irradiance for a given instrument relative spectral response valid for an earth - sun distance of one AU.", "response": "def inband_solarirradiance(self, rsr, scale=1.0, **options):\n        \"\"\"Derive the inband solar irradiance for a given instrument relative\n        spectral response valid for an earth-sun distance of one AU.\"\"\"\n\n        return self._band_calculations(rsr, False, scale, **options)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _band_calculations(self, rsr, flux, scale, **options):\n        from scipy.interpolate import InterpolatedUnivariateSpline\n\n        if 'detector' in options:\n            detector = options['detector']\n        else:\n            detector = 1\n\n        # Resample/Interpolate the response curve:\n        if self.wavespace == 'wavelength':\n            if 'response' in rsr:\n                wvl = rsr['wavelength'] * scale\n                resp = rsr['response']\n            else:\n                wvl = rsr['det-{0:d}'.format(detector)]['wavelength'] * scale\n                resp = rsr['det-{0:d}'.format(detector)]['response']\n        else:\n            if 'response' in rsr:\n                wvl = rsr['wavenumber'] * scale\n                resp = rsr['response']\n            else:\n                wvl = rsr['det-{0:d}'.format(detector)]['wavenumber'] * scale\n                resp = rsr['det-{0:d}'.format(detector)]['response']\n\n        start = wvl[0]\n        end = wvl[-1]\n        # print \"Start and end: \", start, end\n        LOG.debug(\"Begin and end wavelength/wavenumber: %f %f \", start, end)\n        dlambda = self._dlambda\n        xspl = np.linspace(start, end, round((end - start) / self._dlambda) + 1)\n\n        ius = InterpolatedUnivariateSpline(wvl, resp)\n        resp_ipol = ius(xspl)\n\n        # Interpolate solar spectrum to specified resolution and over specified\n        # Spectral interval:\n        self.interpolate(dlambda=dlambda, ival_wavelength=(start, end))\n\n        # Mask out outside the response curve:\n        maskidx = np.logical_and(np.greater_equal(self.ipol_wavelength, start),\n                                 np.less_equal(self.ipol_wavelength, end))\n        wvl = np.repeat(self.ipol_wavelength, maskidx)\n        irr = np.repeat(self.ipol_irradiance, maskidx)\n\n        # Calculate the solar-flux: (w/m2)\n        if flux:\n            return np.trapz(irr * resp_ipol, wvl)\n        else:\n            # Divide by the equivalent band width:\n            return np.trapz(irr * resp_ipol, wvl) / np.trapz(resp_ipol, wvl)", "response": "Calculates the inband solar irradiance for a given frequency domain and instrument relative spectral response valid for an earth - sun distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate(self, **options):\n        from scipy.interpolate import InterpolatedUnivariateSpline\n\n        # The user defined wavelength span is not yet used:\n        # FIXME!\n        if 'ival_wavelength' in options:\n            ival_wavelength = options['ival_wavelength']\n        else:\n            ival_wavelength = None\n\n        if 'dlambda' in options:\n            self._dlambda = options['dlambda']\n\n        if ival_wavelength is None:\n            if self.wavespace == 'wavelength':\n                start = self.wavelength[0]\n                end = self.wavelength[-1]\n            else:\n                start = self.wavenumber[0]\n                end = self.wavenumber[-1]\n        else:\n            start, end = ival_wavelength\n\n        xspl = np.linspace(start, end, round((end - start) / self._dlambda) + 1)\n        if self.wavespace == 'wavelength':\n            ius = InterpolatedUnivariateSpline(\n                self.wavelength, self.irradiance)\n        else:\n            ius = InterpolatedUnivariateSpline(\n                self.wavenumber, self.irradiance)\n        yspl = ius(xspl)\n\n        self.ipol_wavelength = xspl\n        self.ipol_irradiance = yspl", "response": "Interpolate Irradiance to a specified evenly spaced resolution and grid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self, plotname=None, **options):\n        if 'color' in options:\n            color = options['color']\n        else:\n            color = 'blue'\n\n        if self.wavespace == \"wavelength\":\n            xlabel = r\"Wavelength ($\\mu m$)\"\n            ylabel = r\"Irradiance ($W/(m^2 \\mu m$))\"\n            xlim = [0, 4.2]\n            xwl, yir = self.wavelength, self.irradiance\n        elif self.wavespace == \"wavenumber\":\n            xlabel = \"Wavenumber ($cm^{-1}$)\"\n            ylabel = \"Irradiance ($mW/m^2 (cm^{-1})^{-1}$))\"\n            xlim = [0, 35000]\n            xwl, yir = self.wavenumber, self.irradiance\n        else:\n            raise TypeError('Neither wavelengths nor wavenumbers available!')\n\n        from matplotlib import pyplot as plt\n        from matplotlib import rcParams\n        rcParams['text.usetex'] = True\n        rcParams['text.latex.unicode'] = True\n\n        fig, axl = plt.subplots(figsize=(8, 4))\n        plot_title = \"Solar Irradiance Spectrum\"\n        axl.set_title(plot_title)\n\n        axl.plot(xwl, yir, '-', color=color)\n\n        axl.set_xlabel(xlabel)\n        axl.set_ylabel(ylabel)\n        axl.set_xlim(xlim)\n        axl.set_ylim([0, yir.max()])\n        axl.grid(True)\n\n        if plotname is None:\n            plt.show()\n        else:\n            fig.savefig(plotname)", "response": "Plot the data of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef viewzen_corr(data, view_zen):\n    def ratio(value, v_null, v_ref):\n        return (value - v_null) / (v_ref - v_null)\n\n    def tau0(t):\n        T_0 = 210.0\n        T_REF = 320.0\n        TAU_REF = 9.85\n        return (1 + TAU_REF)**ratio(t, T_0, T_REF) - 1\n\n    def tau(t):\n        T_0 = 170.0\n        T_REF = 295.0\n        TAU_REF = 1.0\n        M = 4\n        return TAU_REF * ratio(t, T_0, T_REF)**M\n\n    def delta(z):\n        Z_0 = 0.0\n        Z_REF = 70.0\n        DELTA_REF = 6.2\n        return (1 + DELTA_REF)**ratio(z, Z_0, Z_REF) - 1\n\n    y0, x0 = np.ma.where(view_zen == 0)\n    data[y0, x0] += tau0(data[y0, x0])\n\n    y, x = np.ma.where((view_zen > 0) & (view_zen < 90) & (~data.mask))\n    data[y, x] += tau(data[y, x]) * delta(view_zen[y, x])\n    return data", "response": "Apply atmospheric correction on the given data using the satellite zenith angles view_zen."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert2hdf5(platform_name):\n\n    import h5py\n\n    ahi = AhiRSR(platform_name)\n    filename = os.path.join(ahi.output_dir,\n                            \"rsr_ahi_{platform}.h5\".format(platform=platform_name))\n\n    with h5py.File(filename, \"w\") as h5f:\n        h5f.attrs['description'] = 'Relative Spectral Responses for AHI'\n        h5f.attrs['platform_name'] = platform_name\n        h5f.attrs['sensor'] = 'ahi'\n        h5f.attrs['band_names'] = AHI_BAND_NAMES.values()\n\n        for chname in AHI_BAND_NAMES.values():\n\n            grp = h5f.create_group(chname)\n            wvl = ahi.rsr[chname][\n                'wavelength'][~np.isnan(ahi.rsr[chname]['wavelength'])]\n            rsp = ahi.rsr[chname][\n                'response'][~np.isnan(ahi.rsr[chname]['wavelength'])]\n            grp.attrs['central_wavelength'] = get_central_wave(wvl, rsp)\n            arr = ahi.rsr[chname]['wavelength']\n            dset = grp.create_dataset('wavelength', arr.shape, dtype='f')\n            dset.attrs['unit'] = 'm'\n            dset.attrs['scale'] = 1e-06\n            dset[...] = arr\n            arr = ahi.rsr[chname]['response']\n            dset = grp.create_dataset('response', arr.shape, dtype='f')\n            dset[...] = arr", "response": "Retrieve original RSR data and convert to internal hdf5 format"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the Himawari AHI RSR data for the band requested", "response": "def _load(self, filename=None):\n        \"\"\"Load the Himawari AHI RSR data for the band requested\n        \"\"\"\n        if not filename:\n            filename = self.filename\n\n        wb_ = open_workbook(filename)\n        self.rsr = {}\n        sheet_names = []\n        for sheet in wb_.sheets():\n            if sheet.name in ['Title', ]:\n                continue\n            ch_name = AHI_BAND_NAMES.get(\n                sheet.name.strip(), sheet.name.strip())\n            sheet_names.append(sheet.name.strip())\n            self.rsr[ch_name] = {'wavelength': None,\n                                 'response': None}\n            wvl = np.array(\n                sheet.col_values(0, start_rowx=5, end_rowx=5453))\n            resp = np.array(\n                sheet.col_values(2, start_rowx=5, end_rowx=5453))\n            self.rsr[ch_name]['wavelength'] = wvl\n            self.rsr[ch_name]['response'] = resp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_band(plt_in, band_name, rsr_obj, **kwargs):\n    if 'platform_name_in_legend' in kwargs:\n        platform_name_in_legend = kwargs['platform_name_in_legend']\n    else:\n        platform_name_in_legend = False\n\n    detectors = rsr_obj.rsr[band_name].keys()\n    # for det in detectors:\n    det = sorted(detectors)[0]\n    resp = rsr_obj.rsr[band_name][det]['response']\n    wvl = rsr_obj.rsr[band_name][det]['wavelength']\n\n    resp = np.ma.masked_less_equal(resp, minimum_response)\n    wvl = np.ma.masked_array(wvl, resp.mask)\n    resp.compressed()\n    wvl.compressed()\n\n    if platform_name_in_legend:\n        plt_in.plot(wvl, resp, label='{platform} - {band}'.format(\n            platform=rsr_obj.platform_name, band=band_name))\n    else:\n        plt_in.plot(wvl, resp, label='{band}'.format(band=band_name))\n\n    return plt_in", "response": "Do the plotting of one band"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the command line arguments for the daemon.", "response": "def get_arguments():\n    \"\"\"Get the command line arguments\"\"\"\n    import argparse\n    parser = argparse.ArgumentParser(\n        description='Plot spectral responses for a set of satellite imagers')\n\n    parser.add_argument(\"--platform_name\", '-p', nargs='*',\n                        help=\"The Platform name\",\n                        type=str, required=True)\n    parser.add_argument(\"--sensor\", '-s', nargs='*',\n                        help=\"The sensor/instrument name\",\n                        type=str, required=True)\n    parser.add_argument(\"-x\", \"--xlimits\", nargs=2,\n                        help=(\"x-axis boundaries for plot\"),\n                        default=None, type=float)\n    parser.add_argument(\"-y\", \"--ylimits\", nargs=2,\n                        help=(\"y-axis boundaries for plot\"),\n                        default=None, type=float)\n    parser.add_argument(\"-t\", \"--minimum_response\",\n                        help=(\"Minimum response: Any response lower than \" +\n                              \"this will be ignored when plotting\"),\n                        default=0.015, type=float)\n\n    parser.add_argument(\"-no_platform_name_in_legend\", help=(\"No platform name in legend\"),\n                        action='store_true')\n    parser.add_argument(\"--title\", help=(\"Plot title\"),\n                        default=None, type=str)\n    parser.add_argument(\"--wavelength_resolution\",\n                        help=(\"The step in wavelength (nanometers) when scanning\\n\" +\n                              \" the spectral range trying to find bands\"),\n                        default=0.005, type=float)\n    parser.add_argument(\"-o\", \"--filename\", help=(\"Output plot file name\"),\n                        default=None, type=str)\n    parser.add_argument(\n        \"-v\", '--verbose', help=(\"Turn logging on\"), action='store_true')\n\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--bandname\", '-b',\n                       help=\"The sensor band name\", type=str)\n    group.add_argument(\"--wavelength\", \"-w\", type=float,\n                       help='the approximate spectral wavelength in micron')\n    group.add_argument(\"--range\", \"-r\", nargs='*',\n                       help=\"The wavelength range for the plot\",\n                       default=[None, None], type=float)\n\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert2wavenumber(rsr):\n\n    retv = {}\n    for chname in rsr.keys():  # Go through bands/channels\n        retv[chname] = {}\n        for det in rsr[chname].keys():  # Go through detectors\n            retv[chname][det] = {}\n            if 'wavenumber' in rsr[chname][det].keys():\n                # Make a copy. Data are already in wave number space\n                retv[chname][det] = rsr[chname][det].copy()\n                LOG.debug(\"RSR data already in wavenumber space. No conversion needed.\")\n                continue\n\n            for sat in rsr[chname][det].keys():\n                if sat == \"wavelength\":\n                    # micro meters to cm\n                    wnum = 1. / (1e-4 * rsr[chname][det][sat])\n                    retv[chname][det]['wavenumber'] = wnum[::-1]\n                elif sat == \"response\":\n                    # Flip the response array:\n                    if type(rsr[chname][det][sat]) is dict:\n                        retv[chname][det][sat] = {}\n                        for name in rsr[chname][det][sat].keys():\n                            resp = rsr[chname][det][sat][name]\n                            retv[chname][det][sat][name] = resp[::-1]\n                    else:\n                        resp = rsr[chname][det][sat]\n                        retv[chname][det][sat] = resp[::-1]\n\n    unit = 'cm-1'\n    si_scale = 100.0\n    return retv, {'unit': unit, 'si_scale': si_scale}", "response": "Convert the rsr data set to a set of wavelengths and normalised responses and the corresponding wavelengths and normalised responses and the corresponding wavelengths and normalised responses and the corresponding wavelengths and responses in the wave number space."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the central wavelength or central wavenumber depending on the parameters.", "response": "def get_central_wave(wav, resp, weight=1.0):\n    \"\"\"Calculate the central wavelength or the central wavenumber, depending on\n    which parameters is input.  On default the weighting funcion is\n    f(lambda)=1.0, but it is possible to add a custom weight, e.g. f(lambda) =\n    1./lambda**4 for Rayleigh scattering calculations\n\n    \"\"\"\n\n    # info: {'unit': unit, 'si_scale': si_scale}\n    # To get the wavelenght/wavenumber in SI units (m or m-1):\n    # wav = wav * info['si_scale']\n\n    # res = np.trapz(resp*wav, wav) / np.trapz(resp, wav)\n    # Check if it is a wavelength or a wavenumber and convert to microns or cm-1:\n    # This should perhaps be user defined!?\n    # if info['unit'].find('-1') > 0:\n    # Wavenumber:\n    #     res *=\n\n    return np.trapz(resp * wav * weight, wav) / np.trapz(resp * weight, wav)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the bandname from h5 rsr provided the approximate wavelength.", "response": "def get_bandname_from_wavelength(sensor, wavelength, rsr, epsilon=0.1, multiple_bands=False):\n    \"\"\"Get the bandname from h5 rsr provided the approximate wavelength.\"\"\"\n    # channel_list = [channel for channel in rsr.rsr if abs(\n    # rsr.rsr[channel]['det-1']['central_wavelength'] - wavelength) < epsilon]\n\n    chdist_min = 2.0\n    chfound = []\n    for channel in rsr:\n        chdist = abs(\n            rsr[channel]['det-1']['central_wavelength'] - wavelength)\n        if chdist < chdist_min and chdist < epsilon:\n            chfound.append(BANDNAMES.get(sensor, BANDNAMES['generic']).get(channel, channel))\n\n    if len(chfound) == 1:\n        return chfound[0]\n    elif len(chfound) > 1:\n        bstrlist = ['band={}'.format(b) for b in chfound]\n        if not multiple_bands:\n            raise AttributeError(\"More than one band found with that wavelength! {}\".format(str(bstrlist)))\n        else:\n            LOG.debug(\"More than one band found with requested wavelength: %s\", str(bstrlist))\n        return chfound\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort_data(x_vals, y_vals):\n    # Sort data\n    idxs = np.argsort(x_vals)\n    x_vals = x_vals[idxs]\n    y_vals = y_vals[idxs]\n\n    # De-duplicate data\n    mask = np.r_[True, (np.diff(x_vals) > 0)]\n    if not mask.all():\n        # what is this for?\n        numof_duplicates = np.repeat(mask, np.equal(mask, False)).shape[0]\n        del numof_duplicates\n    x_vals = x_vals[mask]\n    y_vals = y_vals[mask]\n\n    return x_vals, y_vals", "response": "Sort the data so that x is monotonically increasing and contains\n    no duplicates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert2hdf5(ClassIn, platform_name, bandnames, scale=1e-06):\n    import h5py\n\n    instr = ClassIn(bandnames[0], platform_name)\n    instr_name = instr.instrument.replace('/', '')\n    filename = os.path.join(instr.output_dir,\n                            \"rsr_{0}_{1}.h5\".format(instr_name,\n                                                    platform_name))\n\n    with h5py.File(filename, \"w\") as h5f:\n        h5f.attrs['description'] = ('Relative Spectral Responses for ' +\n                                    instr.instrument.upper())\n        h5f.attrs['platform_name'] = platform_name\n        h5f.attrs['band_names'] = bandnames\n\n        for chname in bandnames:\n            sensor = ClassIn(chname, platform_name)\n            grp = h5f.create_group(chname)\n            wvl = sensor.rsr['wavelength'][~np.isnan(sensor.rsr['wavelength'])]\n            rsp = sensor.rsr['response'][~np.isnan(sensor.rsr['wavelength'])]\n            grp.attrs['central_wavelength'] = get_central_wave(wvl, rsp)\n            arr = sensor.rsr['wavelength']\n            dset = grp.create_dataset('wavelength', arr.shape, dtype='f')\n            dset.attrs['unit'] = 'm'\n            dset.attrs['scale'] = scale\n            dset[...] = arr\n            arr = sensor.rsr['response']\n            dset = grp.create_dataset('response', arr.shape, dtype='f')\n            dset[...] = arr", "response": "Retrieve original RSR data and convert to internal hdf5 format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download_rsr(**kwargs):\n\n    #\n    import tarfile\n    import requests\n    TQDM_LOADED = True\n    try:\n        from tqdm import tqdm\n    except ImportError:\n        TQDM_LOADED = False\n\n    dest_dir = kwargs.get('dest_dir', LOCAL_RSR_DIR)\n    dry_run = kwargs.get('dry_run', False)\n\n    LOG.info(\"Download RSR files and store in directory %s\", dest_dir)\n\n    filename = os.path.join(dest_dir, \"pyspectral_rsr_data.tgz\")\n    LOG.debug(\"Get data. URL: %s\", HTTP_PYSPECTRAL_RSR)\n    LOG.debug(\"Destination = %s\", dest_dir)\n    if dry_run:\n        return\n\n    response = requests.get(HTTP_PYSPECTRAL_RSR)\n    if TQDM_LOADED:\n        with open(filename, \"wb\") as handle:\n            for data in tqdm(response.iter_content()):\n                handle.write(data)\n    else:\n        with open(filename, \"wb\") as handle:\n            for data in response.iter_content():\n                handle.write(data)\n\n    tar = tarfile.open(filename)\n    tar.extractall(dest_dir)\n    tar.close()\n    os.remove(filename)", "response": "Download the pre - compiled hdf5 formatet relative spectral response functions\n    from the internet and store it in a local directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_luts(**kwargs):\n    #\n    import tarfile\n    import requests\n    TQDM_LOADED = True\n    try:\n        from tqdm import tqdm\n    except ImportError:\n        TQDM_LOADED = False\n\n    dry_run = kwargs.get('dry_run', False)\n\n    if 'aerosol_type' in kwargs:\n        if isinstance(kwargs['aerosol_type'], (list, tuple, set)):\n            aerosol_types = kwargs['aerosol_type']\n        else:\n            aerosol_types = [kwargs['aerosol_type'], ]\n    else:\n        aerosol_types = HTTPS_RAYLEIGH_LUTS.keys()\n\n    chunk_size = 10124\n\n    for subname in aerosol_types:\n\n        LOG.debug('Aerosol type: %s', subname)\n        http = HTTPS_RAYLEIGH_LUTS[subname]\n        LOG.debug('URL = %s', http)\n\n        subdir_path = RAYLEIGH_LUT_DIRS[subname]\n        try:\n            LOG.debug('Create directory: %s', subdir_path)\n            if not dry_run:\n                os.makedirs(subdir_path)\n        except OSError:\n            if not os.path.isdir(subdir_path):\n                raise\n\n        if dry_run:\n            continue\n\n        response = requests.get(http)\n        total_size = int(response.headers['content-length'])\n\n        filename = os.path.join(\n            subdir_path, \"pyspectral_rayleigh_correction_luts.tgz\")\n        if TQDM_LOADED:\n            with open(filename, \"wb\") as handle:\n                for data in tqdm(iterable=response.iter_content(chunk_size=chunk_size),\n                                 total=(total_size / chunk_size), unit='kB'):\n                    handle.write(data)\n        else:\n            with open(filename, \"wb\") as handle:\n                for data in response.iter_content():\n                    handle.write(data)\n\n        tar = tarfile.open(filename)\n        tar.extractall(subdir_path)\n        tar.close()\n        os.remove(filename)", "response": "Download the luts from internet."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a logger with null handle", "response": "def get_logger(name):\n    \"\"\"Return logger with null handle\n    \"\"\"\n\n    log = logging.getLogger(name)\n    if not log.handlers:\n        log.addHandler(NullHandler())\n    return log"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the AATSR rsr data from the file", "response": "def _load(self, filename=None):\n        \"\"\"Read the AATSR rsr data\"\"\"\n        if not filename:\n            filename = self.aatsr_path\n\n        wb_ = open_workbook(filename)\n\n        for sheet in wb_.sheets():\n            ch_name = sheet.name.strip()\n            if ch_name == 'aatsr_' + self.bandname:\n\n                data = np.array([s.split() for s in\n                                 sheet.col_values(0,\n                                                  start_rowx=3, end_rowx=258)])\n                data = data.astype('f')\n                wvl = data[:, 0]\n                resp = data[:, 1]\n\n                self.rsr = {'wavelength': wvl, 'response': resp}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    bands = OLI_BAND_NAMES.values()\n    bands.sort()\n    for platform_name in ['Landsat-8', ]:\n        tohdf5(OliRSR, platform_name, bands)", "response": "Main function for the main function of the main function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the Landsat OLI relative spectral responses", "response": "def _load(self, scale=0.001):\n        \"\"\"Load the Landsat OLI relative spectral responses\n        \"\"\"\n\n        with open_workbook(self.path) as wb_:\n            for sheet in wb_.sheets():\n                if sheet.name in ['Plot of AllBands', ]:\n                    continue\n                ch_name = OLI_BAND_NAMES.get(sheet.name.strip())\n\n                if ch_name != self.bandname:\n                    continue\n\n                wvl = sheet.col_values(0, 2)\n                resp = sheet.col_values(1, 2)\n\n                self.rsr = {'wavelength': np.array(wvl) / 1000.,\n                            'response': np.array(resp)}\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_metimage_file(platform_name):\n    import h5py\n\n    bandnames = METIMAGE_BAND_NAMES\n    instr = MetImageRSR(bandnames[0], platform_name)\n    instr_name = instr.instrument.replace('/', '')\n    filename = os.path.join(instr.output_dir,\n                            \"rsr_{0}_{1}.h5\".format(instr_name,\n                                                    platform_name))\n\n    with h5py.File(filename, \"w\") as h5f:\n        h5f.attrs['description'] = ('Relative Spectral Responses for ' +\n                                    instr.instrument.upper())\n        h5f.attrs['platform_name'] = platform_name\n        h5f.attrs['band_names'] = bandnames\n\n        for chname in bandnames:\n            metimage = MetImageRSR(chname, platform_name)\n            grp = h5f.create_group(chname)\n            grp.attrs['number_of_detectors'] = len(metimage.rsr.keys())\n            # Loop over each detector to check if the sampling wavelengths are\n            # identical:\n            det_names = list(metimage.rsr.keys())\n            wvl = metimage.rsr[det_names[0]]['wavelength']\n            wvl, idx = np.unique(wvl, return_index=True)\n            wvl_is_constant = True\n            for det in det_names[1:]:\n                det_wvl = np.unique(metimage.rsr[det]['wavelength'])\n                if not np.alltrue(wvl == det_wvl):\n                    LOG.warning(\n                        \"Wavelngth arrays are not the same among detectors!\")\n                    wvl_is_constant = False\n\n            if wvl_is_constant:\n                arr = wvl\n                dset = grp.create_dataset('wavelength', arr.shape, dtype='f')\n                dset.attrs['unit'] = 'm'\n                dset.attrs['scale'] = 1e-06\n                dset[...] = arr\n\n            # Loop over each detector:\n            for det in metimage.rsr:\n                det_grp = grp.create_group(det)\n                wvl = metimage.rsr[det]['wavelength'][\n                    ~np.isnan(metimage.rsr[det]['wavelength'])]\n                rsp = metimage.rsr[det]['response'][\n                    ~np.isnan(metimage.rsr[det]['wavelength'])]\n                wvl, idx = np.unique(wvl, return_index=True)\n                rsp = np.take(rsp, idx)\n                LOG.debug(\"wvl.shape: %s\", str(wvl.shape))\n                det_grp.attrs[\n                    'central_wavelength'] = get_central_wave(wvl, rsp)\n                if not wvl_is_constant:\n                    arr = wvl\n                    dset = det_grp.create_dataset(\n                        'wavelength', arr.shape, dtype='f')\n                    dset.attrs['unit'] = 'm'\n                    dset.attrs['scale'] = 1e-06\n                    dset[...] = arr\n\n                dset = det_grp.create_dataset('response', rsp.shape, dtype='f')\n                dset[...] = rsp", "response": "Generate the original RSR data and convert to internal hdf5 format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load(self, scale=1.0):\n        data = np.genfromtxt(self.requested_band_filename,\n                             unpack=True,\n                             names=['wavenumber',\n                                    'response'],\n                             skip_header=4)\n\n        # Data are wavenumbers in cm-1:\n        wavelength = 1. / data['wavenumber'] * 10000.\n        response = data['response']\n\n        # The real MetImage has 24 detectors. However, for now we store the\n        # single rsr as 'detector-1', indicating that there will be multiple\n        # detectors in the future:\n        detectors = {}\n        detectors['det-1'] = {'wavelength': wavelength, 'response': response}\n        self.rsr = detectors", "response": "Load the MetImage RSR data for the requested band"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nderives the 3. 9 radiance correction factor for the emitted 3. 9 radiance by CO2 absorption band.", "response": "def derive_rad39_corr(self, bt11, bt13, method='rosenfeld'):\n        \"\"\"Derive the 3.9 radiance correction factor to account for the\n        attenuation of the emitted 3.9 radiance by CO2\n        absorption. Requires the 11 micron window band and the 13.4\n        CO2 absorption band, as e.g. available on SEVIRI. Currently\n        only supports the Rosenfeld method\n        \"\"\"\n\n        if method != 'rosenfeld':\n            raise AttributeError(\"Only CO2 correction for SEVIRI using \"\n                                 \"the Rosenfeld equation is supported!\")\n\n        LOG.debug(\"Derive the 3.9 micron radiance CO2 correction coefficent\")\n        self._rad3x_correction = (bt11 - 0.25 * (bt11 - bt13)) ** 4 / bt11 ** 4"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_solarflux(self):\n        solar_spectrum = \\\n            SolarIrradianceSpectrum(TOTAL_IRRADIANCE_SPECTRUM_2000ASTM,\n                                    dlambda=0.0005,\n                                    wavespace=self.wavespace)\n        self.solar_flux = solar_spectrum.inband_solarflux(self.rsr[self.bandname])", "response": "Derive the in - band solar flux from the rsr over the Near IR band."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef emissive_part_3x(self, tb=True):\n        try:\n            # Emissive part:\n            self._e3x = self._rad3x_t11 * (1 - self._r3x)\n            # Unsure how much sense it makes to apply the co2 correction term here!?\n            # FIXME!\n            # self._e3x *= self._rad3x_correction\n\n        except TypeError:\n            LOG.warning(\n                \"Couldn't derive the emissive part \\n\" +\n                \"Please derive the relfectance prior to requesting the emissive part\")\n\n        if tb:\n            return self.radiance2tb(self._e3x)\n        else:\n            return self._e3x", "response": "Get the emissive part of the 3. x band."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reflectance_from_tbs(self, sun_zenith, tb_near_ir, tb_thermal, **kwargs):\n        # Check for dask arrays\n        if hasattr(tb_near_ir, 'compute') or hasattr(tb_thermal, 'compute'):\n            compute = False\n        else:\n            compute = True\n        if hasattr(tb_near_ir, 'mask') or hasattr(tb_thermal, 'mask'):\n            is_masked = True\n        else:\n            is_masked = False\n\n        if np.isscalar(tb_near_ir):\n            tb_nir = np.array([tb_near_ir, ])\n        else:\n            tb_nir = np.asanyarray(tb_near_ir)\n\n        if np.isscalar(tb_thermal):\n            tb_therm = np.array([tb_thermal, ])\n        else:\n            tb_therm = np.asanyarray(tb_thermal)\n\n        if tb_therm.shape != tb_nir.shape:\n            errmsg = 'Dimensions do not match! {0} and {1}'.format(\n                str(tb_therm.shape), str(tb_nir.shape))\n            raise ValueError(errmsg)\n\n        tb_ir_co2 = kwargs.get('tb_ir_co2')\n        lut = kwargs.get('lut', self.lut)\n\n        if tb_ir_co2 is None:\n            co2corr = False\n            tbco2 = None\n        else:\n            co2corr = True\n            if np.isscalar(tb_ir_co2):\n                tbco2 = np.array([tb_ir_co2, ])\n            else:\n                tbco2 = np.asanyarray(tb_ir_co2)\n\n        if not self.rsr:\n            raise NotImplementedError(\"Reflectance calculations without \"\n                                      \"rsr not yet supported!\")\n\n        # Assume rsr is in microns!!!\n        # FIXME!\n        self._rad3x_t11 = self.tb2radiance(tb_therm, lut=lut)['radiance']\n        thermal_emiss_one = self._rad3x_t11 * self.rsr_integral\n\n        l_nir = self.tb2radiance(tb_nir, lut=lut)['radiance'] * self.rsr_integral\n\n        if thermal_emiss_one.ravel().shape[0] < 10:\n            LOG.info('thermal_emiss_one = %s', str(thermal_emiss_one))\n        if l_nir.ravel().shape[0] < 10:\n            LOG.info('l_nir = %s', str(l_nir))\n\n        sunzmask = (sun_zenith < 0.0) | (sun_zenith > 88.0)\n        sunz = where(sunzmask, 88.0, sun_zenith)\n\n        mu0 = np.cos(np.deg2rad(sunz))\n        # mu0 = np.where(np.less(mu0, 0.1), 0.1, mu0)\n        self._rad3x = l_nir\n        self._solar_radiance = self.solar_flux * mu0 / np.pi\n\n        # CO2 correction to the 3.9 radiance, only if tbs of a co2 band around\n        # 13.4 micron is provided:\n        if co2corr:\n            self.derive_rad39_corr(tb_therm, tbco2)\n            LOG.info(\"CO2 correction applied...\")\n        else:\n            self._rad3x_correction = 1.0\n\n        nomin = l_nir - thermal_emiss_one * self._rad3x_correction\n        denom = self._solar_radiance - thermal_emiss_one * self._rad3x_correction\n        data = nomin / denom\n        mask = (self._solar_radiance - thermal_emiss_one *\n                self._rad3x_correction) < EPSILON\n\n        logical_or(sunzmask, mask, out=mask)\n        logical_or(mask, np.isnan(tb_nir), out=mask)\n\n        self._r3x = where(mask, np.nan, data)\n\n        # Reflectances should be between 0 and 1, but values above 1 is\n        # perfectly possible and okay! (Multiply by 100 to get reflectances\n        # in percent)\n        if hasattr(self._r3x, 'compute') and compute:\n            res = self._r3x.compute()\n        else:\n            res = self._r3x\n        if is_masked:\n            res = np.ma.masked_array(res, mask=np.isnan(res))\n        return res", "response": "This function calculates the reflectance of a given set of IR Tb s at every pixel."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the OLCI relative spectral responses", "response": "def _load(self, scale=0.001):\n        \"\"\"Load the OLCI relative spectral responses\n        \"\"\"\n\n        ncf = Dataset(self.path, 'r')\n\n        bandnum = OLCI_BAND_NAMES.index(self.bandname)\n        # cam = 0\n        # view = 0\n        # resp = ncf.variables[\n        #     'spectral_response_function'][bandnum, cam, view, :]\n        # wvl = ncf.variables[\n        #     'spectral_response_function_wavelength'][bandnum, cam, view, :] * scale\n        resp = ncf.variables[\n            'mean_spectral_response_function'][bandnum, :]\n        wvl = ncf.variables[\n            'mean_spectral_response_function_wavelength'][bandnum, :] * scale\n\n        self.rsr = {'wavelength': wvl, 'response': resp}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the MODIS relative spectral responses from a file.", "response": "def read_modis_response(filename, scale=1.0):\n    \"\"\"Read the Terra/Aqua MODIS relative spectral responses. Be aware that\n    MODIS has several detectors (more than one) compared to e.g. AVHRR which\n    has always only one.\n    \"\"\"\n    with open(filename, \"r\") as fid:\n        lines = fid.readlines()\n\n    nodata = -99.0\n    # The IR channels seem to be in microns, whereas the short wave channels are\n    # in nanometers! For VIS/NIR scale should be 0.001\n    detectors = {}\n    for line in lines:\n        if line.find(\"#\") == 0:\n            continue\n        dummy, det_num, s_1, s_2 = line.split()\n        detector_name = 'det-{0:d}'.format(int(det_num))\n        if detector_name not in detectors:\n            detectors[detector_name] = {'wavelength': [], 'response': []}\n\n        detectors[detector_name]['wavelength'].append(float(s_1) * scale)\n        detectors[detector_name]['response'].append(float(s_2))\n\n    for key in detectors:\n        mask = np.array(detectors[key]['response']) == nodata\n        detectors[key]['response'] = np.ma.masked_array(\n            detectors[key]['response'], mask=mask).compressed()\n        detectors[key]['wavelength'] = np.ma.masked_array(\n            detectors[key]['wavelength'], mask=mask).compressed()\n\n    return detectors"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve original RSR data and convert to internal hdf5 format", "response": "def convert2hdf5(platform_name):\n    \"\"\"Retrieve original RSR data and convert to internal hdf5 format\"\"\"\n    import h5py\n\n    modis = ModisRSR('20', platform_name)\n    mfile = os.path.join(modis.output_dir,\n                         \"rsr_modis_{platform}.h5\".format(platform=platform_name))\n\n    with h5py.File(mfile, \"w\") as h5f:\n        h5f.attrs['description'] = 'Relative Spectral Responses for MODIS'\n        h5f.attrs['platform_name'] = platform_name\n        h5f.attrs['band_names'] = MODIS_BAND_NAMES\n\n        for chname in MODIS_BAND_NAMES:\n            modis = ModisRSR(chname, platform_name)\n            grp = h5f.create_group(chname)\n            grp.attrs['number_of_detectors'] = len(modis.rsr.keys())\n            # Loop over each detector to check if the sampling wavelengths are\n            # identical:\n            det_names = modis.rsr.keys()\n            wvl = modis.rsr[det_names[0]]['wavelength']\n            wvl_is_constant = True\n            for det in det_names[1:]:\n                if not np.alltrue(wvl == modis.rsr[det]['wavelength']):\n                    wvl_is_constant = False\n                    break\n\n            if wvl_is_constant:\n                arr = modis.rsr[det_names[0]]['wavelength']\n                dset = grp.create_dataset('wavelength', arr.shape, dtype='f')\n                dset.attrs['unit'] = 'm'\n                dset.attrs['scale'] = 1e-06\n                dset[...] = arr\n\n            # Loop over each detector:\n            for det in modis.rsr:\n                det_grp = grp.create_group(det)\n                wvl = modis.rsr[det]['wavelength'][\n                    ~np.isnan(modis.rsr[det]['wavelength'])]\n                rsp = modis.rsr[det]['response'][\n                    ~np.isnan(modis.rsr[det]['wavelength'])]\n                det_grp.attrs[\n                    'central_wavelength'] = get_central_wave(wvl, rsp)\n                if not wvl_is_constant:\n                    arr = modis.rsr[det]['wavelength']\n                    dset = det_grp.create_dataset(\n                        'wavelength', arr.shape, dtype='f')\n                    dset.attrs['unit'] = 'm'\n                    dset.attrs['scale'] = 1e-06\n                    dset[...] = arr\n\n                arr = modis.rsr[det]['response']\n                dset = det_grp.create_dataset('response', arr.shape, dtype='f')\n                dset[...] = arr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_bandfilenames(self):\n        path = self.path\n\n        for band in MODIS_BAND_NAMES:\n            bnum = int(band)\n            LOG.debug(\"Band = %s\", str(band))\n            if self.platform_name == 'EOS-Terra':\n                filename = os.path.join(path,\n                                        \"rsr.{0:d}.inb.final\".format(bnum))\n            else:\n                if bnum in [5, 6, 7] + range(20, 37):\n                    filename = os.path.join(\n                        path, \"{0:>02d}.tv.1pct.det\".format(bnum))\n                else:\n                    filename = os.path.join(\n                        path, \"{0:>02d}.amb.1pct.det\".format(bnum))\n\n            self.filenames[band] = filename", "response": "Get the filenames for the MODIS rsr files"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the MODIS RSR data for the requested band", "response": "def _load(self):\n        \"\"\"Load the MODIS RSR data for the band requested\"\"\"\n        if self.is_sw or self.platform_name == 'EOS-Aqua':\n            scale = 0.001\n        else:\n            scale = 1.0\n        detector = read_modis_response(self.requested_band_filename, scale)\n        self.rsr = detector\n        if self._sort:\n            self.sort()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort(self):\n        if 'wavelength' in self.rsr:\n            # Only one detector apparently:\n            self.rsr['wavelength'], self.rsr['response'] = \\\n                sort_data(self.rsr['wavelength'], self.rsr['response'])\n        else:\n            for detector_name in self.rsr:\n                (self.rsr[detector_name]['wavelength'],\n                 self.rsr[detector_name]['response']) = \\\n                    sort_data(self.rsr[detector_name]['wavelength'],\n                              self.rsr[detector_name]['response'])", "response": "Sort the data so that x is monotonically increasing and contains\n        no duplicates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    import sys\n    import h5py\n\n    handler = logging.StreamHandler(sys.stderr)\n\n    formatter = logging.Formatter(fmt=_DEFAULT_LOG_FORMAT,\n                                  datefmt=_DEFAULT_TIME_FORMAT)\n    handler.setFormatter(formatter)\n    handler.setLevel(logging.DEBUG)\n    LOG.setLevel(logging.DEBUG)\n    LOG.addHandler(handler)\n\n    platform_name = \"NOAA-20\"\n    # platform_name = \"Suomi-NPP\"\n    viirs = ViirsRSR('M1', platform_name)\n    filename = os.path.join(viirs.output_dir,\n                            \"rsr_viirs_{0}.h5\".format(platform_name))\n\n    with h5py.File(filename, \"w\") as h5f:\n        h5f.attrs['description'] = 'Relative Spectral Responses for VIIRS'\n        h5f.attrs['platform_name'] = platform_name\n        h5f.attrs['sensor'] = 'viirs'\n        h5f.attrs['band_names'] = VIIRS_BAND_NAMES\n\n        for chname in VIIRS_BAND_NAMES:\n\n            viirs = ViirsRSR(chname, platform_name)\n            grp = h5f.create_group(chname)\n            grp.attrs['number_of_detectors'] = len(viirs.rsr.keys())\n            # Loop over each detector to check if the sampling wavelengths are\n            # identical:\n            det_names = viirs.rsr.keys()\n            wvl = viirs.rsr[det_names[0]]['wavelength']\n            wvl, idx = np.unique(wvl, return_index=True)\n            wvl_is_constant = True\n            for det in det_names[1:]:\n                det_wvl = np.unique(viirs.rsr[det]['wavelength'])\n                if not np.alltrue(wvl == det_wvl):\n                    LOG.warning(\n                        \"Wavelngth arrays are not the same among detectors!\")\n                    wvl_is_constant = False\n\n            if wvl_is_constant:\n                arr = wvl\n                dset = grp.create_dataset('wavelength', arr.shape, dtype='f')\n                dset.attrs['unit'] = 'm'\n                dset.attrs['scale'] = 1e-06\n                dset[...] = arr\n\n            # Loop over each detector:\n            for det in viirs.rsr:\n                det_grp = grp.create_group(det)\n                wvl = viirs.rsr[det]['wavelength'][\n                    ~np.isnan(viirs.rsr[det]['wavelength'])]\n                rsp = viirs.rsr[det]['response'][\n                    ~np.isnan(viirs.rsr[det]['wavelength'])]\n                wvl, idx = np.unique(wvl, return_index=True)\n                rsp = np.take(rsp, idx)\n                LOG.debug(\"wvl.shape: %s\", str(wvl.shape))\n                det_grp.attrs[\n                    'central_wavelength'] = get_central_wave(wvl, rsp)\n                if not wvl_is_constant:\n                    arr = wvl\n                    dset = det_grp.create_dataset(\n                        'wavelength', arr.shape, dtype='f')\n                    dset.attrs['unit'] = 'm'\n                    dset.attrs['scale'] = 1e-06\n                    dset[...] = arr\n\n                dset = det_grp.create_dataset('response', rsp.shape, dtype='f')\n                dset[...] = rsp", "response": "Main function for the base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget filename for each band", "response": "def _get_bandfilenames(self, **options):\n        \"\"\"Get filename for each band\"\"\"\n        conf = options[self.platform_name + '-viirs']\n\n        rootdir = conf['rootdir']\n        for section in conf:\n            if not section.startswith('section'):\n                continue\n            bandnames = conf[section]['bands']\n            for band in bandnames:\n                filename = os.path.join(rootdir, conf[section]['filename'])\n                self.bandfilenames[band] = compose(\n                    filename, {'bandname': band})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_bandfile(self, **options):\n\n        # Need to understand why there are A&B files for band M16. FIXME!\n        # Anyway, the absolute response differences are small, below 0.05\n\n        # LOG.debug(\"paths = %s\", str(self.bandfilenames))\n\n        path = self.bandfilenames[self.bandname]\n        if not os.path.exists(path):\n            raise IOError(\"Couldn't find an existing file for this band ({band}): {path}\".format(\n                band=self.bandname, path=path))\n\n        self.filename = path\n        return", "response": "Get the VIIRS rsr filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the VIIRS RSR data for the band requested", "response": "def _load(self, scale=0.001):\n        \"\"\"Load the VIIRS RSR data for the band requested\"\"\"\n        if self.bandname == 'DNB':\n            header_lines_to_skip = N_HEADER_LINES_DNB[self.platform_name]\n        else:\n            header_lines_to_skip = N_HEADER_LINES[self.platform_name]\n\n        try:\n            data = np.genfromtxt(self.filename,\n                                 unpack=True, skip_header=header_lines_to_skip,\n                                 names=NAMES1[self.platform_name],\n                                 dtype=DTYPE1[self.platform_name])\n        except ValueError:\n            data = np.genfromtxt(self.filename,\n                                 unpack=True, skip_header=N_HEADER_LINES[\n                                     self.platform_name],\n                                 names=NAMES2[self.platform_name],\n                                 dtype=DTYPE2[self.platform_name])\n\n        wavelength = data['wavelength'] * scale\n        response = data['response']\n        det = data['detector']\n\n        detectors = {}\n        for idx in range(int(max(det))):\n            detectors[\"det-{0:d}\".format(idx + 1)] = {}\n            detectors[\n                \"det-{0:d}\".format(idx + 1)]['wavelength'] = np.repeat(wavelength, np.equal(det, idx + 1))\n            detectors[\n                \"det-{0:d}\".format(idx + 1)]['response'] = np.repeat(response, np.equal(det, idx + 1))\n\n        self.rsr = detectors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the Sentinel - 2 MSI relative spectral responses from the file.", "response": "def _load(self, scale=0.001):\n        \"\"\"Load the Sentinel-2 MSI relative spectral responses\n        \"\"\"\n\n        with open_workbook(self.path) as wb_:\n            for sheet in wb_.sheets():\n                if sheet.name not in SHEET_HEADERS.keys():\n                    continue\n\n                plt_short_name = PLATFORM_SHORT_NAME.get(self.platform_name)\n                if plt_short_name != SHEET_HEADERS.get(sheet.name):\n                    continue\n\n                wvl = sheet.col_values(0, 1)\n                for idx in range(1, sheet.row_len(0)):\n                    ch_name = MSI_BAND_NAMES[plt_short_name].get(str(sheet.col_values(idx, 0, 1)[0]))\n                    if ch_name != self.bandname:\n                        continue\n\n                    resp = sheet.col_values(idx, 1)\n                    resp = np.array(resp)\n                    resp = np.where(resp == '', 0, resp).astype('float32')\n                    mask = np.less_equal(resp, 0.00001)\n                    wvl0 = np.ma.masked_array(wvl, mask=mask)\n                    wvl_mask = np.ma.masked_outside(wvl, wvl0.min() - 2, wvl0.max() + 2)\n\n                    wvl = wvl_mask.compressed()\n                    resp = np.ma.masked_array(resp, mask=wvl_mask.mask).compressed()\n                    self.rsr = {'wavelength': wvl / 1000., 'response': resp}\n\n                    break\n\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load(self, scale=1.0):\n\n        LOG.debug(\"File: %s\", str(self.requested_band_filename))\n        ncf = Dataset(self.requested_band_filename, 'r')\n\n        wvl = ncf.variables['wavelength'][:] * scale\n        resp = ncf.variables['response'][:]\n\n        self.rsr = {'wavelength': wvl, 'response': resp}", "response": "Load the SLSTR relative spectral responses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config():\n    if CONFIG_FILE is not None:\n        configfile = CONFIG_FILE\n    else:\n        configfile = BUILTIN_CONFIG_FILE\n\n    config = {}\n    with open(configfile, 'r') as fp_:\n        config = recursive_dict_update(config, yaml.load(fp_, Loader=UnsafeLoader))\n\n    app_dirs = AppDirs('pyspectral', 'pytroll')\n    user_datadir = app_dirs.user_data_dir\n    config['rsr_dir'] = expanduser(config.get('rsr_dir', user_datadir))\n    config['rayleigh_dir'] = expanduser(config.get('rayleigh_dir', user_datadir))\n\n    return config", "response": "Get the configuration from file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the LUT with reflectances as a function of wavelength satellite_zenith_secant azimuth difference angle and sun_zenith_secant.", "response": "def get_reflectance_lut(filename):\n    \"\"\"Read the LUT with reflectances as a function of wavelength, satellite\n    zenith secant, azimuth difference angle, and sun zenith secant\n\n    \"\"\"\n\n    h5f = h5py.File(filename, 'r')\n\n    tab = h5f['reflectance']\n    wvl = h5f['wavelengths']\n    azidiff = h5f['azimuth_difference']\n    satellite_zenith_secant = h5f['satellite_zenith_secant']\n    sun_zenith_secant = h5f['sun_zenith_secant']\n\n    if HAVE_DASK:\n        tab = from_array(tab, chunks=(10, 10, 10, 10))\n        wvl = wvl[:]  # no benefit to dask-ifying this\n        azidiff = from_array(azidiff, chunks=(1000,))\n        satellite_zenith_secant = from_array(satellite_zenith_secant,\n                                             chunks=(1000,))\n        sun_zenith_secant = from_array(sun_zenith_secant,\n                                       chunks=(1000,))\n    else:\n        # load all of the data we are going to use in to memory\n        tab = tab[:]\n        wvl = wvl[:]\n        azidiff = azidiff[:]\n        satellite_zenith_secant = satellite_zenith_secant[:]\n        sun_zenith_secant = sun_zenith_secant[:]\n        h5f.close()\n\n    return tab, wvl, azidiff, satellite_zenith_secant, sun_zenith_secant"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the version of the atm correction luts from the version file in the specific aerosol correction directory and return the version string.", "response": "def _get_lutfiles_version(self):\n        \"\"\"Check the version of the atm correction luts from the version file in the\n           specific aerosol correction directory\n\n        \"\"\"\n        basedir = RAYLEIGH_LUT_DIRS[self._aerosol_type]\n        lutfiles_version_path = os.path.join(basedir,\n                                             ATM_CORRECTION_LUT_VERSION[self._aerosol_type]['filename'])\n\n        if not os.path.exists(lutfiles_version_path):\n            return \"v0.0.0\"\n\n        with open(lutfiles_version_path, 'r') as fpt:\n            # Get the version from the file\n            return fpt.readline().strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_effective_wavelength(self, bandname):\n        try:\n            rsr = RelativeSpectralResponse(self.platform_name, self.sensor)\n        except(IOError, OSError):\n            LOG.exception(\n                \"No spectral responses for this platform and sensor: %s %s\", self.platform_name, self.sensor)\n            if isinstance(bandname, (float, integer_types)):\n                LOG.warning(\n                    \"Effective wavelength is set to the requested band wavelength = %f\", bandname)\n                return bandname\n\n            msg = (\"Can't get effective wavelength for band %s on platform %s and sensor %s\" %\n                   (str(bandname), self.platform_name, self.sensor))\n            raise KeyError(msg)\n\n        if isinstance(bandname, str):\n            bandname = BANDNAMES.get(self.sensor, BANDNAMES['generic']).get(bandname, bandname)\n        elif isinstance(bandname, (float, integer_types)):\n            if not(0.4 < bandname < 0.8):\n                raise BandFrequencyOutOfRange(\n                    'Requested band frequency should be between 0.4 and 0.8 microns!')\n            bandname = get_bandname_from_wavelength(self.sensor, bandname, rsr.rsr)\n\n        wvl, resp = rsr.rsr[bandname][\n            'det-1']['wavelength'], rsr.rsr[bandname]['det-1']['response']\n\n        cwvl = get_central_wave(wvl, resp, weight=1. / wvl**4)\n        LOG.debug(\"Band name: %s  Effective wavelength: %f\", bandname, cwvl)\n\n        return cwvl", "response": "Get the effective wavelength for a given bandname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the LUT with reflectances as a function of wavelength satellite zenith secant azimuth difference angle and sun zenith secant.", "response": "def get_reflectance_lut(self):\n        \"\"\"Read the LUT with reflectances as a function of wavelength, satellite zenith\n        secant, azimuth difference angle, and sun zenith secant\n\n        \"\"\"\n        if self._rayl is None:\n            lut_vars = get_reflectance_lut(self.reflectance_lut_filename)\n            self._rayl = lut_vars[0]\n            self._wvl_coord = lut_vars[1]\n            self._azid_coord = lut_vars[2]\n            self._satz_sec_coord = lut_vars[3]\n            self._sunz_sec_coord = lut_vars[4]\n        return self._rayl, self._wvl_coord, self._azid_coord,\\\n            self._satz_sec_coord, self._sunz_sec_coord"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the reflectance from the three sun - saturation angles and bandnames.", "response": "def get_reflectance(self, sun_zenith, sat_zenith, azidiff, bandname, redband=None):\n        \"\"\"Get the reflectance from the three sun-sat angles\"\"\"\n        # Get wavelength in nm for band:\n        if isinstance(bandname, float):\n            LOG.warning('A wavelength is provided instead of band name - ' +\n                        'disregard the relative spectral responses and assume ' +\n                        'it is the effective wavelength: %f (micro meter)', bandname)\n            wvl = bandname * 1000.0\n        else:\n            wvl = self.get_effective_wavelength(bandname)\n            wvl = wvl * 1000.0\n\n        rayl, wvl_coord, azid_coord, satz_sec_coord, sunz_sec_coord = self.get_reflectance_lut()\n\n        # force dask arrays\n        compute = False\n        if HAVE_DASK and not isinstance(sun_zenith, Array):\n            compute = True\n            sun_zenith = from_array(sun_zenith, chunks=sun_zenith.shape)\n            sat_zenith = from_array(sat_zenith, chunks=sat_zenith.shape)\n            azidiff = from_array(azidiff, chunks=azidiff.shape)\n            if redband is not None:\n                redband = from_array(redband, chunks=redband.shape)\n\n        clip_angle = rad2deg(arccos(1. / sunz_sec_coord.max()))\n        sun_zenith = clip(sun_zenith, 0, clip_angle)\n        sunzsec = 1. / cos(deg2rad(sun_zenith))\n        clip_angle = rad2deg(arccos(1. / satz_sec_coord.max()))\n        sat_zenith = clip(sat_zenith, 0, clip_angle)\n        satzsec = 1. / cos(deg2rad(sat_zenith))\n        shape = sun_zenith.shape\n\n        if not(wvl_coord.min() < wvl < wvl_coord.max()):\n            LOG.warning(\n                \"Effective wavelength for band %s outside 400-800 nm range!\",\n                str(bandname))\n            LOG.info(\n                \"Set the rayleigh/aerosol reflectance contribution to zero!\")\n            if HAVE_DASK:\n                chunks = sun_zenith.chunks if redband is None else redband.chunks\n                res = zeros(shape, chunks=chunks)\n                return res.compute() if compute else res\n            else:\n                return zeros(shape)\n\n        idx = np.searchsorted(wvl_coord, wvl)\n        wvl1 = wvl_coord[idx - 1]\n        wvl2 = wvl_coord[idx]\n\n        fac = (wvl2 - wvl) / (wvl2 - wvl1)\n        raylwvl = fac * rayl[idx - 1, :, :, :] + (1 - fac) * rayl[idx, :, :, :]\n        tic = time.time()\n\n        smin = [sunz_sec_coord[0], azid_coord[0], satz_sec_coord[0]]\n        smax = [sunz_sec_coord[-1], azid_coord[-1], satz_sec_coord[-1]]\n        orders = [\n            len(sunz_sec_coord), len(azid_coord), len(satz_sec_coord)]\n        f_3d_grid = atleast_2d(raylwvl.ravel())\n\n        if HAVE_DASK and isinstance(smin[0], Array):\n            # compute all of these at the same time before passing to the interpolator\n            # otherwise they are computed separately\n            smin, smax, orders, f_3d_grid = da.compute(smin, smax, orders, f_3d_grid)\n        minterp = MultilinearInterpolator(smin, smax, orders)\n        minterp.set_values(f_3d_grid)\n\n        if HAVE_DASK:\n            ipn = map_blocks(self._do_interp, minterp, sunzsec, azidiff,\n                             satzsec, dtype=raylwvl.dtype, chunks=azidiff.chunks)\n        else:\n            ipn = self._do_interp(minterp, sunzsec, azidiff, satzsec)\n\n        LOG.debug(\"Time - Interpolation: {0:f}\".format(time.time() - tic))\n\n        ipn *= 100\n        res = ipn\n        if redband is not None:\n            res = where(redband < 20., res,\n                        (1 - (redband - 20) / 80) * res)\n\n        res = clip(res, 0, 100)\n        if compute:\n            res = res.compute()\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef griditer(x, y, ncol, nrow=None, step=1):\n    if nrow is None:\n        nrow = ncol\n    yield from itertools.product(range(x, x + ncol, step),\n                                 range(y, y + nrow, step))", "response": "Iterate through a grid of tiles."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bboxiter(tile_bounds, tiles_per_row_per_region=1):\n    x_lower = math.floor(tile_bounds.min.x / tiles_per_row_per_region)\n    y_lower = math.floor(tile_bounds.min.y / tiles_per_row_per_region)\n    ncol = math.ceil(tile_bounds.max.x / tiles_per_row_per_region) - x_lower\n    nrow = math.ceil(tile_bounds.max.y / tiles_per_row_per_region) - y_lower\n    yield from griditer(x_lower, y_lower, ncol, nrow)", "response": "Iterate through a grid of tiles defined by a TileBB."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_tile(self, zoom):\n        p1 = self.min.to_tile(zoom)\n        p2 = self.max.to_tile(zoom)\n        return GridBB(zoom, p1.x, p2.y, p2.x, p1.y)", "response": "Converts EPSG 900913 to tile coordinates in given zoom level"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolution(self):\n        geo_coords = self.to_geographic()\n        resolution = abs(_initialresolution * math.cos(geo_coords.lat * _pi_180) / (2**self.zoom))\n        return resolution", "response": "Get the tile resolution at the current position."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tiles(self):\n        for x, y in griditer(self.root_tile.x, self.root_tile.y, ncol=self.tiles_per_row):\n            yield TileCoordinate(self.root_tile.zoom, x, y)", "response": "Get all tiles contained in the region"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maps_json():\n    map_sources = {\n        id: {\n                \"id\": map_source.id,\n                \"name\": map_source.name,\n                \"folder\": map_source.folder,\n                \"min_zoom\": map_source.min_zoom,\n                \"max_zoom\": map_source.max_zoom,\n                \"layers\": [\n                    {\n                        \"min_zoom\": layer.min_zoom,\n                        \"max_zoom\": layer.max_zoom,\n                        \"tile_url\": layer.tile_url.replace(\"$\", \"\"),\n                    } for layer in map_source.layers\n                    ]\n\n            } for id, map_source in app.config[\"mapsources\"].items()\n        }\n    return jsonify(map_sources)", "response": "Generates a json object which serves as bridge between the web interface and the map source collection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map_to_pdf(map_source, zoom, x, y, width, height):\n    map_source = app.config[\"mapsources\"][map_source]\n    pdf_file = print_map(map_source, x=float(x), y=float(y),\n                         zoom=int(zoom), width=float(width), height=float(height), format='pdf')\n    return send_file(pdf_file,\n                     attachment_filename=\"map.pdf\",\n                     as_attachment=True)", "response": "Generate a PDF at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef kml_region(map_source, z, x, y):\n    map = app.config[\"mapsources\"][map_source]\n    kml_doc = KMLRegion(app.config[\"url_formatter\"], map, app.config[\"LOG_TILES_PER_ROW\"],\n                        z, x, y)\n    return kml_response(kml_doc)", "response": "KML region fetched by a Google Earth network link."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef count_elements(doc):\n    \"Counts the number of times each element is used in a document\"\n    summary = {}\n    for el in doc.iter():\n        try:\n            namespace, element_name = re.search('^{(.+)}(.+)$', el.tag).groups()\n        except:\n            namespace = None\n            element_name = el.tag\n        if not summary.has_key(namespace):\n            summary[namespace] = {}\n        if not summary[namespace].has_key(element_name):\n            summary[namespace][element_name] = 1\n        else:\n            summary[namespace][element_name] += 1\n    return summary", "response": "Counts the number of times each element is used in a document"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the correct factory object for a given namespace", "response": "def get_factory_object_name(namespace):\n    \"Returns the correct factory object for a given namespace\"\n    \n    factory_map = {\n        'http://www.opengis.net/kml/2.2': 'KML',\n        'http://www.w3.org/2005/Atom': 'ATOM',\n        'http://www.google.com/kml/ext/2.2': 'GX'\n    }\n    if namespace:\n        if factory_map.has_key(namespace):\n            factory_object_name = factory_map[namespace]\n        else:\n            factory_object_name = None\n    else:\n        # use the KML factory object as the default, if no namespace is given\n        factory_object_name = 'KML'\n    return factory_object_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a python script that will construct a given KML document", "response": "def write_python_script_for_kml_document(doc):\n    \"Generates a python script that will construct a given KML document\"\n    import StringIO\n    from pykml.helpers import separate_namespace\n    \n    output = StringIO.StringIO()\n    indent_size = 2\n    \n    # add the etree package so that comments can be handled\n    output.write('from lxml import etree\\n')\n    \n    # add the namespace declaration section\n    output.write('from pykml.factory import KML_ElementMaker as KML\\n')\n    output.write('from pykml.factory import ATOM_ElementMaker as ATOM\\n')\n    output.write('from pykml.factory import GX_ElementMaker as GX\\n')\n    output.write('\\n')\n    \n    level = 0\n    xml = StringIO.StringIO(etree.tostring(doc))\n    context = etree.iterparse(xml, events=(\"start\", \"end\", \"comment\"))\n    output.write('doc = ')\n    last_action = None\n    main_element_processed_flag = False\n    previous_list = []  # list of comments before the root element\n    posterior_list = []  # list of comments after the root element\n    for action, elem in context:\n        # TODO: remove the following redundant conditional\n        if action in ('start','end','comment'):\n            namespace, element_name = separate_namespace(elem.tag)\n            if action in ('comment'):\n                indent = ' ' * level * indent_size\n                if elem.text:\n                    text_list = elem.text.split('\\n')\n                    if len(text_list) == 1:\n                        text = repr(elem.text)\n                    else: # len(text_list) > 1\n                        # format and join all non-empty lines\n                        text = '\\n' + ''.join(\n                                ['{indent}{content}\\n'.format(\n                                        indent=' '*(len(t)-len(t.lstrip())),\n                                        content=repr(t.lstrip())\n                                    ) for t in text_list if len(t.strip())>0\n                                ]\n                            ) + indent\n                else:\n                    text = ''\n                if level == 0:\n                    # store the comment so that it can be appended later\n                    if main_element_processed_flag:\n                        posterior_list.append(\"{indent}etree.Comment({comment})\".format(\n                            indent = indent,\n                            comment = text,\n                        ))\n                    else:\n                        previous_list.append(\"{indent}etree.Comment({comment})\".format(\n                            indent = indent,\n                            comment = text,\n                        ))\n                else:\n                    output.write(\"{indent}etree.Comment({comment}),\\n\".format(\n                        indent = indent,\n                        comment = text,\n                    ))\n            elif action in ('start'):\n                main_element_processed_flag = True\n                if last_action == None:\n                    indent = ''\n                else:\n                    indent = ' ' * level * indent_size\n                level += 1\n                if elem.text:\n                    # METHOD 1\n#                    # treat all text string the same. this works but gets\n#                    # messy for multi-line test strings\n#                    text = repr(elem.text)\n                    # METHOD 2 - format multiline strings differently\n                    text_list = elem.text.split('\\n')\n                    if len(text_list) == 1:\n                        text = repr(elem.text)\n                    else: # len(text_list) > 1\n                        # format and join all non-empty lines\n                        text = '\\n' + ''.join(\n                                ['{indent}{content}\\n'.format(\n                                        indent=' '*(len(t)-len(t.lstrip())),\n                                        content=repr(t.lstrip())\n                                    ) for t in text_list if len(t.strip())>0\n                                ]\n                            ) + indent\n                else:\n                    text = ''\n                output.write('{indent}{factory}.{tag}({text}\\n'.format(\n                    indent = indent,\n                    factory = get_factory_object_name(namespace),\n                    tag = element_name,\n                    text = text,\n                ))\n            elif action in ('end'):\n                level -= 1\n                if last_action == 'start':\n                    output.pos -= 1\n                    indent = ''\n                else:\n                    indent = ' ' * level * indent_size\n                for att,val in elem.items():\n                    output.write('{0}  {1}=\"{2}\",\\n'.format(indent,att,val))\n                output.write('{0}),\\n'.format(indent))\n        last_action = action\n    \n    # remove the last comma\n    output.pos -= 2\n    output.truncate()\n    output.write('\\n')\n    \n    for entry in previous_list:\n        output.write('doc.addprevious({entry})\\n'.format(\n            entry=entry\n        ))\n    for entry in posterior_list:\n        output.write('doc.addnext({entry})\\n'.format(\n            entry=entry\n        ))\n    \n    # add python code to print out the KML document\n    output.write('print etree.tostring(etree.ElementTree(doc),pretty_print=True)\\n')\n    \n    return output.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kml_element_name(grid_coords, elem_id=\"KML\"):\n    return \"_\".join(str(x) for x in [elem_id, grid_coords.zoom, grid_coords.x, grid_coords.y])", "response": "Create a unique element name for KML"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the north south east and west tags for a LatLonBox or LatLonAltBox", "response": "def kml_lat_lon_box(geo_bb):\n    \"\"\"\n    Create the north/south/east/west tags for a <LatLonBox> or <LatLonAltBox> Bounding Box\n\n    Args:\n        geo_bb (GeographicBB):\n\n    Returns:\n        Tuple: (<north>, <south>, <east>, <west>) KMLElements\n    \"\"\"\n    return (\n        KML.north(geo_bb.max.lat),\n        KML.south(geo_bb.min.lat),\n        KML.east(geo_bb.max.lon),\n        KML.west(geo_bb.min.lon)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new KML LevelOfDetail Tag.", "response": "def kml_lod(min_lod_pixels=DEFAULT_MIN_LOD_PIXELS, max_lod_pixels=DEFAULT_MAX_LOD_PIXELS):\n    \"\"\"\n    Create the KML LevelOfDetail (LOD) Tag.\n\n    In a Region, the <minLodPixels> and <maxLodPixels> elements allow you to specify\n    an area of the screen (in square pixels). When your data is projected onto the screen,\n    it must occupy an area of the screen that is greater than <minLodPixels> and less\n    than <maxLodPixels> in order to be visible. Once the projected size of the Region goes\n    outside of these limits, it is no longer visible, and the Region becomes inactive.\n    (from https://developers.google.com/kml/documentation/kml_21tutorial)\n\n    Args:\n        min_lod_pixels (int):\n        max_lod_pixels (int):\n    \"\"\"\n    return KML.Lod(\n        KML.minLodPixels(min_lod_pixels),\n        KML.maxLodPixels(max_lod_pixels))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef kml_region(region_coords, min_lod_pixels=DEFAULT_MIN_LOD_PIXELS,\n               max_lod_pixels=DEFAULT_MAX_LOD_PIXELS):\n    \"\"\"\n    Create the KML <Region> tag with the appropriate geographic coordinates\n\n    Args:\n        region_coords (RegionCoordinate):\n        min_lod_pixels (int): see `kml_lod`\n        max_lod_pixels (int): see `kml_lod`\n\n    Returns:\n        KMLElement: the KML <Region>\n    \"\"\"\n    bbox = region_coords.geographic_bounds()\n    return KML.Region(\n        kml_lod(min_lod_pixels=min_lod_pixels, max_lod_pixels=max_lod_pixels),\n        KML.LatLonAltBox(\n            *kml_lat_lon_box(bbox)\n        )\n    )", "response": "Create the KML region tag with the appropriate geographic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kml_network_link(href, name=None, region_coords=None, visible=True):\n    nl = KML.NetworkLink()\n    if name is None and region_coords is not None:\n        name = kml_element_name(region_coords, \"NL\")\n    if name is not None:\n        nl.append(KML.name(name))\n    if region_coords is not None:\n        min_lod_pixels = DEFAULT_MIN_LOD_PIXELS * (2 ** region_coords.log_tiles_per_row)\n        nl.append(kml_region(region_coords, min_lod_pixels=min_lod_pixels))\n    if not visible:\n        nl.append(KML.visibility(0))\n\n    nl.append(KML.Link(\n        KML.href(href), KML.viewRefreshMode(\"onRegion\")))\n\n    return nl", "response": "Create the KML NetworkLink tag for a certain region in the RegionGrid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kml_ground_overlay(tile_coords, tile_url):\n    return KML.GroundOverlay(\n        KML.name(kml_element_name(tile_coords, \"GO\")),\n        KML.drawOrder(tile_coords.zoom),\n        KML.Icon(\n            KML.href(tile_url)\n        ),\n        KML.LatLonBox(\n            *kml_lat_lon_box(tile_coords.geographic_bounds())\n        ),\n    )", "response": "Create a KML GroundOverlay for a certain TileCoordinate."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_abs_url(self, rel_url):\n        rel_url = rel_url.lstrip(\"/\")\n        return \"{}://{}:{}/{}\".format(self.url_scheme, self.host, self.port, rel_url)", "response": "Create an absolute url from a relative one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_map_url(self, mapsource, grid_coords):\n        return self.get_abs_url(\n                \"/maps/{}/{}/{}/{}.kml\".format(mapsource.id, grid_coords.zoom,\n                                               grid_coords.x, grid_coords.y))", "response": "Get the URL to a map region."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the KML Document as formatted kml / xml", "response": "def get_kml(self):\n        \"\"\"Return the KML Document as formatted kml/xml\"\"\"\n        return etree.tostring(self.kml_root, pretty_print=True, xml_declaration=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_network_link(self, mapsource):\n        return kml_network_link(self.url_formatter.get_map_root_url(mapsource),\n                                 name=mapsource.name, visible=False)", "response": "Get KML NetworkLink for a given mapsource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef separate_namespace(qname):\n    \"Separates the namespace from the element\"\n    import re\n    try:\n        namespace, element_name = re.search('^{(.+)}(.+)$', qname).groups()\n    except:\n        namespace = None\n        element_name = qname\n    return namespace, element_name", "response": "Separates the namespace from the element"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_max_decimal_places(doc, max_decimals):\n    \n    def replace_delimited_string_member(\n            delimited_str,\n            separator,\n            index_no,\n            decimal_places):\n        \"Modify the number of decimal places for a delimited string member\"\n        values = delimited_str.split(separator)\n        values[index_no] = str(round(float(values[index_no]), decimal_places))\n        return separator.join(values)\n    \n    if max_decimals.has_key('longitude'):\n        data_type = 'longitude'\n        index_no = 0 # longitude is in the first position\n        # modify <longitude>\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}longitude\"):\n            new_val = round(float(el.text), max_decimals[data_type])\n            el.getparent().longitude = K.longitude(new_val)\n        # modify <coordinates> elements\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}coordinates\"):\n            vertex_str_list = []\n            for vertex in el.text.strip().split(' '):\n                vertex_str_list.append(\n                    replace_delimited_string_member(\n                        delimited_str=vertex,\n                        separator=',',\n                        index_no=index_no,\n                        decimal_places=max_decimals[data_type]\n                    )\n                )\n            el_new = K.coordinates(' '.join(vertex_str_list).strip())\n            el.getparent().replace(el, el_new)\n        # modify <gx:coords> elements\n        for el in doc.findall(\".//{http://www.google.com/kml/ext/2.2}coord\"):\n            el._setText(\n                replace_delimited_string_member(\n                    delimited_str=el.text,\n                    separator=' ',\n                    index_no=index_no,\n                    decimal_places=max_decimals[data_type]\n                )\n            )\n    \n    if max_decimals.has_key('latitude'):\n        data_type = 'latitude'\n        index_no = 1 # latitude is in the second position\n        # modify <latitude> elements\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}latitude\"):\n            new_val = round(float(el.text), max_decimals[data_type])\n            el.getparent().latitude = K.latitude(new_val)\n        # modify <coordinates> elements\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}coordinates\"):\n            vertex_str_list = []\n            for vertex in el.text.strip().split(' '):\n                vertex_str_list.append(\n                    replace_delimited_string_member(\n                        delimited_str=vertex,\n                        separator=',',\n                        index_no=index_no,\n                        decimal_places=max_decimals[data_type]\n                    )\n                )\n            el_new = K.coordinates(' '.join(vertex_str_list).strip())\n            el.getparent().replace(el, el_new)\n        # modify <gx:coords> elements\n        for el in doc.findall(\".//{http://www.google.com/kml/ext/2.2}coord\"):\n            el._setText(\n                replace_delimited_string_member(\n                    delimited_str=el.text,\n                    separator=' ',\n                    index_no=index_no,\n                    decimal_places=max_decimals[data_type]\n                )\n            )\n\n    if max_decimals.has_key('altitude'):\n        data_type = 'altitude'\n        index_no = 2 # altitude is in the third position\n        # modify <altitude> elements\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}altitude\"):\n            new_val = round(float(el.text), max_decimals[data_type])\n            el.getparent().altitude = K.altitude(new_val)\n        # modify <coordinates> elements\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}coordinates\"):\n            vertex_str_list = []\n            for vertex in el.text.strip().split(' '):\n                vertex_str_list.append(\n                    replace_delimited_string_member(\n                        delimited_str=vertex,\n                        separator=',',\n                        index_no=index_no,\n                        decimal_places=max_decimals[data_type]\n                    )\n                )\n            el_new = K.coordinates(' '.join(vertex_str_list).strip())\n            el.getparent().replace(el, el_new)\n        # modify <gx:coords> elements\n        for el in doc.findall(\".//{http://www.google.com/kml/ext/2.2}coord\"):\n            el._setText(\n                replace_delimited_string_member(\n                    delimited_str=el.text,\n                    separator=' ',\n                    index_no=index_no,\n                    decimal_places=max_decimals[data_type]\n                )\n            )\n    \n    if max_decimals.has_key('heading'):\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}heading\"):\n            new_val = round(float(el.text), max_decimals['heading'])\n            el.getparent().heading = K.heading(new_val)\n    if max_decimals.has_key('tilt'):\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}tilt\"):\n            new_val = round(float(el.text), max_decimals['tilt'])\n            el.getparent().tilt = K.tilt(new_val)\n    if max_decimals.has_key('range'):\n        for el in doc.findall(\".//{http://www.opengis.net/kml/2.2}range\"):\n            new_val = round(float(el.text), max_decimals['range'])\n            el.getparent().range = K.range(new_val)", "response": "This method sets the maximum number of decimal places used by KML elements in the KML document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting a map to a temporary file.", "response": "def print_map(map_source, x, y, zoom=14, width=297, height=210, dpi=300, format=\"pdf\"):\n    \"\"\"\n    Download map tiles and stitch them together in a single image, ready for printing.\n\n    Args:\n        map_source (MapSource): Map to download\n        x (float): map center x-coordinate in Mercator projection (EPSG:4326)\n        y (float): map center y-coordinate in Mercator projection (EPSG:4326)\n        zoom (int): tile zoom level to use for printing\n        width (float): page width in mm\n        height (float): page height in mm\n        dpi (int): resolution in dots per inch\n        format (str): output format. Anything supported by ``Pillow.Image.save``. E.g. \"pdf\", \"jpeg\", \"png\".\n\n    Returns:\n        str: path of temporary output file.\n\n    \"\"\"\n    bbox = get_print_bbox(x, y, zoom, width, height, dpi)\n    tiles = [\n        get_tiles(tile_layer, bbox) for tile_layer in map_source.layers if\n        tile_layer.min_zoom <= zoom <= tile_layer.max_zoom\n        ]\n    img = stitch_map(tiles, width, height, bbox, dpi)\n    outfile = NamedTemporaryFile(delete=False)\n    img.save(outfile, format, quality=100, dpi=(dpi, dpi))\n    outfile.close()\n    return outfile.name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_print_bbox(x, y, zoom, width, height, dpi):\n    tiles_h = width * dpi_to_dpmm(dpi) / TILE_SIZE\n    tiles_v = height * dpi_to_dpmm(dpi) / TILE_SIZE\n    mercator_coords = MercatorCoordinate(x, y)\n    tile_coords = mercator_coords.to_tile(zoom)\n    tile_bb = GridBB(zoom,\n                     min_x=tile_coords.x - math.ceil(tiles_h / 2),\n                     max_x=tile_coords.x + math.ceil(tiles_h / 2),\n                     min_y=tile_coords.y - math.ceil(tiles_v / 2),\n                     max_y=tile_coords.y + math.ceil(tiles_v / 2))\n    return tile_bb", "response": "This function calculates the bounding box of the next larger tile - box that covers the specified base map."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_tile(map_layer, zoom, x, y):\n    try:\n        tile_url = map_layer.get_tile_url(zoom, x, y)\n        tmp_file, headers = urllib.request.urlretrieve(tile_url)\n        return (x, y), tmp_file\n    except URLError as e:\n        app.logger.info(\"Error downloading tile x={}, y={}, z={} for layer {}: {}\".format(\n            x, y, zoom, map_layer, e.reason))\n        return (x, y), pkg_resources.resource_filename(\"geos\", \"static/empty_tile.png\")", "response": "Download a given tile from the tile server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tiles(map_layer, bbox, n_workers=N_DOWNLOAD_WORKERS):\n    p = Pool(n_workers)\n    tiles = {}\n    for (x, y), tmp_file in p.imap_unordered(_download_tile_wrapper, zip(itertools.repeat(map_layer),\n                                                                         itertools.repeat(bbox.zoom),\n                                                                         *zip(*bboxiter(bbox)))):\n        app.logger.info(\"Downloaded tile x={}, y={}, z={}\".format(x, y, bbox.zoom))\n        tiles[(x, y)] = tmp_file\n\n    return tiles", "response": "Download tiles from a map layer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stitch_map(tiles, width, height, bbox, dpi):\n    size = (int(width * dpi_to_dpmm(dpi)), int(height * dpi_to_dpmm(dpi)))\n    background = Image.new('RGBA', size, (255, 255, 255))\n    for layer in tiles:\n        layer_img = Image.new(\"RGBA\", size)\n        for (x, y), tile_path in layer.items():\n            tile = Image.open(tile_path)\n            layer_img.paste(tile, ((x - bbox.min.x) * TILE_SIZE, (y - bbox.min.y) * TILE_SIZE))\n        background = Image.alpha_composite(background, layer_img)\n    add_scales_bar(background, bbox)\n    return background.convert(\"RGB\")", "response": "Merge tiles together into one image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a scales bar to the \u00b5s map.", "response": "def add_scales_bar(img, bbox):\n    \"\"\"\n    Add a scales bar to the  map.\n\n    Calculates the resolution at the current latitude and\n    inserts the corresponding scales bar on the map.\n\n    Args:\n        img (Image): Image object to which the scales bar will be added.\n        bbox (TileBB): boundaries of the map\n\n    \"\"\"\n    tc = TileCoordinate(bbox.min.zoom, bbox.min.x, bbox.min.y)\n    meters_per_pixel = tc.resolution()\n    one_km_bar = int(1000 * (1 / meters_per_pixel))\n    col_black = (0, 0, 0)\n\n    line_start = (100, img.size[1] - 100)  # px\n    line_end = (line_start[0] + one_km_bar, line_start[1])\n    whiskers_left = [line_start[0], line_start[1] - 15, line_start[0], line_start[1] + 15]\n    whiskers_right = [line_end[0], line_end[1] - 15, line_end[0], line_end[1] + 15]\n\n    draw = ImageDraw.Draw(img)\n    draw.line([line_start, line_end], fill=col_black, width=5)\n    draw.line(whiskers_left, fill=col_black, width=2)\n    draw.line(whiskers_right, fill=col_black, width=2)\n    draw.text((line_start[0] + 10, line_start[1] + 10), fill=col_black, text=\"1 km\")\n    del draw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fromstring(text, schema=None):\n    if schema:\n        parser = objectify.makeparser(schema = schema.schema)\n        return objectify.fromstring(text, parser=parser)\n    else:\n        return objectify.fromstring(text)", "response": "Parses a KML text string and optionally validates it against a FACTORY object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a KML file object and validates it against a VISA VISS V IS A VISA VA VISA VA V", "response": "def parse(fileobject, schema=None):\n    \"\"\"Parses a file object\n    \n    This functon parses a KML file object, and optionally validates it against \n    a provided schema.\n    \"\"\"\n    if schema:\n        # with validation\n        parser = objectify.makeparser(schema = schema.schema, strip_cdata=False)\n        return objectify.parse(fileobject, parser=parser)\n    else:\n        # without validation\n        return objectify.parse(fileobject)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_maps(maps_dir):\n    maps_dir = os.path.abspath(maps_dir)\n    maps = {}\n    for root, dirnames, filenames in os.walk(maps_dir):\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                xml_file = os.path.join(root, filename)\n                map = MapSource.from_xml(xml_file, maps_dir)\n                if map.id in maps:\n                    raise MapSourceException(\"duplicate map id: {} in file {}\".format(map.id, xml_file))\n                else:\n                    maps[map.id] = map\n    return maps", "response": "Load all xml map sources from a given directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwalk through a list of MapSources and returns a list of all the map sources in the order they appear.", "response": "def walk_mapsources(mapsources, root=\"\"):\n    \"\"\"\n    recursively walk through foldernames of mapsources.\n\n    Like os.walk, only for a list of mapsources.\n\n    Args:\n        mapsources (list of MapSource):\n\n    Yields:\n        (root, foldernames, maps)\n\n    >>> mapsources = load_maps(\"test/mapsources\")\n    >>> pprint([x for x in walk_mapsources(mapsources.values())])\n    [('',\n      ['asia', 'europe'],\n      [<MapSource: osm1 (root 1), n_layers: 1, min_zoom:5, max_zoom:18>,\n       <MapSource: osm10 (root 2), n_layers: 1, min_zoom:5, max_zoom:18>]),\n     ('/asia',\n      [],\n      [<MapSource: osm6 (asia), n_layers: 1, min_zoom:5, max_zoom:18>]),\n     ('/europe',\n      ['france', 'germany', 'switzerland'],\n      [<MapSource: osm4 (eruope 1), n_layers: 1, min_zoom:1, max_zoom:18>]),\n     ('/europe/france',\n      [],\n      [<MapSource: osm2 (europe/france 1), n_layers: 1, min_zoom:5, max_zoom:18>,\n       <MapSource: osm3 (europe/france 2), n_layers: 1, min_zoom:1, max_zoom:18>,\n       <MapSource: osm5 (europe/france 3), n_layers: 1, min_zoom:5, max_zoom:18>]),\n     ('/europe/germany',\n      [],\n      [<MapSource: osm7 (europe/germany 1), n_layers: 1, min_zoom:5, max_zoom:18>,\n       <MapSource: osm8 (europe/germany 2), n_layers: 1, min_zoom:5, max_zoom:18>]),\n     ('/europe/switzerland',\n      [],\n      [<MapSource: osm9 (europe/switzerland), n_layers: 1, min_zoom:5, max_zoom:18>])]\n\n    \"\"\"\n    def get_first_folder(path):\n        \"\"\"\n        Get the first folder in a path\n        > get_first_folder(\"europe/switzerland/bs\")\n        europe\n        \"\"\"\n        path = path[len(root):]\n        path = path.lstrip(F_SEP)\n        return path.split(F_SEP)[0]\n\n    path_tuples = sorted(((get_first_folder(m.folder), m) for m in mapsources), key=lambda x: x[0])\n    groups = {k: [x for x in g] for k, g in itertools.groupby(path_tuples, lambda x: x[0])}\n    folders = sorted([x for x in groups.keys() if x != \"\"])\n    mapsources = sorted([t[1] for t in groups.get(\"\", [])], key=lambda x: x.id)\n    yield (root, folders, mapsources)\n    for fd in folders:\n        yield from walk_mapsources([t[1] for t in groups[fd]], F_SEP.join([root, fd]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tile_url(self, zoom, x, y):\n        return self.tile_url.format(**{\"$z\": zoom, \"$x\": x, \"$y\": y})", "response": "Get the tile url for the specified zoom and x and y."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the minimal zoom level of all layers.", "response": "def min_zoom(self):\n        \"\"\"\n        Get the minimal zoom level of all layers.\n\n        Returns:\n            int: the minimum of all zoom levels of all layers\n\n        Raises:\n            ValueError: if no layers exist\n\n        \"\"\"\n        zoom_levels = [map_layer.min_zoom for map_layer in self.layers]\n        return min(zoom_levels)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef max_zoom(self):\n        zoom_levels = [map_layer.max_zoom for map_layer in self.layers]\n        return max(zoom_levels)", "response": "Get the maximal zoom level of all layers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the xml element containing a region element and returns a GeographicBB object", "response": "def parse_xml_boundary(xml_region):\n        \"\"\"\n        Get the geographic bounds from an XML element\n\n        Args:\n            xml_region (Element): The <region> tag as XML Element\n\n        Returns:\n            GeographicBB:\n        \"\"\"\n        try:\n            bounds = {}\n            for boundary in xml_region.getchildren():\n                bounds[boundary.tag] = float(boundary.text)\n            bbox = GeographicBB(min_lon=bounds[\"west\"], max_lon=bounds[\"east\"],\n                                min_lat=bounds[\"south\"], max_lat=bounds[\"north\"])\n            return bbox\n        except (KeyError, ValueError):\n            raise MapSourceException(\"region boundaries are invalid. \")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_xml_layers(xml_layers):\n        layers = []\n        for custom_map_source in xml_layers.getchildren():\n            layers.append(MapSource.parse_xml_layer(custom_map_source))\n        return layers", "response": "Parses the MapLayers XML element xml_layers and returns a list of MapLayer objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a MapSource XML element and returns a MapLayer object.", "response": "def parse_xml_layer(xml_custom_map_source):\n        \"\"\"\n        Get one MapLayer from an XML element\n\n        Args:\n            xml_custom_map_source (Element): The <customMapSource> element tag wrapped\n               in a <layers> tag as XML Element\n\n        Returns:\n            MapLayer:\n\n        \"\"\"\n        map_layer = MapLayer()\n        try:\n            for elem in xml_custom_map_source.getchildren():\n                if elem.tag == 'url':\n                    map_layer.tile_url = elem.text\n                elif elem.tag == 'minZoom':\n                    map_layer.min_zoom = int(elem.text)\n                elif elem.tag == 'maxZoom':\n                    map_layer.max_zoom = int(elem.text)\n        except ValueError:\n            raise MapSourceException(\"minZoom/maxZoom must be an integer. \")\n\n        if map_layer.tile_url is None:\n            raise MapSourceException(\"Layer requires a tile_url parameter. \")\n\n        return map_layer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a MapSource object from a MOBAC mapsource xml file.", "response": "def from_xml(xml_path, mapsource_prefix=\"\"):\n        \"\"\"\n        Create a MapSource object from a MOBAC\n        mapsource xml.\n\n        Args:\n            xml_path: path to the MOBAC mapsource xml file.\n            mapsource_prefix: root path of the mapsource folder.\n              Used to determine relative path within the maps\n              directory.\n\n        Note:\n            The Meta-Information is read from the xml\n            <id>, <folder>, <name> tags. If <id> is not available it defaults\n            to the xml file basename. If <folder> is not available if defaults to\n            the folder of the xml file with the `mapsource_prefix` removed.\n\n            The function first tries <url>, <minZoom>, <maxZoom> from <customMapSource>\n            tags within the <layers> tag. If the <layers> tag is not available,\n            the function tries to find <url>, <minZoom> and <maxZoom> on the top level.\n            If none of thie information is found, a MapSourceException is raised. \n\n        Returns:\n            MapSource:\n\n        Raises:\n            MapSourceException: when the xml file could not be parsed properly.\n\n        \"\"\"\n        xmldoc = xml.etree.ElementTree.parse(xml_path).getroot()\n\n        map_id = os.path.splitext(os.path.basename(xml_path))[0]\n        map_name = map_id\n        map_folder = re.sub(\"^\" + re.escape(mapsource_prefix), \"\", os.path.dirname(xml_path))\n        bbox = None\n        layers = None\n\n        for elem in xmldoc.getchildren():\n            if elem.tag == 'id':\n                map_id = elem.text\n            elif elem.tag == 'name':\n                map_name = elem.text\n            elif elem.tag == 'folder':\n                map_folder = elem.text\n            elif elem.tag == 'region':\n                bbox = MapSource.parse_xml_boundary(elem)\n            elif elem.tag == 'layers':\n                layers = MapSource.parse_xml_layers(elem)\n\n        if map_folder is None:\n            map_folder = \"\" # fallback if bad specification in xml\n        if layers is None:  # layers tag not found, expect url etc. in main xmldoc\n            layers = [MapSource.parse_xml_layer(xmldoc)]\n\n        ms = MapSource(map_id, map_name, map_folder, bbox=bbox)\n        ms.layers = layers\n        return ms"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_saml_slos(cls, logout_request):\n        try:\n            root = etree.fromstring(logout_request)\n            return root.xpath(\n                \"//samlp:SessionIndex\",\n                namespaces={'samlp': \"urn:oasis:names:tc:SAML:2.0:protocol\"})\n        except etree.XMLSyntaxError:\n            return None", "response": "returns saml logout ticket info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_logout_request(cls, logout_request, ticket):\n        try:\n            session_index = cls.get_saml_slos(logout_request)\n            session_index = session_index[0].text\n            if session_index == ticket:\n                return True\n            else:\n                return False\n        except (AttributeError, IndexError):\n            return False", "response": "verifies the logout request came from the CAS server returns True if the logout_request is valid False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_proxy_ticket(self, pgt):\n        response = requests.get(self.get_proxy_url(pgt))\n        if response.status_code == 200:\n            from lxml import etree\n            root = etree.fromstring(response.content)\n            tickets = root.xpath(\n                \"//cas:proxyTicket\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(tickets) == 1:\n                return tickets[0].text\n            errors = root.xpath(\n                \"//cas:authenticationFailure\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(errors) == 1:\n                raise CASError(errors[0].attrib['code'], errors[0].text)\n        raise CASError(\"Bad http code %s\" % response.status_code)", "response": "Returns proxy ticket given the proxy granting ticket"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_ticket(self, ticket):\n        params = [('ticket', ticket), ('service', self.service_url)]\n        url = (urllib_parse.urljoin(self.server_url, 'validate') + '?' +\n               urllib_parse.urlencode(params))\n        page = requests.get(\n            url,\n            stream=True,\n            verify=self.verify_ssl_certificate\n        )\n        try:\n            page_iterator = page.iter_lines(chunk_size=8192)\n            verified = next(page_iterator).strip()\n            if verified == 'yes':\n                return next(page_iterator).strip(), None, None\n            else:\n                return None, None, None\n        finally:\n            page.close()", "response": "Verifies CAS 1. 0 authentication ticket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_ticket(self, ticket, **kwargs):\n\n        try:\n            from xml.etree import ElementTree\n        except ImportError:\n            from elementtree import ElementTree\n\n        page = self.fetch_saml_validation(ticket)\n\n        try:\n            user = None\n            attributes = {}\n            response = page.content\n            tree = ElementTree.fromstring(response)\n            # Find the authentication status\n            success = tree.find('.//' + SAML_1_0_PROTOCOL_NS + 'StatusCode')\n            if success is not None and success.attrib['Value'].endswith(':Success'):\n                # User is validated\n                name_identifier = tree.find('.//' + SAML_1_0_ASSERTION_NS + 'NameIdentifier')\n                if name_identifier is not None:\n                    user = name_identifier.text\n                attrs = tree.findall('.//' + SAML_1_0_ASSERTION_NS + 'Attribute')\n                for at in attrs:\n                    if self.username_attribute in list(at.attrib.values()):\n                        user = at.find(SAML_1_0_ASSERTION_NS + 'AttributeValue').text\n                        attributes['uid'] = user\n\n                    values = at.findall(SAML_1_0_ASSERTION_NS + 'AttributeValue')\n                    if len(values) > 1:\n                        values_array = []\n                        for v in values:\n                            values_array.append(v.text)\n                            attributes[at.attrib['AttributeName']] = values_array\n                    else:\n                        attributes[at.attrib['AttributeName']] = values[0].text\n            return user, attributes, None\n        finally:\n            page.close()", "response": "Verifies CAS 3. 0 + XML - based authentication ticket and returns username and attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if any of instances match an active switch. Otherwise returns False.", "response": "def is_active(self, key, *instances, **kwargs):\n        \"\"\"\n        Returns ``True`` if any of ``instances`` match an active switch. Otherwise\n        returns ``False``.\n\n        >>> gargoyle.is_active('my_feature', request) #doctest: +SKIP\n        \"\"\"\n        default = kwargs.pop('default', False)\n\n        # Check all parents for a disabled state\n        parts = key.split(':')\n        if len(parts) > 1:\n            child_kwargs = kwargs.copy()\n            child_kwargs['default'] = None\n            result = self.is_active(':'.join(parts[:-1]), *instances, **child_kwargs)\n\n            if result is False:\n                return result\n            elif result is True:\n                default = result\n\n        try:\n            switch = self[key]\n        except KeyError:\n            # switch is not defined, defer to parent\n            return default\n\n        if switch.status == GLOBAL:\n            return True\n        elif switch.status == DISABLED:\n            return False\n        elif switch.status == INHERIT:\n            return default\n\n        conditions = switch.value\n        # If no conditions are set, we inherit from parents\n        if not conditions:\n            return default\n\n        if instances:\n            # HACK: support request.user by swapping in User instance\n            instances = list(instances)\n            for v in instances:\n                if isinstance(v, HttpRequest) and hasattr(v, 'user'):\n                    instances.append(v.user)\n\n        # check each switch to see if it can execute\n        return_value = False\n\n        for switch in self._registry.itervalues():\n            result = switch.has_active_condition(conditions, instances)\n            if result is False:\n                return False\n            elif result is True:\n                return_value = True\n\n        # there were no matching conditions, so it must not be enabled\n        return return_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register(self, condition_set):\n\n        if callable(condition_set):\n            condition_set = condition_set()\n        self._registry[condition_set.get_id()] = condition_set", "response": "Registers a condition set with the manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unregister(self, condition_set):\n        if callable(condition_set):\n            condition_set = condition_set()\n        self._registry.pop(condition_set.get_id(), None)", "response": "Unregisters a condition set with the manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a generator which yields groups of lists of conditions.", "response": "def get_all_conditions(self):\n        \"\"\"\n        Returns a generator which yields groups of lists of conditions.\n\n        >>> for set_id, label, field in gargoyle.get_all_conditions(): #doctest: +SKIP\n        >>>     print \"%(label)s: %(field)s\" % (label, field.label) #doctest: +SKIP\n        \"\"\"\n        for condition_set in sorted(self.get_condition_sets(), key=lambda x: x.get_group_label()):\n            group = unicode(condition_set.get_group_label())\n            for field in condition_set.fields.itervalues():\n                yield condition_set.get_id(), group, field"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an instance and the name of an attribute returns the value of that attribute on the instance.", "response": "def get_field_value(self, instance, field_name):\n        \"\"\"\n        Given an instance, and the name of an attribute, returns the value\n        of that attribute on the instance.\n\n        Default behavior will map the ``percent`` attribute to ``id``.\n        \"\"\"\n        # XXX: can we come up w/ a better API?\n        # Ensure we map ``percent`` to the ``id`` column\n        if field_name == 'percent':\n            field_name = 'id'\n        value = getattr(instance, field_name)\n        if callable(value):\n            value = value()\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a boolean reprsenting if any of the conditions active for the given list of instances.", "response": "def has_active_condition(self, conditions, instances):\n        \"\"\"\n        Given a list of instances, and the conditions active for\n        this switch, returns a boolean reprsenting if any\n        conditional is met, including a non-instance default.\n        \"\"\"\n        return_value = None\n        for instance in itertools.chain(instances, [None]):\n            if not self.can_execute(instance):\n                continue\n            result = self.is_active(instance, conditions)\n            if result is False:\n                return False\n            elif result is True:\n                return_value = True\n        return return_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a boolean representing if the feature is active for this switch.", "response": "def is_active(self, instance, conditions):\n        \"\"\"\n        Given an instance, and the conditions active for this switch, returns\n        a boolean representing if the feature is active.\n        \"\"\"\n        return_value = None\n        for name, field in self.fields.iteritems():\n            field_conditions = conditions.get(self.get_namespace(), {}).get(name)\n            if field_conditions:\n                value = self.get_field_value(instance, name)\n                for status, condition in field_conditions:\n                    exclude = status == EXCLUDE\n                    if field.is_active(condition, value):\n                        if exclude:\n                            return False\n                        return_value = True\n                    else:\n                        if exclude:\n                            return_value = True\n        return return_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the switch is active.", "response": "def is_active(self, instance, conditions):\n        \"\"\"\n        value is the current value of the switch\n        instance is the instance of our type\n        \"\"\"\n        if isinstance(instance, User):\n            return super(UserConditionSet, self).is_active(instance, conditions)\n\n        # HACK: allow is_authenticated to work on AnonymousUser\n        condition = conditions.get(self.get_namespace(), {}).get('is_anonymous')\n        if condition is not None:\n            return bool(condition)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new condition and registers it in the global gargoyle switch manager.", "response": "def add_condition(self, manager, condition_set, field_name, condition, exclude=False, commit=True):\n        \"\"\"\n        Adds a new condition and registers it in the global ``gargoyle`` switch manager.\n\n        If ``commit`` is ``False``, the data will not be written to the database.\n\n        >>> switch = gargoyle['my_switch'] #doctest: +SKIP\n        >>> condition_set_id = condition_set.get_id() #doctest: +SKIP\n        >>> switch.add_condition(condition_set_id, 'percent', [0, 50], exclude=False) #doctest: +SKIP\n        \"\"\"\n        condition_set = manager.get_condition_set_by_id(condition_set)\n\n        assert isinstance(condition, basestring), 'conditions must be strings'\n\n        namespace = condition_set.get_namespace()\n\n        if namespace not in self.value:\n            self.value[namespace] = {}\n        if field_name not in self.value[namespace]:\n            self.value[namespace][field_name] = []\n        if condition not in self.value[namespace][field_name]:\n            self.value[namespace][field_name].append((exclude and EXCLUDE or INCLUDE, condition))\n\n        if commit:\n            self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear all conditions given a set of parameters.", "response": "def clear_conditions(self, manager, condition_set, field_name=None, commit=True):\n        \"\"\"\n        Clears conditions given a set of parameters.\n\n        If ``commit`` is ``False``, the data will not be written to the database.\n\n        Clear all conditions given a ConditionSet, and a field name:\n\n        >>> switch = gargoyle['my_switch'] #doctest: +SKIP\n        >>> condition_set_id = condition_set.get_id() #doctest: +SKIP\n        >>> switch.clear_conditions(condition_set_id, 'percent') #doctest: +SKIP\n\n        You can also clear all conditions given a ConditionSet:\n\n        >>> switch = gargoyle['my_switch'] #doctest: +SKIP\n        >>> condition_set_id = condition_set.get_id() #doctest: +SKIP\n        >>> switch.clear_conditions(condition_set_id) #doctest: +SKIP\n        \"\"\"\n        condition_set = manager.get_condition_set_by_id(condition_set)\n\n        namespace = condition_set.get_namespace()\n\n        if namespace not in self.value:\n            return\n\n        if not field_name:\n            del self.value[namespace]\n        elif field_name not in self.value[namespace]:\n            return\n        else:\n            del self.value[namespace][field_name]\n\n        if commit:\n            self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize(self):\n        res = []\n        for _sort in self.sort_orders:\n            res.append(_sort.serialize())\n        return res or None", "response": "Serialize the search to a structure as passed for a search body."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a term factory facet", "response": "def add_term_facet(self, *args, **kwargs):\n        \"\"\"Add a term factory facet\"\"\"\n        self.facets.append(TermFacet(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_date_facet(self, *args, **kwargs):\n        self.facets.append(DateHistogramFacet(*args, **kwargs))", "response": "Add a date factory facet"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_geo_facet(self, *args, **kwargs):\n        self.facets.append(GeoDistanceFacet(*args, **kwargs))", "response": "Add a geo factory facet"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a class or class from a name.", "response": "def symbol_by_name(name, aliases={}, imp=None, package=None,\n        sep='.', default=None, **kwargs):\n    \"\"\"Get symbol by qualified name.\n\n    The name should be the full dot-separated path to the class::\n\n        modulename.ClassName\n\n    Example::\n\n        celery.concurrency.processes.TaskPool\n                                    ^- class name\n\n    or using ':' to separate module and symbol::\n\n        celery.concurrency.processes:TaskPool\n\n    If `aliases` is provided, a dict containing short name/long name\n    mappings, the name is looked up in the aliases first.\n\n    Examples:\n\n        >>> symbol_by_name(\"celery.concurrency.processes.TaskPool\")\n        <class 'celery.concurrency.processes.TaskPool'>\n\n        >>> symbol_by_name(\"default\", {\n        ...     \"default\": \"celery.concurrency.processes.TaskPool\"})\n        <class 'celery.concurrency.processes.TaskPool'>\n\n        # Does not try to look up non-string names.\n        >>> from celery.concurrency.processes import TaskPool\n        >>> symbol_by_name(TaskPool) is TaskPool\n        True\n\n    \"\"\"\n    if imp is None:\n        imp = importlib.import_module\n\n    if not isinstance(name, six.string_types):\n        return name                                 # already a class\n\n    name = aliases.get(name) or name\n    sep = ':' if ':' in name else sep\n    module_name, _, cls_name = name.rpartition(sep)\n    if not module_name:\n        cls_name, module_name = None, package if package else cls_name\n    try:\n        try:\n            module = imp(module_name, package=package, **kwargs)\n        except ValueError as exc:\n            raise ValueError(\"Couldn't import %r: %s\" % (name, exc))\n        return getattr(module, cls_name) if cls_name else module\n    except (ImportError, AttributeError):\n        if default is None:\n            raise\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nversions of : func : imp. find_module supporting dots.", "response": "def find_module(module, path=None, imp=None):\n    \"\"\"Version of :func:`imp.find_module` supporting dots.\"\"\"\n    if imp is None:\n        imp = importlib.import_module\n    with cwd_in_path():\n        if \".\" in module:\n            last = None\n            parts = module.split(\".\")\n            for i, part in enumerate(parts[:-1]):\n                mpart = imp(\".\".join(parts[:i + 1]))\n                try:\n                    path = mpart.__path__\n                except AttributeError:\n                    raise NotAPackage(module)\n                last = _imp.find_module(parts[i + 1], path)\n            return last\n        return _imp.find_module(module)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports module from current directory.", "response": "def import_from_cwd(module, imp=None, package=None):\n    \"\"\"Import module, but make sure it finds modules\n    located in the current directory.\n\n    Modules located in the current directory has\n    precedence over modules located in `sys.path`.\n    \"\"\"\n    if imp is None:\n        imp = importlib.import_module\n    with cwd_in_path():\n        return imp(module, package=package)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef file_to_attachment(filename, filehandler=None):\n    if filehandler:\n        return {'_name': filename,\n                'content': base64.b64encode(filehandler.read())\n        }\n    with open(filename, 'rb') as _file:\n        return {'_name': filename,\n                'content': base64.b64encode(_file.read())\n        }", "response": "Convert a file to attachment"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert rogue and mysterious data types into ISO - 8601 strings.", "response": "def default(self, value):\n        \"\"\"Convert rogue and mysterious data types.\n        Conversion notes:\n\n        - ``datetime.date`` and ``datetime.datetime`` objects are\n        converted into datetime strings.\n        \"\"\"\n\n        if isinstance(value, datetime):\n            return value.isoformat()\n        elif isinstance(value, date):\n            dt = datetime(value.year, value.month, value.day, 0, 0, 0)\n            return dt.isoformat()\n        elif isinstance(value, Decimal):\n            return float(str(value))\n        elif isinstance(value, set):\n            return list(value)\n        # raise TypeError\n        return super(ESJsonEncoder, self).default(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes a datetime string to a datetime object", "response": "def string_to_datetime(self, obj):\n        \"\"\"\n        Decode a datetime string to a datetime object\n        \"\"\"\n        if isinstance(obj, six.string_types) and len(obj) == 19:\n            try:\n                return datetime.strptime(obj, \"%Y-%m-%dT%H:%M:%S\")\n            except ValueError:\n                pass\n        if isinstance(obj, six.string_types) and len(obj) > 19:\n            try:\n                return datetime.strptime(obj, \"%Y-%m-%dT%H:%M:%S.%f\")\n            except ValueError:\n                pass\n        if isinstance(obj, six.string_types) and len(obj) == 10:\n            try:\n                return datetime.strptime(obj, \"%Y-%m-%d\")\n            except ValueError:\n                pass\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode datetime value from string to datetime", "response": "def dict_to_object(self, d):\n        \"\"\"\n        Decode datetime value from string to datetime\n        \"\"\"\n        for k, v in list(d.items()):\n            if isinstance(v, six.string_types) and len(v) == 19:\n                # Decode a datetime string to a datetime object\n                try:\n                    d[k] = datetime.strptime(v, \"%Y-%m-%dT%H:%M:%S\")\n                except ValueError:\n                    pass\n            elif isinstance(v, six.string_types) and len(v) > 20:\n                try:\n                    d[k] = datetime.strptime(v, \"%Y-%m-%dT%H:%M:%S.%f\")\n                except ValueError:\n                    pass\n            elif isinstance(v, list):\n                d[k] = [self.string_to_datetime(elem) for elem in v]\n        return DotDict(d)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking the servers variable and convert in a valid tuple form", "response": "def _check_servers(self):\n        \"\"\"Check the servers variable and convert in a valid tuple form\"\"\"\n        new_servers = []\n\n        def check_format(server):\n            if server.scheme not in [\"thrift\", \"http\", \"https\"]:\n                raise RuntimeError(\"Unable to recognize protocol: \\\"%s\\\"\" % _type)\n\n            if server.scheme == \"thrift\":\n                if not thrift_connect:\n                    raise RuntimeError(\"If you want to use thrift, please install thrift. \\\"pip install thrift\\\"\")\n                if server.port is None:\n                    raise RuntimeError(\"If you want to use thrift, please provide a port number\")\n\n            new_servers.append(server)\n\n        for server in self.servers:\n            if isinstance(server, (tuple, list)):\n                if len(list(server)) != 3:\n                    raise RuntimeError(\"Invalid server definition: \\\"%s\\\"\" % repr(server))\n                _type, host, port = server\n                server = urlparse('%s://%s:%s' % (_type, host, port))\n                check_format(server)\n            elif isinstance(server, six.string_types):\n                if server.startswith((\"thrift:\", \"http:\", \"https:\")):\n                    server = urlparse(server)\n                    check_format(server)\n                    continue\n                else:\n                    tokens = [t for t in server.split(\":\") if t.strip()]\n                    if len(tokens) == 2:\n                        host = tokens[0]\n                        try:\n                            port = int(tokens[1])\n                        except ValueError:\n                            raise RuntimeError(\"Invalid port: \\\"%s\\\"\" % tokens[1])\n\n                        if 9200 <= port <= 9299:\n                            _type = \"http\"\n                        elif 9500 <= port <= 9599:\n                            _type = \"thrift\"\n                        else:\n                            raise RuntimeError(\"Unable to recognize port-type: \\\"%s\\\"\" % port)\n\n                        server = urlparse('%s://%s:%s' % (_type, host, port))\n                        check_format(server)\n\n        self.servers = new_servers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_connection(self):\n        #detect connectiontype\n        if not self.servers:\n            raise RuntimeError(\"No server defined\")\n\n        server = random.choice(self.servers)\n        if server.scheme in [\"http\", \"https\"]:\n            self.connection = http_connect(\n                [server for server in self.servers if server.scheme in [\"http\", \"https\"]],\n                timeout=self.timeout, basic_auth=self.basic_auth, max_retries=self.max_retries, retry_time=self.retry_time)\n            return\n        elif server.scheme == \"thrift\":\n            self.connection = thrift_connect(\n                [server for server in self.servers if server.scheme == \"thrift\"],\n                timeout=self.timeout, max_retries=self.max_retries, retry_time=self.retry_time)", "response": "Create initial connection pool\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind other servers asking nodes to given server", "response": "def _discovery(self):\n        \"\"\"\n        Find other servers asking nodes to given server\n        \"\"\"\n        data = self.cluster_nodes()\n        self.cluster_name = data[\"cluster_name\"]\n        for _, nodedata in list(data[\"nodes\"].items()):\n            server = nodedata['http_address'].replace(\"]\", \"\").replace(\"inet[\", \"http:/\")\n            if server not in self.servers:\n                self.servers.append(server)\n        self._init_connection()\n        return self.servers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_bulk_size(self, bulk_size):\n        self._bulk_size = bulk_size\n        self.bulker.bulk_size = bulk_size", "response": "Set the bulk size of the log entries"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_raise_on_bulk_item_failure(self, raise_on_bulk_item_failure):\n        self._raise_on_bulk_item_failure = raise_on_bulk_item_failure\n        self.bulker.raise_on_bulk_item_failure = raise_on_bulk_item_failure", "response": "Set the raise_on_bulk_item_failure property of the bulker instance"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate_indices(self, indices=None):\n        if indices is None:\n            indices = self.default_indices\n        if isinstance(indices, six.string_types):\n            indices = [indices]\n        return indices", "response": "Return a valid list of indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_types(self, types=None):\n        types = types or self.default_types\n        if types is None:\n            types = []\n        if isinstance(types, six.string_types):\n            types = [types]\n        return types", "response": "Return a valid list of types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a bulker object and return it", "response": "def create_bulker(self):\n        \"\"\"\n        Create a bulker object and return it to allow to manage custom bulk policies\n        \"\"\"\n        return self.bulker_class(self, bulk_size=self.bulk_size,\n                                 raise_on_bulk_item_failure=self.raise_on_bulk_item_failure)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure if an index with mapping exists", "response": "def ensure_index(self, index, mappings=None, settings=None, clear=False):\n        \"\"\"\n        Ensure if an index with mapping exists\n        \"\"\"\n        mappings = mappings or []\n        if isinstance(mappings, dict):\n            mappings = [mappings]\n        exists = self.indices.exists_index(index)\n        if exists and not mappings and not clear:\n            return\n        if exists and clear:\n            self.indices.delete_index(index)\n            exists = False\n\n        if exists:\n            if not mappings:\n                self.indices.delete_index(index)\n                self.indices.refresh()\n                self.indices.create_index(index, settings)\n                return\n\n            if clear:\n                for maps in mappings:\n                    for key in list(maps.keys()):\n                        self.indices.delete_mapping(index, doc_type=key)\n                self.indices.refresh()\n            if isinstance(mappings, SettingsBuilder):\n                for name, data in list(mappings.mappings.items()):\n                    self.indices.put_mapping(doc_type=name, mapping=data, indices=index)\n\n            else:\n                from pyes.mappings import DocumentObjectField, ObjectField\n\n                for maps in mappings:\n                    if isinstance(maps, tuple):\n                        name, mapping = maps\n                        self.indices.put_mapping(doc_type=name, mapping=mapping, indices=index)\n                    elif isinstance(maps, dict):\n                        for name, data in list(maps.items()):\n                            self.indices.put_mapping(doc_type=name, mapping=maps, indices=index)\n                    elif isinstance(maps, (DocumentObjectField, ObjectField)):\n                        self.put_mapping(doc_type=maps.name, mapping=maps.as_dict(), indices=index)\n\n                return\n\n        if settings:\n            if isinstance(settings, dict):\n                settings = SettingsBuilder(settings, mappings)\n        else:\n            if isinstance(mappings, SettingsBuilder):\n                settings = mappings\n            else:\n                settings = SettingsBuilder(mappings=mappings)\n        if not exists:\n            self.indices.create_index(index, settings)\n            self.indices.refresh(index, timesleep=1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nput new warmer into index (or type) :param doc_types: list of document types :param warmer: anything with ``serialize`` method or a dictionary :param name: warmer name :param querystring_args: additional arguments passed as GET params to ES", "response": "def put_warmer(self, doc_types=None, indices=None, name=None, warmer=None, querystring_args=None):\n        \"\"\"\n        Put new warmer into index (or type)\n\n        :param doc_types: list of document types\n        :param warmer: anything with ``serialize`` method or a dictionary\n        :param name: warmer name\n        :param querystring_args: additional arguments passed as GET params to ES\n        \"\"\"\n        if not querystring_args:\n            querystring_args = {}\n        doc_types_str = ''\n        if doc_types:\n            doc_types_str = '/' + ','.join(doc_types)\n        path = '/{0}{1}/_warmer/{2}'.format(','.join(indices), doc_types_str, name)\n        if hasattr(warmer, 'serialize'):\n            body = warmer.serialize()\n        else:\n            body = warmer\n        return self._send_request(method='PUT', path=path, body=body, params=querystring_args)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_warmer(self, doc_types=None, indices=None, name=None, querystring_args=None):\n        name = name or ''\n        if not querystring_args:\n            querystring_args = {}\n        doc_types_str = ''\n        if doc_types:\n            doc_types_str = '/' + ','.join(doc_types)\n        path = '/{0}{1}/_warmer/{2}'.format(','.join(indices), doc_types_str, name)\n\n        return self._send_request(method='GET', path=path, params=querystring_args)", "response": "Retrieve a warmer from the ES server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect_info(self):\n        try:\n            info = {}\n            res = self._send_request('GET', \"/\")\n            info['server'] = {}\n            info['server']['name'] = res['name']\n            info['server']['version'] = res['version']\n            info['allinfo'] = res\n            info['status'] = self.cluster.status()\n            info['aliases'] = self.indices.aliases()\n            self.info = info\n            return True\n        except:\n            self.info = {}\n            return False", "response": "Collect info about the connection and fill the info dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index_raw_bulk(self, header, document):\n        self.bulker.add(\"%s%s\" % (header, document))\n        return self.flush_bulk()", "response": "Function helper for fast inserting\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index(self, doc, index, doc_type, id=None, parent=None, force_insert=False,\n              op_type=None, bulk=False, version=None, querystring_args=None, ttl=None):\n        \"\"\"\n        Index a typed JSON document into a specific index and make it searchable.\n        \"\"\"\n        if querystring_args is None:\n            querystring_args = {}\n\n        if bulk:\n            if op_type is None:\n                op_type = \"index\"\n            if force_insert:\n                op_type = \"create\"\n            cmd = {op_type: {\"_index\": index, \"_type\": doc_type}}\n            if parent:\n                cmd[op_type]['_parent'] = parent\n            if version:\n                cmd[op_type]['_version'] = version\n            if 'routing' in querystring_args:\n                cmd[op_type]['_routing'] = querystring_args['routing']\n            if 'percolate' in querystring_args:\n                cmd[op_type]['percolate'] = querystring_args['percolate']\n            if id is not None:  #None to support 0 as id\n                cmd[op_type]['_id'] = id\n            if ttl is not None:\n                cmd[op_type]['_ttl'] = ttl\n\n            if isinstance(doc, dict):\n                doc = json.dumps(doc, cls=self.encoder)\n            command = \"%s\\n%s\" % (json.dumps(cmd, cls=self.encoder), doc)\n            self.bulker.add(command)\n            return self.flush_bulk()\n\n        if force_insert:\n            querystring_args['op_type'] = 'create'\n        if op_type:\n            querystring_args['op_type'] = op_type\n\n        if parent:\n            if not isinstance(parent, str):\n                parent = str(parent)\n            querystring_args['parent'] = parent\n\n        if version:\n            if not isinstance(version, str):\n                version = str(version)\n            querystring_args['version'] = version\n\n        if ttl is not None:\n            if not isinstance(ttl, str):\n                ttl = str(ttl)\n            querystring_args['ttl'] = ttl\n\n        if id is None:\n            request_method = 'POST'\n        else:\n            request_method = 'PUT'\n\n        path = make_path(index, doc_type, id)\n        return self._send_request(request_method, path, doc, querystring_args)", "response": "Index a typed JSON document into a specific index and make it searchable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_file(self, filename, index, doc_type, id=None, name=None):\n        if id is None:\n            request_method = 'POST'\n        else:\n            request_method = 'PUT'\n        path = make_path(index, doc_type, id)\n        doc = file_to_attachment(filename)\n        if name:\n            doc[\"_name\"] = name\n        return self._send_request(request_method, path, doc)", "response": "Store a file in an index"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file(self, index, doc_type, id=None):\n        data = self.get(index, doc_type, id)\n        return data['_name'], base64.standard_b64decode(data['content'])", "response": "Get the filename and memory data stream of a specific document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_by_function(self, extra_doc, index, doc_type, id, querystring_args=None,\n                           update_func=None, attempts=2):\n        \"\"\"\n        Update an already indexed typed JSON document.\n\n        The update happens client-side, i.e. the current document is retrieved,\n        updated locally and finally pushed to the server. This may repeat up to\n        ``attempts`` times in case of version conflicts.\n\n        :param update_func: A callable ``update_func(current_doc, extra_doc)``\n            that computes and returns the updated doc. Alternatively it may\n            update ``current_doc`` in place and return None. The default\n            ``update_func`` is ``dict.update``.\n\n        :param attempts: How many times to retry in case of version conflict.\n        \"\"\"\n        if querystring_args is None:\n            querystring_args = {}\n\n        if update_func is None:\n            update_func = dict.update\n\n        for attempt in range(attempts - 1, -1, -1):\n            current_doc = self.get(index, doc_type, id, **querystring_args)\n            new_doc = update_func(current_doc, extra_doc)\n            if new_doc is None:\n                new_doc = current_doc\n            try:\n                return self.index(new_doc, index, doc_type, id,\n                                  version=current_doc._meta.version, querystring_args=querystring_args)\n            except VersionConflictEngineException:\n                if attempt <= 0:\n                    raise\n                self.refresh(index)", "response": "Update an already indexed typed JSON document by a function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a typed JSON document from a specific index based on its id.", "response": "def delete(self, index, doc_type, id, bulk=False, **query_params):\n        \"\"\"\n        Delete a typed JSON document from a specific index based on its id.\n        If bulk is True, the delete operation is put in bulk mode.\n        \"\"\"\n        if bulk:\n            cmd = {\"delete\": {\"_index\": index, \"_type\": doc_type,\n                              \"_id\": id}}\n            self.bulker.add(json.dumps(cmd, cls=self.encoder))\n            return self.flush_bulk()\n\n        path = make_path(index, doc_type, id)\n        return self._send_request('DELETE', path, params=query_params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting documents from one or more indices based on a query.", "response": "def delete_by_query(self, indices, doc_types, query, **query_params):\n        \"\"\"\n        Delete documents from one or more indices and one or more types based on a query.\n        \"\"\"\n        path = self._make_path(indices, doc_types, '_query')\n        body = {\"query\": query.serialize()}\n        return self._send_request('DELETE', path, body, query_params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exists(self, index, doc_type, id, **query_params):\n        path = make_path(index, doc_type, id)\n        return self._send_request('HEAD', path, params=query_params)", "response": "Check if a document exists in the index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a typed JSON document from an index based on its id.", "response": "def get(self, index, doc_type, id, fields=None, model=None, **query_params):\n        \"\"\"\n        Get a typed JSON document from an index based on its id.\n        \"\"\"\n        path = make_path(index, doc_type, id)\n        if fields is not None:\n            query_params[\"fields\"] = \",\".join(fields)\n        model = model or self.model\n        return model(self, self._send_request('GET', path, params=query_params))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a stub object to be manipulated by the user.", "response": "def factory_object(self, index, doc_type, data=None, id=None):\n        \"\"\"\n        Create a stub object to be manipulated\n        \"\"\"\n        data = data or {}\n        obj = self.model()\n        obj._meta.index = index\n        obj._meta.type = doc_type\n        obj._meta.connection = self\n        if id:\n            obj._meta.id = id\n        if data:\n            obj.update(data)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget multi JSON documents.", "response": "def mget(self, ids, index=None, doc_type=None, **query_params):\n        \"\"\"\n        Get multi JSON documents.\n\n        ids can be:\n            list of tuple: (index, type, id)\n            list of ids: index and doc_type are required\n        \"\"\"\n        if not ids:\n            return []\n\n        body = []\n        for value in ids:\n            if isinstance(value, tuple):\n                if len(value) == 3:\n                    a, b, c = value\n                    body.append({\"_index\": a,\n                                 \"_type\": b,\n                                 \"_id\": c})\n                elif len(value) == 4:\n                    a, b, c, d = value\n                    body.append({\"_index\": a,\n                                 \"_type\": b,\n                                 \"_id\": c,\n                                 \"fields\": d})\n\n            else:\n                if index is None:\n                    raise InvalidQuery(\"index value is required for id\")\n                if doc_type is None:\n                    raise InvalidQuery(\"doc_type value is required for id\")\n                body.append({\"_index\": index,\n                             \"_type\": doc_type,\n                             \"_id\": value})\n\n        results = self._send_request('GET', \"/_mget\", body={'docs': body},\n                                     params=query_params)\n        if 'docs' in results:\n            model = self.model\n            return [model(self, item) for item in results['docs']]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a search against one or more indices to get the search hits.", "response": "def search_raw(self, query, indices=None, doc_types=None, headers=None, **query_params):\n        \"\"\"Execute a search against one or more indices to get the search hits.\n\n        `query` must be a Search object, a Query object, or a custom\n        dictionary of search parameters using the query DSL to be passed\n        directly.\n        \"\"\"\n        from .query import Search, Query\n\n        if isinstance(query, Query):\n            query = query.search()\n        if isinstance(query, Search):\n            query = query.serialize()\n        body = self._encode_query(query)\n        path = self._make_path(indices, doc_types, \"_search\")\n        return self._send_request('GET', path, body, params=query_params, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a search against one or more indices to get the resultset.", "response": "def search(self, query, indices=None, doc_types=None, model=None, scan=False, headers=None, **query_params):\n        \"\"\"Execute a search against one or more indices to get the resultset.\n\n        `query` must be a Search object, a Query object, or a custom\n        dictionary of search parameters using the query DSL to be passed\n        directly.\n        \"\"\"\n        if isinstance(query, Search):\n            search = query\n        elif isinstance(query, (Query, dict)):\n            search = Search(query)\n        else:\n            raise InvalidQuery(\"search() must be supplied with a Search or Query object, or a dict\")\n\n        if scan:\n            query_params.setdefault(\"search_type\", \"scan\")\n            query_params.setdefault(\"scroll\", \"10m\")\n\n        return ResultSet(self, search, indices=indices, doc_types=doc_types,\n                         model=model, query_params=query_params, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef suggest(self, name, text, field, type='term', size=None, params=None,\n                **kwargs):\n        \"\"\"\n        Execute suggester of given type.\n\n        :param name: name for the suggester\n        :param text: text to search for\n        :param field: field to search\n        :param type: One of: completion, phrase, term\n        :param size: number of results\n        :param params: additional suggester params\n        :param kwargs:\n        :return:\n        \"\"\"\n\n        from .query import Suggest\n\n        suggest = Suggest()\n\n        suggest.add(text, name, field, type=type, size=size, params=params)\n\n        return self.suggest_from_object(suggest, **kwargs)", "response": "Execute suggester of given type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a query against one or more indices and get the number of hits.", "response": "def count(self, query=None, indices=None, doc_types=None, **query_params):\n        \"\"\"\n        Execute a query against one or more indices and get hits count.\n        \"\"\"\n        from .query import MatchAllQuery\n\n        if query is None:\n            query = MatchAllQuery()\n        body = {\"query\": query.serialize()}\n        path = self._make_path(indices, doc_types, \"_count\")\n        return self._send_request('GET', path, body, params=query_params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating meta data for a specific document type.", "response": "def update_mapping_meta(self, doc_type, values, indices=None):\n        \"\"\"\n        Update mapping meta\n        :param doc_type: a doc type or a list of doctypes\n        :param values: the dict of meta\n        :param indices: a list of indices\n        :return:\n        \"\"\"\n        indices = self._validate_indices(indices)\n        for index in indices:\n            mapping = self.mappings.get_doctype(index, doc_type)\n            if mapping is None:\n                continue\n            meta = mapping.get_meta()\n            meta.update(values)\n            mapping = {doc_type: {\"_meta\": meta}}\n            self.indices.put_mapping(doc_type=doc_type, mapping=mapping, indices=indices)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef morelikethis(self, index, doc_type, id, fields, **query_params):\n        path = make_path(index, doc_type, id, '_mlt')\n        query_params['mlt_fields'] = ','.join(fields)\n        body = query_params[\"body\"] if \"body\" in query_params else None\n        return self._send_request('GET', path, body=body, params=query_params)", "response": "Execute a more like this search query against one or more fields and get back search hits."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_percolator(self, index, name, query, **kwargs):\n        if isinstance(query, Query):\n            query = {\"query\": query.serialize()}\n        if not isinstance(query, dict):\n            raise InvalidQuery(\"create_percolator() must be supplied with a Query object or dict\")\n        if kwargs:\n            query.update(kwargs)\n\n        path = make_path(index, '.percolator', name)\n        body = json.dumps(query, cls=self.encoder)\n        return self._send_request('PUT', path, body)", "response": "Create a percolator document."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef percolate(self, index, doc_types, query):\n        if doc_types is None:\n            raise RuntimeError('percolate() must be supplied with at least one doc_type')\n\n        path = self._make_path(index, doc_types, '_percolate')\n        body = self._encode_query(query)\n        return self._send_request('GET', path, body)", "response": "Perform a percolation query on a specific set of items in a specific index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize to json a serializable object.", "response": "def encode_json(self, serializable):\n        \"\"\"\n        Serialize to json a serializable object (Search, Query, Filter, etc).\n        \"\"\"\n        return json.dumps(serializable.serialize(), cls=self.encoder)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_aggs(self):\n        aggs = self.aggs\n        for key in list(aggs.keys()):\n            _type = aggs[key].get(\"_type\", \"unknown\")\n            if _type == \"date_histogram\":\n                for entry in aggs[key].get(\"entries\", []):\n                    for k, v in list(entry.items()):\n                        if k in [\"count\", \"max\", \"min\", \"total_count\", \"mean\", \"total\"]:\n                            continue\n                        if not isinstance(entry[k], datetime):\n                            entry[k] = datetime.utcfromtimestamp(v / 1e3)", "response": "This function converts date_histogram aggs to datetime_histogram aggs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fix_keys(self):\n        if not self.valid:\n            return\n\n        for hit in self._results['hits']['hits']:\n            for key, item in list(hit.items()):\n                if key.startswith(\"_\"):\n                    hit[key[1:]] = item\n                    del hit[key]", "response": "Fix the keys of the results to include the _ in the results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the empty highlight from the hit if the record is not in the highlight list", "response": "def clean_highlight(self):\n        \"\"\"\n        Remove the empty highlight\n        \"\"\"\n        if not self.valid:\n            return\n\n        for hit in self._results['hits']['hits']:\n            if 'highlight' in hit:\n                hl = hit['highlight']\n                for key, item in list(hl.items()):\n                    if not item:\n                        del hl[key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a field to Highlinghter", "response": "def add_field(self, name, fragment_size=150, number_of_fragments=3, fragment_offset=None, order=\"score\", type=None):\n        \"\"\"\n        Add a field to Highlinghter\n        \"\"\"\n        data = {}\n        if fragment_size:\n            data['fragment_size'] = fragment_size\n        if number_of_fragments is not None:\n            data['number_of_fragments'] = number_of_fragments\n        if fragment_offset is not None:\n            data['fragment_offset'] = fragment_offset\n        if type is not None:\n            data['type'] = type\n        data['order'] = order\n        self.fields[name] = data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, *args, **kwargs):\n        clone = self.filter(*args, **kwargs)\n        num = len(clone)\n        if num == 1:\n            return clone._result_cache[0]\n        if not num:\n            raise self.model.DoesNotExist(\n                \"%s matching query does not exist. \"\n                \"Lookup parameters were %s\" %\n                #(self.model._meta.object_name, kwargs))\n                (self.model.__class__.__name__, kwargs))\n        raise self.model.MultipleObjectsReturned(\n            \"get() returned more than one %s -- it returned %s! \"\n            \"Lookup parameters were %s\" %\n            #(self.model._meta.object_name, num, kwargs))\n            (self.model.__class__.__name__, num, kwargs))", "response": "Performs the query and returns a single object matching the given kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, **kwargs):\n        obj = self.model(**kwargs)\n        meta = obj.get_meta()\n        meta.connection = get_es_connection(self.es_url, self.es_kwargs)\n        meta.index=self.index\n        meta.type=self.type\n        obj.save(force=True)\n        return obj", "response": "Creates a new object with the given kwargs saves it to the database\n        and returns the created object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_or_create(self, **kwargs):\n        assert kwargs, \\\n                'get_or_create() must be passed at least one keyword argument'\n        defaults = kwargs.pop('defaults', {})\n        lookup = kwargs.copy()\n        #TODO: check fields\n        try:\n            return self.get(**lookup), False\n        except self.model.DoesNotExist:\n            params = dict([(k, v) for k, v in kwargs.items() if '__' not in k])\n            params.update(defaults)\n            obj = self.model(**params)\n            meta = obj.get_meta()\n            meta.connection = get_es_connection(self.es_url, self.es_kwargs)\n            meta.index=self.index\n            meta.type=self.type\n            obj.save(force=True)\n            return obj, True", "response": "Gets an object with the given kwargs creating one if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef in_bulk(self, id_list):\n        if not id_list:\n            return {}\n        qs = self._clone()\n        qs.add_filter(('pk__in', id_list))\n        qs._clear_ordering(force_empty=True)\n        return dict([(obj._get_pk_val(), obj) for obj in qs])", "response": "Returns a dictionary mapping each of the given IDs to the object with\n        that ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the records in the current QuerySet.", "response": "def delete(self):\n        \"\"\"\n        Deletes the records in the current QuerySet.\n        \"\"\"\n        del_query = self._clone()\n\n        # The delete is actually 2 queries - one to find related objects,\n        # and one to delete. Make sure that the discovery of related\n        # objects is performed on the same database as the deletion.\n        del_query._clear_ordering()\n        get_es_connection(self.es_url, self.es_kwargs).delete_by_query(self.index, self.type, self._build_query())\n        # Clear the result cache, in case this QuerySet gets reused.\n        self._result_cache = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, **kwargs):\n        query = self._build_query()\n        connection = get_es_connection(self.es_url, self.es_kwargs)\n        results = connection.search(query, indices=self.index, doc_types=self.type,\n                                             model=self.model, scan=True)\n        for item in results:\n            item.update(kwargs)\n            item.save(bulk=True)\n        connection.flush_bulk(True)\n        # Clear the result cache, in case this QuerySet gets reused.\n        self._result_cache = None", "response": "Updates all elements in the current QuerySet setting all the given\n        fields to the appropriate values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dates(self, field_name, kind, order='ASC'):\n\n        assert kind in (\"month\", \"year\", \"day\", \"week\", \"hour\", \"minute\"), \\\n                \"'kind' must be one of 'year', 'month', 'day', 'week', 'hour' and 'minute'.\"\n        assert order in ('ASC', 'DESC'), \\\n                \"'order' must be either 'ASC' or 'DESC'.\"\n\n        search= self._build_search()\n        search.facet.reset()\n        search.facet.add_date_facet(name=field_name.replace(\"__\", \".\"),\n                 field=field_name, interval=kind)\n        search.agg.reset()\n        search.agg.add_date_agg(name=field_name.replace(\"__\", \".\"),\n                 field=field_name, interval=kind)\n        search.size=0\n        resulset = get_es_connection(self.es_url, self.es_kwargs).search(search, indices=self.index, doc_types=self.type)\n        resulset.fix_aggs()\n        entries = []\n        for val in resulset.aggs.get(field_name.replace(\"__\", \".\")).get(\"entries\", []):\n            if \"time\" in val:\n                entries.append(val[\"time\"])\n        if order==\"ASC\":\n            return sorted(entries)\n\n        return sorted(entries, reverse=True)", "response": "Returns a list of datetime objects representing all available dates for the given field_name scoped to kind."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef complex_filter(self, filter_obj):\n        if isinstance(filter_obj, Filter):\n            clone = self._clone()\n            clone._filters.add(filter_obj)\n            return clone\n        return self._filter_or_exclude(None, **filter_obj)", "response": "Returns a new QuerySet instance with complex filter_obj added to the filters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new QuerySet with the annotated objects.", "response": "def annotate(self, *args, **kwargs):\n        \"\"\"\n        Return a query set in which the returned objects have been annotated\n        with data aggregated from related fields.\n        \"\"\"\n        obj = self._clone()\n\n        for arg in args:\n            if isinstance(arg, Facet):\n                obj._facets.append(arg)\n            elif isinstance(arg, six.string_types):\n                obj._facets.append(TermFacet(arg.replace(\"__\", \".\")))\n            elif isinstance(arg, Agg):\n                obj._aggs.append(arg)\n            elif isinstance(arg, six.string_types):\n                obj._facets.append(TermFacet(arg.replace(\"__\", \".\")))\n            else:\n                raise NotImplementedError(\"invalid type\")\n\n\n        # Add the aggregates/facet to the query\n        for name, field in kwargs.items():\n            obj._facets.append(TermFacet(field=field.replace(\"__\", \".\"), name=name.replace(\"__\", \".\")))\n            obj._aggs.append(TermsAgg(field=field.replace(\"__\", \".\"), name=name.replace(\"__\", \".\")))\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef order_by(self, *field_names):\n        obj = self._clone()\n        obj._clear_ordering()\n        for field in field_names:\n            if field.startswith(\"-\"):\n                obj._ordering.append({ field.lstrip(\"-\").replace(\"__\", \".\") : \"desc\" })\n            else:\n                obj._ordering.append({ field : \"asc\" })\n        return obj", "response": "Returns a new QuerySet instance with the ordering changed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new queryset with only the specified fields.", "response": "def only(self, *fields):\n        \"\"\"\n        Essentially, the opposite of defer. Only the fields passed into this\n        method and that are not already specified as deferred are loaded\n        immediately when the queryset is evaluated.\n        \"\"\"\n        clone = self._clone()\n        clone._fields=fields\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new QuerySet with the specified alias.", "response": "def using(self, alias):\n        \"\"\"\n        Selects which database this QuerySet should excecute its query against.\n        \"\"\"\n        clone = self._clone()\n        clone._index = alias\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new queryset using class cls using qs using kwargs", "response": "def from_qs(cls, qs, **kwargs):\n        \"\"\"\n        Creates a new queryset using class `cls` using `qs'` data.\n\n        :param qs: The query set to clone\n        :keyword kwargs: The kwargs to pass to _clone method\n        \"\"\"\n        assert issubclass(cls, QuerySet), \"%s is not a QuerySet subclass\" % cls\n        assert isinstance(qs, QuerySet), \"qs has to be an instance of queryset\"\n        return qs._clone(klass=cls, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new mapping to the current mapping.", "response": "def add_mapping(self, data, name=None):\n        \"\"\"\n        Add a new mapping\n        \"\"\"\n        from .mappings import DocumentObjectField\n        from .mappings import NestedObject\n        from .mappings import ObjectField\n\n        if isinstance(data, (DocumentObjectField, ObjectField, NestedObject)):\n            self.mappings[data.name] = data.as_dict()\n            return\n\n        if name:\n            self.mappings[name] = data\n            return\n            if isinstance(data, dict):\n                self.mappings.update(data)\n            elif isinstance(data, list):\n                for d in data:\n                    if isinstance(d, dict):\n                        self.mappings.update(d)\n                    elif isinstance(d, DocumentObjectField):\n                        self.mappings[d.name] = d.as_dict()\n                    else:\n                        name, data = d\n                        self.add_mapping(data, name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_values(instance, go_into={}, exclude=(), extra=(), skip_none=False):\n    from django.db.models.manager import Manager\n    from django.db.models import Model\n\n    SIMPLE_TYPES = (int, long, str, list, dict, tuple, bool, float, bool,\n                    unicode, NoneType)\n\n    if not isinstance(instance, Model):\n        raise TypeError(\"Argument is not a Model\")\n\n    value = {\n        'pk': instance.pk,\n        }\n\n    # check for simple string instead of tuples\n    # and dicts; this is shorthand syntax\n    if isinstance(go_into, str):\n        go_into = {go_into: {}}\n\n    if isinstance(exclude, str):\n        exclude = (exclude,)\n\n    if isinstance(extra, str):\n        extra = (extra,)\n\n    # process the extra properties/function/whatever\n    for field in extra:\n        property = getattr(instance, field)\n\n        if callable(property):\n            property = property()\n\n        if skip_none and property is None:\n            continue\n        elif isinstance(property, SIMPLE_TYPES):\n            value[field] = property\n        else:\n            value[field] = repr(property)\n\n    field_options = instance._meta.get_all_field_names()\n    for field in field_options:\n        try:\n            property = getattr(instance, field)\n        except:\n            continue\n        if skip_none and property is None:\n            continue\n\n        if field in exclude or field[0] == '_' or isinstance(property, Manager):\n            # if it's in the exclude tuple, ignore it\n            # if it's a \"private\" field, ignore it\n            # if it's an instance of manager (this means a more complicated\n            # relationship), ignore it\n            continue\n        elif go_into.has_key(field):\n            # if it's in the go_into dict, make a recursive call for that field\n            try:\n                field_go_into = go_into[field].get('go_into', {})\n            except AttributeError:\n                field_go_into = {}\n\n            try:\n                field_exclude = go_into[field].get('exclude', ())\n            except AttributeError:\n                field_exclude = ()\n\n            try:\n                field_extra = go_into[field].get('extra', ())\n            except AttributeError:\n                field_extra = ()\n\n            value[field] = get_values(property,\n                field_go_into,\n                field_exclude,\n                field_extra, skip_none=skip_none)\n        else:\n            if isinstance(property, Model):\n                # if it's a model, we need it's PK #\n                value[field] = property.pk\n            elif isinstance(property, (datetime.date,\n                                       datetime.time,\n                                       datetime.datetime)):\n                value[field] = property\n            else:\n                # else, we just put the value #\n                if callable(property):\n                    property = property()\n\n                if isinstance(property, SIMPLE_TYPES):\n                    value[field] = property\n                else:\n                    value[field] = repr(property)\n\n    return value", "response": "Returns a dict that can be used for serialization of a single object in a Sequences structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a value to boolean", "response": "def to_bool(value):\n    \"\"\"\n    Convert a value to boolean\n    :param value: the value to convert\n    :type value: any type\n    :return: a boolean value\n    :rtype: a boolean\n    \"\"\"\n    if value is None:\n        return None\n    if isinstance(value, bool):\n        return value\n    elif isinstance(value, str):\n        if value==\"no\":\n            return False\n        elif value==\"yes\":\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a valid Field by given data", "response": "def get_field(name, data, default=\"object\", document_object_field=None, is_document=False):\n    \"\"\"\n    Return a valid Field by given data\n    \"\"\"\n    if isinstance(data, AbstractField):\n        return data\n    data = keys_to_string(data)\n    _type = data.get('type', default)\n    if _type == \"string\":\n        return StringField(name=name, **data)\n    elif _type == \"binary\":\n        return BinaryField(name=name, **data)\n    elif _type == \"boolean\":\n        return BooleanField(name=name, **data)\n    elif _type == \"byte\":\n        return ByteField(name=name, **data)\n    elif _type == \"short\":\n        return ShortField(name=name, **data)\n    elif _type == \"integer\":\n        return IntegerField(name=name, **data)\n    elif _type == \"long\":\n        return LongField(name=name, **data)\n    elif _type == \"float\":\n        return FloatField(name=name, **data)\n    elif _type == \"double\":\n        return DoubleField(name=name, **data)\n    elif _type == \"ip\":\n        return IpField(name=name, **data)\n    elif _type == \"date\":\n        return DateField(name=name, **data)\n    elif _type == \"multi_field\":\n        return MultiField(name=name, **data)\n    elif _type == \"geo_point\":\n        return GeoPointField(name=name, **data)\n    elif _type == \"attachment\":\n        return AttachmentField(name=name, **data)\n    elif is_document or _type == \"document\":\n        if document_object_field:\n            return document_object_field(name=name, **data)\n        else:\n            data.pop(\"name\",None)\n            return DocumentObjectField(name=name, **data)\n\n    elif _type == \"object\":\n        if '_timestamp' in data or \"_all\" in data:\n            if document_object_field:\n                return document_object_field(name=name, **data)\n            else:\n                return DocumentObjectField(name=name, **data)\n\n        return ObjectField(name=name, **data)\n    elif _type == \"nested\":\n        return NestedObject(name=name, **data)\n    raise RuntimeError(\"Invalid type: %s\" % _type)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_diff(self, other_mapping):\n        result = MultiField(name=self.name)\n        new_fields = set(self.fields.keys())\n        if not isinstance(other_mapping, MultiField):\n            n_mapping = MultiField(name=self.name)\n            n_mapping.add_fields([other_mapping])\n            other_mapping = n_mapping\n\n        old_fields = set(other_mapping.fields.keys())\n        #we propagate new fields\n        added = new_fields - old_fields\n        if added:\n            result.add_fields([(add, self.fields[add]) for add in added])\n            #TODO: raise in field changed\n        if len(result.fields) > 0:\n            return result\n        return None", "response": "Returns a Multifield with diff fields. If not changes returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_properties_by_type(self, type, recursive=True, parent_path=\"\"):\n        if parent_path:\n            parent_path += \".\"\n\n        if isinstance(type, str):\n            if type == \"*\":\n                type = set(MAPPING_NAME_TYPE.keys()) - set([\"nested\", \"multi_field\", \"multifield\"])\n            else:\n                type = [type]\n        properties = []\n        for prop in list(self.properties.values()):\n            if prop.type in type:\n                properties.append((parent_path + prop.name, prop))\n                continue\n            elif prop.type == \"multi_field\" and prop.name in prop.fields and prop.fields[prop.name].type in type:\n                properties.append((parent_path + prop.name, prop))\n                continue\n\n            if not recursive:\n                continue\n            if prop.type in [\"nested\", \"object\"]:\n                properties.extend(\n                    prop.get_properties_by_type(type, recursive=recursive, parent_path=parent_path + prop.name))\n        return sorted(properties)", "response": "Returns a list of properties that match the type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a mapped object.", "response": "def get_property_by_name(self, name):\n        \"\"\"\n        Returns a mapped object.\n\n        :param name the name of the property\n        :returns the mapped object or exception NotFoundMapping\n\n        \"\"\"\n        if \".\" not in name and name in self.properties:\n            return self.properties[name]\n\n        tokens = name.split(\".\")\n        object = self\n        for token in tokens:\n            if isinstance(object, (DocumentObjectField, ObjectField, NestedObject)):\n                if token in object.properties:\n                    object = object.properties[token]\n                    continue\n            elif isinstance(object, MultiField):\n                if token in object.fields:\n                    object = object.fields[token]\n                    continue\n            raise MappedFieldNotFoundException(token)\n        if isinstance(object, (AbstractField, MultiField)):\n            return object\n        raise MappedFieldNotFoundException(object)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_available_facets(self):\n        result = []\n        for k, v in list(self.properties.items()):\n            if isinstance(v, DateField):\n                if not v.tokenize:\n                    result.append((k, \"date\"))\n            elif isinstance(v, NumericFieldAbstract):\n                result.append((k, \"numeric\"))\n            elif isinstance(v, StringField):\n                if not v.tokenize:\n                    result.append((k, \"term\"))\n            elif isinstance(v, GeoPointField):\n                if not v.tokenize:\n                    result.append((k, \"geo\"))\n            elif isinstance(v, ObjectField):\n                for n, t in self.get_available_facets():\n                    result.append((self.name + \".\" + k, t))\n        return result", "response": "Returns a list of available facets for the document."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict of property. path and property.", "response": "def get_datetime_properties(self, recursive=True):\n        \"\"\"\n        Returns a dict of property.path and property.\n\n        :param recursive the name of the property\n        :returns a dict\n\n        \"\"\"\n        res = {}\n        for name, field in self.properties.items():\n            if isinstance(field, DateField):\n                res[name] = field\n            elif recursive and isinstance(field, ObjectField):\n                for n, f in field.get_datetime_properties(recursive=recursive):\n                    res[name + \".\" + n] = f\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving two mappings it extracts a schema evolution mapping. Returns None if no evolutions are required", "response": "def get_diff(self, new_mapping):\n        \"\"\"\n        Given two mapping it extracts a schema evolution mapping. Returns None if no evolutions are required\n        :param new_mapping: the new mapping\n        :return: a new evolution mapping or None\n        \"\"\"\n        import copy\n        result = copy.deepcopy(new_mapping)\n        result.clear_properties()\n\n        no_check_types = (BooleanField, IntegerField, FloatField, DateField, LongField, BinaryField,\n                          GeoPointField, IpField)\n\n        old_fields = set(self.properties.keys())\n        new_fields = set(new_mapping.properties.keys())\n        #we propagate new fields\n        added = new_fields - old_fields\n        if added:\n            for add in added:\n                result.add_property(new_mapping.properties[add])\n\n        #we check common fields\n        common_fields = new_fields & old_fields\n\n        #removing standard data\n        common_fields = [c for c in common_fields if not isinstance(new_mapping.properties[c], no_check_types)]\n\n        for field in common_fields:\n            prop = new_mapping.properties[field]\n            if isinstance(prop, StringField):\n                continue\n            if isinstance(prop, MultiField):\n                diff = prop.get_diff(self.properties[field])\n                if diff:\n                    result.add_property(diff)\n            elif isinstance(prop, ObjectField):\n                diff = self.properties[field].get_diff(prop)\n                if diff:\n                    result.add_property(diff)\n\n        if len(result.properties) > 0:\n            return result\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the meta data.", "response": "def get_meta(self, subtype=None):\n        \"\"\"\n        Return the meta data.\n        \"\"\"\n        if subtype:\n            return DotDict(self._meta.get(subtype, {}))\n        return  self._meta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_doctypes(self, index, edges=True):\n        if index not in self.indices:\n            self.get_all_indices()\n        return self.indices.get(index, {})", "response": "Returns a list of doctypes given an index"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_doctype(self, index, name):\n        if index not in self.indices:\n            self.get_all_indices()\n        return self.indices.get(index, {}).get(name, None)", "response": "Returns a doctype given an index and a name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a property of a given type", "response": "def get_property(self, index, doctype, name):\n        \"\"\"\n        Returns a property of a given type\n\n        :return a mapped property\n        \"\"\"\n\n        return self.indices[index][doctype].properties[name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef migrate(self, mapping, index, doc_type):\n        old_mapping = self.get_doctype(index, doc_type)\n        #case missing\n        if not old_mapping:\n            self.connection.indices.put_mapping(doc_type=doc_type, mapping=mapping, indices=index)\n            return mapping\n            # we need to calculate the diff\n        mapping_diff = old_mapping.get_diff(mapping)\n        if not mapping_diff:\n            return None\n        from pprint import pprint\n\n        pprint(mapping_diff.as_dict())\n        mapping_diff.connection = old_mapping.connection\n        mapping_diff.save()", "response": "Migrate an ES mapping to a new index"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the samples. py file with the given number of items", "response": "def generate_datafile_old(number_items=1000):\n    \"\"\"\n    Create the samples.py file\n    \"\"\"\n    from utils import get_names, generate_dataset\n    from pprint import pprint\n    filename = \"samples.py\"\n    dataset = generate_dataset(number_items)\n    fo = open(filename, \"wb\")\n    fo.write(\"#!/usr/bin/env python\\n\")\n    fo.write(\"# -*- coding: utf-8 -*-\\n\")\n    fo.write(\"#Brainaetic: http://www.thenetplanet.com\\n\\n\")\n    fo.write(\"SAMPLES = \")\n    pprint(dataset, fo)\n    fo.close()\n    print \"%s generated with %d samples\" % (filename, number_items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_datafile(number_items=20000):\n    from utils import generate_dataset_shelve\n    filename = \"samples.shelve\"\n    dataset = generate_dataset_shelve(filename, number_items)\n    print \"%s generated with %d samples\" % (filename, number_items)", "response": "Generate the samples. py file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a field to the script_fields dictionary.", "response": "def add_field(self, name, script, lang=None, params=None, ignore_failure=False):\n        \"\"\"\n        Add a field to script_fields\n        \"\"\"\n        data = {}\n        if lang:\n            data[\"lang\"] = lang\n\n        if script:\n            data['script'] = script\n        else:\n            raise ScriptFieldsError(\"Script is required for script_fields definition\")\n        if params:\n            if isinstance(params, dict):\n                if len(params):\n                    data['params'] = params\n            else:\n                raise ScriptFieldsError(\"Parameters should be a valid dictionary\")\n        if ignore_failure:\n            data['ignore_failure'] = ignore_failure\n        self.fields[name] = data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_parameter(self, field_name, param_name, param_value):\n        try:\n            self.fields[field_name]['params'][param_name] = param_value\n        except Exception as ex:\n            raise ScriptFieldsError(\"Error adding parameter %s with value %s :%s\" % (param_name, param_value, ex))\n\n        return self", "response": "Adds a parameter to a field into script_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add(self, text, name, field, type='term', size=None, params=None):\n        func = None\n\n        if type == 'completion':\n            func = self.add_completion\n        elif type == 'phrase':\n            func = self.add_phrase\n        elif type == 'term':\n            func = self.add_term\n        else:\n            raise InvalidQuery('Invalid type')\n\n        func(text=text, name=name, field=field, size=size, params=params)", "response": "Add a new entry to the suggester."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_completion(self, text, name, field, size=None, params=None):\n        data = {\n            'field': field\n        }\n\n        if size:\n            data['size'] = size\n\n        if params:\n            data.update(params)\n\n        self.fields[name] = {\n            'text': text,\n            'completion': data,\n        }", "response": "Add completion - type suggestion for the current suggester."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serialize(self):\n        res = {}\n        if self.query:\n            if isinstance(self.query, dict):\n                res[\"query\"] = self.query\n            elif isinstance(self.query, Query):\n                res[\"query\"] = self.query.serialize()\n            else:\n                raise InvalidQuery(\"Invalid query\")\n        if self.filter:\n            res['filter'] = self.filter.serialize()\n        if self.facet.facets:\n            res['facets'] = self.facet.serialize()\n        if self.agg.aggs:\n            res['aggs'] = self.agg.serialize()\n        if self.rescore:\n            res['rescore'] = self.rescore.serialize()\n        if self.window_size:\n            res['window_size'] = self.window_size\n        if self.fields is not None: #Deal properly with self.fields = []\n            res['fields'] = self.fields\n        if self.size is not None:\n            res['size'] = self.size\n        if self.start is not None:\n            res['from'] = self.start\n        if self._highlight:\n            res['highlight'] = self._highlight.serialize()\n        if self.sort:\n            if isinstance(self.sort, SortFactory):\n                sort = self.sort.serialize()\n                if sort:\n                    res['sort'] = sort\n            else:\n                res['sort'] = self.sort\n        if self.explain:\n            res['explain'] = self.explain\n        if self.version:\n            res['version'] = self.version\n        if self.track_scores:\n            res['track_scores'] = self.track_scores\n        if self._script_fields:\n            if isinstance(self.script_fields, ScriptFields):\n                res['script_fields'] = self.script_fields.serialize()\n            elif isinstance(self.script_fields, dict):\n                res['script_fields'] = self.script_fields\n            else:\n                raise ScriptFieldsError(\"Parameter script_fields should of type ScriptFields or dict\")\n        if self.index_boost:\n            res['indices_boost'] = self.index_boost\n        if self.min_score:\n            res['min_score'] = self.min_score\n        if self.stats:\n            res['stats'] = self.stats\n        if self.partial_fields:\n            res['partial_fields'] = self.partial_fields\n        if self._source:\n            res['_source'] = self._source\n        if self.timeout:\n            res['timeout'] = self.timeout\n        return res", "response": "Serialize the search to a structure as passed for a search body."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_highlight(self, field, fragment_size=None,\n                      number_of_fragments=None, fragment_offset=None, type=None):\n        \"\"\"Add a highlight field.\n\n        The Search object will be returned, so calls to this can be chained.\n\n        \"\"\"\n        if self._highlight is None:\n            self._highlight = HighLighter(\"<b>\", \"</b>\")\n        self._highlight.add_field(field, fragment_size, number_of_fragments, fragment_offset, type=type)\n        return self", "response": "Add a highlight field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_index_boost(self, index, boost):\n        if boost is None:\n            if index in self.index_boost:\n                del(self.index_boost[index])\n        else:\n            self.index_boost[index] = boost\n        return self", "response": "Add a boost on an index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, filter_or_query):\n        if isinstance(filter_or_query, Filter):\n            if self.queries:\n                raise QueryError(\"A Query is required\")\n            self.filters.append(filter_or_query)\n        elif isinstance(filter_or_query, Query):\n            if self.filters:\n                raise QueryError(\"A Filter is required\")\n            self.queries.append(filter_or_query)\n        else:\n            for item in filter_or_query:\n                self.add(item)\n        return self", "response": "Add a filter or a list of filters to the query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing the query to a structure using the query DSL.", "response": "def serialize(self):\n        \"\"\"Serialize the query to a structure using the query DSL.\"\"\"\n        data = {'doc': self.doc}\n        if isinstance(self.query, Query):\n            data['query'] = self.query.serialize()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serialize(self):\n\n        data = {self._internal_name: self.query.serialize()}\n        if self.query_weight is not None:\n            data['query_weight'] = self.query_weight\n        if self.rescore_query_weight is not None:\n            data['rescore_query_weight'] = self.rescore_query_weight\n\n        return data", "response": "Serialize the query to a structure using the query DSL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_names():\n    return [n.strip() for n in codecs.open(os.path.join(\"data\", \"names.txt\"),\"rb\",'utf8').readlines()]", "response": "Return a list of names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_dataset(number_items=1000):\n    data = []\n    names = get_names()\n    totalnames = len(names)\n    #init random seeder\n    random.seed()\n    #calculate items\n#    names = random.sample(names, number_items)\n    for i in range(number_items):\n        data.append({\"name\":names[random.randint(0,totalnames-1)],\n                     \"age\":random.randint(1,100),\n                     \"description\":li_words(50, False)})\n    return data", "response": "Generate a dataset with number_items elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_dataset_shelve(filename, number_items=1000):\n    if os.path.exists(filename):\n        os.remove(filename)\n    data = shelve.open(filename)\n    names = get_names()\n    totalnames = len(names)\n    #init random seeder\n    random.seed()\n    #calculate items\n#    names = random.sample(names, number_items)\n    for i in range(number_items):\n        data[str(i+1)] = {\"name\":names[random.randint(0,totalnames-1)],\n                     \"age\":random.randint(1,100),\n                     \"description\":li_words(50, False)}\n    data.close()", "response": "Generate a dataset with number_items elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ext_process(listname, hostname, url, filepath, msg):\n    from pyes import ES\n    from pyes.exceptions import ClusterBlockException, NoServerAvailable\n    import datetime\n\n    #CHANGE this settings to reflect your configuration\n    _ES_SERVERS = ['127.0.0.1:9500'] # I prefer thrift\n    _indexname = \"mailman\"\n    _doctype = \"mail\"\n    date = datetime.datetime.today()\n\n    try:\n        iconn = ES(_ES_SERVERS)\n        status = None\n        try:\n            status = iconn.status(_indexname)\n            logger.debug(\"Indexer status:%s\" % status)\n        except:\n            iconn.create_index(_indexname)\n            time.sleep(1)\n            status = iconn.status(_indexname)\n            mappings = { u'text': {'boost': 1.0,\n                                     'index': 'analyzed',\n                                     'store': 'yes',\n                                     'type': u'string',\n                                     \"term_vector\" : \"with_positions_offsets\"},\n                             u'url': {'boost': 1.0,\n                                        'index': 'not_analyzed',\n                                        'store': 'yes',\n                                        'type': u'string',\n                                        \"term_vector\" : \"no\"},\n                             u'title': {'boost': 1.0,\n                                        'index': 'analyzed',\n                                        'store': 'yes',\n                                        'type': u'string',\n                                        \"term_vector\" : \"with_positions_offsets\"},\n                             u'date': {'store': 'yes',\n                                        'type': u'date'}}\n            time.sleep(1)\n            status = iconn.put_mapping(_doctype, mappings, _indexname)\n\n\n        data = dict(url=url,\n                    title=msg.get('subject'),\n                    date=date,\n                    text=str(msg)\n                    )\n        iconn.index(data, _indexname, _doctype)\n\n        syslog('debug', 'listname: %s, hostname: %s, url: %s, path: %s, msg: %s',\n               listname, hostname, url, filepath, msg)\n    except ClusterBlockException:\n        syslog('error', 'Cluster in revocery state: listname: %s, hostname: %s, url: %s, path: %s, msg: %s',\n               listname, hostname, url, filepath, msg)\n    except NoServerAvailable:\n        syslog('error', 'No server available: listname: %s, hostname: %s, url: %s, path: %s, msg: %s',\n               listname, hostname, url, filepath, msg)\n    except:\n        import traceback\n        syslog('error', 'Unknown: listname: %s, hostname: %s, url: %s, path: %s, msg: %s\\nstacktrace: %s',\n               listname, hostname, url, filepath, msg, repr(traceback.format_exc()))\n\n    return", "response": "This function is used to deal with the just archived message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a string id from a text value", "response": "def make_id(value):\n    \"\"\"\n    Build a string id from a value\n    :param value: a text value\n    :return: a string\n    \"\"\"\n    if isinstance(value, six.string_types):\n        value = value.encode(\"utf8\", \"ignore\")\n    from hashlib import md5\n\n    val = uuid.UUID(bytes=md5(value).digest(), version=4)\n\n    return base64.urlsafe_b64encode(val.get_bytes())[:-2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_path(*path_components):\n    path_components = [quote(component) for component in path_components if component]\n    path = '/'.join(path_components)\n    if not path.startswith('/'):\n        path = '/' + path\n    return path", "response": "Make a path from a list of path components."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean_string(text):\n    if isinstance(text, six.string_types):\n        return text.translate(UNI_SPECIAL_CHARS).strip()\n    return text.translate(None, STR_SPECIAL_CHARS).strip()", "response": "Remove Lucene reserved characters from query string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions to convert all the unicode keys in string keys", "response": "def keys_to_string(data):\n    \"\"\"\n    Function to convert all the unicode keys in string keys\n    \"\"\"\n    if isinstance(data, dict):\n        for key in list(data.keys()):\n            if isinstance(key, six.string_types):\n                value = data[key]\n                val = keys_to_string(value)\n                del data[key]\n                data[key.encode(\"utf8\", \"ignore\")] = val\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef negate(self):\n        self.from_value, self.to_value = self.to_value, self.from_value\n        self.include_lower, self.include_upper = self.include_upper, self.include_lower", "response": "Reverse the range of the value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nraises an appropriate exception if the result is an error.", "response": "def raise_if_error(status, result, request=None):\n    \"\"\"Raise an appropriate exception if the result is an error.\n\n    Any result with a status code of 400 or higher is considered an error.\n\n    The exception raised will either be an ElasticSearchException, or a more\n    specific subclass of ElasticSearchException if the type is recognised.\n\n    The status code and result can be retrieved from the exception by accessing its\n    status and result properties.\n\n    Optionally, this can take the original RestRequest instance which generated\n    this error, which will then get included in the exception.\n\n    \"\"\"\n    assert isinstance(status, int)\n\n    if status < 400:\n        return\n\n    if status == 404 and isinstance(result, dict) and 'error' not in result:\n        raise exceptions.NotFoundException(\"Item not found\", status, result, request)\n\n    if not isinstance(result, dict) or 'error' not in result:\n        raise exceptions.ElasticSearchException(\"Unknown exception type: %d, %s\" % (status, result), status,\n            result, request)\n\n    error = result['error']\n    if '; nested: ' in error:\n        error_list = error.split('; nested: ')\n        error = error_list[len(error_list) - 1]\n\n    bits = error.split('[', 1)\n    if len(bits) == 2:\n        excClass = exceptions_by_name.get(bits[0], None)\n        if excClass is not None:\n            msg = bits[1]\n            if msg.endswith(']'):\n                msg = msg[:-1]\n            '''\n            if request:\n                msg += ' (' + str(request) + ')'\n            '''\n            raise excClass(msg, status, result, request)\n\n    for pattern, excClass in list(exception_patterns_trailing.items()):\n        if not error.endswith(pattern):\n            continue\n            # For these exceptions, the returned value is the whole descriptive\n        # message.\n        raise excClass(error, status, result, request)\n\n    raise exceptions.ElasticSearchException(error, status, result, request)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_valid_scan(self):\n        if self._scan is not None:\n            return self._scan\n        if self._ordering:\n            return False\n        if self._facets:\n            return False\n            #TODO check for child or nested query\n        return True", "response": "Detect if a scan is valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef aggregate(self, *args, **kwargs):\n        obj = self._clone()\n        obj._facets = [] #reset facets\n        from django.db.models import Avg, Max, Min\n\n        for arg in args:\n            if isinstance(arg, (Avg, Max, Min)):\n\n                field, djfield = self._django_to_es_field(arg.lookup)\n                if not djfield:\n                    obj = obj.annotate(field)\n                from pyes.facets import StatisticalFacet\n\n                obj = obj.annotate(StatisticalFacet(field, field))\n        facets = obj.get_facets()\n        #collecting results\n        result = {}\n        for name, values in facets.items():\n            for k, v in values.items():\n                if k.startswith(\"_\"):\n                    continue\n                result[u'%s__%s' % (name, k)] = v\n\n        return result", "response": "Returns a dictionary containing the calculations over the current queryset\n        over the current queryset\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new object with the given kwargs saves it to the database and returns the created object.", "response": "def create(self, **kwargs):\n        \"\"\"\n        Creates a new object with the given kwargs, saving it to the database\n        and returning the created object.\n        \"\"\"\n        obj = self.model(**kwargs)\n        obj.save(force_insert=True, using=self._using)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbulk create a set of objects into the database.", "response": "def bulk_create(self, objs, batch_size=None):\n        \"\"\"\n        Inserts each of the instances into the database. This does *not* call\n        save() on each of the instances, does not send any pre/post save\n        signals, and does not set the primary key attribute if it is an\n        autoincrement field.\n        \"\"\"\n        self._insert(objs, batch_size=batch_size, return_id=False, force_insert=True)\n        self.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_or_create(self, **kwargs):\n        assert kwargs, \\\n            'get_or_create() must be passed at least one keyword argument'\n        defaults = kwargs.pop('defaults', {})\n        lookup = kwargs.copy()\n        #TODO: check fields\n        try:\n            return self.get(**lookup), False\n        except self.model.DoesNotExist:\n            params = dict([(k, v) for k, v in kwargs.items() if '__' not in k])\n            params.update(defaults)\n            obj = self.model(**params)\n            obj.save(force_insert=True)\n            return obj, True", "response": "Returns a tuple of object created if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the records in the current QuerySet.", "response": "def delete(self):\n        \"\"\"\n        Deletes the records in the current QuerySet.\n        \"\"\"\n        del_query = self._clone()\n\n        # The delete is actually 2 queries - one to find related objects,\n        # and one to delete. Make sure that the discovery of related\n        # objects is performed on the same database as the deletion.\n        del_query._clear_ordering()\n        connection = self.model._meta.dj_connection\n\n        connection.connection.delete_by_query(indices=connection.database, doc_types=self.model._meta.db_table,\n                                              query=del_query._build_query())\n        connection.connection.indices.refresh(indices=connection.database)\n        # Clear the result cache, in case this QuerySet gets reused.\n        self._result_cache = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate all elements in the current QuerySet with the given arguments.", "response": "def update(self, *args, **kwargs):\n        \"\"\"\n        Updates all elements in the current QuerySet, setting all the given\n        fields to the appropriate values.\n        \"\"\"\n        if args:\n            for arg in args:\n                if not isinstance(arg, ScriptField):\n                    raise InvalidParameter(\"%s this must be a ScriptField or derivated\" % arg)\n\n        query = self._build_query()\n        connection = self.model._meta.dj_connection\n        query.fields = [\"_id\", \"_type\", \"_index\"]\n        # results = connection.connection.search(query, indices=connection.database, doc_types=self.type,\n        #                             model=self.model, scan=True)\n\n        conn = connection.connection\n        results = conn.search(query, indices=connection.database, doc_types=self.type, scan=True)\n\n        for item in results:\n            if kwargs:\n                conn.update(item._meta.index, item._meta.type, item._meta.id, document=kwargs, bulk=True)\n            for script in args:\n                conn.update(item._meta.index, item._meta.type, item._meta.id,\n                            script=script.script, lang=script.lang, params=script.params, bulk=True)\n\n        connection.connection.flush_bulk(True)\n        # Clear the result cache, in case this QuerySet gets reused.\n        self._result_cache = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert django field name to ES field name", "response": "def _django_to_es_field(self, field):\n        \"\"\"We use this function in value_list and ordering to get the correct fields name\"\"\"\n        from django.db import models\n\n        prefix = \"\"\n        if field.startswith(\"-\"):\n            prefix = \"-\"\n            field = field.lstrip(\"-\")\n\n        if field in [\"id\", \"pk\"]:\n            return \"_id\", models.AutoField\n\n        try:\n            dj_field, _, _, _ = self.model._meta.get_field_by_name(field)\n            if isinstance(dj_field, models.ForeignKey):\n                return prefix + field + \"_id\", models.ForeignKey\n            else:\n                return prefix + field, dj_field\n        except models.FieldDoesNotExist:\n            pass\n\n        return prefix + field.replace(FIELD_SEPARATOR, \".\"), None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cooked_fields(self, dj_fields):\n        from django.db import models\n\n        valids = []\n        for field in dj_fields:\n            try:\n                dj_field, _, _, _ = self.model._meta.get_field_by_name(field)\n                if isinstance(dj_field, models.ForeignKey):\n                    valids.append((field + \"_id\", field, dj_field))\n                else:\n                    valids.append((field, field, dj_field))\n            except models.FieldDoesNotExist:\n                valids.append((field, field, None))\n        return valids", "response": "Returns a tuple of cooked fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dates(self, field_name, kind, order='ASC'):\n\n        assert kind in (\"month\", \"year\", \"day\", \"week\", \"hour\", \"minute\"), \\\n            \"'kind' must be one of 'year', 'month', 'day', 'week', 'hour' and 'minute'.\"\n        assert order in ('ASC', 'DESC'), \\\n            \"'order' must be either 'ASC' or 'DESC'.\"\n\n        search = self._build_search()\n        search.facet.reset()\n        search.facet.add_date_facet(name=field_name.replace(\"__\", \".\"),\n                                    field=field_name, interval=kind)\n        search.size = 0\n        connection = self.model._meta.dj_connection\n\n        resulset = connection.connection.search(search, indices=connection.database, doc_types=self.type)\n        resulset.fix_facets()\n        entries = []\n        for val in resulset.facets.get(field_name.replace(\"__\", \".\")).get(\"entries\", []):\n            if \"time\" in val:\n                entries.append(val[\"time\"])\n        if order == \"ASC\":\n            return sorted(entries)\n\n        return sorted(entries, reverse=True)", "response": "Returns a list of datetime objects representing all available dates for the given field_name scoped to kind."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an empty QuerySet.", "response": "def none(self):\n        \"\"\"\n        Returns an empty QuerySet.\n        \"\"\"\n        return EmptyQuerySet(model=self.model, using=self._using, connection=self._connection)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new QuerySet instance with the args ANDed to the existing set.", "response": "def query(self, *args, **kwargs):\n        \"\"\"\n        Returns a new QuerySet instance with the args ANDed to the existing\n        set.\n        \"\"\"\n        clone = self._clone()\n        queries = []\n        from pyes.query import Query\n\n        if args:\n            for f in args:\n                if isinstance(f, Query):\n                    queries.append(f)\n                else:\n                    raise TypeError('Only Query objects can be passed as argument')\n\n        for field, value in kwargs.items():\n            if value == [None]:\n                value = [\"$$$$\"]\n            queries.append(self._build_inner_query(field, value))\n\n        clone._queries.extend(queries)\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rescorer(self, rescorer):\n        clone = self._clone()\n        clone._rescorer=rescorer\n        return clone", "response": "Returns a new QuerySet with a set rescorer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_filter_modifier(self, field):\n        tokens = field.split(FIELD_SEPARATOR)\n        if len(tokens) == 1:\n            return field, \"\"\n        if tokens[-1] in self.FILTER_OPERATORS.keys():\n            return u'.'.join(tokens[:-1]), tokens[-1]\n        return u'.'.join(tokens), \"\"", "response": "Detect the filter modifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares the value for the record.", "response": "def _prepare_value(self, value, dj_field=None):\n        \"\"\"Cook the value\"\"\"\n        from django.db import models\n\n        if isinstance(value, (six.string_types, int, float)):\n            return value\n        elif isinstance(value, SimpleLazyObject):\n            return value.pk\n        elif isinstance(value, models.Model):\n            if dj_field:\n                if isinstance(dj_field, models.ForeignKey):\n                    return value.pk\n                    #TODO validate other types\n            return value\n        elif isinstance(value, (ResultSet, EmptyQuerySet, EmptyResultSet, list)):\n            return [self._prepare_value(v, dj_field=dj_field) for v in value]\n        elif isinstance(value, QuerySet):\n            value = value._clone()\n            return [self._prepare_value(v, dj_field=dj_field) for v in value]\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a Q object to a filter object.", "response": "def _Q_to_filter(self, q):\n        \"\"\"\n        Convert a Q object to filter\n        :param q: a Q Object\n        :return: a filter object\n        \"\"\"\n        default_filter = ANDFilter\n        if q.connector == \"OR\":\n            default_filter = ORFilter\n        filters = []\n        for child in q.children:\n            if isinstance(child, Q):\n                if child.children:\n                    filters.append(self._Q_to_filter(child))\n            elif isinstance(child, tuple):\n                field, value = child\n                filters.append(self._build_inner_filter(field, value))\n        if len(filters) == 1:\n            filter = filters[0]\n            if q.negated:\n                return NotFilter(filter)\n            return filter\n        if q.negated:\n            return NotFilter(default_filter(filters))\n        return default_filter(filters)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef annotate(self, *args, **kwargs):\n        obj = self._clone()\n        if args:\n            for arg in args:\n                if isinstance(arg, Facet):\n                    obj._facets.append(arg)\n                elif isinstance(arg, six.string_types):\n                    modifier = \"term\"\n                    tokens = arg.split(\"__\")\n                    if len(tokens)>1 and tokens[-1] in [\"term\", \"stat\", \"histo\", \"date\"]:\n                        modifier=tokens[-1]\n                        tokens=tokens[:-1]\n                    field, djfield = self._django_to_es_field(\"__\".join(tokens))\n                    if modifier==\"term\":\n                        obj._facets.append(TermFacet(name=arg, field=field, **kwargs))\n                    elif modifier==\"term_stat\":\n                        obj._facets.append(TermStatsFacet(name=arg, **kwargs))\n                    elif modifier==\"stat\":\n                        obj._facets.append(StatisticalFacet(name=arg, field=field, **kwargs))\n                    elif modifier==\"histo\":\n                        obj._facets.append(HistogramFacet(name=arg, field=field, **kwargs))\n                    elif modifier==\"date\":\n                        obj._facets.append(DateHistogramFacet(name=arg, field=field, **kwargs))\n                else:\n                    raise NotImplementedError(\"invalid type\")\n        else:\n            # Add the aggregates/facet to the query\n            for name, field in kwargs.items():\n                obj._facets.append(\n                    TermFacet(field=field.replace(FIELD_SEPARATOR, \".\"), name=name.replace(FIELD_SEPARATOR, \".\")))\n\n        return obj", "response": "Return a new QuerySet with the annotated objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new QuerySet with the ordering changed.", "response": "def order_by(self, *field_names):\n        \"\"\"\n        Returns a new QuerySet instance with the ordering changed.\n        We have a special field \"_random\"\n        \"\"\"\n        obj = self._clone()\n        obj._clear_ordering()\n        self._insert_ordering(obj, *field_names)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new QuerySet with the given alias.", "response": "def index(self, alias):\n        \"\"\"\n        Selects which database this QuerySet should execute its query against.\n        \"\"\"\n        clone = self._clone()\n        clone._index = alias\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the size of this QuerySet.", "response": "def size(self, size):\n        \"\"\"\n        Set the query size of  this QuerySet should execute its query against.\n        \"\"\"\n        clone = self._clone()\n        clone._size = size\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the aliases of one or more indices.", "response": "def aliases(self, indices=None):\n        \"\"\"\n        Retrieve the aliases of one or more indices.\n        ( See :ref:`es-guide-reference-api-admin-indices-aliases`)\n\n        :keyword indices: an index or a list of indices\n\n        \"\"\"\n        path = self.conn._make_path(indices, (), '_aliases')\n        return self.conn._send_request('GET', path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_aliases(self, commands):\n        body = {'actions': [{command: dict(index=index, alias=alias, **params)}\n                            for (command, index, alias, params) in commands]}\n        return self.conn._send_request('POST', \"_aliases\", body)", "response": "Change the aliases stored in the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_alias(self, alias, indices, **kwargs):\n        indices = self.conn._validate_indices(indices)\n\n        return self.change_aliases(['add', index, alias,\n                                    self._get_alias_params(**kwargs)]\n                                    for index in indices)", "response": "Add an alias to point to a set of indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting an alias. (See :ref:`es-guide-reference-api-admin-indices-aliases`) The specified index or indices are deleted from the alias, if they are in it to start with. This won't report an error even if the indices aren't present in the alias. :param alias: the name of an alias :param indices: a list of indices", "response": "def delete_alias(self, alias, indices):\n        \"\"\"\n        Delete an alias.\n        (See :ref:`es-guide-reference-api-admin-indices-aliases`)\n\n        The specified index or indices are deleted from the alias, if they are\n        in it to start with.  This won't report an error even if the indices\n        aren't present in the alias.\n\n        :param alias: the name of an alias\n        :param indices: a list of indices\n        \"\"\"\n        indices = self.conn._validate_indices(indices)\n\n        return self.change_aliases(['remove', index, alias, {}] for index in indices)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset an alias. (See :ref:`es-guide-reference-api-admin-indices-aliases`) This handles removing the old list of indices pointed to by the alias. Warning: there is a race condition in the implementation of this function - if another client modifies the indices which this alias points to during this call, the old value of the alias may not be correctly set. :param alias: the name of an alias :param indices: a list of indices", "response": "def set_alias(self, alias, indices, **kwargs):\n        \"\"\"\n        Set an alias.\n        (See :ref:`es-guide-reference-api-admin-indices-aliases`)\n\n        This handles removing the old list of indices pointed to by the alias.\n\n        Warning: there is a race condition in the implementation of this\n        function - if another client modifies the indices which this alias\n        points to during this call, the old value of the alias may not be\n        correctly set.\n\n        :param alias: the name of an alias\n        :param indices: a list of indices\n        \"\"\"\n        indices = self.conn._validate_indices(indices)\n        try:\n            old_indices = self.get_alias(alias)\n        except IndexMissingException:\n            old_indices = []\n        commands = [['remove', index, alias, {}] for index in old_indices]\n        commands.extend([['add', index, alias,\n                          self._get_alias_params(**kwargs)] for index in indices])\n        if len(commands) > 0:\n            return self.change_aliases(commands)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the statistic of one or more indices", "response": "def stats(self, indices=None):\n        \"\"\"\n        Retrieve the statistic of one or more indices\n        (See :ref:`es-guide-reference-api-admin-indices-stats`)\n\n        :keyword indices: an index or a list of indices\n        \"\"\"\n        path = self.conn._make_path(indices, (), \"_stats\")\n        return self.conn._send_request('GET', path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the status of one or more indices", "response": "def status(self, indices=None):\n        \"\"\"\n        Retrieve the status of one or more indices\n        (See :ref:`es-guide-reference-api-admin-indices-status`)\n\n        :keyword indices: an index or a list of indices\n        \"\"\"\n        path = self.conn._make_path(indices, (), '_status', allow_all_indices=False)\n        return self.conn._send_request('GET', path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the field data stats for one or more indices", "response": "def field_stats(self, indices=''):\n        \"\"\"\n        Retrieve the field data stats for one or more indices\n        (See :ref:'es-guide-reference-api-admin-cluster-nodes-stats')\n\n        :keyword indices: an index or a list of indices\n        \"\"\"\n        path = self.conn._make_path(indices, (), '_stats','fielddata')\n        return self.conn._send_request('GET', path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_index_if_missing(self, index, settings=None):\n        try:\n            return self.create_index(index, settings)\n        except IndexAlreadyExistsException as e:\n            return e.result", "response": "Creates an index if it doesn t already exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a dict holding the indices that exist in the cluster.", "response": "def get_indices(self, include_aliases=False):\n        \"\"\"\n        Get a dict holding an entry for each index which exists.\n\n        If include_alises is True, the dict will also contain entries for\n        aliases.\n\n        The key for each entry in the dict is the index or alias name.  The\n        value is a dict holding the following properties:\n\n         - num_docs: Number of documents in the index or alias.\n         - alias_for: Only present for an alias: holds a list of indices which\n           this is an alias for.\n\n        \"\"\"\n        state = self.conn.cluster.state()\n        status = self.status()\n        result = {}\n        indices_status = status['indices']\n        indices_metadata = state['metadata']['indices']\n        for index in sorted(indices_status.keys()):\n            info = indices_status[index]\n            try:\n                num_docs = info['docs']['num_docs']\n            except KeyError:\n                num_docs = 0\n            result[index] = dict(num_docs=num_docs)\n\n            if not include_aliases:\n                continue\n            try:\n                metadata = indices_metadata[index]\n            except KeyError:\n                continue\n            for alias in metadata.get('aliases', []):\n                try:\n                    alias_obj = result[alias]\n                except KeyError:\n                    alias_obj = {}\n                    result[alias] = alias_obj\n                alias_obj['num_docs'] = alias_obj.get('num_docs', 0) + num_docs\n                try:\n                    alias_obj['alias_for'].append(index)\n                except KeyError:\n                    alias_obj['alias_for'] = [index]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_closed_indices(self):\n        state = self.conn.cluster.state()\n        status = self.status()\n\n        indices_metadata = set(state['metadata']['indices'].keys())\n        indices_status = set(status['indices'].keys())\n\n        return indices_metadata.difference(indices_status)", "response": "Get all closed indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nflush one or more indices.", "response": "def flush(self, indices=None, refresh=None):\n        \"\"\"\n        Flushes one or more indices (clear memory)\n        If a bulk is full, it sends it.\n\n        (See :ref:`es-guide-reference-api-admin-indices-flush`)\n\n\n        :keyword indices: an index or a list of indices\n        :keyword refresh: set the refresh parameter\n\n        \"\"\"\n        self.conn.force_bulk()\n        path = self.conn._make_path(indices, '_flush')\n        args = {}\n        if refresh is not None:\n            args['refresh'] = refresh\n        return self.conn._send_request('POST', path, params=args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrefreshes one or more indices", "response": "def refresh(self, indices=None, timesleep=None, timeout=0):\n        \"\"\"\n        Refresh one or more indices\n        If a bulk is full, it sends it.\n        (See :ref:`es-guide-reference-api-admin-indices-refresh`)\n\n        :keyword indices: an index or a list of indices\n        :keyword timesleep: seconds to wait\n        :keyword timeout: seconds to wait before timing out when waiting for\n            the cluster's health.\n        \"\"\"\n        self.conn.force_bulk()\n        path = self.conn._make_path(indices, (), '_refresh', allow_all_indices=False)\n        result = self.conn._send_request('POST', path)\n        if timesleep:\n            time.sleep(timesleep)\n        self.conn.cluster.health(wait_for_status='green', timeout=timeout)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef optimize(self, indices=None,\n                 wait_for_merge=False,\n                 max_num_segments=None,\n                 only_expunge_deletes=False,\n                 refresh=True,\n                 flush=True):\n        \"\"\"\n        Optimize one or more indices.\n        (See :ref:`es-guide-reference-api-admin-indices-optimize`)\n\n\n        :keyword indices: the list of indices to optimise.  If not supplied, all\n                          default_indices are optimised.\n\n        :keyword wait_for_merge: If True, the operation will not return until the merge has been completed.\n                                 Defaults to False.\n\n        :keyword max_num_segments: The number of segments to optimize to. To fully optimize the index, set it to 1.\n                                   Defaults to half the number configured by the merge policy (which in turn defaults\n                                   to 10).\n\n\n        :keyword only_expunge_deletes: Should the optimize process only expunge segments with deletes in it.\n                                       In Lucene, a document is not deleted from a segment, just marked as deleted.\n                                       During a merge process of segments, a new segment is created that does have\n                                       those deletes.\n                                       This flag allow to only merge segments that have deletes. Defaults to false.\n\n        :keyword refresh: Should a refresh be performed after the optimize. Defaults to true.\n\n        :keyword flush: Should a flush be performed after the optimize. Defaults to true.\n\n        \"\"\"\n        path = self.conn._make_path(indices, (), '_optimize')\n        params = dict(\n            wait_for_merge=wait_for_merge,\n            only_expunge_deletes=only_expunge_deletes,\n            refresh=refresh,\n            flush=flush,\n            )\n        for k, v in params.items():\n            params[k] = v and \"true\" or \"false\"\n        if max_num_segments is not None:\n            params['max_num_segments'] = max_num_segments\n        return self.conn._send_request('POST', path, params=params)", "response": "This method is used to optimize one or more indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming an analysis process on a text and returns the tokens breakdown of the text.", "response": "def analyze(self, text, index=None, analyzer=None, tokenizer=None, filters=None, field=None):\n        \"\"\"\n        Performs the analysis process on a text and return the tokens breakdown of the text\n\n        (See :ref:`es-guide-reference-api-admin-indices-optimize`)\n\n        \"\"\"\n        if filters is None:\n            filters = []\n        argsets = 0\n        args = {}\n\n        if analyzer:\n            args['analyzer'] = analyzer\n            argsets += 1\n        if tokenizer or filters:\n            if tokenizer:\n                args['tokenizer'] = tokenizer\n            if filters:\n                args['filters'] = ','.join(filters)\n            argsets += 1\n        if field:\n            args['field'] = field\n            argsets += 1\n\n        if argsets > 1:\n            raise ValueError('Argument conflict: Specify either analyzer, tokenizer/filters or field')\n\n        if field and index is None:\n            raise ValueError('field can only be specified with an index')\n\n        path = make_path(index, '_analyze')\n        return self.conn._send_request('POST', path, text, args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gateway_snapshot(self, indices=None):\n        path = self.conn._make_path(indices, (), '_gateway', 'snapshot')\n        return self.conn._send_request('POST', path)", "response": "This method is used to create a snapshot of one or more indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters specific mapping definition for a specific type against one or more indices.", "response": "def put_mapping(self, doc_type=None, mapping=None, indices=None, ignore_conflicts=None):\n        \"\"\"\n        Register specific mapping definition for a specific type against one or more indices.\n        (See :ref:`es-guide-reference-api-admin-indices-put-mapping`)\n\n        \"\"\"\n        if not isinstance(mapping, dict):\n            if mapping is None:\n                mapping = {}\n            if hasattr(mapping, \"as_dict\"):\n                mapping = mapping.as_dict()\n\n        if doc_type:\n            path = self.conn._make_path(indices, doc_type, \"_mapping\")\n            if doc_type not in mapping:\n                mapping = {doc_type: mapping}\n        else:\n            path = self.conn._make_path(indices, (), \"_mapping\")\n\n        parameters = {}\n\n        if ignore_conflicts is not None:\n            parameters['ignore_conflicts'] = ignore_conflicts\n\n        return self.conn._send_request('PUT', path, mapping, params=parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting specific mapping definition for a specific type against one or more indices.", "response": "def get_mapping(self, doc_type=None, indices=None, raw=False):\n        \"\"\"\n        Register specific mapping definition for a specific type against one or more indices.\n        (See :ref:`es-guide-reference-api-admin-indices-get-mapping`)\n\n        \"\"\"\n        if doc_type is None and indices is None:\n            path = make_path(\"_mapping\")\n            is_mapping = False\n        else:\n            indices = self.conn._validate_indices(indices)\n            if doc_type:\n                path = make_path(','.join(indices), doc_type, \"_mapping\")\n                is_mapping = True\n            else:\n                path = make_path(','.join(indices), \"_mapping\")\n                is_mapping = False\n        result = self.conn._send_request('GET', path)\n        if raw:\n            return result\n        from pyes.mappings import Mapper\n        mapper = Mapper(result, is_mapping=False,\n                        connection=self.conn,\n                        document_object_field=self.conn.document_object_field)\n        if doc_type:\n            return mapper.mappings[doc_type]\n        return mapper"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_mapping(self, index, doc_type):\n        path = make_path(index, doc_type)\n        return self.conn._send_request('DELETE', path)", "response": "Delete a typed JSON document type from a specific index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_settings(self, index=None):\n        path = make_path(index, \"_settings\")\n        return self.conn._send_request('GET', path)", "response": "Returns the current settings for an index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_settings(self, index, newvalues):\n        path = make_path(index, \"_settings\")\n        return self.conn._send_request('PUT', path, newvalues)", "response": "Update the settings of an index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck the current cluster health.", "response": "def health(self, indices=None, level=\"cluster\", wait_for_status=None,\n               wait_for_relocating_shards=None, timeout=30):\n        \"\"\"\n        Check the current :ref:`cluster health <es-guide-reference-api-admin-cluster-health>`.\n        Request Parameters\n\n        The cluster health API accepts the following request parameters:\n\n        :param level: Can be one of cluster, indices or shards. Controls the\n                        details level of the health information returned.\n                        Defaults to *cluster*.\n        :param wait_for_status: One of green, yellow or red. Will wait (until\n                                the timeout provided) until the status of the\n                                cluster changes to the one provided.\n                                By default, will not wait for any status.\n        :param wait_for_relocating_shards: A number controlling to how many\n                                           relocating shards to wait for.\n                                           Usually will be 0 to indicate to\n                                           wait till all relocation have\n                                           happened. Defaults to not to wait.\n        :param timeout: A time based parameter controlling how long to wait\n                        if one of the wait_for_XXX are provided.\n                        Defaults to 30s.\n        \"\"\"\n        if indices:\n            path = make_path(\"_cluster\", \"health\", \",\".join(indices))\n        else:\n            path = make_path(\"_cluster\", \"health\")\n        mapping = {}\n        if level != \"cluster\":\n            if level not in [\"cluster\", \"indices\", \"shards\"]:\n                raise ValueError(\"Invalid level: %s\" % level)\n            mapping['level'] = level\n        if wait_for_status:\n            if wait_for_status not in [\"green\", \"yellow\", \"red\"]:\n                raise ValueError(\"Invalid wait_for_status: %s\" % wait_for_status)\n            mapping['wait_for_status'] = wait_for_status\n\n            mapping['timeout'] = \"%ds\" % timeout\n        return self.conn._send_request('GET', path, params=mapping)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the state of the cluster.", "response": "def state(self, filter_nodes=None, filter_routing_table=None,\n                      filter_metadata=None, filter_blocks=None,\n                      filter_indices=None):\n        \"\"\"\n        Retrieve the :ref:`cluster state <es-guide-reference-api-admin-cluster-state>`.\n\n        :param filter_nodes: set to **true** to filter out the **nodes** part\n                             of the response.\n        :param filter_routing_table: set to **true** to filter out the\n                                     **routing_table** part of the response.\n        :param filter_metadata: set to **true** to filter out the **metadata**\n                                part of the response.\n        :param filter_blocks: set to **true** to filter out the **blocks**\n                              part of the response.\n        :param filter_indices: when not filtering metadata, a comma separated\n                               list of indices to include in the response.\n\n        \"\"\"\n        path = make_path(\"_cluster\", \"state\")\n        parameters = {}\n\n        if filter_nodes is not None:\n            parameters['filter_nodes'] = filter_nodes\n\n        if filter_routing_table is not None:\n            parameters['filter_routing_table'] = filter_routing_table\n\n        if filter_metadata is not None:\n            parameters['filter_metadata'] = filter_metadata\n\n        if filter_blocks is not None:\n            parameters['filter_blocks'] = filter_blocks\n\n        if filter_blocks is not None:\n            if isinstance(filter_indices, six.string_types):\n                parameters['filter_indices'] = filter_indices\n            else:\n                parameters['filter_indices'] = \",\".join(filter_indices)\n\n        return self.conn._send_request('GET', path, params=parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nodes_info(self, nodes=None):\n        parts = [\"_cluster\", \"nodes\"]\n        if nodes:\n            parts.append(\",\".join(nodes))\n        path = make_path(*parts)\n        return self.conn._send_request('GET', path)", "response": "Get information about the nodes in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef node_stats(self, nodes=None):\n        parts = [\"_cluster\", \"nodes\", \"stats\"]\n        if nodes:\n            parts = [\"_cluster\", \"nodes\", \",\".join(nodes), \"stats\"]\n\n        path = make_path(*parts)\n        return self.conn._send_request('GET', path)", "response": "The cluster :ref:`nodes info <es-guide-reference-api-admin-cluster-nodes-stats>` API allows to retrieve one or more (or all) of\n        the cluster nodes information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves field stats per node or cluster wide", "response": "def node_field_stats(self, nodes=None):\n        \"\"\"\n        Retrieve field stats per node(s) or cluster wide\n        :ref:`nodes info <es-guide-reference-api-admin-cluster-nodes-stats>` API allows to retrieve one or more (or all) \n        the cluster nodes info about field data memory usage\n        \"\"\"\n        parts = ['_nodes', 'stats', 'indices']\n        if nodes:\n            paths = ['_nodes', \",\".join(nodes), 'stats', 'indices']\n\n        path = make_path(*parts)\n        return self.conn._send_request('GET', path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the object from the index.", "response": "def delete(self, bulk=False):\n        \"\"\"\n        Delete the object\n        \"\"\"\n        meta = self._meta\n        conn = meta['connection']\n        conn.delete(meta.index, meta.type, meta.id, bulk=bulk)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the object and returns the id.", "response": "def save(self, bulk=False, id=None, parent=None, routing=None, force=False):\n        \"\"\"\n        Save the object and returns id\n        \"\"\"\n        meta = self._meta\n        conn = meta['connection']\n        id = id or meta.get(\"id\", None)\n        parent = parent or meta.get('parent', None)\n        routing = routing or meta.get('routing', None)\n        qargs = None\n        if routing:\n            qargs={'routing': routing}\n        version = meta.get('version', None)\n        if force:\n            version = None\n        res = conn.index(self,\n                         meta.index, meta.type, id, parent=parent, bulk=bulk,\n                         version=version, force_insert=force,\n                         querystring_args=qargs)\n        if not bulk:\n            self._meta.id = res._id\n            self._meta.version = res._version\n            return res._id\n        return id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the id of the object", "response": "def get_id(self):\n        \"\"\" Force the object saveing to get an id\"\"\"\n        _id = self._meta.get(\"id\", None)\n        if _id is None:\n            _id = self.save()\n        return _id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_bulk(self, create=False):\n        result = []\n        op_type = \"index\"\n        if create:\n            op_type = \"create\"\n        meta = self._meta\n        cmd = {op_type: {\"_index\": meta.index, \"_type\": meta.type}}\n        if meta.parent:\n            cmd[op_type]['_parent'] = meta.parent\n        if meta.version:\n            cmd[op_type]['_version'] = meta.version\n        if meta.id:\n            cmd[op_type]['_id'] = meta.id\n        result.append(json.dumps(cmd, cls=self._meta.connection.encoder))\n        result.append(\"\\n\")\n        result.append(json.dumps(self, cls=self._meta.connection.encoder))\n        result.append(\"\\n\")\n        return ''.join(result)", "response": "Return bulk code for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting the key value pair before the item with the given index.", "response": "def insert(self, index, key, value):\n        \"\"\"Inserts the key, value pair before the item with the given index.\"\"\"\n        if key in self.keyOrder:\n            n = self.keyOrder.index(key)\n            del self.keyOrder[n]\n            if n < index:\n                index -= 1\n        self.keyOrder.insert(index, key)\n        super(SortedDict, self).__setitem__(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self):\n        # This way of initializing the copy means it works for subclasses, too.\n        obj = self.__class__(self)\n        obj.keyOrder = self.keyOrder[:]\n        return obj", "response": "Returns a copy of this object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(servers=None, framed_transport=False, timeout=None,\n            retry_time=60, recycle=None, round_robin=None, max_retries=3):\n    \"\"\"\n    Constructs a single ElasticSearch connection. Connects to a randomly chosen\n    server on the list.\n\n    If the connection fails, it will attempt to connect to each server on the\n    list in turn until one succeeds. If it is unable to find an active server,\n    it will throw a NoServerAvailable exception.\n\n    Failing servers are kept on a separate list and eventually retried, no\n    sooner than `retry_time` seconds after failure.\n\n    :keyword servers: [server]\n                      List of ES servers with format: \"hostname:port\"\n                      Default: [(\"127.0.0.1\",9500)]\n\n    :keyword framed_transport: If True, use a TFramedTransport instead of a TBufferedTransport\n\n    :keyword timeout: Timeout in seconds (e.g. 0.5)\n                      Default: None (it will stall forever)\n\n    :keyword retry_time: Minimum time in seconds until a failed server is reinstated. (e.g. 0.5)\n                         Default: 60\n\n    :keyword recycle:  Max time in seconds before an open connection is closed and returned to the pool.\n                       Default: None (Never recycle)\n\n    :keyword max_retries: Max retry time on connection down\n\n    :keyword round_robin: *DEPRECATED*\n\n    :return ES client\n    \"\"\"\n\n    if servers is None:\n        servers = [DEFAULT_SERVER]\n    return ThreadLocalConnection(servers, framed_transport, timeout,\n        retry_time, recycle, max_retries=max_retries)", "response": "Construct a new connection to one server on the list of servers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ensure_connection(self):\n        conn = self.connect()\n        if conn.recycle and conn.recycle < time.time():\n            logger.debug('Client session expired after %is. Recycling.', self._recycle)\n            self.close()\n            conn = self.connect()\n        return conn", "response": "Make sure we have a valid connection and return it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconnect to the server if it doesn t already exist.", "response": "def connect(self):\n        \"\"\"Create new connection unless we already have one.\"\"\"\n        if not getattr(self._local, 'conn', None):\n            try:\n                server = self._servers.get()\n                logger.debug('Connecting to %s', server)\n                self._local.conn = ClientTransport(server, self._framed_transport,\n                    self._timeout, self._recycle)\n            except (Thrift.TException, socket.timeout, socket.error):\n                logger.warning('Connection to %s failed.', server)\n                self._servers.mark_dead(server)\n                return self.connect()\n        return self._local.conn"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a request and return a response", "response": "def execute(self, request):\n        \"\"\"Execute a request and return a response\"\"\"\n        url = request.uri\n        if request.parameters:\n            url += '?' + urlencode(request.parameters)\n\n        if request.headers:\n            headers = dict(self._headers, **request.headers)\n        else:\n            headers = self._headers\n\n        \n        retry = 0\n        server = getattr(self._local, \"server\", None)\n        while True:\n            if not server:\n                self._local.server = server = self._get_server()\n            try:\n                parse_result = urlparse(server)\n                conn = get_pool().connection_from_host(parse_result.hostname,\n                                                         parse_result.port,\n                                                         parse_result.scheme)\n                kwargs = dict(\n                    method=Method._VALUES_TO_NAMES[request.method],\n                    url=parse_result.path + url,\n                    body=request.body,\n                    headers=headers,\n                    timeout=self._timeout,\n                )\n\n                response = conn.urlopen(**kwargs)\n                return RestResponse(status=response.status,\n                                    body=response.data,\n                                    headers=response.headers)\n            except (IOError, urllib3.exceptions.HTTPError) as ex:\n                self._drop_server(server)\n                self._local.server = server = None\n                if retry >= self._max_retries:\n                    logger.error(\"Client error: bailing out after %d failed retries\",\n                                 self._max_retries, exc_info=1)\n                    raise NoServerAvailable(ex)\n                logger.exception(\"Client error: %d retries left\", self._max_retries - retry)\n                retry += 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_dirs(directory):\n    return [f for f in pathlib.Path(directory).iterdir() if f.is_dir()]", "response": "Returns all directories in a given directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_files(directory):\n    return [f for f in pathlib.Path(directory).iterdir() if f.is_file() and not f.name.startswith('.')]", "response": "Returns all files in a given directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit one very class folder into two separate folder.", "response": "def split_class_dir_fixed(class_dir, output, fixed, seed):\n    \"\"\"Splits one very class folder\n    \"\"\"\n    files = setup_files(class_dir, seed)\n\n    assert len(files) > sum(fixed)\n\n    split_train = len(files) - sum(fixed)\n    split_val = split_train + fixed[0]\n\n    li = split_files(files, split_train, split_val, len(fixed) == 2)\n    copy_files(li, class_dir, output)\n    return len(files)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsplit one very class folder into two separate folder.", "response": "def split_class_dir_ratio(class_dir, output, ratio, seed):\n    \"\"\"Splits one very class folder\n    \"\"\"\n    files = setup_files(class_dir, seed)\n\n    split_train = int(ratio[0] * len(files))\n    split_val = split_train + int(ratio[1] * len(files))\n\n    li = split_files(files, split_train, split_val, len(ratio) == 3)\n    copy_files(li, class_dir, output)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit the files along the provided indices", "response": "def split_files(files, split_train, split_val, use_test):\n    \"\"\"Splits the files along the provided indices\n    \"\"\"\n    files_train = files[:split_train]\n    files_val = files[split_train:split_val] if use_test else files[split_train:]\n\n    li = [(files_train, 'train'), (files_val, 'val')]\n\n    # optional test folder\n    if use_test:\n        files_test = files[split_val:]\n        li.append((files_test, 'test'))\n    return li"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy_files(files_type, class_dir, output):\n    # get the last part within the file\n    class_name = path.split(class_dir)[1]\n    for (files, folder_type) in files_type:\n        full_path = path.join(output, folder_type, class_name)\n\n        pathlib.Path(full_path).mkdir(\n            parents=True, exist_ok=True)\n        for f in files:\n            shutil.copy2(f, full_path)", "response": "Copies the files from the input folder to the output folder\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_config(self):\n        config = {\n            'location': self.location,\n            'language': self.language,\n            'topic': self.topic,\n        }\n        return config", "response": "function to get current configuration"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to get params dict for HTTP request", "response": "def params_dict(self):\n        \"\"\"\n        function to get params dict for HTTP request\n        \"\"\"\n        location_code = 'US'\n        language_code = 'en'\n        if len(self.location):\n            location_code = locationMap[process.extractOne(self.location, self.locations)[0]]\n        if len(self.language):\n            language_code = langMap[process.extractOne(self.language, self.languages)[0]]\n        params = {\n            'hl': language_code,\n            'gl': location_code,\n            'ceid': '{}:{}'.format(location_code, language_code)\n        }\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_news(self):\n        if self.topic is None or self.topic is 'Top Stories':\n            resp = requests.get(top_news_url, params=self.params_dict)\n        else:\n            topic_code = topicMap[process.extractOne(self.topic, self.topics)[0]]\n            resp = requests.get(topic_url.format(topic_code), params=self.params_dict)\n        return self.parse_feed(resp.content)", "response": "function to get news articles"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_feed(content):\n        feed = feedparser.parse(content)\n        articles = []\n        for entry in feed['entries']:\n            article = {\n                'title': entry['title'],\n                'link': entry['link']\n            }\n            try:\n                article['media'] = entry['media_content'][0]['url']\n            except KeyError:\n                article['media'] = None\n            articles.append(article)\n        return articles", "response": "utility function to parse feed content into a list of dicts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_signature(uri, nonce, signature, auth_token=''):\n\n    auth_token = bytes(auth_token.encode('utf-8'))\n    nonce = bytes(nonce.encode('utf-8'))\n    signature = bytes(signature.encode('utf-8'))\n\n    parsed_uri = urlparse(uri.encode('utf-8'))\n    base_url = urlunparse((parsed_uri.scheme.decode('utf-8'),\n                           parsed_uri.netloc.decode('utf-8'),\n                           parsed_uri.path.decode('utf-8'), '', '',\n                           '')).encode('utf-8')\n\n    return encodestring(hnew(auth_token, base_url + nonce, sha256)\n                        .digest()).strip() == signature", "response": "Validates the request against the nonce and signature."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_credentials(auth_id, auth_token):\n\n    if not (auth_id and auth_token):\n        try:\n            auth_id = os.environ['PLIVO_AUTH_ID']\n            auth_token = os.environ['PLIVO_AUTH_TOKEN']\n        except KeyError:\n            raise AuthenticationError('The Plivo Python SDK '\n                                      'could not find your auth credentials.')\n\n    if not (is_valid_mainaccount(auth_id) or is_valid_subaccount(auth_id)):\n        raise AuthenticationError('Invalid auth_id supplied: %s' % auth_id)\n\n    return AuthenticationCredentials(auth_id=auth_id, auth_token=auth_token)", "response": "Fetches the right credentials either from params or from environment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_response(self,\n                         method,\n                         response,\n                         response_type=None,\n                         objects_type=None):\n        \"\"\"Processes the API response based on the status codes and method used\n        to access the API\n        \"\"\"\n\n        try:\n            response_json = response.json(\n                object_hook=\n                lambda x: ResponseObject(x) if isinstance(x, dict) else x)\n            if response_type:\n                r = response_type(self, response_json.__dict__)\n                response_json = r\n\n            if 'objects' in response_json and objects_type:\n                response_json.objects = [\n                    objects_type(self, obj.__dict__)\n                    for obj in response_json.objects\n                ]\n        except ValueError:\n            response_json = None\n\n        if response.status_code == 400:\n            if response_json and 'error' in response_json:\n                raise ValidationError(response_json.error)\n            raise ValidationError(\n                'A parameter is missing or is invalid while accessing resource'\n                'at: {url}'.format(url=response.url))\n\n        if response.status_code == 401:\n            if response_json and 'error' in response_json:\n                raise AuthenticationError(response_json.error)\n            raise AuthenticationError(\n                'Failed to authenticate while accessing resource at: '\n                '{url}'.format(url=response.url))\n\n        if response.status_code == 404:\n            if response_json and 'error' in response_json:\n                raise ResourceNotFoundError(response_json.error)\n            raise ResourceNotFoundError(\n                'Resource not found at: {url}'.format(url=response.url))\n\n        if response.status_code == 405:\n            if response_json and 'error' in response_json:\n                raise InvalidRequestError(response_json.error)\n            raise InvalidRequestError(\n                'HTTP method \"{method}\" not allowed to access resource at: '\n                '{url}'.format(method=method, url=response.url))\n\n        if response.status_code == 500:\n            if response_json and 'error' in response_json:\n                raise PlivoServerError(response_json.error)\n            raise PlivoServerError(\n                'A server error occurred while accessing resource at: '\n                '{url}'.format(url=response.url))\n\n        if method == 'DELETE':\n            if response.status_code != 204:\n                raise PlivoRestError('Resource at {url} could not be '\n                                     'deleted'.format(url=response.url))\n\n        elif response.status_code not in [200, 201, 202]:\n            raise PlivoRestError(\n                'Received status code {status_code} for the HTTP method '\n                '\"{method}\"'.format(\n                    status_code=response.status_code, method=method))\n\n        return response_json", "response": "Processes the API response based on the status codes and method used to access the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest Update :param params: :param path: :param random:", "response": "def update(self, params, path, **kwargs):\n        \"\"\"\n        Test Update\n        :param params:\n        :param path:\n        :param random:\n        \"\"\"\n        self.client.request('POST', params, path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef member(self, phlo_id, node_id,\n               member_id, action,\n               node_type='conference_bridge'):\n        \"\"\"\n        :param phlo_id:\n        :param node_id:\n        :param member_id:\n        :param action:\n        :param node_type: default value `conference_bridge`\n        :return:\n        \"\"\"\n        return self.client.phlo.member(phlo_id, node_id,\n                                       member_id, action,\n                                       node_type\n                                       )", "response": "Get a member from a PHLO."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef member(self, phlo_id, node_id,\n               member_id, action,\n               node_type='conference_bridge'):\n        \"\"\"\n        :param phlo_id:\n        :param node_id:\n        :param member_id:\n        :param action:\n        :param node_type: default value `conference_bridge`\n        :return:\n        \"\"\"\n        data = {\n            'member_id': member_id,\n            'phlo_id': phlo_id,\n            'node_id': node_id,\n            'node_type': node_type\n        }\n        member = Member(self.client, data)\n        return getattr(member, action)()", "response": "Get a member from a node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ci(data, statfunction=None, alpha=0.05, n_samples=10000,\n       method='bca', output='lowhigh', epsilon=0.001, multi=None,\n       _iter=True):\n    \"\"\"\nGiven a set of data ``data``, and a statistics function ``statfunction`` that\napplies to that data, computes the bootstrap confidence interval for\n``statfunction`` on that data. Data points are assumed to be delineated by\naxis 0.\n\nParameters\n----------\ndata: array_like, shape (N, ...) OR tuple of array_like all with shape (N, ...)\n    Input data. Data points are assumed to be delineated by axis 0. Beyond this,\n    the shape doesn't matter, so long as ``statfunction`` can be applied to the\n    array. If a tuple of array_likes is passed, then samples from each array (along\n    axis 0) are passed in order as separate parameters to the statfunction. The\n    type of data (single array or tuple of arrays) can be explicitly specified\n    by the multi parameter.\nstatfunction: function (data, weights=(weights, optional)) -> value\n    This function should accept samples of data from ``data``. It is applied\n    to these samples individually. \n    \n    If using the ABC method, the function _must_ accept a named ``weights`` \n    parameter which will be an array_like with weights for each sample, and \n    must return a _weighted_ result. Otherwise this parameter is not used\n    or required. Note that numpy's np.average accepts this. (default=np.average)\nalpha: float or iterable, optional\n    The percentiles to use for the confidence interval (default=0.05). If this\n    is a float, the returned values are (alpha/2, 1-alpha/2) percentile confidence\n    intervals. If it is an iterable, alpha is assumed to be an iterable of\n    each desired percentile.\nn_samples: float, optional\n    The number of bootstrap samples to use (default=10000)\nmethod: string, optional\n    The method to use: one of 'pi', 'bca', or 'abc' (default='bca')\noutput: string, optional\n    The format of the output. 'lowhigh' gives low and high confidence interval\n    values. 'errorbar' gives transposed abs(value-confidence interval value) values\n    that are suitable for use with matplotlib's errorbar function. (default='lowhigh')\nepsilon: float, optional (only for ABC method)\n    The step size for finite difference calculations in the ABC method. Ignored for\n    all other methods. (default=0.001)\nmulti: boolean, optional\n    If False, assume data is a single array. If True, assume data is a tuple/other\n    iterable of arrays of the same length that should be sampled together. If None,\n    decide based on whether the data is an actual tuple. (default=None)\n    \nReturns\n-------\nconfidences: tuple of floats\n    The confidence percentiles specified by alpha\n\nCalculation Methods\n-------------------\n'pi': Percentile Interval (Efron 13.3)\n    The percentile interval method simply returns the 100*alphath bootstrap\n    sample's values for the statistic. This is an extremely simple method of \n    confidence interval calculation. However, it has several disadvantages \n    compared to the bias-corrected accelerated method, which is the default.\n'bca': Bias-Corrected Accelerated (BCa) Non-Parametric (Efron 14.3) (default)\n    This method is much more complex to explain. However, it gives considerably\n    better results, and is generally recommended for normal situations. Note\n    that in cases where the statistic is smooth, and can be expressed with\n    weights, the ABC method will give approximated results much, much faster.\n    Note that in a case where the statfunction results in equal output for every\n    bootstrap sample, the BCa confidence interval is technically undefined, as\n    the acceleration value is undefined. To match the percentile interval method\n    and give reasonable output, the implementation of this method returns a\n    confidence interval of zero width using the 0th bootstrap sample in this\n    case, and warns the user.  \n'abc': Approximate Bootstrap Confidence (Efron 14.4, 22.6)\n    This method provides approximated bootstrap confidence intervals without\n    actually taking bootstrap samples. This requires that the statistic be \n    smooth, and allow for weighting of individual points with a weights=\n    parameter (note that np.average allows this). This is _much_ faster\n    than all other methods for situations where it can be used.\n\nExamples\n--------\nTo calculate the confidence intervals for the mean of some numbers:\n\n>> boot.ci( np.randn(100), np.average )\n\nGiven some data points in arrays x and y calculate the confidence intervals\nfor all linear regression coefficients simultaneously:\n\n>> boot.ci( (x,y), scipy.stats.linregress )\n\nReferences\n----------\nEfron, An Introduction to the Bootstrap. Chapman & Hall 1993\n    \"\"\"\n\n    # Deal with the alpha values\n    if np.iterable(alpha):\n        alphas = np.array(alpha)\n    else:\n        alphas = np.array([alpha/2, 1-alpha/2])\n\n    if multi is None:\n        if isinstance(data, tuple):\n            multi = True\n        else:\n            multi = False\n\n    if statfunction is None:\n        if _iter:\n            statfunction = np.average\n        else:\n            def statfunc_wrapper(x, *args, **kwargs):\n                return np.average(x, axis=-1, *args, **kwargs)\n            statfunction = statfunc_wrapper\n        \n    # Ensure that the data is actually an array. This isn't nice to pandas,\n    # but pandas seems much much slower and the indexes become a problem.\n    if not multi:\n        data = np.array(data)\n        tdata = (data,)\n    else:\n        tdata = tuple( np.array(x) for x in data )\n\n    # Deal with ABC *now*, as it doesn't need samples.\n    if method == 'abc':\n        n = tdata[0].shape[0]*1.0\n        nn = tdata[0].shape[0]\n\n        I = np.identity(nn)\n        ep = epsilon / n*1.0\n        p0 = np.repeat(1.0/n,nn)\n\n        try:\n            t0 = statfunction(*tdata,weights=p0)\n        except TypeError as e:\n            raise TypeError(\"statfunction does not accept correct arguments for ABC ({0})\".format(e.message))\n\n        di_full = I - p0\n        tp = np.fromiter((statfunction(*tdata, weights=p0+ep*di)\n                          for di in di_full), dtype=np.float)\n        tm = np.fromiter((statfunction(*tdata, weights=p0-ep*di)\n                          for di in di_full), dtype=np.float)\n        t1 = (tp-tm)/(2*ep)\n        t2 = (tp-2*t0+tm)/ep**2\n\n        sighat = np.sqrt(np.sum(t1**2))/n\n        a = (np.sum(t1**3))/(6*n**3*sighat**3)\n        delta = t1/(n**2*sighat)\n        cq = (statfunction(*tdata,weights=p0+ep*delta)-2*t0+statfunction(*tdata,weights=p0-ep*delta))/(2*sighat*ep**2)\n        bhat = np.sum(t2)/(2*n**2)\n        curv = bhat/sighat-cq\n        z0 = nppf(2*ncdf(a)*ncdf(-curv))\n        Z = z0+nppf(alphas)\n        za = Z/(1-a*Z)**2\n        # stan = t0 + sighat * nppf(alphas)\n        abc = np.zeros_like(alphas)\n        for i in range(0,len(alphas)):\n            abc[i] = statfunction(*tdata,weights=p0+za[i]*delta)\n\n        if output == 'lowhigh':\n            return abc\n        elif output == 'errorbar':\n            return abs(abc-statfunction(tdata))[np.newaxis].T\n        else:\n            raise ValueError(\"Output option {0} is not supported.\".format(output))\n\n    # We don't need to generate actual samples; that would take more memory.\n    # Instead, we can generate just the indexes, and then apply the statfun\n    # to those indexes.\n    if _iter:\n        bootindexes = bootstrap_indexes(tdata[0], n_samples)\n        stat = np.array([statfunction(*(x[indexes] for x in tdata))\n                         for indexes in bootindexes])\n    else:\n        bootindexes = bootstrap_indexes_array(tdata[0], n_samples)\n        stat = statfunction(*(x[bootindexes] for x in tdata))\n\n    stat.sort(axis=0)\n\n    # Percentile Interval Method\n    if method == 'pi':\n        avals = alphas\n\n    # Bias-Corrected Accelerated Method\n    elif method == 'bca':\n\n        # The value of the statistic function applied just to the actual data.\n        ostat = statfunction(*tdata)\n\n        # The bias correction value.\n        z0 = nppf( ( 1.0*np.sum(stat < ostat, axis=0)  ) / n_samples )\n\n        # Statistics of the jackknife distribution\n        jackindexes = jackknife_indexes(tdata[0])\n        jstat = [statfunction(*(x[indexes] for x in tdata)) for indexes in jackindexes]\n        jmean = np.mean(jstat,axis=0)\n\n        # Temporarily kill numpy warnings:\n        oldnperr = np.seterr(invalid='ignore')\n        # Acceleration value\n        a = np.sum((jmean - jstat)**3, axis=0) / (\n            6.0 * np.sum((jmean - jstat)**2, axis=0)**1.5)\n        if np.any(np.isnan(a)):\n            nanind = np.nonzero(np.isnan(a))\n            warnings.warn(\"BCa acceleration values for indexes {} were undefined. \\\nStatistic values were likely all equal. Affected CI will \\\nbe inaccurate.\".format(nanind), InstabilityWarning, stacklevel=2)\n        \n        zs = z0 + nppf(alphas).reshape(alphas.shape+(1,)*z0.ndim)\n\n        avals = ncdf(z0 + zs/(1-a*zs))\n        np.seterr(**oldnperr)\n    else:\n        raise ValueError(\"Method {0} is not supported.\".format(method))\n\n    nvals = np.round((n_samples-1)*avals)\n    \n    oldnperr = np.seterr(invalid='ignore')\n    if np.any(np.isnan(nvals)):\n        warnings.warn(\"Some values were NaN; results are probably unstable \" +\n                      \"(all values were probably equal)\", InstabilityWarning,\n                      stacklevel=2)\n    if np.any(nvals == 0) or np.any(nvals == n_samples-1):\n        warnings.warn(\"Some values used extremal samples; \" +\n                      \"results are probably unstable.\",\n                      InstabilityWarning, stacklevel=2)\n    elif np.any(nvals < 10) or np.any(nvals >= n_samples-10):\n        warnings.warn(\"Some values used top 10 low/high samples; \" +\n                      \"results may be unstable.\",\n                      InstabilityWarning, stacklevel=2)\n    np.seterr(**oldnperr)\n    \n    nvals = np.nan_to_num(nvals).astype('int')\n\n    if output == 'lowhigh':\n        if nvals.ndim == 1:\n            # All nvals are the same. Simple broadcasting\n            return stat[nvals]\n        else:\n            # Nvals are different for each data point. Not simple broadcasting.\n            # Each set of nvals along axis 0 corresponds to the data at the same\n            # point in other axes.\n            return stat[(nvals, np.indices(nvals.shape)[1:].squeeze())]\n    elif output == 'errorbar':\n        if nvals.ndim == 1:\n            return abs(statfunction(data)-stat[nvals])[np.newaxis].T\n        else:\n            return abs(statfunction(data)-stat[(nvals, np.indices(nvals.shape)[1:])])[np.newaxis].T\n    else:\n        raise ValueError(\"Output option {0} is not supported.\".format(output))", "response": "Compute the bootstrap confidence interval for a set of data points."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bootstrap_indexes(data, n_samples=10000):\n    for _ in xrange(n_samples):\n        yield randint(data.shape[0], size=(data.shape[0],))", "response": "Returns a generator for random bootstrap indexes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of arrays where each array is a set of jackknife indexes.", "response": "def jackknife_indexes(data):\n    \"\"\"\nGiven data points data, where axis 0 is considered to delineate points, return\na list of arrays where each array is a set of jackknife indexes.\n\nFor a given set of data Y, the jackknife sample J[i] is defined as the data set\nY with the ith data point deleted.\n    \"\"\"\n    base = np.arange(0,len(data))\n    return (np.delete(base,i) for i in base)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subsample_indexes(data, n_samples=1000, size=0.5):\n    if size == -1:\n        size = len(data)\n    elif (size < 1) and (size > 0):\n        size = int(round(size*len(data)))\n    elif size > 1:\n        pass\n    else:\n        raise ValueError(\"size cannot be {0}\".format(size))\n    base = np.tile(np.arange(len(data)),(n_samples,1))\n    for sample in base: np.random.shuffle(sample)\n    return base[:,0:size]", "response": "This function returns a list of arrays where each array is indexes a subsample of the data of size size."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate moving - block bootstrap samples.", "response": "def bootstrap_indexes_moving_block(data, n_samples=10000,\n                                   block_length=3, wrap=False):\n    \"\"\"Generate moving-block bootstrap samples.\n\nGiven data points `data`, where axis 0 is considered to delineate points,\nreturn a generator for sets of bootstrap indexes. This can be used as a\nlist of bootstrap indexes (with list(bootstrap_indexes_moving_block(data))) as\nwell.\n\nParameters\n----------\n\nn_samples [default 10000]: the number of subsamples to generate.\n\nblock_length [default 3]: the length of block.\n\nwrap [default False]: if false, choose only blocks within the data, making\nthe last block for data of length L start at L-block_length.  If true, choose\nblocks starting anywhere, and if they extend past the end of the data, wrap\naround to the beginning of the data again.\n\"\"\"\n    n_obs = data.shape[0]\n    n_blocks = int(ceil(n_obs / block_length))\n    nexts = np.repeat(np.arange(0, block_length)[None, :], n_blocks, axis=0)\n\n    if wrap:\n        last_block = n_obs\n    else:\n        last_block = n_obs - block_length\n    \n    for _ in xrange(n_samples):\n        blocks = np.random.randint(0, last_block, size=n_blocks)\n        if not wrap:\n            yield (blocks[:, None]+nexts).ravel()[:n_obs]\n        else:\n            yield np.mod((blocks[:, None]+nexts).ravel()[:n_obs], n_obs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new instance of the given class from a signature.", "response": "def from_signature(klass, sig, recid, h, curve):\n        \"\"\" See http://www.secg.org/download/aid-780/sec1-v2.pdf, chapter 4.1.6 \"\"\"\n        from ecdsa import util, numbertheory\n        import msqr\n        curveFp = curve.curve\n        G = curve.generator\n        order = G.order()\n        # extract r,s from signature\n        r, s = util.sigdecode_string(sig, order)\n        # 1.1\n        x = r + (recid/2) * order\n        # 1.3\n        alpha = ( x * x * x  + curveFp.a() * x + curveFp.b() ) % curveFp.p()\n        beta = msqr.modular_sqrt(alpha, curveFp.p())\n        y = beta if (beta - recid) % 2 == 0 else curveFp.p() - beta\n        # 1.4 the constructor checks that nR is at infinity\n        R = Point(curveFp, x, y, order)\n        # 1.5 compute e from message:\n        e = string_to_number(h)\n        minus_e = -e % order\n        # 1.6 compute Q = r^-1 (sR - eG)\n        inv_r = numbertheory.inverse_mod(r,order)\n        Q = inv_r * ( s * R + minus_e * G )\n        return klass.from_public_point( Q, curve )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modular_sqrt(a, p):\n    # Simple cases\n    #\n    if legendre_symbol(a, p) != 1:\n        return 0\n    elif a == 0:\n        return 0\n    elif p == 2:\n        return p\n    elif p % 4 == 3:\n        return pow(a, (p + 1) / 4, p)\n\n    # Partition p-1 to s * 2^e for an odd s (i.e.\n    # reduce all the powers of 2 from p-1)\n    #\n    s = p - 1\n    e = 0\n    while s % 2 == 0:\n        s /= 2\n        e += 1\n\n    # Find some 'n' with a legendre symbol n|p = -1.\n    # Shouldn't take long.\n    #\n    n = 2\n    while legendre_symbol(n, p) != -1:\n        n += 1\n\n    # Here be dragons!\n    # Read the paper \"Square roots from 1; 24, 51,\n    # 10 to Dan Shanks\" by Ezra Brown for more\n    # information\n    #\n\n    # x is a guess of the square root that gets better\n    # with each iteration.\n    # b is the \"fudge factor\" - by how much we're off\n    # with the guess. The invariant x^2 = ab (mod p)\n    # is maintained throughout the loop.\n    # g is used for successive powers of n to update\n    # both a and b\n    # r is the exponent - decreases with each update\n    #\n    x = pow(a, (s + 1) / 2, p)\n    b = pow(a, s, p)\n    g = pow(n, s, p)\n    r = e\n\n    while True:\n        t = b\n        m = 0\n        for m in xrange(r):\n            if t == 1:\n                break\n            t = pow(t, 2, p)\n\n        if m == 0:\n            return x\n\n        gs = pow(g, 2 ** (r - m - 1), p)\n        g = (gs * gs) % p\n        x = (x * gs) % p\n        b = (b * g) % p\n        r = m", "response": "Find a modular square root of a."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef legendre_symbol(a, p):\n    ls = pow(a, (p - 1) / 2, p)\n    return -1 if ls == p - 1 else ls", "response": "Compute the Legendre symbol a |p using the Euler s criterion."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_protein_substitution_language() -> ParserElement:\n    parser_element = psub_tag + nest(\n        amino_acid(PSUB_REFERENCE),\n        ppc.integer(PSUB_POSITION),\n        amino_acid(PSUB_VARIANT),\n    )\n    parser_element.setParseAction(_handle_psub)\n    return parser_element", "response": "Build a protein substitution parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nripping off the closing parentheses and adds canonicalized modification. I did this because writing a whole new parsing model for the data would be sad and difficult :param bel_string: BEL string representing node :param dict location_model: A dictionary containing keys :code:`pybel.constants.TO_LOC` and :code:`pybel.constants.FROM_LOC` :return: A part of a BEL string representing the location", "response": "def postpend_location(bel_string: str, location_model) -> str:\n    \"\"\"Rip off the closing parentheses and adds canonicalized modification.\n\n    I did this because writing a whole new parsing model for the data would be sad and difficult\n\n    :param bel_string: BEL string representing node\n    :param dict location_model: A dictionary containing keys :code:`pybel.constants.TO_LOC` and\n                                :code:`pybel.constants.FROM_LOC`\n    :return: A part of a BEL string representing the location\n    \"\"\"\n    if not all(k in location_model for k in {NAMESPACE, NAME}):\n        raise ValueError('Location model missing namespace and/or name keys: {}'.format(location_model))\n\n    return \"{}, loc({}:{}))\".format(\n        bel_string[:-1],\n        location_model[NAMESPACE],\n        ensure_quotes(location_model[NAME])\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _decanonicalize_edge_node(node: BaseEntity, edge_data: EdgeData, node_position: str) -> str:\n    node_str = node.as_bel()\n\n    if node_position not in edge_data:\n        return node_str\n\n    node_edge_data = edge_data[node_position]\n\n    if LOCATION in node_edge_data:\n        node_str = postpend_location(node_str, node_edge_data[LOCATION])\n\n    modifier = node_edge_data.get(MODIFIER)\n\n    if modifier is None:\n        return node_str\n\n    if DEGRADATION == modifier:\n        return \"deg({})\".format(node_str)\n\n    effect = node_edge_data.get(EFFECT)\n\n    if ACTIVITY == modifier:\n        if effect is None:\n            return \"act({})\".format(node_str)\n\n        if effect[NAMESPACE] == BEL_DEFAULT_NAMESPACE:\n            return \"act({}, ma({}))\".format(node_str, effect[NAME])\n\n        return \"act({}, ma({}:{}))\".format(node_str, effect[NAMESPACE], ensure_quotes(effect[NAME]))\n\n    if TRANSLOCATION == modifier:\n        if effect is None:\n            return 'tloc({})'.format(node_str)\n\n        to_loc_data = effect[TO_LOC]\n        from_loc_data = effect[FROM_LOC]\n\n        if from_loc_data[NAMESPACE] == BEL_DEFAULT_NAMESPACE and from_loc_data[NAME] == INTRACELLULAR:\n            if to_loc_data[NAMESPACE] == BEL_DEFAULT_NAMESPACE and to_loc_data[NAME] == EXTRACELLULAR:\n                return 'sec({})'.format(node_str)\n            if to_loc_data[NAMESPACE] == BEL_DEFAULT_NAMESPACE and to_loc_data[NAME] == CELL_SURFACE:\n                return 'surf({})'.format(node_str)\n\n        from_loc = _get_tloc_terminal('fromLoc', from_loc_data)\n        to_loc = _get_tloc_terminal('toLoc', to_loc_data)\n\n        return \"tloc({}, {}, {})\".format(node_str, from_loc, to_loc)\n\n    raise ValueError('invalid modifier: {}'.format(modifier))", "response": "Canonicalize a node with its modifiers stored in the given edge to a BEL string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking two nodes and gives back a BEL string representing the statement.", "response": "def edge_to_bel(u: BaseEntity, v: BaseEntity, data: EdgeData, sep: Optional[str] = None) -> str:\n    \"\"\"Take two nodes and gives back a BEL string representing the statement.\n\n    :param u: The edge's source's PyBEL node data dictionary\n    :param v: The edge's target's PyBEL node data dictionary\n    :param data: The edge's data dictionary\n    :param sep: The separator between the source, relation, and target. Defaults to ' '\n    \"\"\"\n    sep = sep or ' '\n    u_str = _decanonicalize_edge_node(u, data, node_position=SUBJECT)\n    v_str = _decanonicalize_edge_node(v, data, node_position=OBJECT)\n\n    return sep.join((u_str, data[RELATION], v_str))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort_qualified_edges(graph) -> Iterable[EdgeTuple]:\n    qualified_edges = (\n        (u, v, k, d)\n        for u, v, k, d in graph.edges(keys=True, data=True)\n        if graph.has_edge_citation(u, v, k) and graph.has_edge_evidence(u, v, k)\n    )\n    return sorted(qualified_edges, key=_sort_qualified_edges_helper)", "response": "Return the qualified edges sorted first by citation evidence then by annotations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _citation_sort_key(t: EdgeTuple) -> str:\n    return '\"{}\", \"{}\"'.format(t[3][CITATION][CITATION_TYPE], t[3][CITATION][CITATION_REFERENCE])", "response": "Make a confusing 4 tuple sortable by citation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a set annotation string.", "response": "def _set_annotation_to_str(annotation_data: Mapping[str, Mapping[str, bool]], key: str) -> str:\n    \"\"\"Return a set annotation string.\"\"\"\n    value = annotation_data[key]\n\n    if len(value) == 1:\n        return 'SET {} = \"{}\"'.format(key, list(value)[0])\n\n    x = ('\"{}\"'.format(v) for v in sorted(value))\n\n    return 'SET {} = {{{}}}'.format(key, ', '.join(x))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an unset annotation string.", "response": "def _unset_annotation_to_str(keys: List[str]) -> str:\n    \"\"\"Return an unset annotation string.\"\"\"\n    if len(keys) == 1:\n        return 'UNSET {}'.format(list(keys)[0])\n\n    return 'UNSET {{{}}}'.format(', '.join('{}'.format(key) for key in keys))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _to_bel_lines_header(graph) -> Iterable[str]:\n    yield '# This document was created by PyBEL v{} and bel-resources v{} on {}\\n'.format(\n        VERSION, bel_resources.constants.VERSION, time.asctime()\n    )\n    yield from make_knowledge_header(\n        namespace_url=graph.namespace_url,\n        namespace_patterns=graph.namespace_pattern,\n        annotation_url=graph.annotation_url,\n        annotation_patterns=graph.annotation_pattern,\n        annotation_list=graph.annotation_list,\n        **graph.document\n    )", "response": "Iterate the lines of a BEL graph s corresponding BEL script s header."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an iterator over pairs of citation values and their corresponding edge iterators.", "response": "def group_citation_edges(edges: Iterable[EdgeTuple]) -> Iterable[Tuple[str, Iterable[EdgeTuple]]]:\n    \"\"\"Return an iterator over pairs of citation values and their corresponding edge iterators.\"\"\"\n    return itt.groupby(edges, key=_citation_sort_key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an iterator over pairs of evidence values and their corresponding edge iterators.", "response": "def group_evidence_edges(edges: Iterable[EdgeTuple]) -> Iterable[Tuple[str, Iterable[EdgeTuple]]]:\n    \"\"\"Return an iterator over pairs of evidence values and their corresponding edge iterators.\"\"\"\n    return itt.groupby(edges, key=_evidence_sort_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\niterates the lines of a BEL graph s corresponding BEL script s body.", "response": "def _to_bel_lines_body(graph) -> Iterable[str]:\n    \"\"\"Iterate the lines of a BEL graph's corresponding BEL script's body.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    qualified_edges = sort_qualified_edges(graph)\n\n    for citation, citation_edges in group_citation_edges(qualified_edges):\n        yield 'SET Citation = {{{}}}\\n'.format(citation)\n\n        for evidence, evidence_edges in group_evidence_edges(citation_edges):\n            yield 'SET SupportingText = \"{}\"'.format(evidence)\n\n            for u, v, _, data in evidence_edges:\n                annotations_data = data.get(ANNOTATIONS)\n\n                keys = sorted(annotations_data) if annotations_data is not None else tuple()\n                for key in keys:\n                    yield _set_annotation_to_str(annotations_data, key)\n\n                yield graph.edge_to_bel(u, v, data)\n\n                if keys:\n                    yield _unset_annotation_to_str(keys)\n\n            yield 'UNSET SupportingText'\n        yield 'UNSET Citation\\n'\n        yield '#' * 80"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding a list of lines that are used to generate a BEL script s footer.", "response": "def _to_bel_lines_footer(graph) -> Iterable[str]:\n    \"\"\"Iterate the lines of a BEL graph's corresponding BEL script's footer.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    unqualified_edges_to_serialize = [\n        (u, v, d)\n        for u, v, d in graph.edges(data=True)\n        if d[RELATION] in UNQUALIFIED_EDGES and EVIDENCE not in d\n    ]\n\n    isolated_nodes_to_serialize = [\n        node\n        for node in graph\n        if not graph.pred[node] and not graph.succ[node]\n    ]\n\n    if unqualified_edges_to_serialize or isolated_nodes_to_serialize:\n        yield '###############################################\\n'\n        yield 'SET Citation = {\"PubMed\",\"Added by PyBEL\",\"29048466\"}'\n        yield 'SET SupportingText = \"{}\"'.format(PYBEL_AUTOEVIDENCE)\n\n        for u, v, data in unqualified_edges_to_serialize:\n            yield '{} {} {}'.format(u.as_bel(), data[RELATION], v.as_bel())\n\n        for node in isolated_nodes_to_serialize:\n            yield node.as_bel()\n\n        yield 'UNSET SupportingText'\n        yield 'UNSET Citation'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over the lines of the BEL graph as a canonical BEL Script.", "response": "def to_bel_lines(graph) -> Iterable[str]:\n    \"\"\"Iterate over the lines of the BEL graph as a canonical BEL Script (.bel).\n\n    :param pybel.BELGraph graph: the BEL Graph to output as a BEL Script\n    \"\"\"\n    return itt.chain(\n        _to_bel_lines_header(graph),\n        _to_bel_lines_body(graph),\n        _to_bel_lines_footer(graph),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_bel(graph, file: Optional[TextIO] = None) -> None:\n    for line in to_bel_lines(graph):\n        print(line, file=file)", "response": "Output the given BEL graph as a canonical BEL script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_bel_path(graph, path: str, mode: str = 'w', **kwargs) -> None:\n    with open(path, mode=mode, **kwargs) as bel_file:\n        to_bel(graph, bel_file)", "response": "Write the BEL graph to the given path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_canonical_name(data: BaseEntity) -> str:\n    if data[FUNCTION] == COMPLEX and NAMESPACE in data:\n        return data[NAME]\n\n    if VARIANTS in data:\n        return data.as_bel()\n\n    if FUSION in data:\n        return data.as_bel()\n\n    if data[FUNCTION] in {REACTION, COMPOSITE, COMPLEX}:\n        return data.as_bel()\n\n    if VARIANTS not in data and FUSION not in data:  # this is should be a simple node\n        if IDENTIFIER in data and NAME in data:\n            return '{namespace}:{identifier} ({name})'.format(**data)\n\n        if IDENTIFIER in data:\n            return '{namespace}:{identifier}'.format(namespace=data[NAMESPACE], identifier=data[IDENTIFIER])\n\n        return data[NAME]\n\n    raise ValueError('Unexpected node data: {}'.format(data))", "response": "Calculate the canonical name for a given node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a BEL graph from a list of edges.", "response": "def graph_from_edges(edges: Iterable[Edge], **kwargs) -> BELGraph:\n    \"\"\"Build a BEL graph from edges.\"\"\"\n    graph = BELGraph(**kwargs)\n\n    for edge in edges:\n        edge.insert_into_graph(graph)\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery nodes in the database.", "response": "def query_nodes(self,\n                    bel: Optional[str] = None,\n                    type: Optional[str] = None,\n                    namespace: Optional[str] = None,\n                    name: Optional[str] = None,\n                    ) -> List[Node]:\n        \"\"\"Query nodes in the database.\n\n        :param bel: BEL term that describes the biological entity. e.g. ``p(HGNC:APP)``\n        :param type: Type of the biological entity. e.g. Protein\n        :param namespace: Namespace keyword that is used in BEL. e.g. HGNC\n        :param name: Name of the biological entity. e.g. APP\n        \"\"\"\n        q = self.session.query(Node)\n\n        if bel:\n            q = q.filter(Node.bel.contains(bel))\n\n        if type:\n            q = q.filter(Node.type == type)\n\n        if namespace or name:\n            q = q.join(NamespaceEntry)\n\n            if namespace:\n                q = q.join(Namespace).filter(Namespace.keyword.contains(namespace))\n\n            if name:\n                q = q.filter(NamespaceEntry.name.contains(name))\n\n        return q"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the edges with the given citation.", "response": "def get_edges_with_citation(self, citation: Citation) -> List[Edge]:\n        \"\"\"Get the edges with the given citation.\"\"\"\n        return self.session.query(Edge).join(Evidence).filter(Evidence.citation == citation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets edges with one of the given citations.", "response": "def get_edges_with_citations(self, citations: Iterable[Citation]) -> List[Edge]:\n        \"\"\"Get edges with one of the given citations.\"\"\"\n        return self.session.query(Edge).join(Evidence).filter(Evidence.citation.in_(citations)).all()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_edges_with_evidence(self, evidence: str) -> List[Edge]:\n        return self.session.query(Edge).join(Evidence).filter(Evidence.text.like(evidence)).all()", "response": "Search edges with the given evidence."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches edges with given BEL string.", "response": "def search_edges_with_bel(self, bel: str) -> List[Edge]:\n        \"\"\"Search edges with given BEL.\n\n        :param bel: A BEL string to use as a search\n        \"\"\"\n        return self.session.query(Edge).filter(Edge.bel.like(bel))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_edges_with_annotation(self, annotation: str, value: str) -> List[Edge]:\n        query = self.session.query(Edge).join(NamespaceEntry, Edge.annotations).join(Namespace)\n        query = query.filter(Namespace.keyword == annotation).filter(NamespaceEntry.name == value)\n        return query.all()", "response": "Search edges with the given annotation and value pair."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_edge_function_filter(query, edge_node_id, node_type):\n        return query.join(Node, edge_node_id == Node.id).filter(Node.type == node_type)", "response": "Add a filter to query_edges that filters out the edge_node_id and node_type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquery the edges in the database.", "response": "def query_edges(self,\n                    bel: Optional[str] = None,\n                    source_function: Optional[str] = None,\n                    source: Union[None, str, Node] = None,\n                    target_function: Optional[str] = None,\n                    target: Union[None, str, Node] = None,\n                    relation: Optional[str] = None,\n                    ):\n        \"\"\"Return a query over the edges in the database.\n\n        Usually this means that you should call ``list()`` or ``.all()`` on this result.\n\n        :param bel: BEL statement that represents the desired edge.\n        :param source_function: Filter source nodes with the given BEL function\n        :param source: BEL term of source node e.g. ``p(HGNC:APP)`` or :class:`Node` object.\n        :param target_function: Filter target nodes with the given BEL function\n        :param target: BEL term of target node e.g. ``p(HGNC:APP)`` or :class:`Node` object.\n        :param relation: The relation that should be present between source and target node.\n        \"\"\"\n        if bel:\n            return self.search_edges_with_bel(bel)\n\n        query = self.session.query(Edge)\n\n        if relation:\n            query = query.filter(Edge.relation.like(relation))\n\n        if source_function:\n            query = self._add_edge_function_filter(query, Edge.source_id, source_function)\n\n        if target_function:\n            query = self._add_edge_function_filter(query, Edge.target_id, target_function)\n\n        if source:\n            if isinstance(source, str):\n                source = self.query_nodes(bel=source)\n                if source.count() == 0:\n                    return []\n                source = source.first()  # FIXME what if this matches multiple?\n                query = query.filter(Edge.source == source)\n            elif isinstance(source, Node):\n                query = query.filter(Edge.source == source)\n            else:\n                raise TypeError('Invalid type of {}: {}'.format(source, source.__class__.__name__))\n\n        if target:\n            if isinstance(target, str):\n                targets = self.query_nodes(bel=target).all()\n                target = targets[0]  # FIXME what if this matches multiple?\n                query = query.filter(Edge.target == target)\n            elif isinstance(target, Node):\n                query = query.filter(Edge.target == target)\n            else:\n                raise TypeError('Invalid type of {}: {}'.format(target, target.__class__.__name__))\n\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_citations(self,\n                        type: Optional[str] = None,\n                        reference: Optional[str] = None,\n                        name: Optional[str] = None,\n                        author: Union[None, str, List[str]] = None,\n                        date: Union[None, str, datetime.date] = None,\n                        evidence_text: Optional[str] = None,\n                        ) -> List[Citation]:\n        \"\"\"Query citations in the database.\n\n        :param type: Type of the citation. e.g. PubMed\n        :param reference: The identifier used for the citation. e.g. PubMed_ID\n        :param name: Title of the citation.\n        :param author: The name or a list of names of authors participated in the citation.\n        :param date: Publishing date of the citation.\n        :param evidence_text:\n        \"\"\"\n        query = self.session.query(Citation)\n\n        if author is not None:\n            query = query.join(Author, Citation.authors)\n            if isinstance(author, str):\n                query = query.filter(Author.name.like(author))\n            elif isinstance(author, Iterable):\n                query = query.filter(Author.has_name_in(set(author)))\n            else:\n                raise TypeError\n\n        if type and not reference:\n            query = query.filter(Citation.type.like(type))\n        elif reference and type:\n            query = query.filter(Citation.reference == reference)\n        elif reference and not type:\n            raise ValueError('reference specified without type')\n\n        if name:\n            query = query.filter(Citation.name.like(name))\n\n        if date:\n            if isinstance(date, datetime.date):\n                query = query.filter(Citation.date == date)\n            elif isinstance(date, str):\n                query = query.filter(Citation.date == parse_datetime(date))\n\n        if evidence_text:\n            query = query.join(Evidence).filter(Evidence.text.like(evidence_text))\n\n        return query.all()", "response": "Query the citations in the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all edges annotated to the documents identified by the given PubMed identifiers.", "response": "def query_edges_by_pubmed_identifiers(self, pubmed_identifiers: List[str]) -> List[Edge]:\n        \"\"\"Get all edges annotated to the documents identified by the given PubMed identifiers.\"\"\"\n        fi = and_(Citation.type == CITATION_TYPE_PUBMED, Citation.reference.in_(pubmed_identifiers))\n        return self.session.query(Edge).join(Evidence).join(Citation).filter(fi).all()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _edge_both_nodes(nodes: List[Node]):\n        node_ids = [node.id for node in nodes]\n\n        return and_(\n            Edge.source_id.in_(node_ids),\n            Edge.target_id.in_(node_ids),\n        )", "response": "Get edges where both the source and target are in the list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_induction(self, nodes: List[Node]) -> List[Edge]:\n        if len(nodes) < 2:\n            raise ValueError('not enough nodes given to induce over')\n\n        return self.session.query(Edge).filter(self._edge_both_nodes(nodes)).all()", "response": "Get all edges between any of the given nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _edge_one_node(nodes: List[Node]):\n        node_ids = [node.id for node in nodes]\n\n        return or_(\n            Edge.source_id.in_(node_ids),\n            Edge.target_id.in_(node_ids),\n        )", "response": "Return an iterable of edges where either the source or target are in the list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all edges incident to any of the given nodes.", "response": "def query_neighbors(self, nodes: List[Node]) -> List[Edge]:\n        \"\"\"Get all edges incident to any of the given nodes.\"\"\"\n        return self.session.query(Edge).filter(self._edge_one_node(nodes)).all()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_indra_statements(stmts,\n                          name: Optional[str] = None,\n                          version: Optional[str] = None,\n                          description: Optional[str] = None,\n                          authors: Optional[str] = None,\n                          contact: Optional[str] = None,\n                          license: Optional[str] = None,\n                          copyright: Optional[str] = None,\n                          disclaimer: Optional[str] = None,\n                          ):\n    \"\"\"Import a model from :mod:`indra`.\n\n    :param List[indra.statements.Statement] stmts: A list of statements\n    :param name: The graph's name\n    :param version: The graph's version. Recommended to use `semantic versioning <http://semver.org/>`_ or\n                        ``YYYYMMDD`` format.\n    :param description: The description of the graph\n    :param authors: The authors of this graph\n    :param contact: The contact email for this graph\n    :param license: The license for this graph\n    :param copyright: The copyright for this graph\n    :param disclaimer: The disclaimer for this graph\n    :rtype: pybel.BELGraph\n    \"\"\"\n    from indra.assemblers.pybel import PybelAssembler\n\n    pba = PybelAssembler(\n        stmts=stmts,\n        name=name,\n        version=version,\n        description=description,\n        authors=authors,\n        contact=contact,\n        license=license,\n        copyright=copyright,\n        disclaimer=disclaimer,\n    )\n\n    graph = pba.make_model()\n    return graph", "response": "Imports a BEL graph from a list of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_indra_pickle(path: str,\n                      name: Optional[str] = None,\n                      version: Optional[str] = None,\n                      description: Optional[str] = None,\n                      authors: Optional[str] = None,\n                      contact: Optional[str] = None,\n                      license: Optional[str] = None,\n                      copyright: Optional[str] = None,\n                      disclaimer: Optional[str] = None,\n                      ):\n    \"\"\"Import a model from :mod:`indra`.\n\n    :param path: Path to pickled list of :class:`indra.statements.Statement`\n    :param name: The name for the BEL graph\n    :param version: The version of the BEL graph\n    :param description: The description of the graph\n    :param authors: The authors of this graph\n    :param contact: The contact email for this graph\n    :param license: The license for this graph\n    :param copyright: The copyright for this graph\n    :param disclaimer: The disclaimer for this graph\n    :rtype: pybel.BELGraph\n    \"\"\"\n    with open(path, 'rb') as f:\n        statements = load(f)\n\n    return from_indra_statements(\n        stmts=statements,\n        name=name,\n        version=version,\n        description=description,\n        authors=authors,\n        contact=contact,\n        license=license,\n        copyright=copyright,\n        disclaimer=disclaimer,\n    )", "response": "Import a BEL graph from a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports this graph as a list of INDRA statements using the PybelProcessor.", "response": "def to_indra_statements(graph):\n    \"\"\"Export this graph as a list of INDRA statements using the :py:class:`indra.sources.pybel.PybelProcessor`.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :rtype: list[indra.statements.Statement]\n    \"\"\"\n    from indra.sources.bel import process_pybel_graph\n\n    pbp = process_pybel_graph(graph)\n    return pbp.statements"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport a BEL graph from a BioPAX OWL file.", "response": "def from_biopax(path: str,\n                name: Optional[str] = None,\n                version: Optional[str] = None,\n                description: Optional[str] = None,\n                authors: Optional[str] = None,\n                contact: Optional[str] = None,\n                license: Optional[str] = None,\n                copyright: Optional[str] = None,\n                disclaimer: Optional[str] = None,\n                ):\n    \"\"\"Import a model encoded in Pathway Commons `BioPAX <http://www.biopax.org/>`_ via :mod:`indra`.\n\n    :param path: Path to a BioPAX OWL file\n    :param name: The name for the BEL graph\n    :param version: The version of the BEL graph\n    :param description: The description of the graph\n    :param authors: The authors of this graph\n    :param contact: The contact email for this graph\n    :param license: The license for this graph\n    :param copyright: The copyright for this graph\n    :param disclaimer: The disclaimer for this graph\n    :rtype: pybel.BELGraph\n\n    .. warning:: Not compatible with all BioPAX! See INDRA documentation.\n    \"\"\"\n    from indra.sources.biopax import process_owl\n\n    model = process_owl(path)\n\n    return from_indra_statements(\n        stmts=model.statements,\n        name=name,\n        version=version,\n        description=description,\n        authors=authors,\n        contact=contact,\n        license=license,\n        copyright=copyright,\n        disclaimer=disclaimer,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_subgraphs_by_annotation(graph, annotation, sentinel=None):\n    if sentinel is not None:\n        subgraphs = _get_subgraphs_by_annotation_keep_undefined(graph, annotation, sentinel)\n    else:\n        subgraphs = _get_subgraphs_by_annotation_disregard_undefined(graph, annotation)\n\n    cleanup(graph, subgraphs)\n\n    return subgraphs", "response": "Stratify the given graph into sub - graphs based on the values for edges with the given annotation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the optional annotations shared by BEL namespace and annotation resource documents.", "response": "def extract_shared_optional(bel_resource, definition_header: str = 'Namespace'):\n    \"\"\"Get the optional annotations shared by BEL namespace and annotation resource documents.\n\n    :param dict bel_resource: A configuration dictionary representing a BEL resource\n    :param definition_header: ``Namespace`` or ``AnnotationDefinition``\n    :rtype: dict\n    \"\"\"\n    shared_mapping = {\n        'description': (definition_header, 'DescriptionString'),\n        'version': (definition_header, 'VersionString'),\n        'author': ('Author', 'NameString'),\n        'license': ('Author', 'CopyrightString'),\n        'contact': ('Author', 'ContactInfoString'),\n        'citation': ('Citation', 'NameString'),\n        'citation_description': ('Citation', 'DescriptionString'),\n        'citation_version': ('Citation', 'PublishedVersionString'),\n        'citation_url': ('Citation', 'ReferenceURL'),\n    }\n\n    result = {}\n\n    update_insert_values(bel_resource, shared_mapping, result)\n\n    if 'PublishedDate' in bel_resource.get('Citation', {}):\n        result['citation_published'] = parse_datetime(bel_resource['Citation']['PublishedDate'])\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the value dictionary with a BEL resource dictionary.", "response": "def update_insert_values(bel_resource: Mapping, mapping: Mapping[str, Tuple[str, str]], values: Dict[str, str]) -> None:\n    \"\"\"Update the value dictionary with a BEL resource dictionary.\"\"\"\n    for database_column, (section, key) in mapping.items():\n        if section in bel_resource and key in bel_resource[section]:\n            values[database_column] = bel_resource[section][key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsafes converts an integer to an integer or passes through None.", "response": "def int_or_str(v: Optional[str]) -> Union[None, int, str]:\n    \"\"\"Safe converts an string represent an integer to an integer or passes through ``None``.\"\"\"\n    if v is None:\n        return\n    try:\n        return int(v)\n    except ValueError:\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a graph to the receiver service and return the response object.", "response": "def to_web(graph: BELGraph,\n           host: Optional[str] = None,\n           user: Optional[str] = None,\n           password: Optional[str] = None,\n           public: bool = False,\n           ) -> requests.Response:\n    \"\"\"Send a graph to the receiver service and returns the :mod:`requests` response object.\n\n    :param graph: A BEL graph\n    :param host: The location of the BEL Commons server. Alternatively, looks up in PyBEL config with\n     ``PYBEL_REMOTE_HOST`` or the environment as ``PYBEL_REMOTE_HOST`` Defaults to\n     :data:`pybel.constants.DEFAULT_SERVICE_URL`\n    :param user: Username for BEL Commons. Alternatively, looks up in PyBEL config with\n     ``PYBEL_REMOTE_USER`` or the environment as ``PYBEL_REMOTE_USER``\n    :param password: Password for BEL Commons. Alternatively, looks up in PyBEL config with\n     ``PYBEL_REMOTE_PASSWORD`` or the environment as ``PYBEL_REMOTE_PASSWORD``\n    :return: The response object from :mod:`requests`\n    \"\"\"\n    if host is None:\n        host = _get_host()\n        log.debug('using host: %s', host)\n\n    if user is None:\n        user = _get_user()\n\n        if user is None:\n            raise ValueError('no user found')\n\n    if password is None:\n        password = _get_password()\n\n        if password is None:\n            raise ValueError('no password found')\n\n    url = host.rstrip('/') + RECIEVE_ENDPOINT\n\n    response = requests.post(\n        url,\n        json=to_json(graph),\n        headers={\n            'content-type': 'application/json',\n            'User-Agent': 'PyBEL v{}'.format(get_version()),\n            'bel-commons-public': 'true' if public else 'false',\n        },\n        auth=(user, password),\n    )\n    log.debug('received response: %s', response)\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve a public network from BEL Commons.", "response": "def from_web(network_id: int, host: Optional[str] = None) -> BELGraph:\n    \"\"\"Retrieve a public network from BEL Commons.\n\n    In the future, this function may be extended to support authentication.\n\n    :param int network_id: The BEL Commons network identifier\n    :param Optional[str] host: The location of the BEL Commons server. Alternatively, looks up in PyBEL config with\n     ``PYBEL_REMOTE_HOST`` or the environment as ``PYBEL_REMOTE_HOST`` Defaults to\n     :data:`pybel.constants.DEFAULT_SERVICE_URL`\n    :rtype: pybel.BELGraph\n    \"\"\"\n    if host is None:\n        host = _get_host()\n\n    url = host + GET_ENDPOINT.format(network_id)\n    res = requests.get(url)\n    graph_json = res.json()\n    graph = from_json(graph_json)\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a fusion parser.", "response": "def get_fusion_language(identifier: ParserElement, permissive: bool = True) -> ParserElement:\n    \"\"\"Build a fusion parser.\"\"\"\n    range_coordinate = Suppress('\"') + range_coordinate_unquoted + Suppress('\"')\n\n    if permissive:  # permissive to wrong quoting\n        range_coordinate = range_coordinate | range_coordinate_unquoted\n\n    return fusion_tags + nest(\n        Group(identifier)(PARTNER_5P),\n        Group(range_coordinate)(RANGE_5P),\n        Group(identifier)(PARTNER_3P),\n        Group(range_coordinate)(RANGE_3P),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a legacy fusion parser.", "response": "def get_legacy_fusion_langauge(identifier: ParserElement, reference: str) -> ParserElement:\n    \"\"\"Build a legacy fusion parser.\"\"\"\n    break_start = (ppc.integer | '?').setParseAction(_fusion_break_handler_wrapper(reference, start=True))\n    break_end = (ppc.integer | '?').setParseAction(_fusion_break_handler_wrapper(reference, start=False))\n\n    res = (\n        identifier(PARTNER_5P) +\n        WCW +\n        fusion_tags +\n        nest(\n            identifier(PARTNER_3P) +\n            Optional(WCW + Group(break_start)(RANGE_5P) + WCW + Group(break_end)(RANGE_3P))\n        )\n    )\n\n    res.setParseAction(_fusion_legacy_handler)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling a legacy fusion.", "response": "def _fusion_legacy_handler(_, __, tokens):\n    \"\"\"Handle a legacy fusion.\"\"\"\n    if RANGE_5P not in tokens:\n        tokens[RANGE_5P] = {FUSION_MISSING: '?'}\n    if RANGE_3P not in tokens:\n        tokens[RANGE_3P] = {FUSION_MISSING: '?'}\n    return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites this graph to GraphML XML file using NetworkX.", "response": "def to_graphml(graph: BELGraph, path: Union[str, BinaryIO]) -> None:\n    \"\"\"Write this graph to GraphML XML file using :func:`networkx.write_graphml`.\n\n    The .graphml file extension is suggested so Cytoscape can recognize it.\n    \"\"\"\n    rv = nx.MultiDiGraph()\n\n    for node in graph:\n        rv.add_node(node.as_bel(), function=node.function)\n\n    for u, v, key, edge_data in graph.edges(data=True, keys=True):\n        rv.add_edge(\n            u.as_bel(),\n            v.as_bel(),\n            interaction=edge_data[RELATION],\n            bel=graph.edge_to_bel(u, v, edge_data),\n            key=key,\n        )\n\n    nx.write_graphml(rv, path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_csv(graph: BELGraph, file: Optional[TextIO] = None, sep: Optional[str] = None) -> None:\n    if sep is None:\n        sep = '\\t'\n\n    for u, v, data in graph.edges(data=True):\n        print(\n            graph.edge_to_bel(u, v, edge_data=data, sep=sep),\n            json.dumps(data),\n            sep=sep,\n            file=file,\n        )", "response": "Write the graph as a tab - separated edge list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_csv_path(graph: BELGraph, path: str, sep: Optional[str] = None) -> None:\n    with open(path, 'w') as file:\n        to_csv(graph, file, sep=sep)", "response": "Write the graph as a tab - separated edge list to a file at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_sif(graph: BELGraph, file: Optional[TextIO] = None, sep: Optional[str] = None) -> None:\n    if sep is None:\n        sep = '\\t'\n\n    for u, v, data in graph.edges(data=True):\n        print(\n            graph.edge_to_bel(u, v, edge_data=data, sep=sep),\n            file=file,\n        )", "response": "Write the graph as a tab - separated SIF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the graph as a tab - separated SIF file.", "response": "def to_sif_path(graph: BELGraph, path: str, sep: Optional[str] = None) -> None:\n    \"\"\"Write the graph as a tab-separated SIF file. to a file at the given path.\"\"\"\n    with open(path, 'w') as file:\n        to_sif(graph, file, sep=sep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the genes and gene products to a GRP file for use with GSEA gene set enrichment analysis.", "response": "def to_gsea(graph: BELGraph, file: Optional[TextIO] = None) -> None:\n    \"\"\"Write the genes/gene products to a GRP file for use with GSEA gene set enrichment analysis.\n\n    .. seealso::\n\n        - GRP `format specification <http://software.broadinstitute.org/cancer/software/gsea/wiki/index.php/Data_formats#GRP:_Gene_set_file_format_.28.2A.grp.29>`_\n        - GSEA `publication <https://doi.org/10.1073/pnas.0506580102>`_\n    \"\"\"\n    print('# {}'.format(graph.name), file=file)\n    nodes = {\n        data[NAME]\n        for data in graph\n        if NAMESPACE in data and data[NAMESPACE].upper() == 'HGNC' and NAME in data\n    }\n    for node in sorted(nodes):\n        print(node, file=file)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_gsea_path(graph: BELGraph, path: str) -> None:\n    with open(path, 'w') as file:\n        to_gsea(graph, file)", "response": "Write the genes and gene products to a GRP file at the given path for use with GSEA gene set enrichment analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles node ( s ) that might be dictionaries.", "response": "def _handle_nodes(nodes: MaybeNodeList) -> List[BaseEntity]:\n    \"\"\"Handle node(s) that might be dictionaries.\"\"\"\n    if isinstance(nodes, BaseEntity):\n        return [nodes]\n\n    return [\n        (\n            parse_result_to_dsl(node)\n            if not isinstance(node, BaseEntity) else\n            node\n        )\n        for node in nodes\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a seed induction method for single annotation s values.", "response": "def append_annotation(self, annotation: str, values: Set[str]) -> 'Seeding':\n        \"\"\"Add a seed induction method for single annotation's values.\n\n        :param annotation: The annotation to filter by\n        :param values: The values of the annotation to keep\n        :returns: self for fluid API\n        \"\"\"\n        return self._append_seed(SEED_TYPE_ANNOTATION, {\n            'annotations': {\n                annotation: values,\n            }\n        })"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append_sample(self, **kwargs) -> 'Seeding':\n        data = {\n            'seed': random.randint(0, 1000000)\n        }\n        data.update(kwargs)\n\n        return self._append_seed(SEED_TYPE_SAMPLE, data)", "response": "Add sample induction methods."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _append_seed(self, seed_type: str, data: Any) -> 'Seeding':\n        self.append({\n            SEED_METHOD: seed_type,\n            SEED_DATA: data,\n        })\n        return self", "response": "Add a seeding method and returns self."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _append_seed_handle_nodes(self, seed_type: str, nodes: MaybeNodeList) -> 'Seeding':\n        return self._append_seed(seed_type, _handle_nodes(nodes))", "response": "Add a seeding method and returns self.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, graph):\n        if not self:\n            log.debug('no seeding, returning graph: %s', graph)\n            return graph\n\n        subgraphs = []\n\n        for seed in self:\n            seed_method, seed_data = seed[SEED_METHOD], seed[SEED_DATA]\n\n            log.debug('seeding with %s: %s', seed_method, seed_data)\n            subgraph = get_subgraph(graph, seed_method=seed_method, seed_data=seed_data)\n\n            if subgraph is None:\n                log.debug('seed returned empty graph: %s', seed)\n                continue\n\n            subgraphs.append(subgraph)\n\n        if not subgraphs:\n            log.debug('no subgraphs returned')\n            return\n\n        return union(subgraphs)", "response": "Seed the graph or return none if not possible."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump this seeding container to a file as JSON.", "response": "def dump(self, file, sort_keys: bool = True, **kwargs) -> None:\n        \"\"\"Dump this seeding container to a file as JSON.\"\"\"\n        json.dump(self.to_json(), file, sort_keys=sort_keys, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndump this query to a string as JSON.", "response": "def dumps(self, sort_keys: bool = True, **kwargs) -> str:\n        \"\"\"Dump this query to a string as JSON.\"\"\"\n        return json.dumps(self.to_json(), sort_keys=sort_keys, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef function_inclusion_filter_builder(func: Strings) -> NodePredicate:\n    if isinstance(func, str):\n        return _single_function_inclusion_filter_builder(func)\n\n    elif isinstance(func, Iterable):\n        return _collection_function_inclusion_builder(func)\n\n    raise TypeError('Invalid type for argument: {}'.format(func))", "response": "Build a filter that only passes on nodes of the given function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _single_function_inclusion_filter_builder(func: str) -> NodePredicate:  # noqa: D202\n\n    def function_inclusion_filter(_: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass only for a node that has the enclosed function.\"\"\"\n        return node.function == func\n\n    return function_inclusion_filter", "response": "Build a function inclusion filter for a single function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _collection_function_inclusion_builder(funcs: Iterable[str]) -> NodePredicate:\n    funcs = set(funcs)\n\n    if not funcs:\n        raise ValueError('can not build function inclusion filter with empty list of functions')\n\n    def functions_inclusion_filter(_: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass only for a node that is one of the enclosed functions.\"\"\"\n        return node.function in funcs\n\n    return functions_inclusion_filter", "response": "Build a function inclusion filter for a collection of functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a filter that passes only on nodes that don t have the given key in their data dictionary.", "response": "def data_missing_key_builder(key: str) -> NodePredicate:  # noqa: D202\n    \"\"\"Build a filter that passes only on nodes that don't have the given key in their data dictionary.\n\n    :param str key: A key for the node's data dictionary\n    \"\"\"\n\n    def data_does_not_contain_key(graph: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass only for a node that doesn't contain the enclosed key in its data dictionary.\"\"\"\n        return key not in graph.nodes[node]\n\n    return data_does_not_contain_key"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a filter for nodes whose associated data with the given key passes the given predicate.", "response": "def build_node_data_search(key: str, data_predicate: Callable[[Any], bool]) -> NodePredicate:  # noqa: D202\n    \"\"\"Build a filter for nodes whose associated data with the given key passes the given predicate.\n\n    :param key: The node data dictionary key to check\n    :param data_predicate: The filter to apply to the node data dictionary\n    \"\"\"\n\n    def node_data_filter(_: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass if the given node has a given data annotated and passes the contained filter.\"\"\"\n        value = node.get(key)\n        return value is not None and data_predicate(value)\n\n    return node_data_filter"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_node_graph_data_search(key: str, data_predicate: Callable[[Any], bool]):  # noqa: D202\n\n    def node_data_filter(graph: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass if the given node has a given data annotated and passes the contained filter.\"\"\"\n        value = graph.nodes[node].get(key)\n        return value is not None and data_predicate(value)\n\n    return node_data_filter", "response": "Build a function for testing data associated with the node in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a node filter for nodes whose values for the given key are superstrings of the query string ( s.", "response": "def build_node_key_search(query, key) -> NodePredicate:\n    \"\"\"Build a node filter for nodes whose values for the given key are superstrings of the query string(s).\n\n    :param query: The query string or strings to check if they're in the node name\n    :type query: str or iter[str]\n    :param str key: The key for the node data dictionary. Should refer only to entries that have str values\n    \"\"\"\n    if isinstance(query, str):\n        return build_node_data_search(key, lambda s: query.lower() in s.lower())\n\n    if isinstance(query, Iterable):\n        return build_node_data_search(key, lambda s: any(q.lower() in s.lower() for q in query))\n\n    raise TypeError('query is wrong type: %s', query)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a predicate for namespace inclusion.", "response": "def namespace_inclusion_builder(namespace: Strings) -> NodePredicate:\n    \"\"\"Build a predicate for namespace inclusion.\"\"\"\n    if isinstance(namespace, str):\n        def namespace_filter(_: BELGraph, node: BaseEntity) -> bool:\n            \"\"\"Pass only for a node that has the enclosed namespace.\"\"\"\n            return NAMESPACE in node and node[NAMESPACE] == namespace\n\n    elif isinstance(namespace, Iterable):\n        namespaces = set(namespace)\n\n        def namespace_filter(_: BELGraph, node: BaseEntity) -> bool:\n            \"\"\"Pass only for a node that has a namespace in the enclosed set.\"\"\"\n            return NAMESPACE in node and node[NAMESPACE] in namespaces\n\n    else:\n        raise TypeError('Invalid type for argument: {}'.format(namespace))\n\n    return namespace_filter"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that the namespace has either been defined by an enumeration or a regular expression.", "response": "def has_namespace(self, namespace: str) -> bool:\n        \"\"\"Check that the namespace has either been defined by an enumeration or a regular expression.\"\"\"\n        return self.has_enumerated_namespace(namespace) or self.has_regex_namespace(namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the namespace is defined by an enumeration and that the name is a member.", "response": "def has_enumerated_namespace_name(self, namespace: str, name: str) -> bool:\n        \"\"\"Check that the namespace is defined by an enumeration and that the name is a member.\"\"\"\n        return self.has_enumerated_namespace(namespace) and name in self.namespace_to_terms[namespace]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_regex_namespace_name(self, namespace: str, name: str) -> bool:\n        return self.has_regex_namespace(namespace) and self.namespace_to_pattern[namespace].match(name)", "response": "Check that the namespace is defined as a regular expression and the name matches it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the namespace is defined and has the given name.", "response": "def has_namespace_name(self, line: str, position: int, namespace: str, name: str) -> bool:\n        \"\"\"Check that the namespace is defined and has the given name.\"\"\"\n        self.raise_for_missing_namespace(line, position, namespace, name)\n        return self.has_enumerated_namespace_name(namespace, name) or self.has_regex_namespace_name(namespace, name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raise_for_missing_namespace(self, line: str, position: int, namespace: str, name: str) -> None:\n        if not self.has_namespace(namespace):\n            raise UndefinedNamespaceWarning(self.get_line_number(), line, position, namespace, name)", "response": "Raise an exception if the namespace is not defined."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nraises an exception if the namespace is not defined or if it does not validate the given name.", "response": "def raise_for_missing_name(self, line: str, position: int, namespace: str, name: str) -> None:\n        \"\"\"Raise an exception if the namespace is not defined or if it does not validate the given name.\"\"\"\n        self.raise_for_missing_namespace(line, position, namespace, name)\n\n        if self.has_enumerated_namespace(namespace) and not self.has_enumerated_namespace_name(namespace, name):\n            raise MissingNamespaceNameWarning(self.get_line_number(), line, position, namespace, name)\n\n        if self.has_regex_namespace(namespace) and not self.has_regex_namespace_name(namespace, name):\n            raise MissingNamespaceRegexWarning(self.get_line_number(), line, position, namespace, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef raise_for_missing_default(self, line: str, position: int, name: str) -> None:\n        if not self.default_namespace:\n            raise ValueError('Default namespace is not set')\n\n        if name not in self.default_namespace:\n            raise MissingDefaultNameWarning(self.get_line_number(), line, position, name)", "response": "Raise an exeception if the name does not belong to the default namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_identifier_qualified(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        namespace, name = tokens[NAMESPACE], tokens[NAME]\n\n        self.raise_for_missing_namespace(line, position, namespace, name)\n        self.raise_for_missing_name(line, position, namespace, name)\n\n        return tokens", "response": "Handle parsing a qualified identifier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle parsing an identifier for the default namespace.", "response": "def handle_namespace_default(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle parsing an identifier for the default namespace.\"\"\"\n        name = tokens[NAME]\n        self.raise_for_missing_default(line, position, name)\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles parsing an identifier for name missing a namespac ethat are outside the default namespace.", "response": "def handle_namespace_lenient(line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle parsing an identifier for name missing a namespac ethat are outside the default namespace.\"\"\"\n        tokens[NAMESPACE] = DIRTY\n        log.debug('Naked namespace: [%d] %s', position, line)\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_namespace_invalid(self, line: str, position: int, tokens: ParseResults) -> None:\n        name = tokens[NAME]\n        raise NakedNameWarning(self.get_line_number(), line, position, name)", "response": "Raise an exception when parsing a name missing a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting a namespace URL.", "response": "def hbp_namespace(name: str, sha: Optional[str] = None) -> str:\n    \"\"\"Format a namespace URL.\"\"\"\n    return HBP_NAMESPACE_URL.format(sha=sha or LAST_HASH, keyword=name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef edge_predicate(func: DictEdgePredicate) -> EdgePredicate:  # noqa: D202\n\n    @wraps(func)\n    def _wrapped(*args):\n        x = args[0]\n\n        if isinstance(x, BELGraph):\n            u, v, k = args[1:4]\n            return func(x[u][v][k])\n\n        return func(*args)\n\n    return _wrapped", "response": "Decorate an edge predicate function that only takes a single argument and a PyBEL node data dictionary as its singular argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the edge has a PubMed citation.", "response": "def has_pubmed(edge_data: EdgeData) -> bool:\n    \"\"\"Check if the edge has a PubMed citation.\"\"\"\n    return CITATION in edge_data and CITATION_TYPE_PUBMED == edge_data[CITATION][CITATION_TYPE]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the edge contains author information for its citation.", "response": "def has_authors(edge_data: EdgeData) -> bool:\n    \"\"\"Check if the edge contains author information for its citation.\"\"\"\n    return CITATION in edge_data and CITATION_AUTHORS in edge_data[CITATION] and edge_data[CITATION][CITATION_AUTHORS]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the given edge has the given modifier.", "response": "def _has_modifier(edge_data: EdgeData, modifier: str) -> bool:\n    \"\"\"Check if the edge has the given modifier.\n\n    :param edge_data: The edge data dictionary\n    :param modifier: The modifier to check. One of :data:`pybel.constants.ACTIVITY`,\n                        :data:`pybel.constants.DEGRADATION`, or :data:`pybel.constants.TRANSLOCATION`.\n    :return: Does either the subject or object have the given modifier\n    \"\"\"\n    return part_has_modifier(edge_data, SUBJECT, modifier) or part_has_modifier(edge_data, OBJECT, modifier)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef edge_has_annotation(edge_data: EdgeData, key: str) -> Optional[Any]:\n    annotations = edge_data.get(ANNOTATIONS)\n\n    if annotations is None:\n        return None\n\n    return annotations.get(key)", "response": "Check if an edge has the given annotation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the subject of this edge is a pathology and has a causal relationship with a non - bioprocess / pathology.", "response": "def has_pathology_causal(graph: BELGraph, u: BaseEntity, v: BaseEntity, k: str) -> bool:\n    \"\"\"Check if the subject is a pathology and has a causal relationship with a non bioprocess/pathology.\n\n    :return: If the subject of this edge is a pathology and it participates in a causal reaction.\n    \"\"\"\n    return (\n        isinstance(u, Pathology) and\n        is_causal_relation(graph, u, v, k) and\n        not isinstance(v, (Pathology, BiologicalProcess))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_gene_substitution_language() -> ParserElement:\n    parser_element = gsub_tag + nest(\n        dna_nucleotide(GSUB_REFERENCE),\n        ppc.integer(GSUB_POSITION),\n        dna_nucleotide(GSUB_VARIANT),\n    )\n    parser_element.setParseAction(_handle_gsub)\n    return parser_element", "response": "Build a gene substitution parser."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append_seeding_induction(self, nodes: Union[BaseEntity, List[BaseEntity], List[Dict]]) -> Seeding:\n        return self.seeding.append_induction(nodes)", "response": "Add a seed induction method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append_seeding_neighbors(self, nodes: Union[BaseEntity, List[BaseEntity], List[Dict]]) -> Seeding:\n        return self.seeding.append_neighbors(nodes)", "response": "Add a seed by neighbors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a seed induction method for single annotation s values.", "response": "def append_seeding_annotation(self, annotation: str, values: Set[str]) -> Seeding:\n        \"\"\"Add a seed induction method for single annotation's values.\n\n        :param annotation: The annotation to filter by\n        :param values: The values of the annotation to keep\n        \"\"\"\n        return self.seeding.append_annotation(annotation, values)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append_pipeline(self, name, *args, **kwargs) -> Pipeline:\n        return self.pipeline.append(name, *args, **kwargs)", "response": "Adds an entry to the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning this query and returns the resulting BEL graph.", "response": "def run(self, manager):\n        \"\"\"Run this query and returns the resulting BEL graph.\n\n        :param manager: A cache manager\n        :rtype: Optional[pybel.BELGraph]\n        \"\"\"\n        universe = self._get_universe(manager)\n        graph = self.seeding.run(universe)\n        return self.pipeline.run(graph, universe=universe)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn this query as a JSON object.", "response": "def to_json(self) -> Dict:\n        \"\"\"Return this query as a JSON object.\"\"\"\n        rv = {\n            'network_ids': self.network_ids,\n        }\n\n        if self.seeding:\n            rv['seeding'] = self.seeding.to_json()\n\n        if self.pipeline:\n            rv['pipeline'] = self.pipeline.to_json()\n\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_json(data: Mapping) -> 'Query':\n        network_ids = data.get('network_ids')\n        if network_ids is None:\n            raise QueryMissingNetworksError('query JSON did not have key \"network_ids\"')\n\n        seeding_data = data.get('seeding')\n        seeding = (\n            Seeding.from_json(seeding_data)\n            if seeding_data is not None else\n            None\n        )\n\n        pipeline_data = data.get('pipeline')\n        pipeline = (\n            Pipeline.from_json(pipeline_data)\n            if pipeline_data is not None else\n            None\n        )\n\n        return Query(\n            network_ids=network_ids,\n            seeding=seeding,\n            pipeline=pipeline,\n        )", "response": "Load a query from a JSON dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle a BEL 2. 0 style molecular activity with BEL default names.", "response": "def handle_molecular_activity_default(_: str, __: int, tokens: ParseResults) -> ParseResults:\n    \"\"\"Handle a BEL 2.0 style molecular activity with BEL default names.\"\"\"\n    upgraded = language.activity_labels[tokens[0]]\n    tokens[NAMESPACE] = BEL_DEFAULT_NAMESPACE\n    tokens[NAME] = upgraded\n    return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle BEL 1. 0 activities.", "response": "def handle_activity_legacy(_: str, __: int, tokens: ParseResults) -> ParseResults:\n    \"\"\"Handle BEL 1.0 activities.\"\"\"\n    legacy_cls = language.activity_labels[tokens[MODIFIER]]\n    tokens[MODIFIER] = ACTIVITY\n    tokens[EFFECT] = {\n        NAME: legacy_cls,\n        NAMESPACE: BEL_DEFAULT_NAMESPACE\n    }\n    log.log(5, 'upgraded legacy activity to %s', legacy_cls)\n    return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_legacy_tloc(line: str, position: int, tokens: ParseResults) -> ParseResults:\n    log.log(5, 'legacy translocation statement: %s [%d]', line, position)\n    return tokens", "response": "Handle translocations that lack the fromLoc and toLoc entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef modifier_po_to_dict(tokens):\n    attrs = {}\n\n    if LOCATION in tokens:\n        attrs[LOCATION] = dict(tokens[LOCATION])\n\n    if MODIFIER not in tokens:\n        return attrs\n\n    if LOCATION in tokens[TARGET]:\n        attrs[LOCATION] = tokens[TARGET][LOCATION].asDict()\n\n    if tokens[MODIFIER] == DEGRADATION:\n        attrs[MODIFIER] = tokens[MODIFIER]\n\n    elif tokens[MODIFIER] == ACTIVITY:\n        attrs[MODIFIER] = tokens[MODIFIER]\n\n        if EFFECT in tokens:\n            attrs[EFFECT] = dict(tokens[EFFECT])\n\n    elif tokens[MODIFIER] == TRANSLOCATION:\n        attrs[MODIFIER] = tokens[MODIFIER]\n\n        if EFFECT in tokens:\n            attrs[EFFECT] = tokens[EFFECT].asDict()\n\n    elif tokens[MODIFIER] == CELL_SECRETION:\n        attrs.update(secretion())\n\n    elif tokens[MODIFIER] == CELL_SURFACE_EXPRESSION:\n        attrs.update(cell_surface_expression())\n\n    else:\n        raise ValueError('Invalid value for tokens[MODIFIER]: {}'.format(tokens[MODIFIER]))\n\n    return attrs", "response": "Get the location activity and transformation information as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the dictionary of namespace to encoding mappings.", "response": "def _namespace_dict(self) -> Mapping[str, Mapping[str, str]]:\n        \"\"\"Get the dictionary of {namespace: {name: encoding}} stored in the internal identifier parser.\"\"\"\n        return self.identifier_parser.namespace_to_terms"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_nested_relation(self, line: str, position: int, tokens: ParseResults):\n        if not self.allow_nested:\n            raise NestedRelationWarning(self.get_line_number(), line, position)\n\n        self._handle_relation_harness(line, position, {\n            SUBJECT: tokens[SUBJECT],\n            RELATION: tokens[RELATION],\n            OBJECT: tokens[OBJECT][SUBJECT],\n        })\n\n        self._handle_relation_harness(line, position, {\n            SUBJECT: tokens[OBJECT][SUBJECT],\n            RELATION: tokens[OBJECT][RELATION],\n            OBJECT: tokens[OBJECT][OBJECT],\n        })\n        return tokens", "response": "Handle nested relation statements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nraising an exception if the function used on the tokens is wrong.", "response": "def check_function_semantics(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Raise an exception if the function used on the tokens is wrong.\n\n        :raises: InvalidFunctionSemantic\n        \"\"\"\n        if not self._namespace_dict or NAMESPACE not in tokens:\n            return tokens\n\n        namespace, name = tokens[NAMESPACE], tokens[NAME]\n\n        if namespace in self.identifier_parser.namespace_to_pattern:\n            return tokens\n\n        if self._allow_naked_names and tokens[NAMESPACE] == DIRTY:  # Don't check dirty names in lenient mode\n            return tokens\n\n        valid_functions = set(itt.chain.from_iterable(\n            belns_encodings.get(k, set())\n            for k in self._namespace_dict[namespace][name]\n        ))\n\n        if not valid_functions:\n            raise InvalidEntity(self.get_line_number(), line, position, namespace, name)\n\n        if tokens[FUNCTION] not in valid_functions:\n            raise InvalidFunctionSemantic(self.get_line_number(), line, position, tokens[FUNCTION], namespace, name,\n                                          valid_functions)\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_term(self, _, __, tokens: ParseResults) -> ParseResults:\n        self.ensure_node(tokens)\n        return tokens", "response": "Handle BEL terms (the subject and object of BEL relations)."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide the functionality for list operations.", "response": "def _handle_list_helper(self, tokens: ParseResults, relation: str) -> ParseResults:\n        \"\"\"Provide the functionality for :meth:`handle_has_members` and :meth:`handle_has_components`.\"\"\"\n        parent_node_dsl = self.ensure_node(tokens[0])\n\n        for child_tokens in tokens[2]:\n            child_node_dsl = self.ensure_node(child_tokens)\n            self.graph.add_unqualified_edge(parent_node_dsl, child_node_dsl, relation)\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_has_members(self, _, __, tokens: ParseResults) -> ParseResults:\n        return self._handle_list_helper(tokens, HAS_MEMBER)", "response": "Handle list relations like p X hasMembers list ( Y Z..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling list relations like p X hasComponents list ( Y Z...", "response": "def handle_has_components(self, _, __, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle list relations like ``p(X) hasComponents list(p(Y), p(Z), ...)``.\"\"\"\n        return self._handle_list_helper(tokens, HAS_COMPONENT)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a qualified edge from the internal aspects of the parser.", "response": "def _add_qualified_edge_helper(self, u, v, relation, annotations, subject_modifier, object_modifier) -> str:\n        \"\"\"Add a qualified edge from the internal aspects of the parser.\"\"\"\n        return self.graph.add_qualified_edge(\n            u,\n            v,\n            relation=relation,\n            evidence=self.control_parser.evidence,\n            citation=self.control_parser.citation.copy(),\n            annotations=annotations,\n            subject_modifier=subject_modifier,\n            object_modifier=object_modifier,\n            **{LINE: self.get_line_number()}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an edge then adds the opposite direction edge if it should.", "response": "def _add_qualified_edge(self, u, v, relation, annotations, subject_modifier, object_modifier) -> str:\n        \"\"\"Add an edge, then adds the opposite direction edge if it should.\"\"\"\n        sha512 = self._add_qualified_edge_helper(\n            u,\n            v,\n            relation=relation,\n            annotations=annotations,\n            subject_modifier=subject_modifier,\n            object_modifier=object_modifier,\n        )\n\n        if relation in TWO_WAY_RELATIONS:\n            self._add_qualified_edge_helper(\n                v,\n                u,\n                relation=relation,\n                annotations=annotations,\n                object_modifier=subject_modifier,\n                subject_modifier=object_modifier,\n            )\n\n        return sha512"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle BEL relations based on the policy specified on instantiation.", "response": "def _handle_relation_harness(self, line: str, position: int, tokens: Union[ParseResults, Dict]) -> ParseResults:\n        \"\"\"Handle BEL relations based on the policy specified on instantiation.\n\n        Note: this can't be changed after instantiation!\n        \"\"\"\n        if not self.control_parser.citation:\n            raise MissingCitationException(self.get_line_number(), line, position)\n\n        if not self.control_parser.evidence:\n            raise MissingSupportWarning(self.get_line_number(), line, position)\n\n        missing_required_annotations = self.control_parser.get_missing_required_annotations()\n        if missing_required_annotations:\n            raise MissingAnnotationWarning(self.get_line_number(), line, position, missing_required_annotations)\n\n        self._handle_relation(tokens)\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling statements like p ( X label for X.", "response": "def handle_label_relation(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle statements like ``p(X) label \"Label for X\"``.\n\n        :raises: RelabelWarning\n        \"\"\"\n        subject_node_dsl = self.ensure_node(tokens[SUBJECT])\n        description = tokens[OBJECT]\n\n        if self.graph.has_node_description(subject_node_dsl):\n            raise RelabelWarning(\n                line_number=self.get_line_number(),\n                line=line,\n                position=position,\n                node=self.graph.node,\n                old_label=self.graph.get_node_description(subject_node_dsl),\n                new_label=description\n            )\n\n        self.graph.set_node_description(subject_node_dsl, description)\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nturns parsed tokens into canonical node name and makes sure it s in the graph.", "response": "def ensure_node(self, tokens: ParseResults) -> BaseEntity:\n        \"\"\"Turn parsed tokens into canonical node name and makes sure its in the graph.\"\"\"\n        if MODIFIER in tokens:\n            return self.ensure_node(tokens[TARGET])\n\n        node = parse_result_to_dsl(tokens)\n        self.graph.add_node_from_data(node)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_translocation_illegal(self, line: str, position: int, tokens: ParseResults) -> None:\n        raise MalformedTranslocationWarning(self.get_line_number(), line, position, tokens)", "response": "Handle a malformed translocation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dsl_by_hash(self, node_hash: str) -> Optional[BaseEntity]:\n        node = self.get_node_by_hash(node_hash)\n        if node is not None:\n            return node.as_bel()", "response": "Look up a node by the hash and returns the corresponding PyBEL node tuple."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlook up a node by its hash.", "response": "def get_node_by_hash(self, node_hash: str) -> Optional[Node]:\n        \"\"\"Look up a node by its hash.\"\"\"\n        return self.session.query(Node).filter(Node.sha512 == node_hash).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooking up several nodes by their hashes.", "response": "def get_nodes_by_hashes(self, node_hashes: List[str]) -> List[Node]:\n        \"\"\"Look up several nodes by their hashes.\"\"\"\n        return self.session.query(Node).filter(Node.sha512.in_(node_hashes)).all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_node_by_dsl(self, node_dict: BaseEntity) -> Optional[Node]:\n        return self.get_node_by_hash(node_dict.as_sha512())", "response": "Look up a node by its data dictionary by hashing it then using get_node_by_hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_edge_by_hash(self, edge_hash: str) -> Optional[Edge]:\n        return self.session.query(Edge).filter(Edge.sha512 == edge_hash).one_or_none()", "response": "Look up an edge by its hash."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_edges_by_hashes(self, edge_hashes: List[str]) -> List[Edge]:\n        return self.session.query(Edge).filter(Edge.sha512.in_(edge_hashes)).all()", "response": "Look up several edges by hashes of their PyBEL edge data dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_citation_by_pmid(self, pubmed_identifier: str) -> Optional[Citation]:\n        return self.get_citation_by_reference(reference=pubmed_identifier, type=CITATION_TYPE_PUBMED)", "response": "Get a citation object by its PubMed identifier."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a citation object by its type and reference.", "response": "def get_citation_by_reference(self, type: str, reference: str) -> Optional[Citation]:\n        \"\"\"Get a citation object by its type and reference.\"\"\"\n        citation_hash = hash_citation(type=type, reference=reference)\n        return self.get_citation_by_hash(citation_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a citation object by its hash.", "response": "def get_citation_by_hash(self, citation_hash: str) -> Optional[Citation]:\n        \"\"\"Get a citation object by its hash.\"\"\"\n        return self.session.query(Citation).filter(Citation.sha512 == citation_hash).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_author_by_name(self, name: str) -> Optional[Author]:\n        return self.session.query(Author).filter(Author.has_name(name)).one_or_none()", "response": "Get an author by name if it exists in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlooks up an evidence by its hash.", "response": "def get_evidence_by_hash(self, evidence_hash: str) -> Optional[Evidence]:\n        \"\"\"Look up an evidence by its hash.\"\"\"\n        return self.session.query(Evidence).filter(Evidence.sha512 == evidence_hash).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a node predicate that is the inverse of the given node predicate.", "response": "def invert_node_predicate(node_predicate: NodePredicate) -> NodePredicate:  # noqa: D202\n    \"\"\"Build a node predicate that is the inverse of the given node predicate.\"\"\"\n\n    def inverse_predicate(graph: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Return the inverse of the enclosed node predicate applied to the graph and node.\"\"\"\n        return not node_predicate(graph, node)\n\n    return inverse_predicate"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconcatenate multiple node predicates to a new predicate that requires all predicates to be met.", "response": "def concatenate_node_predicates(node_predicates: NodePredicates) -> NodePredicate:\n    \"\"\"Concatenate multiple node predicates to a new predicate that requires all predicates to be met.\n\n    Example usage:\n\n    >>> from pybel.dsl import protein, gene\n    >>> from pybel.struct.filters.node_predicates import not_pathology, node_exclusion_predicate_builder\n    >>> app_protein = protein(name='APP', namespace='HGNC')\n    >>> app_gene = gene(name='APP', namespace='HGNC')\n    >>> app_predicate = node_exclusion_predicate_builder([app_protein, app_gene])\n    >>> my_predicate = concatenate_node_predicates([not_pathology, app_predicate])\n    \"\"\"\n    # If a predicate outside a list is given, just return it\n    if not isinstance(node_predicates, Iterable):\n        return node_predicates\n\n    node_predicates = tuple(node_predicates)\n\n    # If only one predicate is given, don't bother wrapping it\n    if 1 == len(node_predicates):\n        return node_predicates[0]\n\n    def concatenated_node_predicate(graph: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass only for a nodes that pass all enclosed predicates.\"\"\"\n        return all(\n            node_predicate(graph, node)\n            for node_predicate in node_predicates\n        )\n\n    return concatenated_node_predicate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_nodes(graph: BELGraph, node_predicates: NodePredicates) -> Iterable[BaseEntity]:\n    concatenated_predicate = concatenate_node_predicates(node_predicates=node_predicates)\n    for node in graph:\n        if concatenated_predicate(graph, node):\n            yield node", "response": "Apply a set of predicates to the nodes iterator of a BEL graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the set of all nodes that pass the predicates.", "response": "def get_nodes(graph: BELGraph, node_predicates: NodePredicates) -> Set[BaseEntity]:\n    \"\"\"Get the set of all nodes that pass the predicates.\"\"\"\n    return set(filter_nodes(graph, node_predicates=node_predicates))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count_passed_node_filter(graph: BELGraph, node_predicates: NodePredicates) -> int:\n    return sum(1 for _ in filter_nodes(graph, node_predicates=node_predicates))", "response": "Count how many nodes pass a given set of node predicates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all genes who have only one connection that s a transcription to its RNA.", "response": "def get_gene_leaves(graph) -> Iterable[BaseEntity]:\n    \"\"\"Iterate over all genes who have only one connection, that's a transcription to its RNA.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    for node in get_nodes_by_function(graph, GENE):\n        if graph.in_degree(node) != 0:\n            continue\n\n        if graph.out_degree(node) != 1:\n            continue\n\n        _, _, d = list(graph.out_edges(node, data=True))[0]\n\n        if d[RELATION] == TRANSCRIBED_TO:\n            yield node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_rna_leaves(graph) -> Iterable[BaseEntity]:\n    for node in get_nodes_by_function(graph, RNA):\n        if graph.in_degree(node) != 0:\n            continue\n\n        if graph.out_degree(node) != 1:\n            continue\n\n        _, _, d = list(graph.out_edges(node, data=True))[0]\n\n        if d[RELATION] == TRANSLATED_TO:\n            yield node", "response": "Iterate over all RNAs who have only one connection that s a translation to its protein."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_bel(self) -> str:\n        return \"{}({}:{})\".format(\n            self._func,\n            self.namespace,\n            ensure_quotes(self._priority_id)\n        )", "response": "Return this node as a BEL string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns this node as a BEL string.", "response": "def as_bel(self) -> str:\n        \"\"\"Return this node as a BEL string.\"\"\"\n        variants = self.get(VARIANTS)\n\n        if not variants:\n            return super(CentralDogma, self).as_bel()\n\n        variants_canon = sorted(map(str, variants))\n\n        return \"{}({}:{}, {})\".format(\n            self._func,\n            self.namespace,\n            ensure_quotes(self._priority_id),\n            ', '.join(variants_canon)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_parent(self) -> Optional['CentralDogma']:\n        if VARIANTS not in self:\n            return\n\n        return self.__class__(namespace=self.namespace, name=self.name, identifier=self.identifier)", "response": "Get the parent of this node or None if it s already a reference node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_variants(self, variants: Union['Variant', List['Variant']]) -> 'CentralDogma':\n        return self.__class__(\n            namespace=self.namespace,\n            name=self.name,\n            identifier=self.identifier,\n            variants=variants,\n        )", "response": "Create a new entity with the given variants."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_bel(self) -> str:\n        return 'pmod({}{})'.format(\n            str(self[IDENTIFIER]),\n            ''.join(', {}'.format(self[x]) for x in PMOD_ORDER[2:] if x in self)\n        )", "response": "Return this protein modification variant as a BEL string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef range(self) -> str:\n        if FRAGMENT_MISSING in self:\n            return '?'\n\n        return '{}_{}'.format(self[FRAGMENT_START], self[FRAGMENT_STOP])", "response": "Get the range of this fragment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning this fragment variant as a BEL string.", "response": "def as_bel(self) -> str:\n        \"\"\"Return this fragment variant as a BEL string.\"\"\"\n        res = '\"{}\"'.format(self.range)\n\n        if FRAGMENT_DESCRIPTION in self:\n            res += ', \"{}\"'.format(self[FRAGMENT_DESCRIPTION])\n\n        return 'frag({})'.format(res)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the corresponding gene object.", "response": "def get_gene(self) -> Gene:\n        \"\"\"Get the corresponding gene or raise an exception if it's not the reference node.\n\n        :raises: InferCentralDogmaException\n        \"\"\"\n        if self.variants:\n            raise InferCentralDogmaException('can not get gene for variant')\n\n        return Gene(\n            namespace=self.namespace,\n            name=self.name,\n            identifier=self.identifier\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_rna(self) -> Rna:\n        if self.variants:\n            raise InferCentralDogmaException('can not get rna for variant')\n\n        return Rna(\n            namespace=self.namespace,\n            name=self.name,\n            identifier=self.identifier\n        )", "response": "Get the corresponding RNA object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_bel(self) -> str:\n        return '{reference}.{start}_{stop}'.format(\n            reference=self[FUSION_REFERENCE],\n            start=self[FUSION_START],\n            stop=self[FUSION_STOP],\n        )", "response": "Return this fusion range as a BEL string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning this fusion as a BEL string.", "response": "def as_bel(self) -> str:\n        \"\"\"Return this fusion as a BEL string.\"\"\"\n        return '{}(fus({}:{}, \"{}\", {}:{}, \"{}\"))'.format(\n            self._func,\n            self.partner_5p.namespace,\n            self.partner_5p._priority_id,\n            self.range_5p.as_bel(),\n            self.partner_3p.namespace,\n            self.partner_3p._priority_id,\n            self.range_3p.as_bel(),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over the key - value pairs with duplicates for each annotation used in a BEL graph.", "response": "def iter_annotation_value_pairs(graph) -> Iterable[Tuple[str, str]]:\n    \"\"\"Iterate over the key/value pairs, with duplicates, for each annotation used in a BEL graph.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    return (\n        (key, value)\n        for _, _, data in graph.edges(data=True)\n        if ANNOTATIONS in data\n        for key, values in data[ANNOTATIONS].items()\n        for value in values\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all of the values for an annotation used in the graph.", "response": "def iter_annotation_values(graph, annotation: str) -> Iterable[str]:\n    \"\"\"Iterate over all of the values for an annotation used in the graph.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param str annotation: The annotation to grab\n    \"\"\"\n    return (\n        value\n        for _, _, data in graph.edges(data=True)\n        if edge_has_annotation(data, annotation)\n        for value in data[ANNOTATIONS][annotation]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _group_dict_set(iterator):\n    d = defaultdict(set)\n    for key, value in iterator:\n        d[key].add(value)\n    return dict(d)", "response": "Return a dict that accumulates the values for each key in an iterator of doubles."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all values for the given annotation.", "response": "def get_annotation_values(graph, annotation: str) -> Set[str]:\n    \"\"\"Get all values for the given annotation.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param annotation: The annotation to summarize\n    :return: A set of all annotation values\n    \"\"\"\n    return set(iter_annotation_values(graph, annotation))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a histogram over all relationships in a BEL graph.", "response": "def count_relations(graph) -> Counter:\n    \"\"\"Return a histogram over all relationships in a graph.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :return: A Counter from {relation type: frequency}\n    \"\"\"\n    return Counter(\n        data[RELATION]\n        for _, _, data in graph.edges(data=True)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate over the annotation keys.", "response": "def _annotation_iter_helper(graph) -> Iterable[str]:\n    \"\"\"Iterate over the annotation keys.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    return (\n        key\n        for _, _, data in graph.edges(data=True)\n        if ANNOTATIONS in data\n        for key in data[ANNOTATIONS]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all of the unused values for list annotations.", "response": "def get_unused_list_annotation_values(graph) -> Mapping[str, Set[str]]:\n    \"\"\"Get all of the unused values for list annotations.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :return: A dictionary of {str annotation: set of str values that aren't used}\n    \"\"\"\n    result = {}\n    for annotation, values in graph.annotation_list.items():\n        used_values = get_annotation_values(graph, annotation)\n        if len(used_values) == len(values):  # all values have been used\n            continue\n        result[annotation] = set(values) - used_values\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_lines(lines: Iterable[str], **kwargs) -> BELGraph:\n    graph = BELGraph()\n    parse_lines(graph=graph, lines=lines, **kwargs)\n    return graph", "response": "Load a BEL graph from an iterable over the lines of a BEL script."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a BEL graph from a file.", "response": "def from_path(path: str, encoding: str = 'utf-8', **kwargs) -> BELGraph:\n    \"\"\"Load a BEL graph from a file resource. This function is a thin wrapper around :func:`from_lines`.\n\n    :param path: A file path\n    :param encoding: the encoding to use when reading this file. Is passed to :code:`codecs.open`. See the python\n     `docs <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ for a list of standard encodings. For\n     example, files starting with a UTF-8 BOM should use :code:`utf_8_sig`.\n\n    The remaining keyword arguments are passed to :func:`pybel.io.line_utils.parse_lines`.\n    \"\"\"\n    log.info('Loading from path: %s', path)\n    graph = BELGraph(path=path)\n    with codecs.open(os.path.expanduser(path), encoding=encoding) as lines:\n        parse_lines(graph=graph, lines=lines, **kwargs)\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a BEL graph from a URL.", "response": "def from_url(url: str, **kwargs) -> BELGraph:\n    \"\"\"Load a BEL graph from a URL resource.\n\n    :param url: A valid URL pointing to a BEL document\n\n    The remaining keyword arguments are passed to :func:`pybel.io.line_utils.parse_lines`.\n    \"\"\"\n    log.info('Loading from url: %s', url)\n    res = download(url)\n    lines = (line.decode('utf-8') for line in res.iter_lines())\n\n    graph = BELGraph(path=url)\n    parse_lines(graph=graph, lines=lines, **kwargs)\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexpands around the predecessors of the given node in the result graph.", "response": "def expand_node_predecessors(universe, graph, node: BaseEntity) -> None:\n    \"\"\"Expand around the predecessors of the given node in the result graph.\n\n    :param pybel.BELGraph universe: The graph containing the stuff to add\n    :param pybel.BELGraph graph: The graph to add stuff to\n    :param node: A BEL node\n    \"\"\"\n    skip_successors = set()\n    for successor in universe.successors(node):\n        if successor in graph:\n            skip_successors.add(successor)\n            continue\n\n        graph.add_node(successor, **universe.nodes[successor])\n\n    graph.add_edges_from(\n        (source, successor, key, data)\n        for source, successor, key, data in universe.out_edges(node, data=True, keys=True)\n        if successor not in skip_successors\n    )\n\n    update_node_helper(universe, graph)\n    update_metadata(universe, graph)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expand_node_successors(universe, graph, node: BaseEntity) -> None:\n    skip_predecessors = set()\n    for predecessor in universe.predecessors(node):\n        if predecessor in graph:\n            skip_predecessors.add(predecessor)\n            continue\n\n        graph.add_node(predecessor, **universe.nodes[predecessor])\n\n    graph.add_edges_from(\n        (predecessor, target, key, data)\n        for predecessor, target, key, data in universe.in_edges(node, data=True, keys=True)\n        if predecessor not in skip_predecessors\n    )\n\n    update_node_helper(universe, graph)\n    update_metadata(universe, graph)", "response": "Expand around the successors of the given node in the result graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpanding around the neighborhoods of the given node in the result graph.", "response": "def expand_node_neighborhood(universe, graph, node: BaseEntity) -> None:\n    \"\"\"Expand around the neighborhoods of the given node in the result graph.\n\n    :param pybel.BELGraph universe: The graph containing the stuff to add\n    :param pybel.BELGraph graph: The graph to add stuff to\n    :param node: A BEL node\n    \"\"\"\n    expand_node_predecessors(universe, graph, node)\n    expand_node_successors(universe, graph, node)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expand_nodes_neighborhoods(universe, graph, nodes: Iterable[BaseEntity]) -> None:\n    for node in nodes:\n        expand_node_neighborhood(universe, graph, node)", "response": "Expand around the neighborhoods of the given nodes in the result graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expand_all_node_neighborhoods(universe, graph, filter_pathologies: bool = False) -> None:\n    for node in list(graph):\n        if filter_pathologies and is_pathology(node):\n            continue\n\n        expand_node_neighborhood(universe, graph, node)", "response": "Expand the neighborhoods of all nodes in the given graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a list of lines into a BEL graph.", "response": "def parse_lines(graph: BELGraph,\n                lines: Iterable[str],\n                manager: Optional[Manager] = None,\n                allow_nested: bool = False,\n                citation_clearing: bool = True,\n                use_tqdm: bool = False,\n                tqdm_kwargs: Optional[Mapping[str, Any]] = None,\n                no_identifier_validation: bool = False,\n                disallow_unqualified_translocations: bool = False,\n                allow_redefinition: bool = False,\n                allow_definition_failures: bool = False,\n                allow_naked_names: bool = False,\n                required_annotations: Optional[List[str]] = None,\n                ):\n    \"\"\"Parse an iterable of lines into this graph.\n\n    Delegates to :func:`parse_document`, :func:`parse_definitions`, and :func:`parse_statements`.\n\n    :param graph: A BEL graph\n    :param lines: An iterable over lines of BEL script\n    :param manager: A PyBEL database manager\n    :param allow_nested: If true, turns off nested statement failures\n    :param citation_clearing: Should :code:`SET Citation` statements clear evidence and all annotations?\n                                   Delegated to :class:`pybel.parser.ControlParser`\n    :param use_tqdm: Use :mod:`tqdm` to show a progress bar?\n    :param tqdm_kwargs: Keywords to pass to ``tqdm``\n    :param disallow_unqualified_translocations: If true, allow translocations without TO and FROM clauses.\n    :param required_annotations: Annotations that are required for all statements\n\n    .. warning::\n\n        These options allow concessions for parsing BEL that is either **WRONG** or **UNSCIENTIFIC**. Use them at\n        risk to reproducibility and validity of your results.\n\n    :param no_identifier_validation: If true, turns off namespace validation\n    :param allow_naked_names: If true, turns off naked namespace failures\n    :param allow_redefinition: If true, doesn't fail on second definition of same name or annotation\n    :param allow_definition_failures: If true, allows parsing to continue if a terminology file download/parse fails\n    \"\"\"\n    docs, definitions, statements = split_file_to_annotations_and_definitions(lines)\n\n    if manager is None:\n        manager = Manager()\n\n    metadata_parser = MetadataParser(\n        manager,\n        allow_redefinition=allow_redefinition,\n        skip_validation=no_identifier_validation,\n    )\n\n    parse_document(\n        graph,\n        docs,\n        metadata_parser,\n    )\n\n    parse_definitions(\n        graph,\n        definitions,\n        metadata_parser,\n        allow_failures=allow_definition_failures,\n        use_tqdm=use_tqdm,\n        tqdm_kwargs=tqdm_kwargs,\n    )\n\n    bel_parser = BELParser(\n        graph=graph,\n        # terminologies\n        namespace_to_term=metadata_parser.namespace_to_term,\n        namespace_to_pattern=metadata_parser.namespace_to_pattern,\n        annotation_to_term=metadata_parser.annotation_to_term,\n        annotation_to_pattern=metadata_parser.annotation_to_pattern,\n        annotation_to_local=metadata_parser.annotation_to_local,\n        # language settings\n        allow_nested=allow_nested,\n        citation_clearing=citation_clearing,\n        skip_validation=no_identifier_validation,\n        allow_naked_names=allow_naked_names,\n        disallow_unqualified_translocations=disallow_unqualified_translocations,\n        required_annotations=required_annotations,\n    )\n\n    parse_statements(\n        graph,\n        statements,\n        bel_parser,\n        use_tqdm=use_tqdm,\n        tqdm_kwargs=tqdm_kwargs,\n    )\n\n    log.info('Network has %d nodes and %d edges', graph.number_of_nodes(), graph.number_of_edges())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_document(graph: BELGraph,\n                   enumerated_lines: Iterable[Tuple[int, str]],\n                   metadata_parser: MetadataParser,\n                   ) -> None:\n    \"\"\"Parse the lines in the document section of a BEL script.\"\"\"\n    parse_document_start_time = time.time()\n\n    for line_number, line in enumerated_lines:\n        try:\n            metadata_parser.parseString(line, line_number=line_number)\n        except VersionFormatWarning as exc:\n            _log_parse_exception(graph, exc)\n            graph.add_warning(exc)\n        except Exception as e:\n            exc = MalformedMetadataException(line_number, line, 0)\n            _log_parse_exception(graph, exc)\n            raise exc from e\n\n    for required in REQUIRED_METADATA:\n        required_metadatum = metadata_parser.document_metadata.get(required)\n        if required_metadatum is not None:\n            continue\n\n        required_metadatum_key = INVERSE_DOCUMENT_KEYS[required]\n        # This has to be insert since it needs to go on the front!\n        exc = MissingMetadataException.make(required_metadatum_key)\n        graph.warnings.insert(0, (None, exc, {}))\n        _log_parse_exception(graph, exc)\n\n    graph.document.update(metadata_parser.document_metadata)\n\n    log.info('Finished parsing document section in %.02f seconds', time.time() - parse_document_start_time)", "response": "Parse the lines in the document section of a BEL script."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the lines in the definitions section of a BEL script.", "response": "def parse_definitions(graph: BELGraph,\n                      enumerated_lines: Iterable[Tuple[int, str]],\n                      metadata_parser: MetadataParser,\n                      allow_failures: bool = False,\n                      use_tqdm: bool = False,\n                      tqdm_kwargs: Optional[Mapping[str, Any]] = None,\n                      ) -> None:\n    \"\"\"Parse the lines in the definitions section of a BEL script.\n\n    :param graph: A BEL graph\n    :param enumerated_lines: An enumerated iterable over the lines in the definitions section of a BEL script\n    :param metadata_parser: A metadata parser\n    :param allow_failures: If true, allows parser to continue past strange failures\n    :param use_tqdm: Use :mod:`tqdm` to show a progress bar?\n    :param tqdm_kwargs: Keywords to pass to ``tqdm``\n    :raises: pybel.parser.parse_exceptions.InconsistentDefinitionError\n    :raises: pybel.resources.exc.ResourceError\n    :raises: sqlalchemy.exc.OperationalError\n    \"\"\"\n    parse_definitions_start_time = time.time()\n\n    if use_tqdm:\n        _tqdm_kwargs = dict(desc='Definitions', leave=False)\n        if tqdm_kwargs:\n            _tqdm_kwargs.update(tqdm_kwargs)\n        enumerated_lines = tqdm(list(enumerated_lines), **_tqdm_kwargs)\n\n    for line_number, line in enumerated_lines:\n        try:\n            metadata_parser.parseString(line, line_number=line_number)\n        except (InconsistentDefinitionError, ResourceError) as e:\n            parse_log.exception(LOG_FMT, line_number, 0, e.__class__.__name__, line)\n            raise e\n        except OperationalError as e:\n            parse_log.warning('Need to upgrade database. See '\n                              'http://pybel.readthedocs.io/en/latest/installation.html#upgrading')\n            raise e\n        except Exception as e:\n            if not allow_failures:\n                exc = MalformedMetadataException(line_number, line, 0)\n                _log_parse_exception(graph, exc)\n                raise exc from e\n\n    graph.namespace_url.update(metadata_parser.namespace_url_dict)\n    graph.namespace_pattern.update({\n        keyword: pattern.pattern\n        for keyword, pattern in metadata_parser.namespace_to_pattern.items()\n    })\n\n    graph.annotation_url.update(metadata_parser.annotation_url_dict)\n    graph.annotation_pattern.update({\n        keyword: pattern.pattern\n        for keyword, pattern in metadata_parser.annotation_to_pattern.items()\n    })\n    graph.annotation_list.update(metadata_parser.annotation_to_local)\n    graph.uncached_namespaces.update(metadata_parser.uncachable_namespaces)\n\n    log.info('Finished parsing definitions section in %.02f seconds', time.time() - parse_definitions_start_time)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a list of statements from a BEL Script.", "response": "def parse_statements(graph: BELGraph,\n                     enumerated_lines: Iterable[Tuple[int, str]],\n                     bel_parser: BELParser,\n                     use_tqdm: bool = False,\n                     tqdm_kwargs: Optional[Mapping[str, Any]] = None,\n                     ) -> None:\n    \"\"\"Parse a list of statements from a BEL Script.\n\n    :param graph: A BEL graph\n    :param enumerated_lines: An enumerated iterable over the lines in the statements section of a BEL script\n    :param bel_parser: A BEL parser\n    :param use_tqdm: Use :mod:`tqdm` to show a progress bar? Requires reading whole file to memory.\n    :param tqdm_kwargs: Keywords to pass to ``tqdm``\n    \"\"\"\n    parse_statements_start_time = time.time()\n\n    if use_tqdm:\n        _tqdm_kwargs = dict(desc='Statements')\n        if tqdm_kwargs:\n            _tqdm_kwargs.update(tqdm_kwargs)\n        enumerated_lines = tqdm(list(enumerated_lines), **_tqdm_kwargs)\n\n    for line_number, line in enumerated_lines:\n        try:\n            bel_parser.parseString(line, line_number=line_number)\n        except ParseException as e:\n            exc = BELSyntaxError(line_number, line, e.loc)\n            _log_parse_exception(graph, exc)\n            graph.add_warning(exc, bel_parser.get_annotations())\n        except PlaceholderAminoAcidWarning as exc:\n            exc.line_number = line_number\n            _log_parse_exception(graph, exc)\n            graph.add_warning(exc, bel_parser.get_annotations())\n        except BELParserWarning as exc:\n            _log_parse_exception(graph, exc)\n            graph.add_warning(exc, bel_parser.get_annotations())\n        except Exception:\n            parse_log.exception(LOG_FMT, line_number, 0, 'General Failure', line)\n            raise\n\n    log.info('Parsed statements section in %.02f seconds with %d warnings', time.time() - parse_statements_start_time,\n             len(graph.warnings))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring a graph in a database.", "response": "def to_database(graph, manager: Optional[Manager] = None, store_parts: bool = True, use_tqdm: bool = False):\n    \"\"\"Store a graph in a database.\n\n    :param BELGraph graph: A BEL graph\n    :param store_parts: Should the graph be stored in the edge store?\n    :return: If successful, returns the network object from the database.\n    :rtype: Optional[Network]\n    \"\"\"\n    if manager is None:\n        manager = Manager()\n\n    try:\n        return manager.insert_graph(graph, store_parts=store_parts, use_tqdm=use_tqdm)\n    except (IntegrityError, OperationalError):\n        manager.session.rollback()\n        log.exception('Error storing graph')\n    except Exception as e:\n        manager.session.rollback()\n        raise e"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_database(name: str, version: Optional[str] = None, manager: Optional[Manager] = None):\n    if manager is None:\n        manager = Manager()\n\n    if version is None:\n        return manager.get_graph_by_most_recent(name)\n\n    return manager.get_graph_by_name_version(name, version)", "response": "Load a BEL graph from a database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntagging a function that takes a dictionary to also accept a pair of BELGraph and node.", "response": "def node_predicate(f: Callable[[BaseEntity], bool]) -> NodePredicate:  # noqa: D202\n    \"\"\"Tag a node predicate that takes a dictionary to also accept a pair of (BELGraph, node).\n\n    Apply this as a decorator to a function that takes a single argument, a PyBEL node, to make\n    sure that it can also accept a pair of arguments, a BELGraph and a PyBEL node as well.\n    \"\"\"\n\n    @wraps(f)\n    def wrapped(*args):\n        x = args[0]\n\n        if isinstance(x, BELGraph):\n            return f(args[1], *args[2:])\n\n        # Assume:\n        # if isinstance(x, dict):\n        return f(*args)\n\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning true if the node has at least one of the given variant.", "response": "def _node_has_variant(node: BaseEntity, variant: str) -> bool:\n    \"\"\"Return true if the node has at least one of the given variant.\n\n    :param variant: :data:`PMOD`, :data:`HGVS`, :data:`GMOD`, or :data:`FRAGMENT`\n    \"\"\"\n    return VARIANTS in node and any(\n        variant_dict[KIND] == variant\n        for variant_dict in node[VARIANTS]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _node_has_modifier(graph: BELGraph, node: BaseEntity, modifier: str) -> bool:\n    modifier_in_subject = any(\n        part_has_modifier(d, SUBJECT, modifier)\n        for _, _, d in graph.out_edges(node, data=True)\n    )\n\n    modifier_in_object = any(\n        part_has_modifier(d, OBJECT, modifier)\n        for _, _, d in graph.in_edges(node, data=True)\n    )\n\n    return modifier_in_subject or modifier_in_object", "response": "Return true if over any of a nodes edges it has a given modifier."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_activity(graph: BELGraph, node: BaseEntity) -> bool:\n    return _node_has_modifier(graph, node, ACTIVITY)", "response": "Return true if the node has an activity."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns true if the node is degraded.", "response": "def is_degraded(graph: BELGraph, node: BaseEntity) -> bool:\n    \"\"\"Return true if over any of the node's edges, it is degraded.\"\"\"\n    return _node_has_modifier(graph, node, DEGRADATION)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn true if the node is translocated.", "response": "def is_translocated(graph: BELGraph, node: BaseEntity) -> bool:\n    \"\"\"Return true if over any of the node's edges, it is translocated.\"\"\"\n    return _node_has_modifier(graph, node, TRANSLOCATION)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_causal_in_edges(graph: BELGraph, node: BaseEntity) -> bool:\n    return any(\n        data[RELATION] in CAUSAL_RELATIONS\n        for _, _, data in graph.in_edges(node, data=True)\n    )", "response": "Return true if the node contains any in_edges that are causal."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true if the node contains any out_edges that are causal.", "response": "def has_causal_out_edges(graph: BELGraph, node: BaseEntity) -> bool:\n    \"\"\"Return true if the node contains any out_edges that are causal.\"\"\"\n    return any(\n        data[RELATION] in CAUSAL_RELATIONS\n        for _, _, data in graph.out_edges(node, data=True)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef node_exclusion_predicate_builder(nodes: Iterable[BaseEntity]) -> NodePredicate:\n    nodes = set(nodes)\n\n    @node_predicate\n    def node_exclusion_predicate(node: BaseEntity) -> bool:\n        \"\"\"Return true if the node is not in the given set of nodes.\"\"\"\n        return node not in nodes\n\n    return node_exclusion_predicate", "response": "Build a node predicate that returns false for the given nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a function that returns true for the given nodes.", "response": "def node_inclusion_predicate_builder(nodes: Iterable[BaseEntity]) -> NodePredicate:\n    \"\"\"Build a function that returns true for the given nodes.\"\"\"\n    nodes = set(nodes)\n\n    @node_predicate\n    def node_inclusion_predicate(node: BaseEntity) -> bool:\n        \"\"\"Return true if the node is in the given set of nodes.\"\"\"\n        return node in nodes\n\n    return node_inclusion_predicate"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_causal_source(graph: BELGraph, node: BaseEntity) -> bool:\n    # TODO reimplement to be faster\n    return not has_causal_in_edges(graph, node) and has_causal_out_edges(graph, node)", "response": "Return true of the node is a causal source."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns true if the node is a causal sink.", "response": "def is_causal_sink(graph: BELGraph, node: BaseEntity) -> bool:\n    \"\"\"Return true if the node is a causal sink.\n\n    - Does have causal in edge(s)\n    - Doesn't have any causal out edge(s)\n    \"\"\"\n    return has_causal_in_edges(graph, node) and not has_causal_out_edges(graph, node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn true if the node is a causal central.", "response": "def is_causal_central(graph: BELGraph, node: BaseEntity) -> bool:\n    \"\"\"Return true if the node is neither a causal sink nor a causal source.\n\n    - Does have causal in edges(s)\n    - Does have causal out edge(s)\n    \"\"\"\n    return has_causal_in_edges(graph, node) and has_causal_out_edges(graph, node)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn if the node is a list abundance but has no qualified edges.", "response": "def is_isolated_list_abundance(graph: BELGraph, node: BaseEntity, cls: Type[ListAbundance] = ListAbundance) -> bool:\n    \"\"\"Return if the node is a list abundance but has no qualified edges.\"\"\"\n    return (\n        isinstance(node, cls) and\n        0 == graph.in_degree(node) and\n        all(\n            data[RELATION] == HAS_COMPONENT\n            for _, __, data in graph.out_edges(node, data=True)\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollapsing a pair of nodes into a single node.", "response": "def collapse_pair(graph, survivor: BaseEntity, victim: BaseEntity) -> None:\n    \"\"\"Rewire all edges from the synonymous node to the survivor node, then deletes the synonymous node.\n\n    Does not keep edges between the two nodes.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param survivor: The BEL node to collapse all edges on the synonym to\n    :param victim: The BEL node to collapse into the surviving node\n    \"\"\"\n    graph.add_edges_from(\n        (survivor, successor, key, data)\n        for _, successor, key, data in graph.out_edges(victim, keys=True, data=True)\n        if successor != survivor\n    )\n\n    graph.add_edges_from(\n        (predecessor, survivor, key, data)\n        for predecessor, _, key, data in graph.in_edges(victim, keys=True, data=True)\n        if predecessor != survivor\n    )\n\n    graph.remove_node(victim)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collapse_nodes(graph, survivor_mapping: Mapping[BaseEntity, Set[BaseEntity]]) -> None:\n    inconsistencies = surviors_are_inconsistent(survivor_mapping)\n    if inconsistencies:\n        raise ValueError('survivor mapping is inconsistent: {}'.format(inconsistencies))\n\n    for survivor, victims in survivor_mapping.items():\n        for victim in victims:\n            collapse_pair(graph, survivor=survivor, victim=victim)\n\n    _remove_self_edges(graph)", "response": "Collapse all nodes in values to the key nodes in place."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef surviors_are_inconsistent(survivor_mapping: Mapping[BaseEntity, Set[BaseEntity]]) -> Set[BaseEntity]:\n    victim_mapping = set()\n    for victim in itt.chain.from_iterable(survivor_mapping.values()):\n        if victim in survivor_mapping:\n            victim_mapping.add(victim)\n    return victim_mapping", "response": "Check that there s no transitive shit going on."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collapse_all_variants(graph) -> None:\n    has_variant_predicate = build_relation_predicate(HAS_VARIANT)\n\n    edges = list(filter_edges(graph, has_variant_predicate))\n\n    for u, v, _ in edges:\n        collapse_pair(graph, survivor=u, victim=v)\n\n    _remove_self_edges(graph)", "response": "Collapse all variants in a BEL graph to their parents."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the namespace and annotation metadata in the target graph.", "response": "def update_metadata(source, target) -> None:\n    \"\"\"Update the namespace and annotation metadata in the target graph.\n\n    :param pybel.BELGraph source:\n    :param pybel.BELGraph target:\n    \"\"\"\n    target.namespace_url.update(source.namespace_url)\n    target.namespace_pattern.update(source.namespace_pattern)\n\n    target.annotation_url.update(source.annotation_url)\n    target.annotation_pattern.update(source.annotation_pattern)\n    for keyword, values in source.annotation_list.items():\n        if keyword not in target.annotation_list:\n            target.annotation_list[keyword] = values\n        else:\n            target.annotation_list[keyword].update(values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_node_helper(source: nx.Graph, target: nx.Graph) -> None:\n    for node in target:\n        if node in source:\n            target.nodes[node].update(source.nodes[node])", "response": "Update the nodes data dictionaries in the target graph from the source graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist the syntax errors encountered during compilation of a BEL script.", "response": "def get_syntax_errors(graph: BELGraph) -> List[WarningTuple]:\n    \"\"\"List the syntax errors encountered during compilation of a BEL script.\"\"\"\n    return [\n        (path, exc, an)\n        for path, exc, an in graph.warnings\n        if isinstance(exc, BELSyntaxError)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncount the occurrence of each error type in a graph.", "response": "def count_error_types(graph: BELGraph) -> Counter:\n    \"\"\"Count the occurrence of each type of error in a graph.\n\n    :return: A Counter of {error type: frequency}\n    \"\"\"\n    return Counter(\n        exc.__class__.__name__\n        for _, exc, _ in graph.warnings\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _naked_names_iter(graph: BELGraph) -> Iterable[str]:\n    for _, exc, _ in graph.warnings:\n        if isinstance(exc, NakedNameWarning):\n            yield exc.name", "response": "Iterate over naked names from a graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_incorrect_name_dict(graph: BELGraph) -> Mapping[str, List[str]]:\n    missing = defaultdict(list)\n\n    for namespace, name in _iterate_namespace_name(graph):\n        missing[namespace].append(name)\n\n    return dict(missing)", "response": "Get missing names grouped by namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_error_by_annotation(graph: BELGraph, annotation: str) -> Mapping[str, List[str]]:\n    results = defaultdict(list)\n\n    for _, exc, ctx in graph.warnings:\n        if not ctx or not edge_has_annotation(ctx, annotation):\n            continue\n\n        values = ctx[ANNOTATIONS][annotation]\n\n        if isinstance(values, str):\n            results[values].append(exc.__class__.__name__)\n\n        elif isinstance(values, Iterable):\n            for value in values:\n                results[value].append(exc.__class__.__name__)\n\n    return dict(results)", "response": "Group error names by a given annotation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninduces a sub - graph on all edges that pass the given filters.", "response": "def get_subgraph_by_edge_filter(graph, edge_predicates: Optional[EdgePredicates] = None):\n    \"\"\"Induce a sub-graph on all edges that pass the given filters.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param edge_predicates: An edge predicate or list of edge predicates\n    :return: A BEL sub-graph induced over the edges passing the given filters\n    :rtype: pybel.BELGraph\n    \"\"\"\n    rv = graph.fresh_copy()\n    expand_by_edge_filter(graph, rv, edge_predicates=edge_predicates)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_subgraph_by_induction(graph, nodes: Iterable[BaseEntity]):\n    nodes = tuple(nodes)\n\n    if all(node not in graph for node in nodes):\n        return\n\n    return subgraph(graph, nodes)", "response": "Induce a sub - graph over the given nodes or return None if none of the nodes are in the given graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_multi_causal_upstream(graph, nbunch: Union[BaseEntity, Iterable[BaseEntity]]):\n    result = get_upstream_causal_subgraph(graph, nbunch)\n    expand_upstream_causal(graph, result)\n    return result", "response": "Get the union of all the 2 - level deep causal upstream subgraphs from the nbunch."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_multi_causal_downstream(graph, nbunch: Union[BaseEntity, Iterable[BaseEntity]]):\n    result = get_downstream_causal_subgraph(graph, nbunch)\n    expand_downstream_causal(graph, result)\n    return result", "response": "Get the union of all of the 2 - level deep causal downstream subgraphs from the nbunch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a graph around the neighborhoods of the given nodes and expand to the neighborhood of those nodes.", "response": "def get_subgraph_by_second_neighbors(graph, nodes: Iterable[BaseEntity], filter_pathologies: bool = False):\n    \"\"\"Get a graph around the neighborhoods of the given nodes and expand to the neighborhood of those nodes.\n\n    Returns none if none of the nodes are in the graph.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param nodes: An iterable of BEL nodes\n    :param filter_pathologies: Should expansion take place around pathologies?\n    :return: A BEL graph induced around the neighborhoods of the given nodes\n    :rtype: Optional[pybel.BELGraph]\n    \"\"\"\n    result = get_subgraph_by_neighborhood(graph, nodes)\n\n    if result is None:\n        return\n\n    expand_all_node_neighborhoods(graph, result, filter_pathologies=filter_pathologies)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes a URL for the BEL graph.", "response": "def _normalize_url(graph: BELGraph, keyword: str) -> Optional[str]:  # FIXME move to utilities and unit test\n    \"\"\"Normalize a URL for the BEL graph.\"\"\"\n    if keyword == BEL_DEFAULT_NAMESPACE and BEL_DEFAULT_NAMESPACE not in graph.namespace_url:\n        return BEL_DEFAULT_NAMESPACE_URL\n\n    return graph.namespace_url.get(keyword)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndropping the namespace at the given URL.", "response": "def drop_namespace_by_url(self, url: str) -> None:\n        \"\"\"Drop the namespace at the given URL.\n\n        Won't work if the edge store is in use.\n\n        :param url: The URL of the namespace to drop\n        \"\"\"\n        namespace = self.get_namespace_by_url(url)\n        self.session.query(NamespaceEntry).filter(NamespaceEntry.namespace == namespace).delete()\n        self.session.delete(namespace)\n        self.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlook up a namespace by url.", "response": "def get_namespace_by_url(self, url: str) -> Optional[Namespace]:\n        \"\"\"Look up a namespace by url.\"\"\"\n        return self.session.query(Namespace).filter(Namespace.url == url).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_namespace_by_keyword_version(self, keyword: str, version: str) -> Optional[Namespace]:\n        filt = and_(Namespace.keyword == keyword, Namespace.version == version)\n        return self.session.query(Namespace).filter(filt).one_or_none()", "response": "Get a namespace with a given keyword and version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ensure_default_namespace(self) -> Namespace:\n        namespace = self.get_namespace_by_keyword_version(BEL_DEFAULT_NAMESPACE, BEL_DEFAULT_NAMESPACE_VERSION)\n\n        if namespace is None:\n            namespace = Namespace(\n                name='BEL Default Namespace',\n                contact='charles.hoyt@scai.fraunhofer.de',\n                keyword=BEL_DEFAULT_NAMESPACE,\n                version=BEL_DEFAULT_NAMESPACE_VERSION,\n                url=BEL_DEFAULT_NAMESPACE_URL,\n            )\n\n            for name in set(chain(pmod_mappings, gmod_mappings, activity_mapping, compartment_mapping)):\n                entry = NamespaceEntry(name=name, namespace=namespace)\n                self.session.add(entry)\n\n            self.session.add(namespace)\n            self.session.commit()\n\n        return namespace", "response": "Get or create the BEL default namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting or create a namespace from the given URL.", "response": "def get_or_create_namespace(self, url: str) -> Union[Namespace, Dict]:\n        \"\"\"Insert the namespace file at the given location to the cache.\n\n        If not cachable, returns the dict of the values of this namespace.\n\n        :raises: pybel.resources.exc.ResourceError\n        \"\"\"\n        result = self.get_namespace_by_url(url)\n\n        if result is not None:\n            return result\n\n        t = time.time()\n\n        bel_resource = get_bel_resource(url)\n\n        _clean_bel_namespace_values(bel_resource)\n\n        values = bel_resource['Values']\n\n        if not_resource_cachable(bel_resource):\n            log.debug('not caching namespace: %s (%d terms in %.2f seconds)', url, len(values), time.time() - t)\n            log.debug('loaded uncached namespace: %s (%d)', url, len(values))\n            return values\n\n        namespace_insert_values = _get_namespace_insert_values(bel_resource)\n\n        namespace = Namespace(\n            url=url,\n            **namespace_insert_values\n        )\n        namespace.entries = [\n            NamespaceEntry(name=name, encoding=encoding)\n            for name, encoding in values.items()\n        ]\n\n        log.info('inserted namespace: %s (%d terms in %.2f seconds)', url, len(values), time.time() - t)\n\n        self.session.add(namespace)\n        self.session.commit()\n\n        return namespace"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a namespace with a given keyword and pattern.", "response": "def get_namespace_by_keyword_pattern(self, keyword: str, pattern: str) -> Optional[Namespace]:\n        \"\"\"Get a namespace with a given keyword and pattern.\"\"\"\n        filt = and_(Namespace.keyword == keyword, Namespace.pattern == pattern)\n        return self.session.query(Namespace).filter(filt).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_regex_namespace(self, keyword: str, pattern: str) -> Namespace:\n        if pattern is None:\n            raise ValueError('cannot have null pattern')\n\n        namespace = self.get_namespace_by_keyword_pattern(keyword, pattern)\n\n        if namespace is None:\n            log.info('creating regex namespace: %s:%s', keyword, pattern)\n            namespace = Namespace(\n                keyword=keyword,\n                pattern=pattern\n            )\n            self.session.add(namespace)\n            self.session.commit()\n\n        return namespace", "response": "Get or create a regular expression namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_namespace_entry(self, url: str, name: str) -> Optional[NamespaceEntry]:\n        entry_filter = and_(Namespace.url == url, NamespaceEntry.name == name)\n        result = self.session.query(NamespaceEntry).join(Namespace).filter(entry_filter).all()\n\n        if 0 == len(result):\n            return\n\n        if 1 < len(result):\n            log.warning('result for get_namespace_entry is too long. Returning first of %s', [str(r) for r in result])\n\n        return result[0]", "response": "Get a given namespace entry object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_entity_by_identifier(self, url: str, identifier: str) -> Optional[NamespaceEntry]:\n        entry_filter = and_(Namespace.url == url, NamespaceEntry.identifier == identifier)\n        return self.session.query(NamespaceEntry).join(Namespace).filter(entry_filter).one_or_none()", "response": "Get a given entity by its url and identifier combination."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_or_create_regex_namespace_entry(self, namespace: str, pattern: str, name: str) -> NamespaceEntry:\n        namespace = self.ensure_regex_namespace(namespace, pattern)\n\n        n_filter = and_(Namespace.pattern == pattern, NamespaceEntry.name == name)\n\n        name_model = self.session.query(NamespaceEntry).join(Namespace).filter(n_filter).one_or_none()\n\n        if name_model is None:\n            name_model = NamespaceEntry(\n                namespace=namespace,\n                name=name\n            )\n            self.session.add(name_model)\n\n        return name_model", "response": "Get a namespace entry from a regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count_annotations(self) -> int:\n        return self.session.query(Namespace).filter(Namespace.is_annotation).count()", "response": "Count the number of annotations in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncount the number of annotation entries in the database.", "response": "def count_annotation_entries(self) -> int:\n        \"\"\"Count the number of annotation entries in the database.\"\"\"\n        return self.session.query(NamespaceEntry).filter(NamespaceEntry.is_annotation).count()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets or create a new annotation for the given URL.", "response": "def get_or_create_annotation(self, url: str) -> Namespace:\n        \"\"\"Insert the namespace file at the given location to the cache.\n\n        :raises: pybel.resources.exc.ResourceError\n        \"\"\"\n        result = self.get_namespace_by_url(url)\n\n        if result is not None:\n            return result\n\n        t = time.time()\n\n        bel_resource = get_bel_resource(url)\n\n        result = Namespace(\n            url=url,\n            is_annotation=True,\n            **_get_annotation_insert_values(bel_resource)\n        )\n        result.entries = [\n            NamespaceEntry(name=name, identifier=label)\n            for name, label in bel_resource['Values'].items()\n            if name\n        ]\n\n        self.session.add(result)\n        self.session.commit()\n\n        log.info('inserted annotation: %s (%d terms in %.2f seconds)', url, len(bel_resource['Values']),\n                 time.time() - t)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dict of annotations and their labels for the given annotation file.", "response": "def get_annotation_entry_names(self, url: str) -> Set[str]:\n        \"\"\"Return a dict of annotations and their labels for the given annotation file.\n\n        :param url: the location of the annotation file\n        \"\"\"\n        annotation = self.get_or_create_annotation(url)\n        return {\n            x.name\n            for x in self.session.query(NamespaceEntry.name).filter(NamespaceEntry.namespace == annotation)\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of annotation entries by the given names.", "response": "def get_annotation_entries_by_names(self, url: str, names: Iterable[str]) -> List[NamespaceEntry]:\n        \"\"\"Get annotation entries by URL and names.\n\n        :param url: The url of the annotation source\n        :param names: The names of the annotation entries from the given url's document\n        \"\"\"\n        annotation_filter = and_(Namespace.url == url, NamespaceEntry.name.in_(names))\n        return self.session.query(NamespaceEntry).join(Namespace).filter(annotation_filter).all()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting the most recently created version of each network.", "response": "def list_recent_networks(self) -> List[Network]:\n        \"\"\"List the most recently created version of each network (by name).\"\"\"\n        most_recent_times = (\n            self.session\n            .query(\n                Network.name.label('network_name'),\n                func.max(Network.created).label('max_created')\n            )\n            .group_by(Network.name)\n            .subquery('most_recent_times')\n        )\n\n        and_condition = and_(\n            most_recent_times.c.network_name == Network.name,\n            most_recent_times.c.max_created == Network.created\n        )\n\n        most_recent_networks = self.session.query(Network).join(most_recent_times, and_condition)\n\n        return most_recent_networks.all()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if there exists a network with the name and version combination in the database.", "response": "def has_name_version(self, name: str, version: str) -> bool:\n        \"\"\"Check if there exists a network with the name/version combination in the database.\"\"\"\n        return self.session.query(exists().where(and_(Network.name == name, Network.version == version))).scalar()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef drop_network_by_id(self, network_id: int) -> None:\n        network = self.session.query(Network).get(network_id)\n        self.drop_network(network)", "response": "Drop a network by its database identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef drop_network(self, network: Network) -> None:\n        # get the IDs of the edges that will be orphaned by deleting this network\n        # FIXME: this list could be a problem if it becomes very large; possible optimization is a temporary table in DB\n        edge_ids = [result.edge_id for result in self.query_singleton_edges_from_network(network)]\n\n        # delete the network-to-node mappings for this network\n        self.session.query(network_node).filter(network_node.c.network_id == network.id).delete(\n            synchronize_session=False)\n\n        # delete the edge-to-property mappings for the to-be-orphaned edges\n        self.session.query(edge_property).filter(edge_property.c.edge_id.in_(edge_ids)).delete(\n            synchronize_session=False)\n\n        # delete the edge-to-annotation mappings for the to-be-orphaned edges\n        self.session.query(edge_annotation).filter(edge_annotation.c.edge_id.in_(edge_ids)).delete(\n            synchronize_session=False)\n\n        # delete the edge-to-network mappings for this network\n        self.session.query(network_edge).filter(network_edge.c.network_id == network.id).delete(\n            synchronize_session=False)\n\n        # delete the now-orphaned edges\n        self.session.query(Edge).filter(Edge.id.in_(edge_ids)).delete(synchronize_session=False)\n\n        # delete the network\n        self.session.query(Network).filter(Network.id == network.id).delete(synchronize_session=False)\n\n        # commit it!\n        self.session.commit()", "response": "Drop a network while also cleaning up any edges that are no longer part of any network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query_singleton_edges_from_network(self, network: Network) -> sqlalchemy.orm.query.Query:\n        ne1 = aliased(network_edge, name='ne1')\n        ne2 = aliased(network_edge, name='ne2')\n        singleton_edge_ids_for_network = (\n            self.session\n                .query(ne1.c.edge_id)\n                .outerjoin(ne2, and_(\n                ne1.c.edge_id == ne2.c.edge_id,\n                ne1.c.network_id != ne2.c.network_id\n            ))\n                .filter(and_(\n                ne1.c.network_id == network.id,\n                ne2.c.edge_id == None  # noqa: E711\n            ))\n        )\n        return singleton_edge_ids_for_network", "response": "Return a query selecting all edge ids that only belong to the given network."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all of the versions of a network with the given name.", "response": "def get_network_versions(self, name: str) -> Set[str]:\n        \"\"\"Return all of the versions of a network with the given name.\"\"\"\n        return {\n            version\n            for version, in self.session.query(Network.version).filter(Network.name == name).all()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the network with the given name and version if it exists.", "response": "def get_network_by_name_version(self, name: str, version: str) -> Optional[Network]:\n        \"\"\"Load the network with the given name and version if it exists.\"\"\"\n        name_version_filter = and_(Network.name == name, Network.version == version)\n        network = self.session.query(Network).filter(name_version_filter).one_or_none()\n        return network"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the BEL graph with the given name or allows for specification of version.", "response": "def get_graph_by_name_version(self, name: str, version: str) -> Optional[BELGraph]:\n        \"\"\"Load the BEL graph with the given name, or allows for specification of version.\"\"\"\n        network = self.get_network_by_name_version(name, version)\n\n        if network is None:\n            return\n\n        return network.as_bel()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all networks with the given name. Useful for getting all versions of a given network.", "response": "def get_networks_by_name(self, name: str) -> List[Network]:\n        \"\"\"Get all networks with the given name. Useful for getting all versions of a given network.\"\"\"\n        return self.session.query(Network).filter(Network.name.like(name)).all()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_most_recent_network_by_name(self, name: str) -> Optional[Network]:\n        return self.session.query(Network).filter(Network.name == name).order_by(Network.created.desc()).first()", "response": "Get the most recently created network with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_graph_by_most_recent(self, name: str) -> Optional[BELGraph]:\n        network = self.get_most_recent_network_by_name(name)\n\n        if network is None:\n            return\n\n        return network.as_bel()", "response": "Get the most recently created network with the given name as a BELGraph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_network_by_id(self, network_id: int) -> Network:\n        return self.session.query(Network).get(network_id)", "response": "Get a network from the database by its identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a network from the database by its identifier and converts it to BEL graph.", "response": "def get_graph_by_id(self, network_id: int) -> BELGraph:\n        \"\"\"Get a network from the database by its identifier and converts it to a BEL graph.\"\"\"\n        network = self.get_network_by_id(network_id)\n        log.debug('converting network [id=%d] %s to bel graph', network_id, network)\n        return network.as_bel()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of networks with the given identifiers.", "response": "def get_networks_by_ids(self, network_ids: Iterable[int]) -> List[Network]:\n        \"\"\"Get a list of networks with the given identifiers.\n\n        Note: order is not necessarily preserved.\n        \"\"\"\n        log.debug('getting networks by identifiers: %s', network_ids)\n        return self.session.query(Network).filter(Network.id_in(network_ids)).all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_graphs_by_ids(self, network_ids: Iterable[int]) -> List[BELGraph]:\n        rv = [\n            self.get_graph_by_id(network_id)\n            for network_id in network_ids\n        ]\n        log.debug('returning graphs for network identifiers: %s', network_ids)\n        return rv", "response": "Get a list of graphs by the given identifiers and converts to BEL graphs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a BEL Graph from a list of network identifiers.", "response": "def get_graph_by_ids(self, network_ids: List[int]) -> BELGraph:\n        \"\"\"Get a combine BEL Graph from a list of network identifiers.\"\"\"\n        if len(network_ids) == 1:\n            return self.get_graph_by_id(network_ids[0])\n\n        log.debug('getting graph by identifiers: %s', network_ids)\n        graphs = self.get_graphs_by_ids(network_ids)\n\n        log.debug('getting union of graphs: %s', network_ids)\n        rv = union(graphs)\n\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_graph(self, graph: BELGraph, store_parts: bool = True, use_tqdm: bool = False) -> Network:\n        if not graph.name:\n            raise ValueError('Can not upload a graph without a name')\n\n        if not graph.version:\n            raise ValueError('Can not upload a graph without a version')\n\n        log.debug('inserting %s v%s', graph.name, graph.version)\n\n        t = time.time()\n\n        self.ensure_default_namespace()\n\n        namespace_urls = graph.namespace_url.values()\n        if use_tqdm:\n            namespace_urls = tqdm(namespace_urls, desc='namespaces')\n\n        for namespace_url in namespace_urls:\n            if namespace_url not in graph.uncached_namespaces:\n                self.get_or_create_namespace(namespace_url)\n\n        for keyword, pattern in graph.namespace_pattern.items():\n            self.ensure_regex_namespace(keyword, pattern)\n\n        annotation_urls = graph.annotation_url.values()\n        if use_tqdm:\n            annotation_urls = tqdm(annotation_urls, desc='annotations')\n\n        for annotation_url in annotation_urls:\n            self.get_or_create_annotation(annotation_url)\n\n        network = Network(**{\n            key: value\n            for key, value in graph.document.items()\n            if key in METADATA_INSERT_KEYS\n        })\n\n        network.store_bel(graph)\n\n        if store_parts:\n            network.nodes, network.edges = self._store_graph_parts(graph, use_tqdm=use_tqdm)\n\n        self.session.add(network)\n        self.session.commit()\n\n        log.info('inserted %s v%s in %.2f seconds', graph.name, graph.version, time.time() - t)\n\n        return network", "response": "Insert a BEL graph into the database and return the corresponding Network model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _store_graph_parts(self, graph: BELGraph, use_tqdm: bool = False) -> Tuple[List[Node], List[Edge]]:\n        log.debug('inserting %s into edge store', graph)\n        log.debug('building node models')\n        node_model_build_start = time.time()\n\n        nodes = list(graph)\n        if use_tqdm:\n            nodes = tqdm(nodes, total=graph.number_of_nodes(), desc='nodes')\n\n        node_model = {}\n        for node in nodes:\n            namespace = node.get(NAMESPACE)\n\n            if graph.skip_storing_namespace(namespace):\n                continue  # already know this node won't be cached\n\n            node_object = self.get_or_create_node(graph, node)\n\n            if node_object is None:\n                log.warning('can not add node %s', node)\n                continue\n\n            node_model[node] = node_object\n\n        node_models = list(node_model.values())\n        log.debug('built %d node models in %.2f seconds', len(node_models), time.time() - node_model_build_start)\n\n        node_model_commit_start = time.time()\n        self.session.add_all(node_models)\n        self.session.commit()\n        log.debug('stored %d node models in %.2f seconds', len(node_models), time.time() - node_model_commit_start)\n\n        log.debug('building edge models')\n        edge_model_build_start = time.time()\n\n        edges = graph.edges(keys=True, data=True)\n        if use_tqdm:\n            edges = tqdm(edges, total=graph.number_of_edges(), desc='edges')\n\n        edge_models = list(self._get_edge_models(graph, node_model, edges))\n\n        log.debug('built %d edge models in %.2f seconds', len(edge_models), time.time() - edge_model_build_start)\n\n        edge_model_commit_start = time.time()\n        self.session.add_all(edge_models)\n        self.session.commit()\n        log.debug('stored %d edge models in %.2f seconds', len(edge_models), time.time() - edge_model_commit_start)\n\n        return node_models, edge_models", "response": "Store the given graph into the edge store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over the key - value pairs in this edge data dictionary normalized to their source URLs.", "response": "def _iter_from_annotations_dict(graph: BELGraph,\n                                    annotations_dict: AnnotationsDict,\n                                    ) -> Iterable[Tuple[str, Set[str]]]:\n        \"\"\"Iterate over the key/value pairs in this edge data dictionary normalized to their source URLs.\"\"\"\n        for key, names in annotations_dict.items():\n            if key in graph.annotation_url:\n                url = graph.annotation_url[key]\n            elif key in graph.annotation_list:\n                continue  # skip those\n            elif key in graph.annotation_pattern:\n                log.debug('pattern annotation in database not implemented yet not implemented')  # FIXME\n                continue\n            else:\n                raise ValueError('Graph resources does not contain keyword: {}'.format(key))\n\n            yield url, set(names)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_annotation_entries_from_data(self, graph: BELGraph, data: EdgeData) -> Optional[List[NamespaceEntry]]:\n        annotations_dict = data.get(ANNOTATIONS)\n        if annotations_dict is not None:\n            return [\n                entry\n                for url, names in self._iter_from_annotations_dict(graph, annotations_dict=annotations_dict)\n                for entry in self.get_annotation_entries_by_names(url, names)\n            ]", "response": "Get the annotation entries from an edge data dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_qualified_edge(self, graph: BELGraph, source: Node, target: Node, key: str, bel: str, data: EdgeData):\n        citation_dict = data[CITATION]\n\n        citation = self.get_or_create_citation(\n            type=citation_dict.get(CITATION_TYPE),\n            reference=citation_dict.get(CITATION_REFERENCE),\n            name=citation_dict.get(CITATION_NAME),\n            title=citation_dict.get(CITATION_TITLE),\n            volume=citation_dict.get(CITATION_VOLUME),\n            issue=citation_dict.get(CITATION_ISSUE),\n            pages=citation_dict.get(CITATION_PAGES),\n            date=citation_dict.get(CITATION_DATE),\n            first=citation_dict.get(CITATION_FIRST_AUTHOR),\n            last=citation_dict.get(CITATION_LAST_AUTHOR),\n            authors=citation_dict.get(CITATION_AUTHORS),\n        )\n\n        evidence = self.get_or_create_evidence(citation, data[EVIDENCE])\n\n        properties = self.get_or_create_properties(graph, data)\n        if properties is None:\n            return\n\n        annotations = self._get_annotation_entries_from_data(graph, data)\n\n        return self.get_or_create_edge(\n            source=source,\n            target=target,\n            relation=data[RELATION],\n            bel=bel,\n            sha512=key,\n            data=data,\n            evidence=evidence,\n            properties=properties,\n            annotations=annotations,\n        )", "response": "Add a qualified edge to the network."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_unqualified_edge(self, source: Node, target: Node, key: str, bel: str, data: EdgeData) -> Edge:\n        return self.get_or_create_edge(\n            source=source,\n            target=target,\n            relation=data[RELATION],\n            bel=bel,\n            sha512=key,\n            data=data,\n        )", "response": "Add an unqualified edge to the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_or_create_evidence(self, citation: Citation, text: str) -> Evidence:\n        sha512 = hash_evidence(text=text, type=str(citation.type), reference=str(citation.reference))\n\n        if sha512 in self.object_cache_evidence:\n            evidence = self.object_cache_evidence[sha512]\n            self.session.add(evidence)\n            return evidence\n\n        evidence = self.get_evidence_by_hash(sha512)\n\n        if evidence is not None:\n            self.object_cache_evidence[sha512] = evidence\n            return evidence\n\n        evidence = Evidence(\n            text=text,\n            citation=citation,\n            sha512=sha512,\n        )\n\n        self.session.add(evidence)\n        self.object_cache_evidence[sha512] = evidence\n        return evidence", "response": "Create an entry and object for given evidence if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an entry and object for given node.", "response": "def get_or_create_node(self, graph: BELGraph, node: BaseEntity) -> Optional[Node]:\n        \"\"\"Create an entry and object for given node if it does not exist.\"\"\"\n        sha512 = node.as_sha512()\n        if sha512 in self.object_cache_node:\n            return self.object_cache_node[sha512]\n\n        node_model = self.get_node_by_hash(sha512)\n\n        if node_model is not None:\n            self.object_cache_node[sha512] = node_model\n            return node_model\n\n        node_model = Node._start_from_base_entity(node)\n\n        namespace = node.get(NAMESPACE)\n        if namespace is None:\n            pass\n\n        elif namespace in graph.namespace_url:\n            url = graph.namespace_url[namespace]\n            name = node[NAME]\n            entry = self.get_namespace_entry(url, name)\n\n            if entry is None:\n                log.debug('skipping node with identifier %s: %s', url, name)\n                return\n\n            self.session.add(entry)\n            node_model.namespace_entry = entry\n\n        elif namespace in graph.namespace_pattern:\n            name = node[NAME]\n            pattern = graph.namespace_pattern[namespace]\n            entry = self.get_or_create_regex_namespace_entry(namespace, pattern, name)\n\n            self.session.add(entry)\n            node_model.namespace_entry = entry\n\n        else:\n            log.warning(\"No reference in BELGraph for namespace: {}\".format(node[NAMESPACE]))\n            return\n\n        if VARIANTS in node or FUSION in node:\n            node_model.is_variant = True\n            node_model.has_fusion = FUSION in node\n\n            modifications = self.get_or_create_modification(graph, node)\n\n            if modifications is None:\n                log.warning('could not create %s because had an uncachable modification', node.as_bel())\n                return\n\n            node_model.modifications = modifications\n\n        self.session.add(node_model)\n        self.object_cache_node[sha512] = node_model\n        return node_model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_nodes(self) -> None:\n        t = time.time()\n\n        self.session.query(Node).delete()\n        self.session.commit()\n\n        log.info('dropped all nodes in %.2f seconds', time.time() - t)", "response": "Drop all nodes in the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drop_edges(self) -> None:\n        t = time.time()\n\n        self.session.query(Edge).delete()\n        self.session.commit()\n\n        log.info('dropped all edges in %.2f seconds', time.time() - t)", "response": "Drop all edges in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets or create an edge if it does not exist.", "response": "def get_or_create_edge(self,\n                           source: Node,\n                           target: Node,\n                           relation: str,\n                           bel: str,\n                           sha512: str,\n                           data: EdgeData,\n                           evidence: Optional[Evidence] = None,\n                           annotations: Optional[List[NamespaceEntry]] = None,\n                           properties: Optional[List[Property]] = None,\n                           ) -> Edge:\n        \"\"\"Create an edge if it does not exist, or return it if it does.\n\n        :param source: Source node of the relation\n        :param target: Target node of the relation\n        :param relation: Type of the relation between source and target node\n        :param bel: BEL statement that describes the relation\n        :param sha512: The SHA512 hash of the edge as a string\n        :param data: The PyBEL data dictionary\n        :param Evidence evidence: Evidence object that proves the given relation\n        :param properties: List of all properties that belong to the edge\n        :param annotations: List of all annotations that belong to the edge\n        \"\"\"\n        if sha512 in self.object_cache_edge:\n            edge = self.object_cache_edge[sha512]\n            self.session.add(edge)\n            return edge\n\n        edge = self.get_edge_by_hash(sha512)\n\n        if edge is not None:\n            self.object_cache_edge[sha512] = edge\n            return edge\n\n        edge = Edge(\n            source=source,\n            target=target,\n            relation=relation,\n            bel=bel,\n            sha512=sha512,\n            data=json.dumps(data),\n        )\n\n        if evidence is not None:\n            edge.evidence = evidence\n        if properties is not None:\n            edge.properties = properties\n        if annotations is not None:\n            edge.annotations = annotations\n\n        self.session.add(edge)\n        self.object_cache_edge[sha512] = edge\n        return edge"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_or_create_citation(self,\n                               reference: str,\n                               type: Optional[str] = None,\n                               name: Optional[str] = None,\n                               title: Optional[str] = None,\n                               volume: Optional[str] = None,\n                               issue: Optional[str] = None,\n                               pages: Optional[str] = None,\n                               date: Optional[str] = None,\n                               first: Optional[str] = None,\n                               last: Optional[str] = None,\n                               authors: Union[None, List[str]] = None,\n                               ) -> Citation:\n        \"\"\"Create a citation if it does not exist, or return it if it does.\n\n        :param type: Citation type (e.g. PubMed)\n        :param reference: Identifier of the given citation (e.g. PubMed id)\n        :param name: Name of the publication\n        :param title: Title of article\n        :param volume: Volume of publication\n        :param issue: Issue of publication\n        :param pages: Pages of issue\n        :param date: Date of publication in ISO 8601 (YYYY-MM-DD) format\n        :param first: Name of first author\n        :param last: Name of last author\n        :param authors: Either a list of authors separated by |, or an actual list of authors\n        \"\"\"\n        if type is None:\n            type = CITATION_TYPE_PUBMED\n\n        sha512 = hash_citation(type=type, reference=reference)\n\n        if sha512 in self.object_cache_citation:\n            citation = self.object_cache_citation[sha512]\n            self.session.add(citation)\n            return citation\n\n        citation = self.get_citation_by_hash(sha512)\n\n        if citation is not None:\n            self.object_cache_citation[sha512] = citation\n            return citation\n\n        citation = Citation(\n            type=type,\n            reference=reference,\n            sha512=sha512,\n            name=name,\n            title=title,\n            volume=volume,\n            issue=issue,\n            pages=pages\n        )\n        if date is not None:\n            citation.date = parse_datetime(date)\n\n        if first is not None:\n            citation.first = self.get_or_create_author(first)\n\n        if last is not None:\n            citation.last = self.get_or_create_author(last)\n\n        if authors is not None:\n            for author in authors:\n                author_model = self.get_or_create_author(author)\n                if author_model not in citation.authors:\n                    citation.authors.append(author_model)\n\n        self.session.add(citation)\n        self.object_cache_citation[sha512] = citation\n        return citation", "response": "Get or create a citation if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget an author by name or creates one if it does not exist.", "response": "def get_or_create_author(self, name: str) -> Author:\n        \"\"\"Get an author by name, or creates one if it does not exist.\"\"\"\n        author = self.object_cache_author.get(name)\n\n        if author is not None:\n            self.session.add(author)\n            return author\n\n        author = self.get_author_by_name(name)\n\n        if author is not None:\n            self.object_cache_author[name] = author\n            return author\n\n        author = self.object_cache_author[name] = Author.from_name(name=name)\n        self.session.add(author)\n        return author"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a modification by a SHA512 hash.", "response": "def get_modification_by_hash(self, sha512: str) -> Optional[Modification]:\n        \"\"\"Get a modification by a SHA512 hash.\"\"\"\n        return self.session.query(Modification).filter(Modification.sha512 == sha512).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_or_create_modification(self, graph: BELGraph, node: BaseEntity) -> Optional[List[Modification]]:\n        modification_list = []\n        if FUSION in node:\n            mod_type = FUSION\n            node = node[FUSION]\n            p3_namespace_url = graph.namespace_url[node[PARTNER_3P][NAMESPACE]]\n\n            if p3_namespace_url in graph.uncached_namespaces:\n                log.warning('uncached namespace %s in fusion()', p3_namespace_url)\n                return\n\n            p3_name = node[PARTNER_3P][NAME]\n            p3_namespace_entry = self.get_namespace_entry(p3_namespace_url, p3_name)\n\n            if p3_namespace_entry is None:\n                log.warning('Could not find namespace entry %s %s', p3_namespace_url, p3_name)\n                return  # FIXME raise?\n\n            p5_namespace_url = graph.namespace_url[node[PARTNER_5P][NAMESPACE]]\n\n            if p5_namespace_url in graph.uncached_namespaces:\n                log.warning('uncached namespace %s in fusion()', p5_namespace_url)\n                return\n\n            p5_name = node[PARTNER_5P][NAME]\n            p5_namespace_entry = self.get_namespace_entry(p5_namespace_url, p5_name)\n\n            if p5_namespace_entry is None:\n                log.warning('Could not find namespace entry %s %s', p5_namespace_url, p5_name)\n                return  # FIXME raise?\n\n            fusion_dict = {\n                'type': mod_type,\n                'p3_partner': p3_namespace_entry,\n                'p5_partner': p5_namespace_entry,\n            }\n\n            node_range_3p = node.get(RANGE_3P)\n            if node_range_3p and FUSION_REFERENCE in node_range_3p:\n                fusion_dict.update({\n                    'p3_reference': node_range_3p[FUSION_REFERENCE],\n                    'p3_start': node_range_3p[FUSION_START],\n                    'p3_stop': node_range_3p[FUSION_STOP],\n                })\n\n            node_range_5p = node.get(RANGE_5P)\n            if node_range_5p and FUSION_REFERENCE in node_range_5p:\n                fusion_dict.update({\n                    'p5_reference': node_range_5p[FUSION_REFERENCE],\n                    'p5_start': node_range_5p[FUSION_START],\n                    'p5_stop': node_range_5p[FUSION_STOP],\n                })\n\n            modification_list.append(fusion_dict)\n\n        else:\n            for variant in node[VARIANTS]:\n                mod_type = variant[KIND].strip()\n                if mod_type == HGVS:\n                    modification_list.append({\n                        'type': mod_type,\n                        'variantString': variant[IDENTIFIER]\n                    })\n\n                elif mod_type == FRAGMENT:\n                    if FRAGMENT_MISSING in variant:\n                        modification_list.append({\n                            'type': mod_type,\n                        })\n                    else:\n                        modification_list.append({\n                            'type': mod_type,\n                            'p3_start': variant[FRAGMENT_START],\n                            'p3_stop': variant[FRAGMENT_STOP]\n                        })\n\n                elif mod_type in {GMOD, PMOD}:\n                    variant_identifier = variant[IDENTIFIER]\n                    namespace_url = _normalize_url(graph, variant_identifier[NAMESPACE])\n\n                    if namespace_url in graph.uncached_namespaces:\n                        log.warning('uncached namespace %s in fusion()', namespace_url)\n                        return\n\n                    mod_entry = self.get_namespace_entry(namespace_url, variant_identifier[NAME])\n\n                    if mod_type == GMOD:\n                        modification_list.append({\n                            'type': mod_type,\n                            'identifier': mod_entry\n                        })\n                    if mod_type == PMOD:\n                        modification_list.append({\n                            'type': mod_type,\n                            'identifier': mod_entry,\n                            'residue': variant[PMOD_CODE].strip() if PMOD_CODE in variant else None,\n                            'position': variant[PMOD_POSITION] if PMOD_POSITION in variant else None\n                        })\n\n        modifications = []\n        for modification in modification_list:\n            mod_hash = hash_dump(modification)\n\n            mod = self.object_cache_modification.get(mod_hash)\n            if mod is None:\n                mod = self.get_modification_by_hash(mod_hash)\n                if not mod:\n                    modification['sha512'] = mod_hash\n                    mod = Modification(**modification)\n\n                self.object_cache_modification[mod_hash] = mod\n            modifications.append(mod)\n\n        return modifications", "response": "Get or create a list of modification objects that belong to the given node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_property_by_hash(self, property_hash: str) -> Optional[Property]:\n        return self.session.query(Property).filter(Property.sha512 == property_hash).one_or_none()", "response": "Get a property by its hash if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_property_from_dict(self, property_def: Dict) -> Property:\n        property_hash = hash_dump(property_def)\n\n        edge_property_model = self.object_cache_property.get(property_hash)\n        if edge_property_model is None:\n            edge_property_model = self.get_property_by_hash(property_hash)\n\n            if not edge_property_model:\n                property_def['sha512'] = property_hash\n                edge_property_model = Property(**property_def)\n\n            self.object_cache_property[property_hash] = edge_property_model\n\n        return edge_property_model", "response": "Build an edge property from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_or_create_properties(self, graph: BELGraph, edge_data: EdgeData) -> Optional[List[Property]]:\n        property_list = []\n        for participant in (SUBJECT, OBJECT):\n            participant_data = edge_data.get(participant)\n            if participant_data is None:\n                continue\n\n            location = participant_data.get(LOCATION)\n            if location is not None:\n                location_property_dict = {\n                    'is_subject': participant == SUBJECT,\n                    'modifier': LOCATION\n                }\n\n                location_namespace = location[NAMESPACE]\n\n                namespace_url = graph.namespace_url[location_namespace]\n\n                if namespace_url in graph.uncached_namespaces:\n                    log.warning('uncached namespace %s in loc() on line %s', location_namespace,\n                                edge_data.get(LINE))\n                    return\n\n                participant_name = location[NAME]\n                location_property_dict['effect'] = self.get_namespace_entry(namespace_url, participant_name)\n                if location_property_dict['effect'] is None:\n                    raise IndexError('did not get {}: {}'.format(namespace_url, participant_name))\n\n                property_list.append(location_property_dict)\n\n            modifier = participant_data.get(MODIFIER)\n            if modifier is not None:\n                modifier_property_dict = {\n                    'is_subject': participant == SUBJECT,\n                    'modifier': modifier\n                }\n\n                if modifier == TRANSLOCATION:\n                    for effect_type, effect_value in participant_data.get(EFFECT, {}).items():\n                        tmp_dict = deepcopy(modifier_property_dict)\n                        tmp_dict['relative_key'] = effect_type\n\n                        if NAMESPACE not in effect_value:\n                            tmp_dict['propValue'] = effect_value\n                            raise ValueError('shouldnt use propValue')\n                        else:\n                            effect_namespace = effect_value[NAMESPACE]\n\n                            if effect_namespace in graph.namespace_url:\n                                namespace_url = graph.namespace_url[effect_namespace]\n                            elif effect_namespace == BEL_DEFAULT_NAMESPACE:\n                                namespace_url = BEL_DEFAULT_NAMESPACE_URL\n                            else:\n                                log.warning('namespace not enumerated in modifier %s', effect_namespace)\n                                return\n\n                            if namespace_url in graph.uncached_namespaces:\n                                log.warning('uncached namespace %s in tloc() on line %s ', effect_namespace,\n                                            edge_data.get(LINE))\n                                return\n\n                            effect_name = effect_value[NAME]\n                            tmp_dict['effect'] = self.get_namespace_entry(namespace_url, effect_name)\n\n                            if tmp_dict['effect'] is None:\n                                log.warning('could not find tloc() %s %s', namespace_url, effect_name)\n                                return  # FIXME raise?\n\n                        property_list.append(tmp_dict)\n\n                elif modifier == ACTIVITY:\n                    effect = participant_data.get(EFFECT)\n                    if effect is not None:\n                        namespace_url = _normalize_url(graph, effect[NAMESPACE])\n\n                        if namespace_url in graph.uncached_namespaces:\n                            log.warning('uncached namespace %s in fusion()', namespace_url)\n                            return\n\n                        modifier_property_dict['effect'] = self.get_namespace_entry(namespace_url, effect[NAME])\n\n                    property_list.append(modifier_property_dict)\n\n                elif modifier == DEGRADATION:\n                    property_list.append(modifier_property_dict)\n\n                else:\n                    raise ValueError('unknown modifier: {}'.format(modifier))\n\n        return [\n            self._make_property_from_dict(property_def)\n            for property_def in property_list\n        ]", "response": "Get or create a list of all subject and object properties of the edge."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clean_annotations(annotations_dict: AnnotationsHint) -> AnnotationsDict:\n    return {\n        key: (\n            values if isinstance(values, dict) else\n            {v: True for v in values} if isinstance(values, set) else\n            {values: True}\n        )\n        for key, values in annotations_dict.items()\n    }", "response": "Fix the formatting of annotation dict."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef defined_namespace_keywords(self) -> Set[str]:  # noqa: D401\n        return set(self.namespace_pattern) | set(self.namespace_url)", "response": "The set of all keywords defined as namespaces in this graph."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef defined_annotation_keywords(self) -> Set[str]:\n        return (\n            set(self.annotation_pattern) |\n            set(self.annotation_url) |\n            set(self.annotation_list)\n        )", "response": "Get the set of all keywords defined as annotations in this graph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the namespace should be skipped.", "response": "def skip_storing_namespace(self, namespace: Optional[str]) -> bool:\n        \"\"\"Check if the namespace should be skipped.\n\n        :param namespace: The keyword of the namespace to check.\n        \"\"\"\n        return (\n            namespace is not None and\n            namespace in self.namespace_url and\n            self.namespace_url[namespace] in self.uncached_namespaces\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a warning to the internal warning log.", "response": "def add_warning(self,\n                    exception: BELParserWarning,\n                    context: Optional[Mapping[str, Any]] = None,\n                    ) -> None:\n        \"\"\"Add a warning to the internal warning log in the graph, with optional context information.\n\n        :param exception: The exception that occurred\n        :param context: The context from the parser when the exception occurred\n        \"\"\"\n        self.warnings.append((\n            self.path,\n            exception,\n            {} if context is None else context,\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhelp add a pre - built edge.", "response": "def _help_add_edge(self, u: BaseEntity, v: BaseEntity, attr: Mapping) -> str:\n        \"\"\"Help add a pre-built edge.\"\"\"\n        self.add_node_from_data(u)\n        self.add_node_from_data(v)\n\n        return self._help_add_edge_helper(u, v, attr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_unqualified_edge(self, u: BaseEntity, v: BaseEntity, relation: str) -> str:\n        attr = {RELATION: relation}\n        return self._help_add_edge(u, v, attr)", "response": "Add a unique edge that has no annotations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_transcription(self, gene: Gene, rna: Union[Rna, MicroRna]) -> str:\n        return self.add_unqualified_edge(gene, rna, TRANSCRIBED_TO)", "response": "Add a transcription relation from a gene to an RNA or miRNA node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a translation relation from a RNA to a protein.", "response": "def add_translation(self, rna: Rna, protein: Protein) -> str:\n        \"\"\"Add a translation relation from a RNA to a protein.\n\n        :param rna: An RNA node\n        :param protein: A protein node\n        \"\"\"\n        return self.add_unqualified_edge(rna, protein, TRANSLATED_TO)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_two_way_unqualified_edge(self, u: BaseEntity, v: BaseEntity, relation: str) -> str:\n        self.add_unqualified_edge(v, u, relation)\n        return self.add_unqualified_edge(u, v, relation)", "response": "Add an unqualified edge between two ways."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a qualified edge to the data model.", "response": "def add_qualified_edge(\n            self,\n            u,\n            v,\n            *,\n            relation: str,\n            evidence: str,\n            citation: Union[str, Mapping[str, str]],\n            annotations: Optional[AnnotationsHint] = None,\n            subject_modifier: Optional[Mapping] = None,\n            object_modifier: Optional[Mapping] = None,\n            **attr\n    ) -> str:\n        \"\"\"Add a qualified edge.\n\n        Qualified edges have a relation, evidence, citation, and optional annotations, subject modifications,\n        and object modifications.\n\n        :param u: The source node\n        :param v: The target node\n        :param relation: The type of relation this edge represents\n        :param evidence: The evidence string from an article\n        :param citation: The citation data dictionary for this evidence. If a string is given,\n         assumes it's a PubMed identifier and auto-fills the citation type.\n        :param annotations: The annotations data dictionary\n        :param subject_modifier: The modifiers (like activity) on the subject node. See data model documentation.\n        :param object_modifier: The modifiers (like activity) on the object node. See data model documentation.\n\n        :return: The hash of the edge\n        \"\"\"\n        attr.update({\n            RELATION: relation,\n            EVIDENCE: evidence,\n        })\n\n        if isinstance(citation, str):\n            attr[CITATION] = {\n                CITATION_TYPE: CITATION_TYPE_PUBMED,\n                CITATION_REFERENCE: citation,\n            }\n        elif isinstance(citation, dict):\n            attr[CITATION] = citation\n        else:\n            raise TypeError\n\n        if annotations:  # clean up annotations\n            attr[ANNOTATIONS] = _clean_annotations(annotations)\n\n        if subject_modifier:\n            attr[SUBJECT] = subject_modifier\n\n        if object_modifier:\n            attr[OBJECT] = object_modifier\n\n        return self._help_add_edge(u, v, attr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_node_from_data(self, node: BaseEntity) -> BaseEntity:\n        assert isinstance(node, BaseEntity)\n\n        if node in self:\n            return node\n\n        self.add_node(node)\n\n        if VARIANTS in node:\n            self.add_has_variant(node.get_parent(), node)\n\n        elif MEMBERS in node:\n            for member in node[MEMBERS]:\n                self.add_has_component(node, member)\n\n        elif PRODUCTS in node and REACTANTS in node:\n            for reactant_tokens in node[REACTANTS]:\n                self.add_has_reactant(node, reactant_tokens)\n            for product_tokens in node[PRODUCTS]:\n                self.add_has_product(node, product_tokens)\n\n        return node", "response": "Add an entity to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_edge_citation(self, u: BaseEntity, v: BaseEntity, key: str) -> bool:\n        return self._has_edge_attr(u, v, key, CITATION)", "response": "Check if the given edge has a citation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the given edge has an evidence.", "response": "def has_edge_evidence(self, u: BaseEntity, v: BaseEntity, key: str) -> bool:\n        \"\"\"Check if the given edge has an evidence.\"\"\"\n        return self._has_edge_attr(u, v, key, EVIDENCE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the citation for a given edge.", "response": "def get_edge_citation(self, u: BaseEntity, v: BaseEntity, key: str) -> Optional[CitationDict]:\n        \"\"\"Get the citation for a given edge.\"\"\"\n        return self._get_edge_attr(u, v, key, CITATION)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the evidence for a given edge.", "response": "def get_edge_evidence(self, u: BaseEntity, v: BaseEntity, key: str) -> Optional[str]:\n        \"\"\"Get the evidence for a given edge.\"\"\"\n        return self._get_edge_attr(u, v, key, EVIDENCE)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_edge_annotations(self, u, v, key: str) -> Optional[AnnotationsDict]:\n        return self._get_edge_attr(u, v, key, ANNOTATIONS)", "response": "Get the annotations for a given edge."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_node_description(self, node: BaseEntity) -> Optional[str]:\n        return self._get_node_attr(node, DESCRIPTION)", "response": "Get the description for a given node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_node_description(self, node: BaseEntity, description: str) -> None:\n        self._set_node_attr(node, DESCRIPTION, description)", "response": "Set the description for a given node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef edge_to_bel(u: BaseEntity, v: BaseEntity, edge_data: EdgeData, sep: Optional[str] = None) -> str:\n        return edge_to_bel(u, v, data=edge_data, sep=sep)", "response": "Serialize a pair of nodes and related edge data as a BEL relation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _equivalent_node_iterator_helper(self, node: BaseEntity, visited: Set[BaseEntity]) -> BaseEntity:\n        for v in self[node]:\n            if v in visited:\n                continue\n\n            if self._has_no_equivalent_edge(node, v):\n                continue\n\n            visited.add(v)\n            yield v\n            yield from self._equivalent_node_iterator_helper(v, visited)", "response": "Iterate over nodes and their data that are equal to the given node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_equivalent_nodes(self, node: BaseEntity) -> Iterable[BaseEntity]:\n        yield node\n        yield from self._equivalent_node_iterator_helper(node, {node})", "response": "Iterate over nodes that are equivalent to the given node including the original."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a set of equivalent nodes to this node excluding the given node.", "response": "def get_equivalent_nodes(self, node: BaseEntity) -> Set[BaseEntity]:\n        \"\"\"Get a set of equivalent nodes to this node, excluding the given node.\"\"\"\n        if isinstance(node, BaseEntity):\n            return set(self.iter_equivalent_nodes(node))\n\n        return set(self.iter_equivalent_nodes(node))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the node has namespace information.", "response": "def _node_has_namespace_helper(node: BaseEntity, namespace: str) -> bool:\n        \"\"\"Check that the node has namespace information.\n\n        Might have cross references in future.\n        \"\"\"\n        return namespace == node.get(NAMESPACE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef node_has_namespace(self, node: BaseEntity, namespace: str) -> bool:\n        return any(\n            self._node_has_namespace_helper(n, namespace)\n            for n in self.iter_equivalent_nodes(node)\n        )", "response": "Check if the node has the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _describe_list(self) -> List[Tuple[str, float]]:\n        number_nodes = self.number_of_nodes()\n        return [\n            ('Number of Nodes', number_nodes),\n            ('Number of Edges', self.number_of_edges()),\n            ('Number of Citations', self.number_of_citations()),\n            ('Number of Authors', self.number_of_authors()),\n            ('Network Density', '{:.2E}'.format(nx.density(self))),\n            ('Number of Components', nx.number_weakly_connected_components(self)),\n            ('Number of Warnings', self.number_of_warnings()),\n        ]", "response": "Return useful information about the graph as a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string that summarizes the graph.", "response": "def summary_str(self) -> str:\n        \"\"\"Return a string that summarizes the graph.\"\"\"\n        return '{}\\n'.format(self) + '\\n'.join(\n            '{}: {}'.format(label, value)\n            for label, value in self._describe_list()\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef summarize(self, file: Optional[TextIO] = None) -> None:\n        print(self.summary_str(), file=file)", "response": "Print a summary of the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize(self, *, fmt: str = 'nodelink', file: Union[None, str, TextIO] = None, **kwargs):\n        if file is None:\n            return self._serialize_object(fmt=fmt, **kwargs)\n        elif isinstance(file, str):\n            with open(file, 'w') as file_obj:\n                self._serialize_file(fmt=fmt, file=file_obj, **kwargs)\n        else:\n            self._serialize_file(fmt=fmt, file=file, **kwargs)", "response": "Serialize the object to a file or to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving isolated nodes from the network in place.", "response": "def remove_isolated_nodes(graph):\n    \"\"\"Remove isolated nodes from the network, in place.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    nodes = list(nx.isolates(graph))\n    graph.remove_nodes_from(nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new graph excluding the isolated nodes.", "response": "def remove_isolated_nodes_op(graph):\n    \"\"\"Build a new graph excluding the isolated nodes.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :rtype: pybel.BELGraph\n    \"\"\"\n    rv = graph.copy()\n    nodes = list(nx.isolates(rv))\n    rv.remove_nodes_from(nodes)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpands a target graph by edges in the source matching the given predicates.", "response": "def expand_by_edge_filter(source, target, edge_predicates: EdgePredicates):\n    \"\"\"Expand a target graph by edges in the source matching the given predicates.\n\n    :param pybel.BELGraph source: A BEL graph\n    :param pybel.BELGraph target: A BEL graph\n    :param edge_predicates: An edge predicate or list of edge predicates\n    :return: A BEL sub-graph induced over the edges passing the given filters\n    :rtype: pybel.BELGraph\n    \"\"\"\n    target.add_edges_from(\n        (u, v, k, source[u][v][k])\n        for u, v, k in filter_edges(source, edge_predicates=edge_predicates)\n    )\n\n    update_node_helper(source, target)\n    update_metadata(source, target)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_document(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        key = tokens['key']\n        value = tokens['value']\n\n        if key not in DOCUMENT_KEYS:\n            raise InvalidMetadataException(self.get_line_number(), line, position, key, value)\n\n        norm_key = DOCUMENT_KEYS[key]\n\n        if norm_key in self.document_metadata:\n            log.warning('Tried to overwrite metadata: %s', key)\n            return tokens\n\n        self.document_metadata[norm_key] = value\n\n        if norm_key == METADATA_VERSION:\n            self.raise_for_version(line, position, value)\n\n        return tokens", "response": "Handle statements like SET DOCUMENT X = Y."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nraises an exception if a namespace is already defined.", "response": "def raise_for_redefined_namespace(self, line: str, position: int, namespace: str) -> None:\n        \"\"\"Raise an exception if a namespace is already defined.\n\n        :raises: RedefinedNamespaceError\n        \"\"\"\n        if self.disallow_redefinition and self.has_namespace(namespace):\n            raise RedefinedNamespaceError(self.get_line_number(), line, position, namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle statements like DEFINE NAMESPACE X AS URL Y.", "response": "def handle_namespace_url(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle statements like ``DEFINE NAMESPACE X AS URL \"Y\"``.\n\n        :raises: RedefinedNamespaceError\n        :raises: pybel.resources.exc.ResourceError\n        \"\"\"\n        namespace = tokens['name']\n        self.raise_for_redefined_namespace(line, position, namespace)\n\n        url = tokens['url']\n        self.namespace_url_dict[namespace] = url\n\n        if self.skip_validation:\n            return tokens\n\n        namespace_result = self.manager.get_or_create_namespace(url)\n\n        if isinstance(namespace_result, dict):\n            self.namespace_to_term[namespace] = namespace_result\n            self.uncachable_namespaces.add(url)\n        else:\n            self.namespace_to_term[namespace] = self.manager.get_namespace_encoding(url)\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_namespace_pattern(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        namespace = tokens['name']\n        self.raise_for_redefined_namespace(line, position, namespace)\n        self.namespace_to_pattern[namespace] = re.compile(tokens['value'])\n        return tokens", "response": "Handle statements like DEFINE NAMESPACE X AS PATTERN Y."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises an exception if the given annotation is already defined.", "response": "def raise_for_redefined_annotation(self, line: str, position: int, annotation: str) -> None:\n        \"\"\"Raise an exception if the given annotation is already defined.\n\n        :raises: RedefinedAnnotationError\n        \"\"\"\n        if self.disallow_redefinition and self.has_annotation(annotation):\n            raise RedefinedAnnotationError(self.get_line_number(), line, position, annotation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles statements like DEFINE ANNOTATION X AS URL Y.", "response": "def handle_annotations_url(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle statements like ``DEFINE ANNOTATION X AS URL \"Y\"``.\n\n        :raises: RedefinedAnnotationError\n        \"\"\"\n        keyword = tokens['name']\n        self.raise_for_redefined_annotation(line, position, keyword)\n\n        url = tokens['url']\n        self.annotation_url_dict[keyword] = url\n\n        if self.skip_validation:\n            return tokens\n\n        self.annotation_to_term[keyword] = self.manager.get_annotation_entry_names(url)\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling statements like DEFINE ANNOTATION X AS LIST Y Z....", "response": "def handle_annotation_list(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle statements like ``DEFINE ANNOTATION X AS LIST {\"Y\",\"Z\", ...}``.\n\n        :raises: RedefinedAnnotationError\n        \"\"\"\n        annotation = tokens['name']\n        self.raise_for_redefined_annotation(line, position, annotation)\n        self.annotation_to_local[annotation] = set(tokens['values'])\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_annotation_pattern(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        annotation = tokens['name']\n        self.raise_for_redefined_annotation(line, position, annotation)\n        self.annotation_to_pattern[annotation] = re.compile(tokens['value'])\n        return tokens", "response": "Handle statements like DEFINE ANNOTATION X AS PATTERN Y."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if this annotation is defined.", "response": "def has_annotation(self, annotation: str) -> bool:\n        \"\"\"Check if this annotation is defined.\"\"\"\n        return (\n            self.has_enumerated_annotation(annotation) or\n            self.has_regex_annotation(annotation) or\n            self.has_local_annotation(annotation)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raise_for_version(self, line: str, position: int, version: str) -> None:\n        if valid_date_version(version):\n            return\n\n        if not SEMANTIC_VERSION_STRING_RE.match(version):\n            raise VersionFormatWarning(self.get_line_number(), line, position, version)", "response": "Raise an exception if the version string is not a valid BEL version string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a ChEBI abundance node.", "response": "def chebi(name=None, identifier=None) -> Abundance:\n    \"\"\"Build a ChEBI abundance node.\"\"\"\n    return Abundance(namespace='CHEBI', name=name, identifier=identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hgnc(name=None, identifier=None) -> Protein:\n    return Protein(namespace='HGNC', name=name, identifier=identifier)", "response": "Build an HGNC protein node."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds an engine and a SQLAlchemy session.", "response": "def build_engine_session(connection: str,\n                         echo: bool = False,\n                         autoflush: Optional[bool] = None,\n                         autocommit: Optional[bool] = None,\n                         expire_on_commit: Optional[bool] = None,\n                         scopefunc=None) -> Tuple:\n    \"\"\"Build an engine and a session.\n\n    :param connection: An RFC-1738 database connection string\n    :param echo: Turn on echoing SQL\n    :param autoflush: Defaults to True if not specified in kwargs or configuration.\n    :param autocommit: Defaults to False if not specified in kwargs or configuration.\n    :param expire_on_commit: Defaults to False if not specified in kwargs or configuration.\n    :param scopefunc: Scoped function to pass to :func:`sqlalchemy.orm.scoped_session`\n    :rtype: tuple[Engine,Session]\n\n    From the Flask-SQLAlchemy documentation:\n\n    An extra key ``'scopefunc'`` can be set on the ``options`` dict to\n    specify a custom scope function.  If it's not provided, Flask's app\n    context stack identity is used. This will ensure that sessions are\n    created and removed with the request/response cycle, and should be fine\n    in most cases.\n    \"\"\"\n    if connection is None:\n        raise ValueError('can not build engine when connection is None')\n\n    engine = create_engine(connection, echo=echo)\n\n    if autoflush is None:\n        autoflush = config.get('PYBEL_MANAGER_AUTOFLUSH', False)\n\n    if autocommit is None:\n        autocommit = config.get('PYBEL_MANAGER_AUTOCOMMIT', False)\n\n    if expire_on_commit is None:\n        expire_on_commit = config.get('PYBEL_MANAGER_AUTOEXPIRE', True)\n\n    log.debug('auto flush: %s, auto commit: %s, expire on commmit: %s', autoflush, autocommit, expire_on_commit)\n\n    #: A SQLAlchemy session maker\n    session_maker = sessionmaker(\n        bind=engine,\n        autoflush=autoflush,\n        autocommit=autocommit,\n        expire_on_commit=expire_on_commit,\n    )\n\n    #: A SQLAlchemy session object\n    session = scoped_session(\n        session_maker,\n        scopefunc=scopefunc,\n    )\n\n    return engine, session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the PyBEL cache s database and tables.", "response": "def create_all(self, checkfirst: bool = True) -> None:\n        \"\"\"Create the PyBEL cache's database and tables.\n\n        :param checkfirst: Check if the database exists before trying to re-make it\n        \"\"\"\n        self.base.metadata.create_all(bind=self.engine, checkfirst=checkfirst)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndropping all data tables and databases for the PyBEL cache.", "response": "def drop_all(self, checkfirst: bool = True) -> None:\n        \"\"\"Drop all data, tables, and databases for the PyBEL cache.\n\n        :param checkfirst: Check if the database exists before trying to drop it\n        \"\"\"\n        self.session.close()\n        self.base.metadata.drop_all(bind=self.engine, checkfirst=checkfirst)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbinds the metadata to the engine and session.", "response": "def bind(self) -> None:\n        \"\"\"Bind the metadata to the engine and session.\"\"\"\n        self.base.metadata.bind = self.engine\n        self.base.query = self.session.query_property()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting the models in this class.", "response": "def _list_model(self, model_cls: Type[X]) -> List[X]:\n        \"\"\"List the models in this class.\"\"\"\n        return self.session.query(model_cls).all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sanitize_date(publication_date: str) -> str:\n    if re1.search(publication_date):\n        return datetime.strptime(publication_date, '%Y %b %d').strftime('%Y-%m-%d')\n\n    if re2.search(publication_date):\n        return datetime.strptime(publication_date, '%Y %b').strftime('%Y-%m-01')\n\n    if re3.search(publication_date):\n        return publication_date + \"-01-01\"\n\n    if re4.search(publication_date):\n        return datetime.strptime(publication_date[:-4], '%Y %b').strftime('%Y-%m-01')\n\n    s = re5.search(publication_date)\n\n    if s:\n        year, season = s.groups()\n        return '{}-{}-01'.format(year, season_map[season])\n\n    s = re6.search(publication_date)\n\n    if s:\n        return datetime.strptime(publication_date, '%Y %b %d-{}'.format(s.groups()[0])).strftime('%Y-%m-%d')\n\n    s = re7.search(publication_date)\n\n    if s:\n        return datetime.strptime(publication_date, '%Y %b %d-{}'.format(s.groups()[0])).strftime('%Y-%m-%d')", "response": "Sanitize lots of different date strings into ISO - 8601."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_pubmed_identifiers(pmids: Iterable[str]) -> List[str]:\n    return sorted({str(pmid).strip() for pmid in pmids})", "response": "Clean a list of PubMed identifiers with string strips deduplicates and sorting."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the response from PubMed E - Utils for a given list of PubMed identifiers.", "response": "def get_pubmed_citation_response(pubmed_identifiers: Iterable[str]):\n    \"\"\"Get the response from PubMed E-Utils for a given list of PubMed identifiers.\n\n    :param pubmed_identifiers:\n    :rtype: dict\n    \"\"\"\n    pubmed_identifiers = list(pubmed_identifiers)\n    url = EUTILS_URL_FMT.format(','.join(\n        pubmed_identifier\n        for pubmed_identifier in pubmed_identifiers\n        if pubmed_identifier\n    ))\n    response = requests.get(url)\n    return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enrich_citation_model(manager, citation, p) -> bool:\n    if 'error' in p:\n        log.warning('Error downloading PubMed')\n        return False\n\n    citation.name = p['fulljournalname']\n    citation.title = p['title']\n    citation.volume = p['volume']\n    citation.issue = p['issue']\n    citation.pages = p['pages']\n    citation.first = manager.get_or_create_author(p['sortfirstauthor'])\n    citation.last = manager.get_or_create_author(p['lastauthor'])\n\n    if 'authors' in p:\n        for author in p['authors']:\n            author_model = manager.get_or_create_author(author['name'])\n            if author_model not in citation.authors:\n                citation.authors.append(author_model)\n\n    publication_date = p['pubdate']\n\n    sanitized_publication_date = sanitize_date(publication_date)\n    if sanitized_publication_date:\n        citation.date = datetime.strptime(sanitized_publication_date, '%Y-%m-%d')\n    else:\n        log.info('result had date with strange format: %s', publication_date)\n\n    return True", "response": "Enrich a citation model with the information from PubMed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a dictionary of citation information for a given list of PubMed identifiers.", "response": "def get_citations_by_pmids(manager,\n                           pmids: Iterable[Union[str, int]],\n                           group_size: Optional[int] = None,\n                           sleep_time: Optional[int] = None,\n                           ) -> Tuple[Dict[str, Dict], Set[str]]:\n    \"\"\"Get citation information for the given list of PubMed identifiers using the NCBI's eUtils service.\n\n    :type manager: pybel.Manager\n    :param pmids: an iterable of PubMed identifiers\n    :param group_size: The number of PubMed identifiers to query at a time. Defaults to 200 identifiers.\n    :param sleep_time: Number of seconds to sleep between queries. Defaults to 1 second.\n    :return: A dictionary of {pmid: pmid data dictionary} or a pair of this dictionary and a set ot erroneous\n            pmids if return_errors is :data:`True`\n    \"\"\"\n    group_size = group_size if group_size is not None else 200\n    sleep_time = sleep_time if sleep_time is not None else 1\n\n    pmids = clean_pubmed_identifiers(pmids)\n    log.info('Ensuring %d PubMed identifiers', len(pmids))\n\n    result = {}\n    unenriched_pmids = {}\n\n    for pmid in pmids:\n        citation = manager.get_or_create_citation(type=CITATION_TYPE_PUBMED, reference=pmid)\n\n        if not citation.date or not citation.name or not citation.authors:\n            unenriched_pmids[pmid] = citation\n            continue\n\n        result[pmid] = citation.to_json()\n\n    manager.session.commit()\n\n    log.debug('Found %d PubMed identifiers in database', len(pmids) - len(unenriched_pmids))\n\n    if not unenriched_pmids:\n        return result, set()\n\n    number_unenriched = len(unenriched_pmids)\n    log.info('Querying PubMed for %d identifiers', number_unenriched)\n\n    errors = set()\n    t = time.time()\n\n    for pmid_group_index, pmid_list in enumerate(grouper(group_size, unenriched_pmids), start=1):\n        pmid_list = list(pmid_list)\n        log.info('Getting group %d having %d PubMed identifiers', pmid_group_index, len(pmid_list))\n\n        response = get_pubmed_citation_response(pmid_list)\n        response_pmids = response['result']['uids']\n\n        for pmid in response_pmids:\n            p = response['result'][pmid]\n            citation = unenriched_pmids[pmid]\n\n            successful_enrichment = enrich_citation_model(manager, citation, p)\n\n            if not successful_enrichment:\n                log.warning(\"Error downloading PubMed identifier: %s\", pmid)\n                errors.add(pmid)\n                continue\n\n            result[pmid] = citation.to_json()\n            manager.session.add(citation)\n\n        manager.session.commit()  # commit in groups\n\n        # Don't want to hit that rate limit\n        time.sleep(sleep_time)\n\n    log.info('retrieved %d PubMed identifiers in %.02f seconds', len(unenriched_pmids), time.time() - t)\n\n    return result, errors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverwriting all PubMed citations with values from NCBI's eUtils lookup service. Sets authors as list, so probably a good idea to run :func:`pybel_tools.mutation.serialize_authors` before exporting. :type manager: pybel.manager.Manager :type graph: pybel.BELGraph :param group_size: The number of PubMed identifiers to query at a time. Defaults to 200 identifiers. :param sleep_time: Number of seconds to sleep between queries. Defaults to 1 second. :return: A set of PMIDs for which the eUtils service crashed", "response": "def enrich_pubmed_citations(manager,\n                            graph,\n                            group_size: Optional[int] = None,\n                            sleep_time: Optional[int] = None,\n                            ) -> Set[str]:\n    \"\"\"Overwrite all PubMed citations with values from NCBI's eUtils lookup service.\n\n    Sets authors as list, so probably a good idea to run :func:`pybel_tools.mutation.serialize_authors` before\n    exporting.\n\n    :type manager: pybel.manager.Manager\n    :type graph: pybel.BELGraph\n    :param group_size: The number of PubMed identifiers to query at a time. Defaults to 200 identifiers.\n    :param sleep_time: Number of seconds to sleep between queries. Defaults to 1 second.\n    :return: A set of PMIDs for which the eUtils service crashed\n    \"\"\"\n    pmids = get_pubmed_identifiers(graph)\n    pmid_data, errors = get_citations_by_pmids(manager, pmids=pmids, group_size=group_size, sleep_time=sleep_time)\n\n    for u, v, k in filter_edges(graph, has_pubmed):\n        pmid = graph[u][v][k][CITATION][CITATION_REFERENCE].strip()\n\n        if pmid not in pmid_data:\n            log.warning('Missing data for PubMed identifier: %s', pmid)\n            errors.add(pmid)\n            continue\n\n        graph[u][v][k][CITATION].update(pmid_data[pmid])\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_collapse_to_gene_dict(graph) -> Dict[BaseEntity, Set[BaseEntity]]:\n    collapse_dict = defaultdict(set)\n    r2g = {}\n\n    for gene_node, rna_node, d in graph.edges(data=True):\n        if d[RELATION] != TRANSCRIBED_TO:\n            continue\n\n        collapse_dict[gene_node].add(rna_node)\n        r2g[rna_node] = gene_node\n\n    for rna_node, protein_node, d in graph.edges(data=True):\n        if d[RELATION] != TRANSLATED_TO:\n            continue\n\n        if rna_node not in r2g:\n            raise ValueError('Should complete origin before running this function')\n\n        collapse_dict[r2g[rna_node]].add(protein_node)\n\n    return collapse_dict", "response": "Builds a collapse dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collapse_to_genes(graph):\n    enrich_protein_and_rna_origins(graph)\n    collapse_dict = _build_collapse_to_gene_dict(graph)\n    collapse_nodes(graph, collapse_dict)", "response": "Collapse all protein RNA miRNA nodes to their corresponding gene nodes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncommanding line interface for PyBEL.", "response": "def main(ctx, connection):\n    \"\"\"Command line interface for PyBEL.\"\"\"\n    ctx.obj = Manager(connection=connection)\n    ctx.obj.bind()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompiles a BEL script to a graph.", "response": "def compile(manager, path, allow_naked_names, allow_nested, disallow_unqualified_translocations,\n            no_identifier_validation, no_citation_clearing, required_annotations, skip_tqdm, verbose):\n    \"\"\"Compile a BEL script to a graph.\"\"\"\n    if verbose:\n        logging.basicConfig(level=logging.DEBUG)\n        log.setLevel(logging.DEBUG)\n        log.debug('using connection: %s', manager.engine.url)\n\n    click.secho('Compilation', fg='red', bold=True)\n    if skip_tqdm:\n        click.echo('```')\n    graph = from_path(\n        path,\n        manager=manager,\n        use_tqdm=(not (skip_tqdm or verbose)),\n        allow_nested=allow_nested,\n        allow_naked_names=allow_naked_names,\n        disallow_unqualified_translocations=disallow_unqualified_translocations,\n        citation_clearing=(not no_citation_clearing),\n        required_annotations=required_annotations,\n        no_identifier_validation=no_identifier_validation,\n        allow_definition_failures=True,\n    )\n    if skip_tqdm:\n        click.echo('```')\n    to_pickle(graph, get_corresponding_pickle_path(path))\n\n    click.echo('')\n    _print_summary(graph, ticks=skip_tqdm)\n\n    sys.exit(0 if 0 == graph.number_of_warnings() else 1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninserting a graph to the database.", "response": "def insert(manager, graph: BELGraph):\n    \"\"\"Insert a graph to the database.\"\"\"\n    to_database(graph, manager=manager, use_tqdm=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post(graph: BELGraph, host: str):\n    resp = to_web(graph, host=host)\n    resp.raise_for_status()", "response": "Upload a graph to BEL Commons."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize a graph to various formats.", "response": "def serialize(graph: BELGraph, csv, sif, gsea, graphml, json, bel):\n    \"\"\"Serialize a graph to various formats.\"\"\"\n    if csv:\n        log.info('Outputting CSV to %s', csv)\n        to_csv(graph, csv)\n\n    if sif:\n        log.info('Outputting SIF to %s', sif)\n        to_sif(graph, sif)\n\n    if graphml:\n        log.info('Outputting GraphML to %s', graphml)\n        to_graphml(graph, graphml)\n\n    if gsea:\n        log.info('Outputting GRP to %s', gsea)\n        to_gsea(graph, gsea)\n\n    if json:\n        log.info('Outputting JSON to %s', json)\n        to_json_file(graph, json)\n\n    if bel:\n        log.info('Outputting BEL to %s', bel)\n        to_bel(graph, bel)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting content from the INDRA machine and upload it to BEL Commons.", "response": "def machine(manager: Manager, agents: List[str], local: bool, host: str):\n    \"\"\"Get content from the INDRA machine and upload to BEL Commons.\"\"\"\n    from indra.sources import indra_db_rest\n    from pybel import from_indra_statements\n\n    statements = indra_db_rest.get_statements(agents=agents)\n    click.echo('got {} statements from INDRA'.format(len(statements)))\n\n    graph = from_indra_statements(\n        statements,\n        name='INDRA Machine for {}'.format(', '.join(sorted(agents))),\n        version=time.strftime('%Y%m%d'),\n    )\n    click.echo('built BEL graph with {} nodes and {} edges'.format(graph.number_of_nodes(), graph.number_of_edges()))\n\n    if 0 == len(graph):\n        click.echo('not uploading empty graph')\n        sys.exit(-1)\n\n    if local:\n        to_database(graph, manager=manager)\n    else:\n        resp = to_web(graph, host=host)\n        resp.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading examples to the database.", "response": "def examples(manager: Manager):\n    \"\"\"Load examples to the database.\"\"\"\n    for graph in (sialic_acid_graph, statin_graph, homology_graph, braf_graph, egf_graph):\n        if manager.has_name_version(graph.name, graph.version):\n            click.echo('already inserted {}'.format(graph))\n            continue\n        click.echo('inserting {}'.format(graph))\n        manager.insert_graph(graph, use_tqdm=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ls(manager: Manager):\n    for n in manager.list_networks():\n        click.echo('{}\\t{}\\t{}'.format(n.id, n.name, n.version))", "response": "List network names versions and optionally descriptions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndropping a network by its identifier or drop all networks.", "response": "def drop(manager: Manager, network_id: Optional[int], yes):\n    \"\"\"Drop a network by its identifier or drop all networks.\"\"\"\n    if network_id:\n        manager.drop_network_by_id(network_id)\n\n    elif yes or click.confirm('Drop all networks?'):\n        manager.drop_networks()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ls(manager: Manager, offset: Optional[int], limit: Optional[int]):\n    q = manager.session.query(Edge)\n\n    if offset:\n        q = q.offset(offset)\n\n    if limit > 0:\n        q = q.limit(limit)\n\n    for e in q:\n        click.echo(e.bel)", "response": "List all the available resources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprune nodes not belonging to any edges.", "response": "def prune(manager: Manager):\n    \"\"\"Prune nodes not belonging to any edges.\"\"\"\n    nodes_to_delete = [\n        node\n        for node in tqdm(manager.session.query(Node), total=manager.count_nodes())\n        if not node.networks\n    ]\n    manager.session.delete(nodes_to_delete)\n    manager.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsummarize the contents of the database.", "response": "def summarize(manager: Manager):\n    \"\"\"Summarize the contents of the database.\"\"\"\n    click.echo('Networks: {}'.format(manager.count_networks()))\n    click.echo('Edges: {}'.format(manager.count_edges()))\n    click.echo('Nodes: {}'.format(manager.count_nodes()))\n    click.echo('Namespaces: {}'.format(manager.count_namespaces()))\n    click.echo('Namespaces entries: {}'.format(manager.count_namespace_entries()))\n    click.echo('Annotations: {}'.format(manager.count_annotations()))\n    click.echo('Annotation entries: {}'.format(manager.count_annotation_entries()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noutputting the warnings from a BEL graph with Click and the system s pager.", "response": "def echo_warnings_via_pager(warnings: List[WarningTuple], sep: str = '\\t') -> None:\n    \"\"\"Output the warnings from a BEL graph with Click and the system's pager.\"\"\"\n    # Exit if no warnings\n    if not warnings:\n        click.echo('Congratulations! No warnings.')\n        sys.exit(0)\n\n    max_line_width = max(\n        len(str(exc.line_number))\n        for _, exc, _ in warnings\n    )\n\n    max_warning_width = max(\n        len(exc.__class__.__name__)\n        for _, exc, _ in warnings\n    )\n\n    s1 = '{:>' + str(max_line_width) + '}' + sep\n    s2 = '{:>' + str(max_warning_width) + '}' + sep\n\n    def _make_line(path: str, exc: BELParserWarning):\n        s = click.style(path, fg='cyan') + sep\n        s += click.style(s1.format(exc.line_number), fg='blue', bold=True)\n        s += click.style(s2.format(exc.__class__.__name__),\n                         fg=('red' if exc.__class__.__name__.endswith('Error') else 'yellow'))\n        s += click.style(exc.line, bold=True) + sep\n        s += click.style(str(exc))\n        return s\n\n    click.echo_via_pager('\\n'.join(\n        _make_line(path, exc)\n        for path, exc, _ in warnings\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a ParseResult to a PyBEL DSL object.", "response": "def parse_result_to_dsl(tokens):\n    \"\"\"Convert a ParseResult to a PyBEL DSL object.\n\n    :type tokens: dict or pyparsing.ParseResults\n    :rtype: BaseEntity\n    \"\"\"\n    if MODIFIER in tokens:\n        return parse_result_to_dsl(tokens[TARGET])\n\n    elif REACTION == tokens[FUNCTION]:\n        return _reaction_po_to_dict(tokens)\n\n    elif VARIANTS in tokens:\n        return _variant_po_to_dict(tokens)\n\n    elif MEMBERS in tokens:\n        return _list_po_to_dict(tokens)\n\n    elif FUSION in tokens:\n        return _fusion_to_dsl(tokens)\n\n    return _simple_po_to_dict(tokens)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a PyParsing data dictionary to a PyBEL fusion data dictionary.", "response": "def _fusion_to_dsl(tokens) -> FusionBase:\n    \"\"\"Convert a PyParsing data dictionary to a PyBEL fusion data dictionary.\n\n    :param tokens: A PyParsing data dictionary representing a fusion\n    :type tokens: ParseResult\n    \"\"\"\n    func = tokens[FUNCTION]\n    fusion_dsl = FUNC_TO_FUSION_DSL[func]\n    member_dsl = FUNC_TO_DSL[func]\n\n    partner_5p = member_dsl(\n        namespace=tokens[FUSION][PARTNER_5P][NAMESPACE],\n        name=tokens[FUSION][PARTNER_5P][NAME]\n    )\n\n    partner_3p = member_dsl(\n        namespace=tokens[FUSION][PARTNER_3P][NAMESPACE],\n        name=tokens[FUSION][PARTNER_3P][NAME]\n    )\n\n    range_5p = _fusion_range_to_dsl(tokens[FUSION][RANGE_5P])\n    range_3p = _fusion_range_to_dsl(tokens[FUSION][RANGE_3P])\n\n    return fusion_dsl(\n        partner_5p=partner_5p,\n        partner_3p=partner_3p,\n        range_5p=range_5p,\n        range_3p=range_3p,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fusion_range_to_dsl(tokens) -> FusionRangeBase:\n    if FUSION_MISSING in tokens:\n        return missing_fusion_range()\n\n    return fusion_range(\n        reference=tokens[FUSION_REFERENCE],\n        start=tokens[FUSION_START],\n        stop=tokens[FUSION_STOP]\n    )", "response": "Convert a PyParsing data dictionary into a PyBEL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a simple named entity to a DSL object.", "response": "def _simple_po_to_dict(tokens) -> BaseAbundance:\n    \"\"\"Convert a simple named entity to a DSL object.\n\n    :type tokens: ParseResult\n    \"\"\"\n    dsl = FUNC_TO_DSL.get(tokens[FUNCTION])\n    if dsl is None:\n        raise ValueError('invalid tokens: {}'.format(tokens))\n\n    return dsl(\n        namespace=tokens[NAMESPACE],\n        name=tokens.get(NAME),\n        identifier=tokens.get(IDENTIFIER),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a PyParsing data dictionary to a central dogma abundance.", "response": "def _variant_po_to_dict(tokens) -> CentralDogma:\n    \"\"\"Convert a PyParsing data dictionary to a central dogma abundance (i.e., Protein, RNA, miRNA, Gene).\n\n    :type tokens: ParseResult\n    \"\"\"\n    dsl = FUNC_TO_DSL.get(tokens[FUNCTION])\n    if dsl is None:\n        raise ValueError('invalid tokens: {}'.format(tokens))\n\n    return dsl(\n        namespace=tokens[NAMESPACE],\n        name=tokens[NAME],\n        variants=[\n            _variant_to_dsl_helper(variant_tokens)\n            for variant_tokens in tokens[VARIANTS]\n        ],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _variant_to_dsl_helper(tokens) -> Variant:\n    kind = tokens[KIND]\n\n    if kind == HGVS:\n        return hgvs(tokens[IDENTIFIER])\n\n    if kind == GMOD:\n        return gmod(\n            name=tokens[IDENTIFIER][NAME],\n            namespace=tokens[IDENTIFIER][NAMESPACE],\n        )\n\n    if kind == PMOD:\n        return pmod(\n            name=tokens[IDENTIFIER][NAME],\n            namespace=tokens[IDENTIFIER][NAMESPACE],\n            code=tokens.get(PMOD_CODE),\n            position=tokens.get(PMOD_POSITION),\n        )\n\n    if kind == FRAGMENT:\n        start, stop = tokens.get(FRAGMENT_START), tokens.get(FRAGMENT_STOP)\n        return fragment(\n            start=start,\n            stop=stop,\n            description=tokens.get(FRAGMENT_DESCRIPTION)\n        )\n\n    raise ValueError('invalid fragment kind: {}'.format(kind))", "response": "Convert a ParseResult into a DSL object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a reaction parse object to a DSL.", "response": "def _reaction_po_to_dict(tokens) -> Reaction:\n    \"\"\"Convert a reaction parse object to a DSL.\n\n    :type tokens: ParseResult\n    \"\"\"\n    return Reaction(\n        reactants=_reaction_part_po_to_dict(tokens[REACTANTS]),\n        products=_reaction_part_po_to_dict(tokens[PRODUCTS]),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _list_po_to_dict(tokens) -> ListAbundance:\n    func = tokens[FUNCTION]\n    dsl = FUNC_TO_LIST_DSL[func]\n\n    members = [parse_result_to_dsl(token) for token in tokens[MEMBERS]]\n\n    return dsl(members)", "response": "Convert a list parse object to a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_subgraph_by_annotations(graph, annotations, or_=None):\n    edge_filter_builder = (\n        build_annotation_dict_any_filter\n        if (or_ is None or or_) else\n        build_annotation_dict_all_filter\n    )\n\n    return get_subgraph_by_edge_filter(graph, edge_filter_builder(annotations))", "response": "Induce a subgraph given an annotation filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninducing a subgraph over all edges whose annotations match the given key and value.", "response": "def get_subgraph_by_annotation_value(graph, annotation, values):\n    \"\"\"Induce a sub-graph over all edges whose annotations match the given key and value.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param str annotation: The annotation to group by\n    :param values: The value(s) for the annotation\n    :type values: str or iter[str]\n    :return: A subgraph of the original BEL graph\n    :rtype: pybel.BELGraph\n    \"\"\"\n    if isinstance(values, str):\n        values = {values}\n\n    return get_subgraph_by_annotations(graph, {annotation: values})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef invert_edge_predicate(edge_predicate: EdgePredicate) -> EdgePredicate:  # noqa: D202\n\n    def _inverse_filter(graph, u, v, k):\n        return not edge_predicate(graph, u, v, k)\n\n    return _inverse_filter", "response": "Build an edge predicate that is the inverse of the given edge predicate."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef and_edge_predicates(edge_predicates: EdgePredicates) -> EdgePredicate:\n    # If something that isn't a list or tuple is given, assume it's a function and return it\n    if not isinstance(edge_predicates, Iterable):\n        return edge_predicates\n\n    edge_predicates = tuple(edge_predicates)\n\n    # If only one predicate is given, don't bother wrapping it\n    if 1 == len(edge_predicates):\n        return edge_predicates[0]\n\n    def concatenated_edge_predicate(graph: BELGraph, u: BaseEntity, v: BaseEntity, k: str) -> bool:\n        \"\"\"Pass only for an edge that pass all enclosed predicates.\n\n        :return: If the edge passes all enclosed predicates\n        \"\"\"\n        return all(\n            edge_predicate(graph, u, v, k)\n            for edge_predicate in edge_predicates\n        )\n\n    return concatenated_edge_predicate", "response": "Concatenate multiple edge predicates to a new predicate that requires all predicates to be met."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies a set of filters to the edges iterator of a BEL graph.", "response": "def filter_edges(graph: BELGraph, edge_predicates: EdgePredicates) -> EdgeIterator:\n    \"\"\"Apply a set of filters to the edges iterator of a BEL graph.\n\n    :return: An iterable of edges that pass all predicates\n    \"\"\"\n    compound_edge_predicate = and_edge_predicates(edge_predicates=edge_predicates)\n    for u, v, k in graph.edges(keys=True):\n        if compound_edge_predicate(graph, u, v, k):\n            yield u, v, k"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of edges passing a given set of predicates.", "response": "def count_passed_edge_filter(graph: BELGraph, edge_predicates: EdgePredicates) -> int:\n    \"\"\"Return the number of edges passing a given set of predicates.\"\"\"\n    return sum(\n        1\n        for _ in filter_edges(graph, edge_predicates=edge_predicates)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subgraph(graph, nodes: Iterable[BaseEntity]):\n    sg = graph.subgraph(nodes)\n\n    # see implementation for .copy()\n    result = graph.fresh_copy()\n    result.graph.update(sg.graph)\n\n    for node, data in sg.nodes(data=True):\n        result.add_node(node, **data)\n\n    result.add_edges_from(\n        (u, v, key, datadict.copy())\n        for u, v, key, datadict in sg.edges(keys=True, data=True)\n    )\n\n    return result", "response": "Induce a sub - graph over the given nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef left_full_join(g, h) -> None:\n    g.add_nodes_from(\n        (node, data)\n        for node, data in h.nodes(data=True)\n        if node not in g\n    )\n    g.add_edges_from(\n        (u, v, key, data)\n        for u, v, key, data in h.edges(keys=True, data=True)\n        if u not in g or v not in g[u] or key not in g[u][v]\n    )\n\n    update_metadata(h, g)\n    update_node_helper(h, g)\n    g.warnings.extend(h.warnings)", "response": "Left - full join a graph with a BEL graph h."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef left_outer_join(g, h) -> None:\n    g_nodes = set(g)\n    for comp in nx.weakly_connected_components(h):\n        if g_nodes.intersection(comp):\n            h_subgraph = subgraph(h, comp)\n            left_full_join(g, h_subgraph)", "response": "Only add components from the g that are touching h."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef union(graphs, use_tqdm: bool = False):\n    it = iter(graphs)\n\n    if use_tqdm:\n        it = tqdm(it, desc='taking union')\n\n    try:\n        target = next(it)\n    except StopIteration as e:\n        raise ValueError('no graphs given') from e\n\n    try:\n        graph = next(it)\n    except StopIteration:\n        return target\n    else:\n        target = target.copy()\n        left_full_join(target, graph)\n\n    for graph in it:\n        left_full_join(target, graph)\n\n    return target", "response": "Take the union over a collection of graphs into a new graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking the intersection over two graphs.", "response": "def left_node_intersection_join(g, h):\n    \"\"\"Take the intersection over two graphs.\n\n    This intersection of two graphs is defined by the union of the sub-graphs induced over the intersection of their nodes\n\n    :param BELGraph g: A BEL graph\n    :param BELGraph h: A BEL graph\n    :rtype: BELGraph\n\n    Example usage:\n\n    >>> import pybel\n    >>> g = pybel.from_path('...')\n    >>> h = pybel.from_path('...')\n    >>> merged = left_node_intersection_join(g, h)\n    \"\"\"\n    intersecting = set(g).intersection(set(h))\n\n    g_inter = subgraph(g, intersecting)\n    h_inter = subgraph(h, intersecting)\n\n    left_full_join(g_inter, h_inter)\n\n    return g_inter"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes the node intersection over a collection of graphs into a new graph.", "response": "def node_intersection(graphs):\n    \"\"\"Take the node intersection over a collection of graphs into a new graph.\n\n    This intersection is defined the same way as by :func:`left_node_intersection_join`\n\n    :param iter[BELGraph] graphs: An iterable of graphs. Since it's iterated over twice, it gets converted to a\n     tuple first, so this isn't a safe operation for infinite lists.\n    :rtype: BELGraph\n\n    Example usage:\n\n    >>> import pybel\n    >>> g = pybel.from_path('...')\n    >>> h = pybel.from_path('...')\n    >>> k = pybel.from_path('...')\n    >>> merged = node_intersection([g, h, k])\n    \"\"\"\n    graphs = tuple(graphs)\n\n    n_graphs = len(graphs)\n\n    if n_graphs == 0:\n        raise ValueError('no graphs given')\n\n    if n_graphs == 1:\n        return graphs[0]\n\n    nodes = set(graphs[0].nodes())\n\n    for graph in graphs[1:]:\n        nodes.intersection_update(graph)\n\n    return union(\n        subgraph(graph, nodes)\n        for graph in graphs\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving all the annotations from a BEL graph.", "response": "def strip_annotations(graph):\n    \"\"\"Strip all the annotations from a BEL graph.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    for u, v, k in graph.edges(keys=True):\n        if ANNOTATIONS in graph[u][v][k]:\n            del graph[u][v][k][ANNOTATIONS]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the given annotation value pair to all qualified edges.", "response": "def add_annotation_value(graph, annotation, value):\n    \"\"\"Add the given annotation/value pair to all qualified edges.\n\n    :param pybel.BELGraph graph:\n    :param str annotation:\n    :param str value:\n    \"\"\"\n    if annotation not in graph.defined_annotation_keywords:\n        raise ValueError('annotation not defined: {}'.format(annotation))\n\n    for u, v, k in graph.edges(keys=True):\n        if ANNOTATIONS not in graph[u][v][k]:\n            continue\n\n        if annotation not in graph[u][v][k][ANNOTATIONS]:\n            graph[u][v][k][ANNOTATIONS][annotation] = {}\n\n        graph[u][v][k][ANNOTATIONS][annotation][value] = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the given annotation value pair from all qualified edges.", "response": "def remove_annotation_value(graph, annotation, value):\n    \"\"\"Remove the given annotation/value pair to all qualified edges.\n\n    :param pybel.BELGraph graph:\n    :param str annotation:\n    :param str value:\n    \"\"\"\n    if annotation not in graph.defined_annotation_keywords:\n        log.warning('annotation was not defined: %s', annotation)\n        return\n\n    for u, v, k in graph.edges(keys=True):\n        if ANNOTATIONS not in graph[u][v][k]:\n            continue\n\n        if annotation not in graph[u][v][k][ANNOTATIONS]:\n            continue\n\n        if value not in graph[u][v][k][ANNOTATIONS][annotation]:\n            continue\n\n        del graph[u][v][k][ANNOTATIONS][annotation][value]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the metadata associated with a citation.", "response": "def remove_citation_metadata(graph):\n    \"\"\"Remove the metadata associated with a citation.\n\n    Best practice is to add this information programmatically.\n    \"\"\"\n    for u, v, k in graph.edges(keys=True):\n        if CITATION not in graph[u][v][k]:\n            continue\n        for key in list(graph[u][v][k][CITATION]):\n            if key not in _CITATION_KEEP_KEYS:\n                del graph[u][v][k][CITATION][key]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting this graph to a Node - Link JSON object.", "response": "def to_json(graph: BELGraph) -> Mapping[str, Any]:\n    \"\"\"Convert this graph to a Node-Link JSON object.\"\"\"\n    graph_json_dict = node_link_data(graph)\n\n    # Convert annotation list definitions (which are sets) to canonicalized/sorted lists\n    graph_json_dict['graph'][GRAPH_ANNOTATION_LIST] = {\n        keyword: list(sorted(values))\n        for keyword, values in graph_json_dict['graph'].get(GRAPH_ANNOTATION_LIST, {}).items()\n    }\n\n    # Convert set to list\n    graph_json_dict['graph'][GRAPH_UNCACHED_NAMESPACES] = list(\n        graph_json_dict['graph'].get(GRAPH_UNCACHED_NAMESPACES, [])\n    )\n\n    return graph_json_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites this graph as a Node - Link JSON file.", "response": "def to_json_path(graph: BELGraph, path: str, **kwargs) -> None:\n    \"\"\"Write this graph to the given path as a Node-Link JSON.\"\"\"\n    with open(os.path.expanduser(path), 'w') as f:\n        to_json_file(graph, file=f, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting this graph as Node - Link JSON to a file.", "response": "def to_json_file(graph: BELGraph, file: TextIO, **kwargs) -> None:\n    \"\"\"Write this graph as Node-Link JSON to a file.\"\"\"\n    graph_json_dict = to_json(graph)\n    json.dump(graph_json_dict, file, ensure_ascii=False, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_jsons(graph: BELGraph, **kwargs) -> str:\n    graph_json_str = to_json(graph)\n    return json.dumps(graph_json_str, ensure_ascii=False, **kwargs)", "response": "Dump this graph as a Node - Link JSON object to a string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_json(graph_json_dict: Mapping[str, Any], check_version=True) -> BELGraph:\n    graph = node_link_graph(graph_json_dict)\n    return ensure_version(graph, check_version=check_version)", "response": "Build a graph from a Node - Link JSON Object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a graph from a file containing Node - Link JSON.", "response": "def from_json_path(path: str, check_version: bool = True) -> BELGraph:\n    \"\"\"Build a graph from a file containing Node-Link JSON.\"\"\"\n    with open(os.path.expanduser(path)) as f:\n        return from_json_file(f, check_version=check_version)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_json_file(file: TextIO, check_version=True) -> BELGraph:\n    graph_json_dict = json.load(file)\n    return from_json(graph_json_dict, check_version=check_version)", "response": "Build a graph from the Node - Link JSON contained in the given file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a BEL graph from a Node - Link JSON string.", "response": "def from_jsons(graph_json_str: str, check_version: bool = True) -> BELGraph:\n    \"\"\"Read a BEL graph from a Node-Link JSON string.\"\"\"\n    graph_json_dict = json.loads(graph_json_str)\n    return from_json(graph_json_dict, check_version=check_version)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a BEL graph to a node - link format.", "response": "def node_link_data(graph: BELGraph) -> Mapping[str, Any]:\n    \"\"\"Convert a BEL graph to a node-link format.\n\n    Adapted from :func:`networkx.readwrite.json_graph.node_link_data`\n    \"\"\"\n    nodes = sorted(graph, key=methodcaller('as_bel'))\n\n    mapping = dict(zip(nodes, count()))\n\n    return {\n        'directed': True,\n        'multigraph': True,\n        'graph': graph.graph.copy(),\n        'nodes': [\n            _augment_node(node)\n            for node in nodes\n        ],\n        'links': [\n            dict(chain(\n                data.copy().items(),\n                [('source', mapping[u]), ('target', mapping[v]), ('key', key)]\n            ))\n            for u, v, key, data in graph.edges(keys=True, data=True)\n        ]\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the SHA - 512 identifier to a node s dictionary.", "response": "def _augment_node(node: BaseEntity) -> BaseEntity:\n    \"\"\"Add the SHA-512 identifier to a node's dictionary.\"\"\"\n    rv = node.copy()\n    rv['id'] = node.as_sha512()\n    rv['bel'] = node.as_bel()\n    for m in chain(node.get(MEMBERS, []), node.get(REACTANTS, []), node.get(PRODUCTS, [])):\n        m.update(_augment_node(m))\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a node - link graph from node - link data format.", "response": "def node_link_graph(data: Mapping[str, Any]) -> BELGraph:\n    \"\"\"Return graph from node-link data format.\n\n    Adapted from :func:`networkx.readwrite.json_graph.node_link_graph`\n    \"\"\"\n    graph = BELGraph()\n    graph.graph = data.get('graph', {})\n    graph.graph[GRAPH_ANNOTATION_LIST] = {\n        keyword: set(values)\n        for keyword, values in graph.graph.get(GRAPH_ANNOTATION_LIST, {}).items()\n    }\n\n    mapping = []\n\n    for node_data in data['nodes']:\n        _dsl = parse_result_to_dsl(node_data)\n        node = graph.add_node_from_data(_dsl)\n        mapping.append(node)\n\n    for data in data['links']:\n        u = mapping[data['source']]\n        v = mapping[data['target']]\n\n        edge_data = {\n            k: v\n            for k, v in data.items()\n            if k not in {'source', 'target', 'key'}\n        }\n        graph.add_edge(u, v, key=data['key'], **edge_data)\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing multiple lines in succession.", "response": "def parse_lines(self, lines: Iterable[str]) -> List[ParseResults]:\n        \"\"\"Parse multiple lines in succession.\"\"\"\n        return [\n            self.parseString(line, line_number)\n            for line_number, line in enumerate(lines)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a string with the language represented by this parser.", "response": "def parseString(self, line: str, line_number: int = 0) -> ParseResults:  # noqa: N802\n        \"\"\"Parse a string with the language represented by this parser.\n\n        :param line: A string representing an instance of this parser's language\n        :param line_number: The current line number of the parser\n        \"\"\"\n        self._line_number = line_number\n        return self.language.parseString(line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef streamline(self):\n        t = time.time()\n        self.language.streamline()\n        log.info('streamlined %s in %.02f seconds', self.__class__.__name__, time.time() - t)", "response": "Streamline the language represented by this parser to make queries run faster."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_neo4j(graph, neo_connection, use_tqdm=False):\n    import py2neo\n\n    if isinstance(neo_connection, str):\n        neo_connection = py2neo.Graph(neo_connection)\n\n    tx = neo_connection.begin()\n\n    node_map = {}\n\n    nodes = list(graph)\n    if use_tqdm:\n        nodes = tqdm(nodes, desc='nodes')\n\n    for node in nodes:\n        if NAMESPACE not in node or VARIANTS in node or MEMBERS in node or FUSION in node:\n            attrs = {'name': node.as_bel()}\n        else:\n            attrs = {'namespace': node.namespace}\n\n            if node.name and node.identifier:\n                attrs['name'] = node.name\n                attrs['identifier'] = node.identifier\n            elif node.identifier and not node.name:\n                attrs['name'] = node.identifier\n            elif node.name and not node.identifier:\n                attrs['name'] = node.name\n\n        node_map[node] = py2neo.Node(node.function, **attrs)\n\n        tx.create(node_map[node])\n\n    edges = graph.edges(keys=True, data=True)\n    if use_tqdm:\n        edges = tqdm(edges, desc='edges')\n\n    for u, v, key, node in edges:\n        rel_type = node[RELATION]\n\n        d = node.copy()\n        del d[RELATION]\n\n        attrs = {}\n\n        annotations = d.pop(ANNOTATIONS, None)\n        if annotations:\n            for annotation, values in annotations.items():\n                attrs[annotation] = list(values)\n\n        citation = d.pop(CITATION, None)\n        if citation:\n            attrs[CITATION] = '{}:{}'.format(citation[CITATION_TYPE], citation[CITATION_REFERENCE])\n\n        if EVIDENCE in d:\n            attrs[EVIDENCE] = d[EVIDENCE]\n\n        for side in (SUBJECT, OBJECT):\n            side_data = d.get(side)\n            if side_data:\n                attrs.update(flatten_dict(side_data, parent_key=side))\n\n        rel = py2neo.Relationship(node_map[u], rel_type, node_map[v], key=key, **attrs)\n        tx.create(rel)\n\n    tx.commit()", "response": "Upload a BEL graph to a Neo4j graph database using the Py2Neo4j API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate over children of the node.", "response": "def iter_children(graph, node: BaseEntity) -> Iterable[BaseEntity]:\n    \"\"\"Iterate over children of the node.\"\"\"\n    return (\n        node\n        for node, _, d in graph.in_edges(node, data=True)\n        if d[RELATION] == IS_A\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transfer_causal_edges(graph, source: BaseEntity, target: BaseEntity) -> Iterable[str]:\n    for _, v, data in graph.out_edges(source, data=True):\n        if data[RELATION] not in CAUSAL_RELATIONS:\n            continue\n\n        yield graph.add_qualified_edge(\n            target,\n            v,\n            relation=data[RELATION],\n            evidence=data[EVIDENCE],\n            citation=data[CITATION],\n            annotations=data.get(ANNOTATIONS),\n            subject_modifier=data.get(SUBJECT),\n            object_modifier=data.get(OBJECT),\n        )\n\n    for u, _, data in graph.in_edges(source, data=True):\n        if data[RELATION] not in CAUSAL_RELATIONS:\n            continue\n\n        yield graph.add_qualified_edge(\n            u,\n            target,\n            relation=data[RELATION],\n            evidence=data[EVIDENCE],\n            citation=data[CITATION],\n            annotations=data.get(ANNOTATIONS),\n            subject_modifier=data.get(SUBJECT),\n            object_modifier=data.get(OBJECT),\n        )", "response": "Transfer causal edges that the source has to the target and yield the resulting hashes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_child_relations(graph, node: BaseEntity) -> List[str]:\n    return list(_infer_child_relations_iter(graph, node))", "response": "Propagate causal relations to children."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers causal relations to children.", "response": "def _infer_child_relations_iter(graph, node: BaseEntity) -> Iterable[str]:\n    \"\"\"Propagate causal relations to children.\"\"\"\n    for child in iter_children(graph, node):\n        yield from transfer_causal_edges(graph, node, child)\n        yield from infer_child_relations(graph, child)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a dictionary to a tuple.", "response": "def _get_protocol_tuple(data: Dict[str, Any]) -> Tuple[str, List, Dict]:\n    \"\"\"Convert a dictionary to a tuple.\"\"\"\n    return data['function'], data.get('args', []), data.get('kwargs', {})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a pipeline from a list of functions.", "response": "def from_functions(functions) -> 'Pipeline':\n        \"\"\"Build a pipeline from a list of functions.\n\n        :param functions: A list of functions or names of functions\n        :type functions: iter[((pybel.BELGraph) -> pybel.BELGraph) or ((pybel.BELGraph) -> None) or str]\n\n        Example with function:\n\n        >>> from pybel.struct.pipeline import Pipeline\n        >>> from pybel.struct.mutation import remove_associations\n        >>> pipeline = Pipeline.from_functions([remove_associations])\n\n        Equivalent example with function names:\n\n        >>> from pybel.struct.pipeline import Pipeline\n        >>> pipeline = Pipeline.from_functions(['remove_associations'])\n\n        Lookup by name is possible for built in functions, and those that have been registered correctly using one of\n        the four decorators:\n\n        1. :func:`pybel.struct.pipeline.transformation`,\n        2. :func:`pybel.struct.pipeline.in_place_transformation`,\n        3. :func:`pybel.struct.pipeline.uni_transformation`,\n        4. :func:`pybel.struct.pipeline.uni_in_place_transformation`,\n        \"\"\"\n        result = Pipeline()\n\n        for func in functions:\n            result.append(func)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_function(self, name: str):\n        f = mapped.get(name)\n\n        if f is None:\n            raise MissingPipelineFunctionError('{} is not registered as a pipeline function'.format(name))\n\n        if name in universe_map and name in in_place_map:\n            return self._wrap_in_place(self._wrap_universe(f))\n\n        if name in universe_map:\n            return self._wrap_universe(f)\n\n        if name in in_place_map:\n            return self._wrap_in_place(f)\n\n        return f", "response": "Wrap a function with the universe and in - place."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append(self, name, *args, **kwargs) -> 'Pipeline':\n        if isinstance(name, types.FunctionType):\n            return self.append(name.__name__, *args, **kwargs)\n        elif isinstance(name, str):\n            get_transformation(name)\n        else:\n            raise TypeError('invalid function argument: {}'.format(name))\n\n        av = {\n            'function': name,\n        }\n\n        if args:\n            av['args'] = args\n\n        if kwargs:\n            av['kwargs'] = kwargs\n\n        self.protocol.append(av)\n        return self", "response": "Adds a function to the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extend(self, protocol: Union[Iterable[Dict], 'Pipeline']) -> 'Pipeline':\n        for data in protocol:\n            name, args, kwargs = _get_protocol_tuple(data)\n            self.append(name, *args, **kwargs)\n\n        return self", "response": "Add another pipeline to the end of the current pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_helper(self, graph, protocol: Iterable[Dict]):\n        result = graph\n\n        for entry in protocol:\n            meta_entry = entry.get('meta')\n\n            if meta_entry is None:\n                name, args, kwargs = _get_protocol_tuple(entry)\n                func = self._get_function(name)\n                result = func(result, *args, **kwargs)\n            else:\n                networks = (\n                    self._run_helper(graph, subprotocol)\n                    for subprotocol in entry['pipelines']\n                )\n\n                if meta_entry == META_UNION:\n                    result = union(networks)\n\n                elif meta_entry == META_INTERSECTION:\n                    result = node_intersection(networks)\n\n                else:\n                    raise MetaValueError('invalid meta-command: {}'.format(meta_entry))\n\n        return result", "response": "Help run the protocol."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning the contained protocol on a seed graph.", "response": "def run(self, graph, universe=None):\n        \"\"\"Run the contained protocol on a seed graph.\n\n        :param pybel.BELGraph graph: The seed BEL graph\n        :param pybel.BELGraph universe: Allows just-in-time setting of the universe in case it wasn't set before.\n                                        Defaults to the given network.\n        :return: The new graph is returned if not applied in-place\n        :rtype: pybel.BELGraph\n        \"\"\"\n        self.universe = universe or graph.copy()\n        return self._run_helper(graph.copy(), self.protocol)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping a function that needs a universe graph as the first argument and returns a wrapped one.", "response": "def _wrap_universe(self, func):  # noqa: D202\n        \"\"\"Take a function that needs a universe graph as the first argument and returns a wrapped one.\"\"\"\n\n        @wraps(func)\n        def wrapper(graph, *args, **kwargs):\n            \"\"\"Apply the enclosed function with the universe given as the first argument.\"\"\"\n            if self.universe is None:\n                raise MissingUniverseError(\n                    'Can not run universe function [{}] - No universe is set'.format(func.__name__))\n\n            return func(self.universe, graph, *args, **kwargs)\n\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps a function that doesn t return the graph and returns the graph.", "response": "def _wrap_in_place(func):  # noqa: D202\n        \"\"\"Take a function that doesn't return the graph and returns the graph.\"\"\"\n\n        @wraps(func)\n        def wrapper(graph, *args, **kwargs):\n            \"\"\"Apply the enclosed function and returns the graph.\"\"\"\n            func(graph, *args, **kwargs)\n            return graph\n\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps this protocol to a file in JSON.", "response": "def dump(self, file: TextIO, **kwargs) -> None:\n        \"\"\"Dump this protocol to a file in JSON.\"\"\"\n        return json.dump(self.to_json(), file, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_meta(meta: str, pipelines: Iterable['Pipeline']) -> 'Pipeline':\n        return Pipeline(protocol=[\n            {\n                'meta': meta,\n                'pipelines': [\n                    pipeline.protocol\n                    for pipeline in pipelines\n                ]\n            },\n        ])", "response": "Build a pipeline with a given meta - argument."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_names(graph):\n    rv = defaultdict(set)\n    for namespace, name in _identifier_filtered_iterator(graph):\n        rv[namespace].add(name)\n    return dict(rv)", "response": "Get all names for each namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _identifier_filtered_iterator(graph):\n    for data in graph:\n        for pair in _get_node_names(data):\n            yield pair\n\n        for member in data.get(MEMBERS, []):\n            for pair in _get_node_names(member):\n                yield pair\n\n    for ((_, _, data), side) in itt.product(graph.edges(data=True), (SUBJECT, OBJECT)):\n        side_data = data.get(side)\n        if side_data is None:\n            continue\n\n        modifier = side_data.get(MODIFIER)\n        effect = side_data.get(EFFECT)\n\n        if modifier == ACTIVITY and effect is not None and NAMESPACE in effect and NAME in effect:\n            yield effect[NAMESPACE], effect[NAME]\n\n        elif modifier == TRANSLOCATION and effect is not None:\n            from_loc = effect.get(FROM_LOC)\n            if NAMESPACE in from_loc and NAME in from_loc:\n                yield from_loc[NAMESPACE], from_loc[NAME]\n\n            to_loc = effect.get(TO_LOC)\n            if NAMESPACE in to_loc and NAME in to_loc:\n                yield to_loc[NAMESPACE], to_loc[NAME]\n\n        location = side_data.get(LOCATION)\n        if location is not None and NAMESPACE in location and NAME in location:\n            yield location[NAMESPACE], location[NAME]", "response": "Iterate over names in the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _namespace_filtered_iterator(graph, namespace):\n    for it_namespace, name in _identifier_filtered_iterator(graph):\n        if namespace == it_namespace:\n            yield name", "response": "Iterate over names in the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count_names_by_namespace(graph, namespace):\n    if namespace not in graph.defined_namespace_keywords:\n        raise IndexError('{} is not defined in {}'.format(namespace, graph))\n\n    return Counter(_namespace_filtered_iterator(graph, namespace))", "response": "Return a Counter of all of the names in a given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_names_by_namespace(graph, namespace):\n    if namespace not in graph.defined_namespace_keywords:\n        raise IndexError('{} is not defined in {}'.format(namespace, graph))\n\n    return set(_namespace_filtered_iterator(graph, namespace))", "response": "Get all of the names belonging to a given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_variants(graph):\n    return Counter(\n        variant_data[KIND]\n        for data in graph\n        if has_variant(graph, data)\n        for variant_data in data[VARIANTS]\n    )", "response": "Count how many of each type of variant a graph has."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_top_hubs(graph, *, n: Optional[int] = 15) -> List[Tuple[BaseEntity, int]]:\n    return Counter(dict(graph.degree())).most_common(n=n)", "response": "Get the top hubs in the graph by BEL."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates over edges in which either the source or target is a pathology node.", "response": "def _pathology_iterator(graph):\n    \"\"\"Iterate over edges in which either the source or target is a pathology node.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :rtype: iter\n    \"\"\"\n    for node in itt.chain.from_iterable(graph.edges()):\n        if isinstance(node, Pathology):\n            yield node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_top_pathologies(graph, n: Optional[int] = 15) -> List[Tuple[BaseEntity, int]]:\n    return count_pathologies(graph).most_common(n)", "response": "Get the top n pathologies in a BEL graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the corresponding RNA node for each protein node and connect them with a translation edge.", "response": "def enrich_proteins_with_rnas(graph):\n    \"\"\"Add the corresponding RNA node for each protein node and connect them with a translation edge.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    for protein_node in list(graph):\n        if not isinstance(protein_node, Protein):\n            continue\n\n        if protein_node.variants:\n            continue\n\n        rna_node = protein_node.get_rna()\n        graph.add_translation(rna_node, protein_node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enrich_rnas_with_genes(graph):\n    for rna_node in list(graph):\n        if rna_node[FUNCTION] not in {MIRNA, RNA} or FUSION in rna_node or VARIANTS in rna_node:\n            continue\n\n        gene_node = rna_node.get_gene()\n        graph.add_transcription(gene_node, rna_node)", "response": "Add the corresponding gene node for each RNA node and connect them with a transcription edge."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninducing a sub - graph from all of the upstream causal entities of the nbunch.", "response": "def get_upstream_causal_subgraph(graph, nbunch: Union[BaseEntity, Iterable[BaseEntity]]):\n    \"\"\"Induce a sub-graph from all of the upstream causal entities of the nodes in the nbunch.\n\n    :type graph: pybel.BELGraph\n    :rtype: pybel.BELGraph\n    \"\"\"\n    return get_subgraph_by_edge_filter(graph, build_upstream_edge_predicate(nbunch))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_downstream_causal_subgraph(graph, nbunch: Union[BaseEntity, Iterable[BaseEntity]]):\n    return get_subgraph_by_edge_filter(graph, build_downstream_edge_predicate(nbunch))", "response": "Induce a sub - graph from all of the downstream causal entities of the nbunch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cleanup(graph, subgraphs):\n    for subgraph in subgraphs.values():\n        update_node_helper(graph, subgraph)\n        update_metadata(graph, subgraph)", "response": "Clean up the metadata in the subgraphs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all nodes with the given function.", "response": "def get_nodes_by_function(graph: BELGraph, func: Strings) -> Set[BaseEntity]:\n    \"\"\"Get all nodes with the given function(s).\"\"\"\n    return get_nodes(graph, function_inclusion_filter_builder(func))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_nodes_by_namespace(graph: BELGraph, namespaces: Strings) -> Set[BaseEntity]:\n    return get_nodes(graph, namespace_inclusion_builder(namespaces))", "response": "Get all nodes identified by the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a protein fragment parser.", "response": "def get_fragment_language() -> ParserElement:\n    \"\"\"Build a protein fragment parser.\"\"\"\n    _fragment_value_inner = fragment_range | missing_fragment(FRAGMENT_MISSING)\n    _fragment_value = _fragment_value_inner | And([Suppress('\"'), _fragment_value_inner, Suppress('\"')])\n    parser_element = fragment_tag + nest(_fragment_value + Optional(WCW + quote(FRAGMENT_DESCRIPTION)))\n    return parser_element"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a HGVS language.", "response": "def get_hgvs_language() -> ParserElement:\n    \"\"\"Build a HGVS :class:`pyparsing.ParseElement`.\"\"\"\n    hgvs = (variant_characters | quote)(IDENTIFIER)\n    language = variant_tags + nest(hgvs)\n    return language"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_protein_modification_language(identifier_qualified: ParserElement) -> ParserElement:\n    pmod_identifier = MatchFirst([\n        identifier_qualified,\n        pmod_default_ns,\n        pmod_legacy_ns\n    ])\n\n    return pmod_tag + nest(\n        Group(pmod_identifier)(IDENTIFIER) +\n        Optional(\n            WCW +\n            amino_acid(PMOD_CODE) +\n            Optional(\n                WCW +\n                ppc.integer(PMOD_POSITION)\n            )\n        )\n    )", "response": "Build a protein modification parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupload CBN data to edge store", "response": "def upload_cbn_dir(dir_path, manager):\n    \"\"\"Uploads CBN data to edge store\n\n    :param str dir_path: Directory full of CBN JGIF files\n    :param pybel.Manager manager:\n    \"\"\"\n    t = time.time()\n\n    for jfg_path in os.listdir(dir_path):\n        if not jfg_path.endswith('.jgf'):\n            continue\n\n        path = os.path.join(dir_path, jfg_path)\n\n        log.info('opening %s', path)\n\n        with open(path) as f:\n            cbn_jgif_dict = json.load(f)\n\n        graph = pybel.from_cbn_jgif(cbn_jgif_dict)\n\n        out_path = os.path.join(dir_path, jfg_path.replace('.jgf', '.bel'))\n        with open(out_path, 'w') as o:\n            pybel.to_bel(graph, o)\n\n        strip_annotations(graph)\n        enrich_pubmed_citations(manager=manager, graph=graph)\n        pybel.to_database(graph, manager=manager)\n\n        log.info('')\n\n    log.info('done in %.2f', time.time() - t)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake an activity dictionary. :param str modifier: :param Optional[dict] location: An entity from :func:`pybel.dsl.entity` :rtype: dict", "response": "def _activity_helper(modifier: str, location=None):\n    \"\"\"Make an activity dictionary.\n\n    :param str modifier:\n    :param Optional[dict] location: An entity from :func:`pybel.dsl.entity`\n    :rtype: dict\n    \"\"\"\n    rv = {MODIFIER: modifier}\n\n    if location:\n        rv[LOCATION] = location\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a subject object modifier dictionary.", "response": "def activity(name=None, namespace=None, identifier=None, location=None):\n    \"\"\"Make a subject/object modifier dictionary.\n\n    :param str name: The name of the activity. If no namespace given, uses BEL default namespace\n    :param Optional[str] namespace: The namespace of the activity\n    :param Optional[str] identifier: The identifier of the name in the database\n    :param Optional[dict] location: An entity from :func:`pybel.dsl.entity` representing the location of the node\n    :rtype: dict\n    \"\"\"\n    rv = _activity_helper(ACTIVITY, location=location)\n\n    if name or (namespace and identifier):\n        rv[EFFECT] = Entity(\n            namespace=(namespace or BEL_DEFAULT_NAMESPACE),\n            name=name,\n            identifier=identifier,\n        )\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a translocation dictionary.", "response": "def translocation(from_loc, to_loc):\n    \"\"\"Make a translocation dictionary.\n\n    :param dict from_loc: An entity dictionary from :func:`pybel.dsl.entity`\n    :param dict to_loc: An entity dictionary from :func:`pybel.dsl.entity`\n    :rtype: dict\n    \"\"\"\n    rv = _activity_helper(TRANSLOCATION)\n    rv[EFFECT] = {\n        FROM_LOC: Entity(namespace=BEL_DEFAULT_NAMESPACE, name=from_loc) if isinstance(from_loc, str) else from_loc,\n        TO_LOC: Entity(namespace=BEL_DEFAULT_NAMESPACE, name=to_loc) if isinstance(to_loc, str) else to_loc,\n    }\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over all PubMed identifiers in a BEL graph.", "response": "def iterate_pubmed_identifiers(graph) -> Iterable[str]:\n    \"\"\"Iterate over all PubMed identifiers in a graph.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :return: An iterator over the PubMed identifiers in the graph\n    \"\"\"\n    return (\n        data[CITATION][CITATION_REFERENCE].strip()\n        for _, _, data in graph.edges(data=True)\n        if has_pubmed(data)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove edges passing the given edge predicates.", "response": "def remove_filtered_edges(graph, edge_predicates=None):\n    \"\"\"Remove edges passing the given edge predicates.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param edge_predicates: A predicate or list of predicates\n    :type edge_predicates: None or ((pybel.BELGraph, tuple, tuple, int) -> bool) or iter[(pybel.BELGraph, tuple, tuple, int) -> bool]]\n    :return:\n    \"\"\"\n    edges = list(filter_edges(graph, edge_predicates=edge_predicates))\n    graph.remove_edges_from(edges)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_filtered_nodes(graph, node_predicates=None):\n    nodes = list(filter_nodes(graph, node_predicates=node_predicates))\n    graph.remove_nodes_from(nodes)", "response": "Remove nodes passing the given node predicates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the modifier is in the given subject or object part.", "response": "def part_has_modifier(edge_data: EdgeData, part: str, modifier: str) -> bool:\n    \"\"\"Return true if the modifier is in the given subject/object part.\n\n    :param edge_data: PyBEL edge data dictionary\n    :param part: either :data:`pybel.constants.SUBJECT` or :data:`pybel.constants.OBJECT`\n    :param modifier: The modifier to look for\n    \"\"\"\n    part_data = edge_data.get(part)\n\n    if part_data is None:\n        return False\n\n    found_modifier = part_data.get(MODIFIER)\n\n    if found_modifier is None:\n        return False\n\n    return found_modifier == modifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a BEL graph around the neighborhoods of the given nodes.", "response": "def get_subgraph_by_neighborhood(graph, nodes: Iterable[BaseEntity]):\n    \"\"\"Get a BEL graph around the neighborhoods of the given nodes.\n\n    Returns none if no nodes are in the graph.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param nodes: An iterable of BEL nodes\n    :return: A BEL graph induced around the neighborhoods of the given nodes\n    :rtype: Optional[pybel.BELGraph]\n    \"\"\"\n    node_set = set(nodes)\n\n    if not any(node in graph for node in node_set):\n        return\n\n    rv = graph.fresh_copy()\n    rv.add_edges_from(itt.chain(\n        graph.in_edges(nodes, keys=True, data=True),\n        graph.out_edges(nodes, keys=True, data=True),\n    ))\n\n    update_node_helper(graph, rv)\n    update_metadata(graph, rv)\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a gene modification parser.", "response": "def get_gene_modification_language(identifier_qualified: ParserElement) -> ParserElement:\n    \"\"\"Build a gene modification parser.\"\"\"\n    gmod_identifier = MatchFirst([\n        identifier_qualified,\n        gmod_default_ns,\n    ])\n\n    return gmod_tag + nest(\n        Group(gmod_identifier)(IDENTIFIER)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expand_dict(flat_dict, sep='_'):\n    res = {}\n    rdict = defaultdict(list)\n\n    for flat_key, value in flat_dict.items():\n        key = flat_key.split(sep, 1)\n        if 1 == len(key):\n            res[key[0]] = value\n        else:\n            rdict[key[0]].append((key[1:], value))\n\n    for k, v in rdict.items():\n        res[k] = expand_dict({ik: iv for (ik,), iv in v})\n\n    return res", "response": "Expand a flattened dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nflattening a nested dictionary.", "response": "def flatten_dict(data, parent_key='', sep='_'):\n    \"\"\"Flatten a nested dictionary.\n\n    :param data: A nested dictionary\n    :type data: dict or MutableMapping\n    :param str parent_key: The parent's key. This is a value for tail recursion, so don't set it yourself.\n    :param str sep: The separator used between dictionary levels\n    :rtype: dict\n\n    .. seealso:: http://stackoverflow.com/a/6027615\n    \"\"\"\n    items = {}\n\n    for key, value in data.items():\n        # prepend the parent key\n        key = parent_key + sep + key if parent_key else key\n\n        if isinstance(value, (dict, MutableMapping)):\n            items.update(flatten_dict(value, key, sep=sep))\n        elif isinstance(value, (set, list)):\n            items[key] = ','.join(value)\n        else:\n            items[key] = value\n\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to parse a datetime object from a standard datetime format or date format.", "response": "def parse_datetime(s: str) -> datetime.date:\n    \"\"\"Try to parse a datetime object from a standard datetime format or date format.\"\"\"\n    for fmt in (CREATION_DATE_FMT, PUBLISHED_DATE_FMT, PUBLISHED_DATE_FMT_2):\n        try:\n            dt = datetime.strptime(s, fmt)\n        except ValueError:\n            pass\n        else:\n            return dt\n\n    raise ValueError('Incorrect datetime format for {}'.format(s))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_edge_tuple(source,\n                    target,\n                    edge_data: EdgeData,\n                    ) -> Tuple[str, str, str, Optional[str], Tuple[str, Optional[Tuple], Optional[Tuple]]]:\n    \"\"\"Convert an edge to a consistent tuple.\n\n    :param BaseEntity source: The source BEL node\n    :param BaseEntity target: The target BEL node\n    :param edge_data: The edge's data dictionary\n    :return: A tuple that can be hashed representing this edge. Makes no promises to its structure.\n    \"\"\"\n    return (\n        source.as_bel(),\n        target.as_bel(),\n        _get_citation_str(edge_data),\n        edge_data.get(EVIDENCE),\n        canonicalize_edge(edge_data),\n    )", "response": "Convert an edge to a consistent tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hash_edge(source, target, edge_data: EdgeData) -> str:\n    edge_tuple = _get_edge_tuple(source, target, edge_data)\n    return _hash_tuple(edge_tuple)", "response": "Convert an edge tuple to a SHA - 512 hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if all the keys in the query dict are in the target dict and that their values match.", "response": "def subdict_matches(target: Mapping, query: Mapping, partial_match: bool = True) -> bool:\n    \"\"\"Check if all the keys in the query dict are in the target dict, and that their values match.\n\n    1. Checks that all keys in the query dict are in the target dict\n    2. Matches the values of the keys in the query dict\n        a. If the value is a string, then must match exactly\n        b. If the value is a set/list/tuple, then will match any of them\n        c. If the value is a dict, then recursively check if that subdict matches\n\n    :param target: The dictionary to search\n    :param query: A query dict with keys to match\n    :param partial_match: Should the query values be used as partial or exact matches? Defaults to :code:`True`.\n    :return: if all keys in b are in target_dict and their values match\n    \"\"\"\n    for k, v in query.items():\n        if k not in target:\n            return False\n        elif not isinstance(v, (int, str, dict, Iterable)):\n            raise ValueError('invalid value: {}'.format(v))\n        elif isinstance(v, (int, str)) and target[k] != v:\n            return False\n        elif isinstance(v, dict):\n            if partial_match:\n                if not isinstance(target[k], dict):\n                    return False\n                elif not subdict_matches(target[k], v, partial_match):\n                    return False\n            elif not partial_match and target[k] != v:\n                return False\n        elif isinstance(v, Iterable) and target[k] not in v:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhashes an arbitrary JSON - serializable object by dumping it in sorted order then hashing the bytes.", "response": "def hash_dump(data) -> str:\n    \"\"\"Hash an arbitrary JSON dictionary by dumping it in sorted order, encoding it in UTF-8, then hashing the bytes.\n\n    :param data: An arbitrary JSON-serializable object\n    :type data: dict or list or tuple\n    \"\"\"\n    return hashlib.sha512(json.dumps(data, sort_keys=True).encode('utf-8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a hash for a type and reference pair of a citation.", "response": "def hash_citation(type: str, reference: str) -> str:\n    \"\"\"Create a hash for a type/reference pair of a citation.\n\n    :param type: The corresponding citation type\n    :param reference: The citation reference\n    \"\"\"\n    s = u'{type}:{reference}'.format(type=type, reference=reference)\n    return hashlib.sha512(s.encode('utf8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hash_evidence(text: str, type: str, reference: str) -> str:\n    s = u'{type}:{reference}:{text}'.format(type=type, reference=reference, text=text)\n    return hashlib.sha512(s.encode('utf8')).hexdigest()", "response": "Create a hash for an evidence and its citation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _canonicalize_edge_modifications(edge_data: EdgeData) -> Optional[Tuple]:\n    if edge_data is None:\n        return\n\n    modifier = edge_data.get(MODIFIER)\n    location = edge_data.get(LOCATION)\n    effect = edge_data.get(EFFECT)\n\n    if modifier is None and location is None:\n        return\n\n    result = []\n\n    if modifier == ACTIVITY:\n        if effect:\n            effect_name = effect.get(NAME)\n            effect_identifier = effect.get(IDENTIFIER)\n\n            t = (\n                ACTIVITY,\n                effect[NAMESPACE],\n                effect_name or effect_identifier,\n            )\n\n        else:\n            t = (ACTIVITY,)\n\n        result.append(t)\n\n    elif modifier == DEGRADATION:\n        t = (DEGRADATION,)\n        result.append(t)\n\n    elif modifier == TRANSLOCATION:\n        if effect:\n            from_loc_name = effect[FROM_LOC].get(NAME)\n            from_loc_identifier = effect[FROM_LOC].get(IDENTIFIER)\n            to_loc_name = effect[TO_LOC].get(NAME)\n            to_loc_identifier = effect[TO_LOC].get(IDENTIFIER)\n\n            t = (\n                TRANSLOCATION,\n                edge_data[EFFECT][FROM_LOC][NAMESPACE],\n                from_loc_name or from_loc_identifier,\n                edge_data[EFFECT][TO_LOC][NAMESPACE],\n                to_loc_name or to_loc_identifier,\n            )\n        else:\n            t = (TRANSLOCATION,)\n        result.append(t)\n\n    if location:\n        location_name = location.get(NAME)\n        location_identifier = location.get(IDENTIFIER)\n\n        t = (\n            LOCATION,\n            location[NAMESPACE],\n            location_name or location_identifier,\n        )\n        result.append(t)\n\n    if not result:\n        raise ValueError('Invalid data: {}'.format(edge_data))\n\n    return tuple(result)", "response": "Return the SUBJECT or OBJECT entry of a PyBEL edge data dictionary as a canonical tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstratifies the graph based on citations.", "response": "def get_subgraphs_by_citation(graph):\n    \"\"\"Stratify the graph based on citations.\n\n    :type graph: pybel.BELGraph\n    :rtype: dict[tuple[str,str],pybel.BELGraph]\n    \"\"\"\n    rv = defaultdict(graph.fresh_copy)\n\n    for u, v, key, data in graph.edges(keys=True, data=True):\n        if CITATION not in data:\n            continue\n        dk = data[CITATION][CITATION_TYPE], data[CITATION][CITATION_REFERENCE]\n\n        rv[dk].add_edge(u, v, key=key, **data)\n\n    cleanup(graph, rv)\n\n    return dict(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nraise an exception if the annotation is not defined.", "response": "def raise_for_undefined_annotation(self, line: str, position: int, annotation: str) -> None:\n        \"\"\"Raise an exception if the annotation is not defined.\n\n        :raises: UndefinedAnnotationWarning\n        \"\"\"\n        if self._in_debug_mode:\n            return\n\n        if not self.has_annotation(annotation):\n            raise UndefinedAnnotationWarning(self.get_line_number(), line, position, annotation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nraises an exception if the annotation value is not defined.", "response": "def raise_for_invalid_annotation_value(self, line: str, position: int, key: str, value: str) -> None:\n        \"\"\"Raise an exception if the annotation is not defined.\n\n        :raises: IllegalAnnotationValueWarning or MissingAnnotationRegexWarning\n        \"\"\"\n        if self._in_debug_mode:\n            return\n\n        if self.has_enumerated_annotation(key) and value not in self.annotation_to_term[key]:\n            raise IllegalAnnotationValueWarning(self.get_line_number(), line, position, key, value)\n\n        elif self.has_regex_annotation(key) and not self.annotation_to_pattern[key].match(value):\n            raise MissingAnnotationRegexWarning(self.get_line_number(), line, position, key, value)\n\n        elif self.has_local_annotation(key) and value not in self.annotation_to_local[key]:  # TODO condense\n            raise IllegalAnnotationValueWarning(self.get_line_number(), line, position, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef raise_for_missing_citation(self, line: str, position: int) -> None:\n        if self.citation_clearing and not self.citation:\n            raise MissingCitationException(self.get_line_number(), line, position)", "response": "Raise an exception if there is no citation present in the parser."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_annotation_key(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        key = tokens['key']\n        self.raise_for_missing_citation(line, position)\n        self.raise_for_undefined_annotation(line, position, key)\n        return tokens", "response": "Handle an annotation key before parsing to validate that it s either enumerated or as a regex."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_set_statement_group(self, _, __, tokens: ParseResults) -> ParseResults:\n        self.statement_group = tokens['group']\n        return tokens", "response": "Handle a SET STATEMENT_GROUP = X statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_set_citation(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        self.clear_citation()\n\n        values = tokens['values']\n\n        if len(values) < 2:\n            raise CitationTooShortException(self.get_line_number(), line, position)\n\n        citation_type = values[0]\n\n        if citation_type not in CITATION_TYPES:\n            raise InvalidCitationType(self.get_line_number(), line, position, citation_type)\n\n        if 2 == len(values):\n            return self.handle_set_citation_double(line, position, tokens)\n\n        citation_reference = values[2]\n\n        if citation_type == CITATION_TYPE_PUBMED and not is_int(citation_reference):\n            raise InvalidPubMedIdentifierWarning(self.get_line_number(), line, position, citation_reference)\n\n        if 4 <= len(values) and not valid_date(values[3]):\n            log.debug('Invalid date: %s. Truncating entry.', values[3])\n            self.citation = dict(zip(CITATION_ENTRIES, values[:3]))\n            return tokens\n\n        if 5 <= len(values):\n            values[4] = [value.strip() for value in values[4].split('|')]\n\n        if 6 < len(values):\n            raise CitationTooLongException(self.get_line_number(), line, position)\n\n        self.citation = dict(zip(CITATION_ENTRIES, values))\n\n        return tokens", "response": "Handle a SET Citation statement."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling a SET Citation double statement.", "response": "def handle_set_citation_double(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle a ``SET Citation = {\"X\", \"Y\"}`` statement.\"\"\"\n        values = tokens['values']\n\n        if values[0] == CITATION_TYPE_PUBMED and not is_int(values[1]):\n            raise InvalidPubMedIdentifierWarning(self.get_line_number(), line, position, values[1])\n\n        self.citation = dict(zip((CITATION_TYPE, CITATION_REFERENCE), values))\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles a SET Evidence = \"\" statement.", "response": "def handle_set_evidence(self, _, __, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle a ``SET Evidence = \"\"`` statement.\"\"\"\n        self.evidence = tokens['value']\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_set_command(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        key, value = tokens['key'], tokens['value']\n        self.raise_for_invalid_annotation_value(line, position, key, value)\n        self.annotations[key] = value\n        return tokens", "response": "Handle a SET X = Y statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_set_command_list(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        key, values = tokens['key'], tokens['values']\n        for value in values:\n            self.raise_for_invalid_annotation_value(line, position, key, value)\n        self.annotations[key] = set(values)\n        return tokens", "response": "Handle a SET X = Y Z..."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_unset_statement_group(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        if self.statement_group is None:\n            raise MissingAnnotationKeyWarning(self.get_line_number(), line, position, BEL_KEYWORD_STATEMENT_GROUP)\n        self.statement_group = None\n        return tokens", "response": "Unset the statement group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_unset_evidence(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        if self.evidence is None:\n            raise MissingAnnotationKeyWarning(self.get_line_number(), line, position, tokens[EVIDENCE])\n        self.evidence = None\n        return tokens", "response": "Unset the evidence, or throws an exception if it is not already set.\n\n        The value for ``tokens[EVIDENCE]`` corresponds to which alternate of SupportingText or Evidence was used in\n        the BEL script.\n\n        :raises: MissingAnnotationKeyWarning"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nraises an exception when trying to unset X.", "response": "def validate_unset_command(self, line: str, position: int, annotation: str) -> None:\n        \"\"\"Raise an exception when trying to ``UNSET X`` if ``X`` is not already set.\n\n        :raises: MissingAnnotationKeyWarning\n        \"\"\"\n        if annotation not in self.annotations:\n            raise MissingAnnotationKeyWarning(self.get_line_number(), line, position, annotation)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle an UNSET X statement.", "response": "def handle_unset_command(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle an ``UNSET X`` statement or raises an exception if it is not already set.\n\n        :raises: MissingAnnotationKeyWarning\n        \"\"\"\n        key = tokens['key']\n        self.validate_unset_command(line, position, key)\n        del self.annotations[key]\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles ``UNSET {A, B, ...}`` or raises an exception of any of them are not present. Consider that all unsets are in peril if just one of them is wrong! :raises: MissingAnnotationKeyWarning", "response": "def handle_unset_list(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle ``UNSET {A, B, ...}`` or raises an exception of any of them are not present.\n\n        Consider that all unsets are in peril if just one of them is wrong!\n\n        :raises: MissingAnnotationKeyWarning\n        \"\"\"\n        for key in tokens['values']:\n            if key in {BEL_KEYWORD_EVIDENCE, BEL_KEYWORD_SUPPORT}:\n                self.evidence = None\n            else:\n                self.validate_unset_command(line, position, key)\n                del self.annotations[key]\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the current annotations.", "response": "def get_annotations(self) -> Dict:\n        \"\"\"Get the current annotations.\"\"\"\n        return {\n            EVIDENCE: self.evidence,\n            CITATION: self.citation.copy(),\n            ANNOTATIONS: self.annotations.copy()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_missing_required_annotations(self) -> List[str]:\n        return [\n            required_annotation\n            for required_annotation in self.required_annotations\n            if required_annotation not in self.annotations\n        ]", "response": "Return missing required annotations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_citation(self):\n        self.citation.clear()\n\n        if self.citation_clearing:\n            self.evidence = None\n            self.annotations.clear()", "response": "Clear the citation and the evidence and annotations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear(self):\n        self.statement_group = None\n        self.citation.clear()\n        self.evidence = None\n        self.annotations.clear()", "response": "Clear the statement group citation evidence and annotations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef map_cbn(d):\n    for i, edge in enumerate(d['graph']['edges']):\n        if 'metadata' not in d['graph']['edges'][i]:\n            continue\n\n        if 'evidences' not in d['graph']['edges'][i]['metadata']:\n            continue\n\n        for j, evidence in enumerate(d['graph']['edges'][i]['metadata']['evidences']):\n            if EXPERIMENT_CONTEXT not in evidence:\n                continue\n\n            # ctx = {k.strip().lower(): v.strip() for k, v in evidence[EXPERIMENT_CONTEXT].items() if v.strip()}\n\n            new_context = {}\n\n            for key, value in evidence[EXPERIMENT_CONTEXT].items():\n                if not value:\n                    log.debug('key %s without value', key)\n                    continue\n\n                value = value.strip()\n\n                if not value:\n                    log.debug('key %s without value', key)\n                    continue\n\n                key = key.strip().lower()\n\n                if key == 'species_common_name':\n                    new_context['Species'] = species_map[value.lower()]\n                elif key in annotation_map:\n                    new_context[annotation_map[key]] = value\n                else:\n                    new_context[key] = value\n\n            '''\n            for k, v in annotation_map.items():\n                if k not in ctx:\n                    continue\n\n                d['graph']['edges'][i]['metadata']['evidences'][j][EXPERIMENT_CONTEXT][v] = ctx[k]\n                del d['graph']['edges'][i]['metadata']['evidences'][j][EXPERIMENT_CONTEXT][k]\n\n            if 'species_common_name' in ctx:\n                species_name = ctx['species_common_name'].strip().lower()\n                d['graph']['edges'][i]['metadata']['evidences'][j][EXPERIMENT_CONTEXT]['Species'] = species_map[\n                    species_name]\n                del d['graph']['edges'][i]['metadata']['evidences'][j][EXPERIMENT_CONTEXT][\n                    'species_common_name']\n            '''\n\n            d['graph']['edges'][i]['metadata']['evidences'][j][EXPERIMENT_CONTEXT] = new_context\n\n    return d", "response": "Map the JSON from the CBN to the JGIF."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_cbn_jgif(graph_jgif_dict):\n    graph_jgif_dict = map_cbn(graph_jgif_dict)\n\n    graph_jgif_dict['graph']['metadata'].update({\n        METADATA_AUTHORS: 'Causal Biological Networks Database',\n        METADATA_LICENSES: \"\"\"\n        Please cite:\n\n        - www.causalbionet.com\n        - https://bionet.sbvimprover.com\n\n        as well as any relevant publications.\n\n        The sbv IMPROVER project, the website and the Symposia are part of a collaborative project\n        designed to enable scientists to learn about and contribute to the development of a new crowd\n        sourcing method for verification of scientific data and results. The current challenges, website\n        and biological network models were developed and are maintained as part of a collaboration among\n        Selventa, OrangeBus and ADS. The project is led and funded by Philip Morris International. For more\n        information on the focus of Philip Morris International\u2019s research, please visit www.pmi.com.\n        \"\"\".replace('\\n', '\\t'),\n        METADATA_CONTACT: 'CausalBiologicalNetworks.RD@pmi.com',\n    })\n\n    graph = from_jgif(graph_jgif_dict)\n\n    graph.namespace_url.update({\n        'HGNC': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/hgnc-human-genes/hgnc-human-genes-20150601.belns',\n        'GOBP': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/go-biological-process/go-biological-process-20150601.belns',\n        'SFAM': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/selventa-protein-families/selventa-protein-families-20150601.belns',\n        'GOCC': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/go-cellular-component/go-cellular-component-20170511.belns',\n        'MESHPP': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/mesh-processes/mesh-processes-20150601.belns',\n        'MGI': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/mgi-mouse-genes/mgi-mouse-genes-20150601.belns',\n        'RGD': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/rgd-rat-genes/rgd-rat-genes-20150601.belns',\n        'CHEBI': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/chebi/chebi-20150601.belns',\n        'SCHEM': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/selventa-legacy-chemicals/selventa-legacy-chemicals-20150601.belns',\n        'EGID': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/entrez-gene-ids/entrez-gene-ids-20150601.belns',\n        'MESHD': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/mesh-diseases/mesh-diseases-20150601.belns',\n        'SDIS': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/selventa-legacy-diseases/selventa-legacy-diseases-20150601.belns',\n        'SCOMP': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/selventa-named-complexes/selventa-named-complexes-20150601.belns',\n        'MESHC': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/mesh-chemicals/mesh-chemicals-20170511.belns',\n        'GOBPID': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/go-biological-process-ids/go-biological-process-ids-20150601.belns',\n        'MESHCS': 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/mesh-cell-structures/mesh-cell-structures-20150601.belns',\n    })\n\n    graph.annotation_url.update({\n        'Cell': 'https://arty.scai.fraunhofer.de/artifactory/bel/annotation/cell-line/cell-line-20150601.belanno',\n        'Disease': 'https://arty.scai.fraunhofer.de/artifactory/bel/annotation/disease/disease-20150601.belanno',\n        'Species': 'https://arty.scai.fraunhofer.de/artifactory/bel/annotation/species-taxonomy-id/species-taxonomy-id-20170511.belanno',\n        'Tissue': 'https://arty.scai.fraunhofer.de/artifactory/bel/annotation/mesh-anatomy/mesh-anatomy-20150601.belanno',\n    })\n\n    return graph", "response": "Build a BEL graph from a CBN JGIF dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a BEL graph from a JGIF dictionary.", "response": "def from_jgif(graph_jgif_dict):\n    \"\"\"Build a BEL graph from a JGIF JSON object.\n\n    :param dict graph_jgif_dict: The JSON object representing the graph in JGIF format\n    :rtype: BELGraph\n    \"\"\"\n    graph = BELGraph()\n\n    root = graph_jgif_dict['graph']\n\n    if 'label' in root:\n        graph.name = root['label']\n\n    if 'metadata' in root:\n        metadata = root['metadata']\n\n        for key in METADATA_INSERT_KEYS:\n            if key in metadata:\n                graph.document[key] = metadata[key]\n\n    parser = BELParser(graph)\n    parser.bel_term.addParseAction(parser.handle_term)\n\n    for node in root['nodes']:\n        node_label = node.get('label')\n\n        if node_label is None:\n            log.warning('node missing label: %s', node)\n            continue\n\n        try:\n            parser.bel_term.parseString(node_label)\n        except NakedNameWarning as e:\n            log.info('Naked name: %s', e)\n        except ParseException:\n            log.info('Parse exception for %s', node_label)\n\n    for i, edge in enumerate(root['edges']):\n        relation = edge.get('relation')\n        if relation is None:\n            log.warning('no relation for edge: %s', edge)\n\n        if relation in {'actsIn', 'translocates'}:\n            continue  # don't need legacy BEL format\n\n        edge_metadata = edge.get('metadata')\n        if edge_metadata is None:\n            log.warning('no metadata for edge: %s', edge)\n            continue\n\n        bel_statement = edge.get('label')\n        if bel_statement is None:\n            log.debug('No BEL statement for edge %s', edge)\n\n        evidences = edge_metadata.get('evidences')\n\n        if relation in UNQUALIFIED_EDGES:\n            pass  # FIXME?\n\n        else:\n            if not evidences:  # is none or is empty list\n                log.debug('No evidence for edge %s', edge)\n                continue\n\n            for evidence in evidences:\n                citation = evidence.get('citation')\n\n                if not citation:\n                    continue\n\n                if 'type' not in citation or 'id' not in citation:\n                    continue\n\n                summary_text = evidence['summary_text'].strip()\n\n                if not summary_text or summary_text == placeholder_evidence:\n                    continue\n\n                parser.control_parser.clear()\n                parser.control_parser.citation = reformat_citation(citation)\n                parser.control_parser.evidence = summary_text\n                parser.control_parser.annotations.update(evidence[EXPERIMENT_CONTEXT])\n\n                try:\n                    parser.parseString(bel_statement, line_number=i)\n                except Exception as e:\n                    log.warning('JGIF relation parse error: %s for %s', e, bel_statement)\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_jgif(graph):\n    node_bel = {}\n    u_v_r_bel = {}\n\n    nodes_entry = []\n    edges_entry = []\n\n    for i, node in enumerate(sorted(graph, key=methodcaller('as_bel'))):\n        node_bel[node] = bel = node.as_bel()\n\n        nodes_entry.append({\n            'id': bel,\n            'label': bel,\n            'nodeId': i,\n            'bel_function_type': node[FUNCTION],\n            'metadata': {}\n        })\n\n    for u, v in graph.edges():\n        relation_evidences = defaultdict(list)\n\n        for data in graph[u][v].values():\n\n            if (u, v, data[RELATION]) not in u_v_r_bel:\n                u_v_r_bel[u, v, data[RELATION]] = graph.edge_to_bel(u, v, edge_data=data)\n\n            bel = u_v_r_bel[u, v, data[RELATION]]\n\n            evidence_dict = {\n                'bel_statement': bel,\n            }\n\n            if ANNOTATIONS in data:\n                evidence_dict['experiment_context'] = data[ANNOTATIONS]\n\n            if EVIDENCE in data:\n                evidence_dict['summary_text'] = data[EVIDENCE]\n\n            if CITATION in data:\n                evidence_dict['citation'] = data[CITATION]\n\n            relation_evidences[data[RELATION]].append(evidence_dict)\n\n        for relation, evidences in relation_evidences.items():\n            edges_entry.append({\n                'source': node_bel[u],\n                'target': node_bel[v],\n                'relation': relation,\n                'label': u_v_r_bel[u, v, relation],\n                'metadata': {\n                    'evidences': evidences\n                }\n            })\n\n    return {\n        'graph': {\n            'metadata': graph.document,\n            'nodes': nodes_entry,\n            'edges': edges_entry\n        }\n    }", "response": "Build a JGIF dictionary from a BEL graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a pipeline query on graph with multiple sub-graph filters and expanders. Order of Operations: 1. Seeding by given function name and data 2. Add nodes 3. Remove nodes :param pybel.BELGraph graph: A BEL graph :param seed_method: The name of the get_subgraph_by_* function to use :param seed_data: The argument to pass to the get_subgraph function :param expand_nodes: Add the neighborhoods around all of these nodes :param remove_nodes: Remove these nodes and all of their in/out edges :rtype: Optional[pybel.BELGraph]", "response": "def get_subgraph(graph,\n                 seed_method: Optional[str] = None,\n                 seed_data: Optional[Any] = None,\n                 expand_nodes: Optional[List[BaseEntity]] = None,\n                 remove_nodes: Optional[List[BaseEntity]] = None,\n                 ):\n    \"\"\"Run a pipeline query on graph with multiple sub-graph filters and expanders.\n\n    Order of Operations:\n\n    1. Seeding by given function name and data\n    2. Add nodes\n    3. Remove nodes\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param seed_method: The name of the get_subgraph_by_* function to use\n    :param seed_data: The argument to pass to the get_subgraph function\n    :param expand_nodes: Add the neighborhoods around all of these nodes\n    :param remove_nodes: Remove these nodes and all of their in/out edges\n    :rtype: Optional[pybel.BELGraph]\n    \"\"\"\n    # Seed by the given function\n    if seed_method == SEED_TYPE_INDUCTION:\n        result = get_subgraph_by_induction(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_PATHS:\n        result = get_subgraph_by_all_shortest_paths(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_NEIGHBORS:\n        result = get_subgraph_by_neighborhood(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_DOUBLE_NEIGHBORS:\n        result = get_subgraph_by_second_neighbors(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_UPSTREAM:\n        result = get_multi_causal_upstream(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_DOWNSTREAM:\n        result = get_multi_causal_downstream(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_PUBMED:\n        result = get_subgraph_by_pubmed(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_AUTHOR:\n        result = get_subgraph_by_authors(graph, seed_data)\n\n    elif seed_method == SEED_TYPE_ANNOTATION:\n        result = get_subgraph_by_annotations(graph, seed_data['annotations'], or_=seed_data.get('or'))\n\n    elif seed_method == SEED_TYPE_SAMPLE:\n        result = get_random_subgraph(\n            graph,\n            number_edges=seed_data.get('number_edges'),\n            seed=seed_data.get('seed')\n        )\n\n    elif not seed_method:  # Otherwise, don't seed a sub-graph\n        result = graph.copy()\n        log.debug('no seed function - using full network: %s', result.name)\n\n    else:\n        raise ValueError('Invalid seed method: {}'.format(seed_method))\n\n    if result is None:\n        log.debug('query returned no results')\n        return\n\n    log.debug('original graph has (%s nodes / %s edges)', result.number_of_nodes(), result.number_of_edges())\n\n    # Expand around the given nodes\n    if expand_nodes:\n        expand_nodes_neighborhoods(graph, result, expand_nodes)\n        log.debug('graph expanded to (%s nodes / %s edges)', result.number_of_nodes(), result.number_of_edges())\n\n    # Delete the given nodes\n    if remove_nodes:\n        for node in remove_nodes:\n            if node not in result:\n                log.debug('%s is not in graph %s', node, graph.name)\n                continue\n            result.remove_node(node)\n        log.debug('graph contracted to (%s nodes / %s edges)', result.number_of_nodes(), result.number_of_edges())\n\n    log.debug(\n        'Subgraph coming from %s (seed type) %s (data) contains %d nodes and %d edges',\n        seed_method,\n        seed_data,\n        result.number_of_nodes(),\n        result.number_of_edges()\n    )\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_pathologies_oop(graph):\n    rv = graph.copy()\n    victims = [\n        node\n        for node in rv\n        if node[FUNCTION] == PATHOLOGY\n    ]\n    rv.remove_nodes_from(victims)\n    return rv", "response": "Remove pathology nodes from the graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a random path from the graph as a list of nodes.", "response": "def get_random_path(graph) -> List[BaseEntity]:\n    \"\"\"Get a random path from the graph as a list of nodes.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    wg = graph.to_undirected()\n\n    nodes = wg.nodes()\n\n    def pick_random_pair() -> Tuple[BaseEntity, BaseEntity]:\n        \"\"\"Get a pair of random nodes.\"\"\"\n        return random.sample(nodes, k=2)\n\n    source, target = pick_random_pair()\n\n    tries = 0\n    sentinel_tries = 5\n\n    while not nx.has_path(wg, source, target) and tries < sentinel_tries:\n        tries += 1\n        source, target = pick_random_pair()\n\n    if tries == sentinel_tries:\n        return [source]\n\n    return nx.shortest_path(wg, source=source, target=target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a BEL graph to bytes with pickle.", "response": "def to_bytes(graph: BELGraph, protocol: int = HIGHEST_PROTOCOL) -> bytes:\n    \"\"\"Convert a graph to bytes with pickle.\n\n    Note that the pickle module has some incompatibilities between Python 2 and 3. To export a universally importable\n    pickle, choose 0, 1, or 2.\n\n    :param graph: A BEL network\n    :param protocol: Pickling protocol to use. Defaults to ``HIGHEST_PROTOCOL``.\n\n    .. seealso:: https://docs.python.org/3.6/library/pickle.html#data-stream-format\n    \"\"\"\n    raise_for_not_bel(graph)\n    return dumps(graph, protocol=protocol)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a graph from a bytes object.", "response": "def from_bytes(bytes_graph: bytes, check_version: bool = True) -> BELGraph:\n    \"\"\"Read a graph from bytes (the result of pickling the graph).\n\n    :param bytes_graph: File or filename to write\n    :param check_version: Checks if the graph was produced by this version of PyBEL\n    \"\"\"\n    graph = loads(bytes_graph)\n\n    raise_for_not_bel(graph)\n    if check_version:\n        raise_for_old_graph(graph)\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite this graph to a pickle file with networkx. write_gpickle.", "response": "def to_pickle(graph: BELGraph, file: Union[str, BinaryIO], protocol: int = HIGHEST_PROTOCOL) -> None:\n    \"\"\"Write this graph to a pickle object with :func:`networkx.write_gpickle`.\n\n    Note that the pickle module has some incompatibilities between Python 2 and 3. To export a universally importable\n    pickle, choose 0, 1, or 2.\n\n    :param graph: A BEL graph\n    :param file: A file or filename to write to\n    :param protocol: Pickling protocol to use. Defaults to ``HIGHEST_PROTOCOL``.\n\n    .. seealso:: https://docs.python.org/3.6/library/pickle.html#data-stream-format\n    \"\"\"\n    raise_for_not_bel(graph)\n    nx.write_gpickle(graph, file, protocol=protocol)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_pickle(path: Union[str, BinaryIO], check_version: bool = True) -> BELGraph:\n    graph = nx.read_gpickle(path)\n\n    raise_for_not_bel(graph)\n    if check_version:\n        raise_for_old_graph(graph)\n\n    return graph", "response": "Read a graph from a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmatching edges with the given dictionary as a sub - dictionary.", "response": "def _annotation_dict_all_filter(edge_data: EdgeData, query: Mapping[str, Iterable[str]]) -> bool:\n    \"\"\"Match edges with the given dictionary as a sub-dictionary.\n\n    :param query: The annotation query dict to match\n    \"\"\"\n    annotations = edge_data.get(ANNOTATIONS)\n\n    if annotations is None:\n        return False\n\n    for key, values in query.items():\n        ak = annotations.get(key)\n\n        if ak is None:\n            return False\n\n        for value in values:\n            if value not in ak:\n                return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_annotation_dict_all_filter(annotations: Mapping[str, Iterable[str]]) -> EdgePredicate:\n    if not annotations:\n        return keep_edge_permissive\n\n    @edge_predicate\n    def annotation_dict_all_filter(edge_data: EdgeData) -> bool:\n        \"\"\"Check if the all of the annotations in the enclosed query match.\"\"\"\n        return _annotation_dict_all_filter(edge_data, query=annotations)\n\n    return annotation_dict_all_filter", "response": "Build an edge predicate that checks if all of the annotations in the given dictionary match."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmatch edges with the given dictionary as a sub - dictionary.", "response": "def _annotation_dict_any_filter(edge_data: EdgeData, query: Mapping[str, Iterable[str]]) -> bool:\n    \"\"\"Match edges with the given dictionary as a sub-dictionary.\n\n    :param query: The annotation query dict to match\n    \"\"\"\n    annotations = edge_data.get(ANNOTATIONS)\n\n    if annotations is None:\n        return False\n\n    return any(\n        key in annotations and value in annotations[key]\n        for key, values in query.items()\n        for value in values\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild an edge predicate that passes for edges whose data dictionaries match the given annotation query dict.", "response": "def build_annotation_dict_any_filter(annotations: Mapping[str, Iterable[str]]) -> EdgePredicate:\n    \"\"\"Build an edge predicate that passes for edges whose data dictionaries match the given dictionary.\n\n    If the given dictionary is empty, will always evaluate to true.\n\n    :param annotations: The annotation query dict to match\n    \"\"\"\n    if not annotations:\n        return keep_edge_permissive\n\n    @edge_predicate\n    def annotation_dict_any_filter(edge_data: EdgeData) -> bool:\n        \"\"\"Check if the any of the annotations in the enclosed query match.\"\"\"\n        return _annotation_dict_any_filter(edge_data, query=annotations)\n\n    return annotation_dict_any_filter"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds an edge predicate that passes for relations for which one of the given nodes is the object.", "response": "def build_upstream_edge_predicate(nodes: Iterable[BaseEntity]) -> EdgePredicate:\n    \"\"\"Build an edge predicate that pass for relations for which one of the given nodes is the object.\"\"\"\n    nodes = set(nodes)\n\n    def upstream_filter(graph: BELGraph, u: BaseEntity, v: BaseEntity, k: str) -> bool:\n        \"\"\"Pass for relations for which one of the given nodes is the object.\"\"\"\n        return v in nodes and graph[u][v][k][RELATION] in CAUSAL_RELATIONS\n\n    return upstream_filter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild an edge predicate that passes for edges for which one of the given nodes is the subject.", "response": "def build_downstream_edge_predicate(nodes: Iterable[BaseEntity]) -> EdgePredicate:\n    \"\"\"Build an edge predicate that passes for edges for which one of the given nodes is the subject.\"\"\"\n    nodes = set(nodes)\n\n    def downstream_filter(graph: BELGraph, u: BaseEntity, v: BaseEntity, k: str) -> bool:\n        \"\"\"Pass for relations for which one of the given nodes is the subject.\"\"\"\n        return u in nodes and graph[u][v][k][RELATION] in CAUSAL_RELATIONS\n\n    return downstream_filter"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_relation_predicate(relations: Strings) -> EdgePredicate:\n    if isinstance(relations, str):\n        @edge_predicate\n        def relation_predicate(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for relations matching the enclosed value.\"\"\"\n            return edge_data[RELATION] == relations\n\n    elif isinstance(relations, Iterable):\n        relation_set = set(relations)\n\n        @edge_predicate\n        def relation_predicate(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for relations matching the enclosed values.\"\"\"\n            return edge_data[RELATION] in relation_set\n\n    else:\n        raise TypeError\n\n    return relation_predicate", "response": "Build an edge predicate that passes for edges with the given relation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_pmid_inclusion_filter(pmids: Strings) -> EdgePredicate:\n    if isinstance(pmids, str):\n        @edge_predicate\n        def pmid_inclusion_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with PubMed citations matching the contained PubMed identifier.\"\"\"\n            return has_pubmed(edge_data) and edge_data[CITATION][CITATION_REFERENCE] == pmids\n\n    elif isinstance(pmids, Iterable):\n        pmids = set(pmids)\n\n        @edge_predicate\n        def pmid_inclusion_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with PubMed citations matching one of the contained PubMed identifiers.\"\"\"\n            return has_pubmed(edge_data) and edge_data[CITATION][CITATION_REFERENCE] in pmids\n\n    else:\n        raise TypeError\n\n    return pmid_inclusion_filter", "response": "Build an edge predicate that passes for edges with citations from the given PubMed identifiers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_author_inclusion_filter(authors: Strings) -> EdgePredicate:\n    if isinstance(authors, str):\n        @edge_predicate\n        def author_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with citations with an author that matches the contained author.\"\"\"\n            return has_authors(edge_data) and authors in edge_data[CITATION][CITATION_AUTHORS]\n\n    elif isinstance(authors, Iterable):\n        authors = set(authors)\n\n        @edge_predicate\n        def author_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with citations with an author that matches one or more of the contained authors.\"\"\"\n            return has_authors(edge_data) and any(\n                author in edge_data[CITATION][CITATION_AUTHORS]\n                for author in authors\n            )\n\n    else:\n        raise TypeError\n\n    return author_filter", "response": "Build an edge predicate that passes for edges with citations written by the given author."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a transformation function under the given name.", "response": "def _register_function(name: str, func, universe: bool, in_place: bool):\n    \"\"\"Register a transformation function under the given name.\n\n    :param name: Name to register the function under\n    :param func: A function\n    :param universe:\n    :param in_place:\n    :return: The same function, with additional properties added\n    \"\"\"\n    if name in mapped:\n        mapped_func = mapped[name]\n        raise PipelineNameError('{name} is already registered with {func_mod}.{func_name}'.format(\n            name=name,\n            func_mod=mapped_func.__module__,\n            func_name=mapped_func.__name__\n        ))\n\n    mapped[name] = func\n\n    if universe:\n        universe_map[name] = func\n\n    if in_place:\n        in_place_map[name] = func\n\n    if _has_arguments(func, universe):\n        has_arguments_map[name] = func\n    else:\n        no_arguments_map[name] = func\n\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a decorator function to tag a transformation function.", "response": "def _build_register_function(universe: bool, in_place: bool):  # noqa: D202\n    \"\"\"Build a decorator function to tag transformation functions.\n\n    :param universe: Does the first positional argument of this function correspond to a universe graph?\n    :param in_place: Does this function return a new graph, or just modify it in-place?\n    \"\"\"\n\n    def register(func):\n        \"\"\"Tag a transformation function.\n\n        :param func: A function\n        :return: The same function, with additional properties added\n        \"\"\"\n        return _register_function(func.__name__, func, universe, in_place)\n\n    return register"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_deprecated(deprecated_name: str):\n    if deprecated_name in mapped:\n        raise DeprecationMappingError('function name already mapped. can not register as deprecated name.')\n\n    def register_deprecated_f(func):\n        name = func.__name__\n\n        log.debug('%s is deprecated. please migrate to %s', deprecated_name, name)\n\n        if name not in mapped:\n            raise MissingPipelineFunctionError('function not mapped with transformation, uni_transformation, etc.')\n\n        universe = name in universe_map\n        in_place = name in in_place_map\n\n        # Add back-reference from deprecated function name to actual function name\n        deprecated[deprecated_name] = name\n\n        return _register_function(deprecated_name, func, universe, in_place)\n\n    return register_deprecated_f", "response": "Register a function as deprecated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_transformation(name: str):\n    func = mapped.get(name)\n\n    if func is None:\n        raise MissingPipelineFunctionError('{} is not registered as a pipeline function'.format(name))\n\n    return func", "response": "Get a transformation function and error if its name is not registered."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _random_edge_iterator(graph, n_edges: int) -> Iterable[Tuple[BaseEntity, BaseEntity, int, Mapping]]:\n    edges = list(graph.edges())\n    edge_sample = random.sample(edges, n_edges)\n    for u, v in edge_sample:\n        keys = list(graph[u][v])\n        k = random.choice(keys)\n        yield u, v, k, graph[u][v][k]", "response": "Get a random set of edges from the graph and randomly samples a key from each."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a new graph from a seeding of edges.", "response": "def get_graph_with_random_edges(graph, n_edges: int):\n    \"\"\"Build a new graph from a seeding of edges.\n\n    :type graph: pybel.BELGraph\n    :param n_edges: Number of edges to randomly select from the given graph\n    :rtype: pybel.BELGraph\n    \"\"\"\n    result = graph.fresh_copy()\n    result.add_edges_from(_random_edge_iterator(graph, n_edges))\n\n    update_metadata(graph, result)\n    update_node_helper(graph, result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_random_node(graph,\n                    node_blacklist: Set[BaseEntity],\n                    invert_degrees: Optional[bool] = None,\n                    ) -> Optional[BaseEntity]:\n    \"\"\"Choose a node from the graph with probabilities based on their degrees.\n\n    :type graph: networkx.Graph\n    :param node_blacklist: Nodes to filter out\n    :param invert_degrees: Should the degrees be inverted? Defaults to true.\n    \"\"\"\n    try:\n        nodes, degrees = zip(*(\n            (node, degree)\n            for node, degree in sorted(graph.degree(), key=itemgetter(1))\n            if node not in node_blacklist\n        ))\n    except ValueError:  # something wrong with graph, probably no elements in graph.degree_iter\n        return\n\n    if invert_degrees is None or invert_degrees:\n        # More likely to choose low degree nodes to explore, so don't make hubs\n        degrees = [1 / degree for degree in degrees]\n\n    wrg = WeightedRandomGenerator(nodes, degrees)\n    return wrg.next()", "response": "Choose a random node from the graph with probabilities based on their degrees."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhelping build a random graph.", "response": "def _helper(result,\n            graph,\n            number_edges_remaining: int,\n            node_blacklist: Set[BaseEntity],\n            invert_degrees: Optional[bool] = None,\n            ):\n    \"\"\"Help build a random graph.\n\n    :type result: networkx.Graph\n    :type graph: networkx.Graph\n    \"\"\"\n    original_node_count = graph.number_of_nodes()\n\n    log.debug('adding remaining %d edges', number_edges_remaining)\n    for _ in range(number_edges_remaining):\n\n        source, possible_step_nodes, c = None, set(), 0\n        while not source or not possible_step_nodes:\n            source = get_random_node(result, node_blacklist, invert_degrees=invert_degrees)\n\n            c += 1\n            if c >= original_node_count:\n                log.warning('infinite loop happening')\n                log.warning('source: %s', source)\n                log.warning('no grow: %s', node_blacklist)\n                return  # Happens when after exhausting the connected components. Try increasing the number seed edges\n\n            if source is None:\n                continue  # maybe do something else?\n\n            # Only keep targets in the original graph that aren't in the result graph\n            possible_step_nodes = set(graph[source]) - set(result[source])\n\n            if not possible_step_nodes:\n                node_blacklist.add(\n                    source)  # there aren't any possible nodes to step to, so try growing from somewhere else\n\n        step_node = random.choice(list(possible_step_nodes))\n\n        # it's not really a big deal which, but it might be possible to weight this by the utility of edges later\n        key, attr_dict = random.choice(list(graph[source][step_node].items()))\n\n        result.add_edge(source, step_node, key=key, **attr_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a random subgraph based on weighted random walks from random seed edges.", "response": "def get_random_subgraph(graph, number_edges=None, number_seed_edges=None, seed=None, invert_degrees=None):\n    \"\"\"Generate a random subgraph based on weighted random walks from random seed edges.\n\n    :type graph: pybel.BELGraph graph\n    :param Optional[int] number_edges: Maximum number of edges. Defaults to\n     :data:`pybel_tools.constants.SAMPLE_RANDOM_EDGE_COUNT` (250).\n    :param Optional[int] number_seed_edges: Number of nodes to start with (which likely results in different components\n     in large graphs). Defaults to :data:`SAMPLE_RANDOM_EDGE_SEED_COUNT` (5).\n    :param Optional[int] seed: A seed for the random state\n    :param Optional[bool] invert_degrees: Should the degrees be inverted? Defaults to true.\n    :rtype: pybel.BELGraph\n    \"\"\"\n    if number_edges is None:\n        number_edges = SAMPLE_RANDOM_EDGE_COUNT\n\n    if number_seed_edges is None:\n        number_seed_edges = SAMPLE_RANDOM_EDGE_SEED_COUNT\n\n    if seed is not None:\n        random.seed(seed)\n\n    # Check if graph will sample full graph, and just return it if it would\n    if graph.number_of_edges() <= number_edges:\n        log.info('sampled full graph')\n        return graph.copy()\n\n    log.debug('getting random sub-graph with %d seed edges, %d final edges, and seed=%s', number_seed_edges,\n              number_edges, seed)\n\n    # Get initial graph with `number_seed_edges` edges\n    result = get_graph_with_random_edges(graph, number_seed_edges)\n\n    number_edges_remaining = number_edges - result.number_of_edges()\n    _helper(\n        result,\n        graph,\n        number_edges_remaining,\n        node_blacklist=set(),  # This is the set of nodes that should no longer be chosen to grow from\n        invert_degrees=invert_degrees,\n    )\n\n    log.debug('removing isolated nodes')\n    remove_isolated_nodes(result)\n\n    # update metadata\n    update_node_helper(graph, result)\n    update_metadata(graph, result)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a random index.", "response": "def next_index(self) -> int:\n        \"\"\"Get a random index.\"\"\"\n        return bisect.bisect_right(self.totals, random.random() * self.total)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(self, include_id: bool = False) -> Mapping[str, str]:\r\n        result = {\r\n            'keyword': self.keyword,\r\n            'name': self.name,\r\n            'version': self.version,\r\n        }\r\n\r\n        if self.url:\r\n            result['url'] = self.url\r\n        else:\r\n            result['pattern'] = self.pattern\r\n\r\n        if include_id:\r\n            result['id'] = self.id\r\n\r\n        return result", "response": "Return the most useful entries as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndescribe the namespaceEntry as a dictionary of Namespace - Keyword Name and Namespace - Identifier.", "response": "def to_json(self, include_id: bool = False) -> Mapping[str, str]:\r\n        \"\"\"Describe the namespaceEntry as dictionary of Namespace-Keyword and Name.\r\n\r\n        :param include_id: If true, includes the model identifier\r\n        \"\"\"\r\n        result = {\r\n            NAMESPACE: self.namespace.keyword,\r\n        }\r\n\r\n        if self.name:\r\n            result[NAME] = self.name\r\n\r\n        if self.identifier:\r\n            result[IDENTIFIER] = self.identifier\r\n\r\n        if include_id:\r\n            result['id'] = self.id\r\n\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning this network as JSON.", "response": "def to_json(self, include_id: bool = False):\r\n        \"\"\"Return this network as JSON.\r\n\r\n        :param include_id: If true, includes the model identifier\r\n        :rtype: dict[str,str]\r\n        \"\"\"\r\n        result = {\r\n            METADATA_NAME: self.name,\r\n            METADATA_VERSION: self.version,\r\n        }\r\n\r\n        if self.created:\r\n            result['created'] = str(self.created)\r\n\r\n        if include_id:\r\n            result['id'] = self.id\r\n\r\n        if self.authors:\r\n            result[METADATA_AUTHORS] = self.authors\r\n\r\n        if self.contact:\r\n            result[METADATA_CONTACT] = self.contact\r\n\r\n        if self.description:\r\n            result[METADATA_DESCRIPTION] = self.description\r\n\r\n        if self.copyright:\r\n            result[METADATA_COPYRIGHT] = self.copyright\r\n\r\n        if self.disclaimer:\r\n            result[METADATA_DISCLAIMER] = self.disclaimer\r\n\r\n        if self.licenses:\r\n            result[METADATA_LICENSES] = self.licenses\r\n\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fusion_to_json(self):\r\n        if self.p5_reference:\r\n            range_5p = fusion_range(\r\n                reference=str(self.p5_reference),\r\n                start=int_or_str(self.p5_start),\r\n                stop=int_or_str(self.p5_stop),\r\n            )\r\n        else:\r\n            range_5p = missing_fusion_range()\r\n\r\n        if self.p3_reference:\r\n            range_3p = fusion_range(\r\n                reference=str(self.p3_reference),\r\n                start=int_or_str(self.p3_start),\r\n                stop=int_or_str(self.p3_stop),\r\n            )\r\n        else:\r\n            range_3p = missing_fusion_range()\r\n\r\n        return {\r\n            PARTNER_5P: self.p5_partner.to_json(),  # just the identifier pair\r\n            PARTNER_3P: self.p3_partner.to_json(),  # just the identifier pair\r\n            RANGE_5P: range_5p,\r\n            RANGE_3P: range_3p,\r\n        }", "response": "Convert this modification to a FUSION data dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrecreates a is_variant dictionary for the given BELGraph.", "response": "def to_json(self):\r\n        \"\"\"Recreate a is_variant dictionary for :class:`BELGraph`.\r\n\r\n        :return: Dictionary that describes a variant or a fusion.\r\n        :rtype: Variant or FusionBase\r\n        \"\"\"\r\n        if self.type == FUSION:\r\n            return self._fusion_to_json()\r\n\r\n        if self.type == FRAGMENT:\r\n            return fragment(\r\n                start=int_or_str(self.p3_start),\r\n                stop=int_or_str(self.p3_stop),\r\n            )\r\n\r\n        if self.type == HGVS:\r\n            return hgvs(str(self.variantString))\r\n\r\n        if self.type == GMOD:\r\n            return gmod(\r\n                namespace=self.identifier.namespace.keyword,\r\n                name=self.identifier.name,\r\n                identifier=self.identifier.identifier,\r\n            )\r\n\r\n        if self.type == PMOD:\r\n            return pmod(\r\n                namespace=self.identifier.namespace.keyword,\r\n                name=self.identifier.name,\r\n                identifier=self.identifier.identifier,\r\n                code=self.residue,\r\n                position=self.position\r\n            )\r\n\r\n        raise TypeError('unhandled type ({}) for modification {}'.format(self.type, self))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _start_from_base_entity(base_entity):\r\n        return Node(\r\n            type=base_entity.function,\r\n            bel=base_entity.as_bel(),\r\n            sha512=base_entity.sha512,\r\n            data=json.dumps(base_entity),\r\n        )", "response": "Convert a base entity to a node model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an author by name automatically populating the hash.", "response": "def from_name(cls, name):\r\n        \"\"\"Create an author by name, automatically populating the hash.\"\"\"\r\n        return Author(name=name, sha512=cls.hash_name(name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_name_in(cls, names):\r\n        return cls.sha512.in_({\r\n            cls.hash_name(name)\r\n            for name in names\r\n        })", "response": "Build a filter if the author has any of the given names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary that is used to recreate the edge data dictionary of a BELGraph.", "response": "def to_json(self, include_id=False):\r\n        \"\"\"Create a citation dictionary that is used to recreate the edge data dictionary of a :class:`BELGraph`.\r\n\r\n        :param bool include_id: If true, includes the model identifier\r\n        :return: Citation dictionary for the recreation of a :class:`BELGraph`.\r\n        :rtype: dict[str,str]\r\n        \"\"\"\r\n        result = {\r\n            CITATION_REFERENCE: self.reference,\r\n            CITATION_TYPE: self.type\r\n        }\r\n\r\n        if include_id:\r\n            result['id'] = self.id\r\n\r\n        if self.name:\r\n            result[CITATION_NAME] = self.name\r\n\r\n        if self.title:\r\n            result[CITATION_TITLE] = self.title\r\n\r\n        if self.volume:\r\n            result[CITATION_VOLUME] = self.volume\r\n\r\n        if self.pages:\r\n            result[CITATION_PAGES] = self.pages\r\n\r\n        if self.date:\r\n            result[CITATION_DATE] = self.date.strftime('%Y-%m-%d')\r\n\r\n        if self.first:\r\n            result[CITATION_FIRST_AUTHOR] = self.first.name\r\n\r\n        if self.last:\r\n            result[CITATION_LAST_AUTHOR] = self.last.name\r\n\r\n        if self.authors:\r\n            result[CITATION_AUTHORS] = sorted(\r\n                author.name\r\n                for author in self.authors\r\n            )\r\n\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a dictionary that is used to recreate the edge data dictionary for a BELGraph.", "response": "def to_json(self, include_id: bool = False):\r\n        \"\"\"Create a dictionary that is used to recreate the edge data dictionary for a :class:`BELGraph`.\r\n\r\n        :param include_id: If true, includes the model identifier\r\n        :return: Dictionary containing citation and evidence for a :class:`BELGraph` edge.\r\n        :rtype: dict\r\n        \"\"\"\r\n        result = {\r\n            CITATION: self.citation.to_json(include_id=include_id),\r\n            EVIDENCE: self.text\r\n        }\r\n\r\n        if include_id:\r\n            result['id'] = self.id\r\n\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a property dictionary that is used to recreate an edge dictionary for a : class : BELGraph.", "response": "def to_json(self):\r\n        \"\"\"Create a property dict that is used to recreate an edge dictionary for a :class:`BELGraph`.\r\n\r\n        :return: Property dictionary of an edge that is participant (sub/obj) related.\r\n        :rtype: dict\r\n        \"\"\"\r\n        participant = self.side\r\n\r\n        prop_dict = {\r\n            participant: {\r\n                MODIFIER: self.modifier  # FIXME this is probably wrong for location\r\n            }\r\n        }\r\n\r\n        if self.modifier == LOCATION:\r\n            prop_dict[participant] = {\r\n                LOCATION: self.effect.to_json()\r\n            }\r\n        if self.relative_key:  # for translocations\r\n            prop_dict[participant][EFFECT] = {\r\n                self.relative_key: self.effect.to_json()\r\n            }\r\n        elif self.effect:  # for activities\r\n            prop_dict[participant][EFFECT] = self.effect.to_json()\r\n\r\n        # degradations don't have modifications\r\n\r\n        return prop_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_annotations_json(self):\r\n        annotations = defaultdict(dict)\r\n\r\n        for entry in self.annotations:\r\n            annotations[entry.namespace.keyword][entry.name] = True\r\n\r\n        return dict(annotations) or None", "response": "Format the annotations properly."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a dictionary of one BEL Edge that can be used to create an edge in a BEL graph.", "response": "def to_json(self, include_id=False):\r\n        \"\"\"Create a dictionary of one BEL Edge that can be used to create an edge in a :class:`BELGraph`.\r\n\r\n        :param bool include_id: Include the database identifier?\r\n        :return: Dictionary that contains information about an edge of a :class:`BELGraph`. Including participants\r\n                 and edge data information.\r\n        :rtype: dict\r\n        \"\"\"\r\n        source_dict = self.source.to_json()\r\n        source_dict['sha512'] = source_dict.sha512\r\n        target_dict = self.target.to_json()\r\n        target_dict['sha512'] = target_dict.sha512\r\n\r\n        result = {\r\n            'source': source_dict,\r\n            'target': target_dict,\r\n            'key': self.sha512,\r\n            'data': json.loads(self.data),\r\n        }\r\n\r\n        if include_id:\r\n            result['id'] = self.id\r\n\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting this edge into a BEL graph.", "response": "def insert_into_graph(self, graph):\r\n        \"\"\"Insert this edge into a BEL graph.\r\n\r\n        :param pybel.BELGraph graph: A BEL graph\r\n        \"\"\"\r\n        u = self.source.as_bel()\r\n        v = self.target.as_bel()\r\n\r\n        if self.evidence:\r\n            return graph.add_qualified_edge(u, v, **json.loads(self.data))\r\n        else:\r\n            return graph.add_unqualified_edge(u, v, self.relation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef raise_for_old_graph(graph):\n    graph_version = tokenize_version(graph.pybel_version)\n    if graph_version < PYBEL_MINIMUM_IMPORT_VERSION:\n        raise ImportVersionWarning(graph_version, PYBEL_MINIMUM_IMPORT_VERSION)", "response": "Raise an ImportVersionWarning if the BEL graph was produced by a legacy version of PyBEL."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefines a delimited list by enumerating each element of the list.", "response": "def nest(*content):\n    \"\"\"Define a delimited list by enumerating each element of the list.\"\"\"\n    if len(content) == 0:\n        raise ValueError('no arguments supplied')\n    return And([LPF, content[0]] + list(itt.chain.from_iterable(zip(itt.repeat(C), content[1:]))) + [RPF])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefine the tags usable in the BEL parser.", "response": "def one_of_tags(tags, canonical_tag, name=None):\n    \"\"\"Define the tags usable in the :class:`BelParser`.\n\n    For example, statements like ``g(HGNC:SNCA)`` can be expressed also as ``geneAbundance(HGNC:SNCA)``. The language\n    must define multiple different tags that get normalized to the same thing.\n\n    :param list[str] tags: a list of strings that are the tags for a function. For example, ['g', 'geneAbundance'] for the\n                    abundance of a gene\n    :param str canonical_tag: the preferred tag name. Does not have to be one of the tags. For example, 'GeneAbundance'\n                            (note capitalization) is used for the abundance of a gene\n    :param str name: this is the key under which the value for this tag is put in the PyParsing framework.\n    :rtype: :class:`pyparsing.ParseElement`\n    \"\"\"\n    element = oneOf(tags).setParseAction(replaceWith(canonical_tag))\n\n    if name is None:\n        return element\n\n    return element.setResultsName(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef triple(subject, relation, obj):\n    return And([Group(subject)(SUBJECT), relation(RELATION), Group(obj)(OBJECT)])", "response": "Build a simple triple in PyParsing that has a subject relation object format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a parser for protein truncations.", "response": "def get_truncation_language() -> ParserElement:\n    \"\"\"Build a parser for protein truncations.\"\"\"\n    l1 = truncation_tag + nest(amino_acid(AMINO_ACID) + ppc.integer(TRUNCATION_POSITION))\n    l1.setParseAction(_handle_trunc)\n    l2 = truncation_tag + nest(ppc.integer(TRUNCATION_POSITION))\n    l2.setParseAction(_handle_trunc_legacy)\n    return l1 | l2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aliases_categories(chr):\n    l = 0\n    r = len(categories_data['code_points_ranges']) - 1\n    c = ord(chr)\n\n    # binary search\n    while r >= l:\n        m = (l + r) // 2\n        if c < categories_data['code_points_ranges'][m][0]:\n            r = m - 1\n        elif c > categories_data['code_points_ranges'][m][1]:\n            l = m + 1\n        else:\n            return (\n                categories_data['iso_15924_aliases'][categories_data['code_points_ranges'][m][2]],\n                categories_data['categories'][categories_data['code_points_ranges'][m][3]])\n    return 'Unknown', 'Zzzz'", "response": "Retrieves the script block alias and unicode category for a unicode character."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_mixed_script(string, allowed_aliases=['COMMON']):\n    allowed_aliases = [a.upper() for a in allowed_aliases]\n    cats = unique_aliases(string) - set(allowed_aliases)\n    return len(cats) > 1", "response": "Checks if string contains mixed - scripts content excluding script blocks aliases in allowed_aliases."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a string contains any of the characters which might be confusable with the given unicode block.", "response": "def is_confusable(string, greedy=False, preferred_aliases=[]):\n    \"\"\"Checks if ``string`` contains characters which might be confusable with\n    characters from ``preferred_aliases``.\n\n    If ``greedy=False``, it will only return the first confusable character\n    found without looking at the rest of the string, ``greedy=True`` returns\n    all of them.\n\n    ``preferred_aliases=[]`` can take an array of unicode block aliases to\n    be considered as your 'base' unicode blocks:\n\n    -  considering ``pa\u03c1a``,\n\n       -  with ``preferred_aliases=['latin']``, the 3rd character ``\u03c1``\n          would be returned because this greek letter can be confused with\n          latin ``p``.\n       -  with ``preferred_aliases=['greek']``, the 1st character ``p``\n          would be returned because this latin letter can be confused with\n          greek ``\u03c1``.\n       -  with ``preferred_aliases=[]`` and ``greedy=True``, you'll discover\n          the 29 characters that can be confused with ``p``, the 23\n          characters that look like ``a``, and the one that looks like ``\u03c1``\n          (which is, of course, *p* aka *LATIN SMALL LETTER P*).\n\n    >>> confusables.is_confusable('pa\u03c1a', preferred_aliases=['latin'])[0]['character']\n    '\u03c1'\n    >>> confusables.is_confusable('pa\u03c1a', preferred_aliases=['greek'])[0]['character']\n    'p'\n    >>> confusables.is_confusable('Ab\u00e7', preferred_aliases=['latin'])\n    False\n    >>> confusables.is_confusable('Allo\u0393', preferred_aliases=['latin'])\n    False\n    >>> confusables.is_confusable('\u03c1\u03c4\u03c4', preferred_aliases=['greek'])\n    False\n    >>> confusables.is_confusable('\u03c1\u03c4.\u03c4', preferred_aliases=['greek', 'common'])\n    False\n    >>> confusables.is_confusable('\u03c1\u03c4\u03c4p')\n    [{'homoglyphs': [{'c': 'p', 'n': 'LATIN SMALL LETTER P'}], 'alias': 'GREEK', 'character': '\u03c1'}]\n\n    :param string: A unicode string\n    :type string: str\n    :param greedy: Don't stop on finding one confusable character - find all of them.\n    :type greedy: bool\n    :param preferred_aliases: Script blocks aliases which we don't want ``string``'s characters\n        to be confused with.\n    :type preferred_aliases: list(str)\n    :return: False if not confusable, all confusable characters and with what they are confusable\n        otherwise.\n    :rtype: bool or list\n    \"\"\"\n    preferred_aliases = [a.upper() for a in preferred_aliases]\n    outputs = []\n    checked = set()\n    for char in string:\n        if char in checked:\n            continue\n        checked.add(char)\n        char_alias = alias(char)\n        if char_alias in preferred_aliases:\n            # it's safe if the character might be confusable with homoglyphs from other\n            # categories than our preferred categories (=aliases)\n            continue\n        found = confusables_data.get(char, False)\n        if found is False:\n            continue\n        # character \u03bb is considered confusable if \u03bb can be confused with a character from\n        # preferred_aliases, e.g. if 'LATIN', '\u03c1' is confusable with 'p' from LATIN.\n        # if 'LATIN', '\u0393' is not confusable because in all the characters confusable with \u0393,\n        # none of them is LATIN.\n        if preferred_aliases:\n            potentially_confusable = []\n            try:\n                for d in found:\n                    aliases = [alias(glyph) for glyph in d['c']]\n                    for a in aliases:\n                        if a in preferred_aliases:\n                            potentially_confusable = found\n                            raise Found()\n            except Found:\n                pass\n        else:\n            potentially_confusable = found\n        if potentially_confusable:  # we found homoglyphs\n            output = {\n                'character': char,\n                'alias': char_alias,\n                'homoglyphs': potentially_confusable,\n            }\n            if not greedy:\n                return [output]\n            outputs.append(output)\n\n    return outputs or False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_categories():\n    # inspired by https://gist.github.com/anonymous/2204527\n    code_points_ranges = []\n    iso_15924_aliases = []\n    categories = []\n\n    match = re.compile(r'([0-9A-F]+)(?:\\.\\.([0-9A-F]+))?\\W+(\\w+)\\s*#\\s*(\\w+)',\n                       re.UNICODE)\n\n    url = 'ftp://ftp.unicode.org/Public/UNIDATA/Scripts.txt'\n    file = get(url)\n    for line in file:\n        p = re.findall(match, line)\n        if p:\n            code_point_range_from, code_point_range_to, alias, category = p[0]\n            alias = u(alias.upper())\n            category = u(category)\n            if alias not in iso_15924_aliases:\n                iso_15924_aliases.append(alias)\n            if category not in categories:\n                categories.append(category)\n            code_points_ranges.append((\n                int(code_point_range_from, 16),\n                int(code_point_range_to or code_point_range_from, 16),\n                iso_15924_aliases.index(alias), categories.index(category))\n            )\n    code_points_ranges.sort()\n\n    categories_data = {\n        'iso_15924_aliases': iso_15924_aliases,\n        'categories': categories,\n        'code_points_ranges': code_points_ranges,\n    }\n\n    dump('categories.json', categories_data)", "response": "Generates the categories. json file from the unicode specification."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the confusables. json file from the unicode specification.", "response": "def generate_confusables():\n    \"\"\"Generates the confusables JSON data file from the unicode specification.\n\n    :return: True for success, raises otherwise.\n    :rtype: bool\n    \"\"\"\n    url = 'ftp://ftp.unicode.org/Public/security/latest/confusables.txt'\n    file = get(url)\n    confusables_matrix = defaultdict(list)\n    match = re.compile(r'[0-9A-F ]+\\s+;\\s*[0-9A-F ]+\\s+;\\s*\\w+\\s*#'\n                       r'\\*?\\s*\\( (.+) \u2192 (.+) \\) (.+) \u2192 (.+)\\t#',\n                       re.UNICODE)\n    for line in file:\n        p = re.findall(match, line)\n        if p:\n            char1, char2, name1, name2 = p[0]\n            confusables_matrix[char1].append({\n                'c': char2,\n                'n': name2,\n            })\n            confusables_matrix[char2].append({\n                'c': char1,\n                'n': name1,\n            })\n\n    dump('confusables.json', dict(confusables_matrix))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef docstring_to_markdown(docstring):\n    new_docstring_lst = []\n\n    for idx, line in enumerate(docstring.split('\\n')):\n        line = line.strip()\n        if set(line) in ({'-'}, {'='}):\n            new_docstring_lst[idx-1] = '**%s**' % new_docstring_lst[idx-1]\n        elif line.startswith('>>>'):\n            line = '    %s' % line\n        new_docstring_lst.append(line)\n\n    for idx, line in enumerate(new_docstring_lst[1:]):\n        if line:\n            if line.startswith('Description : '):\n                new_docstring_lst[idx+1] = (new_docstring_lst[idx+1]\n                                            .replace('Description : ', ''))\n            elif ' : ' in line:\n                line = line.replace(' : ', '` : ')\n                new_docstring_lst[idx+1] = '\\n- `%s\\n' % line\n            elif '**' in new_docstring_lst[idx-1] and '**' not in line:\n                new_docstring_lst[idx+1] = '\\n%s' % line.lstrip()\n            elif '**' not in line:\n                new_docstring_lst[idx+1] = '    %s' % line.lstrip()\n\n    clean_lst = []\n    for line in new_docstring_lst:\n        if set(line.strip()) not in ({'-'}, {'='}):\n            clean_lst.append(line)\n    return clean_lst", "response": "Convert a Python object s docstring to markdown."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the markdown page of a Python object.", "response": "def object_to_markdownpage(obj_name, obj, s=''):\n    \"\"\"Generate the markdown documentation of a Python object.\n\n    Parameters\n    ----------\n    obj_name : str\n        Name of the Python object.\n    obj : object\n        Python object (class, method, function, ...)\n    s : str (default: '')\n        A string to which the documentation will be appended to.\n\n    Returns\n    ---------\n    s : str\n        The markdown page.\n\n    \"\"\"\n    # header\n    s += '## %s\\n' % obj_name\n\n    # function/class/method signature\n    sig = str(inspect.signature(obj)).replace('(self, ', '(')\n    s += '\\n*%s%s*\\n\\n' % (obj_name, sig)\n\n    # docstring body\n    doc = str(inspect.getdoc(obj))\n    ds = docstring_to_markdown(doc)\n    s += '\\n'.join(ds)\n\n    # document methods\n    if inspect.isclass(obj):\n        methods, properties = '\\n\\n### Methods', '\\n\\n### Properties'\n        members = inspect.getmembers(obj)\n        for m in members:\n            if not m[0].startswith('_') and len(m) >= 2:\n                if isinstance(m[1], property):\n                    properties += '\\n\\n<hr>\\n\\n*%s*\\n\\n' % m[0]\n                    m_doc = docstring_to_markdown(str(inspect.getdoc(m[1])))\n                    properties += '\\n'.join(m_doc)\n                else:\n                    sig = str(inspect.signature(m[1]))\n                    sig = sig.replace('(self, ', '(').replace('(self)', '()')\n                    sig = sig.replace('(self)', '()')\n                    methods += '\\n\\n<hr>\\n\\n*%s%s*\\n\\n' % (m[0], sig)\n                    m_doc = docstring_to_markdown(str(inspect.getdoc(m[1])))\n                    methods += '\\n'.join(m_doc)\n        s += methods\n        s += properties\n    return s + '\\n\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports a python package into the current namespace.", "response": "def import_package(rel_path_to_package, package_name):\n    \"\"\"Imports a python package into the current namespace.\n\n    Parameters\n    ----------\n    rel_path_to_package : str\n        Path to the package containing director relative from this script's\n        directory.\n    package_name : str\n        The name of the package to be imported.\n\n    Returns\n    ---------\n    package : The imported package object.\n\n    \"\"\"\n    try:\n        curr_dir = os.path.dirname(os.path.realpath(__file__))\n    except NameError:\n        curr_dir = os.path.dirname(os.path.realpath(os.getcwd()))\n    package_path = os.path.join(curr_dir, rel_path_to_package)\n    if package_path not in sys.path:\n        sys.path = [package_path] + sys.path\n    package = __import__(package_name)\n    return package"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a module level API documentation of a Python package.", "response": "def generate_api_docs(package, api_dir, clean=False, printlog=True):\n    \"\"\"Generate a module level API documentation of a python package.\n\n    Description\n    -----------\n    Generates markdown API files for each module in a Python package whereas\n    the structure is as follows:\n    `package/package.subpackage/package.subpackage.module.md`\n\n    Parameters\n    -----------\n    package : Python package object\n    api_dir : str\n        Output directory path for the top-level package directory\n    clean : bool (default: False)\n        Removes previously existing API directory if True.\n    printlog : bool (default: True)\n        Prints a progress log to the standard output screen if True.\n\n    \"\"\"\n    if printlog:\n        print('\\n\\nGenerating Module Files\\n%s\\n' % (50 * '='))\n\n    prefix = package.__name__ + \".\"\n\n    # clear the previous version\n    if clean:\n        if os.path.isdir(api_dir):\n            shutil.rmtree(api_dir)\n\n    # get subpackages\n    api_docs = {}\n    for importer, pkg_name, is_pkg in pkgutil.iter_modules(\n                                                           package.__path__,\n                                                           prefix):\n        if is_pkg:\n            subpackage = __import__(pkg_name, fromlist=\"dummy\")\n            prefix = subpackage.__name__ + \".\"\n\n            # get functions and classes\n            classes, functions = get_functions_and_classes(subpackage)\n\n            target_dir = os.path.join(api_dir, subpackage.__name__)\n\n            # create the subdirs\n            if not os.path.isdir(target_dir):\n                os.makedirs(target_dir)\n                if printlog:\n                    print('created %s' % target_dir)\n\n            # create markdown documents in memory\n            for obj in classes + functions:\n                md_path = os.path.join(target_dir, obj[0]) + '.md'\n                if md_path not in api_docs:\n                    api_docs[md_path] = object_to_markdownpage(obj_name=obj[0],\n                                                               obj=obj[1],\n                                                               s='')\n                else:\n                    api_docs[md_path] += object_to_markdownpage(obj_name=(\n                                                                obj[0]),\n                                                                obj=obj[1],\n                                                                s='')\n\n    # write to files\n    for d in sorted(api_docs):\n        prev = ''\n        if os.path.isfile(d):\n            with open(d, 'r') as f:\n                prev = f.read()\n            if prev == api_docs[d]:\n                msg = 'skipped'\n            else:\n                msg = 'updated'\n        else:\n            msg = 'created'\n\n        if msg != 'skipped':\n            with open(d, 'w') as f:\n                f.write(api_docs[d])\n\n        if printlog:\n            print('%s %s' % (msg, d))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef summarize_methdods_and_functions(api_modules, out_dir,\n                                     printlog=False, clean=True,\n                                     str_above_header=''):\n    \"\"\"Generates subpacke-level summary files.\n\n    Description\n    -----------\n    A function to generate subpacke-level summary markdown API files from\n    a module-level API documentation previously created via the\n    `generate_api_docs` function.\n    The output structure is:\n        package/package.subpackage.md\n\n    Parameters\n    ----------\n    api_modules : str\n        Path to the API documentation crated via `generate_api_docs`\n    out_dir : str\n        Path to the desired output directory for the new markdown files.\n    clean : bool (default: False)\n        Removes previously existing API directory if True.\n    printlog : bool (default: True)\n        Prints a progress log to the standard output screen if True.\n    str_above_header : str (default: '')\n        Places a string just above the header.\n\n    \"\"\"\n    if printlog:\n        print('\\n\\nGenerating Subpackage Files\\n%s\\n' % (50 * '='))\n\n    if clean:\n        if os.path.isdir(out_dir):\n            shutil.rmtree(out_dir)\n\n    if not os.path.isdir(out_dir):\n        os.mkdir(out_dir)\n        if printlog:\n            print('created %s' % out_dir)\n\n    subdir_paths = [os.path.join(api_modules, d)\n                    for d in os.listdir(api_modules)\n                    if not d.startswith('.')]\n\n    out_files = [os.path.join(out_dir, os.path.basename(d)) + '.md'\n                 for d in subdir_paths]\n\n    for sub_p, out_f in zip(subdir_paths, out_files):\n        module_paths = (os.path.join(sub_p, m)\n                        for m in os.listdir(sub_p)\n                        if not m.startswith('.'))\n\n        new_output = []\n        if str_above_header:\n            new_output.append(str_above_header)\n        for p in module_paths:\n            with open(p, 'r') as r:\n                new_output.extend(r.readlines())\n\n        msg = ''\n        if not os.path.isfile(out_f):\n            msg = 'created'\n\n        if msg != 'created':\n            with open(out_f, 'r') as f:\n                prev = f.readlines()\n            if prev != new_output:\n                msg = 'updated'\n            else:\n                msg = 'skipped'\n\n        if msg != 'skipped':\n            with open(out_f, 'w') as f:\n                f.write(''.join(new_output))\n\n        if printlog:\n            print('%s %s' % (msg, out_f))", "response": "Generates a subpacke - level summary file from a list of API modules and a list of functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the PDB files from local drive and returns a new instance of this class.", "response": "def read_pdb(self, path):\n        \"\"\"Read PDB files (unzipped or gzipped) from local drive\n\n        Attributes\n        ----------\n        path : str\n            Path to the PDB file in .pdb format or gzipped format (.pdb.gz).\n\n        Returns\n        ---------\n        self\n\n        \"\"\"\n        self.pdb_path, self.pdb_text = self._read_pdb(path=path)\n        self._df = self._construct_df(pdb_lines=self.pdb_text.splitlines(True))\n        self.header, self.code = self._parse_header_code()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the contents of the specified PDB file from the Protein Databank at rcsb. org.", "response": "def fetch_pdb(self, pdb_code):\n        \"\"\"Fetches PDB file contents from the Protein Databank at rcsb.org.\n\n        Parameters\n        ----------\n        pdb_code : str\n            A 4-letter PDB code, e.g., \"3eiy\".\n\n        Returns\n        ---------\n        self\n\n        \"\"\"\n        self.pdb_path, self.pdb_text = self._fetch_pdb(pdb_code)\n        self._df = self._construct_df(pdb_lines=self.pdb_text.splitlines(True))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters PDB DataFrames by properties.", "response": "def get(self, s, df=None, invert=False, records=('ATOM', 'HETATM')):\n        \"\"\"Filter PDB DataFrames by properties\n\n        Parameters\n        ----------\n        s : str  in {'main chain', 'hydrogen', 'c-alpha', 'heavy'}\n            String to specify which entries to return.\n\n        df : pandas.DataFrame, default: None\n            Optional DataFrame to perform the filter operation on.\n            If df=None, filters on self.df['ATOM'].\n\n        invert : bool, default: True\n            Inverts the search query. For example if s='hydrogen' and\n            invert=True, all but hydrogen entries are returned.\n\n        records : iterable, default: ('ATOM', 'HETATM')\n            Specify which record sections to consider. For example, to consider\n            both protein and ligand atoms, set `records=('ATOM', 'HETATM')`.\n            This setting is ignored if `df` is not set to None.\n            For downward compatibility, a string argument is still supported\n            but deprecated and will be removed in future versions.\n\n        Returns\n        --------\n        df : pandas.DataFrame\n            Returns a DataFrame view on the filtered entries.\n\n        \"\"\"\n        if isinstance(records, str):\n            warnings.warn('Using a string as `records` argument is '\n                          'deprecated and will not be supported in future'\n                          ' versions. Please use a tuple or'\n                          ' other iterable instead', DeprecationWarning)\n            records = (records,)\n\n        if not self._get_dict:\n            self._get_dict = self._init_get_dict()\n        if s not in self._get_dict.keys():\n            raise AttributeError('s must be in %s' % self._get_dict.keys())\n        if not df:\n            df = pd.concat(objs=[self.df[i] for i in records])\n        return self._get_dict[s](df, invert=invert)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimpute element_symbol from atom_name section.", "response": "def impute_element(self, records=('ATOM', 'HETATM'), inplace=False):\n        \"\"\"Impute element_symbol from atom_name section.\n\n        Parameters\n        ----------\n        records : iterable, default: ('ATOM', 'HETATM')\n            Coordinate sections for which the element symbols should be\n            imputed.\n\n        inplace : bool, (default: False\n            Performs the operation in-place if True and returns a copy of the\n            PDB DataFrame otherwise.\n\n        Returns\n        ---------\n        DataFrame\n\n        \"\"\"\n        if inplace:\n            t = self.df\n        else:\n            t = self.df.copy()\n            for d in self.df:\n                t[d] = self.df[d].copy()\n\n        for sec in records:\n            t[sec]['element_symbol'] = \\\n                t[sec][['atom_name', 'element_symbol']].\\\n                apply(lambda x: x[0][1]\n                      if len(x[1]) == 3\n                      else x[0][0], axis=1)\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rmsd(df1, df2, s=None, invert=False):\n        if df1.shape[0] != df2.shape[0]:\n            raise AttributeError('DataFrames have unequal lengths')\n        get_dict = PandasPdb._init_get_dict()\n        if s:\n            if s not in get_dict.keys():\n                raise AttributeError('s must be in '\n                                     '%s or None' % get_dict.keys())\n            df1 = get_dict[s](df1, invert=invert)\n            df2 = get_dict[s](df2, invert=invert)\n\n        total = ((df1['x_coord'].values - df2['x_coord'].values)**2 +\n                 (df1['y_coord'].values - df2['y_coord'].values)**2 +\n                 (df1['z_coord'].values - df2['z_coord'].values)**2)\n        rmsd = round((total.sum() / df1.shape[0])**0.5, 4)\n        return rmsd", "response": "Compute the Root Mean Square Deviation between two DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing dictionary for filter operations.", "response": "def _init_get_dict():\n        \"\"\"Initialize dictionary for filter operations.\"\"\"\n        get_dict = {'main chain': PandasPdb._get_mainchain,\n                    'hydrogen': PandasPdb._get_hydrogen,\n                    'c-alpha': PandasPdb._get_calpha,\n                    'carbon': PandasPdb._get_carbon,\n                    'heavy': PandasPdb._get_heavy}\n        return get_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the PDB file from local drive.", "response": "def _read_pdb(path):\n        \"\"\"Read PDB file from local drive.\"\"\"\n        r_mode = 'r'\n        openf = open\n        if path.endswith('.gz'):\n            r_mode = 'rb'\n            openf = gzip.open\n        with openf(path, r_mode) as f:\n            txt = f.read()\n        if path.endswith('.gz'):\n            if sys.version_info[0] >= 3:\n                txt = txt.decode('utf-8')\n            else:\n                txt = txt.encode('ascii')\n        return path, txt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fetch_pdb(pdb_code):\n        txt = None\n        url = 'http://www.rcsb.org/pdb/files/%s.pdb' % pdb_code.lower()\n        try:\n            response = urlopen(url)\n            txt = response.read()\n            if sys.version_info[0] >= 3:\n                txt = txt.decode('utf-8')\n            else:\n                txt = txt.encode('ascii')\n        except HTTPError as e:\n            print('HTTP Error %s' % e.code)\n        except URLError as e:\n            print('URL Error %s' % e.args)\n        return url, txt", "response": "Load the PDB file from rcsb. org."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_header_code(self):\n        code, header = '', ''\n        if 'OTHERS' in self.df:\n\n            header = (self.df['OTHERS'][self.df['OTHERS']['record_name'] ==\n                      'HEADER'])\n            if not header.empty:\n                header = header['entry'].values[0]\n                s = header.split()\n                if s:\n                    code = s[-1].lower()\n        return header, code", "response": "Extract header information and PDB code."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_mainchain(df, invert):\n        if invert:\n            mc = df[(df['atom_name'] != 'C') &\n                    (df['atom_name'] != 'O') &\n                    (df['atom_name'] != 'N') &\n                    (df['atom_name'] != 'CA')]\n        else:\n            mc = df[(df['atom_name'] == 'C') |\n                    (df['atom_name'] == 'O') |\n                    (df['atom_name'] == 'N') |\n                    (df['atom_name'] == 'CA')]\n        return mc", "response": "Return only main chain atom entries from a DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct DataFrames from list of PDB lines.", "response": "def _construct_df(pdb_lines):\n        \"\"\"Construct DataFrames from list of PDB lines.\"\"\"\n        valids = tuple(pdb_records.keys())\n        line_lists = {r: [] for r in valids}\n        line_lists['OTHERS'] = []\n        for line_num, line in enumerate(pdb_lines):\n            if line.strip():\n                if line.startswith(valids):\n                    record = line[:6].rstrip()\n                    line_ele = ['' for _ in range(len(\n                        pdb_records[record]) + 1)]\n                    for idx, ele in enumerate(pdb_records[record]):\n                        line_ele[idx] = (line[ele['line'][0]:ele['line'][1]]\n                                         .strip())\n                    line_ele[-1] = line_num\n                    line_lists[record].append(line_ele)\n                else:\n                    line_lists['OTHERS'].append([line[:6].rstrip(),\n                                                line[6:-1].rstrip(), line_num])\n\n        dfs = {}\n        for r in line_lists.items():\n            df = pd.DataFrame(r[1], columns=[c['id'] for c in\n                                             pdb_records[r[0]]] + ['line_idx'])\n            for c in pdb_records[r[0]]:\n                try:\n                    df[c['id']] = df[c['id']].astype(c['type'])\n                except ValueError:\n                    # expect ValueError if float/int columns are empty strings\n                    df[c['id']] = pd.Series(np.nan, index=df.index)\n\n            dfs[r[0]] = df\n        return dfs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef amino3to1(self, record='ATOM',\n                  residue_col='residue_name', fillna='?'):\n        \"\"\"Creates 1-letter amino acid codes from DataFrame\n\n        Non-canonical amino-acids are converted as follows:\n        ASH (protonated ASP) => D\n        CYX (disulfide-bonded CYS) => C\n        GLH (protonated GLU) => E\n        HID/HIE/HIP (different protonation states of HIS) = H\n        HYP (hydroxyproline) => P\n        MSE (selenomethionine) => M\n\n        Parameters\n        ----------\n        record : str, default: 'ATOM'\n            Specfies the record DataFrame.\n        residue_col : str,  default: 'residue_name'\n            Column in `record` DataFrame to look for 3-letter amino acid\n            codes for the conversion.\n        fillna : str, default: '?'\n            Placeholder string to use for unknown amino acids.\n\n        Returns\n        ---------\n        pandas.DataFrame : Pandas DataFrame object consisting of two columns,\n            `'chain_id'` and `'residue_name'`, where the former contains\n            the chain ID of the amino acid and the latter\n            contains the 1-letter amino acid code, respectively.\n\n        \"\"\"\n        tmp = self.df[record]\n        cmp = 'placeholder'\n        indices = []\n\n        for num, ind in zip(tmp['residue_number'], np.arange(tmp.shape[0])):\n            if num != cmp:\n                indices.append(ind)\n            cmp = num\n\n        transl = tmp.iloc[indices][residue_col].map(\n            amino3to1dict).fillna(fillna)\n\n        return pd.concat((tmp.iloc[indices]['chain_id'], transl), axis=1)", "response": "Returns a Pandas DataFrame containing amino acid codes for the given amino acid record and residue name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the Euclidean distance between atoms and a 3D point.", "response": "def distance(self, xyz=(0.00, 0.00, 0.00), records=('ATOM', 'HETATM')):\n        \"\"\"Computes Euclidean distance between atoms and a 3D point.\n\n        Parameters\n        ----------\n        xyz : tuple, default: (0.00, 0.00, 0.00)\n            X, Y, and Z coordinate of the reference center for the distance\n            computation.\n        records : iterable, default: ('ATOM', 'HETATM')\n            Specify which record sections to consider. For example, to consider\n            both protein and ligand atoms, set `records=('ATOM', 'HETATM')`.\n            This setting is ignored if `df` is not set to None.\n            For downward compatibility, a string argument is still supported\n            but deprecated and will be removed in future versions.\n\n        Returns\n        ---------\n        pandas.Series : Pandas Series object containing the Euclidean\n            distance between the atoms in the record section and `xyz`.\n\n        \"\"\"\n\n        if isinstance(records, str):\n            warnings.warn('Using a string as `records` argument is '\n                          'deprecated and will not be supported in future'\n                          ' versions. Please use a tuple or'\n                          ' other iterable instead', DeprecationWarning)\n            records = (records,)\n\n        df = pd.concat(objs=[self.df[i] for i in records])\n\n        return np.sqrt(np.sum(df[[\n            'x_coord', 'y_coord', 'z_coord']]\n            .subtract(xyz, axis=1)**2, axis=1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_pdb(self, path, records=None, gz=False, append_newline=True):\n        if gz:\n            openf = gzip.open\n            w_mode = 'wt'\n        else:\n            openf = open\n            w_mode = 'w'\n        if not records:\n            records = self.df.keys()\n\n        dfs = {r: self.df[r].copy() for r in records if not self.df[r].empty}\n\n        for r in dfs.keys():\n            for col in pdb_records[r]:\n                dfs[r][col['id']] = dfs[r][col['id']].apply(col['strf'])\n                dfs[r]['OUT'] = pd.Series('', index=dfs[r].index)\n\n            for c in dfs[r].columns:\n                if c in {'line_idx', 'OUT'}:\n                    pass\n                elif r in {'ATOM', 'HETATM'} and c not in pdb_df_columns:\n                    warn('Column %s is not an expected column and'\n                         ' will be skipped.' % c)\n                else:\n                    dfs[r]['OUT'] = dfs[r]['OUT'] + dfs[r][c]\n\n        if pd_version < LooseVersion('0.17.0'):\n            warn(\"You are using an old pandas version (< 0.17)\"\n                 \" that relies on the old sorting syntax.\"\n                 \" Please consider updating your pandas\"\n                 \" installation to a more recent version.\",\n                 DeprecationWarning)\n            df.sort(columns='line_idx', inplace=True)\n\n        elif pd_version < LooseVersion('0.23.0'):\n            df = pd.concat(dfs)\n\n        else:\n            df = pd.concat(dfs, sort=False)\n\n        df.sort_values(by='line_idx', inplace=True)\n\n        with openf(path, w_mode) as f:\n\n            s = df['OUT'].tolist()\n            for idx in range(len(s)):\n                if len(s[idx]) < 80:\n                    s[idx] = '%s%s' % (s[idx], ' ' * (80 - len(s[idx])))\n            to_write = '\\n'.join(s)\n            f.write(to_write)\n            if append_newline:\n                if gz:\n                    f.write('\\n')\n                else:\n                    f.write('\\n')", "response": "Write record DataFrames to a PDB file or gzipped PDB file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads mol2 contents into self. _df", "response": "def _load_mol2(self, mol2_lines, mol2_code, columns):\n        \"\"\"Load mol2 contents into assert_raise_message instance\"\"\"\n        if columns is None:\n            col_names = COLUMN_NAMES\n            col_types = COLUMN_TYPES\n        else:\n            col_names, col_types = [], []\n            for i in range(len(columns)):\n                col_names.append(columns[i][0])\n                col_types.append(columns[i][1])\n\n        try:\n            self.mol2_text = ''.join(mol2_lines)\n            self.code = mol2_code\n        except TypeError:\n            mol2_lines = [m.decode() for m in mol2_lines]\n            self.mol2_text = ''.join(mol2_lines)\n            self.code = mol2_code.decode()\n\n        self._df = self._construct_df(mol2_lines, col_names, col_types)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading Mol2 files into a DataFrame containing the relevant attributes.", "response": "def read_mol2(self, path, columns=None):\n        \"\"\"Reads Mol2 files (unzipped or gzipped) from local drive\n\n        Note that if your mol2 file contains more than one molecule,\n        only the first molecule is loaded into the DataFrame\n\n        Attributes\n        ----------\n        path : str\n            Path to the Mol2 file in .mol2 format or gzipped format (.mol2.gz)\n\n        columns : dict or None (default: None)\n            If None, this methods expects a 9-column ATOM section that contains\n            the following columns:\n\n            {0:('atom_id', int), 1:('atom_name', str),\n             2:('x', float), 3:('y', float), 4:('z', float),\n             5:('atom_type', str), 6:('subst_id', int),\n             7:('subst_name', str), 8:('charge', float)}\n\n            If your Mol2 files are formatted differently, you can provide your\n            own column_mapping dictionary in a format similar to the one above.\n            However, note that not all assert_raise_message methods\n            may be supported then.\n\n        Returns\n        ---------\n        self\n\n        \"\"\"\n        mol2_code, mol2_lines = next(split_multimol2(path))\n        self._load_mol2(mol2_lines, mol2_code, columns)\n        self.mol2_path = path\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread Mol2 file from a list into a DataFrames object.", "response": "def read_mol2_from_list(self, mol2_lines, mol2_code, columns=None):\n        r\"\"\"Reads Mol2 file from a list into DataFrames\n\n        Attributes\n        ----------\n        mol2_lines : list\n            A list of lines containing the mol2 file contents. For example,\n            ['@<TRIPOS>MOLECULE\\n',\n             'ZINC38611810\\n',\n             '   65    68     0     0     0\\n',\n             'SMALL\\n',\n             'NO_CHARGES\\n',\n             '\\n',\n             '@<TRIPOS>ATOM\\n',\n             '      1 C1  -1.1786  2.7011  -4.0323 C.3  1 <0>   -0.1537\\n',\n             '      2 C2  -1.2950  1.2442  -3.5798 C.3  1 <0>   -0.1156\\n',\n             ...]\n\n        mol2_code : str or None\n            Name or ID of the molecule.\n\n        columns : dict or None (default: None)\n            If None, this methods expects a 9-column ATOM section that contains\n            the following columns:\n            {0:('atom_id', int), 1:('atom_name', str),\n             2:('x', float), 3:('y', float), 4:('z', float),\n             5:('atom_type', str), 6:('subst_id', int),\n             7:('subst_name', str), 8:('charge', float)}\n            If your Mol2 files are formatted differently, you can provide your\n            own column_mapping dictionary in a format similar to the one above.\n            However, note that not all assert_raise_message methods may be\n            supported then.\n\n        Returns\n        ---------\n        self\n\n        \"\"\"\n        self._load_mol2(mol2_lines, mol2_code, columns)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _construct_df(self, mol2_lines, col_names, col_types):\n        return self._atomsection_to_pandas(self._get_atomsection(mol2_lines),\n                                           col_names=col_names,\n                                           col_types=col_types)", "response": "Construct DataFrames from list of PDB lines."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn atom section from mol2 provided as list of strings", "response": "def _get_atomsection(mol2_lst):\n        \"\"\"Returns atom section from mol2 provided as list of strings\"\"\"\n        started = False\n        for idx, s in enumerate(mol2_lst):\n            if s.startswith('@<TRIPOS>ATOM'):\n                first_idx = idx + 1\n                started = True\n            elif started and s.startswith('@<TRIPOS>'):\n                last_idx_plus1 = idx\n                break\n        return mol2_lst[first_idx:last_idx_plus1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the Root Mean Square Deviation between two molecules and return the RMSD of the two molecules.", "response": "def rmsd(df1, df2, heavy_only=True):\n        \"\"\"Compute the Root Mean Square Deviation between molecules\n\n        Parameters\n        ----------\n        df1 : pandas.DataFrame\n            DataFrame with HETATM, ATOM, and/or ANISOU entries\n        df2 : pandas.DataFrame\n            Second DataFrame for RMSD computation against df1. Must have the\n            same number of entries as df1\n        heavy_only : bool (default: True)\n            Which atoms to compare to compute the RMSD. If `True` (default),\n            computes the RMSD between non-hydrogen atoms only.\n\n        Returns\n        ---------\n        rmsd : float\n            Root Mean Square Deviation between df1 and df2\n\n        \"\"\"\n        if df1.shape[0] != df2.shape[0]:\n            raise AttributeError('DataFrames have unequal lengths')\n\n        if heavy_only:\n            d1 = df1[df1['atom_type'] != 'H']\n            d2 = df2[df2['atom_type'] != 'H']\n        else:\n            d1, d2 = df1, df2\n\n        total = ((d1['x'].values - d2['x'].values)**2 +\n                 (d1['y'].values - d2['y'].values)**2 +\n                 (d1['z'].values - d2['z'].values)**2)\n        rmsd = round((total.sum() / df1.shape[0])**0.5, 4)\n        return rmsd"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the Euclidean distance between atoms in self. df and a 3D point.", "response": "def distance(self, xyz=(0.00, 0.00, 0.00)):\n        \"\"\"Computes Euclidean distance between atoms in\n            self.df and a 3D point.\n\n        Parameters\n        ----------\n        xyz : tuple (0.00, 0.00, 0.00)\n            X, Y, and Z coordinate of the reference center for the distance\n            computation\n\n        Returns\n        ---------\n        pandas.Series : Pandas Series object containing the Euclidean\n            distance between the atoms in the atom section and `xyz`.\n\n        \"\"\"\n        return np.sqrt(np.sum(self.df[['x', 'y', 'z']]\n                       .subtract(xyz, axis=1)**2, axis=1))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the Euclidean distance between atoms and a 3D point.", "response": "def distance_df(df, xyz=(0.00, 0.00, 0.00)):\n        \"\"\"Computes Euclidean distance between atoms and a 3D point.\n\n        Parameters\n        ----------\n        df : DataFrame\n            DataFrame containing entries similar to the PandasMol2.df\n            format for the\n            the distance computation to the `xyz` reference coordinates.\n        xyz : tuple (0.00, 0.00, 0.00)\n            X, Y, and Z coordinate of the reference center for the distance\n            computation\n\n        Returns\n        ---------\n        pandas.Series : Pandas Series object containing the Euclidean\n            distance between the atoms in the atom section and `xyz`.\n\n        \"\"\"\n\n        return np.sqrt(np.sum(df[['x', 'y', 'z']]\n                       .subtract(xyz, axis=1)**2, axis=1))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_yaml_file(self, path):\n        with open(path, 'r') as fp:\n            data = yaml.safe_load(fp)\n\n        if not data:\n            return {}\n\n        def traverse(namespace, d):\n            cfg = {}\n            for key, val in d.items():\n                if isinstance(d[key], dict):\n                    cfg.update(traverse(namespace + [key], d[key]))\n                else:\n                    if not isinstance(val, str):\n                        # All values should be double-quoted strings so they\n                        # parse as strings; anything else is a configuration\n                        # error at parse-time\n                        raise ConfigurationError(\n                            'Invalid value %r in file %s: values must be double-quoted strings' % (\n                                val, path\n                            )\n                        )\n\n                    cfg['_'.join(namespace + [key]).upper()] = val\n\n            return cfg\n\n        return traverse([], data)", "response": "Parse a yaml file at path and return a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves value for key.", "response": "def get(self, key, namespace=None):\n        \"\"\"Retrieve value for key.\"\"\"\n        if not self.path:\n            return NO_VALUE\n\n        logger.debug('Searching %r for key: %s, namepsace: %s', self, key, namespace)\n        full_key = generate_uppercase_key(key, namespace)\n        return get_key_from_envs(self.cfg, full_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_class(clspath):\n    modpath, clsname = split_clspath(clspath)\n    __import__(modpath)\n    module = sys.modules[modpath]\n    return getattr(module, clsname)", "response": "Given a clspath returns the class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating arg value as upper lower or None.", "response": "def upper_lower_none(arg):\n    \"\"\"Validate arg value as \"upper\", \"lower\", or None.\"\"\"\n    if not arg:\n        return arg\n\n    arg = arg.strip().lower()\n    if arg in ['upper', 'lower']:\n        return arg\n\n    raise ValueError('argument must be \"upper\", \"lower\" or None')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(app):\n    app.add_domain(EverettDomain)\n    app.add_directive('autocomponent', AutoComponentDirective)\n\n    return {\n        'version': __version__,\n        'parallel_read_safe': True,\n        'parallel_write_safe': True\n    }", "response": "Register domain and directive in Sphinx."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_signature(self, sig, signode):\n        if sig != 'Configuration':\n            signode.clear()\n\n            # Add \"component\" which is the type of this thing\n            signode += addnodes.desc_annotation('component ', 'component ')\n\n            if '.' in sig:\n                modname, clsname = sig.rsplit('.', 1)\n            else:\n                modname, clsname = '', sig\n\n            # If there's a module name, then we add the module\n            if modname:\n                signode += addnodes.desc_addname(modname + '.', modname + '.')\n\n            # Add the class name\n            signode += addnodes.desc_name(clsname, clsname)\n        else:\n            # Add just \"Configuration\"\n            signode += addnodes.desc_name(sig, sig)\n\n        return sig", "response": "Create a signature for this thing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a target and index for this thing.", "response": "def add_target_and_index(self, name, sig, signode):\n        \"\"\"Add a target and index for this thing.\"\"\"\n        targetname = '%s-%s' % (self.objtype, name)\n\n        if targetname not in self.state.document.ids:\n            signode['names'].append(targetname)\n            signode['ids'].append(targetname)\n            signode['first'] = (not self.names)\n            self.state.document.note_explicit_target(signode)\n\n            objects = self.env.domaindata['everett']['objects']\n            key = (self.objtype, name)\n            if key in objects:\n                self.state_machine.reporter.warning(\n                    'duplicate description of %s %s, ' % (self.objtype, name) +\n                    'other instance in ' + self.env.doc2path(objects[key]),\n                    line=self.lineno\n                )\n            objects[key] = self.env.docname\n\n        indextext = _('%s (component)') % name\n        self.indexnode['entries'].append(('single', indextext, targetname, '', None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_line(self, line, source, *lineno):\n        self.result.append(line, source, *lineno)", "response": "Add a line to the result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_docs(self, clspath, more_content):\n        obj = import_class(clspath)\n        sourcename = 'docstring of %s' % clspath\n        all_options = []\n        indent = '    '\n        config = obj.get_required_config()\n\n        if config.options:\n            # Go through options and figure out relevant information\n            for option in config:\n                if 'namespace' in self.options:\n                    namespaced_key = self.options['namespace'] + '_' + option.key\n                else:\n                    namespaced_key = option.key\n\n                if 'case' in self.options:\n                    if self.options['case'] == 'upper':\n                        namespaced_key = namespaced_key.upper()\n                    elif self.options['case'] == 'lower':\n                        namespaced_key = namespaced_key.lower()\n\n                all_options.append({\n                    'key': namespaced_key,\n                    'parser': qualname(option.parser),\n                    'doc': option.doc,\n                    'default': option.default,\n                })\n\n        if 'hide-classname' not in self.options:\n            modname, clsname = split_clspath(clspath)\n            component_name = clspath\n            component_index = clsname\n        else:\n            component_name = 'Configuration'\n            component_index = 'Configuration'\n\n        if all_options:\n            # Add index entries for options first so they link to the right\n            # place; we do it this way so that we don't have to make options a\n            # real object type and then we don't get to use TypedField\n            # formatting\n            self.add_line('.. index::', sourcename)\n            for option in all_options:\n                self.add_line('   single: %s; (%s)' % (option['key'], component_index), sourcename)\n            self.add_line('', '')\n\n        # Add the classname or 'Configuration'\n        self.add_line('.. everett:component:: %s' % component_name, sourcename)\n        self.add_line('', sourcename)\n\n        # Add the docstring if there is one and if show-docstring\n        if 'show-docstring' in self.options:\n            docstring_attr = self.options['show-docstring'] or '__doc__'\n            docstring = getattr(obj, docstring_attr, None)\n            if docstring:\n                docstringlines = prepare_docstring(docstring, ignore=1)\n                for i, line in enumerate(docstringlines):\n                    self.add_line(indent + line, sourcename, i)\n                self.add_line('', '')\n\n        # Add content from the directive if there was any\n        if more_content:\n            for line, src in zip(more_content.data, more_content.items):\n                self.add_line(indent + line, src[0], src[1])\n            self.add_line('', '')\n\n        if all_options:\n            # Now list the options\n            sourcename = 'class definition'\n\n            for option in all_options:\n                self.add_line(\n                    '%s:option %s %s:' % (indent, option['parser'], option['key']),\n                    sourcename\n                )\n                self.add_line('%s    %s' % (indent, option['doc']), sourcename)\n                if option['default'] is not NO_VALUE:\n                    self.add_line('', '')\n                    self.add_line(\n                        '%s    Defaults to ``%r``.' % (indent, option['default']),\n                        sourcename\n                    )\n                self.add_line('', '')", "response": "Generate documentation for this configman class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the dot name for a given thing.", "response": "def qualname(thing):\n    \"\"\"Return the dot name for a given thing.\n\n    >>> import everett.manager\n    >>> qualname(str)\n    'str'\n    >>> qualname(everett.manager.parse_class)\n    'everett.manager.parse_class'\n    >>> qualname(everett.manager)\n    'everett.manager'\n\n    \"\"\"\n    parts = []\n\n    # Add the module, unless it's a builtin\n    mod = inspect.getmodule(thing)\n    if mod and mod.__name__ not in ('__main__', '__builtin__', 'builtins'):\n        parts.append(mod.__name__)\n\n    # If there's a __qualname__, use that\n    if hasattr(thing, '__qualname__'):\n        parts.append(thing.__qualname__)\n        return '.'.join(parts)\n\n    # If it's a module\n    if inspect.ismodule(thing):\n        return '.'.join(parts)\n\n    # If it's a class\n    if inspect.isclass(thing):\n        parts.append(thing.__name__)\n        return '.'.join(parts)\n\n    # If it's a function\n    if isinstance(thing, (types.FunctionType, types.MethodType)):\n        # If it's a method or function\n        if inspect.ismethod(thing):\n            if thing.im_class is type:\n                # This is a class method\n                parts.append(thing.im_self.__name__)\n            else:\n                # This is an bound/instance method\n                parts.append(thing.im_class.__name__)\n            parts.append(thing.__name__)\n\n        elif inspect.isfunction(thing):\n            parts.append(thing.__name__)\n\n        return '.'.join(parts)\n\n    # It's an instance, so ... let's call repr on it\n    return repr(thing)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a bool value.", "response": "def parse_bool(val):\n    \"\"\"Parse a bool value.\n\n    Handles a series of values, but you should probably standardize on\n    \"true\" and \"false\".\n\n    >>> parse_bool('y')\n    True\n    >>> parse_bool('FALSE')\n    False\n\n    \"\"\"\n    true_vals = ('t', 'true', 'yes', 'y', '1', 'on')\n    false_vals = ('f', 'false', 'no', 'n', '0', 'off')\n\n    val = val.lower()\n    if val in true_vals:\n        return True\n    if val in false_vals:\n        return False\n\n    raise ValueError('\"%s\" is not a valid bool value' % val)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the content of an iterable of lines as. env.", "response": "def parse_env_file(envfile):\n    \"\"\"Parse the content of an iterable of lines as ``.env``.\n\n    Return a dict of config variables.\n\n    >>> parse_env_file(['DUDE=Abides'])\n    {'DUDE': 'Abides'}\n\n    \"\"\"\n    data = {}\n    for line_no, line in enumerate(envfile):\n        line = line.strip()\n        if not line or line.startswith('#'):\n            continue\n        if '=' not in line:\n            raise ConfigurationError('Env file line missing = operator (line %s)' % (line_no + 1))\n        k, v = line.split('=', 1)\n        k = k.strip()\n        if not ENV_KEY_RE.match(k):\n            raise ConfigurationError(\n                'Invalid variable name \"%s\" in env file (line %s)' % (k, (line_no + 1))\n            )\n        v = v.strip().strip('\\'\"')\n        data[k] = v\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_class(val):\n    module, class_name = val.rsplit('.', 1)\n    module = importlib.import_module(module)\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        raise ValueError('\"%s\" is not a valid member of %s' % (\n            class_name, qualname(module))\n        )", "response": "Parse a string imports the module and returns the class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_uppercase_key(key, namespace=None):\n    if namespace:\n        namespace = [part for part in listify(namespace) if part]\n        key = '_'.join(namespace + [key])\n\n    key = key.upper()\n    return key", "response": "Given a key and a namespace generates a final uppercase key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_key_from_envs(envs, key):\n    # if it barks like a dict, make it a list have to use `get` since dicts and\n    # lists both have __getitem__\n    if hasattr(envs, 'get'):\n        envs = [envs]\n\n    for env in envs:\n        if key in env:\n            return env[key]\n\n    return NO_VALUE", "response": "Return the value of a key from the given dict respecting namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving value for key.", "response": "def get(self, key, namespace=None):\n        \"\"\"Retrieve value for key.\"\"\"\n        # Short-circuit to reduce overhead.\n        if not _CONFIG_OVERRIDE:\n            return NO_VALUE\n        full_key = generate_uppercase_key(key, namespace)\n        logger.debug('Searching %s for %s', self, full_key)\n        return get_key_from_envs(reversed(_CONFIG_OVERRIDE), full_key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, key, namespace=None):\n        full_key = generate_uppercase_key(key, namespace)\n        full_key = full_key.lower()\n\n        logger.debug('Searching %s for %s', self, full_key)\n\n        # Build a map of lowercase -> actual key\n        obj_keys = {\n            item.lower(): item\n            for item in dir(self.obj) if not item.startswith('__')\n        }\n\n        if full_key in obj_keys:\n            val = getattr(self.obj, obj_keys[full_key])\n\n            # If the value is None, then we're going to treat it as a non-valid\n            # value.\n            if val is not None:\n                # This is goofy, but this allows people to specify arg parser\n                # defaults, but do the right thing in Everett where everything\n                # is a string until it's parsed.\n                return str(val)\n\n        return NO_VALUE", "response": "Retrieve value for key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving value for key.", "response": "def get(self, key, namespace=None):\n        \"\"\"Retrieve value for key.\"\"\"\n        full_key = generate_uppercase_key(key, namespace)\n        logger.debug('Searching %s for %s', self, full_key)\n        return get_key_from_envs(os.environ, full_key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies options component options to this configuration.", "response": "def with_options(self, component):\n        \"\"\"Apply options component options to this configuration.\"\"\"\n        options = component.get_required_config()\n        component_name = _get_component_name(component)\n        return BoundConfig(self._get_base_config(), component_name, options)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decorate(self, fun):\n        @wraps(fun)\n        def _decorated(*args, **kwargs):\n            # Push the config, run the function and pop it afterwards.\n            self.push_config()\n            try:\n                return fun(*args, **kwargs)\n            finally:\n                self.pop_config()\n        return _decorated", "response": "Decorate a function for overriding configuration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an option to the group.", "response": "def add_option(self, key, default=NO_VALUE, alternate_keys=NO_VALUE,\n                   doc='', parser=str, **meta):\n        \"\"\"Add an option to the group.\n\n        :arg key: the key to look up\n\n        :arg default: the default value (if any); must be a string that is\n            parseable by the specified parser\n\n        :arg alternate_keys: the list of alternate keys to look up;\n            supports a ``root:`` key prefix which will cause this to look at\n            the configuration root rather than the current namespace\n\n        :arg doc: documentation for this config option\n\n        :arg parser: the parser for converting this value to a Python object\n\n        :arg meta: catch-all for other key/value pairs you want to association\n            with this option\n\n        \"\"\"\n        option = Option(key, default, alternate_keys, doc, parser, meta)\n        self.options[key] = option"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates this ConfigOptions using data from another ConfigOptions.", "response": "def update(self, new_options):\n        \"\"\"Update this ConfigOptions using data from another.\"\"\"\n        for option in new_options:\n            if option.key in self.options:\n                del self.options[option.key]\n            self.options[option.key] = option"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_required_config(cls):\n        options = ConfigOptions()\n        for cls in reversed(cls.__mro__):\n            try:\n                options.update(cls.required_config)\n            except AttributeError:\n                pass\n        return options", "response": "Roll up configuration options for this class and all parent classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrolls up the runtime config for this class and all children.", "response": "def get_runtime_config(self, namespace=None):\n        \"\"\"Roll up the runtime config for this class and all children.\n\n        Implement this to call ``.get_runtime_config()`` on child components or\n        to adjust how it works.\n\n        For example, if you created a component that has a child component, you\n        could do something like this::\n\n            class MyComponent(RequiredConfigMixin):\n                ....\n\n                def __init__(self, config):\n                    self.config = config.with_options(self)\n                    self.child = OtherComponent(config.with_namespace('source'))\n\n                def get_runtime_config(self, namespace=None):\n                    for item in super(MyComponent, self).get_runtime_config(namespace):\n                        yield item\n                    for item in self.child.get_runtime_config(['source']):\n                        yield item\n\n\n        Calling this function can give you the complete runtime configuration\n        for a component tree. This is helpful for doing things like printing\n        the configuration being used including default values.\n\n        .. Note::\n\n           If this instance has a ``.config`` attribute and it is a\n           :py:class:`everett.component.BoundConfig`, then this will try to\n           compute the runtime config.\n\n           Otherwise, it'll yield nothing.\n\n        :arg list namespace: list of namespace parts or None\n\n        :returns: list of ``(namespace, key, option)``\n\n        \"\"\"\n        namespace = namespace or []\n\n        cfg = getattr(self, 'config', None)\n        if cfg is None or not isinstance(cfg, BoundConfig):\n            return\n\n        for key, opt in self.get_required_config().options.items():\n            yield (namespace, key, self.config(key, raise_error=False, raw_value=True), opt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_ini_file(self, path):\n        cfgobj = ConfigObj(path, list_values=False)\n\n        def extract_section(namespace, d):\n            cfg = {}\n            for key, val in d.items():\n                if isinstance(d[key], dict):\n                    cfg.update(extract_section(namespace + [key], d[key]))\n                else:\n                    cfg['_'.join(namespace + [key]).upper()] = val\n\n            return cfg\n\n        return extract_section([], cfgobj.dict())", "response": "Parse the ini file at path and return dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get(self, url):\n        return self.session.get(self.READ_URL_PRE + url).json()", "response": "Helper method to get data from given URL on TBA s API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _post(self, url, data):\n        return self.session.post(self.WRITE_URL_PRE + url % self.event_key, data=data, headers={'X-TBA-Auth-Sig': md5((self.auth_secret + '/api/trusted/v1/' + url % self.event_key + data).encode('utf-8')).hexdigest()})", "response": "Helper method to POST data to a given URL on TBA s API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef teams(self, page=None, year=None, simple=False, keys=False):\n        # If the user has requested a specific page, get that page.\n        if page is not None:\n            if year:\n                if keys:\n                    return self._get('teams/%s/%s/keys' % (year, page))\n                else:\n                    return [Team(raw) for raw in self._get('teams/%s/%s%s' % (year, page, '/simple' if simple else ''))]\n            else:\n                if keys:\n                    return self._get('teams/%s/keys' % page)\n                else:\n                    return [Team(raw) for raw in self._get('teams/%s%s' % (page, '/simple' if simple else ''))]\n        # If no page was specified, get all of them and combine.\n        else:\n            teams = []\n            target = 0\n            while True:\n                page_teams = self.teams(page=target, year=year, simple=simple, keys=keys)\n                if page_teams:\n                    teams.extend(page_teams)\n                else:\n                    break\n                target += 1\n            return teams", "response": "Get list of teams from a specific year."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef team(self, team, simple=False):\n        return Team(self._get('team/%s%s' % (self.team_key(team), '/simple' if simple else '')))", "response": "Get data on a single team."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets team events a team has participated in. :param team: Team to get events for. :param year: Year to get events from. :param simple: Get only vital data. :param keys: Get just the keys of the events. Set to True if you only need the keys of each event and not their full data. :return: List of strings or Teams", "response": "def team_events(self, team, year=None, simple=False, keys=False):\n        \"\"\"\n        Get team events a team has participated in.\n\n        :param team: Team to get events for.\n        :param year: Year to get events from.\n        :param simple: Get only vital data.\n        :param keys: Get just the keys of the events. Set to True if you only need the keys of each event and not their full data.\n        :return: List of strings or Teams\n        \"\"\"\n        if year:\n            if keys:\n                return self._get('team/%s/events/%s/keys' % (self.team_key(team), year))\n            else:\n                return [Event(raw) for raw in self._get('team/%s/events/%s%s' % (self.team_key(team), year, '/simple' if simple else ''))]\n        else:\n            if keys:\n                return self._get('team/%s/events/keys' % self.team_key(team))\n            else:\n                return [Event(raw) for raw in self._get('team/%s/events%s' % (self.team_key(team), '/simple' if simple else ''))]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef team_awards(self, team, year=None, event=None):\n        if event:\n            return [Award(raw) for raw in self._get('team/%s/event/%s/awards' % (self.team_key(team), event))]\n        else:\n            if year:\n                return [Award(raw) for raw in self._get('team/%s/awards/%s' % (self.team_key(team), year))]\n            else:\n                return [Award(raw) for raw in self._get('team/%s/awards' % self.team_key(team))]", "response": "Get list of awards of a team has recieved."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef team_matches(self, team, event=None, year=None, simple=False, keys=False):\n        if event:\n            if keys:\n                return self._get('team/%s/event/%s/matches/keys' % (self.team_key(team), event))\n            else:\n                return [Match(raw) for raw in self._get('team/%s/event/%s/matches%s' % (self.team_key(team), event, '/simple' if simple else ''))]\n        elif year:\n            if keys:\n                return self._get('team/%s/matches/%s/keys' % (self.team_key(team), year))\n            else:\n                return [Match(raw) for raw in self._get('team/%s/matches/%s%s' % (self.team_key(team), year, '/simple' if simple else ''))]", "response": "Get list of matches for a team."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all media for a given team and year.", "response": "def team_media(self, team, year=None, tag=None):\n        \"\"\"\n        Get media for a given team.\n\n        :param team: Team to get media of.\n        :param year: Year to get media from.\n        :param tag: Get only media with a given tag.\n        :return: List of Media objects.\n        \"\"\"\n        return [Media(raw) for raw in self._get('team/%s/media%s%s' % (self.team_key(team), ('/tag/%s' % tag) if tag else '', ('/%s' % year) if year else ''))]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets data about a team s robots.", "response": "def team_robots(self, team):\n        \"\"\"\n        Get data about a team's robots.\n\n        :param team: Key for team whose robots you want data on.\n        :return: List of Robot objects\n        \"\"\"\n        return [Robot(raw) for raw in self._get('team/%s/robots' % self.team_key(team))]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets districts a team has competed in.", "response": "def team_districts(self, team):\n        \"\"\"\n        Get districts a team has competed in.\n\n        :param team: Team to get data on.\n        :return: List of District objects.\n        \"\"\"\n        return [District(raw) for raw in self._get('team/%s/districts' % self.team_key(team))]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef team_profiles(self, team):\n        return [Profile(raw) for raw in self._get('team/%s/social_media' % self.team_key(team))]", "response": "Get team s social media profiles linked on their TBA page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the status of a team at an event.", "response": "def team_status(self, team, event):\n        \"\"\"\n        Get status of a team at an event.\n\n        :param team: Team whose status to get.\n        :param event: Event team is at.\n        :return: Status object.\n        \"\"\"\n        return Status(self._get('team/%s/event/%s/status' % (self.team_key(team), event)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of events in a given year.", "response": "def events(self, year, simple=False, keys=False):\n        \"\"\"\n        Get a list of events in a given year.\n\n        :param year: Year to get events from.\n        :param keys: Get only keys of the events rather than full data.\n        :param simple: Get only vital data.\n        :return: List of string event keys or Event objects.\n        \"\"\"\n        if keys:\n            return self._get('events/%s/keys' % year)\n        else:\n            return [Event(raw) for raw in self._get('events/%s%s' % (year, '/simple' if simple else ''))]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef event(self, event, simple=False):\n        return Event(self._get('event/%s%s' % (event, '/simple' if simple else '')))", "response": "Get basic information about an event."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets list of teams at an event.", "response": "def event_teams(self, event, simple=False, keys=False):\n        \"\"\"\n        Get list of teams at an event.\n\n        :param event: Event key to get data on.\n        :param simple: Get only vital data.\n        :param keys: Return list of team keys only rather than full data on every team.\n        :return: List of string keys or Team objects.\n        \"\"\"\n        if keys:\n            return self._get('event/%s/teams/keys' % event)\n        else:\n            return [Team(raw) for raw in self._get('event/%s/teams%s' % (event, '/simple' if simple else ''))]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of matches played at an event.", "response": "def event_matches(self, event, simple=False, keys=False):\n        \"\"\"\n        Get list of matches played at an event.\n\n        :param event: Event key to get data on.\n        :param keys: Return list of match keys only rather than full data on every match.\n        :param simple: Get only vital data.\n        :return: List of string keys or Match objects.\n        \"\"\"\n        if keys:\n            return self._get('event/%s/matches/keys' % event)\n        else:\n            return [Match(raw) for raw in self._get('event/%s/matches%s' % (event, '/simple' if simple else ''))]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match(self, key=None, year=None, event=None, type='qm', number=None, round=None, simple=False):\n        if key:\n            return Match(self._get('match/%s%s' % (key, '/simple' if simple else '')))\n        else:\n            return Match(self._get('match/{year}{event}_{type}{number}{round}{simple}'.format(year=year if not event[0].isdigit() else '',\n                                                                                              event=event,\n                                                                                              type=type,\n                                                                                              number=number,\n                                                                                              round=('m%s' % round) if not type == 'qm' else '',\n                                                                                              simple='/simple' if simple else '')))", "response": "Get data on a match."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef district_events(self, district, simple=False, keys=False):\n        if keys:\n            return self._get('district/%s/events/keys' % district)\n        else:\n            return [Event(raw) for raw in self._get('district/%s/events%s' % (district, '/simple' if simple else ''))]", "response": "Get a list of events in a given district."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget list of teams in a given district.", "response": "def district_teams(self, district, simple=False, keys=False):\n        \"\"\"\n        Get list of teams in the given district.\n\n        :param district: Key for the district to get teams in.\n        :param simple: Get only vital data.\n        :param keys: Return list of team keys only rather than full data on every team.\n        :return: List of string keys or Team objects.\n        \"\"\"\n        if keys:\n            return self._get('district/%s/teams/keys' % district)\n        else:\n            return [Team(raw) for raw in self._get('district/%s/teams' % district)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_trusted(self, auth_id, auth_secret, event_key):\n        self.session.headers.update({'X-TBA-Auth-Id': auth_id})\n        self.auth_secret = auth_secret\n        self.event_key = event_key", "response": "Set Trusted API ID and Secret and the event key they are assigned to."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_event_matches(self, data=None):\n        return self._post('event/%s/matches/delete_all' if data is None else 'event/%s/matches/delete', json.dumps(self.event_key) if data is None else json.dumps(data))", "response": "Delete an event s matches on The Blue Alliance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the right one if the given clause could be a field or value.", "response": "def field_or_value(clause):\n    \"\"\"\n    For a clause that could be a field or value,\n    create the right one and return it\n\n    \"\"\"\n    if hasattr(clause, \"getName\") and clause.getName() != \"field\":\n        return Value(resolve(clause))\n    else:\n        return Field(clause)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_clause(cls, clause):\n        name = clause.getName()\n        if name == \"not\":\n            cond = Invert(cls.from_clause(clause[1]))\n        elif name == \"operator\":\n            cond = OperatorConstraint.from_clause(clause)\n        elif name == \"conjunction\" or clause.conjunction:\n            cond = Conjunction.from_clause(clause)\n        elif name == \"function\":\n            cond = FunctionConstraint.from_clause(clause)\n        elif name == \"between\":\n            cond = BetweenConstraint.from_clause(clause)\n        elif name == \"in\":\n            cond = InConstraint.from_clause(clause)\n        else:\n            raise SyntaxError(\"Unknown constraint type %r\" % name)\n        return cond", "response": "Factory method for creating a new instance of the class from a given clause."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _factory(cls, constraints, op):\n        pieces = []\n        for i, constraint in enumerate(constraints):\n            pieces.append(constraint)\n            if i != len(constraints) - 1:\n                pieces.append(op)\n        return cls(pieces)", "response": "Factory for joining constraints with a single conjunction"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the hash range fields of all joined constraints", "response": "def _get_fields(self, attr):\n        \"\"\" Get the hash/range fields of all joined constraints \"\"\"\n        ret = set()\n        if \"OR\" in self.pieces:\n            return ret\n        for i in range(0, len(self.pieces), 2):\n            const = self.pieces[i]\n            field = getattr(const, attr)\n            if field is not None:\n                ret.add(field)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_clause(cls, clause):\n        (field, operator, val) = clause\n        return cls(field, operator, field_or_value(val))", "response": "Factory method for creating a new object from a given clause."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_clause(cls, clause):\n        fn_name = clause[0]\n        if fn_name == \"size\":\n            return SizeConstraint.from_clause(clause)\n        elif fn_name == \"attribute_type\":\n            return TypeConstraint.from_clause(clause)\n        else:\n            fn_name = clause[0]\n            field = clause[1]\n            if len(clause) > 2:\n                return cls(fn_name, field, resolve(clause[2]))\n            else:\n                return cls(fn_name, field)", "response": "Factory method for creating a new object from a clause."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_clause(cls, clause):\n        [_, field, operator, val] = clause\n        return cls(field, operator, resolve(val))", "response": "Factory method for creating a new object from a clause."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_clause(cls, clause):\n        [field, low, high] = clause\n        return cls(field, resolve(low), resolve(high))", "response": "Factory method for creating a new object from a clause."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_clause(cls, clause):\n        [field, vals] = clause\n        return cls(field, resolve(vals))", "response": "Factory method for creating a new object from a set of values in a COALESCE clause."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef default(value):\n    if isinstance(value, Decimal):\n        primative = float(value)\n        if int(primative) == primative:\n            return int(primative)\n        else:\n            return primative\n    elif isinstance(value, set):\n        return list(value)\n    elif isinstance(value, Binary):\n        return b64encode(value.value)\n    raise TypeError(\"Cannot encode %s value %r\" % (type(value), value))", "response": "Default encoder for JSON"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_query_kwargs(kwargs, visitor, constraints, index):\n    (query_const, filter_const) = constraints.remove_index(index)\n    kwargs[\"key_condition_expr\"] = query_const.build(visitor)\n    if filter_const:\n        kwargs[\"filter\"] = filter_const.build(visitor)\n    if index.name != \"TABLE\":\n        kwargs[\"index\"] = index.name", "response": "Construct KeyConditionExpression and FilterExpression"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate over the items to insert from an INSERT statement.", "response": "def iter_insert_items(tree):\n    \"\"\" Iterate over the items to insert from an INSERT statement \"\"\"\n    if tree.list_values:\n        keys = tree.attrs\n        for values in tree.list_values:\n            if len(keys) != len(values):\n                raise SyntaxError(\n                    \"Values '%s' do not match attributes \" \"'%s'\" % (values, keys)\n                )\n            yield dict(zip(keys, map(resolve, values)))\n    elif tree.map_values:\n        for item in tree.map_values:\n            data = {}\n            for (key, val) in item:\n                data[key] = resolve(val)\n            yield data\n    else:\n        raise SyntaxError(\"No insert data found\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connection(self, connection):\n        if connection is not None:\n            connection.subscribe(\"capacity\", self._on_capacity_data)\n            connection.default_return_capacity = True\n        if self._connection is not None:\n            connection.unsubscribe(\"capacity\", self._on_capacity_data)\n        self._connection = connection\n        self._cloudwatch_connection = None\n        self.cached_descriptions = {}", "response": "Change the dynamo connection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cloudwatch_connection(self):\n        if self._cloudwatch_connection is None:\n            conn = self._session.create_client(\"cloudwatch\", self.connection.region)\n            self._cloudwatch_connection = conn\n        return self._cloudwatch_connection", "response": "Lazy create a connection to cloudwatch"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _format_explain(self):\n        lines = []\n        for (command, kwargs) in self._call_list:\n            lines.append(command + \" \" + pformat(kwargs))\n        return \"\\n\".join(lines)", "response": "Format the results of an EXPLAIN"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats the return value of a query for humans", "response": "def _pretty_format(self, statement, result):\n        \"\"\" Format the return value of a query for humans \"\"\"\n        if result is None:\n            return \"Success\"\n        ret = result\n        if statement.action in (\"SELECT\", \"SCAN\"):\n            if statement.save_file:\n                filename = statement.save_file[0]\n                if filename[0] in ['\"', \"'\"]:\n                    filename = unwrap(filename)\n                ret = \"Saved %d record%s to %s\" % (result, plural(result), filename)\n            elif isinstance(result, int):\n                if result == result.scanned_count:\n                    ret = \"%d\" % result\n                else:\n                    ret = \"%d (scanned count: %d)\" % (result, result.scanned_count)\n        elif statement.action == \"UPDATE\":\n            if isinstance(result, int):\n                ret = \"Updated %d item%s\" % (result, plural(result))\n        elif statement.action == \"DELETE\":\n            ret = \"Deleted %d item%s\" % (result, plural(result))\n        elif statement.action == \"CREATE\":\n            if result:\n                ret = \"Created table %r\" % statement.table\n            else:\n                ret = \"Table %r already exists\" % statement.table\n        elif statement.action == \"INSERT\":\n            ret = \"Inserted %d item%s\" % (result, plural(result))\n        elif statement.action == \"DROP\":\n            if result:\n                ret = \"Dropped table %r\" % statement.table\n            else:\n                ret = \"Table %r does not exist\" % statement.table\n        elif statement.action == \"ANALYZE\":\n            ret = self._pretty_format(statement[1], result)\n        elif statement.action == \"LOAD\":\n            ret = \"Loaded %d item%s\" % (result, plural(result))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef describe_all(self, refresh=True):\n        tables = self.connection.list_tables()\n        descs = []\n        for tablename in tables:\n            descs.append(self.describe(tablename, refresh))\n        return descs", "response": "Describe all tables in the connected region"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_metric(self, metric, tablename, index_name=None):\n        end = time.time()\n        begin = end - 3 * 60  # 3 minute window\n        dimensions = [{\"Name\": \"TableName\", \"Value\": tablename}]\n        if index_name is not None:\n            dimensions.append({\"Name\": \"GlobalSecondaryIndexName\", \"Value\": index_name})\n        period = 60\n        data = self.cloudwatch_connection.get_metric_statistics(\n            Period=period,\n            StartTime=begin,\n            EndTime=end,\n            MetricName=metric,\n            Namespace=\"AWS/DynamoDB\",\n            Statistics=[\"Sum\"],\n            Dimensions=dimensions,\n        )\n        points = data[\"Datapoints\"]\n        if not points:\n            return 0\n        else:\n            points.sort(key=lambda r: r[\"Timestamp\"])\n            return float(points[-1][\"Sum\"]) / period", "response": "Fetch a read or write capacity metric from DynamoDB."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_capacity(self, tablename, index_name=None):\n        # If we're connected to a DynamoDB Local instance, don't connect to the\n        # actual cloudwatch endpoint\n        if self.connection.region == \"local\":\n            return 0, 0\n        # Gracefully fail if we get exceptions from CloudWatch\n        try:\n            return (\n                self._get_metric(\"ConsumedReadCapacityUnits\", tablename, index_name),\n                self._get_metric(\"ConsumedWriteCapacityUnits\", tablename, index_name),\n            )\n        except ClientError:\n            return 0, 0", "response": "Get the consumed read and write capacity for a table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the table meta object for a table.", "response": "def describe(self, tablename, refresh=False, metrics=False, require=False):\n        \"\"\" Get the :class:`.TableMeta` for a table \"\"\"\n        table = self.cached_descriptions.get(tablename)\n        if refresh or table is None or (metrics and not table.consumed_capacity):\n            desc = self.connection.describe_table(tablename)\n            if desc is None:\n                if require:\n                    raise RuntimeError(\"Table %r not found\" % tablename)\n                else:\n                    return None\n            table = TableMeta.from_description(desc)\n            self.cached_descriptions[tablename] = table\n            if metrics:\n                read, write = self.get_capacity(tablename)\n                table.consumed_capacity[\"__table__\"] = {\"read\": read, \"write\": write}\n                for index_name in table.global_indexes:\n                    read, write = self.get_capacity(tablename, index_name)\n                    table.consumed_capacity[index_name] = {\"read\": read, \"write\": write}\n\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing and run a DQL string and return the result.", "response": "def execute(self, commands, pretty_format=False):\n        \"\"\"\n        Parse and run a DQL string\n\n        Parameters\n        ----------\n        commands : str\n            The DQL command string\n        pretty_format : bool\n            Pretty-format the return value. (e.g. 4 -> 'Updated 4 items')\n\n        \"\"\"\n        tree = parser.parseString(commands)\n        self.consumed_capacities = []\n        self._analyzing = False\n        self._query_rate_limit = None\n        for statement in tree:\n            try:\n                result = self._run(statement)\n            except ExplainSignal:\n                return self._format_explain()\n        if pretty_format:\n            return self._pretty_format(tree[-1], result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run(self, tree):\n        if tree.throttle:\n            limiter = self._parse_throttle(tree.table, tree.throttle)\n            self._query_rate_limit = limiter\n            del tree[\"throttle\"]\n            return self._run(tree)\n        if tree.action == \"SELECT\":\n            return self._select(tree, self.allow_select_scan)\n        elif tree.action == \"SCAN\":\n            return self._scan(tree)\n        elif tree.action == \"DELETE\":\n            return self._delete(tree)\n        elif tree.action == \"UPDATE\":\n            return self._update(tree)\n        elif tree.action == \"CREATE\":\n            return self._create(tree)\n        elif tree.action == \"INSERT\":\n            return self._insert(tree)\n        elif tree.action == \"DROP\":\n            return self._drop(tree)\n        elif tree.action == \"ALTER\":\n            return self._alter(tree)\n        elif tree.action == \"DUMP\":\n            return self._dump(tree)\n        elif tree.action == \"LOAD\":\n            return self._load(tree)\n        elif tree.action == \"EXPLAIN\":\n            return self._explain(tree)\n        elif tree.action == \"ANALYZE\":\n            self._analyzing = True\n            self.connection.default_return_capacity = True\n            return self._run(tree[1])\n        else:\n            raise SyntaxError(\"Unrecognized action '%s'\" % tree.action)", "response": "Run a query from a parse tree"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_throttle(self, tablename, throttle):\n        amount = []\n        desc = self.describe(tablename)\n        throughputs = [desc.read_throughput, desc.write_throughput]\n        for value, throughput in zip(throttle[1:], throughputs):\n            if value == \"*\":\n                amount.append(0)\n            elif value[-1] == \"%\":\n                amount.append(throughput * float(value[:-1]) / 100.0)\n            else:\n                amount.append(float(value))\n        cap = Capacity(*amount)  # pylint: disable=E1120\n        return RateLimit(total=cap, callback=self._on_throttle)", "response": "Parse a throttle statement and return a RateLimit object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog the received consumed capacity data", "response": "def _on_capacity_data(self, conn, command, kwargs, response, capacity):\n        \"\"\" Log the received consumed capacity data \"\"\"\n        if self._analyzing:\n            self.consumed_capacities.append((command, capacity))\n        if self._query_rate_limit is not None:\n            self._query_rate_limit.on_capacity(\n                conn, command, kwargs, response, capacity\n            )\n        elif self.rate_limit is not None:\n            self.rate_limit.callback = self._on_throttle\n            self.rate_limit.on_capacity(conn, command, kwargs, response, capacity)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _on_throttle(self, conn, command, kwargs, response, capacity, seconds):\n        LOG.info(\n            \"Throughput limit exceeded during %s. \" \"Sleeping for %d second%s\",\n            command,\n            seconds,\n            plural(seconds),\n        )", "response": "Print out a message when the query is throttled"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexplaining the current state of the user.", "response": "def _explain(self, tree):\n        \"\"\" Set up the engine to do a dry run of a query \"\"\"\n        self._explaining = True\n        self._call_list = []\n        old_call = self.connection.call\n\n        def fake_call(command, **kwargs):\n            \"\"\" Replacement for connection.call that logs args \"\"\"\n            if command == \"describe_table\":\n                return old_call(command, **kwargs)\n            self._call_list.append((command, kwargs))\n            raise ExplainSignal\n\n        self.connection.call = fake_call\n        try:\n            ret = self._run(tree[1])\n            try:\n                list(ret)\n            except TypeError:\n                pass\n        finally:\n            self.connection.call = old_call\n            self._explaining = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_query(self, table, tree, visitor):\n        kwargs = {}\n        index = None\n        if tree.using:\n            index_name = kwargs[\"index\"] = tree.using[1]\n            index = table.get_index(index_name)\n        if tree.where:\n            constraints = ConstraintExpression.from_where(tree.where)\n            possible_hash = constraints.possible_hash_fields()\n            possible_range = constraints.possible_range_fields()\n            if index is None:\n                # See if we can find an index to query on\n                indexes = table.get_matching_indexes(possible_hash, possible_range)\n                if not indexes:\n                    action = \"scan\"\n                    kwargs[\"filter\"] = constraints.build(visitor)\n                    kwargs[\"expr_values\"] = visitor.expression_values\n                    kwargs[\"alias\"] = visitor.attribute_names\n                elif len(indexes) == 1:\n                    index = indexes[0]\n                    action = \"query\"\n                    add_query_kwargs(kwargs, visitor, constraints, index)\n                else:\n                    names = \", \".join([index.name for index in indexes])\n                    raise SyntaxError(\n                        \"No index specified with USING <index>, \"\n                        \"but multiple possibilities for query: \"\n                        \"%s\" % names\n                    )\n            else:\n                if index.hash_key in possible_hash:\n                    action = \"query\"\n                    add_query_kwargs(kwargs, visitor, constraints, index)\n                else:\n                    action = \"scan\"\n                    if not index.scannable:\n                        raise SyntaxError(\"Cannot scan local index %r\" % index_name)\n                    kwargs[\"filter\"] = constraints.build(visitor)\n                    kwargs[\"expr_values\"] = visitor.expression_values\n                    kwargs[\"alias\"] = visitor.attribute_names\n        else:\n            action = \"scan\"\n        return [action, kwargs, index]", "response": "Builds a scan query from a statement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over the KEYS IN and generate primary keys", "response": "def _iter_where_in(self, tree):\n        \"\"\" Iterate over the KEYS IN and generate primary keys \"\"\"\n        desc = self.describe(tree.table, require=True)\n        for keypair in tree.keys_in:\n            yield desc.primary_key(*map(resolve, keypair))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a SELECT statement on the table.", "response": "def _select(self, tree, allow_select_scan):\n        \"\"\" Run a SELECT statement \"\"\"\n        tablename = tree.table\n        desc = self.describe(tablename, require=True)\n        kwargs = {}\n        if tree.consistent:\n            kwargs[\"consistent\"] = True\n\n        visitor = Visitor(self.reserved_words)\n\n        selection = SelectionExpression.from_selection(tree.attrs)\n        if selection.is_count:\n            kwargs[\"select\"] = \"COUNT\"\n\n        if tree.keys_in:\n            if tree.limit:\n                raise SyntaxError(\"Cannot use LIMIT with KEYS IN\")\n            elif tree.using:\n                raise SyntaxError(\"Cannot use USING with KEYS IN\")\n            elif tree.order:\n                raise SyntaxError(\"Cannot use DESC/ASC with KEYS IN\")\n            elif tree.where:\n                raise SyntaxError(\"Cannot use WHERE with KEYS IN\")\n            keys = list(self._iter_where_in(tree))\n            kwargs[\"attributes\"] = selection.build(visitor)\n            kwargs[\"alias\"] = visitor.attribute_names\n            return self.connection.batch_get(tablename, keys=keys, **kwargs)\n\n        if tree.limit:\n            if tree.scan_limit:\n                kwargs[\"limit\"] = Limit(\n                    scan_limit=resolve(tree.scan_limit[2]),\n                    item_limit=resolve(tree.limit[1]),\n                    strict=True,\n                )\n            else:\n                kwargs[\"limit\"] = Limit(item_limit=resolve(tree.limit[1]), strict=True)\n        elif tree.scan_limit:\n            kwargs[\"limit\"] = Limit(scan_limit=resolve(tree.scan_limit[2]))\n\n        (action, query_kwargs, index) = self._build_query(desc, tree, visitor)\n        if action == \"scan\" and not allow_select_scan:\n            raise SyntaxError(\n                \"No index found for query. Please use a SCAN query, or \"\n                \"set allow_select_scan=True\\nopt allow_select_scan true\"\n            )\n        order_by = None\n        if tree.order_by:\n            order_by = tree.order_by[0]\n        reverse = tree.order == \"DESC\"\n        if tree.order:\n            if action == \"scan\" and not tree.order_by:\n                raise SyntaxError(\n                    \"No index found for query, \"\n                    \"cannot use ASC or DESC without \"\n                    \"ORDER BY <field>\"\n                )\n            if action == \"query\":\n                if order_by is None or order_by == index.range_key:\n                    kwargs[\"desc\"] = reverse\n\n        kwargs.update(query_kwargs)\n\n        # This is a special case for when we're querying an index and selecting\n        # fields that aren't projected into the index.\n        # We will change the query to only fetch the primary keys, and then\n        # fill in the selected attributes after the fact.\n        fetch_attrs_after = False\n        if index is not None and not index.projects_all_attributes(\n            selection.all_fields\n        ):\n            kwargs[\"attributes\"] = [\n                visitor.get_field(a) for a in desc.primary_key_attributes\n            ]\n            fetch_attrs_after = True\n        else:\n            kwargs[\"attributes\"] = selection.build(visitor)\n        kwargs[\"expr_values\"] = visitor.expression_values\n        kwargs[\"alias\"] = visitor.attribute_names\n\n        method = getattr(self.connection, action + \"2\")\n        result = method(tablename, **kwargs)\n\n        # If the queried index didn't project the selected attributes, we need\n        # to do a BatchGetItem to fetch all the data.\n        if fetch_attrs_after:\n            if not isinstance(result, list):\n                result = list(result)\n            # If no results, no need to batch_get\n            if not result:\n                return result\n            visitor = Visitor(self.reserved_words)\n            kwargs = {\"keys\": [desc.primary_key(item) for item in result]}\n            kwargs[\"attributes\"] = selection.build(visitor)\n            kwargs[\"alias\"] = visitor.attribute_names\n            result = self.connection.batch_get(tablename, **kwargs)\n\n        def order(items):\n            \"\"\" Sort the items by the specified keys \"\"\"\n            if order_by is None:\n                return items\n            if index is None or order_by != index.range_key:\n                if not isinstance(items, list):\n                    items = list(items)\n                items.sort(key=lambda x: x.get(order_by), reverse=reverse)\n            return items\n\n        # Save the data to a file\n        if tree.save_file:\n            if selection.is_count:\n                raise Exception(\"Cannot use count(*) with SAVE\")\n            count = 0\n            result = order(selection.convert(item, True) for item in result)\n            filename = tree.save_file[0]\n            if filename[0] in ['\"', \"'\"]:\n                filename = unwrap(filename)\n            # If it's still an iterator, convert to a list so we can iterate\n            # multiple times.\n            if not isinstance(result, list):\n                result = list(result)\n            remainder, ext = os.path.splitext(filename)\n            if ext.lower() in [\".gz\", \".gzip\"]:\n                ext = os.path.splitext(remainder)[1]\n                opened = gzip.open(filename, \"wb\")\n            else:\n                opened = open(filename, \"wb\")\n            if ext.lower() == \".csv\":\n                if selection.all_keys:\n                    headers = selection.all_keys\n                else:\n                    # Have to do this to get all the headers :(\n                    result = list(result)\n                    all_headers = set()\n                    for item in result:\n                        all_headers.update(item.keys())\n                    headers = list(all_headers)\n                with opened as ofile:\n                    writer = csv.DictWriter(\n                        ofile, fieldnames=headers, extrasaction=\"ignore\"\n                    )\n                    writer.writeheader()\n                    for item in result:\n                        count += 1\n                        writer.writerow(item)\n            elif ext.lower() == \".json\":\n                with opened as ofile:\n                    for item in result:\n                        count += 1\n                        ofile.write(self._encoder.encode(item))\n                        ofile.write(\"\\n\")\n            else:\n                with opened as ofile:\n                    for item in result:\n                        count += 1\n                        pickle.dump(item, ofile)\n            return count\n        elif not selection.is_count:\n            result = order(selection.convert(item) for item in result)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _query_and_op(self, tree, table, method_name, method_kwargs):\n        result = []\n        if tree.keys_in:\n            if tree.using:\n                raise SyntaxError(\"Cannot use USING with KEYS IN\")\n            keys = self._iter_where_in(tree)\n        else:\n            visitor = Visitor(self.reserved_words)\n            (action, kwargs, _) = self._build_query(table, tree, visitor)\n            attrs = [visitor.get_field(table.hash_key.name)]\n            if table.range_key is not None:\n                attrs.append(visitor.get_field(table.range_key.name))\n            kwargs[\"attributes\"] = attrs\n            kwargs[\"expr_values\"] = visitor.expression_values\n            kwargs[\"alias\"] = visitor.attribute_names\n            # If there is no 'where' on this update/delete, check with the\n            # caution_callback before proceeding.\n            if (\n                visitor.expression_values is None\n                and callable(self.caution_callback)\n                and not self.caution_callback(method_name)  # pylint: disable=E1102\n            ):\n                return False\n            method = getattr(self.connection, action + \"2\")\n            keys = method(table.name, **kwargs)\n            if self._explaining:\n                try:\n                    list(keys)\n                except ExplainSignal:\n                    keys = [{}]\n\n        method = getattr(self.connection, method_name + \"2\")\n        count = 0\n        for key in keys:\n            try:\n                ret = method(table.name, key, **method_kwargs)\n            except CheckFailed:\n                continue\n            count += 1\n            if ret:\n                result.append(ret)\n        if result:\n            return result\n        else:\n            return count", "response": "Query the table and perform an operation on each item"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _delete(self, tree):\n        tablename = tree.table\n        table = self.describe(tablename, require=True)\n        kwargs = {}\n        visitor = Visitor(self.reserved_words)\n        if tree.where:\n            constraints = ConstraintExpression.from_where(tree.where)\n            kwargs[\"condition\"] = constraints.build(visitor)\n        kwargs[\"expr_values\"] = visitor.expression_values\n        kwargs[\"alias\"] = visitor.attribute_names\n        return self._query_and_op(tree, table, \"delete_item\", kwargs)", "response": "Run a DELETE statement"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns an UPDATE statement", "response": "def _update(self, tree):\n        \"\"\" Run an UPDATE statement \"\"\"\n        tablename = tree.table\n        table = self.describe(tablename, require=True)\n        kwargs = {}\n\n        if tree.returns:\n            kwargs[\"returns\"] = \"_\".join(tree.returns)\n        else:\n            kwargs[\"returns\"] = \"NONE\"\n\n        visitor = Visitor(self.reserved_words)\n        updates = UpdateExpression.from_update(tree.update)\n        kwargs[\"expression\"] = updates.build(visitor)\n        if tree.where:\n            constraints = ConstraintExpression.from_where(tree.where)\n            kwargs[\"condition\"] = constraints.build(visitor)\n        kwargs[\"expr_values\"] = visitor.expression_values\n        kwargs[\"alias\"] = visitor.attribute_names\n        return self._query_and_op(tree, table, \"update_item\", kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the related record in DynamoDB.", "response": "def _create(self, tree):\n        \"\"\" Run a SELECT statement \"\"\"\n        tablename = tree.table\n        indexes = []\n        global_indexes = []\n        hash_key = None\n        range_key = None\n        attrs = {}\n        for declaration in tree.attrs:\n            name, type_ = declaration[:2]\n            if len(declaration) > 2:\n                index = declaration[2]\n            else:\n                index = None\n            if index is not None:\n                if index[0] == \"HASH\":\n                    field = hash_key = DynamoKey(name, data_type=TYPES[type_])\n                elif index[0] == \"RANGE\":\n                    field = range_key = DynamoKey(name, data_type=TYPES[type_])\n                else:\n                    index_type = index[0]\n                    kwargs = {}\n                    if index_type[0] in (\"ALL\", \"INDEX\"):\n                        factory = LocalIndex.all\n                    elif index_type[0] == \"KEYS\":\n                        factory = LocalIndex.keys\n                    elif index_type[0] == \"INCLUDE\":\n                        factory = LocalIndex.include\n                        kwargs[\"includes\"] = [resolve(v) for v in index.include_vars]\n                    index_name = resolve(index[1])\n                    field = DynamoKey(name, data_type=TYPES[type_])\n                    idx = factory(index_name, field, **kwargs)\n                    indexes.append(idx)\n            else:\n                field = DynamoKey(name, data_type=TYPES[type_])\n            attrs[field.name] = field\n\n        for gindex in tree.global_indexes:\n            global_indexes.append(self._parse_global_index(gindex, attrs))\n\n        throughput = None\n        if tree.throughput:\n            throughput = Throughput(*map(resolve, tree.throughput))\n\n        try:\n            ret = self.connection.create_table(\n                tablename,\n                hash_key,\n                range_key,\n                indexes=indexes,\n                global_indexes=global_indexes,\n                throughput=throughput,\n            )\n        except DynamoDBError as e:\n            if e.kwargs[\"Code\"] == \"ResourceInUseException\" or tree.not_exists:\n                return False\n            raise\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_global_index(self, clause, attrs):\n        index_type, name = clause[:2]\n        name = resolve(name)\n\n        def get_key(field, data_type=None):\n            \"\"\" Get or set the DynamoKey from the field name \"\"\"\n            if field in attrs:\n                key = attrs[field]\n                if data_type is not None:\n                    if TYPES[data_type] != key.data_type:\n                        raise SyntaxError(\n                            \"Key %r %s already declared with type %s\" % field,\n                            data_type,\n                            key.data_type,\n                        )\n            else:\n                if data_type is None:\n                    raise SyntaxError(\"Missing data type for %r\" % field)\n                key = DynamoKey(field, data_type=TYPES[data_type])\n                attrs[field] = key\n            return key\n\n        g_hash_key = get_key(*clause.hash_key)\n        g_range_key = None\n        # For some reason I can't get the throughput section to have a name\n        # Use an index instead\n        tp_index = 3\n        if clause.range_key:\n            tp_index += 1\n            g_range_key = get_key(*clause.range_key)\n        if clause.include_vars:\n            tp_index += 1\n        kwargs = {}\n        if tp_index < len(clause):\n            throughput = clause[tp_index]\n            kwargs[\"throughput\"] = Throughput(*map(resolve, throughput))\n        index_type = clause.index_type[0]\n        if index_type in (\"ALL\", \"INDEX\"):\n            factory = GlobalIndex.all\n        elif index_type == \"KEYS\":\n            factory = GlobalIndex.keys\n        elif index_type == \"INCLUDE\":\n            factory = GlobalIndex.include\n            if not clause.include_vars:\n                raise SyntaxError(\"Include index %r missing include fields\" % name)\n            kwargs[\"includes\"] = [resolve(v) for v in clause.include_vars]\n        return factory(name, g_hash_key, g_range_key, **kwargs)", "response": "Parse a global index clause and return a GlobalIndex object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _insert(self, tree):\n        tablename = tree.table\n        count = 0\n        kwargs = {}\n        batch = self.connection.batch_write(tablename, **kwargs)\n        with batch:\n            for item in iter_insert_items(tree):\n                batch.put(item)\n                count += 1\n        return count", "response": "Run an INSERT statement"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _drop(self, tree):\n        tablename = tree.table\n        kwargs = {}\n        try:\n            ret = self.connection.delete_table(tablename, **kwargs)\n        except DynamoDBError as e:\n            if e.kwargs[\"Code\"] == \"ResourceNotFoundException\" and tree.exists:\n                return False\n            raise\n        return True", "response": "Run a DROP statement on the table with the related tree s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_throughput(self, tablename, read, write, index):\n\n        def get_desc():\n            \"\"\" Get the table or global index description \"\"\"\n            desc = self.describe(tablename, refresh=True, require=True)\n            if index is not None:\n                return desc.global_indexes[index]\n            return desc\n\n        desc = get_desc()\n\n        def num_or_star(value):\n            \"\"\" Convert * to 0, otherwise resolve a number \"\"\"\n            return 0 if value == \"*\" else resolve(value)\n\n        read = num_or_star(read)\n        write = num_or_star(write)\n        if read <= 0:\n            read = desc.read_throughput\n        if write <= 0:\n            write = desc.write_throughput\n\n        throughput = Throughput(read, write)\n        kwargs = {}\n        if index:\n            kwargs[\"global_indexes\"] = {index: throughput}\n        else:\n            kwargs[\"throughput\"] = throughput\n        self.connection.update_table(tablename, **kwargs)\n        desc = get_desc()\n        while desc.status == \"UPDATING\":  # pragma: no cover\n            time.sleep(5)\n            desc = get_desc()", "response": "Update the throughput on a table or index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _alter(self, tree):\n        if tree.throughput:\n            [read, write] = tree.throughput\n            index = None\n            if tree.index:\n                index = tree.index\n            self._update_throughput(tree.table, read, write, index)\n        elif tree.drop_index:\n            updates = [IndexUpdate.delete(tree.drop_index[0])]\n            try:\n                self.connection.update_table(tree.table, index_updates=updates)\n            except DynamoDBError as e:\n                if tree.exists and e.kwargs[\"Code\"] == \"ResourceNotFoundException\":\n                    pass\n                else:\n                    raise\n        elif tree.create_index:\n            # GlobalIndex\n            attrs = {}\n            index = self._parse_global_index(tree.create_index, attrs)\n            updates = [IndexUpdate.create(index)]\n            try:\n                self.connection.update_table(tree.table, index_updates=updates)\n            except DynamoDBError as e:\n                if (\n                    tree.not_exists\n                    and e.kwargs[\"Code\"] == \"ValidationException\"\n                    and \"already exists\" in e.kwargs[\"Message\"]\n                ):\n                    pass\n                else:\n                    raise\n        else:\n            raise SyntaxError(\"No alter command found\")", "response": "Run an ALTER statement on the table with the related information."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dump(self, tree):\n        schema = []\n        if tree.tables:\n            for table in tree.tables:\n                desc = self.describe(table, refresh=True, require=True)\n                schema.append(desc.schema)\n        else:\n            for table in self.describe_all():\n                schema.append(table.schema)\n\n        return \"\\n\\n\".join(schema)", "response": "Run a DUMP statement"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load(self, tree):\n        filename = tree.load_file[0]\n        if filename[0] in ['\"', \"'\"]:\n            filename = unwrap(filename)\n        if not os.path.exists(filename):\n            raise Exception(\"No such file %r\" % filename)\n        batch = self.connection.batch_write(tree.table)\n        count = 0\n        with batch:\n            remainder, ext = os.path.splitext(filename)\n            if ext.lower() in [\".gz\", \".gzip\"]:\n                ext = os.path.splitext(remainder)[1]\n                opened = gzip.open(filename, \"rb\")\n            else:\n                opened = open(filename, \"r\")\n            with opened as ifile:\n                if ext.lower() == \".csv\":\n                    reader = csv.DictReader(ifile)\n                    for row in reader:\n                        batch.put(row)\n                        count += 1\n                elif ext.lower() == \".json\":\n                    for row in ifile:\n                        batch.put(json.loads(row))\n                        count += 1\n                else:\n                    try:\n                        while True:\n                            batch.put(pickle.load(ifile))\n                            count += 1\n                    except EOFError:\n                        pass\n        return count", "response": "Load a single entry from a file into a batch."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, fragment, pretty_format=True):\n        self.fragments = (self.fragments + \"\\n\" + fragment).lstrip()\n        try:\n            line_parser.parseString(self.fragments)\n        except ParseException:\n            pass\n        else:\n            self.last_query = self.fragments.strip()\n            self.fragments = \"\"\n            return super(FragmentEngine, self).execute(self.last_query, pretty_format)\n        return None", "response": "Execute a fragment and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pformat_exc(self, exc):\n        lines = []\n        try:\n            pre_nl = self.last_query.rindex(\"\\n\", 0, exc.loc) + 1\n        except ValueError:\n            pre_nl = 0\n        try:\n            post_nl = self.last_query.index(\"\\n\", exc.loc)\n        except ValueError:\n            post_nl = len(self.last_query)\n        lines.append(self.last_query[:post_nl])\n        lines.append(\" \" * (exc.loc - pre_nl) + \"^\")\n        lines.append(str(exc))\n        return \"\\n\".join(lines)", "response": "Format an exception message for the last query s parse error."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the right one for a given clause that could be a field or value.", "response": "def field_or_value(clause):\n    \"\"\"\n    For a clause that could be a field or value,\n    create the right one and return it\n\n    \"\"\"\n    if hasattr(clause, \"getName\") and clause.getName() != \"field\":\n        if clause.getName() == \"set_function\":\n            return SetFunction.from_clause(clause)\n        else:\n            return Value(resolve(clause))\n    else:\n        return Field(clause)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_update(cls, update):\n        expressions = []\n        if update.set_expr:\n            expressions.append(UpdateSetMany.from_clause(update.set_expr))\n        if update.remove_expr:\n            expressions.append(UpdateRemove.from_clause(update.remove_expr))\n        if update.add_expr:\n            expressions.append(UpdateAdd.from_clause(update.add_expr))\n        if update.delete_expr:\n            expressions.append(UpdateDelete.from_clause(update.delete_expr))\n        return cls(expressions)", "response": "Factory for creating an Update expression"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_clause(cls, clause):\n        field = clause[0]\n        value1 = field_or_value(clause[1])\n        op = None\n        value2 = None\n        if len(clause) > 2:\n            op = clause[2]\n            value2 = field_or_value(clause[3])\n        return cls(field, value1, op, value2)", "response": "Create a new object from a log entry clause."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_throughput(available, used=None):\n    if used is None:\n        return str(available)\n    percent = float(used) / available\n    return \"{0:.0f}/{1:.0f} ({2:.0%})\".format(used, available, percent)", "response": "Format the read and write throughput for display"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schema(self):\n        if self.key_type is None:\n            return \"%s %s\" % (self.name, self.data_type)\n        else:\n            return \"%s %s %s KEY\" % (self.name, self.data_type, self.key_type)", "response": "The DQL syntax for creating this item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an index field from this field", "response": "def to_index(self, index_type, index_name, includes=None):\n        \"\"\" Create an index field from this field \"\"\"\n        return IndexField(self.name, self.data_type, index_type, index_name, includes)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef schema(self):\n        schema = \"%s %s %s %s('%s'\" % (\n            self.name,\n            self.data_type,\n            self.index_type,\n            self.key_type,\n            self.index_name,\n        )\n        if self.includes is not None:\n            schema += \", [\"\n            schema += \", \".join((\"'%s'\" % i for i in self.includes))\n            schema += \"]\"\n        return schema + \")\"", "response": "The DQL syntax for creating this item"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an object from a dynamo3 response", "response": "def from_description(cls, description, attrs):\n        \"\"\" Create an object from a dynamo3 response \"\"\"\n        hash_key = None\n        range_key = None\n        index_type = description[\"Projection\"][\"ProjectionType\"]\n        includes = description[\"Projection\"].get(\"NonKeyAttributes\")\n        for data in description[\"KeySchema\"]:\n            name = data[\"AttributeName\"]\n            if name not in attrs:\n                continue\n            key_type = data[\"KeyType\"]\n            if key_type == \"HASH\":\n                hash_key = TableField(name, attrs[name].data_type, key_type)\n            elif key_type == \"RANGE\":\n                range_key = TableField(name, attrs[name].data_type, key_type)\n        throughput = description[\"ProvisionedThroughput\"]\n        return cls(\n            description[\"IndexName\"],\n            index_type,\n            description[\"IndexStatus\"],\n            hash_key,\n            range_key,\n            throughput[\"ReadCapacityUnits\"],\n            throughput[\"WriteCapacityUnits\"],\n            description.get(\"IndexSizeBytes\", 0),\n            includes,\n            description,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pformat(self, consumed_capacity=None):\n        consumed_capacity = consumed_capacity or {}\n        lines = []\n        parts = [\"GLOBAL\", self.index_type, \"INDEX\", self.name]\n        if self.status != \"ACTIVE\":\n            parts.insert(0, \"[%s]\" % self.status)\n        lines.append(\" \".join(parts))\n        lines.append(\"  items: {0:,} ({1:,} bytes)\".format(self.item_count, self.size))\n        read = \"Read: \" + format_throughput(\n            self.read_throughput, consumed_capacity.get(\"read\")\n        )\n        write = \"Write: \" + format_throughput(\n            self.write_throughput, consumed_capacity.get(\"write\")\n        )\n        lines.append(\"  \" + read + \"  \" + write)\n        lines.append(\"  \" + self.hash_key.schema)\n        if self.range_key is not None:\n            lines.append(\"  \" + self.range_key.schema)\n\n        if self.includes is not None:\n            keys = \"[%s]\" % \", \".join((\"'%s'\" % i for i in self.includes))\n            lines.append(\"  Projection: %s\" % keys)\n        return \"\\n\".join(lines)", "response": "Pretty format for insertion into table pformat"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over all indexes in the table and all global and local indexes.", "response": "def iter_query_indexes(self):\n        \"\"\"\n        Iterator that constructs :class:`~dql.models.QueryIndex` for all global\n        and local indexes, and a special one for the default table hash & range\n        key with the name 'TABLE'\n\n        \"\"\"\n        if self._table.range_key is None:\n            range_key = None\n        else:\n            range_key = self._table.range_key.name\n        yield QueryIndex(\"TABLE\", True, self._table.hash_key.name, range_key)\n        for index in self._table.indexes:\n            yield QueryIndex.from_table_index(self._table, index)\n        for index in self._table.global_indexes:\n            yield QueryIndex.from_table_index(self._table, index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_matching_indexes(self, possible_hash, possible_range):\n        matches = [\n            index\n            for index in self.iter_query_indexes()\n            if index.hash_key in possible_hash\n        ]\n        range_matches = [\n            index for index in matches if index.range_key in possible_range\n        ]\n        if range_matches:\n            return range_matches\n        return matches", "response": "Get all indexes that can be queried on using a set of keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_indexes(self):\n        ret = {}\n        for index in self.iter_query_indexes():\n            ret[index.name] = index\n        return ret", "response": "Get a dict of index names to index"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_description(cls, table):\n        throughput = table.provisioned_throughput\n        attrs = {}\n        for data in getattr(table, \"attribute_definitions\", []):\n            field = TableField(data[\"AttributeName\"], TYPES_REV[data[\"AttributeType\"]])\n            attrs[field.name] = field\n        for data in getattr(table, \"key_schema\", []):\n            name = data[\"AttributeName\"]\n            attrs[name].key_type = data[\"KeyType\"]\n        for index in getattr(table, \"local_secondary_indexes\", []):\n            for data in index[\"KeySchema\"]:\n                if data[\"KeyType\"] == \"RANGE\":\n                    name = data[\"AttributeName\"]\n                    index_type = index[\"Projection\"][\"ProjectionType\"]\n                    includes = index[\"Projection\"].get(\"NonKeyAttributes\")\n                    attrs[name] = attrs[name].to_index(\n                        index_type, index[\"IndexName\"], includes\n                    )\n                    break\n        global_indexes = {}\n        for index in getattr(table, \"global_secondary_indexes\", []):\n            idx = GlobalIndex.from_description(index, attrs)\n            global_indexes[idx.name] = idx\n        return cls(\n            table,\n            attrs,\n            global_indexes,\n            throughput[\"ReadCapacityUnits\"],\n            throughput[\"WriteCapacityUnits\"],\n            throughput[\"NumberOfDecreasesToday\"],\n            table.table_size_bytes,\n        )", "response": "Create a new instance of the class from a table description."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef primary_key_attributes(self):\n        if self.range_key is None:\n            return (self.hash_key.name,)\n        else:\n            return (self.hash_key.name, self.range_key.name)", "response": "Return the names of the primary key attributes as a tuple"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the primary key tuple from an item", "response": "def primary_key_tuple(self, item):\n        \"\"\" Get the primary key tuple from an item \"\"\"\n        if self.range_key is None:\n            return (item[self.hash_key.name],)\n        else:\n            return (item[self.hash_key.name], item[self.range_key.name])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef primary_key(self, hkey, rkey=None):\n        if isinstance(hkey, dict):\n\n            def decode(val):\n                \"\"\" Convert Decimals back to primitives \"\"\"\n                if isinstance(val, Decimal):\n                    return float(val)\n                return val\n\n            pkey = {self.hash_key.name: decode(hkey[self.hash_key.name])}\n            if self.range_key is not None:\n                pkey[self.range_key.name] = decode(hkey[self.range_key.name])\n            return pkey\n        else:\n            pkey = {self.hash_key.name: hkey}\n            if self.range_key is not None:\n                if rkey is None:\n                    raise ValueError(\"Range key is missing!\")\n                pkey[self.range_key.name] = rkey\n            return pkey", "response": "Construct a dictionary that represents the primary key of the item."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the total read throughput of all tables and global indexes.", "response": "def total_read_throughput(self):\n        \"\"\" Combined read throughput of table and global indexes \"\"\"\n        total = self.read_throughput\n        for index in itervalues(self.global_indexes):\n            total += index.read_throughput\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef total_write_throughput(self):\n        total = self.write_throughput\n        for index in itervalues(self.global_indexes):\n            total += index.write_throughput\n        return total", "response": "Returns the total write throughput of all tables and global indexes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef schema(self):\n        attrs = self.attrs.copy()\n        parts = [\"CREATE\", \"TABLE\", self.name, \"(%s,\" % self.hash_key.schema]\n        del attrs[self.hash_key.name]\n        if self.range_key:\n            parts.append(self.range_key.schema + \",\")\n            del attrs[self.range_key.name]\n        if attrs:\n            attr_def = \", \".join([attr.schema for attr in itervalues(attrs)])\n            parts.append(attr_def + \",\")\n\n        parts.append(\n            \"THROUGHPUT (%d, %d))\" % (self.read_throughput, self.write_throughput)\n        )\n        parts.extend([g.schema for g in itervalues(self.global_indexes)])\n        return \" \".join(parts) + \";\"", "response": "The DQL query that will construct this table s schema"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve(val):\n    name = val.getName()\n    if name == \"number\":\n        try:\n            return int(val.number)\n        except ValueError:\n            return Decimal(val.number)\n    elif name == \"str\":\n        return unwrap(val.str)\n    elif name == \"null\":\n        return None\n    elif name == \"binary\":\n        return Binary(val.binary[2:-1])\n    elif name == \"set\":\n        if val.set == \"()\":\n            return set()\n        return set([resolve(v) for v in val.set])\n    elif name == \"bool\":\n        return val.bool == \"TRUE\"\n    elif name == \"list\":\n        return [resolve(v) for v in val.list]\n    elif name == \"dict\":\n        dict_val = {}\n        for k, v in val.dict:\n            dict_val[resolve(k)] = resolve(v)\n        return dict_val\n    elif name == \"ts_function\":\n        return dt_to_ts(eval_function(val.ts_function))\n    elif name == \"ts_expression\":\n        return dt_to_ts(eval_expression(val))\n    else:\n        raise SyntaxError(\"Unable to resolve value '%s'\" % val)", "response": "Resolve a pyparsing value to the python type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a datetime to timestamp", "response": "def dt_to_ts(value):\n    \"\"\" If value is a datetime, convert to timestamp \"\"\"\n    if not isinstance(value, datetime):\n        return value\n    return calendar.timegm(value.utctimetuple()) + value.microsecond / 1000000.0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eval_function(value):\n    name, args = value[0], value[1:]\n    if name == \"NOW\":\n        return datetime.utcnow().replace(tzinfo=tzutc())\n    elif name in [\"TIMESTAMP\", \"TS\"]:\n        return parse(unwrap(args[0])).replace(tzinfo=tzlocal())\n    elif name in [\"UTCTIMESTAMP\", \"UTCTS\"]:\n        return parse(unwrap(args[0])).replace(tzinfo=tzutc())\n    elif name == \"MS\":\n        return 1000 * resolve(args[0])\n    else:\n        raise SyntaxError(\"Unrecognized function %r\" % name)", "response": "Evaluate a timestamp function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eval_interval(interval):\n    kwargs = {\n        \"years\": 0,\n        \"months\": 0,\n        \"weeks\": 0,\n        \"days\": 0,\n        \"hours\": 0,\n        \"minutes\": 0,\n        \"seconds\": 0,\n        \"microseconds\": 0,\n    }\n    for section in interval[1:]:\n        name = section.getName()\n        if name == \"year\":\n            kwargs[\"years\"] += int(section[0])\n        elif name == \"month\":\n            kwargs[\"months\"] += int(section[0])\n        elif name == \"week\":\n            kwargs[\"weeks\"] += int(section[0])\n        elif name == \"day\":\n            kwargs[\"days\"] += int(section[0])\n        elif name == \"hour\":\n            kwargs[\"hours\"] += int(section[0])\n        elif name == \"minute\":\n            kwargs[\"minutes\"] += int(section[0])\n        elif name == \"second\":\n            kwargs[\"seconds\"] += int(section[0])\n        elif name == \"millisecond\":\n            kwargs[\"microseconds\"] += 1000 * int(section[0])\n        elif name == \"microsecond\":\n            kwargs[\"microseconds\"] += int(section[0])\n        else:\n            raise SyntaxError(\"Unrecognized interval type %r: %s\" % (name, section))\n    return relativedelta(**kwargs)", "response": "Evaluate an interval expression"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eval_expression(value):\n    start = eval_function(value.ts_expression[0])\n    interval = eval_interval(value.ts_expression[2])\n    op = value.ts_expression[1]\n    if op == \"+\":\n        return start + interval\n    elif op == \"-\":\n        return start - interval\n    else:\n        raise SyntaxError(\"Unrecognized operator %r\" % op)", "response": "Evaluate a full time expression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nresolve source code to github", "response": "def linkcode_resolve(domain, info):\n    \"\"\" Link source code to github \"\"\"\n    if domain != 'py' or not info['module']:\n        return None\n    filename = info['module'].replace('.', '/')\n    mod = importlib.import_module(info['module'])\n    basename = os.path.splitext(mod.__file__)[0]\n    if basename.endswith('__init__'):\n        filename += '/__init__'\n    item = mod\n    lineno = ''\n    for piece in info['fullname'].split('.'):\n        item = getattr(item, piece)\n        try:\n            lineno = '#L%d' % inspect.getsourcelines(item)[1]\n        except (TypeError, IOError):\n            pass\n    return (\"https://github.com/%s/%s/blob/%s/%s.py%s\" %\n            (github_user, project, release, filename, lineno))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing curses and refresh in a loop", "response": "def run(self, stdscr):\n        \"\"\" Initialize curses and refresh in a loop \"\"\"\n        self.win = stdscr\n        curses.curs_set(0)\n        stdscr.timeout(0)\n        curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK)\n        curses.init_pair(2, curses.COLOR_GREEN, curses.COLOR_BLACK)\n        curses.init_pair(3, curses.COLOR_YELLOW, curses.COLOR_BLACK)\n        curses.init_pair(4, curses.COLOR_RED, curses.COLOR_BLACK)\n        while True:\n            self.refresh(True)\n            now = time.time()\n            while time.time() - now < self._refresh_rate:\n                time.sleep(0.1)\n                self.refresh(False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the minimum allowable width for a table.", "response": "def _calc_min_width(self, table):\n        \"\"\" Calculate the minimum allowable width for a table \"\"\"\n        width = len(table.name)\n        cap = table.consumed_capacity[\"__table__\"]\n        width = max(width, 4 + len(\"%.1f/%d\" % (cap[\"read\"], table.read_throughput)))\n        width = max(width, 4 + len(\"%.1f/%d\" % (cap[\"write\"], table.write_throughput)))\n        for index_name, cap in iteritems(table.consumed_capacity):\n            if index_name == \"__table__\":\n                continue\n            index = table.global_indexes[index_name]\n            width = max(\n                width,\n                4 + len(index_name + \"%.1f/%d\" % (cap[\"read\"], index.read_throughput)),\n            )\n            width = max(\n                width,\n                4\n                + len(index_name + \"%.1f/%d\" % (cap[\"write\"], index.write_throughput)),\n            )\n        return width"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _progress_bar(self, width, percent, left=\"\", right=\"\", fill=\"|\"):\n        text = left + (width - len(left) - len(right)) * \" \" + right\n        cutoff = int(round(percent * width))\n        text = text[:cutoff].replace(\" \", fill) + text[cutoff:]\n        low_cutoff = int(round(0.7 * width))\n        med_cutoff = int(round(0.9 * width))\n        if percent < 0.7:\n            yield 2, text[:cutoff]\n            yield 0, text[cutoff:]\n        elif percent < 0.9:\n            yield 2, text[:low_cutoff]\n            yield 3, text[low_cutoff:cutoff]\n            yield 0, text[cutoff:]\n        else:\n            yield 2, text[:low_cutoff]\n            yield 3, text[low_cutoff:med_cutoff]\n            yield 4, text[med_cutoff:cutoff]\n            yield 0, text[cutoff:]", "response": "Get the green or yellow pieces of a text + bar display"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_throughput(self, y, x, width, op, title, available, used):\n        percent = float(used) / available\n        self.win.addstr(y, x, \"[\")\n        # Because we have disabled scrolling, writing the lower right corner\n        # character in a terminal can throw an error (this is inside the curses\n        # implementation). If that happens (and it will only ever happen here),\n        # we should just catch it and continue.\n        try:\n            self.win.addstr(y, x + width - 1, \"]\")\n        except curses.error:\n            pass\n        x += 1\n        right = \"%.1f/%d:%s\" % (used, available, op)\n        pieces = self._progress_bar(width - 2, percent, title, right)\n        for color, text in pieces:\n            self.win.addstr(y, x, text, curses.color_pair(color))\n            x += len(text)", "response": "Write a single throughput measure to a row"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrefresh the display for the available tables.", "response": "def refresh(self, fetch_data):\n        \"\"\" Redraw the display \"\"\"\n        self.win.erase()\n        height, width = getmaxyx()\n        if curses.is_term_resized(height, width):\n            self.win.clear()\n            curses.resizeterm(height, width)\n        y = 1  # Starts at 1 because of date string\n        x = 0\n        columns = []\n        column = []\n        for table in self._tables:\n            desc = self.engine.describe(table, fetch_data, True)\n            line_count = 1 + 2 * len(desc.consumed_capacity)\n            if (column or columns) and line_count + y > height:\n                columns.append(column)\n                column = []\n                y = 1\n            y += line_count\n            column.append(desc)\n\n        columns.append(column)\n        y = 1\n\n        # Calculate the min width of each column\n        column_widths = []\n        for column in columns:\n            column_widths.append(max(map(self._calc_min_width, column)))\n        # Find how many columns we can support\n        while len(columns) > 1 and sum(column_widths) > width - len(columns) + 1:\n            columns.pop()\n            column_widths.pop()\n        effective_width = width - len(columns) + 1\n        # Iteratively expand columns until we fit the width or they are all max\n        while sum(column_widths) < effective_width and any(\n            (w < self._max_width for w in column_widths)\n        ):\n            smallest = min(column_widths)\n            i = column_widths.index(smallest)\n            column_widths[i] += 1\n\n        status = datetime.now().strftime(\"%H:%M:%S\")\n        status += \" %d tables\" % len(self._tables)\n        num_displayed = sum(map(len, columns))\n        if num_displayed < len(self._tables):\n            status += \" (%d visible)\" % num_displayed\n        self.win.addstr(0, 0, status[:width])\n\n        for column, col_width in zip(columns, column_widths):\n            for table in column:\n                cap = table.consumed_capacity[\"__table__\"]\n                self.win.addstr(y, x, table.name, curses.color_pair(1))\n                self._add_throughput(\n                    y + 1, x, col_width, \"R\", \"\", table.read_throughput, cap[\"read\"]\n                )\n                self._add_throughput(\n                    y + 2, x, col_width, \"W\", \"\", table.write_throughput, cap[\"write\"]\n                )\n                y += 3\n                for index_name, cap in iteritems(table.consumed_capacity):\n                    if index_name == \"__table__\":\n                        continue\n                    index = table.global_indexes[index_name]\n                    self._add_throughput(\n                        y,\n                        x,\n                        col_width,\n                        \"R\",\n                        index_name,\n                        index.read_throughput,\n                        cap[\"read\"],\n                    )\n                    self._add_throughput(\n                        y + 1,\n                        x,\n                        col_width,\n                        \"W\",\n                        index_name,\n                        index.write_throughput,\n                        cap[\"write\"],\n                    )\n                    y += 2\n            x += col_width + 1\n            y = 1\n\n        self.win.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(a, b):\n    if a is None:\n        if b is None:\n            return None\n        else:\n            return b\n    elif b is None:\n        return a\n    return a + b", "response": "Add two values ignoring None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubtract two values ignoring None", "response": "def sub(a, b):\n    \"\"\" Subtract two values, ignoring None \"\"\"\n    if a is None:\n        if b is None:\n            return None\n        else:\n            return -1 * b\n    elif b is None:\n        return a\n    return a - b"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mul(a, b):\n    if a is None:\n        if b is None:\n            return None\n        else:\n            return b\n    elif b is None:\n        return a\n    return a * b", "response": "Multiply two values ignoring None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndivides two values ignoring None", "response": "def div(a, b):\n    \"\"\" Divide two values, ignoring None \"\"\"\n    if a is None:\n        if b is None:\n            return None\n        else:\n            return 1 / b\n    elif b is None:\n        return a\n    return a / b"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a clause that could be a field value or expression.", "response": "def parse_expression(clause):\n    \"\"\" For a clause that could be a field, value, or expression \"\"\"\n    if isinstance(clause, Expression):\n        return clause\n    elif hasattr(clause, \"getName\") and clause.getName() != \"field\":\n        if clause.getName() == \"nested\":\n            return AttributeSelection.from_statement(clause)\n        elif clause.getName() == \"function\":\n            return SelectFunction.from_statement(clause)\n        else:\n            return Value(resolve(clause[0]))\n    else:\n        return Field(clause[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a selection function from a statement", "response": "def from_statement(cls, statement):\n        \"\"\" Create a selection function from a statement \"\"\"\n        if statement[0] in [\"TS\", \"TIMESTAMP\", \"UTCTIMESTAMP\", \"UTCTS\"]:\n            return TimestampFunction.from_statement(statement)\n        elif statement[0] in [\"NOW\", \"UTCNOW\"]:\n            return NowFunction.from_statement(statement)\n        else:\n            raise SyntaxError(\"Unknown function %r\" % statement[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the function expressions for selection", "response": "def select_functions(expr):\n    \"\"\" Create the function expressions for selection \"\"\"\n    body = Group(expr)\n    return Group(\n        function(\"timestamp\", body, caseless=True)\n        | function(\"ts\", body, caseless=True)\n        | function(\"utctimestamp\", body, caseless=True)\n        | function(\"utcts\", body, caseless=True)\n        | function(\"now\", caseless=True)\n        | function(\"utcnow\", caseless=True)\n    ).setResultsName(\"function\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_selection():\n    operation = Forward()\n    nested = Group(Suppress(\"(\") + operation + Suppress(\")\")).setResultsName(\"nested\")\n    select_expr = Forward()\n    functions = select_functions(select_expr)\n    maybe_nested = functions | nested | Group(var_val)\n    operation <<= maybe_nested + OneOrMore(oneOf(\"+ - * /\") + maybe_nested)\n    select_expr <<= operation | maybe_nested\n    alias = Group(Suppress(upkey(\"as\")) + var).setResultsName(\"alias\")\n    full_select = Group(\n        Group(select_expr).setResultsName(\"selection\") + Optional(alias)\n    )\n    return Group(\n        Keyword(\"*\") | upkey(\"count(*)\") | delimitedList(full_select)\n    ).setResultsName(\"attrs\")", "response": "Create a selection expression"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a constraint for a query WHERE clause", "response": "def create_query_constraint():\n    \"\"\" Create a constraint for a query WHERE clause \"\"\"\n    op = oneOf(\"= < > >= <= != <>\", caseless=True).setName(\"operator\")\n    basic_constraint = (var + op + var_val).setResultsName(\"operator\")\n    between = (\n        var + Suppress(upkey(\"between\")) + value + Suppress(and_) + value\n    ).setResultsName(\"between\")\n    is_in = (var + Suppress(upkey(\"in\")) + set_).setResultsName(\"in\")\n    fxn = (\n        function(\"attribute_exists\", var)\n        | function(\"attribute_not_exists\", var)\n        | function(\"attribute_type\", var, types)\n        | function(\"begins_with\", var, Group(string))\n        | function(\"contains\", var, value)\n        | (function(\"size\", var) + op + value)\n    ).setResultsName(\"function\")\n    all_constraints = between | basic_constraint | is_in | fxn\n    return Group(all_constraints).setName(\"constraint\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_where():\n    conjunction = Forward().setResultsName(\"conjunction\")\n    nested = Group(Suppress(\"(\") + conjunction + Suppress(\")\")).setResultsName(\n        \"conjunction\"\n    )\n\n    maybe_nested = nested | constraint\n    inverted = Group(not_ + maybe_nested).setResultsName(\"not\")\n    full_constraint = maybe_nested | inverted\n    conjunction <<= full_constraint + OneOrMore(and_or + full_constraint)\n    return upkey(\"where\") + Group(conjunction | full_constraint).setResultsName(\"where\")", "response": "Create a grammar for the where clause used by select."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_keys_in():\n    keys = Group(\n        Optional(Suppress(\"(\"))\n        + value\n        + Optional(Suppress(\",\") + value)\n        + Optional(Suppress(\")\"))\n    )\n    return (Suppress(upkey(\"keys\") + upkey(\"in\")) + delimitedList(keys)).setResultsName(\n        \"keys_in\"\n    )", "response": "Create a new nparray tree for the KEYS IN clause used for queries"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a item and a field return the value of that field.", "response": "def evaluate(self, item):\n        \"\"\" Pull the field off the item \"\"\"\n        try:\n            for match in PATH_PATTERN.finditer(self.field):\n                path = match.group(0)\n                if path[0] == \"[\":\n                    # If we're selecting an item at a specific index of an\n                    # array, we will usually not get back the whole array from\n                    # Dynamo. It'll return an array with one element.\n                    try:\n                        item = item[int(match.group(1))]\n                    except IndexError:\n                        item = item[0]\n                else:\n                    item = item.get(path)\n        except (IndexError, TypeError, AttributeError):\n            return None\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a standalone dql executable", "response": "def main():\n    \"\"\" Build a standalone dql executable \"\"\"\n    venv_dir = tempfile.mkdtemp()\n    try:\n        make_virtualenv(venv_dir)\n\n        print(\"Downloading dependencies\")\n        pip = os.path.join(venv_dir, \"bin\", \"pip\")\n        subprocess.check_call([pip, \"install\", \"pex\"])\n\n        print(\"Building executable\")\n        pex = os.path.join(venv_dir, \"bin\", \"pex\")\n        subprocess.check_call([pex, \"dql\", \"-m\", \"dql:main\", \"-o\", \"dql\"])\n\n        print(\"dql executable written to %s\" % os.path.abspath(\"dql\"))\n    finally:\n        shutil.rmtree(venv_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace the path of a resource with a new one if needed.", "response": "def _maybe_replace_path(self, match):\n        \"\"\" Regex replacement method that will sub paths when needed \"\"\"\n        path = match.group(0)\n        if self._should_replace(path):\n            return self._replace_path(path)\n        else:\n            return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrue if the path should be replaced", "response": "def _should_replace(self, path):\n        \"\"\" True if should replace the path \"\"\"\n        return (\n            self._reserved_words is None\n            or path.upper() in self._reserved_words\n            or \"-\" in path\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreplacing the path in the dict with the replacement value for the path.", "response": "def _replace_path(self, path):\n        \"\"\" Get the replacement value for a path \"\"\"\n        if path in self._field_to_key:\n            return self._field_to_key[path]\n        next_key = \"#f%d\" % self._next_field\n        self._next_field += 1\n        self._field_to_key[path] = next_key\n        self._fields[next_key] = path\n        return next_key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the next value for a variable name.", "response": "def get_value(self, value):\n        \"\"\" Replace variable names with placeholders (e.g. ':v1') \"\"\"\n        next_key = \":v%d\" % self._next_value\n        self._next_value += 1\n        self._values[next_key] = value\n        return next_key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntruncate a string to a length.", "response": "def truncate(string, length, ellipsis=\"\u2026\"):\n    \"\"\" Truncate a string to a length, ending with '...' if it overflows \"\"\"\n    if len(string) > length:\n        return string[: length - len(ellipsis)] + ellipsis\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping a string at a line length", "response": "def wrap(string, length, indent):\n    \"\"\" Wrap a string at a line length \"\"\"\n    newline = \"\\n\" + \" \" * indent\n    return newline.join((string[i : i + length] for i in range(0, len(string), length)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a relativedelta to a human - readable string", "response": "def delta_to_str(rd):\n    \"\"\" Convert a relativedelta to a human-readable string \"\"\"\n    parts = []\n    if rd.days > 0:\n        parts.append(\"%d day%s\" % (rd.days, plural(rd.days)))\n    clock_parts = []\n    if rd.hours > 0:\n        clock_parts.append(\"%02d\" % rd.hours)\n    if rd.minutes > 0 or rd.hours > 0:\n        clock_parts.append(\"%02d\" % rd.minutes)\n    if rd.seconds > 0 or rd.minutes > 0 or rd.hours > 0:\n        clock_parts.append(\"%02d\" % rd.seconds)\n    if clock_parts:\n        parts.append(\":\".join(clock_parts))\n    return \" \".join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay the contents of a single element of a list of items in a single file.", "response": "def less_display():\n    \"\"\" Use smoke and mirrors to acquire 'less' for pretty paging \"\"\"\n    # here's some magic. We want the nice paging from 'less', so we write\n    # the output to a file and use subprocess to run 'less' on the file.\n    # But the file might have sensitive data, so open it in 0600 mode.\n    _, filename = tempfile.mkstemp()\n    mode = stat.S_IRUSR | stat.S_IWUSR\n    outfile = None\n    outfile = os.fdopen(os.open(filename, os.O_WRONLY | os.O_CREAT, mode), \"wb\")\n    try:\n        yield SmartBuffer(outfile)\n        outfile.flush()\n        subprocess.call([\"less\", \"-FXR\", filename])\n    finally:\n        if outfile is not None:\n            outfile.close()\n        if os.path.exists(filename):\n            os.unlink(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stdout_display():\n    if sys.version_info[0] == 2:\n        yield SmartBuffer(sys.stdout)\n    else:\n        yield SmartBuffer(sys.stdout.buffer)", "response": "Print results straight to stdout"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite results to an output stream", "response": "def display(self):\n        \"\"\" Write results to an output stream \"\"\"\n        total = 0\n        count = 0\n        for i, result in enumerate(self._results):\n            if total == 0:\n                self.pre_write()\n            self.write(result)\n            count += 1\n            total += 1\n            if (\n                count >= self.pagesize\n                and self.pagesize > 0\n                and i < len(self._results) - 1\n            ):\n                self.wait()\n                count = 0\n        if total == 0:\n            self._ostream.write(\"No results\\n\")\n        else:\n            self.post_write()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait(self):\n        text = input(\n            \"Press return for next %d result%s (or type 'all'):\"\n            % (self.pagesize, plural(self.pagesize))\n        )\n        if text:\n            if text.lower() in [\"a\", \"all\"]:\n                self._pagesize = 0\n            elif text.isdigit():\n                self._pagesize = int(text)", "response": "Block for user input"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_field(self, field):\n        if field is None:\n            return \"NULL\"\n        elif isinstance(field, TypeError):\n            return \"TypeError\"\n        elif isinstance(field, Decimal):\n            if field % 1 == 0:\n                return str(int(field))\n            return str(float(field))\n        elif isinstance(field, set):\n            return \"(\" + \", \".join([self.format_field(v) for v in field]) + \")\"\n        elif isinstance(field, datetime):\n            return field.isoformat()\n        elif isinstance(field, timedelta):\n            rd = relativedelta(\n                seconds=int(field.total_seconds()), microseconds=field.microseconds\n            )\n            return delta_to_str(rd)\n        elif isinstance(field, Binary):\n            return \"<Binary %d>\" % len(field.value)\n        pretty = repr(field)\n        if pretty.startswith(\"u'\"):\n            return pretty[1:]\n        return pretty", "response": "Format a single Dynamo field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting out the header of the log file.", "response": "def _write_header(self):\n        \"\"\" Write out the table header \"\"\"\n        self._ostream.write(len(self._header) * \"-\" + \"\\n\")\n        self._ostream.write(self._header)\n        self._ostream.write(\"\\n\")\n        self._ostream.write(len(self._header) * \"-\" + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nblock for user input", "response": "def wait(self):\n        \"\"\" Block for user input \"\"\"\n        self._write_footer()\n        super(ColumnFormat, self).wait()\n        self._write_header()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, arg):\n        if isinstance(arg, str):\n            arg = arg.encode(self.encoding)\n        return self._buffer.write(arg)", "response": "Writes a string or bytes object to the buffer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprompt user for input", "response": "def prompt(msg, default=NO_DEFAULT, validate=None):\n    \"\"\" Prompt user for input \"\"\"\n    while True:\n        response = input(msg + \" \").strip()\n        if not response:\n            if default is NO_DEFAULT:\n                continue\n            return default\n        if validate is None or validate(response):\n            return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef promptyn(msg, default=None):\n    while True:\n        yes = \"Y\" if default else \"y\"\n        if default or default is None:\n            no = \"n\"\n        else:\n            no = \"N\"\n        confirm = prompt(\"%s [%s/%s]\" % (msg, yes, no), \"\").lower()\n        if confirm in (\"y\", \"yes\"):\n            return True\n        elif confirm in (\"n\", \"no\"):\n            return False\n        elif not confirm and default is not None:\n            return default", "response": "Display a blocking prompt until the user confirms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget an enum key by prefix or equality", "response": "def get_enum_key(key, choices):\n    \"\"\" Get an enum by prefix or equality \"\"\"\n    if key in choices:\n        return key\n    keys = [k for k in choices if k.startswith(key)]\n    if len(keys) == 1:\n        return keys[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart the interactive session", "response": "def start(self):\n        \"\"\" Start running the interactive session (blocking) \"\"\"\n        self.running = True\n        while self.running:\n            self.update_prompt()\n            try:\n                self.cmdloop()\n            except KeyboardInterrupt:\n                print()\n            except botocore.exceptions.BotoCoreError as e:\n                print(e)\n            except ParseException as e:\n                print(self.engine.pformat_exc(e))\n            except Exception:\n                traceback.print_exc()\n            self.engine.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a shell command", "response": "def do_shell(self, arglist):\n        \"\"\" Run a shell command \"\"\"\n        proc = subprocess.Popen(\n            shlex.split(arglist), stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n        )\n        print(proc.communicate()[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the config file", "response": "def save_config(self):\n        \"\"\" Save the conf file \"\"\"\n        if not os.path.exists(self._conf_dir):\n            os.makedirs(self._conf_dir)\n        conf_file = os.path.join(self._conf_dir, \"dql.json\")\n        with open(conf_file, \"w\") as ofile:\n            json.dump(self.conf, ofile, indent=2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_config(self):\n        conf_file = os.path.join(self._conf_dir, \"dql.json\")\n        if not os.path.exists(conf_file):\n            return {}\n        with open(conf_file, \"r\") as ifile:\n            return json.load(ifile)", "response": "Load your configuration settings from a file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting and set options", "response": "def do_opt(self, *args, **kwargs):\n        \"\"\" Get and set options \"\"\"\n        args = list(args)\n        if not args:\n            largest = 0\n            keys = [key for key in self.conf if not key.startswith(\"_\")]\n            for key in keys:\n                largest = max(largest, len(key))\n            for key in keys:\n                print(\"%s : %s\" % (key.rjust(largest), self.conf[key]))\n            return\n        option = args.pop(0)\n        if not args and not kwargs:\n            method = getattr(self, \"getopt_\" + option, None)\n            if method is None:\n                self.getopt_default(option)\n            else:\n                method()\n        else:\n            method = getattr(self, \"opt_\" + option, None)\n            if method is None:\n                print(\"Unrecognized option %r\" % option)\n            else:\n                method(*args, **kwargs)\n                self.save_config()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getopt_default(self, option):\n        if option not in self.conf:\n            print(\"Unrecognized option %r\" % option)\n            return\n        print(\"%s: %s\" % (option, self.conf[option]))", "response": "Default method to get an option"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef opt_width(self, width):\n        if width != \"auto\":\n            width = int(width)\n        self.conf[\"width\"] = width", "response": "Set the width of output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset or get the page size of the query output", "response": "def opt_pagesize(self, pagesize):\n        \"\"\" Get or set the page size of the query output \"\"\"\n        if pagesize != \"auto\":\n            pagesize = int(pagesize)\n        self.conf[\"pagesize\"] = pagesize"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef opt_display(self, display):\n        key = get_enum_key(display, DISPLAYS)\n        if key is not None:\n            self.conf[\"display\"] = key\n            self.display = DISPLAYS[key]\n            print(\"Set display %r\" % key)\n        else:\n            print(\"Unknown display %r\" % display)", "response": "Set value for display option"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef complete_opt_display(self, text, *_):\n        return [t + \" \" for t in DISPLAYS if t.startswith(text)]", "response": "Autocomplete for display option"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset value for format option", "response": "def opt_format(self, fmt):\n        \"\"\" Set value for format option \"\"\"\n        key = get_enum_key(fmt, FORMATTERS)\n        if key is not None:\n            self.conf[\"format\"] = key\n            print(\"Set format %r\" % key)\n        else:\n            print(\"Unknown format %r\" % fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_watch(self, *args):\n        tables = []\n        if not self.engine.cached_descriptions:\n            self.engine.describe_all()\n        all_tables = list(self.engine.cached_descriptions)\n        for arg in args:\n            candidates = set((t for t in all_tables if fnmatch(t, arg)))\n            for t in sorted(candidates):\n                if t not in tables:\n                    tables.append(t)\n\n        mon = Monitor(self.engine, tables)\n        mon.start()", "response": "Watch Dynamo tables consumed capacity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_file(self, filename):\n        with open(filename, \"r\") as infile:\n            self._run_cmd(infile.read())", "response": "Read and execute a. sql file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef complete_file(self, text, line, *_):\n        leading = line[len(\"file \") :]\n        curpath = os.path.join(os.path.curdir, leading)\n\n        def isdql(parent, filename):\n            \"\"\" Check if a file is .dql or a dir \"\"\"\n            return not filename.startswith(\".\") and (\n                os.path.isdir(os.path.join(parent, filename))\n                or filename.lower().endswith(\".dql\")\n            )\n\n        def addslash(path):\n            \"\"\" Append a slash if a file is a directory \"\"\"\n            if path.lower().endswith(\".dql\"):\n                return path + \" \"\n            else:\n                return path + \"/\"\n\n        if not os.path.exists(curpath) or not os.path.isdir(curpath):\n            curpath = os.path.dirname(curpath)\n        return [\n            addslash(f)\n            for f in os.listdir(curpath)\n            if f.startswith(text) and isdql(curpath, f)\n        ]", "response": "Autocomplete for a single DQL file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all tables or print details of one table", "response": "def do_ls(self, table=None):\n        \"\"\" List all tables or print details of one table \"\"\"\n        if table is None:\n            fields = OrderedDict(\n                [\n                    (\"Name\", \"name\"),\n                    (\"Status\", \"status\"),\n                    (\"Read\", \"total_read_throughput\"),\n                    (\"Write\", \"total_write_throughput\"),\n                ]\n            )\n            tables = self.engine.describe_all()\n            # Calculate max width of all items for each column\n            sizes = [\n                1 + max([len(str(getattr(t, f))) for t in tables] + [len(title)])\n                for title, f in iteritems(fields)\n            ]\n            # Print the header\n            for size, title in zip(sizes, fields):\n                print(title.ljust(size), end=\"\")\n            print()\n            # Print each table row\n            for row_table in tables:\n                for size, field in zip(sizes, fields.values()):\n                    print(str(getattr(row_table, field)).ljust(size), end=\"\")\n                print()\n        else:\n            print(self.engine.describe(table, refresh=True, metrics=True).pformat())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to a local DynamoDB instance. Use local off to disable.", "response": "def do_local(self, host=\"localhost\", port=8000):\n        \"\"\"\n        Connect to a local DynamoDB instance. Use 'local off' to disable.\n\n        > local\n        > local host=localhost port=8001\n        > local off\n\n        \"\"\"\n        port = int(port)\n        if host == \"off\":\n            self._local_endpoint = None\n        else:\n            self._local_endpoint = (host, port)\n        self.onecmd(\"use %s\" % self.engine.region)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_use(self, region):\n        if self._local_endpoint is not None:\n            host, port = self._local_endpoint  # pylint: disable=W0633\n            self.engine.connect(\n                region, session=self.session, host=host, port=port, is_secure=False\n            )\n        else:\n            self.engine.connect(region, session=self.session)", "response": "Switch the AWS region\n            > use us - east - 1\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the allowed consumed throughput for DQL.", "response": "def do_throttle(self, *args):\n        \"\"\"\n        Set the allowed consumed throughput for DQL.\n\n        # Set the total allowed throughput across all tables\n        > throttle 1000 100\n        # Set the default allowed throughput per-table/index\n        > throttle default 40% 20%\n        # Set the allowed throughput on a table\n        > throttle mytable 10 10\n        # Set the allowed throughput on a global index\n        > throttle mytable myindex 40 6\n\n        see also: unthrottle\n\n        \"\"\"\n        args = list(args)\n        if not args:\n            print(self.throttle)\n            return\n        if len(args) < 2:\n            return self.onecmd(\"help throttle\")\n        args, read, write = args[:-2], args[-2], args[-1]\n        if len(args) == 2:\n            tablename, indexname = args\n            self.throttle.set_index_limit(tablename, indexname, read, write)\n        elif len(args) == 1:\n            tablename = args[0]\n            if tablename == \"default\":\n                self.throttle.set_default_limit(read, write)\n            elif tablename == \"total\":\n                self.throttle.set_total_limit(read, write)\n            else:\n                self.throttle.set_table_limit(tablename, read, write)\n        elif not args:\n            self.throttle.set_total_limit(read, write)\n        else:\n            return self.onecmd(\"help throttle\")\n        self.conf[\"_throttle\"] = self.throttle.save()\n        self.save_config()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the throughput limits for DQL that were set with throttle", "response": "def do_unthrottle(self, *args):\n        \"\"\"\n        Remove the throughput limits for DQL that were set with 'throttle'\n\n        # Remove all limits\n        > unthrottle\n        # Remove the limit on total allowed throughput\n        > unthrottle total\n        # Remove the default limit\n        > unthrottle default\n        # Remove the limit on a table\n        > unthrottle mytable\n        # Remove the limit on a global index\n        > unthrottle mytable myindex\n\n        \"\"\"\n        if not args:\n            if promptyn(\"Are you sure you want to clear all throttles?\"):\n                self.throttle.load({})\n        elif len(args) == 1:\n            tablename = args[0]\n            if tablename == \"total\":\n                self.throttle.set_total_limit()\n            elif tablename == \"default\":\n                self.throttle.set_default_limit()\n            else:\n                self.throttle.set_table_limit(tablename)\n        elif len(args) == 2:\n            tablename, indexname = args\n            self.throttle.set_index_limit(tablename, indexname)\n        else:\n            self.onecmd(\"help unthrottle\")\n        self.conf[\"_throttle\"] = self.throttle.save()\n        self.save_config()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef completedefault(self, text, line, *_):\n        tokens = line.split()\n        try:\n            before = tokens[-2]\n            complete = before.lower() in (\"from\", \"update\", \"table\", \"into\")\n            if tokens[0].lower() == \"dump\":\n                complete = True\n            if complete:\n                return [\n                    t + \" \"\n                    for t in self.engine.cached_descriptions\n                    if t.startswith(text)\n                ]\n        except KeyError:\n            pass", "response": "Autocomplete table names in queries"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_cmd(self, command):\n        if self.throttle:\n            tables = self.engine.describe_all(False)\n            limiter = self.throttle.get_limiter(tables)\n        else:\n            limiter = None\n        self.engine.rate_limit = limiter\n        results = self.engine.execute(command)\n        if results is None:\n            pass\n        elif isinstance(results, basestring):\n            print(results)\n        else:\n            with self.display() as ostream:\n                formatter = FORMATTERS[self.conf[\"format\"]](\n                    results,\n                    ostream,\n                    pagesize=self.conf[\"pagesize\"],\n                    width=self.conf[\"width\"],\n                )\n                formatter.display()\n        print_count = 0\n        total = None\n        for (cmd_fragment, capacity) in self.engine.consumed_capacities:\n            total += capacity\n            print(cmd_fragment)\n            print(indent(str(capacity)))\n            print_count += 1\n        if print_count > 1:\n            print(\"TOTAL\")\n            print(indent(str(total)))", "response": "Run a DQL command"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_command(self, command):\n        self.display = DISPLAYS[\"stdout\"]\n        self.conf[\"pagesize\"] = 0\n        self.onecmd(command)", "response": "Run a command from the command line with - c"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing a parser for a standard function format", "response": "def function(name, *args, **kwargs):\n    \"\"\" Construct a parser for a standard function format \"\"\"\n    if kwargs.get(\"caseless\"):\n        name = upkey(name)\n    else:\n        name = Word(name)\n    fxn_args = None\n    for i, arg in enumerate(args):\n        if i == 0:\n            fxn_args = arg\n        else:\n            fxn_args += Suppress(\",\") + arg\n    if fxn_args is None:\n        return name + Suppress(\"(\") + Suppress(\")\")\n    if kwargs.get(\"optparen\"):\n        return name + ((Suppress(\"(\") + fxn_args + Suppress(\")\")) | fxn_args)\n    else:\n        return name + Suppress(\"(\") + fxn_args + Suppress(\")\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an interval segment", "response": "def make_interval(long_name, short_name):\n    \"\"\" Create an interval segment \"\"\"\n    return Group(\n        Regex(\"(-+)?[0-9]+\")\n        + (\n            upkey(long_name + \"s\")\n            | Regex(long_name + \"s\").setParseAction(upcaseTokens)\n            | upkey(long_name)\n            | Regex(long_name).setParseAction(upcaseTokens)\n            | upkey(short_name)\n            | Regex(short_name).setParseAction(upcaseTokens)\n        )\n    ).setResultsName(long_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_throughput(variable=primitive):\n    return (\n        Suppress(upkey(\"throughput\") | upkey(\"tp\"))\n        + Suppress(\"(\")\n        + variable\n        + Suppress(\",\")\n        + variable\n        + Suppress(\")\")\n    ).setResultsName(\"throughput\")", "response": "Create a throughput specification"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a THROTTLE statement", "response": "def create_throttle():\n    \"\"\" Create a THROTTLE statement \"\"\"\n    throttle_amount = \"*\" | Combine(number + \"%\") | number\n    return Group(\n        function(\"throttle\", throttle_amount, throttle_amount, caseless=True)\n    ).setResultsName(\"throttle\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the grammar for a scan query", "response": "def _query(cmd):\n    \"\"\" Create the grammar for a scan/query \"\"\"\n    action = upkey(cmd).setResultsName(\"action\")\n    consist = upkey(\"consistent\").setResultsName(\"consistent\")\n    order_by = (Suppress(upkey(\"order\") + upkey(\"by\")) + var).setResultsName(\"order_by\")\n    ordering = (upkey(\"desc\") | upkey(\"asc\")).setResultsName(\"order\")\n    save = (Suppress(upkey(\"save\")) + filename).setResultsName(\"save_file\")\n\n    return (\n        action\n        + Optional(consist)\n        + selection\n        + from_\n        + table\n        + Optional(keys_in | where)\n        + Optional(using)\n        + Optional(limit)\n        + Optional(scan_limit)\n        + Optional(order_by)\n        + Optional(ordering)\n        + Optional(throttle)\n        + Optional(save)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _global_index():\n    var_and_type = var + Optional(type_)\n    global_dec = Suppress(upkey(\"global\")) + index\n    range_key_etc = Suppress(\",\") + Group(throughput) | Optional(\n        Group(Suppress(\",\") + var_and_type).setResultsName(\"range_key\")\n    ) + Optional(Suppress(\",\") + include_vars) + Optional(\n        Group(Suppress(\",\") + throughput)\n    )\n    global_spec = (\n        Suppress(\"(\")\n        + primitive\n        + Suppress(\",\")\n        + Group(var_and_type).setResultsName(\"hash_key\")\n        + range_key_etc\n        + Suppress(\")\")\n    )\n    return Group(global_dec + global_spec).setName(\"global index\")", "response": "Create grammar for a global index declaration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_create():\n    create = upkey(\"create\").setResultsName(\"action\")\n    hash_key = Group(upkey(\"hash\") + upkey(\"key\"))\n    range_key = Group(upkey(\"range\") + upkey(\"key\"))\n\n    local_index = Group(\n        index\n        + Suppress(\"(\")\n        + primitive\n        + Optional(Suppress(\",\") + include_vars)\n        + Suppress(\")\")\n    )\n    index_type = (\n        (hash_key | range_key | local_index)\n        .setName(\"index specification\")\n        .setResultsName(\"index\")\n    )\n\n    attr_declaration = (\n        Group(var.setResultsName(\"name\") + type_ + Optional(index_type))\n        .setName(\"attr\")\n        .setResultsName(\"attr\")\n    )\n    attrs_declaration = (\n        Suppress(\"(\")\n        + Group(delimitedList(attr_declaration))\n        .setName(\"attrs\")\n        .setResultsName(\"attrs\")\n        + Optional(Suppress(\",\") + throughput)\n        + Suppress(\")\")\n    )\n\n    global_index = _global_index()\n    global_indexes = Group(OneOrMore(global_index)).setResultsName(\"global_indexes\")\n\n    return (\n        create\n        + table_key\n        + Optional(if_not_exists)\n        + table\n        + attrs_declaration\n        + Optional(global_indexes)\n    )", "response": "Create the grammar for the create statement."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_delete():\n    delete = upkey(\"delete\").setResultsName(\"action\")\n    return (\n        delete\n        + from_\n        + table\n        + Optional(keys_in)\n        + Optional(where)\n        + Optional(using)\n        + Optional(throttle)\n    )", "response": "Create the grammar for the delete statement"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the grammar for the insert statement", "response": "def create_insert():\n    \"\"\" Create the grammar for the 'insert' statement \"\"\"\n    insert = upkey(\"insert\").setResultsName(\"action\")\n\n    # VALUES\n    attrs = Group(delimitedList(var)).setResultsName(\"attrs\")\n    value_group = Group(Suppress(\"(\") + delimitedList(value) + Suppress(\")\"))\n    values = Group(delimitedList(value_group)).setResultsName(\"list_values\")\n    values_insert = Suppress(\"(\") + attrs + Suppress(\")\") + upkey(\"values\") + values\n\n    # KEYWORDS\n    keyword = Group(var + Suppress(\"=\") + value)\n    item = Group(Suppress(\"(\") + delimitedList(keyword) + Suppress(\")\"))\n    keyword_insert = delimitedList(item).setResultsName(\"map_values\")\n\n    return insert + into + table + (values_insert | keyword_insert) + Optional(throttle)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the grammar for an update expression.", "response": "def _create_update_expression():\n    \"\"\" Create the grammar for an update expression \"\"\"\n    ine = (\n        Word(\"if_not_exists\")\n        + Suppress(\"(\")\n        + var\n        + Suppress(\",\")\n        + var_val\n        + Suppress(\")\")\n    )\n    list_append = (\n        Word(\"list_append\")\n        + Suppress(\"(\")\n        + var_val\n        + Suppress(\",\")\n        + var_val\n        + Suppress(\")\")\n    )\n    fxn = Group(ine | list_append).setResultsName(\"set_function\")\n    # value has to come before var to prevent parsing TRUE/FALSE as variables\n    path = value | fxn | var\n    set_val = (path + oneOf(\"+ -\") + path) | path\n    set_cmd = Group(var + Suppress(\"=\") + set_val)\n    set_expr = (Suppress(upkey(\"set\")) + delimitedList(set_cmd)).setResultsName(\n        \"set_expr\"\n    )\n    add_expr = (\n        Suppress(upkey(\"add\")) + delimitedList(Group(var + value))\n    ).setResultsName(\"add_expr\")\n    delete_expr = (\n        Suppress(upkey(\"delete\")) + delimitedList(Group(var + value))\n    ).setResultsName(\"delete_expr\")\n    remove_expr = (Suppress(upkey(\"remove\")) + delimitedList(var)).setResultsName(\n        \"remove_expr\"\n    )\n    return OneOrMore(set_expr | add_expr | delete_expr | remove_expr).setResultsName(\n        \"update\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_update():\n    update = upkey(\"update\").setResultsName(\"action\")\n    returns, none, all_, updated, old, new = map(\n        upkey, [\"returns\", \"none\", \"all\", \"updated\", \"old\", \"new\"]\n    )\n    return_ = returns + Group(\n        none | (all_ + old) | (all_ + new) | (updated + old) | (updated + new)\n    ).setResultsName(\"returns\")\n    return (\n        update\n        + table\n        + update_expr\n        + Optional(keys_in)\n        + Optional(where)\n        + Optional(using)\n        + Optional(return_)\n        + Optional(throttle)\n    )", "response": "Create the grammar for the update statement"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the grammar for the alter statement.", "response": "def create_alter():\n    \"\"\" Create the grammar for the 'alter' statement \"\"\"\n    alter = upkey(\"alter\").setResultsName(\"action\")\n    prim_or_star = primitive | \"*\"\n\n    set_throughput = (\n        Suppress(upkey(\"set\"))\n        + Optional(Suppress(upkey(\"index\")) + var.setResultsName(\"index\"))\n        + create_throughput(prim_or_star)\n    )\n\n    drop_index = (\n        Suppress(upkey(\"drop\") + upkey(\"index\")) + var + Optional(if_exists)\n    ).setResultsName(\"drop_index\")\n    global_index = _global_index()\n    create_index = (\n        Suppress(upkey(\"create\"))\n        + global_index.setResultsName(\"create_index\")\n        + Optional(if_not_exists)\n    )\n\n    return alter + table_key + table + (set_throughput | drop_index | create_index)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the grammar for the dump statement", "response": "def create_dump():\n    \"\"\" Create the grammar for the 'dump' statement \"\"\"\n    dump = upkey(\"dump\").setResultsName(\"action\")\n    return (\n        dump\n        + upkey(\"schema\")\n        + Optional(Group(delimitedList(table)).setResultsName(\"tables\"))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_load():\n    load = upkey(\"load\").setResultsName(\"action\")\n    return (\n        load\n        + Group(filename).setResultsName(\"load_file\")\n        + upkey(\"into\")\n        + table\n        + Optional(throttle)\n    )", "response": "Create the grammar for the load statement"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the language parser", "response": "def create_parser():\n    \"\"\" Create the language parser \"\"\"\n    select = create_select()\n    scan = create_scan()\n    delete = create_delete()\n    update = create_update()\n    insert = create_insert()\n    create = create_create()\n    drop = create_drop()\n    alter = create_alter()\n    dump = create_dump()\n    load = create_load()\n    base = (\n        select | scan | delete | update | insert | create | drop | alter | dump | load\n    )\n    explain = upkey(\"explain\").setResultsName(\"action\") + Group(\n        select | scan | delete | update | insert | create | drop | alter\n    )\n    analyze = upkey(\"analyze\").setResultsName(\"action\") + Group(\n        select | scan | delete | update | insert\n    )\n    dql = explain | analyze | base\n    dql.ignore(\"--\" + restOfLine)\n    return dql"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting the DQL client.", "response": "def main():\n    \"\"\" Start the DQL client. \"\"\"\n    parse = argparse.ArgumentParser(description=main.__doc__)\n    parse.add_argument(\"-c\", \"--command\", help=\"Run this command and exit\")\n    region = os.environ.get(\"AWS_REGION\", \"us-west-1\")\n    parse.add_argument(\n        \"-r\",\n        \"--region\",\n        default=region,\n        help=\"AWS region to connect to (default %(default)s)\",\n    )\n    parse.add_argument(\n        \"-H\",\n        \"--host\",\n        default=None,\n        help=\"Host to connect to if using a local instance \" \"(default %(default)s)\",\n    )\n    parse.add_argument(\n        \"-p\",\n        \"--port\",\n        default=8000,\n        type=int,\n        help=\"Port to connect to \" \"(default %(default)d)\",\n    )\n    parse.add_argument(\n        \"--version\", action=\"store_true\", help=\"Print the version and exit\"\n    )\n    args = parse.parse_args()\n\n    if args.version:\n        print(__version__)\n        return\n\n    logging.config.dictConfig(LOG_CONFIG)\n    cli = DQLClient()\n    cli.initialize(region=args.region, host=args.host, port=args.port)\n\n    if args.command:\n        command = args.command.strip()\n        try:\n            cli.run_command(command)\n            if cli.engine.partial:\n                cli.run_command(\";\")\n        except KeyboardInterrupt:\n            pass\n    else:\n        cli.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates the POST body.", "response": "def _validate_post_body(actual_request_body, body_specification):\n    \"\"\" returns a tuple (boolean, msg)\n        to indicate whether the validation passed\n        if False then msg contains the reason\n        if True then msg is empty\n    \"\"\"\n\n    # If no body specified, return True (POST with empty body is allowed):\n    if \"body\" not in body_specification[\"parameters\"]:\n        return True, \"\"\n\n    # Are there required parameters? - there is only ONE body, so we check that one\n    parameters_required = body_specification['parameters']['body']['required']\n\n    # What if it says 'required' but there is no schema ? - we reject it\n    schema_present = body_specification['parameters']['body'].get('schema')\n    if parameters_required and not schema_present:\n        msg = \"there is no schema given, but it says there are required parameters\"\n        return False, msg\n\n    # What is the mime type ?\n    text_is_accepted = any('text' in item for item in body_specification.get('consumes', []))\n    json_is_accepted = any('json' in item for item in body_specification.get('consumes', []))\n\n    if actual_request_body is '' and not text_is_accepted:\n        msg = \"post body is an empty string, but text is not an accepted mime type\"\n        return False, msg\n\n    if actual_request_body == {} and not json_is_accepted:\n        msg = \"post body is an empty dict, but json is not an accepted mime type\"\n        return False, msg\n\n    # If only json is accepted, but the body is a string, we transform the\n    # string to json and check it then (not sure if the server would accept\n    # that string, though)\n    if (json_is_accepted and not\n            text_is_accepted and\n            type(actual_request_body).__name__ == 'str'):\n        actual_request_body = json.loads(actual_request_body)\n\n    # Handle empty body\n    body_is_empty = actual_request_body in [None, '', {}]\n    if body_is_empty:\n        if parameters_required:\n            msg = \"there is no body, but it says there are required parameters\"\n            return False, msg\n        else:\n            return True, \"\"\n\n    return True, \"\""}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses all definitions in the swagger specification.", "response": "def build_definitions_example(self):\n        \"\"\"Parse all definitions in the swagger specification.\"\"\"\n        for def_name, def_spec in self.specification.get('definitions', {}).items():\n            self.build_one_definition_example(def_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the example for a given definition.", "response": "def build_one_definition_example(self, def_name):\n        \"\"\"Build the example for the given definition.\n\n        Args:\n            def_name: Name of the definition.\n\n        Returns:\n            True if the example has been created, False if an error occured.\n        \"\"\"\n        if def_name in self.definitions_example.keys():  # Already processed\n            return True\n        elif def_name not in self.specification['definitions'].keys():  # Def does not exist\n            return False\n\n        self.definitions_example[def_name] = {}\n        def_spec = self.specification['definitions'][def_name]\n\n        if def_spec.get('type') == 'array' and 'items' in def_spec:\n            item = self.get_example_from_prop_spec(def_spec['items'])\n            self.definitions_example[def_name] = [item]\n            return True\n\n        if 'properties' not in def_spec:\n            self.definitions_example[def_name] = self.get_example_from_prop_spec(def_spec)\n            return True\n\n        # Get properties example value\n        for prop_name, prop_spec in def_spec['properties'].items():\n            example = self.get_example_from_prop_spec(prop_spec)\n            if example is None:\n                return False\n            self.definitions_example[def_name][prop_name] = example\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_type(value, type_def):\n        if type_def == 'integer':\n            try:\n                # We accept string with integer ex: '123'\n                int(value)\n                return True\n            except ValueError:\n                return isinstance(value, six.integer_types) and not isinstance(value, bool)\n        elif type_def == 'number':\n            return isinstance(value, (six.integer_types, float)) and not isinstance(value, bool)\n        elif type_def == 'string':\n            return isinstance(value, (six.text_type, six.string_types, datetime.datetime))\n        elif type_def == 'boolean':\n            return (isinstance(value, bool) or\n                    (isinstance(value, (six.text_type, six.string_types,)) and\n                     value.lower() in ['true', 'false'])\n                    )\n        else:\n            return False", "response": "Check if the value is in the type given in type_def."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_example_from_prop_spec(self, prop_spec, from_allof=False):\n        # Read example directly from (X-)Example or Default value\n        easy_keys = ['example', 'x-example', 'default']\n        for key in easy_keys:\n            if key in prop_spec.keys() and self.use_example:\n                return prop_spec[key]\n        # Enum\n        if 'enum' in prop_spec.keys():\n            return prop_spec['enum'][0]\n        # From definition\n        if '$ref' in prop_spec.keys():\n            return self._example_from_definition(prop_spec)\n        # Process AllOf section\n        if 'allOf' in prop_spec.keys():\n            return self._example_from_allof(prop_spec)\n        # Complex type\n        if 'type' not in prop_spec:\n            return self._example_from_complex_def(prop_spec)\n        # Object - read from properties, without references\n        if prop_spec['type'] == 'object':\n            example, additional_properties = self._get_example_from_properties(prop_spec)\n            if additional_properties or from_allof:\n                return example\n            return [example]\n        # Array\n        if prop_spec['type'] == 'array' or (isinstance(prop_spec['type'], list) and prop_spec['type'][0] == 'array'):\n            return self._example_from_array_spec(prop_spec)\n        # File\n        if prop_spec['type'] == 'file':\n            return (StringIO('my file contents'), 'hello world.txt')\n        # Date time\n        if 'format' in prop_spec.keys() and prop_spec['format'] == 'date-time':\n            return self._get_example_from_basic_type('datetime')[0]\n        # List\n        if isinstance(prop_spec['type'], list):\n            return self._get_example_from_basic_type(prop_spec['type'][0])[0]\n\n        # Default - basic type\n        logging.info(\"falling back to basic type, no other match found\")\n        return self._get_example_from_basic_type(prop_spec['type'])[0]", "response": "Return an example value from a property specification."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_example_from_properties(self, spec):\n        local_spec = deepcopy(spec)\n\n        # Handle additionalProperties if they exist\n        # we replace additionalProperties with two concrete\n        # properties so that examples can be generated\n        additional_property = False\n        if 'additionalProperties' in local_spec:\n            additional_property = True\n            if 'properties' not in local_spec:\n                local_spec['properties'] = {}\n            local_spec['properties'].update({\n                'any_prop1': local_spec['additionalProperties'],\n                'any_prop2': local_spec['additionalProperties'],\n            })\n            del(local_spec['additionalProperties'])\n            required = local_spec.get('required', [])\n            required += ['any_prop1', 'any_prop2']\n            local_spec['required'] = required\n\n        example = {}\n        properties = local_spec.get('properties')\n        if properties is not None:\n            required = local_spec.get('required', properties.keys())\n\n            for inner_name, inner_spec in properties.items():\n                if inner_name not in required:\n                    continue\n                partial = self.get_example_from_prop_spec(inner_spec)\n                # While get_example_from_prop_spec is supposed to return a list,\n                # we don't actually want that when recursing to build from\n                # properties\n                if isinstance(partial, list):\n                    partial = partial[0]\n                example[inner_name] = partial\n\n        return example, additional_property", "response": "Get example from the properties of an object defined inline."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets example from the given type.", "response": "def _get_example_from_basic_type(type):\n        \"\"\"Get example from the given type.\n\n        Args:\n            type: the type you want an example of.\n\n        Returns:\n            An array with two example values of the given type.\n        \"\"\"\n        if type == 'integer':\n            return [42, 24]\n        elif type == 'number':\n            return [5.5, 5.5]\n        elif type == 'string':\n            return ['string', 'string2']\n        elif type == 'datetime':\n            return ['2015-08-28T09:02:57.481Z', '2015-08-28T09:02:57.481Z']\n        elif type == 'boolean':\n            return [False, True]\n        elif type == 'null':\n            return ['null', 'null']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a swagger definition json from a given example", "response": "def _definition_from_example(example):\n        \"\"\"Generates a swagger definition json from a given example\n           Works only for simple types in the dict\n\n        Args:\n            example: The example for which we want a definition\n                     Type is DICT\n\n        Returns:\n            A dict that is the swagger definition json\n        \"\"\"\n        assert isinstance(example, dict)\n\n        def _has_simple_type(value):\n            accepted = (str, int, float, bool)\n            return isinstance(value, accepted)\n\n        definition = {\n            'type': 'object',\n            'properties': {},\n        }\n        for key, value in example.items():\n            if not _has_simple_type(value):\n                raise Exception(\"Not implemented yet\")\n            ret_value = None\n            if isinstance(value, str):\n                ret_value = {'type': 'string'}\n            elif isinstance(value, int):\n                ret_value = {'type': 'integer', 'format': 'int64'}\n            elif isinstance(value, float):\n                ret_value = {'type': 'number', 'format': 'double'}\n            elif isinstance(value, bool):\n                ret_value = {'type': 'boolean'}\n            else:\n                raise Exception(\"Not implemented yet\")\n            definition['properties'][key] = ret_value\n\n        return definition"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the examples from an allOf section.", "response": "def _example_from_allof(self, prop_spec):\n        \"\"\"Get the examples from an allOf section.\n\n        Args:\n            prop_spec: property specification you want an example of.\n\n        Returns:\n            An example dict\n        \"\"\"\n        example_dict = {}\n        for definition in prop_spec['allOf']:\n            update = self.get_example_from_prop_spec(definition, True)\n            example_dict.update(update)\n        return example_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _example_from_definition(self, prop_spec):\n        # Get value from definition\n        definition_name = self.get_definition_name_from_ref(prop_spec['$ref'])\n\n        if self.build_one_definition_example(definition_name):\n            example_dict = self.definitions_example[definition_name]\n            if not isinstance(example_dict, dict):\n                return example_dict\n            example = dict((example_name, example_value) for example_name, example_value in example_dict.items())\n            return example", "response": "Get an example from a property specification linked to a definition."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _example_from_complex_def(self, prop_spec):\n        if 'schema' not in prop_spec:\n            return [{}]\n        elif 'type' not in prop_spec['schema']:\n            definition_name = self.get_definition_name_from_ref(prop_spec['schema']['$ref'])\n            if self.build_one_definition_example(definition_name):\n                return self.definitions_example[definition_name]\n        elif prop_spec['schema']['type'] == 'array':  # Array with definition\n            # Get value from definition\n            if 'items' in prop_spec.keys():\n                definition_name = self.get_definition_name_from_ref(prop_spec['items']['$ref'])\n            else:\n                if '$ref' in prop_spec['schema']['items']:\n                    definition_name = self.get_definition_name_from_ref(prop_spec['schema']['items']['$ref'])\n                else:\n                    definition_name = self.get_definition_name_from_ref(prop_spec['schema']['items']['type'])\n                    return [definition_name]\n            return [self.definitions_example[definition_name]]\n        else:\n            return self.get_example_from_prop_spec(prop_spec['schema'])", "response": "Get an example from a property specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _example_from_array_spec(self, prop_spec):\n        # if items is a list, then each item has its own spec\n        if isinstance(prop_spec['items'], list):\n            return [self.get_example_from_prop_spec(item_prop_spec) for item_prop_spec in prop_spec['items']]\n        # Standard types in array\n        elif 'type' in prop_spec['items'].keys():\n            if 'format' in prop_spec['items'].keys() and prop_spec['items']['format'] == 'date-time':\n                return self._get_example_from_basic_type('datetime')\n            else:\n                return self._get_example_from_basic_type(prop_spec['items']['type'])\n\n        # Array with definition\n        elif ('$ref' in prop_spec['items'].keys() or\n              ('schema' in prop_spec and'$ref' in prop_spec['schema']['items'].keys())):\n            # Get value from definition\n            definition_name = self.get_definition_name_from_ref(prop_spec['items']['$ref']) or \\\n                self.get_definition_name_from_ref(prop_spec['schema']['items']['$ref'])\n            if self.build_one_definition_example(definition_name):\n                example_dict = self.definitions_example[definition_name]\n                if not isinstance(example_dict, dict):\n                    return [example_dict]\n                if len(example_dict) == 1:\n                    try:  # Python 2.7\n                        res = example_dict[example_dict.keys()[0]]\n                    except TypeError:  # Python 3\n                        res = example_dict[list(example_dict)[0]]\n                    return res\n\n                else:\n                    return_value = {}\n                    for example_name, example_value in example_dict.items():\n                        return_value[example_name] = example_value\n                    return [return_value]\n        elif 'properties' in prop_spec['items']:\n            prop_example = {}\n            for prop_name, prop_spec in prop_spec['items']['properties'].items():\n                example = self.get_example_from_prop_spec(prop_spec)\n                if example is not None:\n                    prop_example[prop_name] = example\n            return [prop_example]", "response": "Get an example from a property specification of an array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_dict_definition(self, dict, get_list=False):\n        list_def_candidate = []\n        for definition_name in self.specification['definitions'].keys():\n            if self.validate_definition(definition_name, dict):\n                if not get_list:\n                    return definition_name\n                list_def_candidate.append(definition_name)\n        if get_list:\n            return list_def_candidate\n        return None", "response": "Get the definition name of the given dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the given dict according to the given definition.", "response": "def validate_definition(self, definition_name, dict_to_test, definition=None):\n        \"\"\"Validate the given dict according to the given definition.\n\n        Args:\n            definition_name: name of the the definition.\n            dict_to_test: dict to test.\n\n        Returns:\n            True if the given dict match the definition, False otherwise.\n        \"\"\"\n        if (definition_name not in self.specification['definitions'].keys() and\n                definition is None):\n            # reject unknown definition\n            return False\n\n        # Check all required in dict_to_test\n        spec_def = definition or self.specification['definitions'][definition_name]\n        all_required_keys_present = all(req in dict_to_test.keys() for req in spec_def.get('required', {}))\n        if 'required' in spec_def and not all_required_keys_present:\n            return False\n\n        # Check no extra arg & type\n        properties_dict = spec_def.get('properties', {})\n        for key, value in dict_to_test.items():\n            if value is not None:\n                if key not in properties_dict:  # Extra arg\n                    return False\n                else:  # Check type\n                    if not self._validate_type(properties_dict[key], value):\n                        return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate_type(self, properties_spec, value):\n        if 'type' not in properties_spec.keys():\n            # Validate sub definition\n            def_name = self.get_definition_name_from_ref(properties_spec['$ref'])\n            return self.validate_definition(def_name, value)\n\n        # Validate array\n        elif properties_spec['type'] == 'array':\n            if not isinstance(value, list):\n                return False\n\n            # Check type\n            if ('type' in properties_spec['items'].keys() and\n                    any(not self.check_type(item, properties_spec['items']['type']) for item in value)):\n                return False\n            # Check ref\n            elif ('$ref' in properties_spec['items'].keys()):\n                def_name = self.get_definition_name_from_ref(properties_spec['items']['$ref'])\n                if any(not self.validate_definition(def_name, item) for item in value):\n                    return False\n\n        else:  # Classic types\n            if not self.check_type(value, properties_spec['type']):\n                return False\n\n        return True", "response": "Validate the given value with the given property spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets data for each path in the swagger specification.", "response": "def get_paths_data(self):\n        \"\"\"Get data for each paths in the swagger specification.\n\n        Get also the list of operationId.\n        \"\"\"\n        for path, path_spec in self.specification['paths'].items():\n            path = u'{0}{1}'.format(self.base_path, path)\n            self.paths[path] = {}\n\n            # Add path-level parameters\n            default_parameters = {}\n            if 'parameters' in path_spec:\n                self._add_parameters(default_parameters, path_spec['parameters'])\n\n            for http_method in path_spec.keys():\n                if http_method not in self._HTTP_VERBS:\n                    continue\n\n                self.paths[path][http_method] = {}\n\n                # Add to operation list\n                action = path_spec[http_method]\n                tag = action['tags'][0] if 'tags' in action.keys() and action['tags'] else None\n                if 'operationId' in action.keys():\n                    self.operation[action['operationId']] = (path, http_method, tag)\n                else:\n                    # Note: the encoding chosen below isn't very important in this\n                    #       case; what matters is a byte string that is unique.\n                    #       URL paths and http methods should encode to UTF-8 safely.\n                    h = hashlib.sha256()\n                    h.update((\"{0}|{1}\".format(http_method, path)).encode('utf-8'))\n                    self.generated_operation[h.hexdigest()] = (path, http_method, tag)\n\n                # Get parameters\n                self.paths[path][http_method]['parameters'] = default_parameters.copy()\n                if 'parameters' in action.keys():\n                    self._add_parameters(self.paths[path][http_method]['parameters'], action['parameters'])\n\n                # Get responses\n                self.paths[path][http_method]['responses'] = action['responses']\n\n                # Get mime types for this action\n                if 'consumes' in action.keys():\n                    self.paths[path][http_method]['consumes'] = action['consumes']"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the given parameter map with the list of parameters provided resolving any reference objects encountered.", "response": "def _add_parameters(self, parameter_map, parameter_list):\n        \"\"\"Populates the given parameter map with the list of parameters provided, resolving any reference objects encountered.\n\n        Args:\n            parameter_map: mapping from parameter names to parameter objects\n            parameter_list: list of either parameter objects or reference objects\n        \"\"\"\n        for parameter in parameter_list:\n            if parameter.get('$ref'):\n                # expand parameter from $ref if not specified inline\n                parameter = self.specification['parameters'].get(parameter.get('$ref').split('/')[-1])\n            parameter_map[parameter['name']] = parameter"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the definition name of the given ref value.", "response": "def get_definition_name_from_ref(ref):\n        \"\"\"Get the definition name of the given $ref value(Swagger value).\n\n        Args:\n            ref: ref value (ex: \"#/definitions/CustomDefinition\")\n\n        Returns:\n            The definition name corresponding to the ref.\n        \"\"\"\n        p = re.compile('#\\/definitions\\/(.*)')\n        definition_name = re.sub(p, r'\\1', ref)\n        return definition_name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_path_spec(self, path, action=None):\n        # Get the specification of the given path\n        path_spec = None\n        path_name = None\n        for base_path in self.paths.keys():\n            if path == base_path:\n                path_spec = self.paths[base_path]\n                path_name = base_path\n\n        # Path parameter\n        if path_spec is None:\n            for base_path in self.paths.keys():\n                regex_from_path = re.compile(re.sub('{[^/]*}', '([^/]*)', base_path) + r'$')\n                if re.match(regex_from_path, path):\n                    path_spec = self.paths[base_path]\n                    path_name = base_path\n\n        # Test action if given\n        if path_spec is not None and action is not None:\n            if action not in path_spec.keys():\n                return (None, None)\n            else:\n                path_spec = path_spec[action]\n\n        return (path_name, path_spec)", "response": "Get the path specification matching with the given path and action."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_request(self, path, action, body=None, query=None):\n        path_name, path_spec = self.get_path_spec(path)\n\n        if path_spec is None:  # reject unknown path\n            logging.warn(\"there is no path\")\n            return False\n\n        if action not in path_spec.keys():  # reject unknown http method\n            logging.warn(\"this http method is unknown '{0}'\".format(action))\n            return False\n\n        action_spec = path_spec[action]\n\n        # check general post body guidelines (body + mime type)\n        if action == 'post':\n            is_ok, msg = _validate_post_body(body, action_spec)\n            if not is_ok:\n                logging.warn(\"the general post body did not validate due to '{0}'\".format(msg))\n                return False\n\n        # If the body is empty and it validated so far, we can return here\n        # unless there is something in the query parameters we need to check\n        body_is_empty = body in [None, {}, '']\n        if body_is_empty and query is None:\n            return True\n\n        # Check body parameters\n        is_ok, msg = self._validate_body_parameters(body, action_spec)\n        if not is_ok:\n            logging.warn(\"the parameters in the body did not validate due to '{0}'\".format(msg))\n            return False\n\n        # Check query parameters\n        if query is not None and not self._validate_query_parameters(query, action_spec):\n            return False\n\n        return True", "response": "Check if the given request is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the query parameter for the action specification.", "response": "def _validate_query_parameters(self, query, action_spec):\n        \"\"\"Check the query parameter for the action specification.\n\n        Args:\n            query: query parameter to check.\n            action_spec: specification of the action.\n\n        Returns:\n            True if the query is valid.\n        \"\"\"\n        processed_params = []\n        for param_name, param_value in query.items():\n            if param_name in action_spec['parameters'].keys():\n                processed_params.append(param_name)\n\n                # Check array\n                if action_spec['parameters'][param_name]['type'] == 'array':\n                    if not isinstance(param_value, list):  # Not an array\n                        return False\n                    else:\n                        for i in param_value:  # Check type of all elements in array\n                            if not self.check_type(i, action_spec['parameters'][param_name]['items']['type']):\n                                return False\n\n                elif not self.check_type(param_value, action_spec['parameters'][param_name]['type']):\n                    return False\n\n        # Check required\n        if not all(param in processed_params for param, spec in action_spec['parameters'].items()\n                   if spec['in'] == 'query' and 'required' in spec and spec['required']):\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the body parameter for the action specification.", "response": "def _validate_body_parameters(self, body, action_spec):\n        \"\"\"Check the body parameter for the action specification.\n\n        Args:\n            body: body parameter to check.\n            action_spec: specification of the action.\n\n        Returns:\n            True if the body is valid.\n            A string containing an error msg in case the body did not validate,\n            otherwise the string is empty\n        \"\"\"\n        processed_params = []\n        for param_name, param_spec in action_spec['parameters'].items():\n            if param_spec['in'] == 'body':\n                processed_params.append(param_name)\n\n                # Check type\n                if 'type' in param_spec.keys() and not self.check_type(body, param_spec['type']):\n                    msg = \"Check type did not validate for {0} and {1}\".format(param_spec['type'], body)\n                    return False, msg\n                # Check schema\n                elif 'schema' in param_spec.keys():\n                    if 'type' in param_spec['schema'].keys() and param_spec['schema']['type'] == 'array':\n                        # It is an array get value from definition\n                        definition_name = self.get_definition_name_from_ref(param_spec['schema']['items']['$ref'])\n                        if len(body) > 0 and not self.validate_definition(definition_name, body[0]):\n                            msg = \"The body did not validate against its definition\"\n                            return False, msg\n                    elif ('type' in param_spec['schema'].keys() and not\n                          self.check_type(body, param_spec['schema']['type'])):\n                        # Type but not array\n                        msg = \"Check type did not validate for {0} and {1}\".format(param_spec['schema']['type'], body)\n                        return False, msg\n                    else:\n                        definition_name = self.get_definition_name_from_ref(param_spec['schema']['$ref'])\n                        if not self.validate_definition(definition_name, body):\n                            msg = \"The body did not validate against its definition\"\n                            return False, msg\n        # Check required\n        if not all(param in processed_params for param, spec in action_spec['parameters'].items()\n                   if spec['in'] == 'body' and 'required' in spec and spec['required']):\n            msg = \"Not all required parameters were present\"\n            return False, msg\n\n        return True, \"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a response example from a response spec.", "response": "def get_response_example(self, resp_spec):\n        \"\"\"Get a response example from a response spec.\n\n        \"\"\"\n        if 'schema' in resp_spec.keys():\n            if '$ref' in resp_spec['schema']:  # Standard definition\n                definition_name = self.get_definition_name_from_ref(resp_spec['schema']['$ref'])\n                return self.definitions_example[definition_name]\n            elif 'items' in resp_spec['schema'] and resp_spec['schema']['type'] == 'array':  # Array\n                if '$ref' in resp_spec['schema']['items']:\n                    definition_name = self.get_definition_name_from_ref(resp_spec['schema']['items']['$ref'])\n                else:\n                    if 'type' in resp_spec['schema']['items']:\n                        definition_name = self.get_definition_name_from_ref(resp_spec['schema']['items'])\n                        return [definition_name]\n                    else:\n                        logging.warn(\"No item type in: \" + resp_spec['schema'])\n                        return ''\n                return [self.definitions_example[definition_name]]\n            elif 'type' in resp_spec['schema']:\n                return self.get_example_from_prop_spec(resp_spec['schema'])\n        else:\n            return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the default data and status code of the given path + action request.", "response": "def get_request_data(self, path, action, body=None):\n        \"\"\"Get the default data and status code of the given path + action request.\n\n        Args:\n            path: path of the request.\n            action: action of the request(get, post, delete...)\n            body: body sent, used to sent it back for post request.\n\n        Returns:\n            A tuple with the default response data and status code\n            In case of default status_code, use 0\n        \"\"\"\n        body = body or ''\n        path_name, path_spec = self.get_path_spec(path)\n        response = {}\n\n        # Get all status code\n        if path_spec is not None and action in path_spec.keys():\n            for status_code in path_spec[action]['responses'].keys():\n                resp = path_spec[action]['responses'][status_code]\n                try:\n                    response[int(status_code)] = self.get_response_example(resp)\n                except ValueError:\n                    response[status_code] = self.get_response_example(resp)\n\n        # If there is no status_code add a default 400\n        if response == {}:\n            response[400] = ''\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an example body which is correct to send to the given path with the given action.", "response": "def get_send_request_correct_body(self, path, action):\n        \"\"\"Get an example body which is correct to send to the given path with the given action.\n\n        Args:\n            path: path of the request\n            action: action of the request (get, post, put, delete)\n\n        Returns:\n            A dict representing a correct body for the request or None if no\n            body is required.\n        \"\"\"\n        path_name, path_spec = self.get_path_spec(path)\n\n        if path_spec is not None and action in path_spec.keys():\n                for name, spec in path_spec[action]['parameters'].items():\n                    if spec['in'] == 'body':  # Get body parameter\n                        if 'type' in spec.keys():\n                            # Get value from type\n                            return self.get_example_from_prop_spec(spec)\n                        elif 'schema' in spec.keys():\n                            if 'type' in spec['schema'].keys() and spec['schema']['type'] == 'array':\n                                # It is an array\n                                # Get value from definition\n                                if '$ref' in spec['schema']['items']:\n                                    definition_name = self.get_definition_name_from_ref(spec['schema']\n                                                                                        ['items']['$ref'])\n                                    return [self.definitions_example[definition_name]]\n                                else:\n                                    definition_name = self.get_definition_name_from_ref(spec['schema']\n                                                                                        ['items']['type'])\n                                    return [definition_name]\n                            elif 'type' in spec['schema'].keys():\n                                # Type but not array\n                                return self.get_example_from_prop_spec(spec['schema'])\n                            else:\n                                # Get value from definition\n                                definition_name = self.get_definition_name_from_ref(spec['schema']['$ref'])\n                                return self.definitions_example[definition_name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize a hexadecimal color value to 6 digits lowercase.", "response": "def normalize_hex(hex_value):\n    \"\"\"\n    Normalize a hexadecimal color value to 6 digits, lowercase.\n\n    \"\"\"\n    match = HEX_COLOR_RE.match(hex_value)\n    if match is None:\n        raise ValueError(\n            u\"'{}' is not a valid hexadecimal color value.\".format(hex_value)\n        )\n    hex_digits = match.group(1)\n    if len(hex_digits) == 3:\n        hex_digits = u''.join(2 * s for s in hex_digits)\n    return u'#{}'.format(hex_digits.lower())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _normalize_percent_rgb(value):\n    percent = value.split(u'%')[0]\n    percent = float(percent) if u'.' in percent else int(percent)\n\n    return u'0%' if percent < 0 \\\n        else u'100%' if percent > 100 \\\n        else u'{}%'.format(percent)", "response": "Normalize percent values into a valid range 0 - 100%."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a color name to a normalized hexadecimal color value.", "response": "def name_to_hex(name, spec=u'css3'):\n    \"\"\"\n    Convert a color name to a normalized hexadecimal color value.\n\n    The optional keyword argument ``spec`` determines which\n    specification's list of color names will be used; valid values are\n    ``html4``, ``css2``, ``css21`` and ``css3``, and the default is\n    ``css3``.\n\n    When no color of that name exists in the given specification,\n    ``ValueError`` is raised.\n\n    \"\"\"\n    if spec not in SUPPORTED_SPECIFICATIONS:\n        raise ValueError(SPECIFICATION_ERROR_TEMPLATE.format(spec=spec))\n    normalized = name.lower()\n    hex_value = {u'css2': CSS2_NAMES_TO_HEX,\n                 u'css21': CSS21_NAMES_TO_HEX,\n                 u'css3': CSS3_NAMES_TO_HEX,\n                 u'html4': HTML4_NAMES_TO_HEX}[spec].get(normalized)\n    if hex_value is None:\n        raise ValueError(\n            u\"'{name}' is not defined as a named color in {spec}\".format(\n                name=name, spec=spec\n            )\n        )\n    return hex_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hex_to_name(hex_value, spec=u'css3'):\n    if spec not in SUPPORTED_SPECIFICATIONS:\n        raise ValueError(SPECIFICATION_ERROR_TEMPLATE.format(spec=spec))\n    normalized = normalize_hex(hex_value)\n    name = {u'css2': CSS2_HEX_TO_NAMES,\n            u'css21': CSS21_HEX_TO_NAMES,\n            u'css3': CSS3_HEX_TO_NAMES,\n            u'html4': HTML4_HEX_TO_NAMES}[spec].get(normalized)\n    if name is None:\n        raise ValueError(\n            u\"'{}' has no defined color name in {}\".format(hex_value, spec)\n        )\n    return name", "response": "Convert a hexadecimal color value to its corresponding normalized\n    color name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hex_to_rgb(hex_value):\n    hex_value = normalize_hex(hex_value)\n    hex_value = int(hex_value[1:], 16)\n    return IntegerRGB(\n        hex_value >> 16,\n        hex_value >> 8 & 0xff,\n        hex_value & 0xff\n    )", "response": "Convert a hexadecimal color value to a 3 - tuple suitable for use in an rgb triplet specifying that color."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rgb_to_name(rgb_triplet, spec=u'css3'):\n    return hex_to_name(\n        rgb_to_hex(\n            normalize_integer_triplet(\n                rgb_triplet\n            )\n        ),\n        spec=spec\n    )", "response": "Convert a 3 - tuple of integers suitable for use in an rgb ( rgb_triplet spec is css3 or rgb_triplet to name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a 3 - tuple of integers suitable for use in an rgb ( rgb ) rgb object to a 3 - tuple of percentages suitable for use in an rgb object.", "response": "def rgb_to_rgb_percent(rgb_triplet):\n    \"\"\"\n    Convert a 3-tuple of integers, suitable for use in an ``rgb()``\n    color triplet, to a 3-tuple of percentages suitable for use in\n    representing that color.\n\n    This function makes some trade-offs in terms of the accuracy of\n    the final representation; for some common integer values,\n    special-case logic is used to ensure a precise result (e.g.,\n    integer 128 will always convert to '50%', integer 32 will always\n    convert to '12.5%'), but for all other values a standard Python\n    ``float`` is used and rounded to two decimal places, which may\n    result in a loss of precision for some values.\n\n    \"\"\"\n    # In order to maintain precision for common values,\n    # special-case them.\n    specials = {255: u'100%', 128: u'50%', 64: u'25%',\n                32: u'12.5%', 16: u'6.25%', 0: u'0%'}\n    return PercentRGB._make(\n        specials.get(d, u'{:.02f}%'.format(d / 255.0 * 100))\n        for d in normalize_integer_triplet(rgb_triplet)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a 3 - tuple of percentages suitable for use in an rgb ( rgb_percent_triplet spec is a name of the color triplet that is used to its corresponding normalized color name.", "response": "def rgb_percent_to_name(rgb_percent_triplet, spec=u'css3'):\n    \"\"\"\n    Convert a 3-tuple of percentages, suitable for use in an ``rgb()``\n    color triplet, to its corresponding normalized color name, if any\n    such name exists.\n\n    The optional keyword argument ``spec`` determines which\n    specification's list of color names will be used; valid values are\n    ``html4``, ``css2``, ``css21`` and ``css3``, and the default is\n    ``css3``.\n\n    If there is no matching name, ``ValueError`` is raised.\n\n    \"\"\"\n    return rgb_to_name(\n        rgb_percent_to_rgb(\n            normalize_percent_triplet(\n                rgb_percent_triplet\n            )\n        ),\n        spec=spec\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies the simple color parsing algorithm from section 2. 4. 6 of the HTML5 file.", "response": "def html5_parse_simple_color(input):\n    \"\"\"\n    Apply the simple color parsing algorithm from section 2.4.6 of\n    HTML5.\n\n    \"\"\"\n    # 1. Let input be the string being parsed.\n    #\n    # 2. If input is not exactly seven characters long, then return an\n    #    error.\n    if not isinstance(input, unicode) or len(input) != 7:\n        raise ValueError(\n            u\"An HTML5 simple color must be a Unicode string \"\n            u\"exactly seven characters long.\"\n        )\n\n    # 3. If the first character in input is not a U+0023 NUMBER SIGN\n    #    character (#), then return an error.\n    if not input.startswith('#'):\n        raise ValueError(\n            u\"An HTML5 simple color must begin with the \"\n            u\"character '#' (U+0023).\"\n        )\n\n    # 4. If the last six characters of input are not all ASCII hex\n    #    digits, then return an error.\n    if not all(c in string.hexdigits for c in input[1:]):\n        raise ValueError(\n            u\"An HTML5 simple color must contain exactly six ASCII hex digits.\"\n        )\n\n    # 5. Let result be a simple color.\n    #\n    # 6. Interpret the second and third characters as a hexadecimal\n    #    number and let the result be the red component of result.\n    #\n    # 7. Interpret the fourth and fifth characters as a hexadecimal\n    #    number and let the result be the green component of result.\n    #\n    # 8. Interpret the sixth and seventh characters as a hexadecimal\n    #    number and let the result be the blue component of result.\n    #\n    # 9. Return result.\n    return HTML5SimpleColor(\n        int(input[1:3], 16),\n        int(input[3:5], 16),\n        int(input[5:7], 16)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef html5_serialize_simple_color(simple_color):\n    red, green, blue = simple_color\n\n    # 1. Let result be a string consisting of a single \"#\" (U+0023)\n    #    character.\n    result = u'#'\n\n    # 2. Convert the red, green, and blue components in turn to\n    #    two-digit hexadecimal numbers using lowercase ASCII hex\n    #    digits, zero-padding if necessary, and append these numbers\n    #    to result, in the order red, green, blue.\n    format_string = '{:02x}'\n    result += format_string.format(red)\n    result += format_string.format(green)\n    result += format_string.format(blue)\n\n    # 3. Return result, which will be a valid lowercase simple color.\n    return result", "response": "This function is used to serialize a simple color from section 2. 4. 6 of HTML5."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef html5_parse_legacy_color(input):\n    # 1. Let input be the string being parsed.\n    if not isinstance(input, unicode):\n        raise ValueError(\n            u\"HTML5 legacy color parsing requires a Unicode string as input.\"\n        )\n\n    # 2. If input is the empty string, then return an error.\n    if input == \"\":\n        raise ValueError(\n            u\"HTML5 legacy color parsing forbids empty string as a value.\"\n        )\n\n    # 3. Strip leading and trailing whitespace from input.\n    input = input.strip()\n\n    # 4. If input is an ASCII case-insensitive match for the string\n    #    \"transparent\", then return an error.\n    if input.lower() == u\"transparent\":\n        raise ValueError(\n            u'HTML5 legacy color parsing forbids \"transparent\" as a value.'\n        )\n\n    # 5. If input is an ASCII case-insensitive match for one of the\n    #    keywords listed in the SVG color keywords section of the CSS3\n    #    Color specification, then return the simple color\n    #    corresponding to that keyword.\n    keyword_hex = CSS3_NAMES_TO_HEX.get(input.lower())\n    if keyword_hex is not None:\n        return html5_parse_simple_color(keyword_hex)\n\n    # 6. If input is four characters long, and the first character in\n    #    input is a \"#\" (U+0023) character, and the last three\n    #    characters of input are all ASCII hex digits, then run these\n    #    substeps:\n    if len(input) == 4 and \\\n       input.startswith(u'#') and \\\n       all(c in string.hexdigits for c in input[1:]):\n        # 1. Let result be a simple color.\n        #\n        # 2. Interpret the second character of input as a hexadecimal\n        #    digit; let the red component of result be the resulting\n        #    number multiplied by 17.\n        #\n        # 3. Interpret the third character of input as a hexadecimal\n        #    digit; let the green component of result be the resulting\n        #    number multiplied by 17.\n        #\n        # 4. Interpret the fourth character of input as a hexadecimal\n        #    digit; let the blue component of result be the resulting\n        #    number multiplied by 17.\n        result = HTML5SimpleColor(\n            int(input[1], 16) * 17,\n            int(input[2], 16) * 17,\n            int(input[3], 16) * 17\n        )\n\n        # 5. Return result.\n        return result\n\n    # 7. Replace any characters in input that have a Unicode code\n    #    point greater than U+FFFF (i.e. any characters that are not\n    #    in the basic multilingual plane) with the two-character\n    #    string \"00\".\n\n    # This one's a bit weird due to the existence of multiple internal\n    # Unicode string representations in different versions and builds\n    # of Python.\n    #\n    # From Python 2.2 through 3.2, Python could be compiled with\n    # \"narrow\" or \"wide\" Unicode strings (see PEP 261). Narrow builds\n    # handled Unicode strings with two-byte characters and surrogate\n    # pairs for non-BMP code points. Wide builds handled Unicode\n    # strings with four-byte characters and no surrogates. This means\n    # ord() is only sufficient to identify a non-BMP character on a\n    # wide build.\n    #\n    # Starting with Python 3.3, the internal string representation\n    # (see PEP 393) is now dynamic, and Python chooses an encoding --\n    # either latin-1, UCS-2 or UCS-4 -- wide enough to handle the\n    # highest code point in the string.\n    #\n    # The code below bypasses all of that for a consistently effective\n    # method: encode the string to little-endian UTF-32, then perform\n    # a binary unpack of it as four-byte integers. Those integers will\n    # be the Unicode code points, and from there filtering out non-BMP\n    # code points is easy.\n    encoded_input = input.encode('utf_32_le')\n\n    # Format string is '<' (for little-endian byte order), then a\n    # sequence of 'L' characters (for 4-byte unsigned long integer)\n    # equal to the length of the original string, which is also\n    # one-fourth the encoded length.  For example, for a six-character\n    # input the generated format string will be '<LLLLLL'.\n    format_string = '<' + ('L' * (int(len(encoded_input) / 4)))\n    codepoints = struct.unpack(format_string, encoded_input)\n    input = ''.join(u'00' if c > 0xffff\n                    else unichr(c)\n                    for c in codepoints)\n\n    # 8. If input is longer than 128 characters, truncate input,\n    #    leaving only the first 128 characters.\n    if len(input) > 128:\n        input = input[:128]\n\n    # 9. If the first character in input is a \"#\" (U+0023) character,\n    #    remove it.\n    if input.startswith(u'#'):\n        input = input[1:]\n\n    # 10. Replace any character in input that is not an ASCII hex\n    #     digit with the character \"0\" (U+0030).\n    if any(c for c in input if c not in string.hexdigits):\n        input = ''.join(c if c in string.hexdigits else u'0' for c in input)\n\n    # 11. While input's length is zero or not a multiple of three,\n    #     append a \"0\" (U+0030) character to input.\n    while (len(input) == 0) or (len(input) % 3 != 0):\n        input += u'0'\n\n    # 12. Split input into three strings of equal length, to obtain\n    #     three components. Let length be the length of those\n    #     components (one third the length of input).\n    length = int(len(input) / 3)\n    red = input[:length]\n    green = input[length:length*2]\n    blue = input[length*2:]\n\n    # 13. If length is greater than 8, then remove the leading\n    #     length-8 characters in each component, and let length be 8.\n    if length > 8:\n        red, green, blue = (red[length-8:],\n                            green[length-8:],\n                            blue[length-8:])\n        length = 8\n\n    # 14. While length is greater than two and the first character in\n    #     each component is a \"0\" (U+0030) character, remove that\n    #     character and reduce length by one.\n    while (length > 2) and (red[0] == u'0' and\n                            green[0] == u'0' and\n                            blue[0] == u'0'):\n        red, green, blue = (red[1:],\n                            green[1:],\n                            blue[1:])\n        length -= 1\n\n    # 15. If length is still greater than two, truncate each\n    #     component, leaving only the first two characters in each.\n    if length > 2:\n        red, green, blue = (red[:2],\n                            green[:2],\n                            blue[:2])\n\n    # 16. Let result be a simple color.\n    #\n    # 17. Interpret the first component as a hexadecimal number; let\n    #     the red component of result be the resulting number.\n    #\n    # 18. Interpret the second component as a hexadecimal number; let\n    #     the green component of result be the resulting number.\n    #\n    # 19. Interpret the third component as a hexadecimal number; let\n    #     the blue component of result be the resulting number.\n    #\n    # 20. Return result.\n    return HTML5SimpleColor(\n        int(red, 16),\n        int(green, 16),\n        int(blue, 16)\n    )", "response": "Apply the legacy color parsing algorithm from section 2. 4. 6 of\nEffectiveHTML5."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(self, **kwargs):\n        if '$schema' in self and self['$schema'] is not None:\n            kwargs['cls'] = kwargs.pop('validator', None)\n            _records_state.validate(self, self['$schema'], **kwargs)", "response": "r Validate a single record according to schema defined in jsonschema."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(cls, data, id_=None, **kwargs):\n        from .models import RecordMetadata\n        with db.session.begin_nested():\n            record = cls(data)\n\n            before_record_insert.send(\n                current_app._get_current_object(),\n                record=record\n            )\n\n            record.validate(**kwargs)\n\n            record.model = RecordMetadata(id=id_, json=record)\n\n            db.session.add(record.model)\n\n        after_record_insert.send(\n            current_app._get_current_object(),\n            record=record\n        )\n        return record", "response": "r Create a new record instance and store it in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_record(cls, id_, with_deleted=False):\n        with db.session.no_autoflush:\n            query = RecordMetadata.query.filter_by(id=id_)\n            if not with_deleted:\n                query = query.filter(RecordMetadata.json != None)  # noqa\n            obj = query.one()\n            return cls(obj.json, model=obj)", "response": "Retrieve the record by id. Raise a database exception if the record does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_records(cls, ids, with_deleted=False):\n        with db.session.no_autoflush:\n            query = RecordMetadata.query.filter(RecordMetadata.id.in_(ids))\n            if not with_deleted:\n                query = query.filter(RecordMetadata.json != None)  # noqa\n\n            return [cls(obj.json, model=obj) for obj in query.all()]", "response": "Retrieve multiple records by id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef patch(self, patch):\n        data = apply_patch(dict(self), patch)\n        return self.__class__(data, model=self.model)", "response": "Patch the record metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncommit the current record instance in the database.", "response": "def commit(self, **kwargs):\n        r\"\"\"Store changes of the current record instance in the database.\n\n        #. Send a signal :data:`invenio_records.signals.before_record_update`\n           with the current record to be committed as parameter.\n\n        #. Validate the current record data.\n\n        #. Commit the current record in the database.\n\n        #. Send a signal :data:`invenio_records.signals.after_record_update`\n            with the committed record as parameter.\n\n        :Keyword Arguments:\n          * **format_checker** --\n            An instance of the class :class:`jsonschema.FormatChecker`, which\n            contains validation rules for formats. See\n            :func:`~invenio_records.api.RecordBase.validate` for more details.\n\n          * **validator** --\n            A :class:`jsonschema.IValidator` class that will be used to\n            validate the record. See\n            :func:`~invenio_records.api.RecordBase.validate` for more details.\n\n        :returns: The :class:`Record` instance.\n        \"\"\"\n        if self.model is None or self.model.json is None:\n            raise MissingModelError()\n\n        with db.session.begin_nested():\n            before_record_update.send(\n                current_app._get_current_object(),\n                record=self\n            )\n\n            self.validate(**kwargs)\n\n            self.model.json = dict(self)\n            flag_modified(self.model, 'json')\n\n            db.session.merge(self.model)\n\n        after_record_update.send(\n            current_app._get_current_object(),\n            record=self\n        )\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, force=False):\n        if self.model is None:\n            raise MissingModelError()\n\n        with db.session.begin_nested():\n            before_record_delete.send(\n                current_app._get_current_object(),\n                record=self\n            )\n\n            if force:\n                db.session.delete(self.model)\n            else:\n                self.model.json = None\n                db.session.merge(self.model)\n\n        after_record_delete.send(\n            current_app._get_current_object(),\n            record=self\n        )\n        return self", "response": "Delete a record.\n\n        If `force` is ``False``, the record is soft-deleted: record data will\n        be deleted but the record identifier and the history of the record will\n        be kept. This ensures that the same record identifier cannot be used\n        twice, and that you can still retrieve its history. If `force` is\n        ``True``, then the record is completely deleted from the database.\n\n        #. Send a signal :data:`invenio_records.signals.before_record_delete`\n           with the current record as parameter.\n\n        #. Delete or soft-delete the current record.\n\n        #. Send a signal :data:`invenio_records.signals.after_record_delete`\n           with the current deleted record as parameter.\n\n        :param force: if ``True``, completely deletes the current record from\n               the database, otherwise soft-deletes it.\n        :returns: The deleted :class:`Record` instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreverting the record to a specific revision.", "response": "def revert(self, revision_id):\n        \"\"\"Revert the record to a specific revision.\n\n        #. Send a signal :data:`invenio_records.signals.before_record_revert`\n           with the current record as parameter.\n\n        #. Revert the record to the revision id passed as parameter.\n\n        #. Send a signal :data:`invenio_records.signals.after_record_revert`\n           with the reverted record as parameter.\n\n        :param revision_id: Specify the record revision id\n        :returns: The :class:`Record` instance corresponding to the revision id\n        \"\"\"\n        if self.model is None:\n            raise MissingModelError()\n\n        revision = self.revisions[revision_id]\n\n        with db.session.begin_nested():\n            before_record_revert.send(\n                current_app._get_current_object(),\n                record=self\n            )\n\n            self.model.json = dict(revision)\n\n            db.session.merge(self.model)\n\n        after_record_revert.send(\n            current_app._get_current_object(),\n            record=self\n        )\n        return self.__class__(self.model.json, model=self.model)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(source, ids, force, pid_minter=None):\n    records_deprecation_warning()\n\n    # Make sure that all imports are done with application context.\n    from .api import Record\n    from .models import RecordMetadata\n\n    pid_minter = [process_minter(minter) for minter in pid_minter or []]\n\n    data = json.load(source)\n\n    if isinstance(data, dict):\n        data = [data]\n\n    if ids:\n        assert len(ids) == len(data), 'Not enough identifiers.'\n\n    for record, id_ in zip_longest(data, ids):\n        id_ = id_ or uuid.uuid4()\n        try:\n            for minter in pid_minter:\n                minter(id_, record)\n\n            click.echo(Record.create(record, id_=id_).id)\n        except exc.IntegrityError:\n            if force:\n                current_app.logger.warning(\n                    \"Trying to force insert: {0}\".format(id_))\n                # IMPORTANT: We need to create new transaction for\n                # SQLAlchemy-Continuum as we are using no auto-flush\n                # in Record.get_record.\n                vm = current_app.extensions['invenio-db'].versioning_manager\n                uow = vm.unit_of_work(db.session)\n                uow.create_transaction(db.session)\n                # Use low-level database model to retrieve an instance.\n                model = RecordMetadata.query.get(id_)\n                rec = Record(record, model=model).commit()\n                current_app.logger.info(\"Created new revision {0}\".format(\n                    rec.revision_id))\n                click.echo(rec.id)\n            else:\n                raise click.BadParameter(\n                    'Record with id={0} already exists. If you want to '\n                    'override its data use --force.'.format(id_),\n                    param_hint='ids',\n                )\n        db.session.flush()\n    db.session.commit()", "response": "Create new bibliographic record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch(patch, ids):\n    records_deprecation_warning()\n\n    from .api import Record\n\n    patch_content = patch.read()\n\n    if ids:\n        for id_ in ids:\n            rec = Record.get_record(id_).patch(patch_content).commit()\n            current_app.logger.info(\"Created new revision {0}\".format(\n                rec.revision_id))\n            click.echo(rec.id)\n        db.session.commit()", "response": "Patch existing bibliographic record."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate data using schema with JSONResolver.", "response": "def validate(self, data, schema, **kwargs):\n        \"\"\"Validate data using schema with ``JSONResolver``.\"\"\"\n        if not isinstance(schema, dict):\n            schema = {'$ref': schema}\n        return validate(\n            data,\n            schema,\n            resolver=self.ref_resolver_cls.from_schema(schema),\n            types=self.app.config.get('RECORDS_VALIDATION_TYPES', {}),\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the current object to a Filelink object.", "response": "def to_filelink(self):\n        \"\"\"\n        Checks is the status of the conversion is complete and, if so, converts to a Filelink\n\n        *returns* [Filestack.Filelink]\n\n        ```python\n        filelink = av_convert.to_filelink()\n        ```\n        \"\"\"\n        if self.status != 'completed':\n            return 'Audio/video conversion not complete!'\n\n        response = utils.make_call(self.url, 'get')\n\n        if response.ok:\n            response = response.json()\n            handle = re.match(\n                r'(?:https:\\/\\/cdn\\.filestackcontent\\.com\\/)(\\w+)',\n                response['data']['url']\n            ).group(1)\n            return filestack.models.Filelink(handle, apikey=self.apikey, security=self.security)\n\n        raise Exception(response.text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _return_tag_task(self, task):\n        if self.security is None:\n            raise Exception('Tags require security')\n        tasks = [task]\n        transform_url = get_transform_url(\n            tasks, handle=self.handle, security=self.security,\n            apikey=self.apikey\n        )\n        response = make_call(\n            CDN_URL, 'get', handle=self.handle, security=self.security,\n            transform_url=transform_url\n        )\n        return response.json()", "response": "Returns the tag task from the task list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the URL for the file that can be used to retrieve delete and overwrite the file.", "response": "def url(self):\n        \"\"\"\n        Returns the URL for the instance, which can be used\n        to retrieve, delete, and overwrite the file. If security is enabled, signature and policy parameters will\n        be included,\n\n        *returns* [String]\n\n        ```python\n        filelink = client.upload(filepath='/path/to/file')\n        filelink.url\n        # https://cdn.filestackcontent.com/FILE_HANDLE\n        ```\n        \"\"\"\n        return get_url(CDN_URL, handle=self.handle, security=self.security)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zip(self, store=False, store_params=None):\n        params = locals()\n        params.pop('store')\n        params.pop('store_params')\n\n        new_transform = self.add_transform_task('zip', params)\n\n        if store:\n            return new_transform.store(**store_params) if store_params else new_transform.store()\n\n        return utils.make_call(CDN_URL, 'get', transform_url=new_transform.url)", "response": "Returns a zip file of the current transformation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef av_convert(self, preset=None, force=None, title=None, extname=None, filename=None,\n                   width=None, height=None, upscale=None, aspect_mode=None, two_pass=None,\n                   video_bitrate=None, fps=None, keyframe_interval=None, location=None,\n                   watermark_url=None, watermark_top=None, watermark_bottom=None,\n                   watermark_right=None, watermark_left=None, watermark_width=None, watermark_height=None,\n                   path=None, access=None, container=None, audio_bitrate=None, audio_sample_rate=None,\n                   audio_channels=None, clip_length=None, clip_offset=None):\n\n        \"\"\"\n        ```python\n        from filestack import Client\n\n        client = Client(\"<API_KEY>\")\n        filelink = client.upload(filepath='path/to/file/doom.mp4')\n        av_convert= filelink.av_convert(width=100, height=100)\n        while av_convert.status != 'completed':\n            print(av_convert.status)\n\n        filelink = av_convert.to_filelink()\n        print(filelink.url)\n        ```\n        \"\"\"\n\n        new_transform = self.add_transform_task('video_convert', locals())\n        transform_url = utils.get_transform_url(\n            new_transform._transformation_tasks, external_url=new_transform.external_url,\n            handle=new_transform.handle, security=new_transform.security,\n            apikey=new_transform.apikey, video=True\n        )\n\n        response = utils.make_call(transform_url, 'get')\n\n        if not response.ok:\n            raise Exception(response.text)\n\n        uuid = response.json()['uuid']\n        timestamp = response.json()['timestamp']\n        \n        return filestack.models.AudioVisual(\n            transform_url, uuid, timestamp, apikey=new_transform.apikey, security=new_transform.security\n        )", "response": "This method creates a new transform and returns a new audio object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a transform task to the current instance and returns it", "response": "def add_transform_task(self, transformation, params):\n        \"\"\"\n        Adds a transform task to the current instance and returns it\n\n        *returns* Filestack.Transform\n        \"\"\"\n        if not isinstance(self, filestack.models.Transform):\n            instance = filestack.models.Transform(apikey=self.apikey, security=self.security, handle=self.handle)\n        else:\n            instance = self\n\n        params.pop('self')\n        params = {k: v for k, v in params.items() if v is not None}\n\n        transformation_url = utils.return_transform_task(transformation, params)\n        instance._transformation_tasks.append(transformation_url)\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download(self, destination_path, params=None):\n        if params:\n            CONTENT_DOWNLOAD_SCHEMA.check(params)\n        with open(destination_path, 'wb') as new_file:\n            response = utils.make_call(CDN_URL, 'get',\n                                       handle=self.handle,\n                                       params=params,\n                                       security=self.security,\n                                       transform_url=(self.url if isinstance(self, filestack.models.Transform) else None))\n\n            if response.ok:\n                for chunk in response.iter_content(1024):\n                    if not chunk:\n                        break\n                    new_file.write(chunk)\n\n            return response", "response": "Downloads a file to the given local path and returns the size of the downloaded file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_content(self, params=None):\n        if params:\n            CONTENT_DOWNLOAD_SCHEMA.check(params)\n        response = utils.make_call(CDN_URL, 'get',\n                                   handle=self.handle,\n                                   params=params,\n                                   security=self.security,\n                                   transform_url=(self.url if isinstance(self, filestack.models.Transform) else None))\n\n        return response.content", "response": "Returns the raw byte content of a given Filelink"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_metadata(self, params=None):\n        metadata_url = \"{CDN_URL}/{handle}/metadata\".format(\n            CDN_URL=CDN_URL, handle=self.handle\n        )\n        response = utils.make_call(metadata_url, 'get',\n                                   params=params,\n                                   security=self.security)\n        return response.json()", "response": "Get the metadata for a filehandle."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef overwrite(self, url=None, filepath=None, params=None):\n        if params:\n            OVERWRITE_SCHEMA.check(params)\n        data, files = None, None\n        if url:\n            data = {'url': url}\n        elif filepath:\n            filename = os.path.basename(filepath)\n            mimetype = mimetypes.guess_type(filepath)[0]\n            files = {'fileUpload': (filename, open(filepath, 'rb'), mimetype)}\n        else:\n            raise ValueError(\"You must include a url or filepath parameter\")\n\n        return utils.make_call(API_URL, 'post',\n                               path=FILE_PATH,\n                               params=params,\n                               handle=self.handle,\n                               data=data,\n                               files=files,\n                               security=self.security,\n                               transform_url=self.url if isinstance(self, filestack.models.Transform) else None)", "response": "Overwrite a Filelink by supplying a new file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates a policy and its parameters and raises an error if invalid", "response": "def validate(policy):\n    \"\"\"\n    Validates a policy and its parameters and raises an error if invalid\n    \"\"\"\n    for param, value in policy.items():\n\n        if param not in ACCEPTED_SECURITY_TYPES.keys():\n            raise SecurityError('Invalid Security Parameter: {}'.format(param))\n\n        if type(value) != ACCEPTED_SECURITY_TYPES[param]:\n            raise SecurityError('Invalid Parameter Data Type for {}, '\n                                'Expecting: {} Received: {}'.format(\n                                    param, ACCEPTED_SECURITY_TYPES[param],\n                                    type(value)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef security(policy, app_secret):\n    validate(policy)\n    policy_enc = base64.urlsafe_b64encode(json.dumps(policy).encode('utf-8'))\n\n    signature = hmac.new(app_secret.encode('utf-8'),\n                         policy_enc,\n                         hashlib.sha256).hexdigest()\n\n    return {'policy': policy_enc, 'signature': signature}", "response": "Returns a valid signature and policy based on provided app secret and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform_external(self, external_url):\n        return filestack.models.Transform(apikey=self.apikey, security=self.security, external_url=external_url)", "response": "Turn an external URL into a Filestack Transform object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef urlscreenshot(self, external_url, agent=None, mode=None, width=None, height=None, delay=None):\n        params = locals()\n        params.pop('self')\n        params.pop('external_url')\n\n        params = {k: v for k, v in params.items() if v is not None}\n\n        url_task = utils.return_transform_task('urlscreenshot', params)\n\n        new_transform = filestack.models.Transform(apikey=self.apikey, security=self.security, external_url=external_url)\n        new_transform._transformation_tasks.append(url_task)\n\n        return new_transform", "response": "Takes a screenshot of the given URL and returns a Filestack. Transform object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zip(self, destination_path, files):\n        zip_url = \"{}/{}/zip/[{}]\".format(CDN_URL, self.apikey, ','.join(files))\n        with open(destination_path, 'wb') as new_file:\n            response = utils.make_call(zip_url, 'get')\n            if response.ok:\n                for chunk in response.iter_content(1024):\n                    if not chunk:\n                        break\n                    new_file.write(chunk)\n\n                return response\n\n            return response.text", "response": "Downloads a compressed ZIP archive of the specified files and writes it to the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upload(self, url=None, filepath=None, multipart=True, params=None, upload_processes=None, intelligent=False):\n\n        if params:  # Check the structure of parameters\n            STORE_SCHEMA.check(params)\n\n        if filepath and url:  # Raise an error for using both filepath and external url\n            raise ValueError(\"Cannot upload file and external url at the same time\")\n\n        if filepath:  # Uploading from local drive\n            if intelligent:\n                response = intelligent_ingestion.upload(\n                    self.apikey, filepath, self.storage, params=params, security=self.security\n                )\n\n            elif multipart:\n                response = upload_utils.multipart_upload(\n                    self.apikey, filepath, self.storage,\n                    upload_processes=upload_processes, params=params, security=self.security\n                )\n                handle = response['handle']\n                return filestack.models.Filelink(handle, apikey=self.apikey, security=self.security)\n\n            else:  # Uploading with multipart=False\n                filename = os.path.basename(filepath)\n                mimetype = mimetypes.guess_type(filepath)[0]\n                files = {'fileUpload': (filename, open(filepath, 'rb'), mimetype)}\n\n                if params:\n                    params['key'] = self.apikey\n                else:\n                    params = {'key': self.apikey}\n\n                path = '{path}/{storage}'.format(path=STORE_PATH, storage=self.storage)\n\n                if self.security:\n                    path = \"{path}?policy={policy}&signature={signature}\".format(\n                        path=path, policy=self.security['policy'].decode('utf-8'),\n                        signature=self.security['signature']\n                    )\n\n                response = utils.make_call(\n                    API_URL, 'post', path=path, params=params, files=files\n                )\n\n        else:  # Uploading from an external URL\n            tasks = []\n            request_url_list = []\n\n            if utils.store_params_checker(params):\n                store_task = utils.store_params_maker(params)\n                tasks.append(store_task)\n\n            if self.security:\n                tasks.append(\n                    'security=p:{policy},s:{signature}'.format(\n                        policy=self.security['policy'].decode('utf-8'),\n                        signature=self.security['signature']\n                    )\n                )\n\n            tasks = '/'.join(tasks)\n\n            if tasks:\n                request_url_list.extend((CDN_URL, self.apikey, tasks, url))\n            else:\n                request_url_list.extend((CDN_URL, self.apikey, url))\n\n            request_url = '/'.join(request_url_list)\n\n            response = requests.post(request_url, headers=HEADERS)\n\n        if response.ok:\n            response = response.json()\n            handle = re.match(\n                r'(?:https:\\/\\/cdn\\.filestackcontent\\.com\\/)(\\w+)',\n                response['url']\n            ).group(1)\n            return filestack.models.Filelink(handle, apikey=self.apikey, security=self.security)\n        else:\n            raise Exception('Invalid API response')", "response": "Uploads a file to the local drive."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef url(self):\n        return utils.get_transform_url(\n            self._transformation_tasks, external_url=self.external_url,\n            handle=self.handle, security=self.security, apikey=self.apikey\n        )", "response": "Returns the URL for the current transformation which can be used\n        to retrieve the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store(self, filename=None, location=None, path=None, container=None, region=None, access=None, base64decode=None):\n        if path:\n            path = '\"{}\"'.format(path)\n\n        filelink_obj = self.add_transform_task('store', locals())\n        response = utils.make_call(filelink_obj.url, 'get')\n\n        if response.ok:\n            data = json.loads(response.text)\n            handle = re.match(r'(?:https:\\/\\/cdn\\.filestackcontent\\.com\\/)(\\w+)', data['url']).group(1)\n            return filestack.models.Filelink(handle, apikey=self.apikey, security=self.security)\n        else:\n            raise Exception(response.text)", "response": "Uploads and stores the current transformation as a Fileink\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef debug(self):\n        debug_instance = self.add_transform_task('debug', locals())\n        response = utils.make_call(debug_instance.url, 'get')\n        return response.json()", "response": "Returns a JSON object with inforamtion regarding the current transformation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef real_python3(python, version_dict):\n    args = [python, '-c', 'import sys; print(sys.real_prefix)']\n\n    # get python prefix\n    try:\n        output = subprocess.check_output(args, stderr=subprocess.STDOUT)\n        prefix = output.decode('UTF-8').strip()\n    except subprocess.CalledProcessError:\n        # process fails, implies *not* in active virtualenv\n        return python\n\n    # determine absolute binary path\n    if os.name == 'nt':  # pragma: no cover\n        paths = [os.path.join(prefix, os.path.basename(python))]\n    else:\n        paths = [os.path.join(prefix, 'bin', python) for python in [\n            os.path.basename(python),\n            'python%(major)d.%(minor)d.%(micro)d' % version_dict,\n            'python%(major)d.%(minor)d' % version_dict,\n            'python%(major)d' % version_dict,\n            'python',\n        ]]\n\n    for path in paths:\n        if os.path.isfile(path):\n            break\n    else:\n        path = None\n\n    # the executable path must exist\n    assert path, '\\n- '.join(['Could not find interpreter. Attempted:'] + paths)\n    v1 = subprocess.check_output([python, '--version'])\n    v2 = subprocess.check_output([path, '--version'])\n    assert v1 == v2, 'Expected versions to match (%s != %s).' % (v1, v2)\n\n    return path", "response": "Returns the path of the real python executable that is used for the virtual python executable."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the string to get the appropriate parser function", "response": "def _str_parser(string):\n        \"\"\"\n        return method by the length of string\n        :param string: string\n        :return: method\n        \"\"\"\n        if not any(c.isalpha() for c in string):\n            _string = string[:19]\n            _length = len(_string)\n            if _length > 10:\n                return BaseParser.parse_datetime\n            elif 6 <= _length <= 10:\n                if ':' in _string:\n                    return BaseParser.parse_time\n                else:\n                    return BaseParser.parse_date\n            elif _length < 6:\n                return BaseParser.parse_time\n            else:\n                return BaseParser.parse_special\n        else:\n            return BaseParser.__parse_not_only_str"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a date string to a date object", "response": "def parse_date(string, formation=None):\n        \"\"\"\n        string to date stamp\n        :param string: date string\n        :param formation: format string\n        :return: datetime.date\n        \"\"\"\n        if formation:\n            _stamp = datetime.datetime.strptime(string, formation).date()\n            return _stamp\n\n        _string = string.replace('.', '-').replace('/', '-')\n        if '-' in _string:\n            if len(_string.split('-')[0]) > 3 or len(_string.split('-')[2]) > 3:\n                try:\n                    _stamp = datetime.datetime.strptime(_string, '%Y-%m-%d').date()\n                except ValueError:\n                    try:\n                        _stamp = datetime.datetime.strptime(_string, '%m-%d-%Y').date()\n                    except ValueError:\n                        _stamp = datetime.datetime.strptime(_string, '%d-%m-%Y').date()\n            else:\n                try:\n                    _stamp = datetime.datetime.strptime(_string, '%y-%m-%d').date()\n                except ValueError:\n                    try:\n                        _stamp = datetime.datetime.strptime(_string, '%m-%d-%y').date()\n                    except ValueError:\n                        _stamp = datetime.datetime.strptime(_string, '%d-%m-%y').date()\n        else:\n            if len(_string) > 6:\n                try:\n                    _stamp = datetime.datetime.strptime(_string, '%Y%m%d').date()\n                except ValueError:\n                    _stamp = datetime.datetime.strptime(_string, '%m%d%Y').date()\n            elif len(_string) <= 6:\n                try:\n                    _stamp = datetime.datetime.strptime(_string, '%y%m%d').date()\n                except ValueError:\n                    _stamp = datetime.datetime.strptime(_string, '%m%d%y').date()\n            else:\n                raise CanNotFormatError\n        return _stamp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_diff(base_str):\n        temp_dict = {'years': 0,\n                     'months': 0,\n                     'weeks': 0,\n                     'days': 0,\n                     'hours': 0,\n                     'minutes': 0,\n                     'seconds': 0}\n\n        _pure_str = re.findall(\"[a-zA-Z]+\", base_str)\n        pure_num = [int(_) for _ in re.findall(r'\\d+', base_str)]\n        pure_str = [filter_unit(_) for _ in _pure_str]\n        result_dict = dict(chain(temp_dict.items(), dict(zip(pure_str, pure_num)).items()))\n        if result_dict['months'] >= 12:\n            advance = result_dict['months'] // 12\n            remain = result_dict['months'] % 12\n            result_dict['years'] += advance\n            result_dict['months'] = remain\n        if result_dict['weeks']:\n            result_dict['days'] += result_dict['weeks'] * 7\n        return result_dict", "response": "parse string to regular timedelta\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a date in the format Jan 21st. 2015 returns a datetime of it.", "response": "def from_str(date):\n        \"\"\"\n            Given a date in the format: Jan,21st.2015\n            will return a datetime of it.\n        \"\"\"\n        month = date[:3][0] + date[:3][-2:].lower()\n        if month not in NAMED_MONTHS:\n            raise CanNotFormatError('Month not recognized')\n\n        date = date.replace(',', '').replace(' ', '').replace('.', '')\n        try:\n            day_unit = [x for x in ['st', 'rd', 'nd', 'th'] if x in date][0]\n            day = int(re.search(r'\\d+', date.split(day_unit)[0]).group())\n            year = int(re.search(r'\\d+', date.split(day_unit)[1]).group())\n            numeric_month = NAMED_MONTHS[month]\n            return datetime.date(int(year), numeric_month, day)\n        except:\n            raise CanNotFormatError('Not well formatted. Expecting something like May,21st.2015')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef today(year=None):\n    return datetime.date(int(year), _date.month, _date.day) if year else _date", "response": "this day last year"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tomorrow(date=None):\n    if not date:\n        return _date + datetime.timedelta(days=1)\n    else:\n        current_date = parse(date)\n        return current_date + datetime.timedelta(days=1)", "response": "tomorrow is another day"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all days between first and second", "response": "def days_range(first=None, second=None, wipe=False):\n    \"\"\"\n    get all days between first and second\n\n    :param first: datetime, date or string\n    :param second: datetime, date or string\n    :param wipe: boolean, excludes first and last date from range when True. Default is False.\n    :return: list\n    \"\"\"\n    _first, _second = parse(first), parse(second)\n    (_start, _end) = (_second, _first) if _first > _second else (_first, _second)\n    days_between = (_end - _start).days\n    date_list = [_end - datetime.timedelta(days=x) for x in range(0, days_between + 1)]\n    if wipe and len(date_list) >= 2:\n        date_list = date_list[1:-1]\n    return date_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the current month s last day", "response": "def last_day(year=_year, month=_month):\n    \"\"\"\n    get the current month's last day\n    :param year:  default to current year\n    :param month:  default to current month\n    :return: month's last day\n    \"\"\"\n    last_day = calendar.monthrange(year, month)[1]\n    return datetime.date(year=year, month=month, day=last_day)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef midnight(arg=None):\n    if arg:\n        _arg = parse(arg)\n        if isinstance(_arg, datetime.date):\n            return datetime.datetime.combine(_arg, datetime.datetime.min.time())\n        elif isinstance(_arg, datetime.datetime):\n            return datetime.datetime.combine(_arg.date(), datetime.datetime.min.time())\n    else:\n        return datetime.datetime.combine(_date, datetime.datetime.min.time())", "response": "convert date to datetime as midnight or get current day s midnight\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncounts datetime before base time", "response": "def before(base=_datetime, diff=None):\n    \"\"\"\n    count datetime before `base` time\n    :param base:  minuend -> str/datetime/date\n    :param diff:  str\n    :return: datetime\n    \"\"\"\n    _base = parse(base)\n    if isinstance(_base, datetime.date):\n        _base = midnight(_base)\n    if not diff:\n        return _base\n    result_dict = dp(diff)\n    # weeks already convert to days in diff_parse function(dp)\n    for unit in result_dict:\n        _val = result_dict[unit]\n        if not _val:\n            continue\n        if unit == 'years':\n            _base = _base.replace(year=(_base.year - _val))\n        elif unit == 'months':\n            if _base.month <= _val:\n                _month_diff = 12 - (_val - _base.month)\n                _base = _base.replace(year=_base.year - 1).replace(month=_month_diff)\n            else:\n                _base = _base.replace(month=_base.month - _val)\n        elif unit in ['days', 'hours', 'minutes', 'seconds']:\n            _base = _base - datetime.timedelta(**{unit: _val})\n    return _base"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert datetime to date", "response": "def _datetime_to_date(arg):\n    \"\"\"\n    convert datetime/str to date\n    :param arg:\n    :return:\n    \"\"\"\n    _arg = parse(arg)\n    if isinstance(_arg, datetime.datetime):\n        _arg = _arg.date()\n    return _arg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmothering s day in May", "response": "def mother(year=None):\n    \"\"\"\n    the 2nd Sunday in May\n    :param year: int\n    :return: Mother's day\n    \"\"\"\n    may_first = datetime.date(_year, 5, 1) if not year else datetime.date(int(year), 5, 1)\n    weekday_seq = may_first.weekday()\n    return datetime.date(may_first.year, 5, (14 - weekday_seq))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef father(year=None):\n    june_first = datetime.date(_year, 6, 1) if not year else datetime.date(int(year), 6, 1)\n    weekday_seq = june_first.weekday()\n    return datetime.date(june_first.year, 6, (21 - weekday_seq))", "response": "Father s day in June s order"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef thanks(year=None):\n    nov_first = datetime.date(_year, 11, 1) if not year else datetime.date(int(year), 11, 1)\n    weekday_seq = nov_first.weekday()\n    if weekday_seq > 3:\n        current_day = 32 - weekday_seq\n    else:\n        current_day = 25 - weekday_seq\n    return datetime.date(nov_first.year, 11, current_day)", "response": "Returns the Thanksgiving Day in a Nov\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine whether the Docker image for a given bug has been installed on the server.", "response": "def is_installed(self, bug: Bug) -> bool:\n        \"\"\"\n        Determines whether the Docker image for a given bug has been installed\n        on the server.\n        \"\"\"\n        r = self.__api.get('bugs/{}/installed'.format(bug.name))\n\n        if r.status_code == 200:\n            answer = r.json()\n            assert isinstance(answer, bool)\n            return answer\n\n        # TODO bug not registered on server\n        if r.status_code == 404:\n            raise KeyError(\"no bug found with given name: {}\".format(bug.name))\n\n        self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(self, bug: Bug) -> None:\n        path = \"bugs/{}\".format(bug.name)\n        payload = bug.to_dict()\n        r = self.__api.put(path, json=payload)\n        if r.status_code != 204:\n            self.__api.handle_erroneous_response(r)", "response": "Dynamically registers a given bug with the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the Docker image associated with a given Bug object.", "response": "def build(self, bug: Bug):\n        \"\"\"\n        Instructs the server to build the Docker image associated with a given\n        bug.\n        \"\"\"\n        r = self.__api.post('bugs/{}/build'.format(bug.name))\n\n        if r.status_code == 204:\n            return\n        if r.status_code == 200:\n            raise Exception(\"bug already built: {}\".format(bug.name))\n        # TODO: implement ImageBuildFailed.from_dict\n        if r.status_code == 400:\n            raise Exception(\"build failure\")\n        if r.status_code == 404:\n          raise KeyError(\"no bug found with given name: {}\".format(bug.name))\n\n        self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef refresh(self) -> None:\n        logger.info('refreshing sources')\n        for source in list(self):\n            self.unload(source)\n\n        if not os.path.exists(self.__registry_fn):\n            return\n\n        # TODO add version\n        with open(self.__registry_fn, 'r') as f:\n            registry = yaml.load(f)\n        assert isinstance(registry, list)\n\n        for source_description in registry:\n            source = Source.from_dict(source_description)\n            self.load(source)\n\n        logger.info('refreshed sources')", "response": "Reloads all sources that are registered with this server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self) -> None:\n        for source_old in self:\n            if isinstance(source_old, RemoteSource):\n                repo = git.Repo(source_old.location)\n                origin = repo.remotes.origin\n                origin.pull()\n\n                sha = repo.head.object.hexsha\n                version = repo.git.rev_parse(sha, short=8)\n\n                if version != source_old.version:\n                    source_new  = RemoteSource(source_old.name,\n                                               source_old.location,\n                                               source_old.url,\n                                               version)\n                    logger.info(\"updated source: %s [%s -> %s]\", source_old.name,\n                                                                 source_old.version,\n                                                                 source_new.version)\n                    self.load(source_new)\n\n                else:\n                    logger.debug(\"no updates for source: %s\", source_old.name)\n\n        # write to disk\n        # TODO local directory may be corrupted if program terminates between\n        #   repo being updated and registry being saved; could add a \"fix\"\n        #   command to recalculate versions for remote sources\n        self.save()", "response": "Update the remote sources."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self) -> None:\n        logger.info('saving registry to: %s', self.__registry_fn)\n        d = [s.to_dict() for s in self]\n\n        os.makedirs(self.__path, exist_ok=True)\n        with open(self.__registry_fn, 'w') as f:\n            yaml.dump(d, f, indent=2, default_flow_style=False)\n        logger.info('saved registry to: %s', self.__registry_fn)", "response": "Saves the contents of the source manager to disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunloads a registered source.", "response": "def unload(self, source: Source) -> None:\n        \"\"\"\n        Unloads a registered source, causing all of its associated bugs, tools,\n        and blueprints to also be unloaded. If the given source is not loaded,\n        this function will do nothing.\n        \"\"\"\n        logger.info('unloading source: %s', source.name)\n        try:\n            contents = self.contents(source)\n            del self.__contents[source.name]\n            del self.__sources[source.name]\n\n            for name in contents.bugs:\n                bug = self.__installation.bugs[name]\n                self.__installation.bugs.remove(bug)\n            for name in contents.blueprints:\n                blueprint = self.__installation.build[name]\n                self.__installation.build.remove(blueprint)\n            for name in contents.tools:\n                tool = self.__installation.tools[name]\n                self.__installation.tools.remove(tool)\n        except KeyError:\n            pass\n        logger.info('unloaded source: %s', source.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self, source: Source) -> None:\n        logger.info('loading source %s at %s', source.name, source.location)\n        if source.name in self.__sources:\n            self.unload(source)\n\n        bugs = []\n        blueprints = []\n        tools = []\n\n        # find and parse all bugzoo files\n        glob_pattern = '{}/**/*.bugzoo.y*ml'.format(source.location)\n        for fn in glob.iglob(glob_pattern, recursive=True):\n            if fn.endswith('.yml') or fn.endswith('.yaml'):\n                logger.debug('found manifest file: %s', fn)\n                self.__parse_file(source, fn, bugs, blueprints, tools)\n                logger.debug('parsed manifest file: %s', fn)\n\n        # register contents\n        for bug in bugs:\n            self.__installation.bugs.add(bug)\n        for blueprint in blueprints:\n            self.__installation.build.add(blueprint)\n        for tool in tools:\n            self.__installation.tools.add(tool)\n\n        # record contents of source\n        contents = SourceContents([b.name for b in blueprints],\n                                  [b.name for b in bugs],\n                                  [t.name for t in tools])\n        self.__sources[source.name] = source\n        self.__contents[source.name] = contents\n        logger.info(\"loaded source: %s\", source.name)", "response": "Loads all resources from a given source."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, name: str, path_or_url: str) -> Source:\n        logger.info(\"adding source: %s -> %s\", name, path_or_url)\n        if name in self.__sources:\n            logger.info(\"name already used by existing source: %s\", name)\n            raise NameInUseError(name)\n\n        is_url = False\n        try:\n            scheme = urllib.parse.urlparse(path_or_url).scheme\n            is_url = scheme in ['http', 'https']\n            logger.debug(\"source determined to be remote: %s\", path_or_url)\n        except ValueError:\n            logger.debug(\"source determined to be local: %s\", path_or_url)\n\n        if is_url:\n            url = path_or_url\n\n            # convert url to a local path\n            path = url.replace('https://', '')\n            path = path.replace('/', '_')\n            path = path.replace('.', '_')\n            path = os.path.join(self.__path, path)\n\n            # download from remote to local\n            shutil.rmtree(path, ignore_errors=True)\n            try:\n                # TODO shallow clone\n                logger.debug(\"cloning repository %s to %s\", url, path)\n                repo = git.Repo.clone_from(url, path)\n                logger.debug(\"cloned repository %s to %s\", url, path)\n\n                sha = repo.head.object.hexsha\n                version = repo.git.rev_parse(sha, short=8)\n            except: # TODO determine error type\n                shutil.rmtree(path, ignore_errors=True)\n                logger.error(\"failed to download remote source to local: %s -> %s\", url, path)\n                raise IOError(\"failed to download remote source to local installation: '{}' -> '{}'\".format(url, path))\n            source = RemoteSource(name, path, url, version)\n\n        else:\n            path = os.path.abspath(path_or_url)\n            if not os.path.isdir(path):\n                raise IOError(\"no directory found at path: {}\".format(path))\n            source = LocalSource(name, path)\n\n        self.load(source)\n        self.save()\n        logger.info('added source: %s', name)", "response": "Adds a new source to the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, source: Source) -> None:\n        self.unload(source)\n        if isinstance(source, RemoteSource):\n            shutil.rmtree(source.location, ignore_errors=True)\n        self.save()", "response": "Unregisters a given source with this server and saves the local copy to disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine whether or not the Docker image for a given bug has been installed onto this server.", "response": "def is_installed(self, bug: Bug) -> bool:\n        \"\"\"\n        Determines whether or not the Docker image for a given bug has been\n        installed onto this server.\n\n        See: `BuildManager.is_installed`\n        \"\"\"\n        return self.__installation.build.is_installed(bug.image)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(self,\n              bug: Bug,\n              force: bool = True,\n              quiet: bool = False\n              ) -> None:\n        \"\"\"\n        Builds the Docker image associated with a given bug.\n\n        See: `BuildManager.build`\n        \"\"\"\n        self.__installation.build.build(bug.image,\n                                        force=force,\n                                        quiet=quiet)", "response": "Builds the Docker image associated with a given bug."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uninstall(self,\n                  bug: Bug,\n                  force: bool = False,\n                  noprune: bool = False\n                  ) -> None:\n        \"\"\"\n        Uninstalls all Docker images associated with this bug.\n\n        See: `BuildManager.uninstall`\n        \"\"\"\n        self.__installation.build.uninstall(bug.image,\n                                            force=force,\n                                            noprune=noprune)", "response": "Uninstalls all Docker images associated with this bug."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads the Docker image for a given bug.", "response": "def download(self, bug: Bug, force=True) -> bool:\n        \"\"\"\n        Attempts to download the Docker image for a given bug from\n        `DockerHub <https://hub.docker.com>`_. If the force parameter is set to\n        True, any existing image will be overwritten.\n\n        Returns:\n            `True` if successfully downloaded, else `False`.\n        \"\"\"\n        return self.__installation.build.download(bug.image,\n                                                  force=force)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upload(self, bug: Bug) -> bool:\n        return self.__installation.build.upload(bug.image)", "response": "Uploads the Docker image for a given bug."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating that a given bug successfully builds and that it produces an anonymized set of test suite outcomes.", "response": "def validate(self, bug: Bug, verbose: bool = True) -> bool:\n        \"\"\"\n        Checks that a given bug successfully builds, and that it produces an\n        expected set of test suite outcomes.\n\n        Parameters:\n            verbose: toggles verbosity of output. If set to `True`, the\n                outcomes of each test will be printed to the standard output.\n\n        Returns:\n            `True` if bug behaves as expected, else `False`.\n        \"\"\"\n        # attempt to rebuild -- don't worry, Docker's layer caching prevents us\n        # from actually having to rebuild everything from scratch :-)\n        try:\n            self.build(bug, force=True, quiet=True)\n        except docker.errors.BuildError:\n            print(\"failed to build bug: {}\".format(self.identifier))\n            return False\n\n        # provision a container\n        validated = True\n        try:\n            c = None\n            c = self.__installation.containers.provision(bug)\n\n            # ensure we can compile the bug\n            # TODO: check compilation status!\n            print_task_start('Compiling')\n            self.__installation.containers.compile(c)\n            print_task_end('Compiling', 'OK')\n\n            for t in bug.tests:\n                if t.expected_outcome is True:\n                    task = 'Running test: {}'.format(t.name)\n                    print_task_start(task)\n\n                    outcome = \\\n                        self.__installation.containers.execute(c, t,\n                                                               verbose=verbose)\n                    if not outcome.passed:\n                        validated = False\n                        print_task_end(task, 'UNEXPECTED: FAIL')\n                        response = textwrap.indent(outcome.response.output,\n                                                   ' ' * 4)\n                        print('\\n' + response)\n                    else:\n                        print_task_end(task, 'OK')\n\n                if t.expected_outcome is False:\n                    task = 'Running test: {}'.format(t.name)\n                    print_task_start(task)\n\n                    outcome = \\\n                        self.__installation.containers.execute(c, t,\n                                                               verbose=verbose)\n                    if outcome.passed:\n                        validated = False\n                        print_task_end(task, 'UNEXPECTED: PASS')\n                        response = textwrap.indent(outcome.response.output,\n                                                   ' ' * 4)\n                        print('\\n' + response)\n                    else:\n                        print_task_end(task, 'OK')\n\n        # ensure that the container is destroyed!\n        finally:\n            if c:\n                del self.__installation.containers[c.uid]\n\n        return validated"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the coverage information for each test within the test suite for the given bug.", "response": "def coverage(self, bug: Bug) -> TestSuiteCoverage:\n        \"\"\"\n        Provides coverage information for each test within the test suite\n        for the program associated with this bug.\n\n        Parameters:\n            bug: the bug for which to compute coverage.\n\n        Returns:\n            a test suite coverage report for the given bug.\n        \"\"\"\n        # determine the location of the coverage map on disk\n        fn = os.path.join(self.__installation.coverage_path,\n                          \"{}.coverage.yml\".format(bug.name))\n\n        # is the coverage already cached? if so, load.\n        if os.path.exists(fn):\n            return TestSuiteCoverage.from_file(fn)\n\n        # if we don't have coverage information, compute it\n        mgr_ctr = self.__installation.containers\n        container = None\n        try:\n            container = mgr_ctr.provision(bug)\n            coverage = mgr_ctr.coverage(container)\n\n            # save to disk\n            with open(fn, 'w') as f:\n                yaml.dump(coverage.to_dict(), f, default_flow_style=False)\n        finally:\n            if container:\n                del mgr_ctr[container.id]\n\n        return coverage"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear(self) -> None:\n        logger.debug(\"clearing all running containers\")\n        all_uids = [uid for uid in self.__containers.keys()]\n        for uid in all_uids:\n            try:\n                del self[uid]\n            except KeyError:\n                # Already deleted\n                pass\n        logger.debug(\"cleared all running containers\")", "response": "Clears all running containers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bug(self, container: Container) -> Bug:\n        name = container.bug\n        return self.__installation.bugs[name]", "response": "Returns a description of the bug inside a given container."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef provision(self,\n                  bug: Bug,\n                  uid: str = None,\n                  tools: Optional[List[Tool]] = None,\n                  volumes: Optional[Dict[str, str]] = None,\n                  network_mode: str = 'bridge',\n                  ports: Optional[Dict[int, int]] = None,\n                  interactive: bool = False\n                  ) -> Container:\n        \"\"\"\n        Provisions and returns a container for a given bug.\n\n        Parameters:\n            bug: the bug that should be used to provision a container.\n            uid: a unique identifier (UID) for the container. If no UID is\n                provided then one will be automatically generated.\n\n        Returns:\n            a description of the provisioned container.\n        \"\"\"\n        if tools is None:\n            tools = []\n        if volumes is None:\n            volumes = {}\n        if ports is None:\n            ports = {}\n\n        if uid is None:\n            uid = str(uuid.uuid4())\n        logger.debug(\"provisioning container for bug %s: %s\",\n                     bug.name, uid)\n\n        tool_containers = [self.__installation.tools.provision(t) for t in tools]\n        self.__dockerc_tools[uid] = tool_containers\n        tool_container_ids = [c.id for c in tool_containers]\n\n        # prepare the environment for the container\n        logger.debug(\"creating temporary environment file for container %s\",\n                     uid)\n        env = [(k, v) for t in tools for (k, v) in t.environment.items()]\n        env = [\"{}=\\\"{}\\\"\".format(k, v) for (k, v) in env]\n        env = \"\\n\".join(env)\n        env_file = tempfile.NamedTemporaryFile(mode='w', suffix='.bugzoo.env')\n        env_file.write(env)\n        env_file.flush()\n        self.__env_files[uid] = env_file\n        logger.debug(\"created temporary environment file for container %s: %s\",\n                     uid, env_file.name)\n\n        volumes = copy.deepcopy(volumes)\n        volumes[env_file.name] = \\\n            {'bind': '/.environment.host', 'mode': 'ro'}\n\n        # we copy the environment variables from the host machine, load them,\n        # and save the complete set of environment variables to /.environment.\n        # all future calls to the container will source the variables in\n        # /.environment.\n        cmd = (\n            \"sudo cp /.environment.host /.environment && \"\n            \"sudo chown $(whoami):$(whoami) /.environment && \"\n            \"source /.environment && \"\n            \"sudo rm /.environment && \"\n            \"export | sudo tee /.environment > /dev/null && \"\n            \"sudo chmod 444 /.environment && \"\n            \"echo 'BUGZOO IS READY TO GO!' && \"\n            \"/bin/bash\"\n        )\n        cmd = '/bin/bash -c \"{}\"'.format(cmd)\n\n        logger.debug(\"creating Docker container for BugZoo container: %s\", uid)  # noqa: pycodestyle\n        dockerc = \\\n            self.__client_docker.containers.create(\n                bug.image,\n                cmd,\n                name=uid,\n                volumes=volumes,\n                volumes_from=tool_container_ids,\n                ports=ports,\n                network_mode=network_mode,\n                stdin_open=True,\n                tty=False,\n                # tty=interactive,\n                detach=True)  # type: docker.Container # noqa: pycodestyle\n        self.__dockerc[uid] = dockerc\n        logger.debug(\"created Docker container for BugZoo container: %s\", uid)\n        logger.debug(\"starting Docker container for BugZoo container: %s\", uid)  # noqa: pycodestyle\n        dockerc.start()\n        logger.debug(\"started Docker container for BugZoo container: %s\", uid)\n\n        # block until /.environment is ready\n        logger.debug(\"blocking until environment file has been constructed for container: %s\", uid)  # noqa: pycodestyle\n        ready = False\n        output_startup = []  # type: List[str]\n        for line in self.__api_docker.logs(dockerc.id, stream=True):\n            line = line.decode('utf-8').strip()\n            output_startup.append(line)\n            if line == \"BUGZOO IS READY TO GO!\":\n                ready = True\n                break\n        if not ready:\n            response = indent(''.join(output_startup), 4)\n            response = \"[RESPONSE]\\n{}\\n[/RESPONSE]\".format(response)\n            response = indent(response, 2)\n            msg = \"failed to start Docker container, {}:\\n{}\"\n            msg = msg.format(uid, response)\n            logger.error(msg)\n            raise Exception(msg)  # TODO add exception; DockerException, maybe?\n        logger.debug(\"environment file has been constructed for container: %s\", uid)  # noqa: pycodestyle\n\n        container = Container(bug=bug.name,\n                              uid=uid,\n                              tools=[t.name for t in tools])\n        self.__containers[uid] = container\n        logger.debug(\"provisioned container for bug %s: %s\",\n                     bug.name, uid)\n\n        logger.debug(\"STATUS OF CONTAINER: %s\", dockerc.status)\n        return container", "response": "Provisions and returns a container for a given bug."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a named temporary file within a given container.", "response": "def mktemp(self, container: Container) -> str:\n        \"\"\"\n        Creates a named temporary file within a given container.\n\n        Returns:\n            the absolute path to the created temporary file.\n        \"\"\"\n        logger.debug(\"creating a temporary file inside container %s\",\n                     container.uid)\n        response = self.command(container, \"mktemp\")\n\n        if response.code != 0:\n            msg = \"failed to create temporary file for container {}: [{}] {}\"\n            msg = msg.format(uid, response.code, response.output)\n            logger.error(msg)\n            raise Exception(msg)  # FIXME add new exception\n\n        assert response.code == 0, \"failed to create temporary file\"\n        fn = response.output.strip()\n        logger.debug(\"created temporary file inside container %s: %s\",\n                     container.uid, fn)\n        return fn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_alive(self, container: Container) -> bool:\n        uid = container.uid\n        return uid in self.__dockerc and \\\n               self.__dockerc[uid].status == 'running'", "response": "Determines whether a given container is still alive."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ip_address(self,\n                   container: Container,\n                   raise_error: bool = False\n                   ) -> Optional[Union[IPv4Address, IPv6Address]]:\n        \"\"\"\n        The IP address used by a given container, or None if no IP address has\n        been assigned to that container.\n        \"\"\"\n         # TODO: refactor!\n        api_client = docker.APIClient(base_url='unix://var/run/docker.sock')\n        docker_info = api_client.inspect_container(container.id)\n        address = docker_info['NetworkSettings']['IPAddress']\n        try:\n            return IPv4Address(address)\n        except ipaddress.AddressValueError:\n            try:\n                return IPv6Address(address)\n            except ipaddress.AddressValueError:\n                if raise_error:\n                    raise\n                return None", "response": "Returns the IP address used by a given container."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef patch(self, container: Container, p: Patch) -> bool:\n        assert isinstance(p, Patch)\n        file_container = None\n        dockerc = self.__dockerc[container.uid]\n        bug = self.__installation.bugs[container.bug]\n        logger.debug(\"Applying patch to container [%s]:\\n%s\",\n                     container.uid,\n                     str(p))\n\n        try:\n            file_host = NamedTemporaryFile(mode='w', suffix='bugzoo')\n            file_host.write(str(p))\n            file_host.flush()\n\n            # copy contents to a temporary file on the container\n            (retcode, file_container) = dockerc.exec_run('mktemp')\n            assert retcode == 0\n            file_container = file_container.decode(sys.stdout.encoding).strip()\n\n            self.copy_to(container, file_host.name, file_container)\n\n            # run patch command inside the source directory\n            cmd = 'sudo chown $(whoami) \"{}\" && patch -p0 < \"{}\"'\n            cmd = cmd.format(file_container, file_container)\n            outcome = self.command(container, cmd, context=bug.source_dir)\n            logger.debug(\"Patch application outcome [%s]: (retcode=%d)\\n%s\",\n                         container.uid,\n                         outcome.code,\n                         outcome.output)\n            return outcome.code == 0\n\n        finally:\n            if file_container:\n                dockerc.exec_run('rm \"{}\"'.format(file_container))", "response": "Applies a given patch to the source code of a given container."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninteracting with the container.", "response": "def interact(self, container: Container) -> None:\n        \"\"\"\n        Connects to the PTY (pseudo-TTY) for a given container.\n        Blocks until the user exits the PTY.\n        \"\"\"\n        cmd = \"/bin/bash -c 'source /.environment && /bin/bash'\"\n        cmd = \"docker exec -it {} {}\".format(container.id, cmd)\n        subprocess.call(cmd, shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the coverage extractor for a given container.", "response": "def coverage_extractor(self, container: Container) -> CoverageExtractor:\n        \"\"\"\n        Retrieves the coverage extractor for a given container.\n        \"\"\"\n        return CoverageExtractor.build(self.__installation, container)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute line coverage information over a set of tests for a given container.", "response": "def coverage(self,\n                 container: Container,\n                 tests: Optional[Iterable[TestCase]] = None,\n                 *,\n                 instrument: bool = True\n                 ) -> TestSuiteCoverage:\n        \"\"\"\n        Computes line coverage information over a provided set of tests for\n        the program inside a given container.\n        \"\"\"\n        extractor = self.coverage_extractor(container)\n        if tests is None:\n            bugs = self.__installation.bugs\n            bug = bugs[container.bug]\n            tests = bug.tests\n        return extractor.run(tests, instrument=instrument)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a specified test inside a given container.", "response": "def execute(self,\n                container: Container,\n                test: TestCase,\n                verbose: bool = False\n                ) -> TestOutcome:\n        \"\"\"\n        Runs a specified test inside a given container.\n\n        Returns:\n            the outcome of the test execution.\n        \"\"\"\n        bug = self.__installation.bugs[container.bug]  # type: Bug\n        response = self.command(container,\n                                cmd=test.command,\n                                context=test.context,\n                                stderr=True,\n                                time_limit=test.time_limit,\n                                kill_after=test.kill_after,\n                                verbose=verbose)\n        passed = test.oracle.check(response)\n        return TestOutcome(response, passed)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compile(self,\n                container: Container,\n                verbose: bool = False\n                ) -> CompilationOutcome:\n        \"\"\"\n        Attempts to compile the program inside a given container.\n\n        Params:\n            verbose: specifies whether to print the stdout and stderr produced\n                by the compilation command to the stdout. If `True`, then the\n                stdout and stderr will be printed.\n\n        Returns:\n            a summary of the outcome of the compilation attempt.\n        \"\"\"\n        # TODO use container name\n        bug = self.__installation.bugs[container.bug]\n        return bug.compiler.compile(self, container, verbose=verbose)", "response": "Attempts to compile the program inside a given container."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to compile the program inside a given container with instrumentation enabled.", "response": "def compile_with_instrumentation(self,\n                                     container: Container,\n                                     verbose: bool = False\n                                     ) -> CompilationOutcome:\n        \"\"\"\n        Attempts to compile the program inside a given container with\n        instrumentation enabled.\n\n        See: `Container.compile`\n        \"\"\"\n        bug = self.__installation.bugs[container.bug]\n        bug.compiler.clean(self, container, verbose=verbose) # TODO port\n        return bug.compiler.compile_with_coverage_instrumentation(self,\n                                                                  container,\n                                                                  verbose=verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy a file from the host machine to the specified location inside a container.", "response": "def copy_to(self,\n                container: Container,\n                fn_host: str,\n                fn_container: str\n                ) -> None:\n        \"\"\"\n        Copies a file from the host machine to a specified location inside a\n        container.\n\n        Raises:\n            FileNotFound: if the host file wasn't found.\n            subprocess.CalledProcessError: if the file could not be copied to\n                the container.\n        \"\"\"\n        logger.debug(\"Copying file to container, %s: %s -> %s\",\n                     container.uid, fn_host, fn_container)\n\n        if not os.path.exists(fn_host):\n            logger.error(\"Failed to copy file [%s] to [%s] in container [%s]: not found.\",  # noqa: pycodestyle\n                         fn_host, fn_container, container.uid)\n            raise FileNotFound(fn_host)\n\n        cmd = \"docker cp '{}' '{}:{}'\".format(fn_host, container.id, fn_container)\n        try:\n            subprocess.check_output(cmd, shell=True)\n            logger.debug(\"Copied file to container, %s: %s -> %s\",\n                         container.uid, fn_host, fn_container)\n\n            r = self.command(container,\n                             \"sudo chown $(whoami) '{}'\".format(fn_container))\n            if r.code != 0:\n                m = \"failed to update permissions for container file [{}] (exit code: {}): {}\"  # noqa: pycodestyle\n                m = m.format(fn_container, r.code, r.output)\n                raise BugZooException(m)\n        # TODO implement error handling\n        except subprocess.CalledProcessError:\n            logger.exception(\"Failed to copy file to container, %s: %s -> %s\",\n                             container.uid, fn_host, fn_container)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy_from(self,\n                  container: Container,\n                  fn_container: str,\n                  fn_host: str\n                  ) -> None:\n        \"\"\"\n        Copies a given file from the container to a specified location on the\n        host machine.\n        \"\"\"\n        logger.debug(\"Copying file from container, %s: %s -> %s\",\n                     container.uid, fn_container, fn_host)\n        cmd = \"docker cp '{}:{}' '{}'\".format(container.id, fn_container, fn_host)\n        try:\n            subprocess.check_output(cmd, shell=True)\n            logger.debug(\"Copied file from container, %s: %s -> %s\",\n                         container.uid, fn_container, fn_host)\n        # TODO implement error handling\n        except subprocess.CalledProcessError:\n            logger.exception(\"Failed to copy file from container, %s: %s -> %s\",  # noqa: pycodestyle\n                             container.uid, fn_container, fn_host)\n            raise", "response": "Copies a given file from a given container to a specified location on the host machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef command(self,\n                container: Container,\n                cmd: str,\n                context: Optional[str] = None,\n                stdout: bool = True,\n                stderr: bool = False,\n                block: bool = True,\n                verbose: bool = False,\n                time_limit: Optional[int] = None,\n                kill_after: Optional[int] = 1\n                ) -> Union[ExecResponse, PendingExecResponse]:\n        \"\"\"\n        Executes a provided shell command inside a given container.\n\n        Parameters:\n            time_limit: an optional parameter that is used to specify the\n                number of seconds that the command should be allowed to run\n                without completing before it is aborted. Only supported by\n                blocking calls.\n\n        Returns:\n            a description of the response.\n\n        Raises:\n            TimeoutError: if a time limit is given and the command fails\n                to complete within that time. Only supported by blocking calls.\n        \"\"\"\n        cmd_original = cmd\n        logger_c = logger.getChild(container.uid)\n        logger_c.debug('executing command \"%s\"', cmd)\n        bug = self.__installation.bugs[container.bug]\n\n        # TODO: we need a better long-term alternative\n        if context is None:\n            context = os.path.join(bug.source_dir, '..')\n        logger_c.debug('using execution context: %s', context)\n\n        cmd = 'source /.environment && cd {} && {}'.format(context, cmd)\n        cmd_wrapped = \"/bin/bash -c '{}'\".format(cmd)\n        if time_limit is not None and time_limit > 0:\n            logger_c.debug(\"running command with time limit: %d seconds\", time_limit)  # noqa: pycodestyle\n            cmd_template = \"timeout --kill-after={} --signal=SIGTERM {} {}\"\n            cmd_wrapped = cmd_template.format(kill_after,\n                                              time_limit,\n                                              cmd_wrapped)\n        cmd = cmd_wrapped\n\n        # based on: https://github.com/roidelapluie/docker-py/commit/ead9ffa34193281967de8cc0d6e1c0dcbf50eda5\n        logger_c.debug(\"executing raw command: %s\", cmd)\n        logger_c.debug('creating exec object for command: %s', cmd)\n        docker_client = self.__installation.docker\n        try:\n            response = docker_client.api.exec_create(container.id,\n                                                     cmd,\n                                                     tty=True,\n                                                     stdout=stdout,\n                                                     stderr=stderr)\n        except docker.errors.APIError:\n            logger_c.exception('failed to create exec object for command: %s', cmd)  # noqa: pycodestyle\n            raise\n        logger_c.debug(\"created exec object for command: %s\", cmd)\n        exec_id = response['Id']\n\n        time_start = timer()\n        out = self.__api_docker.exec_start(exec_id, stream=True)\n\n        if not block:\n            return PendingExecResponse(response, out)\n\n        output = []\n        for line in out:\n            line = line.decode('utf-8').rstrip('\\n')\n            if verbose:\n                print(line, flush=True)\n            output.append(line)\n        time_running = timer() - time_start\n        output = '\\n'.join(output)\n        code = self.__api_docker.exec_inspect(exec_id)['ExitCode']\n        logger_c.debug('finished executing command: %s. (exited with code %d and took %.2f seconds.)\\n%s',  # noqa: pycodestyle\n                       cmd_original, code, time_running, output)\n\n        return ExecResponse(code, time_running, output)", "response": "Executes a command inside a given container."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npersist a given container to a given Docker image on this server.", "response": "def persist(self, container: Container, image: str) -> None:\n        \"\"\"\n        Persists the state of a given container to a BugZoo image on this\n        server.\n\n        Parameters:\n            container: the container to persist.\n            image: the name of the Docker image that should be created.\n\n        Raises:\n            ImageAlreadyExists: if the image name is already in use by another\n                Docker image on this server.\n        \"\"\"\n        logger_c = logger.getChild(container.uid)\n        logger_c.debug(\"Persisting container as a Docker image: %s\", image)\n        try:\n            docker_container = self.__dockerc[container.uid]\n        except KeyError:\n            logger_c.exception(\"Failed to persist container: container no longer exists.\")  # noqa: pycodestyle\n            raise\n        try:\n            _ = self.__client_docker.images.get(image)\n            logger_c.error(\"Failed to persist container: image, '%s', already exists.\",  # noqa: pycodestyle\n                         image)\n            raise ImageAlreadyExists(image)\n        except docker.errors.ImageNotFound:\n            pass\n\n        cmd = \"docker commit {} {}\"\n        cmd = cmd.format(docker_container.id, image)\n        try:\n            subprocess.check_output(cmd, shell=True)\n        except subprocess.CalledProcessError:\n            logger.exception(\"Failed to persist container (%s) to image (%s).\",  # noqa: pycodestyle\n                             container.uid, image)\n            raise\n        logger_c.debug(\"Persisted container as a Docker image: %s\", image)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap a function responsible for implementing an API endpoint such that any server errors that are thrown are automatically transformed into appropriate HTTP responses.", "response": "def throws_errors(func):\n    \"\"\"\n    Wraps a function responsible for implementing an API endpoint such that\n    any server errors that are thrown are automatically transformed into\n    appropriate HTTP responses.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        response = func(*args, **kwargs)\n\n        # if no status code is provided, assume 400\n        if isinstance(response, BugZooException):\n            err_jsn = flask.jsonify(response.to_dict())\n            return err_jsn, 400\n\n        if isinstance(response, tuple) \\\n           and isinstance(response[0], BugZooException):\n            err = response[0] # type: BugZooException\n            err_jsn = flask.jsonify(err.to_dict())\n            return (err_jsn, *response[1:])\n\n        return response\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlaunch an ephemeral server instance and returns a client that will be immediately closed when no longer in context.", "response": "def ephemeral(*,\n              port: int = 6060,\n              timeout_connection: int = 30,\n              verbose: bool = False\n              ) -> Iterator[Client]:\n    \"\"\"\n    Launches an ephemeral server instance that will be immediately\n    close when no longer in context.\n\n    Parameters:\n        port: the port that the server should run on.\n        verbose: if set to True, the server will print its output to the\n            stdout, otherwise it will remain silent.\n\n    Returns:\n        a client for communicating with the server.\n    \"\"\"\n    url = \"http://127.0.0.1:{}\".format(port)\n    cmd = [\"bugzood\", \"--debug\", \"-p\", str(port)]\n    try:\n        stdout = None if verbose else subprocess.DEVNULL\n        stderr = None if verbose else subprocess.DEVNULL\n        proc = subprocess.Popen(cmd,\n                                preexec_fn=os.setsid,\n                                stdout=stdout,\n                                stderr=stderr)\n        yield Client(url, timeout_connection=timeout_connection)\n    finally:\n        os.killpg(proc.pid, signal.SIGTERM)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a coverage extractor class under a given name.", "response": "def register(name: str):\n    \"\"\"\n    Registers a coverage extractor class under a given name.\n\n    .. code: python\n\n        from bugzoo.mgr.coverage import CoverageExtractor, register\n\n        @register('mycov')\n        class MyCoverageExtractor(CoverageExtractor):\n            ...\n    \"\"\"\n    def decorator(cls: Type['CoverageExtractor']):\n        cls.register(name)\n        return cls\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_as_default(language: Language):\n    def decorator(cls: Type['CoverageExtractor']):\n        cls.register_as_default(language)\n        return cls\n    return decorator", "response": "Registers a coverage extractor class as the default coverage extractor for a given language."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(installation: 'BugZoo',\n              container: Container\n              ) -> 'CoverageExtractor':\n        \"\"\"\n        Constructs a CoverageExtractor for a given container using the coverage\n        instructions provided by its accompanying bug description.\n        \"\"\"\n        bug = installation.bugs[container.bug]  # type: Bug\n        instructions = bug.instructions_coverage\n        if instructions is None:\n            raise exceptions.NoCoverageInstructions\n\n        name = instructions.__class__.registered_under_name()\n        extractor_cls = _NAME_TO_EXTRACTOR[name]\n        builder = extractor_cls.from_instructions\n        extractor = builder(installation, container, instructions)\n        return extractor", "response": "Constructs a CoverageExtractor for a given container."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self,\n            tests: Iterable[TestCase],\n            *,\n            instrument: bool = True\n            ) -> TestSuiteCoverage:\n        \"\"\"\n        Computes line coverage information for a given set of tests.\n\n        Parameters:\n            tests: the tests for which coverage should be computed.\n            instrument: if set to True, calls prepare and cleanup before and\n                after running the tests. If set to False, prepare\n                and cleanup are not called, and the responsibility of calling\n                those methods is left to the user.\n        \"\"\"\n        container = self.container\n        logger.debug(\"computing coverage for container: %s\", container.uid)\n\n        try:\n            if instrument:\n                logger.debug(\"instrumenting container\")\n                self.prepare()\n            else:\n                logger.debug(\"not instrumenting container\")\n        except Exception:\n            msg = \"failed to instrument container.\"\n            raise exceptions.FailedToComputeCoverage(msg)\n\n        cov = {}\n        for test in tests:\n            logger.debug(\"Generating coverage for test %s in container %s\",\n                         test.name, container.uid)\n            outcome = self.__installation.containers.execute(container, test)\n            filelines = self.extract()\n            test_coverage = TestCoverage(test.name, outcome, filelines)\n            logger.debug(\"Generated coverage for test %s in container %s\",\n                         test.name, container.uid)\n            cov[test.name] = test_coverage\n\n        self.cleanup()\n\n        coverage = TestSuiteCoverage(cov)\n        logger.debug(\"Computed coverage for container: %s\", container.uid)\n        return coverage", "response": "Computes line coverage information for a given set of tests."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreconstruct a BugZooException from a dictionary - based description.", "response": "def from_dict(d: Dict[str, Any]) -> 'BugZooException':\n        \"\"\"\n        Reconstructs a BugZoo exception from a dictionary-based description.\n        \"\"\"\n        assert 'error' in d\n        d = d['error']\n\n        cls = getattr(sys.modules[__name__], d['kind'])\n        assert issubclass(cls, BugZooException)\n        return cls.from_message_and_data(d['message'],\n                                         d.get('data', {}))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreproduce an exception from the message and data contained in its .", "response": "def from_message_and_data(cls,\n                              message: str,\n                              data: Dict[str, Any]\n                              ) -> 'BugZooException':\n        \"\"\"\n        Reproduces an exception from the message and data contained in its\n        dictionary-based description.\n        \"\"\"\n        return cls(message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_dict(self) -> Dict[str, Any]:\n        jsn = {\n            'kind': self.__class__.__name__,\n            'message': self.message\n        } # type: Dict[str, Any]\n        data = self.data\n        if data:\n            jsn['data'] = data\n        jsn = {'error': jsn}\n        return jsn", "response": "Returns a dictionary representation of this exception."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_next(cls, lines: List[str]) -> 'Hunk':\n        header = lines[0]\n        assert header.startswith('@@ -')\n\n        # sometimes the first line can occur on the same line as the header.\n        # in that case, we inject a new line into the buffer\n        end_header_at = header.index(' @@')\n        bonus_line = header[end_header_at+3:]\n        if bonus_line != \"\":\n            lines.insert(1, bonus_line)\n\n        header = header[4:end_header_at]\n        left, _, right = header.partition(' +')\n        old_start_at = int(left.split(',')[0])\n        new_start_at = int(right.split(',')[0])\n\n        old_line_num = old_start_at\n        new_line_num = new_start_at\n        last_insertion_at = old_start_at\n\n\n        hunk_lines = [] # type: List[HunkLine]\n        while True:\n            # discarding the previous line ensures that we only consume lines\n            # from the line buffer that belong to the hunk\n            lines.pop(0)\n            if not lines:\n                break\n\n            line = lines[0]\n\n            # inserted line\n            if line.startswith('+'):\n                hunk_lines.append(InsertedLine(line[1:]))\n                new_line_num += 1\n\n            # deleted line\n            elif line.startswith('-'):\n                hunk_lines.append(DeletedLine(line[1:]))\n                old_line_num += 1\n\n            # context line\n            elif line.startswith(' '):\n                hunk_lines.append(ContextLine(line[1:]))\n                new_line_num += 1\n                old_line_num += 1\n\n            # end of hunk\n            else:\n                break\n\n        return Hunk(old_start_at, new_start_at, hunk_lines)", "response": "Reads the next hunk from a list of lines."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_unidiff(cls, diff: str) -> 'Patch':\n        lines = diff.split('\\n')\n        file_patches = []\n        while lines:\n            if lines[0] == '' or lines[0].isspace():\n                lines.pop(0)\n                continue\n            file_patches.append(FilePatch._read_next(lines))\n\n        return Patch(file_patches)", "response": "Constructs a Patch from a unified format diff."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear(self) -> None:\n        r = self.__api.delete('containers')\n        if r.status_code != 204:\n            self.__api.handle_erroneous_response(r)", "response": "Destroys all running containers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef provision(self,\n                  bug: Bug,\n                  *,\n                  plugins: Optional[List[Tool]] = None\n                  ) -> Container:\n        \"\"\"\n        Provisions a container for a given bug.\n        \"\"\"\n        if plugins is None:\n            plugins = []\n\n        logger.info(\"provisioning container for bug: %s\", bug.name)\n        endpoint = 'bugs/{}/provision'.format(bug.name)\n        payload = {\n            'plugins': [p.to_dict() for p in plugins]\n        }  # type: Dict[str, Any]\n        r = self.__api.post(endpoint, json=payload)\n\n        if r.status_code == 200:\n            container = Container.from_dict(r.json())\n            logger.info(\"provisioned container (id: %s) for bug: %s\",\n                               container.uid,\n                               bug.name)\n            return container\n\n        if r.status_code == 404:\n            raise KeyError(\"no bug registered with given name: {}\".format(bug.name))\n\n        self.__api.handle_erroneous_response(r)", "response": "Provisions a container for a given bug."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a temporary file for a given container.", "response": "def mktemp(self, container: Container) -> str:\n        \"\"\"\n        Generates a temporary file for a given container.\n\n        Returns:\n            the path to the temporary file inside the given container.\n        \"\"\"\n        r = self.__api.post('containers/{}/tempfile'.format(container.uid))\n        if r.status_code == 200:\n            return r.json()\n        self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the IP address used by a given container.", "response": "def ip_address(self,\n                   container: Container\n                   ) -> Union[IPv4Address, IPv6Address]:\n        \"\"\"\n        The IP address used by a given container, or None if no IP address has\n        been assigned to that container.\n        \"\"\"\n        r = self.__api.get('containers/{}/ip'.format(container.uid))\n        if r.status_code == 200:\n            return r.json()\n        self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether or not a given container is still alive.", "response": "def is_alive(self, container: Container) -> bool:\n        \"\"\"\n        Determines whether or not a given container is still alive.\n        \"\"\"\n        uid = container.uid\n        r = self.__api.get('containers/{}/alive'.format(uid))\n\n        if r.status_code == 200:\n            return r.json()\n\n        if r.status_code == 404:\n            raise KeyError(\"no container found with given UID: {}\".format(uid))\n\n        self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_coverage(self, container: Container) -> FileLineSet:\n        uid = container.uid\n        r = self.__api.post('containers/{}/read-coverage'.format(uid))\n        if r.status_code == 200:\n            return FileLineSet.from_dict(r.json())\n        self.__api.handle_erroneous_response(r)", "response": "Extracts a report of the lines that have been executed since the last\n        time that a coverage report was extracted."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninstrumenting the program inside the container for computing test suite coverage coverage.", "response": "def instrument(self,\n                   container: Container\n                   ) -> None:\n        \"\"\"\n        Instruments the program inside the container for computing test suite\n        coverage.\n\n        Params:\n            container: the container that should be instrumented.\n        \"\"\"\n        path = \"containers/{}/instrument\".format(container.uid)\n        r = self.__api.post(path)\n        if r.status_code != 204:\n            logger.info(\"failed to instrument container: %s\", container.uid)\n            self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to compile the program inside a given container. Params: container: the container whose program should be compiled. verbose: specifies whether to print the stdout and stderr produced by the compilation command to the stdout. If `True`, then the stdout and stderr will be printed. Returns: a summary of the outcome of the attempted compilation. Raises: KeyError: if the container no longer exists.", "response": "def compile(self,\n                container: Container,\n                verbose: bool = False\n                ) -> CompilationOutcome:\n        \"\"\"\n        Attempts to compile the program inside a given container.\n\n        Params:\n            container: the container whose program should be compiled.\n            verbose: specifies whether to print the stdout and stderr produced\n                by the compilation command to the stdout. If `True`, then the\n                stdout and stderr will be printed.\n\n        Returns:\n            a summary of the outcome of the attempted compilation.\n\n        Raises:\n            KeyError: if the container no longer exists.\n        \"\"\"\n        path = \"containers/{}/build\".format(container.uid)\n        params = {}\n        if verbose:\n            params['verbose'] = 'yes'\n        r = self.__api.post(path, params=params)\n        if r.status_code == 200:\n            return CompilationOutcome.from_dict(r.json())\n        self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving complete test suite coverage for a given container.", "response": "def coverage(self,\n                 container: Container,\n                 *,\n                 instrument: bool = True\n                 ) -> TestSuiteCoverage:\n        \"\"\"\n        Computes complete test suite coverage for a given container.\n\n        Parameters:\n            container: the container for which coverage should be computed.\n            rebuild: if set to True, the program will be rebuilt before\n                coverage is computed.\n        \"\"\"\n        uid = container.uid\n        logger.info(\"Fetching coverage information for container: %s\",\n                    uid)\n        uri = 'containers/{}/coverage'.format(uid)\n        r = self.__api.post(uri,\n                            params={'instrument': 'yes' if instrument else 'no'})\n        if r.status_code == 200:\n            jsn = r.json()\n            coverage = TestSuiteCoverage.from_dict(jsn)  # type: ignore\n            logger.info(\"Fetched coverage information for container: %s\",\n                        uid)\n            return coverage\n        try:\n            self.__api.handle_erroneous_response(r)\n        except exceptions.BugZooException as err:\n            logger.exception(\"Failed to fetch coverage information for container %s: %s\", uid, err.message)  # noqa: pycodestyle\n            raise\n        except Exception as err:\n            logger.exception(\"Failed to fetch coverage information for container %s due to unexpected failure: %s\", uid, err)  # noqa: pycodestyle\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exec(self,\n             container: Container,\n             command: str,\n             context: Optional[str] = None,\n             stdout: bool = True,\n             stderr: bool = False,\n             time_limit: Optional[int] = None\n             ) -> ExecResponse:\n        \"\"\"\n        Executes a given command inside a provided container.\n\n        Parameters:\n            container: the container to which the command should be issued.\n            command: the command that should be executed.\n            context: the working directory that should be used to perform the\n                execution. If no context is provided, then the command will be\n                executed at the root of the container.\n            stdout: specifies whether or not output to the stdout should be\n                included in the execution summary.\n            stderr: specifies whether or not output to the stderr should be\n                included in the execution summary.\n            time_limit: an optional time limit that is applied to the\n                execution. If the command fails to execute within the time\n                limit, the command will be aborted and treated as a failure.\n\n        Returns:\n            a summary of the outcome of the execution.\n\n        Raises:\n            KeyError: if the container no longer exists on the server.\n        \"\"\"\n        # FIXME perhaps these should be encoded as path variables?\n        payload = {\n            'command': command,\n            'context': context,\n            'stdout': stdout,\n            'stderr': stderr,\n            'time-limit': time_limit\n        }\n        path = \"containers/{}/exec\".format(container.uid)\n        r = self.__api.post(path, json=payload)\n\n        if r.status_code == 200:\n            return ExecResponse.from_dict(r.json())\n        if r.status_code == 404:\n            raise KeyError(\"no container found with given UID: {}\".format(container.uid))\n\n        self.__api.handle_erroneous_response(r)", "response": "Executes a given command inside a container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch(self, container: Container, patch: Patch) -> bool:\n        path = \"containers/{}\".format(container.uid)\n        payload = str(patch)\n        r = self.__api.patch(path, payload)\n\n        return r.status_code == 204", "response": "Applies a given patch to the source code files of a given container."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef persist(self, container: Container, image_name: str) -> None:\n        logger.debug(\"attempting to persist container (%s) to image (%s).\",\n                     container.id,\n                     image_name)\n        path = \"containers/{}/persist/{}\".format(container.id, image_name)\n        r = self.__api.put(path)\n        if r.status_code == 204:\n            logger.debug(\"persisted container (%s) to image (%s).\",\n                         container.id,\n                         image_name)\n            return\n        try:\n            self.__api.handle_erroneous_response(r)\n        except Exception:\n            logger.exception(\"failed to persist container (%s) to image (%s).\",  # noqa: pycodestyle\n                             container.id,\n                             image_name)\n            raise", "response": "Persist the state of a given container to a Docker image on the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a SimpleCompiler from its dictionary - based description.", "response": "def from_dict(d: dict) -> 'SimpleCompiler':\n        \"\"\"\n        Loads a SimpleCompiler from its dictionary-based description.\n        \"\"\"\n        cmd = d['command']\n        cmd_with_instrumentation = d.get('command_with_instrumentation', None)\n        time_limit = d['time-limit']\n        context = d['context']\n        cmd_clean = d.get('command_clean', 'exit 0')\n        return SimpleCompiler(command=cmd,\n                              command_clean=cmd_clean,\n                              command_with_instrumentation=cmd_with_instrumentation,\n                              context=context,\n                              time_limit=time_limit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compile(self, # type: ignore\n                manager_container,\n                container: 'Container', # type: ignore\n                verbose: bool = False\n                ) -> CompilationOutcome:\n        \"\"\"\n        See `Compiler.compile`\n        \"\"\"\n        return self.__compile(manager_container,\n                              container,\n                              self.__command,\n                              verbose)", "response": "Compile the current command."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compile_with_coverage_instrumentation(self, # type: ignore\n                                              manager_container,\n                                              container: 'Container', # type: ignore\n\n                                              verbose: bool = False\n                                              ) -> CompilationOutcome:\n        \"\"\"\n        See `Compiler.compile_with_coverage_instrumentation`\n        \"\"\"\n        if self.__command_with_instrumentation:\n            cmd = self.__command_with_instrumentation\n        else:\n            cmd = self.__command\n\n        return self.__compile(manager_container, container, cmd, verbose)", "response": "Compile with coverage instrumentation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting tags from a DOM tree", "response": "def get_tags(doc):\n    '''\n    Get tags from a DOM tree\n\n    :param doc: lxml parsed object\n    :return:\n    '''\n    tags = list()\n\n    for el in doc.getroot().iter():\n        if isinstance(el, lxml.html.HtmlElement):\n            tags.append(el.tag)\n        elif isinstance(el, lxml.html.HtmlComment):\n            tags.append('comment')\n        else:\n            raise ValueError('Don\\'t know what to do with element: {}'.format(el))\n\n    return tags"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the structural similarity between two DOMs", "response": "def structural_similarity(document_1, document_2):\n    \"\"\"\n    Computes the structural similarity between two DOM Trees\n    :param document_1: html string\n    :param document_2: html string\n    :return: int\n    \"\"\"\n    try:\n        document_1 = lxml.html.parse(StringIO(document_1))\n        document_2 = lxml.html.parse(StringIO(document_2))\n    except Exception as e:\n        print(e)\n        return 0\n\n    tags1 = get_tags(document_1)\n    tags2 = get_tags(document_2)\n    diff = difflib.SequenceMatcher()\n    diff.set_seq1(tags1)\n    diff.set_seq2(tags2)\n\n    return diff.ratio()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _resolve_path(self, container: Container, filepath: str) -> str:\n        bug = self.__mgr_bug[container.bug]\n        filepath_orig = filepath\n\n        if not os.path.isabs(filepath):\n            filepath = os.path.join(bug.source_dir, filepath)\n            logger.debug(\"converted relative path to absolute path: %s -> %s\",\n                         filepath_orig, filepath)\n\n        return filepath", "response": "Resolves the filepath to absolute paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the contents of a given file belonging to a given container.", "response": "def read(self, container: Container, filepath: str) -> str:\n        \"\"\"\n        Reads the contents of a given file belonging to a container.\n        \"\"\"\n        logger.debug(\"reading contents of file [%s] inside container [%s]\",\n                     filepath, container.id)\n        filepath_orig = filepath\n        filepath = self._resolve_path(container, filepath)\n\n        # TODO what if the file doesn't exist?\n        # TODO what if we encounter a permissions problem?\n\n        # copy the contents the file to a temporary file on the host machine\n        # TODO ensure correct permissions\n        (_, fn_host) = tempfile.mkstemp(suffix='.bugzoo')\n        try:\n            # TODO we can probably move 'copy_from' to this manager\n            self.__mgr_ctr.copy_from(container, filepath, fn_host)\n            with open(fn_host, 'rb') as fh:\n                blob = fh.read()\n        finally:\n            os.remove(fn_host)\n\n        # detect encoding\n        logger.debug(\"detecting encoding for file [%s] in container [%s]\",\n                     filepath_orig, container.id)\n        chardet_res = chardet.detect(blob)\n        encoding = chardet_res['encoding']\n        confidence = chardet_res['confidence']\n        logger.debug(\"detected encoding of file [%s] in container [%s]: %s (%.3f confidence)\",  # noqa: pycodestyle\n                     filepath_orig, container.id, encoding, confidence)\n\n        # if no encoding is detected, return an empty file\n        if encoding is None:\n            return ''\n\n        # decode file\n        logger.debug(\"decoding file [%s] in container [%s]\",\n                     filepath_orig, container.id)\n        try:\n            contents = blob.decode(encoding)\n        except UnicodeDecodeError:\n            logger.exception(\"failed to decode contents of file [%s] in container [%s]\",  # noqa: pycodestyle\n                             filepath_orig, container.id)\n            raise\n        logger.debug(\"decoded file [%s] in container [%s]\",\n                     filepath_orig, container.id)\n\n        logger.debug(\"read contents of file [%s] inside container [%s]\",\n                     filepath_orig, container.id)\n        return contents"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self,\n              container: Container,\n              filepath: str,\n              contents: str\n              ) -> str:\n        \"\"\"\n        Reads the contents of a given file belonging to a container.\n        \"\"\"\n        logger.debug(\"writing to file [%s] inside container [%s]\",\n                     filepath, container.id)\n        filepath = self._resolve_path(container, filepath)\n\n        # write the file contents to a temporary file on the host before\n        # copying that file to the container\n        (_, fn_host) = tempfile.mkstemp(suffix='.bugzoo')\n        try:\n            with open(fn_host, 'w') as fh:\n                fh.write(contents)\n            self.__mgr_ctr.copy_to(container, fn_host, filepath)\n        finally:\n            os.remove(fn_host)\n\n        logger.debug(\"wrote to file [%s] inside container [%s]\",\n                     filepath, container.id)", "response": "Writes the contents of a given file to a given container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _url(self, path: str) -> str:\n        url = \"{}/{}\".format(self.__base_url, path)\n        logger.debug(\"transformed path [%s] into url: %s\", path, url)\n        return url", "response": "Computes the URL for a resource located at a given path on the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle an erroneous response from an ancillary zoo API.", "response": "def handle_erroneous_response(self,\n                                  response: requests.Response\n                                  ) -> NoReturn:\n        \"\"\"\n        Attempts to decode an erroneous response into an exception, and to\n        subsequently throw that exception.\n\n        Raises:\n            BugZooException: the exception described by the error response.\n            UnexpectedResponse: if the response cannot be decoded to an\n                exception.\n        \"\"\"\n        logger.debug(\"handling erroneous response: %s\", response)\n        try:\n            err = BugZooException.from_dict(response.json())\n        except Exception:\n            err = UnexpectedResponse(response)\n        raise err"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining whether the server has a Docker image with a given name.", "response": "def has_image(self, name: str) -> bool:\n        \"\"\"\n        Determines whether the server has a Docker image with a given name.\n        \"\"\"\n        path = \"docker/images/{}\".format(name)\n        r = self.__api.head(path)\n        if r.status_code == 204:\n            return True\n        elif r.status_code == 404:\n            return False\n        self.__api.handle_erroneous_response(r)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_image(self, name: str) -> None:\n        logger.debug(\"deleting Docker image: %s\", name)\n        path = \"docker/images/{}\".format(name)\n        response = self.__api.delete(path)\n        if response.status_code != 204:\n            try:\n                self.__api.handle_erroneous_response(response)\n            except Exception:\n                logger.exception(\"failed to delete Docker image: %s\", name)\n                raise\n        else:\n            logger.info(\"deleted Docker image: %s\", name)", "response": "Deletes a Docker image with a given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresolves the given path to an absolute path.", "response": "def resolve(self, container: Container, fn: str) -> str:\n        \"\"\"\n        Ensures that relative paths are transformed into absolute paths.\n        \"\"\"\n        bug = self.__mgr_bug[container.bug]\n        fn_orig = fn\n\n        if not os.path.isabs(fn):\n            fn = os.path.join(bug.source_dir, fn)\n            logger.debug(\"converted relative path to absolute path: %s -> %s\",\n                         fn_orig, fn)\n\n        return fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the base path for a given file.", "response": "def _file_path(self, container: Container, fn: str) -> str:\n        \"\"\"\n        Computes the base path for a given file.\n        \"\"\"\n        fn = self.resolve(container, fn)\n        assert fn[0] == '/'\n        fn = fn[1:]\n        path = \"files/{}/{}\".format(container.uid, fn)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the contents of a given string into a file at a specified location inside a container.", "response": "def write(self,\n              container: Container,\n              filepath: str,\n              contents: str\n              ) -> None:\n        \"\"\"\n        Dumps the contents of a given string into a file at a specified\n        location inside the container.\n\n        Parameters:\n            container: the container to which the file should be written.\n            filepath: the path to the file inside the container. If a\n                relative path is given, the path will be interpreted as\n                being relative to the source directory for the program\n                under test inside the container.\n            contents: the contents that should be written to the specified\n                file.\n        \"\"\"\n        logger.debug(\"writing to file [%s] in container [%s].\",\n                     filepath, container.uid)\n        path = self._file_path(container, filepath)\n        try:\n            response = self.__api.put(path, data=contents)\n            if response.status_code != 204:\n                self.__api.handle_erroneous_response(response)\n        except BugZooException:\n            logger.exception(\"failed to write to file [%s] in container [%s].\",\n                             filepath, container.uid)\n            raise\n        logger.debug(\"wrote to file [%s] in container [%s].\",\n                     filepath, container.uid)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the contents of a given file in a running cluster.", "response": "def read(self, container: Container, filepath: str) -> str:\n        \"\"\"\n        Attempts to retrieve the contents of a given file in a running\n        container.\n\n        Parameters:\n            container: the container from which the file should be fetched.\n            filepath: the path to the file. If a relative path is given,\n                the path will be interpreted as being relative to the source\n                directory for the program under test inside the container.\n\n        Returns:\n            the contents of the file as a string.\n\n        Raises:\n            KeyError: if the given file was not found.\n        \"\"\"\n        logger.debug(\"reading contents of file [%s] in container [%s].\",\n                     filepath, container.uid)\n        path = self._file_path(container, filepath)\n        response = self.__api.get(path)\n        if response.status_code == 200:\n            contents = response.text\n            logger.debug(\"fetched contents of file [%s] in container [%s]:%s\",\n                         filepath, container.uid,\n                         indent(\"\\n[CONTENTS]\\n{}\\n[/CONTENTS]\".format(contents), 2))  # noqa: pycodestyle\n            return contents\n        try:\n            self.__api.handle_erroneous_response(response)\n        except BugZooException as err:\n            logger.exception(\"failed to read contents of file [%s] in container [%s]: %s\",  # noqa: pycodestyle\n                             filepath, container.uid, err)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef style_similarity(page1, page2):\n    classes_page1 = get_classes(page1)\n    classes_page2 = get_classes(page2)\n    return jaccard_similarity(classes_page1, classes_page2)", "response": "Computes CSS style similarity between two DOM trees."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new Spectra that only contains entries for the given file names.", "response": "def restricted_to_files(self,\n                            filenames: List[str]\n                            ) -> 'Spectra':\n        \"\"\"\n        Returns a variant of this spectra that only contains entries for\n        lines that appear in any of the files whose name appear in the\n        given list.\n        \"\"\"\n        tally_passing = \\\n            {fn: entries for (fn, entries) in self.__tally_passing.items() \\\n             if fn in filenames}\n        tally_failing = \\\n            {fn: entries for (fn, entries) in self.__tally_failing.items() \\\n             if fn in filenames}\n        return Spectra(self.__num_passing,\n                       self.__num_failing,\n                       tally_passing,\n                       tally_failing)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter(self,\n               predicate: Callable[[FileLine], 'FileLineSet']\n               ) -> 'FileLineSet':\n        \"\"\"\n        Returns a subset of the file lines within this set that satisfy a given\n        filtering criterion.\n        \"\"\"\n        filtered = [fileline for fileline in self if predicate(fileline)]\n        return FileLineSet.from_list(filtered)", "response": "Returns a subset of the file lines within this set that satisfy a given predicate."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of file lines that contains the union of the lines within this set and a given set.", "response": "def union(self, other: 'FileLineSet') -> 'FileLineSet':\n        \"\"\"\n        Returns a set of file lines that contains the union of the lines within\n        this set and a given set.\n        \"\"\"\n        # this isn't the most efficient implementation, but frankly, it doesn't\n        # need to be.\n        assert isinstance(other, FileLineSet)\n        l_self = list(self)\n        l_other = list(other)\n        l_union = l_self + l_other\n        return FileLineSet.from_list(l_union)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intersection(self, other: 'FileLineSet') -> 'FileLineSet':\n        assert isinstance(other, FileLineSet)\n        set_self = set(self)\n        set_other = set(other)\n        set_union = set_self & set_other\n        return FileLineSet.from_list(list(set_union))", "response": "Returns a set of file lines that contains the intersection of the lines\n        within this set and a given set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restricted_to_files(self, filenames: List[str]) -> 'FileLineSet':\n        restricted = {} # type: Dict[str, Set[int]]\n        for fn in filenames:\n            if fn in self.__contents:\n                restricted[fn] = set(self.__contents[fn])\n        return FileLineSet(restricted)", "response": "Returns a set that only contains lines that occur in\n        any one of the given files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_dict(self) -> Dict[str, List[int]]:\n        return {fn: list(lines)\n                for (fn, lines) in self.__contents.items()}", "response": "Returns the contents of this set as a JSON - ready dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprovisioning a mountable Docker container for a given tool.", "response": "def provision(self, tool: Tool) -> docker.models.containers.Container:\n        \"\"\"\n        Provisions a mountable Docker container for a given tool.\n        \"\"\"\n        if not self.is_installed(tool):\n            raise Exception(\"tool is not installed: {}\".format(tool.name))\n\n        client = self.__installation.docker\n        return client.containers.create(tool.image)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_installed(self, tool: Tool) -> bool:\n        return self.__installation.build.is_installed(tool.image)", "response": "Determines whether or not the Docker image for a given tool has been installed onto this server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build(self,\n              tool: Tool,\n              force: bool = False,\n              quiet: bool = False\n              ) -> None:\n        \"\"\"\n        Builds the Docker image associated with a given tool.\n\n        See: `BuildManager.build`\n        \"\"\"\n        self.__installation.build.build(tool.image,\n                                        force=force,\n                                        quiet=quiet)", "response": "Builds the Docker image associated with a given tool."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download(self,\n                 tool: Tool,\n                 force=False) -> bool:\n        \"\"\"\n        Attempts to download the Docker image for a given tool from\n        `DockerHub <https://hub.docker.com>`_. If the force parameter is set to\n        True, any existing image will be overwritten.\n\n        Returns:\n            `True` if successfully downloaded, else `False`.\n        \"\"\"\n        return self.__installation.build.download(tool.image,\n                                                  force=force)", "response": "Downloads the Docker image for a given tool from the DockerHub."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upload(self, tool: Tool) -> bool:\n        return self.__installation.build.upload(tool.image)", "response": "Uploads the Docker image for a given tool."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_dict(self) -> Dict[str, str]:\n        return {\n            'type': 'remote',\n            'name': self.name,\n            'location': self.location,\n            'url': self.url,\n            'version': self.version\n        }", "response": "Produces a dictionary - based description of this source."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_installed(self, name: str) -> bool:\n        assert name is not None\n        try:\n            self.__docker.images.get(name)\n            return True\n        except docker.errors.ImageNotFound:\n            return False", "response": "Determines if a given Docker image is installed on this server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a Docker image given by its name using the set of build instructions associated with that image.", "response": "def build(self,\n              name: str,\n              force: bool = False,\n              quiet: bool = False\n              ) -> None:\n        \"\"\"\n        Constructs a Docker image, given by its name, using the set of build\n        instructions associated with that image.\n\n        Parameters:\n            name: the name of the Docker image.\n            force: if `True`, the image will be rebuilt, regardless of whether\n                or not it is already installed on the server. If `False` and\n                a (possibly outdated) version of the image has already been\n                built, then the build will be skipped.\n            quiet: used to enable and disable output from the Docker build\n                process.\n        \"\"\"\n        logger.debug(\"request to build image: %s\", name)\n        instructions = self[name]\n\n        if instructions.depends_on:\n            logger.info(\"building dependent image: %s\",\n                        instructions.depends_on)\n            self.build(instructions.depends_on, force=force, quiet=quiet)\n\n        if not force and self.is_installed(instructions.name):\n            return\n\n        if not quiet:\n            logger.info(\"building image: %s\", name)\n\n        context = instructions.abs_context\n        tf = os.path.join(context, '.Dockerfile')\n        try:\n            success = False\n            shutil.copy(instructions.filename_abs, tf)\n            response = self.__docker.api.build(path=context,\n                                               dockerfile='.Dockerfile',\n                                               tag=name,\n                                               # pull=force,\n                                               buildargs=instructions.arguments,\n                                               decode=True,\n                                               rm=True)\n\n            log = [] # type: List[str]\n            for line in response:\n                if 'stream' in line:\n                    line_msg = line['stream'].rstrip()\n                    log.append(line_msg)\n                    if not quiet:\n                        print(line_msg)\n                    if line_msg.startswith('Successfully built'):\n                        success = True\n\n            if not success:\n                raise ImageBuildFailed(name, log)\n\n            if success and not quiet:\n                logger.info(\"built image: %s\", name)\n                return\n        finally:\n            if os.path.exists(tf):\n                os.remove(tf)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uninstall(self,\n                  name: str,\n                  force: bool = False,\n                  noprune: bool = False\n                  ) -> None:\n        \"\"\"\n        Attempts to uninstall a given Docker image.\n\n        Parameters:\n            name: the name of the Docker image.\n            force: a flag indicating whether or not an exception should be\n                thrown if the image associated with the given build\n                instructions is not installed. If `True`, no exception\n                will be thrown; if `False`, exception will be thrown.\n            noprune: a flag indicating whether or not dangling image layers\n                should also be removed.\n\n        Raises:\n            docker.errors.ImageNotFound: if the image associated with the given\n                instructions can't be found.\n        \"\"\"\n        try:\n            self.__docker.images.remove(image=name,\n                                        force=force,\n                                        noprune=noprune)\n        except docker.errors.ImageNotFound as e:\n            if force:\n                return\n            raise e", "response": "Removes a given Docker image from the registry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download(self,\n                 name: str,\n                 force: bool = False\n                 ) -> bool:\n        \"\"\"\n        Attempts to download a given Docker image. If `force=True`, then any\n        previously installed version of the image (described by the\n        instructions) will be replaced by the image on DockerHub.\n\n        Parameters:\n            name: the name of the Docker image.\n\n        Returns:\n            `True` if successfully downloaded, otherwise `False`.\n        \"\"\"\n        try:\n            self.__docker.images.pull(name)\n            return True\n        except docker.errors.NotFound:\n            print(\"Failed to locate image on DockerHub: {}\".format(name))\n            return False", "response": "Downloads a given Docker image."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupload a given Docker image to DockerHub.", "response": "def upload(self, name: str) -> bool:\n        \"\"\"\n        Attempts to upload a given Docker image from this server to DockerHub.\n\n        Parameters:\n            name: the name of the Docker image.\n\n        Returns:\n            `True` if successfully uploaded, otherwise `False`.\n        \"\"\"\n        try:\n            out = self.__docker.images.push(name, stream=True)\n            for line in out:\n                line = line.strip().decode('utf-8')\n                jsn = json.loads(line)\n                if 'progress' in jsn:\n                    line = \"{}. {}.\".format(jsn['status'], jsn['progress'])\n                    print(line, end='\\r')\n                elif 'status' in jsn:\n                    print(jsn['status'])\n            print('uploaded image to DockerHub: {}'.format(name))\n            return True\n        except docker.errors.NotFound:\n            print(\"Failed to push image ({}): not installed.\".format(name))\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef printflush(s: str, end: str = '\\n') -> None:\n    print(s, end=end)\n    sys.stdout.flush()", "response": "Prints a given string to the standard output immediately flushes the standard output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a set of coverage instructions from a given dictionary.", "response": "def from_dict(d: Dict[str, Any]) -> 'CoverageInstructions':\n        \"\"\"\n        Loads a set of coverage instructions from a given dictionary.\n\n        Raises:\n            BadCoverageInstructions: if the given coverage instructions are\n                illegal.\n        \"\"\"\n        name_type = d['type']\n        cls = _NAME_TO_INSTRUCTIONS[name_type]\n        return cls.from_dict(d)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging into a MyGeotab server and stores the returned credentials.", "response": "def login(session, user, password, database=None, server=None):\n    \"\"\"Logs into a MyGeotab server and stores the returned credentials.\n\n    :param session: The current Session object.\n    :param user: The username used for MyGeotab servers. Usually an email address.\n    :param password: The password associated with the username. Optional if `session_id` is provided.\n    :param database: The database or company name. Optional as this usually gets resolved upon authentication.\n    :param server: The server ie. my23.geotab.com. Optional as this usually gets resolved upon authentication.\n    \"\"\"\n    if not user:\n        user = click.prompt(\"Username\", type=str)\n    if not password:\n        password = click.prompt(\"Password\", hide_input=True, type=str)\n    try:\n        with click.progressbar(length=1, label=\"Logging in...\") as progressbar:\n            session.login(user, password, database, server)\n            progressbar.update(1)\n        if session.credentials:\n            click.echo('Logged in as: %s' % session.credentials)\n            session.load(database)\n        return session.get_api()\n    except mygeotab.AuthenticationException:\n        click.echo('Incorrect credentials. Please try again.')\n        sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sessions(session):\n    active_sessions = session.get_sessions()\n    if not active_sessions:\n        click.echo('(No active sessions)')\n        return\n    for active_session in active_sessions:\n        click.echo(active_session)", "response": "Shows the current logged in sessions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(*tasks: Awaitable, loop: asyncio.AbstractEventLoop=asyncio.get_event_loop()):\n    futures = [asyncio.ensure_future(task, loop=loop) for task in tasks]\n    return loop.run_until_complete(asyncio.gather(*futures))", "response": "Helper to run tasks in the event loop."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def server_call_async(method, server, loop: asyncio.AbstractEventLoop=asyncio.get_event_loop(), timeout=DEFAULT_TIMEOUT,\n                      verify_ssl=True, **parameters):\n    \"\"\"Makes an asynchronous call to an un-authenticated method on a server.\n\n    :param method: The method name.\n    :param server: The MyGeotab server.\n    :param loop: The event loop.\n    :param timeout: The timeout to make the call, in seconds. By default, this is 300 seconds (or 5 minutes).\n    :param verify_ssl: If True, verify the SSL certificate. It's recommended not to modify this.\n    :param parameters: Additional parameters to send (for example, search=dict(id='b123') ).\n    :return: The JSON result (decoded into a dict) from the server.\n    :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n    :raise TimeoutException: Raises when the request does not respond after some time.\n    \"\"\"\n    if method is None:\n        raise Exception(\"A method name must be specified\")\n    if server is None:\n        raise Exception(\"A server (eg. my3.geotab.com) must be specified\")\n    parameters = api.process_parameters(parameters)\n    return await _query(server, method, parameters, timeout=timeout, verify_ssl=verify_ssl, loop=loop)", "response": "Makes an asynchronous call to a method on a MyGeotab server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats and performs the asynchronous query against the API.", "response": "async def _query(server, method, parameters, timeout=DEFAULT_TIMEOUT, verify_ssl=True,\n                 loop: asyncio.AbstractEventLoop=None):\n    \"\"\"Formats and performs the asynchronous query against the API\n\n    :param server: The server to query.\n    :param method: The method name.\n    :param parameters: A dict of parameters to send\n    :param timeout: The timeout to make the call, in seconds. By default, this is 300 seconds (or 5 minutes).\n    :param verify_ssl: Whether or not to verify SSL connections\n    :return: The JSON-decoded result from the server\n    :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server\n    :raise TimeoutException: Raises when the request does not respond after some time.\n    :raise aiohttp.ClientResponseError: Raises when there is an HTTP status code that indicates failure.\n    \"\"\"\n    api_endpoint = api.get_api_url(server)\n    params = dict(id=-1, method=method, params=parameters)\n    headers = get_headers()\n    ssl_context = None\n    verify = verify_ssl\n    if verify_ssl:\n        ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)\n    conn = aiohttp.TCPConnector(verify_ssl=verify, ssl_context=ssl_context, loop=loop)\n    try:\n        async with aiohttp.ClientSession(connector=conn, loop=loop) as session:\n            response = await session.post(api_endpoint,\n                                          data=json.dumps(params,\n                                                          default=object_serializer),\n                                          headers=headers,\n                                          timeout=timeout,\n                                          allow_redirects=True)\n            response.raise_for_status()\n            content_type = response.headers.get('Content-Type')\n            body = await response.text()\n    except TimeoutError:\n        raise TimeoutException(server)\n    if content_type and 'application/json' not in content_type.lower():\n        return body\n    return api._process(json.loads(body, object_hook=object_deserializer))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking an async call to the API.", "response": "async def call_async(self, method, **parameters):\n        \"\"\"Makes an async call to the API.\n\n        :param method: The method name.\n        :param params: Additional parameters to send (for example, search=dict(id='b123') )\n        :return: The JSON result (decoded into a dict) from the server.abs\n        :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n        :raise TimeoutException: Raises when the request does not respond after some time.\n        \"\"\"\n        if method is None:\n            raise Exception('A method name must be specified')\n        params = api.process_parameters(parameters)\n        if self.credentials and not self.credentials.session_id:\n            self.authenticate()\n        if 'credentials' not in params and self.credentials.session_id:\n            params['credentials'] = self.credentials.get_param()\n\n        try:\n            result = await _query(self._server, method, params,\n                                  verify_ssl=self._is_verify_ssl, loop=self.loop)\n            if result is not None:\n                self.__reauthorize_count = 0\n            return result\n        except MyGeotabException as exception:\n            if exception.name == 'InvalidUserException':\n                if self.__reauthorize_count == 0 and self.credentials.password:\n                    self.__reauthorize_count += 1\n                    self.authenticate()\n                    return await self.call_async(method, **parameters)\n                else:\n                    raise AuthenticationException(self.credentials.username,\n                                                  self.credentials.database,\n                                                  self.credentials.server)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def multi_call_async(self, calls):\n        formatted_calls = [dict(method=call[0], params=call[1] if len(call) > 1 else {}) for call in calls]\n        return await self.call_async('ExecuteMultiCall', calls=formatted_calls)", "response": "Performs an async multi - call to the API\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting entities asynchronously using the API.", "response": "async def get_async(self, type_name, **parameters):\n        \"\"\"Gets entities asynchronously using the API. Shortcut for using async_call() with the 'Get' method.\n\n        :param type_name: The type of entity.\n        :param parameters: Additional parameters to send.\n        :return: The JSON result (decoded into a dict) from the server.\n        :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n        :raise TimeoutException: Raises when the request does not respond after some time.\n        \"\"\"\n        if parameters:\n            results_limit = parameters.get('resultsLimit', None)\n            if results_limit is not None:\n                del parameters['resultsLimit']\n            if 'search' in parameters:\n                parameters.update(parameters['search'])\n            parameters = dict(search=parameters, resultsLimit=results_limit)\n        return await self.call_async('Get', type_name=type_name, **parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def add_async(self, type_name, entity):\n        return await self.call_async('Add', type_name=type_name, entity=entity)", "response": "Add an entity asynchronously using the API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting an entity asynchronously using the API.", "response": "async def set_async(self, type_name, entity):\n        \"\"\"Sets an entity asynchronously using the API. Shortcut for using async_call() with the 'Set' method.\n\n        :param type_name: The type of entity\n        :param entity: The entity to set\n        :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server\n        \"\"\"\n        return await self.call_async('Set', type_name=type_name, entity=entity)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves an entity asynchronously using the API.", "response": "async def remove_async(self, type_name, entity):\n        \"\"\"Removes an entity asynchronously using the API. Shortcut for using async_call() with the 'Remove' method.\n\n        :param type_name: The type of entity.\n        :param entity: The entity to remove.\n        :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n        :raise TimeoutException: Raises when the request does not respond after some time.\n        \"\"\"\n        return await self.call_async('Remove', type_name=type_name, entity=entity)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_credentials(credentials, loop: asyncio.AbstractEventLoop=asyncio.get_event_loop()):\n        return API(username=credentials.username, password=credentials.password,\n                   database=credentials.database, session_id=credentials.session_id,\n                   server=credentials.server, loop=loop)", "response": "Returns a new API object populated with MyGeotab credentials."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_iso_datetime(datetime_obj):\n    datetime_obj = localize_datetime(datetime_obj, pytz.utc)\n    if datetime_obj < MIN_DATE:\n        datetime_obj = MIN_DATE\n    elif datetime_obj > MAX_DATE:\n        datetime_obj = MAX_DATE\n    return datetime_obj.replace(tzinfo=None).strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'", "response": "Formats the given datetime object as a UTC - zoned ISO 8601 date string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef localize_datetime(datetime_obj, tz=pytz.utc):\n    if not datetime_obj.tzinfo:\n        return tz.localize(datetime_obj)\n    else:\n        try:\n            return datetime_obj.astimezone(tz)\n        except OverflowError:\n            if datetime_obj < datetime(2, 1, 1, tzinfo=pytz.utc):\n                return MIN_DATE.astimezone(tz)\n            return MAX_DATE.astimezone(tz)", "response": "Converts a naive or UTC - localized datetime into the provided timezone."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run(self):\n        while self.running:\n            try:\n                result = self.client_api.call('GetFeed', type_name=self.type_name, search=self.search,\n                                              from_version=self._version, results_limit=self.results_limit)\n                self._version = result['toVersion']\n                self.listener.on_data(result['data'])\n            except api.MyGeotabException as exception:\n                if self.listener.on_error(exception) is False:\n                    break\n            if not self.running:\n                break\n            sleep(self.interval)\n        self.running = False", "response": "Runs the Data Feed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the data feed.", "response": "def start(self, threaded=True):\n        \"\"\"Start the data feed.\n\n        :param threaded: If True, run in a separate thread.\n        \"\"\"\n        self.running = True\n        if threaded:\n            self._thread = Thread(target=self._run)\n            self._thread.start()\n        else:\n            self._run()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _query(server, method, parameters, timeout=DEFAULT_TIMEOUT, verify_ssl=True):\n    api_endpoint = get_api_url(server)\n    params = dict(id=-1, method=method, params=parameters or {})\n    headers = get_headers()\n    with requests.Session() as session:\n        session.mount('https://', GeotabHTTPAdapter())\n        try:\n            response = session.post(api_endpoint,\n                                    data=json.dumps(params,\n                                                    default=object_serializer),\n                                    headers=headers, allow_redirects=True, timeout=timeout,\n                                    verify=verify_ssl)\n        except Timeout:\n            raise TimeoutException(server)\n    response.raise_for_status()\n    content_type = response.headers.get('Content-Type')\n    if content_type and 'application/json' not in content_type.lower():\n        return response.text\n    return _process(response.json(object_hook=object_deserializer))", "response": "Formats and performs the query against the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a call to an un - authenticated method on a server.", "response": "def server_call(method, server, timeout=DEFAULT_TIMEOUT, verify_ssl=True, **parameters):\n    \"\"\"Makes a call to an un-authenticated method on a server\n\n    :param method: The method name.\n    :type method: str\n    :param server: The MyGeotab server.\n    :type server: str\n    :param timeout: The timeout to make the call, in seconds. By default, this is 300 seconds (or 5 minutes).\n    :type timeout: float\n    :param verify_ssl: If True, verify the SSL certificate. It's recommended not to modify this.\n    :type verify_ssl: bool\n    :param parameters: Additional parameters to send (for example, search=dict(id='b123') ).\n    :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n    :raise TimeoutException: Raises when the request does not respond after some time.\n    :return: The result from the server.\n    \"\"\"\n    if method is None:\n        raise Exception(\"A method name must be specified\")\n    if server is None:\n        raise Exception(\"A server (eg. my3.geotab.com) must be specified\")\n    parameters = process_parameters(parameters)\n    return _query(server, method, parameters, timeout=timeout, verify_ssl=verify_ssl)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_parameters(parameters):\n    if not parameters:\n        return {}\n    params = copy.copy(parameters)\n    for param_name in parameters:\n        value = parameters[param_name]\n        server_param_name = re.sub(r'_(\\w)', lambda m: m.group(1).upper(), param_name)\n        if isinstance(value, dict):\n            value = process_parameters(value)\n        params[server_param_name] = value\n        if server_param_name != param_name:\n            del params[param_name]\n    return params", "response": "Allows the use of Pythonic - style parameters with underscores instead of camel - case."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_api_url(server):\n    parsed = urlparse(server)\n    base_url = parsed.netloc if parsed.netloc else parsed.path\n    base_url.replace('/', '')\n    return 'https://' + base_url + '/apiv1'", "response": "Formats the server URL properly in order to query the API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef multi_call(self, calls):\n        formatted_calls = [dict(method=call[0], params=call[1] if len(call) > 1 else {}) for call in calls]\n        return self.call('ExecuteMultiCall', calls=formatted_calls)", "response": "Performs a multi - call to the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, type_name, **parameters):\n        if parameters:\n            results_limit = parameters.get('resultsLimit', None)\n            if results_limit is not None:\n                del parameters['resultsLimit']\n            if 'search' in parameters:\n                parameters.update(parameters['search'])\n            parameters = dict(search=parameters, resultsLimit=results_limit)\n        return self.call('Get', type_name=type_name, **parameters)", "response": "Gets entities using the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, type_name, entity):\n        return self.call('Add', type_name=type_name, entity=entity)", "response": "Adds an entity using the API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting an entity using the API.", "response": "def set(self, type_name, entity):\n        \"\"\"Sets an entity using the API. Shortcut for using call() with the 'Set' method.\n\n        :param type_name: The type of entity.\n        :type type_name: str\n        :param entity: The entity to set.\n        :type entity: dict\n        :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n        :raise TimeoutException: Raises when the request does not respond after some time.\n        \"\"\"\n        return self.call('Set', type_name=type_name, entity=entity)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving an entity from the MyGeotab server.", "response": "def remove(self, type_name, entity):\n        \"\"\"Removes an entity using the API. Shortcut for using call() with the 'Remove' method.\n\n        :param type_name: The type of entity.\n        :type type_name: str\n        :param entity: The entity to remove.\n        :type entity: dict\n        :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n        :raise TimeoutException: Raises when the request does not respond after some time.\n        \"\"\"\n        return self.call('Remove', type_name=type_name, entity=entity)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nauthenticate against the API server.", "response": "def authenticate(self, is_global=True):\n        \"\"\"Authenticates against the API server.\n\n        :param is_global: If True, authenticate globally. Local login if False.\n        :raise AuthenticationException: Raises if there was an issue with authenticating or logging in.\n        :raise MyGeotabException: Raises when an exception occurs on the MyGeotab server.\n        :raise TimeoutException: Raises when the request does not respond after some time.\n        :return: A Credentials object with a session ID created by the server.\n        :rtype: Credentials\n        \"\"\"\n        auth_data = dict(database=self.credentials.database, userName=self.credentials.username,\n                         password=self.credentials.password)\n        auth_data['global'] = is_global\n        try:\n            result = _query(self._server, 'Authenticate', auth_data, self.timeout,\n                            verify_ssl=self._is_verify_ssl)\n            if result:\n                new_server = result['path']\n                server = self.credentials.server\n                if new_server != 'ThisServer':\n                    server = new_server\n                credentials = result['credentials']\n                self.credentials = Credentials(credentials['userName'], credentials['sessionId'],\n                                               credentials['database'], server)\n                return self.credentials\n        except MyGeotabException as exception:\n            if exception.name == 'InvalidUserException':\n                raise AuthenticationException(self.credentials.username,\n                                              self.credentials.database,\n                                              self.credentials.server)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_credentials(credentials):\n        return API(username=credentials.username, password=credentials.password,\n                   database=credentials.database, session_id=credentials.session_id,\n                   server=credentials.server)", "response": "Returns a new API object populated with MyGeotab credentials."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_param(self):\n        return dict(userName=self.username, sessionId=self.session_id, database=self.database)", "response": "A simple representation of the credentials object for passing into the API. authenticate method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates a sub - entity with the data from the API.", "response": "def _populate_sub_entity(self, entity, type_name):\n        \"\"\"\n        Simple API-backed cache for populating MyGeotab entities\n\n        :param entity: The entity to populate a sub-entity for\n        :param type_name: The type of the sub-entity to populate\n        \"\"\"\n        key = type_name.lower()\n        if isinstance(entity[key], str):\n            # If the expected sub-entity is a string, it's a unknown ID\n            entity[key] = dict(id=entity[key])\n            return\n        cache = self._cache[key]\n        subentity = cache.get(entity[key]['id'])\n        if not subentity:\n            subentities = self.api.get(type_name, id=entity[key]['id'], results_limit=1)\n            if len(subentities) > 0:\n                subentity = subentities[0]\n                entity[key] = subentity\n        else:\n            entity[key] = subentity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_data(self, data):\n        for d in data:\n            self._populate_sub_entity(d, 'Device')\n            self._populate_sub_entity(d, 'Rule')\n            date = dates.localize_datetime(d['activeFrom'])\n            click.echo(\n                '[{date}] {device} ({rule})'.format(date=date, device=d['device'].get('name', '**Unknown Vehicle'),\n                                                    rule=d['rule'].get('name', '**Unknown Rule')))", "response": "The function called when new data has arrived."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _should_allocate_port(pid):\n    if pid <= 0:\n        log.info('Not allocating a port to invalid pid')\n        return False\n    if pid == 1:\n        # The client probably meant to send us its parent pid but\n        # had been reparented to init.\n        log.info('Not allocating a port to init.')\n        return False\n    try:\n        os.kill(pid, 0)\n    except ProcessLookupError:\n        log.info('Not allocating a port to a non-existent process')\n        return False\n    return True", "response": "Determine if we should allocate a port for use by the given process id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_command_line():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--portserver_static_pool',\n        type=str,\n        default='15000-24999',\n        help='Comma separated N-P Range(s) of ports to manage (inclusive).')\n    parser.add_argument(\n        '--portserver_unix_socket_address',\n        type=str,\n        default='@unittest-portserver',\n        help='Address of AF_UNIX socket on which to listen (first @ is a NUL).')\n    parser.add_argument('--verbose',\n                        action='store_true',\n                        default=False,\n                        help='Enable verbose messages.')\n    parser.add_argument('--debug',\n                        action='store_true',\n                        default=False,\n                        help='Enable full debug messages.')\n    return parser.parse_args(sys.argv[1:])", "response": "Configure and parse our command line flags."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a N - P X - Y description of port ranges return a set of ints.", "response": "def _parse_port_ranges(pool_str):\n    \"\"\"Given a 'N-P,X-Y' description of port ranges, return a set of ints.\"\"\"\n    ports = set()\n    for range_str in pool_str.split(','):\n        try:\n            a, b = range_str.split('-', 1)\n            start, end = int(a), int(b)\n        except ValueError:\n            log.error('Ignoring unparsable port range %r.', range_str)\n            continue\n        if start < 1 or end > 65535:\n            log.error('Ignoring out of bounds port range %r.', range_str)\n            continue\n        ports.update(set(range(start, end + 1)))\n    return ports"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring the log global message format and verbosity settings.", "response": "def _configure_logging(verbose=False, debug=False):\n    \"\"\"Configure the log global, message format, and verbosity settings.\"\"\"\n    overall_level = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(\n        format=('{levelname[0]}{asctime}.{msecs:03.0f} {thread} '\n                '{filename}:{lineno}] {message}'),\n        datefmt='%m%d %H:%M:%S',\n        style='{',\n        level=overall_level)\n    global log\n    log = logging.getLogger('portserver')\n    # The verbosity controls our loggers logging level, not the global\n    # one above. This avoids debug messages from libraries such as asyncio.\n    log.setLevel(logging.DEBUG if verbose else overall_level)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nallocate and returns a free port for a process.", "response": "def get_port_for_process(self, pid):\n        \"\"\"Allocates and returns port for pid or 0 if none could be allocated.\"\"\"\n        if not self._port_queue:\n            raise RuntimeError('No ports being managed.')\n\n        # Avoid an infinite loop if all ports are currently assigned.\n        check_count = 0\n        max_ports_to_test = len(self._port_queue)\n        while check_count < max_ports_to_test:\n            # Get the next candidate port and move it to the back of the queue.\n            candidate = self._port_queue.pop()\n            self._port_queue.appendleft(candidate)\n            check_count += 1\n            if (candidate.start_time == 0 or\n                candidate.start_time != _get_process_start_time(candidate.pid)):\n                if _is_port_free(candidate.port):\n                    candidate.pid = pid\n                    candidate.start_time = _get_process_start_time(pid)\n                    if not candidate.start_time:\n                        log.info(\"Can't read start time for pid %d.\", pid)\n                    self.ports_checked_for_last_request = check_count\n                    return candidate.port\n                else:\n                    log.info(\n                        'Port %d unexpectedly in use, last owning pid %d.',\n                        candidate.port, candidate.pid)\n\n        log.info('All ports in use.')\n        self.ports_checked_for_last_request = check_count\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new port to the free pool for allocation.", "response": "def add_port_to_free_pool(self, port):\n        \"\"\"Add a new port to the free pool for allocation.\"\"\"\n        if port < 1 or port > 65535:\n            raise ValueError(\n                'Port must be in the [1, 65535] range, not %d.' % port)\n        port_info = _PortInfo(port=port)\n        self._port_queue.append(port_info)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the request body and respond appropriately.", "response": "def _handle_port_request(self, client_data, writer):\n        \"\"\"Given a port request body, parse it and respond appropriately.\n\n        Args:\n          client_data: The request bytes from the client.\n          writer: The asyncio Writer for the response to be written to.\n        \"\"\"\n        try:\n            pid = int(client_data)\n        except ValueError as error:\n            self._client_request_errors += 1\n            log.warning('Could not parse request: %s', error)\n            return\n\n        log.info('Request on behalf of pid %d.', pid)\n        log.info('cmdline: %s', _get_process_command_line(pid))\n\n        if not _should_allocate_port(pid):\n            self._denied_allocations += 1\n            return\n\n        port = self._port_pool.get_port_for_process(pid)\n        if port > 0:\n            self._total_allocations += 1\n            writer.write('{:d}\\n'.format(port).encode('utf-8'))\n            log.debug('Allocated port %d to pid %d', port, pid)\n        else:\n            self._denied_allocations += 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog statistics of our operation.", "response": "def dump_stats(self):\n        \"\"\"Logs statistics of our operation.\"\"\"\n        log.info('Dumping statistics:')\n        stats = []\n        stats.append(\n            'client-request-errors {}'.format(self._client_request_errors))\n        stats.append('denied-allocations {}'.format(self._denied_allocations))\n        stats.append('num-ports-managed {}'.format(self._port_pool.num_ports()))\n        stats.append('num-ports-checked-for-last-request {}'.format(\n            self._port_pool.ports_checked_for_last_request))\n        stats.append('total-allocations {}'.format(self._total_allocations))\n        for stat in stats:\n            log.info(stat)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a port that is no longer being used so it can be reused.", "response": "def return_port(port):\n    \"\"\"Return a port that is no longer being used so it can be reused.\"\"\"\n    if port in _random_ports:\n        _random_ports.remove(port)\n    elif port in _owned_ports:\n        _owned_ports.remove(port)\n        _free_ports.add(port)\n    elif port in _free_ports:\n        logging.info(\"Returning a port that was already returned: %s\", port)\n    else:\n        logging.info(\"Returning a port that wasn't given by portpicker: %s\",\n                     port)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bind(port, socket_type, socket_proto):\n    got_socket = False\n    for family in (socket.AF_INET6, socket.AF_INET):\n        try:\n            sock = socket.socket(family, socket_type, socket_proto)\n            got_socket = True\n        except socket.error:\n            continue\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            sock.bind(('', port))\n            if socket_type == socket.SOCK_STREAM:\n                sock.listen(1)\n            port = sock.getsockname()[1]\n        except socket.error:\n            return None\n        finally:\n            sock.close()\n    return port if got_socket else None", "response": "Try to bind to a socket of the specified type protocol and port."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pick_unused_port(pid=None, portserver_address=None):\n    try:  # Instead of `if _free_ports:` to handle the race condition.\n        port = _free_ports.pop()\n    except KeyError:\n        pass\n    else:\n        _owned_ports.add(port)\n        return port\n    # Provide access to the portserver on an opt-in basis.\n    if portserver_address:\n        port = get_port_from_port_server(portserver_address, pid=pid)\n        if port:\n            return port\n    if 'PORTSERVER_ADDRESS' in os.environ:\n        port = get_port_from_port_server(os.environ['PORTSERVER_ADDRESS'],\n                                         pid=pid)\n        if port:\n            return port\n    return _pick_unused_port_without_server()", "response": "A pure python implementation of PickUnusedPort."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npicks an unused network port without the help of a port server.", "response": "def _pick_unused_port_without_server():  # Protected. pylint: disable=invalid-name\n    \"\"\"Pick an available network port without the help of a port server.\n\n    This code ensures that the port is available on both TCP and UDP.\n\n    This function is an implementation detail of PickUnusedPort(), and\n    should not be called by code outside of this module.\n\n    Returns:\n      A port number that is unused on both TCP and UDP.\n\n    Raises:\n      NoFreePortFoundError: No free port could be found.\n    \"\"\"\n    # Try random ports first.\n    rng = random.Random()\n    for _ in range(10):\n        port = int(rng.randrange(15000, 25000))\n        if is_port_free(port):\n            _random_ports.add(port)\n            return port\n\n    # Next, try a few times to get an OS-assigned port.\n    # Ambrose discovered that on the 2.6 kernel, calling Bind() on UDP socket\n    # returns the same port over and over. So always try TCP first.\n    for _ in range(10):\n        # Ask the OS for an unused port.\n        port = bind(0, _PROTOS[0][0], _PROTOS[0][1])\n        # Check if this port is unused on the other protocol.\n        if port and bind(port, _PROTOS[1][0], _PROTOS[1][1]):\n            _random_ports.add(port)\n            return port\n\n    # Give up.\n    raise NoFreePortFoundError()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_port_from_port_server(portserver_address, pid=None):\n    if not portserver_address:\n        return None\n    # An AF_UNIX address may start with a zero byte, in which case it is in the\n    # \"abstract namespace\", and doesn't have any filesystem representation.\n    # See 'man 7 unix' for details.\n    # The convention is to write '@' in the address to represent this zero byte.\n    if portserver_address[0] == '@':\n        portserver_address = '\\0' + portserver_address[1:]\n\n    if pid is None:\n        pid = os.getpid()\n\n    try:\n        # Create socket.\n        if hasattr(socket, 'AF_UNIX'):\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        else:\n            # fallback to AF_INET if this is not unix\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            # Connect to portserver.\n            sock.connect(portserver_address)\n\n            # Write request.\n            sock.sendall(('%d\\n' % pid).encode('ascii'))\n\n            # Read response.\n            # 1K should be ample buffer space.\n            buf = sock.recv(1024)\n        finally:\n            sock.close()\n    except socket.error as e:\n        print('Socket error when connecting to portserver:', e,\n              file=sys.stderr)\n        return None\n\n    try:\n        port = int(buf.split(b'\\n')[0])\n    except ValueError:\n        print('Portserver failed to find a port.', file=sys.stderr)\n        return None\n    _owned_ports.add(port)\n    return port", "response": "Request a free port from a system - wide portserver."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(argv):\n    port = pick_unused_port(pid=int(argv[1]) if len(argv) > 1 else None)\n    if not port:\n        sys.exit(1)\n    print(port)", "response": "If passed an arg, treat it as a PID, otherwise portpicker uses getpid."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef soap_request(self, url, urn, action, params, body_elem=\"m\"):\n        soap_body = (\n            '<?xml version=\"1.0\" encoding=\"utf-8\"?>'\n            '<s:Envelope xmlns:s=\"http://schemas.xmlsoap.org/soap/envelope/\"'\n            ' s:encodingStyle=\"http://schemas.xmlsoap.org/soap/encoding/\">'\n            '<s:Body>'\n            '<{body_elem}:{action} xmlns:{body_elem}=\"urn:{urn}\">'\n            '{params}'\n            '</{body_elem}:{action}>'\n            '</s:Body>'\n            '</s:Envelope>'\n        ).format(action=action, urn=urn, params=params, body_elem=body_elem).encode('utf-8')\n\n        headers = {\n            'Host': '{}:{}'.format(self._host, self._port),\n            'Content-Length': len(soap_body),\n            'Content-Type': 'text/xml; charset=utf-8\"',\n            'SOAPAction': '\"urn:{}#{}\"'.format(urn, action),\n        }\n\n        url = 'http://{}:{}/{}'.format(self._host, self._port, url)\n\n        _LOGGER.debug(\"Sending to %s:\\n%s\\n%s\", url, headers, soap_body)\n        req = Request(url, soap_body, headers)\n\n        res = urlopen(req, timeout=5).read()\n        _LOGGER.debug(\"Response: %s\", res)\n        return res", "response": "Send a SOAP request to the TV."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_local_ip(self):\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\n            # Use Google Public DNS server to determine own IP\n            sock.connect(('8.8.8.8', 80))\n\n            return sock.getsockname()[0]\n        except socket.error:\n            try:\n                return socket.gethostbyname(socket.gethostname())\n            except socket.gaierror:\n                return '127.0.0.1'\n        finally:\n            sock.close()", "response": "Try to determine the local IP address of the machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open_webpage(self, url):\n        params = ('<X_AppType>vc_app</X_AppType>'\n                 '<X_LaunchKeyword>resource_id={resource_id}</X_LaunchKeyword>'\n                 ).format(resource_id=1063)\n        res = self.soap_request(URL_CONTROL_NRC, URN_REMOTE_CONTROL,\n                                'X_LaunchApp', params, body_elem=\"s\")\n        root = ET.fromstring(res)\n        el_sessionId = root.find('.//X_SessionId')\n\n        #setup a server socket where URL will be served\n        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        localip = self._get_local_ip() \n        localport = random.randint(1025,65535)\n        server_socket.bind((localip, localport))\n        server_socket.listen(1)\n        _LOGGER.debug(\"Listening on {}:{}\".format(localip,localport))\n \n        params = ('<X_AppType>vc_app</X_AppType>'\n                 '<X_SessionId>{sessionId}</X_SessionId>'\n                 '<X_ConnectKeyword>panasonic-viera 0.2</X_ConnectKeyword>'\n                 '<X_ConnectAddr>{localip}:{localport}</X_ConnectAddr>'\n                 ).format(sessionId=el_sessionId.text, localip=localip, localport=localport)\n\n        res = self.soap_request(URL_CONTROL_NRC, URN_REMOTE_CONTROL,\n                                'X_ConnectApp', params, body_elem=\"s\")\n\n        sockfd, addr = server_socket.accept()\n        _LOGGER.debug(\"Client (%s, %s) connected\" % addr)\n        packet = bytearray([0xf4, 0x01, 0x01, 0x00, 0x00, 0x00, 0x00, len(url)])\n        packet.extend(map(ord, url))\n        packet.append(0x00)\n        sockfd.send(packet)\n        sockfd.close()\n\n        server_socket.close()", "response": "Launch web browser and open url"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_volume(self):\n        params = '<InstanceID>0</InstanceID><Channel>Master</Channel>'\n        res = self.soap_request(URL_CONTROL_DMR, URN_RENDERING_CONTROL,\n                                'GetVolume', params)\n        root = ET.fromstring(res)\n        el_volume = root.find('.//CurrentVolume')\n        return int(el_volume.text)", "response": "Get the current volume level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_volume(self, volume):\n        if volume > 100 or volume < 0:\n            raise Exception('Bad request to volume control. '\n                            'Must be between 0 and 100')\n        params = ('<InstanceID>0</InstanceID><Channel>Master</Channel>'\n                  '<DesiredVolume>{}</DesiredVolume>').format(volume)\n        self.soap_request(URL_CONTROL_DMR, URN_RENDERING_CONTROL,\n                          'SetVolume', params)", "response": "Set a new volume level."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_mute(self):\n        params = '<InstanceID>0</InstanceID><Channel>Master</Channel>'\n        res = self.soap_request(URL_CONTROL_DMR, URN_RENDERING_CONTROL,\n                                'GetMute', params)\n        root = ET.fromstring(res)\n        el_mute = root.find('.//CurrentMute')\n        return el_mute.text != '0'", "response": "Return if the TV is muted."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmuting or unmute the TV.", "response": "def set_mute(self, enable):\n        \"\"\"Mute or unmute the TV.\"\"\"\n        data = '1' if enable else '0'\n        params = ('<InstanceID>0</InstanceID><Channel>Master</Channel>'\n                  '<DesiredMute>{}</DesiredMute>').format(data)\n        self.soap_request(URL_CONTROL_DMR, URN_RENDERING_CONTROL,\n                          'SetMute', params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_key(self, key):\n        if isinstance(key, Keys):\n            key = key.value\n        params = '<X_KeyEvent>{}</X_KeyEvent>'.format(key)\n        self.soap_request(URL_CONTROL_NRC, URN_REMOTE_CONTROL,\n                          'X_SendKey', params)", "response": "Send a key command to the TV."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    parser = argparse.ArgumentParser(prog='panasonic_viera',\n                    description='Remote control a Panasonic Viera TV.')\n    parser.add_argument('host', metavar='host', type=str,\n                    help='Address of the Panasonic Viera TV')\n    parser.add_argument('port', metavar='port', type=int, nargs='?',\n                    default=panasonic_viera.DEFAULT_PORT,\n                    help='Port of the Panasonic Viera TV. Defaults to {}.'.format(panasonic_viera.DEFAULT_PORT))\n    parser.add_argument('--verbose', dest='verbose', action='store_const',\n                    const=True, default=False,\n                    help='debug output')\n    args = parser.parse_args()\n\n    if args.verbose:\n        logging.basicConfig(level=logging.DEBUG)\n\n    remote_control = RemoteControl(panasonic_viera.RemoteControl(args.host, args.port))\n    runner = CommandRunner()\n    runner.command('open_webpage', remote_control.open_webpage)\n    runner.command('get_volume', remote_control.get_volume)\n    runner.command('set_volume', remote_control.set_volume)\n    runner.command('get_mute', remote_control.get_mute)\n    runner.command('set_mute', remote_control.set_mute)\n    runner.command('turn_off', remote_control.turn_off)\n    runner.command('volume_up', remote_control.volume_up)\n    runner.command('volume_down', remote_control.volume_down)\n    runner.command('mute_volume', remote_control.mute_volume)\n    runner.command('turn_off', remote_control.turn_off)\n    runner.command('turn_on', remote_control.turn_on)\n    runner.command('send_key', remote_control.send_key)\n    return Console(runner).run()", "response": "Main function of the command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract a password from an unlocked Keychain using fuzzy matching.", "response": "def item(self, name, fuzzy_threshold=100):\n        \"\"\"\n        Extract a password from an unlocked Keychain using fuzzy\n        matching. ``fuzzy_threshold`` can be an integer between 0 and\n        100, where 100 is an exact match.\n        \"\"\"\n        match = process.extractOne(\n            name,\n            self._items.keys(),\n            score_cutoff=(fuzzy_threshold-1),\n        )\n        if match:\n            exact_name = match[0]\n            item = self._items[exact_name]\n            item.decrypt_with(self)\n            return item\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef key(self, identifier=None, security_level=None):\n        if identifier:\n            try:\n                return self._encryption_keys[identifier]\n            except KeyError:\n                pass\n        if security_level:\n            for key in self._encryption_keys.values():\n                if key.level == security_level:\n                    return key", "response": "Tries to find an encryption key for the given identifier and security level. If no encryption key is found returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n        self._unlock_keychain()\n\n        item = self.keychain.item(\n            self.arguments.item,\n            fuzzy_threshold=self._fuzzy_threshold(),\n        )\n\n        if item is not None:\n            self.stdout.write(\"%s\\n\" % item.password)\n        else:\n            self.stderr.write(\"1pass: Could not find an item named '%s'\\n\" % (\n                self.arguments.item,\n            ))\n            sys.exit(os.EX_DATAERR)", "response": "The main entry point performs the appropriate action for the given\n            arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(css_url=None, js_url=None, serve_local=False):\n        if serve_local or current_app.config['SHARE_SERVE_LOCAL']:\n            css_url = url_for('share.static', filename='css/share.min.css')\n            js_url = url_for('share.static', filename='js/social-share.min.js')\n\n        if css_url is None:\n            css_url = 'https://cdn.bootcss.com/social-share.js/1.0.16/css/share.min.css'\n        if js_url is None:\n            js_url = 'https://cdn.bootcss.com/social-share.js/1.0.16/js/social-share.min.js'\n        return Markup('''<link rel=\"stylesheet\" href=\"%s\" type=\"text/css\">\\n\n            <script src=\"%s\"></script>''' % (css_url, js_url))", "response": "Load the css and js resources."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a social share component.", "response": "def create(title='', sites=None, mobile_sites=None, align='left', addition_class=''):\n        \"\"\"Create a share component.\n\n        :param title: the prompt displayed on the left of the share component.\n        :param sites: a string that consist of sites, separate by comma.\n                      supported site name: weibo, wechat, douban, facebook, twitter,\n                      google, linkedin, qq, qzone.\n                      for example: `'weibo, wechat, qq'`.\n        :param mobile_sites: the sites displayed on mobile.\n        :param align: the align of the share component, default to `'left'`.\n        :param addition_class: the style class added to the share component.\n        \"\"\"\n        if current_app.config['SHARE_HIDE_ON_MOBILE']:\n            platform = request.user_agent.platform\n            if platform is not None:\n                mobile_pattern = re.compile('android|fennec|iemobile|iphone|opera (?:mini|mobi)')\n                m = re.match(mobile_pattern, platform)\n                if m is not None:\n                    return ''\n\n        if sites is None:\n            sites = current_app.config['SHARE_SITES']\n        if mobile_sites is None:\n            mobile_sites = current_app.config['SHARE_MOBILE_SITES']\n        return Markup('''<div class=\"social-share %s\" data-sites=\"%s\" data-mobile-sites=\"%s\"\n        align=\"%s\">%s</div>''' % (addition_class, sites, mobile_sites, align, title))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_text(bytesio):\n    text_chars = (\n        bytearray([7, 8, 9, 10, 11, 12, 13, 27]) +\n        bytearray(range(0x20, 0x7F)) +\n        bytearray(range(0x80, 0X100))\n    )\n    return not bool(bytesio.read(1024).translate(None, text_chars))", "response": "Return whether the first KB of contents seems to be text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_shebang(bytesio):\n    if bytesio.read(2) != b'#!':\n        return ()\n    first_line = bytesio.readline()\n    try:\n        first_line = first_line.decode('UTF-8')\n    except UnicodeDecodeError:\n        return ()\n\n    # Require only printable ascii\n    for c in first_line:\n        if c not in printable:\n            return ()\n\n    cmd = tuple(_shebang_split(first_line.strip()))\n    if cmd[0] == '/usr/bin/env':\n        cmd = cmd[1:]\n    return cmd", "response": "Parse the shebang from a file opened for reading binary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_shebang_from_file(path):\n    if not os.path.lexists(path):\n        raise ValueError('{} does not exist.'.format(path))\n    if not os.access(path, os.X_OK):\n        return ()\n\n    with open(path, 'rb') as f:\n        return parse_shebang(f)", "response": "Parse the shebang given a file path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the spdx id for the license contained in filename.", "response": "def license_id(filename):\n    \"\"\"Return the spdx id for the license contained in `filename`.  If no\n    license is detected, returns `None`.\n\n    spdx: https://spdx.org/licenses/\n    licenses from choosealicense.com: https://github.com/choosealicense.com\n\n    Approximate algorithm:\n\n    1. strip copyright line\n    2. normalize whitespace (replace all whitespace with a single space)\n    3. check exact text match with existing licenses\n    4. failing that use edit distance\n    \"\"\"\n    import editdistance  # `pip install identify[license]`\n\n    with io.open(filename, encoding='UTF-8') as f:\n        contents = f.read()\n\n    norm = _norm_license(contents)\n\n    min_edit_dist = sys.maxsize\n    min_edit_dist_spdx = ''\n\n    # try exact matches\n    for spdx, text in licenses.LICENSES:\n        norm_license = _norm_license(text)\n        if norm == norm_license:\n            return spdx\n\n        # skip the slow calculation if the lengths are very different\n        if norm and abs(len(norm) - len(norm_license)) / len(norm) > .05:\n            continue\n\n        edit_dist = editdistance.eval(norm, norm_license)\n        if edit_dist < min_edit_dist:\n            min_edit_dist = edit_dist\n            min_edit_dist_spdx = spdx\n\n    # if there's less than 5% edited from the license, we found our match\n    if norm and min_edit_dist / len(norm) < .05:\n        return min_edit_dist_spdx\n    else:\n        # no matches :'(\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _credentials_found_in_envars():\n        return any([os.getenv('PAN_ACCESS_TOKEN'),\n                    os.getenv('PAN_CLIENT_ID'),\n                    os.getenv('PAN_CLIENT_SECRET'),\n                    os.getenv('PAN_REFRESH_TOKEN')])", "response": "Check if credentials in envars are found in envars."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresolving credential from envars or credentials store.", "response": "def _resolve_credential(self, credential):\n        \"\"\"Resolve credential from envars or credentials store.\n\n        Args:\n            credential (str): Credential to resolve.\n\n        Returns:\n            str or None: Resolved credential or ``None``.\n\n        \"\"\"\n        if self._credentials_found_in_instance:\n            return\n        elif self._credentials_found_in_envars():\n            return os.getenv('PAN_' + credential.upper())\n        else:\n            return self.storage.fetch_credential(\n                credential=credential, profile=self.profile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts payload field from JWT.", "response": "def decode_jwt_payload(self, access_token=None):\n        \"\"\"Extract payload field from JWT.\n\n        Args:\n            access_token (str): Access token to decode. Defaults to ``None``.\n\n        Returns:\n            dict: JSON object that contains the claims conveyed by the JWT.\n\n        \"\"\"\n        c = self.get_credentials()\n        jwt = access_token or c.access_token\n        try:\n            _, payload, _ = jwt.split('.')  # header, payload, sig\n            rem = len(payload) % 4\n            if rem > 0:  # add padding\n                payload += '=' * (4 - rem)\n            try:\n                decoded_jwt = b64decode(payload).decode(\"utf-8\")\n            except TypeError as e:\n                raise PanCloudError(\n                    \"Failed to base64 decode JWT: %s\" % e)\n            else:\n                try:\n                    x = loads(decoded_jwt)\n                except ValueError as e:\n                    raise PanCloudError(\"Invalid JSON: %s\" % e)\n        except (AttributeError, ValueError) as e:\n            raise PanCloudError(\"Invalid JWT: %s\" % e)\n\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract exp field from access token.", "response": "def _decode_exp(self, access_token=None):\n        \"\"\"Extract exp field from access token.\n\n        Args:\n            access_token (str): Access token to decode. Defaults to ``None``.\n\n        Returns:\n            int: JWT expiration in epoch seconds.\n\n        \"\"\"\n        c = self.get_credentials()\n        jwt = access_token or c.access_token\n        x = self.decode_jwt_payload(jwt)\n\n        if 'exp' in x:\n            try:\n                exp = int(x['exp'])\n            except ValueError:\n                raise PanCloudError(\n                    \"Expiration time (exp) must be an integer\")\n            else:\n                self.jwt_exp = exp\n                return exp\n        else:\n            raise PanCloudError(\"No exp field found in payload\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch_tokens(self, client_id=None, client_secret=None, code=None,\n                     redirect_uri=None, **kwargs):\n        \"\"\"Exchange authorization code for token.\n\n        Args:\n            client_id (str): OAuth2 client ID. Defaults to ``None``.\n            client_secret (str): OAuth2 client secret. Defaults to ``None``.\n            code (str): Authorization code. Defaults to ``None``.\n            redirect_uri (str): Redirect URI. Defaults to ``None``.\n\n        Returns:\n            dict: Response from token URL.\n\n        \"\"\"\n        client_id = client_id or self.client_id\n        client_secret = client_secret or self.client_secret\n        redirect_uri = redirect_uri or self.redirect_uri\n        data = {\n           'grant_type': 'authorization_code',\n           'client_id': client_id,\n           'client_secret': client_secret,\n           'code': code,\n           'redirect_uri': redirect_uri\n        }\n        r = self._httpclient.request(\n            method='POST',\n            url=self.token_url,\n            json=data,\n            path='/api/oauth2/RequestToken',\n            auth=None,\n            **kwargs\n        )\n        if not r.ok:\n            raise PanCloudError(\n                '%s %s: %s' % (r.status_code, r.reason, r.text)\n            )\n        try:\n            r_json = r.json()\n        except ValueError as e:\n            raise PanCloudError(\"Invalid JSON: %s\" % e)\n        else:\n            if r.json().get(\n                'error_description'\n            ) or r.json().get(\n                'error'\n            ):\n                raise PanCloudError(r.text)\n            self.access_token = r_json.get('access_token')\n            self.jwt_exp = self._decode_exp(self.access_token_)\n            self.refresh_token = r_json.get('refresh_token')\n            self.write_credentials()\n            return r_json", "response": "Exchange authorization code for token."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_authorization_url(self, client_id=None, instance_id=None,\n                              redirect_uri=None, region=None, scope=None,\n                              state=None):\n        \"\"\"Generate authorization URL.\n\n        Args:\n            client_id (str): OAuth2 client ID. Defaults to ``None``.\n            instance_id (str): App Instance ID. Defaults to ``None``.\n            redirect_uri (str): Redirect URI. Defaults to ``None``.\n            region (str): App Region. Defaults to ``None``.\n            scope (str): Permissions. Defaults to ``None``.\n            state (str): UUID to detect CSRF. Defaults to ``None``.\n\n        Returns:\n            str, str: Auth URL, state\n\n        \"\"\"\n        client_id = client_id or self.client_id\n        instance_id = instance_id or self.instance_id\n        redirect_uri = redirect_uri or self.redirect_uri\n        region = region or self.region\n        scope = scope or self.scope\n        state = state or str(uuid.uuid4())\n        self.state = state\n        return Request(\n            'GET',\n            self.auth_base_url,\n            params={\n                'client_id': client_id,\n                'instance_id': instance_id,\n                'redirect_uri': redirect_uri,\n                'region': region,\n                'response_type': 'code',\n                'scope': scope,\n                'state': state\n            }\n        ).prepare().url, state", "response": "Generate authorization URL.\n\n        Args:\n            client_id (str): OAuth2 client ID. Defaults to ``None``.\n            instance_id (str): App Instance ID. Defaults to ``None``.\n            redirect_uri (str): Redirect URI. Defaults to ``None``.\n            region (str): App Region. Defaults to ``None``.\n            scope (str): Permissions. Defaults to ``None``.\n            state (str): UUID to detect CSRF. Defaults to ``None``.\n\n        Returns:\n            str, str: Auth URL, state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget read - only credentials.", "response": "def get_credentials(self):\n        \"\"\"Get read-only credentials.\n\n        Returns:\n            class: Read-only credentials.\n\n        \"\"\"\n        return ReadOnlyCredentials(\n            self.access_token, self.client_id, self.client_secret,\n            self.refresh_token\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef jwt_is_expired(self, access_token=None, leeway=0):\n        if access_token is not None:\n            exp = self._decode_exp(access_token)\n        else:\n            exp = self.jwt_exp\n        now = time()\n        if exp < (now - leeway):\n            return True\n        return False", "response": "Validate JWT access token expiration."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrefresh the access token and refreshes the token.", "response": "def refresh(self, access_token=None, **kwargs):\n        \"\"\"Refresh access and refresh tokens.\n\n        Args:\n            access_token (str): Access token to refresh. Defaults to ``None``.\n\n        Returns:\n            str: Refreshed access token.\n\n        \"\"\"\n        if not self.token_lock.locked():\n            with self.token_lock:\n                if access_token == self.access_token or access_token is None:\n                    if self.developer_token is not None:\n                        r = self._httpclient.request(\n                            method='POST',\n                            url=self.developer_token_url,\n                            path='/request_token',\n                            headers={\n                                'Authorization': 'Bearer {}'.format(\n                                    self.developer_token\n                                )\n                            },\n                            timeout=30,\n                            raise_for_status=True\n                        )\n\n                    elif all(\n                        [\n                            self.client_id,\n                            self.client_secret,\n                            self.refresh_token\n                        ]\n                    ):\n                        data = {\n                            'client_id': self.client_id,\n                            'client_secret': self.client_secret,\n                            'refresh_token': self.refresh_token,\n                            'grant_type': 'refresh_token'\n                        }\n                        r = self._httpclient.request(\n                            method='POST',\n                            url=self.token_url,\n                            json=data,\n                            path='/api/oauth2/RequestToken',\n                            **kwargs\n                        )\n                    else:\n                        raise PartialCredentialsError(\n                            \"Missing one or more required credentials\"\n                        )\n\n                    if r:\n                        if not r.ok:\n                            raise PanCloudError(\n                                '%s %s: %s' % (\n                                r.status_code, r.reason, r.text)\n                            )\n                        try:\n                            r_json = r.json()\n                        except ValueError as e:\n                            raise PanCloudError(\"Invalid JSON: %s\" % e)\n                        else:\n                            if r.json().get(\n                                'error_description'\n                            ) or r.json().get(\n                                'error'\n                            ):\n                                raise PanCloudError(r.text)\n                            self.access_token = r_json.get(\n                                'access_token', None\n                            )\n                            self.jwt_exp = self._decode_exp(\n                                self.access_token_)\n                            if r_json.get('refresh_token', None):\n                                self.refresh_token = \\\n                                    r_json.get('refresh_token')\n                            self.write_credentials()\n                        return self.access_token_"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrevoke an access token from the API.", "response": "def revoke_access_token(self, **kwargs):\n        \"\"\"Revoke access token.\"\"\"\n        c = self.get_credentials()\n        data = {\n            'client_id': c.client_id,\n            'client_secret': c.client_secret,\n            'token': c.access_token,\n            'token_type_hint': 'access_token'\n        }\n        r = self._httpclient.request(\n            method='POST',\n            url=self.token_url,\n            json=data,\n            path='/api/oauth2/RevokeToken',\n            **kwargs\n        )\n        if not r.ok:\n            raise PanCloudError(\n                '%s %s: %s' % (r.status_code, r.reason, r.text)\n            )\n        try:\n            r_json = r.json()\n        except ValueError as e:\n            raise PanCloudError(\"Invalid JSON: %s\" % e)\n        else:\n            if r.json().get(\n                'error_description'\n            ) or r.json().get(\n                'error'\n            ):\n                raise PanCloudError(r.text)\n            return r_json"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_credentials(self):\n        c = self.get_credentials()\n        return self.storage.write_credentials(\n            credentials=c, profile=self.profile,\n            cache_token=self.cache_token\n        )", "response": "Write credentials to credentials store."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, query_id=None, **kwargs):  # pragma: no cover\n        path = \"/logging-service/v1/queries/{}\".format(query_id)\n        r = self._httpclient.request(\n            method=\"DELETE\",\n            url=self.url,\n            path=path,\n            **kwargs\n        )\n        return r", "response": "Delete a query job."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an iterator over the pages in a non - greedy manner.", "response": "def iter_poll(self, query_id=None, sequence_no=None, params=None,\n                  **kwargs):  # pragma: no cover\n        \"\"\"Retrieve pages iteratively in a non-greedy manner.\n\n        Automatically increments the sequenceNo as it continues to poll\n        for results until the endpoint reports JOB_FINISHED or\n        JOB_FAILED, or an exception is raised by the pancloud library.\n\n        Args:\n            params (dict): Payload/request dictionary.\n            query_id (str): Specifies the ID of the query job.\n            sequence_no (int): Specifies the sequenceNo.\n            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.\n\n        Yields:\n            requests.Response: Requests Response() object.\n\n        Examples:\n            Refer to ``logging_iter_poll.py`` example.\n\n        \"\"\"\n        while True:\n            r = self.poll(\n                query_id, sequence_no, params, enforce_json=True,\n                **kwargs\n            )\n            if r.json()['queryStatus'] == \"FINISHED\":\n                if sequence_no is not None:\n                    sequence_no += 1\n                else:\n                    sequence_no = 1\n                yield r\n            elif r.json()['queryStatus'] == \"JOB_FINISHED\":\n                yield r\n                break\n            elif r.json()['queryStatus'] == \"JOB_FAILED\":\n                yield r\n                break\n            elif r.json()['queryStatus'] == \"RUNNING\":\n                yield r\n                time.sleep(1)\n            else:\n                raise PanCloudError(\n                    'Bad queryStatus: %s' % r.json()['queryStatus']\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef poll(self, query_id=None, sequence_no=None, params=None,\n             **kwargs):  # pragma: no cover\n        \"\"\"Poll for asynchronous query results.\n\n        Continue to poll for results until this endpoint reports\n        JOB_FINISHED or JOB_FAILED. The results of queries can be\n        returned in multiple pages, each of which may contain many log\n        records. Use this endpoint to poll for query result batches, as\n        well as to track query result status.\n\n        Args:\n            params (dict): Payload/request dictionary.\n            query_id (str): Specifies the ID of the query job.\n            sequence_no (int): Specifies the sequenceNo.\n            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.\n\n        Returns:\n            requests.Response: Requests Response() object.\n\n        Examples:\n            Refer to ``logging_query.py`` example.\n\n        \"\"\"\n        path = \"/logging-service/v1/queries/{}/{}\".format(\n            query_id, sequence_no\n        )\n        r = self._httpclient.request(\n            method=\"GET\",\n            url=self.url,\n            params=params,\n            path=path,\n            **kwargs\n        )\n        return r", "response": "Poll for asynchronous query results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xpoll(self, query_id=None, sequence_no=None, params=None,\n              delete_query=True, **kwargs):  # pragma: no cover\n        \"\"\"Retrieve individual logs iteratively in a non-greedy manner.\n\n        Generator function to return individual log entries from poll\n        API request.\n\n        Args:\n            params (dict): Payload/request dictionary.\n            query_id (str): Specifies the ID of the query job.\n            sequence_no (int): Specifies the sequenceNo.\n            delete_query (bool): True for delete, False otherwise.\n            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.\n\n        Yields:\n            dictionary with single log entry.\n\n        \"\"\"\n        def _delete(query_id, **kwargs):\n            r = self.delete(query_id, **kwargs)\n            try:\n                r_json = r.json()\n            except ValueError as e:\n                raise PanCloudError('Invalid JSON: %s' % e)\n\n            if not (200 <= r.status_code < 300):\n                if 'errorCode' in r_json and 'errorMessage' in r_json:\n                    raise PanCloudError('%s: %s' %\n                                        (r_json['errorCode'],\n                                         r_json['errorMessage']))\n                else:\n                    raise PanCloudError('%s %s' % (r.status_code,\n                                                   r.reason))\n\n            if r.status_code == 200:\n                return\n            else:\n                raise PanCloudError('delete: status_code: %d' %\n                                    r.status_code)\n\n        while True:\n            r = self.poll(query_id, sequence_no, params, **kwargs)\n            try:\n                r_json = r.json()\n            except ValueError as e:\n                raise PanCloudError('Invalid JSON: %s' % e)\n\n            if not (200 <= r.status_code < 300):\n                if 'errorCode' in r_json and 'errorMessage' in r_json:\n                    raise PanCloudError('%s: %s' %\n                                        (r_json['errorCode'],\n                                         r_json['errorMessage']))\n                else:\n                    raise PanCloudError('%s %s' % (r.status_code,\n                                                   r.reason))\n\n            if 'queryStatus' not in r_json:\n                self._debug(r_json)\n                raise PanCloudError('no \"queryStatus\" in response')\n\n            self._debug(r_json['queryStatus'])\n            if r_json['queryStatus'] in ['FINISHED', 'JOB_FINISHED']:\n                try:\n                    hits = r_json['result']['esResult']['hits']['hits']\n                except KeyError as e:\n                    raise PanCloudError('no \"hits\" in response' % e)\n\n                self._debug('hits: %d', len(hits))\n                for x in hits:\n                    yield x\n\n                if r_json['queryStatus'] == 'JOB_FINISHED':\n                    if delete_query:\n                        _delete(query_id, **kwargs)\n                    return\n\n                if sequence_no is not None:\n                    sequence_no += 1\n                else:\n                    sequence_no = 1\n\n            elif r_json['queryStatus'] == 'JOB_FAILED':\n                e = '%s' % r_json['queryStatus']\n                try:\n                    e += ': %s' % r_json['result']['esResult']['error']\n                except KeyError:\n                    self._debug(r_json)\n\n                raise PanCloudError(e)\n\n            elif r_json['queryStatus'] == 'RUNNING':\n                if params is not None and 'maxWaitTime' in params:\n                    pass\n                else:\n                    # XXX\n                    time.sleep(1)\n            else:\n                raise PanCloudError('Bad queryStatus: %s' % r_json['queryStatus'])", "response": "Generator function to retrieve individual logs iteratively in a non - greedy manner."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, vendor_id=None, log_type=None, json=None, **kwargs):\n        path = \"/logging-service/v1/logs/{}/{}\".format(\n            vendor_id, log_type\n        )\n        r = self._httpclient.request(\n            method=\"POST\",\n            url=self.url,\n            json=json,\n            path=path,\n            **kwargs\n        )\n        return r", "response": "Write log records to the Logging Service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_credential(self, credential=None, profile=None):\n        q = self.db.get(self.query.profile == profile)\n        if q is not None:\n            return q.get(credential)", "response": "Fetch a specific credential from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a profile from the credentials file.", "response": "def remove_profile(self, profile=None):\n        \"\"\"Remove profile from credentials file.\n\n        Args:\n            profile (str): Credentials profile to remove.\n\n        Returns:\n            list: List of affected document IDs.\n\n        \"\"\"\n        with self.db:\n            return self.db.remove(self.query.profile == profile)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_credentials(self, credentials=None, profile=None,\n                          cache_token=None):\n        \"\"\"Write credentials.\n\n        Write credentials to credentials file. Performs ``upsert``.\n\n        Args:\n            cache_token (bool): If ``True``, stores ``access_token`` in token store. Defaults to ``True``.\n            credentials (class): Read-only credentials.\n            profile (str): Credentials profile. Defaults to ``'default'``.\n\n        Returns:\n            int: Affected document ID.\n\n        \"\"\"\n        d = {\n            'profile': profile,\n            'client_id': credentials.client_id,\n            'client_secret': credentials.client_secret,\n            'refresh_token': credentials.refresh_token\n        }\n        if cache_token:\n            d.update({'access_token': credentials.access_token})\n        with self.lock:\n            return self.db.upsert(\n                d, self.query.profile == profile\n            )", "response": "Write credentials to credentials file. Performs upsert."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nack(self, channel_id=None, **kwargs):  # pragma: no cover\n        path = \"/event-service/v1/channels/{}/nack\".format(channel_id)\n        r = self._httpclient.request(\n            method=\"POST\",\n            url=self.url,\n            path=path,\n            **kwargs\n        )\n        return r", "response": "Send a negative read - acknowledgement to the service."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread one or more events from a channel.", "response": "def poll(self, channel_id=None, json=None, **kwargs):  # pragma: no cover\n        \"\"\"Read one or more events from a channel.\n\n        Reads events (log records) from the identified channel. Events\n        are read in chronological order.\n\n        Args:\n            channel_id (str): The channel ID.\n            json (dict): Payload/request body.\n            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.\n\n        Returns:\n            requests.Response: Requests Response() object.\n\n        Examples:\n            Refer to ``event_poll.py`` example.\n\n        \"\"\"\n        path = \"/event-service/v1/channels/{}/poll\".format(channel_id)\n        r = self._httpclient.request(\n            method=\"POST\",\n            url=self.url,\n            json=json,\n            path=path,\n            **kwargs\n        )\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving logType event entries from an API call.", "response": "def xpoll(self, channel_id=None, json=None, ack=False,\n              follow=False, pause=None, **kwargs):\n        \"\"\"Retrieve logType, event entries iteratively in a non-greedy manner.\n\n        Generator function to return logType, event entries from poll\n        API request.\n\n        Args:\n            channel_id (str): The channel ID.\n            json (dict): Payload/request body.\n            ack (bool): True to acknowledge read.\n            follow (bool): True to continue polling after channelId empty.\n            pause (float): Seconds to sleep between poll when follow and channelId empty.\n            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.\n\n        Yields:\n            dictionary with single logType and event entries.\n\n        \"\"\"\n        def _ack(channel_id, **kwargs):\n            r = self.ack(channel_id, **kwargs)\n\n            if not (200 <= r.status_code < 300):\n                try:\n                    r_json = r.json()\n                except ValueError as e:\n                    raise PanCloudError('Invalid JSON: %s' % e)\n                if 'errorCode' in r_json and 'errorMessage' in r_json:\n                    raise PanCloudError('%s: %s' %\n                                        (r_json['errorCode'],\n                                         r_json['errorMessage']))\n                else:\n                    raise PanCloudError('%s %s' % (r.status_code,\n                                                   r.reason))\n\n            if r.status_code == 200:\n                return\n            else:\n                raise PanCloudError('ack: status_code: %d' %\n                                    r.status_code)\n\n        while True:\n            r = self.poll(channel_id, json, **kwargs)\n            try:\n                r_json = r.json()\n            except ValueError as e:\n                raise PanCloudError('Invalid JSON: %s' % e)\n\n            if not (200 <= r.status_code < 300):\n                if 'errorCode' in r_json and 'errorMessage' in r_json:\n                    raise PanCloudError('%s: %s' %\n                                        (r_json['errorCode'],\n                                         r_json['errorMessage']))\n                else:\n                    raise PanCloudError('%s %s' % (r.status_code,\n                                                   r.reason))\n\n            if not r_json and not follow:\n                return\n\n            for x in r_json:\n                yield x\n\n            # XXX don't ack if empty response?\n            if ack:\n                _ack(channel_id, **kwargs)\n\n            if not r_json and pause is not None:\n                self._debug('sleep %.2fs', pause)\n                time.sleep(pause)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply credentials to the request headers.", "response": "def _apply_credentials(auto_refresh=True, credentials=None,\n                           headers=None):\n        \"\"\"Update Authorization header.\n\n        Update request headers with latest `access_token`. Perform token\n        `refresh` if token is ``None``.\n\n        Args:\n            auto_refresh (bool): Perform token refresh if access_token is ``None`` or expired. Defaults to ``True``.\n            credentials (class): Read-only credentials.\n            headers (class): Requests `CaseInsensitiveDict`.\n\n        \"\"\"\n        token = credentials.get_credentials().access_token\n        if auto_refresh is True:\n            if token is None:\n                token = credentials.refresh(\n                    access_token=None, timeout=10)\n            elif credentials.jwt_is_expired():\n                token = credentials.refresh(timeout=10)\n        headers.update(\n            {'Authorization': \"Bearer {}\".format(token)}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend HTTP request. Args: enforce_json (bool): Require properly-formatted JSON or raise :exc:`~pancloud.exceptions.PanCloudError`. Defaults to ``False``. method (str): HTTP method. raise_for_status (bool): If ``True``, raises :exc:`~pancloud.exceptions.HTTPError` if status_code not in 2XX. Defaults to ``False``. url (str): Request URL. **kwargs (dict): Re-packed key-word arguments. Returns: requests.Response: Requests Response() object", "response": "def _send_request(self, enforce_json, method, raise_for_status,\n                      url, **kwargs):\n        \"\"\"Send HTTP request.\n\n        Args:\n             enforce_json (bool): Require properly-formatted JSON or raise :exc:`~pancloud.exceptions.PanCloudError`. Defaults to ``False``.\n             method (str): HTTP method.\n             raise_for_status (bool): If ``True``, raises :exc:`~pancloud.exceptions.HTTPError` if status_code not in 2XX. Defaults to ``False``.\n             url (str): Request URL.\n             **kwargs (dict): Re-packed key-word arguments.\n\n         Returns:\n            requests.Response: Requests Response() object\n\n        \"\"\"\n        r = self.session.request(method, url, **kwargs)\n        if raise_for_status:\n            r.raise_for_status()\n        if enforce_json:\n            if 'application/json' in self.session.headers.get(\n                'Accept', ''\n            ):\n                try:\n                    r.json()\n                except ValueError as e:\n                    raise PanCloudError(\n                        \"Invalid JSON: {}\".format(e)\n                    )\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request(self, **kwargs):\n        url = kwargs.pop('url', self.url)\n\n        # Session() overrides\n        auth = kwargs.pop('auth', self.session.auth)\n        cert = kwargs.pop('cert', self.session.cert)\n        cookies = kwargs.pop('cookies', self.session.cookies)\n        headers = kwargs.pop('headers', self.session.headers.copy())\n        params = kwargs.pop('params', self.session.params)\n        proxies = kwargs.pop('proxies', self.session.proxies)\n        stream = kwargs.pop('stream', self.session.stream)\n        verify = kwargs.pop('verify', self.session.verify)\n\n        # Non-Requests key-word arguments\n        auto_refresh = kwargs.pop('auto_refresh', self.auto_refresh)\n        credentials = kwargs.pop('credentials', self.credentials)\n        enforce_json = kwargs.pop('enforce_json', self.enforce_json)\n        path = kwargs.pop('path', '')  # default to empty path\n        raise_for_status = kwargs.pop(\n            'raise_for_status', self.raise_for_status\n        )\n        url = \"{}:{}{}\".format(url, self.port, path)\n\n        if credentials:\n            self._apply_credentials(\n                auto_refresh=auto_refresh,\n                credentials=credentials,\n                headers=headers\n            )\n\n        k = {  # Re-pack kwargs to dictionary\n            'params': params,\n            'headers': headers,\n            'cookies': cookies,\n            'auth': auth,\n            'proxies': proxies,\n            'verify': verify,\n            'stream': stream,\n            'cert': cert\n        }\n\n        # Request() overrides\n        for x in ['allow_redirects', 'data', 'json', 'method',\n                  'timeout']:\n            if x in kwargs:\n                k[x] = kwargs.pop(x)\n\n        # Handle invalid kwargs\n        if len(kwargs) > 0:\n            raise UnexpectedKwargsError(kwargs)\n\n        try:\n            method = k.pop('method')\n        except KeyError:\n            raise RequiredKwargsError('method')\n\n        # Prepare and send the Request() and return Response()\n        try:\n            r = self._send_request(\n                enforce_json, method, raise_for_status, url, **k\n            )\n            return r\n        except requests.RequestException as e:\n            raise HTTPError(e)", "response": "Generate HTTP request using given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the custom directory attribute configuration object.", "response": "def attributes(self, **kwargs):  # pragma: no cover\n        \"\"\"Retrieve the attribute configuration object.\n\n        Retrieves a mapping that identifies the custom directory\n        attributes configured for the Directory SyncService instance,\n        and the mapping of the custom attributes to standard directory\n        attributes.\n\n        Args:\n            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.\n\n        Returns:\n            requests.Response: Requests Response() object.\n\n        Examples:\n            Refer to ``directory_attributes.py`` example.\n\n        \"\"\"\n        path = \"/directory-sync-service/v1/attributes\"\n        r = self._httpclient.request(\n            method=\"GET\",\n            path=path,\n            url=self.url,\n            **kwargs\n        )\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count(self, object_class=None, params=None, **kwargs):  # pragma: no cover\n        path = \"/directory-sync-service/v1/{}/count\".format(\n            object_class\n        )\n        r = self._httpclient.request(\n            method=\"GET\",\n            path=path,\n            url=self.url,\n            params=params,\n            **kwargs\n        )\n        return r", "response": "Retrieves a count of all directory entries that belong to the object identified by object_class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying data stored in directory.", "response": "def query(self, object_class=None, json=None, **kwargs):  # pragma: no cover\n        \"\"\"Query data stored in directory.\n\n        Retrieves directory data by querying a Directory Sync Service\n        cloud-based instance. The directory data is stored with the\n        Directory Sync Service instance using an agent that is installed\n        in the customer's network.This agent retrieves directory data\n        from the customer's Active Directory, and then sends it to the\n        cloud-based Directory Sync Service instance.\n\n        Args:\n            object_class (str): Directory object class.\n            json (dict): Payload/request body.\n            **kwargs: Supported :meth:`~pancloud.httpclient.HTTPClient.request` parameters.\n\n        Returns:\n            requests.Response: Requests Response() object.\n\n        Examples:\n            Coming soon.\n\n        \"\"\"\n        path = \"/directory-sync-service/v1/{}\".format(object_class)\n        r = self._httpclient.request(\n            method=\"POST\",\n            url=self.url,\n            json=json,\n            path=path,\n            **kwargs\n        )\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the geomean of a list of values.", "response": "def geomean(values):\n    \"\"\"\n    The intuitive/naive way of calculating a geometric mean (first\n    multiply the n values, then take the nth-root of the result) does not\n    work in practice. When you multiple an large enough amount of large\n    enough numbers, their product will oferflow the float representation,\n    and the result will be Infinity.\n\n    We use the alternative method described in\n    https://en.wikipedia.org/wiki/Geometric_mean -- topic \"Relationship with\n    arithmetic mean of logarithms\" -- which is exp(sum(log(x_i)/n))\n\n    Zeros are excluded from the calculation. Since for us numbers are usually\n    measurements (time, counts, etc), we interpret 0 as \"does not exist\".\n    Negative numbers are also excluded on the basis that they most probably\n    represent anomalies in the data.\n    \"\"\"\n    values = [v for v in values if v > 0]\n\n    if len(values) == 0:\n        return 0\n\n    n = len(values)\n    log_sum = 0.0\n    for v in values:\n        log_sum = log_sum + log(v)\n    return exp(log_sum / n)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a tuple with the text and html message of the current build.", "response": "def message(self, do_html=True, custom_email_template=None):\n        \"\"\"\n        Returns a tuple with (text_message,html_message)\n        \"\"\"\n        context = {\n            'build': self.build,\n            'important_metadata': self.important_metadata,\n            'metadata': self.metadata,\n            'notification': self,\n            'previous_build': self.previous_build,\n            'regressions_grouped_by_suite': self.comparison.regressions_grouped_by_suite,\n            'fixes_grouped_by_suite': self.comparison.fixes_grouped_by_suite,\n            'known_issues': self.known_issues,\n            'regressions': self.comparison.regressions,\n            'fixes': self.comparison.fixes,\n            'thresholds': self.thresholds,\n            'settings': settings,\n            'summary': self.summary,\n        }\n\n        html_message = ''\n        if custom_email_template:\n            text_template = jinja2.from_string(custom_email_template.plain_text)\n            text_message = text_template.render(context)\n\n            if do_html:\n                html_template = jinja2.from_string(custom_email_template.html)\n                html_message = html_template.render(context)\n        else:\n            text_message = render_to_string(\n                'squad/notification/diff.txt.jinja2',\n                context=context,\n            )\n            if do_html:\n                html_message = render_to_string(\n                    'squad/notification/diff.html.jinja2',\n                    context=context,\n                )\n        return (text_message, html_message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_complex_ops(encoded_querystring, operators=None, negation=True):\n    complex_op_re = COMPLEX_OP_NEG_RE if negation else COMPLEX_OP_RE\n    if operators is None:\n        operators = COMPLEX_OPERATORS\n\n    # decode into: (a%3D1) & (b%3D2) | ~(c%3D3)\n    decoded_querystring = unquote(encoded_querystring)\n    matches = [m for m in complex_op_re.finditer(decoded_querystring)]\n\n    if not matches:\n        msg = _(\"Unable to parse querystring. Decoded: '%(decoded)s'.\")\n        raise SerializerValidationError(msg % {'decoded': decoded_querystring})\n\n    results, errors = [], []\n    for match, has_next in lookahead(matches):\n        negate, querystring, op = match.groups()\n\n        negate = negate == '~'\n        querystring = unquote(querystring)\n        op_func = operators.get(op.strip()) if op else None\n        if op_func is None and has_next:\n            msg = _(\"Invalid querystring operator. Matched: '%(op)s'.\")\n            errors.append(msg % {'op': op})\n\n        results.append(ComplexOp(querystring, negate, op_func))\n\n    trailing_chars = decoded_querystring[matches[-1].end():]\n    if trailing_chars:\n        msg = _(\"Ending querystring must not have trailing characters. Matched: '%(chars)s'.\")\n        errors.append(msg % {'chars': trailing_chars})\n\n    if errors:\n        raise SerializerValidationError(errors)\n\n    return results", "response": "Decodes a string of complex operations into a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nforwarding to the right sub - handler", "response": "def handle(self, *args, **options):\n        \"\"\" Forward to the right sub-handler \"\"\"\n        if options[\"sub_command\"] == \"add\":\n            self.handle_add(options)\n        elif options[\"sub_command\"] == \"update\":\n            self.handle_update(options)\n        elif options[\"sub_command\"] == \"details\":\n            self.handle_details(options[\"username\"])\n        elif options[\"sub_command\"] == \"list\":\n            self.handle_list(options[\"all\"], options[\"csv\"])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new user", "response": "def handle_add(self, options):\n        \"\"\" Create a new user \"\"\"\n        username = options[\"username\"]\n        passwd = options[\"passwd\"]\n        if passwd is None:\n            passwd = User.objects.make_random_password()\n        user = User.objects.create_user(username, options[\"email\"], passwd)\n\n        if options[\"staff\"]:\n            user.is_staff = True\n        if options[\"superuser\"]:\n            user.is_superuser = True\n        user.save()\n\n        if options[\"passwd\"] is None:\n            self.stdout.write(passwd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_plugins_by_feature(features):\n    if not features:\n        return get_all_plugins()\n    plugins = PluginLoader.load_all().items()\n    names = set([f.__name__ for f in features])\n    return [e for e, plugin in plugins if names & set(plugin.__dict__.keys())]", "response": "Returns a list of plugins that implement at least one of the given features."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef metadata(self):\n        if self.__metadata__ is None:\n            metadata = {}\n            for test_run in self.test_runs.defer(None).all():\n                for key, value in test_run.metadata.items():\n                    metadata.setdefault(key, [])\n                    if value not in metadata[key]:\n                        metadata[key].append(value)\n            for key in metadata.keys():\n                if len(metadata[key]) == 1:\n                    metadata[key] = metadata[key][0]\n                else:\n                    metadata[key] = sorted(metadata[key], key=str)\n            self.__metadata__ = metadata\n        return self.__metadata__", "response": "Return the build metadata for this instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef finished(self):\n        reasons = []\n\n        # XXX note that by using test_jobs here, we are adding an implicit\n        # dependency on squad.ci, what in theory violates our architecture.\n        testjobs = self.test_jobs\n        if testjobs.count() > 0:\n            if testjobs.filter(fetched=False).count() > 0:\n                # a build that has pending CI jobs is NOT finished\n                reasons.append(\"There are unfinished CI jobs\")\n            else:\n                # carry on, and check whether the number of expected test runs\n                # per environment is satisfied.\n                pass\n\n        # builds with no CI jobs are finished when each environment has\n        # received the expected amount of test runs\n        testruns = {\n            e.id: {\n                'name': str(e),\n                'expected': e.expected_test_runs,\n                'received': 0\n            }\n            for e in self.project.environments.all()\n        }\n\n        for t in self.test_runs.filter(completed=True).all():\n            testruns[t.environment_id]['received'] += 1\n\n        for env, count in testruns.items():\n            expected = count['expected']\n            received = count['received']\n            env_name = count['name']\n            if expected == 0:\n                continue\n            if received == 0:\n                reasons.append(\"No test runs for %s received so far\" % env_name)\n            if expected and received < expected:\n                reasons.append(\n                    \"%d test runs expected for %s, but only %d received so far\" % (\n                        expected,\n                        env_name,\n                        received,\n                    )\n                )\n        return (len(reasons) == 0, reasons)", "response": "A build that has no pending CI test jobs and has at least N test runs for that environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_or_update(cls, build):\n\n        test_summary = build.test_summary\n        metrics_summary = MetricsSummary(build)\n        now = timezone.now()\n        test_runs_total = build.test_runs.count()\n        test_runs_completed = build.test_runs.filter(completed=True).count()\n        test_runs_incomplete = build.test_runs.filter(completed=False).count()\n        regressions = None\n        fixes = None\n\n        previous_build = Build.objects.filter(\n            status__finished=True,\n            datetime__lt=build.datetime,\n            project=build.project,\n        ).order_by('datetime').last()\n        if previous_build is not None:\n            comparison = TestComparison(previous_build, build)\n            if comparison.regressions:\n                regressions = yaml.dump(comparison.regressions)\n            if comparison.fixes:\n                fixes = yaml.dump(comparison.fixes)\n\n        finished, _ = build.finished\n        data = {\n            'tests_pass': test_summary.tests_pass,\n            'tests_fail': test_summary.tests_fail,\n            'tests_xfail': test_summary.tests_xfail,\n            'tests_skip': test_summary.tests_skip,\n            'metrics_summary': metrics_summary.value,\n            'has_metrics': metrics_summary.has_metrics,\n            'last_updated': now,\n            'finished': finished,\n            'test_runs_total': test_runs_total,\n            'test_runs_completed': test_runs_completed,\n            'test_runs_incomplete': test_runs_incomplete,\n            'regressions': regressions,\n            'fixes': fixes\n        }\n\n        status, created = cls.objects.get_or_create(build=build, defaults=data)\n        if not created and test_summary.tests_total >= status.tests_total:\n            # XXX the test above for the new total number of tests prevents\n            # results that arrived earlier, but are only being processed now,\n            # from overwriting a ProjectStatus created by results that arrived\n            # later but were already processed.\n            status.tests_pass = test_summary.tests_pass\n            status.tests_fail = test_summary.tests_fail\n            status.tests_xfail = test_summary.tests_xfail\n            status.tests_skip = test_summary.tests_skip\n            status.metrics_summary = metrics_summary.value\n            status.has_metrics = metrics_summary.has_metrics\n            status.last_updated = now\n            finished, _ = build.finished\n            status.finished = finished\n            status.build = build\n            status.test_runs_total = test_runs_total\n            status.test_runs_completed = test_runs_completed\n            status.test_runs_incomplete = test_runs_incomplete\n            status.regressions = regressions\n            status.fixes = fixes\n            status.save()\n        return status", "response": "Creates a new ProjectStatus for the given build and\n            sets it to the current state of the build. Returns it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_project_ids(self):\n        user = self.request.user\n        projects = Project.objects.accessible_to(user).values('id')\n        return [p['id'] for p in projects]", "response": "Returns a list of project ids that can be visualized for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef builds(self, request, pk=None):\n        builds = self.get_object().builds.prefetch_related('test_runs').order_by('-datetime')\n        page = self.paginate_queryset(builds)\n        serializer = BuildSerializer(page, many=True, context={'request': request})\n        return self.get_paginated_response(serializer.data)", "response": "List of builds for the current project."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef suites(self, request, pk=None):\n        suites_names = self.get_object().suites.values_list('slug')\n        suites_metadata = SuiteMetadata.objects.filter(kind='suite', suite__in=suites_names)\n        page = self.paginate_queryset(suites_metadata)\n        serializer = SuiteMetadataSerializer(page, many=True, context={'request': request})\n        return self.get_paginated_response(serializer.data)", "response": "Return a list of test suite names available in this project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef email(self, request, pk=None):\n        force = request.query_params.get(\"force\", False)\n        delayed_report, created = self.__return_delayed_report(request)\n        if created or force:\n            delayed_report = prepare_report(delayed_report.pk)\n        if delayed_report.status_code != status.HTTP_200_OK:\n            return Response(yaml.safe_load(delayed_report.error_message or ''), delayed_report.status_code)\n        if delayed_report.output_format == \"text/html\" and delayed_report.output_html:\n            return HttpResponse(delayed_report.output_html, content_type=delayed_report.output_format)\n        return HttpResponse(delayed_report.output_text, content_type=delayed_report.output_format)", "response": "This method generates the body of email notification for the build."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main(\n        source,\n        repo_source_files,\n        requirements_file,\n        local_source_file,\n        work_dir):\n    \"\"\"\n    Bundle up a deployment package for AWS Lambda.\n\n    From your local filesystem:\n\n    \\b\n        $ make-lambda-package .\n        ...\n        dist/lambda-package.zip\n\n    Or from a remote git repository:\n\n    \\b\n        $ make-lambda-package https://github.com/NoRedInk/make-lambda-package.git\n        ...\n        vendor/dist/NoRedInk-make-lambda-package.zip\n\n    Use # fragment to specify a commit or a branch:\n\n    \\b\n        $ make-lambda-package https://github.com/NoRedInk/make-lambda-package.git#v1.0.0\n\n    Dependencies specified with --requirements-file will built using a docker container\n    that replicates AWS Lambda's execution environment, so that extension modules\n    are correctly packaged.\n\n    When packaging a local source, --work-dir defaults to `.`:\n\n    \\b\n    * ./build will hold a virtualenv for building dependencies if specified.\n    * ./dist is where the zipped package will be saved\n\n    When packaging a remote source, --work-dir defaults to `./vendor`.\n    \"\"\"\n    scm_source = fsutil.parse_path_or_url(source)\n    paths = fsutil.decide_paths(scm_source, work_dir)\n\n    if requirements_file:\n        with open(os.devnull, 'w') as devnull:\n            docker_retcode = subprocess.call(['docker', '--help'], stdout=devnull)\n        if docker_retcode != 0:\n            raise click.UsageError(\n                \"`docker` command doesn't seem to be available. \"\n                \"It's required to package dependencies.\")\n\n    if not (requirements_file or repo_source_files or local_source_file):\n        click.secho(\n            'Warning: without --repo-source-files, --requirements-file, '\n            'or --local-source-file, nothing will be included in the zip file. '\n            'Assuming you have good reasons to do this and proceeding.',\n            fg='yellow')\n\n    fsutil.ensure_dirs(paths)\n\n    if isinstance(scm_source, fsutil.RemoteSource):\n        click.echo('Fetching repo..')\n        scm.fetch_repo(scm_source.url, scm_source.ref, paths.src_dir)\n\n    deps_file = None\n    if requirements_file:\n        click.echo('Building deps..')\n        deps_file = deps.build_deps(paths, requirements_file)\n\n    click.echo('Creating zip file..')\n    archive.make_archive(\n        paths,\n        repo_source_files,\n        local_source_file,\n        deps_file)\n\n    click.echo(os.path.relpath(paths.zip_path, os.getcwd()))", "response": "This function is the main function of the command line interface for the AWS Lambda package."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the starting and ending beta range from a dictionary h and J.", "response": "def _default_ising_beta_range(h, J):\n    \"\"\"Determine the starting and ending beta from h J\n\n    Args:\n        h (dict)\n\n        J (dict)\n\n    Assume each variable in J is also in h.\n\n    We use the minimum bias to give a lower bound on the minimum energy gap, such at the\n    final sweeps we are highly likely to settle into the current valley.\n    \"\"\"\n    # Get nonzero, absolute biases\n    abs_h = [abs(hh) for hh in h.values() if hh != 0]\n    abs_J = [abs(jj) for jj in J.values() if jj != 0]\n    abs_biases = abs_h + abs_J\n\n    if not abs_biases:\n        return [0.1, 1.0]\n\n    # Rough approximation of min change in energy when flipping a qubit\n    min_delta_energy = min(abs_biases)\n\n    # Combine absolute biases by variable\n    abs_bias_dict = {k: abs(v) for k, v in h.items()}\n    for (k1, k2), v in J.items():\n        abs_bias_dict[k1] += abs(v)\n        abs_bias_dict[k2] += abs(v)\n\n    # Find max change in energy when flipping a single qubit\n    max_delta_energy = max(abs_bias_dict.values())\n\n    # Selecting betas based on probability of flipping a qubit\n    # Hot temp: We want to scale hot_beta so that for the most unlikely qubit flip, we get at least\n    # 50% chance of flipping.(This means all other qubits will have > 50% chance of flipping\n    # initially.) Most unlikely flip is when we go from a very low energy state to a high energy\n    # state, thus we calculate hot_beta based on max_delta_energy.\n    #   0.50 = exp(-hot_beta * max_delta_energy)\n    #\n    # Cold temp: Towards the end of the annealing schedule, we want to minimize the chance of\n    # flipping. Don't want to be stuck between small energy tweaks. Hence, set cold_beta so that\n    # at minimum energy change, the chance of flipping is set to 1%.\n    #   0.01 = exp(-cold_beta * min_delta_energy)\n    hot_beta = np.log(2) / max_delta_energy\n    cold_beta = np.log(100) / min_delta_energy\n\n    return [hot_beta, cold_beta]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sample(self, bqm, beta_range=None, num_reads=10, sweeps=1000,\n               beta_schedule_type=\"geometric\", seed=None,\n               interrupt_function=None, initial_states=None):\n        \"\"\"Sample from a binary quadratic model using an implemented sample method.\n\n        Args:\n            bqm (:obj:`dimod.BinaryQuadraticModel`):\n                The binary quadratic model to be sampled.\n\n            beta_range (tuple, optional):\n                A 2-tuple defining the beginning and end of the beta schedule, where beta is the\n                inverse temperature. The schedule is applied linearly in beta. Default range is set\n                based on the total bias associated with each node.\n\n            num_reads (int, optional, default=10):\n                Each read is the result of a single run of the simulated annealing algorithm.\n\n            sweeps (int, optional, default=1000):\n                Number of sweeps or steps.\n\n            beta_schedule_type (string, optional, default='geometric'):\n                Beta schedule type, or how the beta values are interpolated between\n                the given 'beta_range'. Supported values are:\n\n                * linear\n                * geometric\n\n            seed (int, optional):\n                Seed to use for the PRNG. Specifying a particular seed with a constant\n                set of parameters produces identical results. If not provided, a random seed\n                is chosen.\n\n            initial_states (tuple(numpy.ndarray, dict), optional):\n                A tuple where the first value is a numpy array of initial states to seed the\n                simulated annealing runs, and the second is a dict defining a linear variable\n                labelling.\n\n            interrupt_function (function, optional):\n                If provided, interrupt_function is called with no parameters between each sample of\n                simulated annealing. If the function returns True, then simulated annealing will\n                terminate and return with all of the samples and energies found so far.\n\n        Returns:\n            :obj:`dimod.Response`: A `dimod` :obj:`~dimod.Response` object.\n\n        Examples:\n            This example runs simulated annealing on a binary quadratic model with some\n            different input parameters.\n\n            >>> import dimod\n            >>> import neal\n            ...\n            >>> sampler = neal.SimulatedAnnealingSampler()\n            >>> bqm = dimod.BinaryQuadraticModel({'a': .5, 'b': -.5}, {('a', 'b'): -1}, 0.0, dimod.SPIN)\n            >>> # Run with default parameters\n            >>> response = sampler.sample(bqm)\n            >>> # Run with specified parameters\n            >>> response = sampler.sample(bqm, seed=1234, beta_range=[0.1, 4.2],\n            ...                                num_reads=1, sweeps=20,\n            ...                                beta_schedule_type='geometric')\n            >>> # Reuse a seed\n            >>> a1 = next((sampler.sample(bqm, seed=88)).samples())['a']\n            >>> a2 = next((sampler.sample(bqm, seed=88)).samples())['a']\n            >>> a1 == a2\n            True\n\n        \"\"\"\n\n        # if already index-labelled, just continue\n        if all(v in bqm.linear for v in range(len(bqm))):\n            _bqm = bqm\n            use_label_map = False\n        else:\n            try:\n                inverse_mapping = dict(enumerate(sorted(bqm.linear)))\n            except TypeError:\n                # in python3 unlike types cannot be sorted\n                inverse_mapping = dict(enumerate(bqm.linear))\n            mapping = {v: i for i, v in iteritems(inverse_mapping)}\n\n            _bqm = bqm.relabel_variables(mapping, inplace=False)\n            use_label_map = True\n\n        # beta_range, sweeps are handled by simulated_annealing\n\n        if not isinstance(num_reads, Integral):\n            raise TypeError(\"'samples' should be a positive integer\")\n        if num_reads < 1:\n            raise ValueError(\"'samples' should be a positive integer\")\n\n        if not (seed is None or isinstance(seed, Integral)):\n            raise TypeError(\"'seed' should be None or a positive integer\")\n        if isinstance(seed, Integral) and not (0 < seed < (2**64 - 1)):\n            error_msg = \"'seed' should be an integer between 0 and 2^64 - 1\"\n            raise ValueError(error_msg)\n\n        if interrupt_function is None:\n            def interrupt_function():\n                return False\n\n        num_variables = len(_bqm)\n\n        # get the Ising linear biases\n        linear = _bqm.spin.linear\n        h = [linear[v] for v in range(num_variables)]\n\n        quadratic = _bqm.spin.quadratic\n        if len(quadratic) > 0:\n            couplers, coupler_weights = zip(*iteritems(quadratic))\n            couplers = map(lambda c: (c[0], c[1]), couplers)\n            coupler_starts, coupler_ends = zip(*couplers)\n        else:\n            coupler_starts, coupler_ends, coupler_weights = [], [], []\n\n        if beta_range is None:\n            beta_range = _default_ising_beta_range(linear, quadratic)\n\n        sweeps_per_beta = max(1, sweeps // 1000.0)\n        num_betas = int(math.ceil(sweeps / sweeps_per_beta))\n        if beta_schedule_type == \"linear\":\n            # interpolate a linear beta schedule\n            beta_schedule = np.linspace(*beta_range, num=num_betas)\n        elif beta_schedule_type == \"geometric\":\n            # interpolate a geometric beta schedule\n            beta_schedule = np.geomspace(*beta_range, num=num_betas)\n        else:\n            raise ValueError(\"Beta schedule type {} not implemented\".format(beta_schedule_type))\n\n        if seed is None:\n            # pick a random seed\n            seed = randint(0, (1 << 64 - 1))\n\n        np_rand = np.random.RandomState(seed % 2**32)\n\n        states_shape = (num_reads, num_variables)\n\n        if initial_states is not None:\n            initial_states_array, init_label_map = initial_states\n            if not initial_states_array.shape == states_shape:\n                raise ValueError(\"`initial_states` must have shape \"\n                                 \"{}\".format(states_shape))\n            if init_label_map is not None:\n                get_label = inverse_mapping.get if use_label_map else lambda i: i\n                initial_states_array = initial_states_array[:, [init_label_map[get_label(i)] for i in range(num_variables)]]\n            numpy_initial_states = np.ascontiguousarray(initial_states_array, dtype=np.int8)\n        else:\n            numpy_initial_states = 2*np_rand.randint(2, size=(num_reads, num_variables)).astype(np.int8) - 1\n\n        # run the simulated annealing algorithm\n        samples, energies = sa.simulated_annealing(num_reads, h,\n                                                   coupler_starts, coupler_ends,\n                                                   coupler_weights,\n                                                   sweeps_per_beta, beta_schedule,\n                                                   seed,\n                                                   numpy_initial_states,\n                                                   interrupt_function)\n        off = _bqm.spin.offset\n        info = {\n            \"beta_range\": beta_range,\n            \"beta_schedule_type\": beta_schedule_type\n        }\n        response = dimod.SampleSet.from_samples(\n            samples,\n            energy=energies+off,\n            info=info,\n            vartype=dimod.SPIN\n        )\n\n        response.change_vartype(_bqm.vartype, inplace=True)\n        if use_label_map:\n            response.relabel_variables(inverse_mapping, inplace=True)\n\n        return response", "response": "Sample from a binary quadratic model using an implemented sample method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fit(self, X, y):\n        self._X = X  # matrix of predictive variables ('independent variables')\n        self._y = y  # vector of values for outcome variable ('dependent variable')\n\n        # Set up the properties for ReliefF -------------------------------------------------------------------------------------\n        self._datalen = len(self._X)  # Number of training instances ('n')\n\n        \"\"\"\"Below: Handles special case where user requests that a proportion of training instances be neighbors for\n        ReliefF rather than a specified 'k' number of neighbors.  Note that if k is specified, then k 'hits' and k\n        'misses' will be used to update feature scores.  Thus total number of neighbors is 2k. If instead a proportion\n        is specified (say 0.1 out of 1000 instances) this represents the total number of neighbors (e.g. 100). In this\n        case, k would be set to 50 (i.e. 50 hits and 50 misses). \"\"\"\n        if hasattr(self, 'n_neighbors') and type(self.n_neighbors) is float:\n            # Halve the number of neighbors because ReliefF uses n_neighbors matches and n_neighbors misses\n            self.n_neighbors = int(self.n_neighbors * self._datalen * 0.5)\n\n        # Number of unique outcome (label) values (used to determine outcome variable type)\n        self._label_list = list(set(self._y))\n        # Determine if label is discrete\n        discrete_label = (len(self._label_list) <= self.discrete_threshold)\n\n        # Identify label type (binary, multiclass, or continuous)\n        if discrete_label:\n            if len(self._label_list) == 2:\n                self._class_type = 'binary'\n                self.mcmap = 0\n            elif len(self._label_list) > 2:\n                self._class_type = 'multiclass'\n                self.mcmap = self._getMultiClassMap()\n            else:\n                raise ValueError('All labels are of the same class.')\n\n        else:\n            self._class_type = 'continuous'\n            self.mcmap = 0\n\n        # Training labels standard deviation -- only used if the training labels are continuous\n        self._labels_std = 0.\n        if len(self._label_list) > self.discrete_threshold:\n            self._labels_std = np.std(self._y, ddof=1)\n\n        self._num_attributes = len(self._X[0])  # Number of features in training data\n        \n        # Number of missing data values in predictor variable matrix.\n        self._missing_data_count = np.isnan(self._X).sum()\n\n        \"\"\"Assign internal headers for the features (scikit-learn does not accept external headers from dataset):\n        The pre_normalize() function relies on the headers being ordered, e.g., X01, X02, etc.\n        If this is changed, then the sort in the pre_normalize() function needs to be adapted as well. \"\"\"\n        xlen = len(self._X[0])\n        mxlen = len(str(xlen + 1))\n        self._headers = ['X{}'.format(str(i).zfill(mxlen)) for i in range(1, xlen + 1)]\n\n        start = time.time()  # Runtime tracking\n\n        # Determine data types for all features/attributes in training data (i.e. discrete or continuous)\n        C = D = False\n        # Examines each feature and applies discrete_threshold to determine variable type.\n        self.attr = self._get_attribute_info()\n        for key in self.attr.keys():\n            if self.attr[key][0] == 'discrete':\n                D = True\n            if self.attr[key][0] == 'continuous':\n                C = True\n\n        # For downstream computational efficiency, determine if dataset is comprised of all discrete, all continuous, or a mix of discrete/continuous features.\n        if C and D:\n            self.data_type = 'mixed'\n        elif D and not C:\n            self.data_type = 'discrete'\n        elif C and not D:\n            self.data_type = 'continuous'\n        else:\n            raise ValueError('Invalid data type in data set.')\n        #--------------------------------------------------------------------------------------------------------------------\n\n        # Compute the distance array between all data points ----------------------------------------------------------------\n        # For downstream efficiency, separate features in dataset by type (i.e. discrete/continuous)\n        diffs, cidx, didx = self._dtype_array()\n        cdiffs = diffs[cidx]  # max/min continuous value difference for continuous features.\n\n        xc = self._X[:, cidx]  # Subset of continuous-valued feature data\n        xd = self._X[:, didx]  # Subset of discrete-valued feature data\n\n        \"\"\" For efficiency, the distance array is computed more efficiently for data with no missing values.\n        This distance array will only be used to identify nearest neighbors. \"\"\"\n        if self._missing_data_count > 0:\n            self._distance_array = self._distarray_missing(xc, xd, cdiffs)\n        else:\n            self._distance_array = self._distarray_no_missing(xc, xd)\n\n        if self.verbose:\n            elapsed = time.time() - start\n            print('Created distance array in {} seconds.'.format(elapsed))\n            print('Feature scoring under way ...')\n\n        start = time.time()\n       #--------------------------------------------------------------------------------------------------------------------\n\n       # Run remainder of algorithm (i.e. identification of 'neighbors' for each instance, and feature scoring).------------\n        # Stores feature importance scores for ReliefF or respective Relief-based algorithm.\n        self.feature_importances_ = self._run_algorithm()\n\n        # Delete the internal distance array because it is no longer needed\n        del self._distance_array\n\n        if self.verbose:\n            elapsed = time.time() - start\n            print('Completed scoring in {} seconds.'.format(elapsed))\n\n        # Compute indices of top features\n        self.top_features_ = np.argsort(self.feature_importances_)[::-1]\n\n        return self", "response": "Fits the ReliefF model to the training data X and y."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform(self, X):\n        if self._num_attributes < self.n_features_to_select:\n            raise ValueError('Number of features to select is larger than the number of features in the dataset.')\n        \n        return X[:, self.top_features_[:self.n_features_to_select]]", "response": "Scikit - learn required : Reduces the feature set down to the top n_features_to_select features."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_attribute_info(self):\n        attr = dict()\n        d = 0\n        limit = self.discrete_threshold\n        w = self._X.transpose()\n\n        for idx in range(len(w)):\n            h = self._headers[idx]\n            z = w[idx]\n            if self._missing_data_count > 0:\n                z = z[np.logical_not(np.isnan(z))]  # Exclude any missing values from consideration\n            zlen = len(np.unique(z))\n            if zlen <= limit:\n                attr[h] = ('discrete', 0, 0, 0, 0)\n                d += 1\n            else:\n                mx = np.max(z)\n                mn = np.min(z)\n                sd = np.std(z)\n                attr[h] = ('continuous', mx, mn, mx - mn, sd)\n        # For each feature/attribute we store (type, max value, min value, max min difference, average, standard deviation) - the latter three values are set to zero if feature is discrete.\n        return attr", "response": "Preprocess the training dataset to identify which features and attributes are discrete vs. continuous valued. Ignores missing values in this determination. Ignores missing values in this determination."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _distarray_no_missing(self, xc, xd):\n        from scipy.spatial.distance import pdist, squareform\n\n        #------------------------------------------#\n        def pre_normalize(x):\n            \"\"\"Normalizes continuous features so they are in the same range (0 to 1)\"\"\"\n            idx = 0\n            # goes through all named features (doesn really need to) this method is only applied to continuous features\n            for i in sorted(self.attr.keys()):\n                if self.attr[i][0] == 'discrete':\n                    continue\n                cmin = self.attr[i][2]\n                diff = self.attr[i][3]\n                x[:, idx] -= cmin\n                x[:, idx] /= diff\n                idx += 1\n            return x\n        #------------------------------------------#\n\n        if self.data_type == 'discrete':  # discrete features only\n            return squareform(pdist(self._X, metric='hamming'))\n        elif self.data_type == 'mixed':  # mix of discrete and continuous features\n            d_dist = squareform(pdist(xd, metric='hamming'))\n            # Cityblock is also known as Manhattan distance\n            c_dist = squareform(pdist(pre_normalize(xc), metric='cityblock'))\n            return np.add(d_dist, c_dist) / self._num_attributes\n\n        else: #continuous features only\n            #xc = pre_normalize(xc)\n            return squareform(pdist(pre_normalize(xc), metric='cityblock'))", "response": "Calculates the distance array for the data with no missing values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dtype_array(self):\n        attrtype = []\n        attrdiff = []\n\n        for key in self._headers:\n            if self.attr[key][0] == 'continuous':\n                attrtype.append(1)\n            else:\n                attrtype.append(0)\n            attrdiff.append(self.attr[key][3])\n\n        attrtype = np.array(attrtype)\n        cidx = np.where(attrtype == 1)[0]\n        didx = np.where(attrtype == 0)[0]\n\n        attrdiff = np.array(attrdiff)\n\n        return attrdiff, cidx, didx", "response": "Return array of max and min diffs of attributes. Return array of discrete 0 / continuous 1 attributes and their indices. Return array of max and min diffs of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _distarray_missing(self, xc, xd, cdiffs):\n        cindices = []\n        dindices = []\n        # Get Boolean mask locating missing values for continuous and discrete features separately. These correspond to xc and xd respectively.\n        for i in range(self._datalen):\n            cindices.append(np.where(np.isnan(xc[i]))[0])\n            dindices.append(np.where(np.isnan(xd[i]))[0])\n\n        if self.n_jobs != 1:\n            dist_array = Parallel(n_jobs=self.n_jobs)(delayed(get_row_missing)(\n                xc, xd, cdiffs, index, cindices, dindices) for index in range(self._datalen))\n        else:\n            # For each instance calculate distance from all other instances (in non-redundant manner) (i.e. computes triangle, and puts zeros in for rest to form square).\n            dist_array = [get_row_missing(xc, xd, cdiffs, index, cindices, dindices)\n                          for index in range(self._datalen)]\n\n        return np.array(dist_array)", "response": "Calculates the distance array for data with missing values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nidentifies k nearest hits and k nearest misses for given instance.", "response": "def _find_neighbors(self, inst):\n        \"\"\" Identify k nearest hits and k nearest misses for given instance. This is accomplished differently based on the type of endpoint (i.e. binary, multiclass, and continuous). \"\"\"\n        # Make a vector of distances between target instance (inst) and all others\n        dist_vect = []\n        for j in range(self._datalen):\n            if inst != j:\n                locator = [inst, j]\n                if inst < j:\n                    locator.reverse()\n                dist_vect.append(self._distance_array[locator[0]][locator[1]])\n            else:\n                # Ensures that target instance is never selected as neighbor.\n                dist_vect.append(sys.maxsize)\n\n        dist_vect = np.array(dist_vect)\n\n        # Identify neighbors-------------------------------------------------------\n        \"\"\" NN for Binary Endpoints: \"\"\"\n        if self._class_type == 'binary':\n            nn_list = []\n            match_count = 0\n            miss_count = 0\n            for nn_index in np.argsort(dist_vect):\n                if self._y[inst] == self._y[nn_index]:  # Hit neighbor identified\n                    if match_count >= self.n_neighbors:\n                        continue\n                    nn_list.append(nn_index)\n                    match_count += 1\n                else:  # Miss neighbor identified\n                    if miss_count >= self.n_neighbors:\n                        continue\n                    nn_list.append(nn_index)\n                    miss_count += 1\n\n                if match_count >= self.n_neighbors and miss_count >= self.n_neighbors:\n                    break\n\n        elif self._class_type == 'multiclass':\n            nn_list = []\n            match_count = 0\n            miss_count = dict.fromkeys(self._label_list, 0)\n            for nn_index in np.argsort(dist_vect):\n                if self._y[inst] == self._y[nn_index]:  # Hit neighbor identified\n                    if match_count >= self.n_neighbors:\n                        continue\n                    nn_list.append(nn_index)\n                    match_count += 1\n                else:\n                    for label in self._label_list:\n                        if self._y[nn_index] == label:\n                            if miss_count[label] >= self.n_neighbors:\n                                continue\n                            nn_list.append(nn_index)\n                            miss_count[label] += 1\n\n                if match_count >= self.n_neighbors and all(v >= self.n_neighbors for v in miss_count.values()):\n                    break\n        else:\n            nn_list = []\n            match_count = 0\n            miss_count = 0\n            for nn_index in np.argsort(dist_vect):\n                if abs(self._y[inst]-self._y[nn_index]) < self._labels_std:  # Hit neighbor identified\n                    if match_count >= self.n_neighbors:\n                        continue\n                    nn_list.append(nn_index)\n                    match_count += 1\n                else:  # Miss neighbor identified\n                    if miss_count >= self.n_neighbors:\n                        continue\n                    nn_list.append(nn_index)\n                    miss_count += 1\n\n                if match_count >= self.n_neighbors and miss_count >= self.n_neighbors:\n                    break\n        return np.array(nn_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, X, y, headers):\n\n        self.X_mat = X\n        self._y = y\n        self.headers = headers\n        self._num_attributes = len(self.X_mat[0]) \n        self._lost = {}\n        \n        #Combine TuRF with specified 'core' Relief-based algorithm\n        if self.core_algorithm.lower() == \"multisurf\":\n            core = MultiSURF(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"multisurfstar\":\n            core = MultiSURFstar(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"surf\":\n            core = SURF(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"surfstar\":\n            core = SURFstar(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"relieff\":\n            core = ReliefF(n_features_to_select=self.n_features_to_select, n_neighbors=self.n_neighbors, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        num_features = X.shape[1]\n\n        iter_count = 0\n        features_iter = []\n        headers_iter = []\n        feature_retain_check = 0\n        \n        #Determine maximum number of iterations. \n        iterMax = int(1/float(self.pct))\n        \n        #Main iterative loop of TuRF\n        while(iter_count < iterMax):\n            #Run Core Relief-based algorithm\n            core_fit = core.fit(self.X_mat, self._y)\n            features_iter.append(core_fit.feature_importances_) #HISTORY\n            headers_iter.append(self.headers) #HISTORY\n            \n            #Calculate features to keep\n            perc_retain = 1 - self.pct\n            feature_retain = int(np.round(num_features*perc_retain))\n            \n            # Edge case (ensures that each iteration, at least one feature is removed)\n            if feature_retain == feature_retain_check:\n                feature_retain -= 1\n\n            num_features = feature_retain\n            feature_retain_check = feature_retain\n            #Identify the index location of the top 'num_feature' scoring features (for this particular iteration)\n            select = np.array(features_iter[iter_count].argsort()[-num_features:])\n            #Make index list of features not removed\n            non_select = np.array(features_iter[iter_count].argsort()[:num_features])\n            #Make a dictionary that stores dropped features and the iteration they were dropped.\n            for i in non_select:\n                self._lost[self.headers[i]] = iterMax - iter_count #For feature name, store iteration rank it was removed (bigger rank for sooner removal)\n                \n            #Drop non-selected features and headers. \n            self.X_mat = self.X_mat[:, select] #select all instances and only features indexed from select. \n            self.headers = [self.headers[i] for i in select]\n\n            iter_count += 1\n\n        #Final scoring iteration\n        core_fit = core.fit(self.X_mat, self._y)\n        features_iter.append(core_fit.feature_importances_) #HISTORY\n        headers_iter.append(self.headers) #HISTORY\n        iter_count += 1\n\n        self.num_iter = iter_count\n        self.feature_history = list(zip(headers_iter, features_iter)) #HISTORY\n\n        #Prepare for assigning token scores to features that had been removed in a previous TuRF iteration.  These scores are only meaningful in that they give an idea of when these feature(s) were removed. \n        low_score = min(core_fit.feature_importances_)\n        reduction = 0.01 * (max(core_fit.feature_importances_) - low_score)\n\n        #For consistency we report feature importances ordered in same way as original dataset.  Same is true for headers. \n        #Step through each feature name\n        self.feature_importances_= []\n\n        for i in headers_iter[0]:\n            #Check lost dictionary\n            if i in self._lost:\n                self.feature_importances_.append(low_score - reduction * self._lost[i]) #append discounted score as a marker of when the feature was removed. \n            else: #Feature made final cut\n                score_index = self.headers.index(i)\n                self.feature_importances_.append(core_fit.feature_importances_[score_index])\n\n        #Turn feature imporance list into array\n        self.feature_importances_= np.array(self.feature_importances_)\n        #self.feature_importances_ = core_fit.feature_importances_\n        \n        self.top_features_ = [headers.index(i) for i in self.headers]\n        self.top_features_ = self.top_features_[::-1]\n        return self", "response": "Fits the TuRF to compute the feature importance scores at each iteration."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nidentifies nearest as well as farthest hits and misses within radius over whole distance array.", "response": "def _find_neighbors(self, inst, avg_dist):\n        \"\"\" Identify nearest as well as farthest hits and misses within radius defined by average distance over whole distance array.\n        This works the same regardless of endpoint type. \"\"\"\n        NN_near = []\n        NN_far = []\n        min_indices = []\n        max_indices = []\n\n        for i in range(self._datalen):\n            if inst != i:\n                locator = [inst, i]\n                if i > inst:\n                    locator.reverse()\n                d = self._distance_array[locator[0]][locator[1]]\n                if d < avg_dist:\n                    min_indices.append(i)\n                if d > avg_dist:\n                    max_indices.append(i)\n\n        for i in range(len(min_indices)):\n            NN_near.append(min_indices[i])\n        for i in range(len(max_indices)):\n            NN_far.append(max_indices[i])\n\n        return np.array(NN_near, dtype=np.int32), np.array(NN_far, dtype=np.int32)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the missing values for each instance in a row.", "response": "def get_row_missing(xc, xd, cdiffs, index, cindices, dindices):\r\n    \"\"\" Calculate distance between index instance and all other instances. \"\"\"\r\n    row = np.empty(0, dtype=np.double)  # initialize empty row\r\n    cinst1 = xc[index]  # continuous-valued features for index instance\r\n    dinst1 = xd[index]  # discrete-valued features for index instance\r\n    # Boolean mask locating missing values for continuous features for index instance\r\n    can = cindices[index]\r\n    # Boolean mask locating missing values for discrete features for index instance\r\n    dan = dindices[index]\r\n    tf = len(cinst1) + len(dinst1)  # total number of features.\r\n\r\n    # Progressively compare current instance to all others. Excludes comparison with self indexed instance. (Building the distance matrix triangle).\r\n    for j in range(index):\r\n        dist = 0\r\n        dinst2 = xd[j]  # discrete-valued features for compared instance\r\n        cinst2 = xc[j]  # continuous-valued features for compared instance\r\n\r\n        # Manage missing values in discrete features\r\n        # Boolean mask locating missing values for discrete features for compared instance\r\n        dbn = dindices[j]\r\n        # indexes where there is at least one missing value in the feature between an instance pair.\r\n        idx = np.unique(np.append(dan, dbn))\r\n        # Number of features excluded from distance calculation due to one or two missing values within instance pair. Used to normalize distance values for comparison.\r\n        dmc = len(idx)\r\n        d1 = np.delete(dinst1, idx)  # delete unique missing features from index instance\r\n        d2 = np.delete(dinst2, idx)  # delete unique missing features from compared instance\r\n\r\n        # Manage missing values in continuous features\r\n        # Boolean mask locating missing values for continuous features for compared instance\r\n        cbn = cindices[j]\r\n        # indexes where there is at least one missing value in the feature between an instance pair.\r\n        idx = np.unique(np.append(can, cbn))\r\n        # Number of features excluded from distance calculation due to one or two missing values within instance pair. Used to normalize distance values for comparison.\r\n        cmc = len(idx)\r\n        c1 = np.delete(cinst1, idx)  # delete unique missing features from index instance\r\n        c2 = np.delete(cinst2, idx)  # delete unique missing features from compared instance\r\n        # delete unique missing features from continuous value difference scores\r\n        cdf = np.delete(cdiffs, idx)\r\n\r\n        # Add discrete feature distance contributions (missing values excluded) - Hamming distance\r\n        dist += len(d1[d1 != d2])\r\n\r\n        # Add continuous feature distance contributions (missing values excluded) - Manhattan distance (Note that 0-1 continuous value normalization is included ~ subtraction of minimums cancel out)\r\n        dist += np.sum(np.absolute(np.subtract(c1, c2)) / cdf)\r\n\r\n        # Normalize distance calculation based on total number of missing values bypassed in either discrete or continuous features.\r\n        tnmc = tf - dmc - cmc  # Total number of unique missing counted\r\n        # Distance normalized by number of features included in distance sum (this seeks to handle missing values neutrally in distance calculation)\r\n        dist = dist/float(tnmc)\r\n\r\n        row = np.append(row, dist)\r\n\r\n    return row"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_score(attr, mcmap, NN, feature, inst, nan_entries, headers, class_type, X, y, labels_std, data_type, near=True):\r\n\r\n    fname = headers[feature]  # feature identifier\r\n    ftype = attr[fname][0]  # feature type\r\n    ctype = class_type  # class type (binary, multiclass, continuous)\r\n    diff_hit = diff_miss = 0.0  # Tracks the score contribution\r\n    # Tracks the number of hits/misses. Used in normalizing scores by 'k' in ReliefF, and by m or h in SURF, SURF*, MultiSURF*, and MultiSURF\r\n    count_hit = count_miss = 0.0\r\n    # Initialize 'diff' (The score contribution for this target instance and feature over all NN)\r\n    diff = 0\r\n    # mmdiff = attr[fname][3] # Max/Min range of values for target feature\r\n\r\n    datalen = float(len(X))\r\n\r\n    # If target instance is missing, then a 'neutral' score contribution of 0 is returned immediately since all NN comparisons will be against this missing value.\r\n    if nan_entries[inst][feature]:\r\n        return 0.\r\n    # Note missing data normalization below regarding missing NN feature values is accomplished by counting hits and misses (missing values are not counted) (happens in parallel with hit/miss imbalance normalization)\r\n\r\n    xinstfeature = X[inst][feature]  # value of target instances target feature.\r\n\r\n    #--------------------------------------------------------------------------\r\n    if ctype == 'binary':\r\n        for i in range(len(NN)):\r\n            if nan_entries[NN[i]][feature]:  # skip any NN with a missing value for this feature.\r\n                continue\r\n\r\n            xNNifeature = X[NN[i]][feature]\r\n\r\n            if near:  # SCORING FOR NEAR INSTANCES\r\n                if y[inst] == y[NN[i]]:   # HIT\r\n                    count_hit += 1\r\n                    if ftype == 'continuous':\r\n                        # diff_hit -= abs(xinstfeature - xNNifeature) / mmdiff #Normalize absolute value of feature value difference by max-min value range for feature (so score update lies between 0 and 1)\r\n                        diff_hit -= ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)\r\n                    else:  # discrete feature\r\n                        if xinstfeature != xNNifeature:  # A difference in feature value is observed\r\n                            # Feature score is reduced when we observe feature difference between 'near' instances with the same class.\r\n                            diff_hit -= 1\r\n                else:  # MISS\r\n                    count_miss += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_miss += abs(xinstfeature - xNNifeature) / mmdiff\r\n                        diff_miss += ramp_function(data_type, attr, fname,\r\n                                                   xinstfeature, xNNifeature)\r\n                    else:  # discrete feature\r\n                        if xinstfeature != xNNifeature:  # A difference in feature value is observed\r\n                            # Feature score is increase when we observe feature difference between 'near' instances with different class values.\r\n                            diff_miss += 1\r\n\r\n            else:  # SCORING FOR FAR INSTANCES (ONLY USED BY MULTISURF* BASED ON HOW CODED)\r\n                if y[inst] == y[NN[i]]:   # HIT\r\n                    count_hit += 1\r\n                    if ftype == 'continuous':\r\n\r\n                        #diff_hit -= abs(xinstfeature - xNNifeature) / mmdiff  #Hits differently add continuous value differences rather than subtract them \r\n                        diff_hit -= (1-ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)) #Sameness should yield most negative score\r\n                    else: #discrete feature\r\n                        if xinstfeature == xNNifeature: # The same feature value is observed (Used for more efficient 'far' scoring, since there should be fewer same values for 'far' instances)\r\n                            diff_hit -= 1 # Feature score is reduced when we observe the same feature value between 'far' instances with the same class.\r\n                else:  # MISS\r\n                    count_miss += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_miss += abs(xinstfeature - xNNifeature) / mmdiff #Misses differntly subtract continuous value differences rather than add them \r\n                        diff_miss += (1-ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)) #Sameness should yield most negative score\r\n                    else: #discrete feature\r\n                        if xinstfeature == xNNifeature: # The same feature value is observed (Used for more efficient 'far' scoring, since there should be fewer same values for 'far' instances)\r\n                            diff_miss += 1 # Feature score is increased when we observe the same feature value between 'far' instances with different class values.\r\n\r\n        \"\"\" Score Normalizations:\r\n        *'n' normalization dividing by the number of training instances (this helps ensure that all final scores end up in the -1 to 1 range\r\n        *'k','h','m' normalization dividing by the respective number of hits and misses in NN (after ignoring missing values), also helps account for class imbalance within nearest neighbor radius)\"\"\"\r\n        if count_hit == 0.0 or count_miss == 0.0:  # Special case, avoid division error\r\n            if count_hit == 0.0 and count_miss == 0.0:\r\n                return 0.0\r\n            elif count_hit == 0.0:\r\n                diff = (diff_miss / count_miss) / datalen\r\n            else:  # count_miss == 0.0\r\n                diff = (diff_hit / count_hit) / datalen\r\n        else:  # Normal diff normalization\r\n            diff = ((diff_hit / count_hit) + (diff_miss / count_miss)) / datalen\r\n\r\n    #--------------------------------------------------------------------------\r\n    elif ctype == 'multiclass':\r\n        class_store = dict() #only 'miss' classes will be stored\r\n        #missClassPSum = 0\r\n\r\n        for each in mcmap:\r\n            if(each != y[inst]):  # Identify miss classes for current target instance.\r\n                class_store[each] = [0, 0]\r\n                #missClassPSum += mcmap[each]\r\n\r\n        for i in range(len(NN)):\r\n            if nan_entries[NN[i]][feature]:  # skip any NN with a missing value for this feature.\r\n                continue\r\n\r\n            xNNifeature = X[NN[i]][feature]\r\n\r\n            if near:  # SCORING FOR NEAR INSTANCES\r\n                if(y[inst] == y[NN[i]]):  # HIT\r\n                    count_hit += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_hit -= abs(xinstfeature - xNNifeature) / mmdiff\r\n                        diff_hit -= ramp_function(data_type, attr, fname, xinstfeature, xNNifeature) \r\n                    else: #discrete feature\r\n                        if xinstfeature != xNNifeature:\r\n                            # Feature score is reduced when we observe feature difference between 'near' instances with the same class.\r\n                            diff_hit -= 1\r\n                else:  # MISS\r\n                    for missClass in class_store:\r\n                        if(y[NN[i]] == missClass):  # Identify which miss class is present\r\n                            class_store[missClass][0] += 1\r\n                            if ftype == 'continuous':\r\n                                #class_store[missClass][1] += abs(xinstfeature - xNNifeature) / mmdiff\r\n                                class_store[missClass][1] += ramp_function(\r\n                                    data_type, attr, fname, xinstfeature, xNNifeature)\r\n                            else:  # discrete feature\r\n                                if xinstfeature != xNNifeature:\r\n                                    # Feature score is increase when we observe feature difference between 'near' instances with different class values.\r\n                                    class_store[missClass][1] += 1\r\n\r\n            else:  # SCORING FOR FAR INSTANCES (ONLY USED BY MULTISURF* BASED ON HOW CODED)\r\n                if(y[inst] == y[NN[i]]):  # HIT\r\n                    count_hit += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_hit -= abs(xinstfeature - xNNifeature) / mmdiff  #Hits differently add continuous value differences rather than subtract them \r\n                        diff_hit -= (1-ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)) #Sameness should yield most negative score\r\n                    else: #discrete features\r\n                        if xinstfeature == xNNifeature:\r\n                            # Feature score is reduced when we observe the same feature value between 'far' instances with the same class.\r\n                            diff_hit -= 1\r\n                else:  # MISS\r\n                    for missClass in class_store:\r\n                        if(y[NN[i]] == missClass):\r\n                            class_store[missClass][0] += 1\r\n                            if ftype == 'continuous':\r\n                                #class_store[missClass][1] += abs(xinstfeature - xNNifeature) / mmdiff\r\n                                class_store[missClass][1] += (1-ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)) #Sameness should yield most negative score\r\n                            else: #discrete feature\r\n                                if xinstfeature == xNNifeature:\r\n                                    # Feature score is increased when we observe the same feature value between 'far' instances with different class values.\r\n                                    class_store[missClass][1] += 1\r\n\r\n        \"\"\" Score Normalizations:\r\n        *'n' normalization dividing by the number of training instances (this helps ensure that all final scores end up in the -1 to 1 range\r\n        *'k','h','m' normalization dividing by the respective number of hits and misses in NN (after ignoring missing values), also helps account for class imbalance within nearest neighbor radius)\r\n        * multiclass normalization - accounts for scoring by multiple miss class, so miss scores don't have too much weight in contrast with hit scoring. If a given miss class isn't included in NN\r\n        then this normalization will account for that possibility. \"\"\"\r\n        # Miss component\r\n        for each in class_store:\r\n            count_miss += class_store[each][0]\r\n\r\n        if count_hit == 0.0 and count_miss == 0.0:\r\n            return 0.0\r\n        else:\r\n            if count_miss == 0:\r\n                pass\r\n            else: #Normal diff normalization\r\n                for each in class_store: #multiclass normalization\r\n                    diff += class_store[each][1] * (class_store[each][0] / count_miss) * len(class_store)# Contribution of given miss class weighted by it's observed frequency within NN set.\r\n                diff = diff / count_miss #'m' normalization\r\n            \r\n            #Hit component: with 'h' normalization\r\n            if count_hit == 0:\r\n                pass\r\n            else:\r\n                diff += (diff_hit / count_hit)\r\n\r\n        diff = diff / datalen  # 'n' normalization\r\n\r\n    #--------------------------------------------------------------------------\r\n    else:  # CONTINUOUS endpoint\r\n        same_class_bound = labels_std\r\n\r\n        for i in range(len(NN)):\r\n            if nan_entries[NN[i]][feature]:  # skip any NN with a missing value for this feature.\r\n                continue\r\n\r\n            xNNifeature = X[NN[i]][feature]\r\n\r\n            if near:  # SCORING FOR NEAR INSTANCES\r\n                if abs(y[inst] - y[NN[i]]) < same_class_bound:  # HIT approximation\r\n                    count_hit += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_hit -= abs(xinstfeature - xNNifeature) / mmdiff\r\n                        diff_hit -= ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)\r\n                    else:  # discrete feature\r\n                        if xinstfeature != xNNifeature:\r\n                            # Feature score is reduced when we observe feature difference between 'near' instances with the same 'class'.\r\n                            diff_hit -= 1\r\n                else:  # MISS approximation\r\n                    count_miss += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_miss += abs(xinstfeature - xNNifeature) / mmdiff\r\n                        diff_miss += ramp_function(data_type, attr, fname,\r\n                                                   xinstfeature, xNNifeature)\r\n                    else:  # discrete feature\r\n                        if xinstfeature != xNNifeature:\r\n                            # Feature score is increase when we observe feature difference between 'near' instances with different class value.\r\n                            diff_miss += 1\r\n\r\n            else:  # SCORING FOR FAR INSTANCES (ONLY USED BY MULTISURF* BASED ON HOW CODED)\r\n                if abs(y[inst] - y[NN[i]]) < same_class_bound:  # HIT approximation\r\n                    count_hit += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_hit += abs(xinstfeature - xNNifeature) / mmdiff\r\n                        diff_hit -= (1-ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)) #Sameness should yield most negative score\r\n                    else: #discrete feature\r\n                        if xinstfeature == xNNifeature:\r\n                            # Feature score is reduced when we observe the same feature value between 'far' instances with the same class.\r\n                            diff_hit -= 1\r\n                else:  # MISS approximation\r\n                    count_miss += 1\r\n                    if ftype == 'continuous':\r\n                        #diff_miss -= abs(xinstfeature - xNNifeature) / mmdiff\r\n                        diff_miss += (1-ramp_function(data_type, attr, fname, xinstfeature, xNNifeature)) #Sameness should yield most negative score\r\n                    else: #discrete feature\r\n                        if xinstfeature == xNNifeature:\r\n                            # Feature score is increased when we observe the same feature value between 'far' instances with different class values.\r\n                            diff_miss += 1\r\n\r\n        \"\"\" Score Normalizations:\r\n        *'n' normalization dividing by the number of training instances (this helps ensure that all final scores end up in the -1 to 1 range\r\n        *'k','h','m' normalization dividing by the respective number of hits and misses in NN (after ignoring missing values), also helps account for class imbalance within nearest neighbor radius)\"\"\"\r\n\r\n        if count_hit == 0.0 or count_miss == 0.0:  # Special case, avoid division error\r\n            if count_hit == 0.0 and count_miss == 0.0:\r\n                return 0.0\r\n            elif count_hit == 0.0:\r\n                diff = (diff_miss / count_miss) / datalen\r\n            else:  # count_miss == 0.0\r\n                diff = (diff_hit / count_hit) / datalen\r\n        else:  # Normal diff normalization\r\n            diff = ((diff_hit / count_hit) + (diff_miss / count_miss)) / datalen\r\n\r\n    return diff", "response": "Compute the score of a single target instance and feature over all items in NN."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the scores for a given instance.", "response": "def ReliefF_compute_scores(inst, attr, nan_entries, num_attributes, mcmap, NN, headers, class_type, X, y, labels_std, data_type):\r\n    \"\"\" Unique scoring procedure for ReliefF algorithm. Scoring based on k nearest hits and misses of current target instance. \"\"\"\r\n    scores = np.zeros(num_attributes)\r\n    for feature_num in range(num_attributes):\r\n        scores[feature_num] += compute_score(attr, mcmap, NN, feature_num, inst,\r\n                                             nan_entries, headers, class_type, X, y, labels_std, data_type)\r\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the algorithm for the nearest neighbor identification and feature scoring.", "response": "def _run_algorithm(self):\n        \"\"\" Runs nearest neighbor (NN) identification and feature scoring to yield MultiSURF scores. \"\"\"\n        nan_entries = np.isnan(self._X)\n\n        NNlist = [self._find_neighbors(datalen) for datalen in range(self._datalen)]\n\n        scores = np.sum(Parallel(n_jobs=self.n_jobs)(delayed(\n            MultiSURF_compute_scores)(instance_num, self.attr, nan_entries, self._num_attributes, self.mcmap,\n                                      NN_near, self._headers, self._class_type, self._X, self._y, self._labels_std, self.data_type)\n            for instance_num, NN_near in zip(range(self._datalen), NNlist)), axis=0)\n\n        return np.array(scores)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit(self, X, y, headers):\n\n        self.X_mat = X\n        self._y = y\n        self.headers = headers\n\n        if self.core_algorithm.lower() == \"multisurf\":\n            core = MultiSURF(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"multisurfstar\":\n            core = MultiSURFstar(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"surf\":\n            core = SURF(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"surfstar\":\n            core = SURFstar(n_features_to_select=self.n_features_to_select, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        elif self.core_algorithm.lower() == \"relieff\":\n            core = ReliefF(n_features_to_select=self.n_features_to_select, n_neighbors=self.n_neighbors, discrete_threshold=self.discrete_threshold, verbose=self.verbose, n_jobs=self.n_jobs)\n\n        total_num_features = X.shape[1]\n        num_features = self.size_feature_subset\n        features_scores_iter = []\n        headers_iter = []\n        features_selected = []\n\n        for iteration in range(self.num_feature_subset):\n            features_selected_id = np.random.choice(\n                range(total_num_features), num_features, replace=False)\n            self.X_train = self.X_mat[:, features_selected_id]\n\n            core_fit = core.fit(self.X_train, self._y)\n\n            features_scores_iter.append(core_fit.feature_importances_)\n            features_selected.append(features_selected_id)\n            # headers_iter.append(self.headers[features_selected_id])\n\n        self.features_scores_iter = features_scores_iter\n        self.features_selected = features_selected\n\n        zip_feat_score = [list(zip(features_selected[i], features_scores_iter[i]))\n                          for i in range(len(features_selected))]\n        feat_score = sorted([item for sublist in zip_feat_score for item in sublist])\n        feat_score_df = pd.DataFrame(feat_score)\n        feat_score_df.columns = ['feature', 'score']\n        feat_score_df = feat_score_df.groupby('feature').max().reset_index()\n\n        feature_scores = feat_score_df.values\n\n        feature_scores = [[int(i[0]), i[1]] for i in feature_scores]\n\n        self.feat_score = feature_scores\n\n        head_idx = [i[0] for i in feature_scores]\n        self.headers_model = list(np.array(self.headers)[head_idx])\n\n        self.feature_importances_ = [i[1] for i in feature_scores]\n        self.top_features_ = np.argsort(self.feature_importances_)[::-1]\n        self.header_top_features_ = [self.headers_model[i] for i in self.top_features_]\n\n        return self", "response": "Fits the VLSRelief model to generate the set of features for each subset of X and y."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_neighbors(self, inst):\n        dist_vect = []\n        for j in range(self._datalen):\n            if inst != j:\n                locator = [inst, j]\n                if inst < j:\n                    locator.reverse()\n                dist_vect.append(self._distance_array[locator[0]][locator[1]])\n\n        dist_vect = np.array(dist_vect)\n        inst_avg_dist = np.average(dist_vect)\n        inst_std = np.std(dist_vect) / 2.\n        near_threshold = inst_avg_dist - inst_std\n        far_threshold = inst_avg_dist + inst_std\n\n        NN_near = []\n        NN_far = []\n        for j in range(self._datalen):\n            if inst != j:\n                locator = [inst, j]\n                if inst < j:\n                    locator.reverse()\n                if self._distance_array[locator[0]][locator[1]] < near_threshold:\n                    NN_near.append(j)\n                elif self._distance_array[locator[0]][locator[1]] > far_threshold:\n                    NN_far.append(j)\n\n        return np.array(NN_near), np.array(NN_far)", "response": "Identify nearest as well as farthest hits and misses within radius defined by average distance and standard deviation of distances from target instanace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_neighbors(self, inst, avg_dist):\n        NN = []\n        min_indicies = []\n\n        for i in range(self._datalen):\n            if inst != i:\n                locator = [inst, i]\n                if i > inst:\n                    locator.reverse()\n                d = self._distance_array[locator[0]][locator[1]]\n                if d < avg_dist:  # Defining the neighborhood with an average distance radius.\n                    min_indicies.append(i)\n        for i in range(len(min_indicies)):\n            NN.append(min_indicies[i])\n        return np.array(NN, dtype=np.int32)", "response": "Identify nearest hits and misses within radius over whole distance array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef leave_module(self, node):\n        for triple_quote in self._tokenized_triple_quotes.values():\n            self._check_triple_quotes(triple_quote)\n\n        # after we are done checking these, clear out the triple-quote\n        # tracking collection so nothing is left over for the next module.\n        self._tokenized_triple_quotes = {}", "response": "Leaves the module node and checks all remaining triple quotes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_for_docstring(self, node, node_type):\n        # if there is no docstring, don't need to do anything.\n        if node.doc is not None:\n\n            # the module is everything, so to find the docstring, we\n            # iterate line by line from the start until the first element\n            # to find the docstring, as it cannot appear after the first\n            # element in the body.\n            if node_type == 'module':\n\n                # if there are no nodes that make up the body, then all we\n                # have is the module docstring\n                if not node.body:\n                    # in this case, we should only have the module docstring\n                    # parsed in the node, so the only record in the\n                    # self._tokenized_triple_quotes dict will correspond to\n                    # the module comment. this can vary by index depending\n                    # on the presence of a shebang, encoding, etc at the top\n                    # of the file.\n                    for key in list(self._tokenized_triple_quotes.keys()):\n                        quote_record = self._tokenized_triple_quotes.get(key)\n                        if quote_record:\n                            self._check_docstring_quotes(quote_record)\n                            del self._tokenized_triple_quotes[key]\n\n                else:\n                    for i in range(0, node.body[0].lineno):\n                        quote_record = self._tokenized_triple_quotes.get(i)\n                        if quote_record:\n                            self._check_docstring_quotes(quote_record)\n                            del self._tokenized_triple_quotes[i]\n                            break\n\n            else:\n                # the node has a docstring so we check the tokenized triple\n                # quotes to find a matching docstring token that follows the\n                # function/class definition.\n\n                if not node.body:\n                    # if there is no body to the class, the class def only\n                    # contains the docstring, so the only quotes we are\n                    # tracking should correspond to the class docstring.\n                    lineno = self._find_docstring_line_for_no_body(node.fromlineno)\n                    quote_record = self._tokenized_triple_quotes.get(lineno)\n                    if quote_record:\n                        self._check_docstring_quotes(quote_record)\n                        del self._tokenized_triple_quotes[lineno]\n\n                else:\n                    doc_row = self._find_docstring_line(node.fromlineno, node.tolineno)\n                    quote_record = self._tokenized_triple_quotes.get(doc_row)\n                    if quote_record:\n                        self._check_docstring_quotes(quote_record)\n                        del self._tokenized_triple_quotes[doc_row]", "response": "Check for docstring quote consistency."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the docstring line for a definition with no body.", "response": "def _find_docstring_line_for_no_body(self, start):\n        \"\"\"Find the docstring associated with a definition with no body\n        in the node.\n\n        In these cases, the provided start and end line number for that\n        element are the same, so we must get the docstring based on the\n        sequential position of known docstrings.\n\n        Args:\n            start: the row where the class / function starts.\n\n        Returns:\n            int: the row number where the docstring is found.\n        \"\"\"\n        tracked = sorted(list(self._tokenized_triple_quotes.keys()))\n\n        for i in tracked:\n            if min(start, i) == start:\n                return i\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the row where a docstring starts in a function or class.", "response": "def _find_docstring_line(self, start, end):\n        \"\"\"Find the row where a docstring starts in a function or class.\n\n        This will search for the first match of a triple quote token in\n        row sequence from the start of the class or function.\n\n        Args:\n            start: the row where the class / function starts.\n            end: the row where the class / function ends.\n\n        Returns:\n            int: the row number where the docstring is found.\n        \"\"\"\n        for i in range(start, end + 1):\n            if i in self._tokenized_triple_quotes:\n                return i\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_tokens(self, tokens):\n        for tok_type, token, (start_row, start_col), _, _ in tokens:\n            if tok_type == tokenize.STRING:\n                # 'token' is the whole un-parsed token; we can look at the start\n                # of it to see whether it's a raw or unicode string etc.\n                self._process_string_token(token, start_row, start_col)", "response": "Process the tokens from the token stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_string_token(self, token, start_row, start_col):\n        for i, char in enumerate(token):\n            if char in QUOTES:\n                break\n\n        # pylint: disable=undefined-loop-variable\n        # ignore prefix markers like u, b, r\n        norm_quote = token[i:]\n\n        # triple-quote strings\n        if len(norm_quote) >= 3 and norm_quote[:3] in TRIPLE_QUOTE_OPTS.values():\n            self._tokenized_triple_quotes[start_row] = (token, norm_quote[:3], start_row, start_col)\n            return\n\n        # single quote strings\n\n        preferred_quote = SMART_QUOTE_OPTS.get(self.config.string_quote)\n\n        # Smart case.\n        if self.config.string_quote in SMART_CONFIG_OPTS:\n            other_quote = next(q for q in QUOTES if q != preferred_quote)\n            # If using the other quote avoids escaping, we switch to the other quote.\n            if preferred_quote in token[i + 1:-1] and other_quote not in token[i + 1:-1]:\n                preferred_quote = other_quote\n\n        if norm_quote[0] != preferred_quote:\n            self._invalid_string_quote(\n                quote=norm_quote[0],\n                row=start_row,\n                correct_quote=preferred_quote,\n                col=start_col,\n            )", "response": "Internal method for identifying and checking string tokens in the token stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_triple_quotes(self, quote_record):\n        _, triple, row, col = quote_record\n        if triple != TRIPLE_QUOTE_OPTS.get(self.config.triple_quote):\n            self._invalid_triple_quote(triple, row, col)", "response": "Check if the triple quote from tokenization is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the docstring quote from tokenization is valid.", "response": "def _check_docstring_quotes(self, quote_record):\n        \"\"\"Check if the docstring quote from tokenization is valid.\n\n        Args:\n            quote_record: a tuple containing the info about the string\n                from tokenization, giving the (token, quote, row number).\n        \"\"\"\n        _, triple, row, col = quote_record\n        if triple != TRIPLE_QUOTE_OPTS.get(self.config.docstring_quote):\n            self._invalid_docstring_quote(triple, row, col)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _invalid_string_quote(self, quote, row, correct_quote=None, col=None):\n        if not correct_quote:\n            correct_quote = SMART_QUOTE_OPTS.get(self.config.string_quote)\n\n        self.add_message(\n            'invalid-string-quote',\n            line=row,\n            args=(quote, correct_quote),\n            **self.get_offset(col)\n        )", "response": "Add a message for an invalid string literal quote."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _invalid_triple_quote(self, quote, row, col=None):\n        self.add_message(\n            'invalid-triple-quote',\n            line=row,\n            args=(quote, TRIPLE_QUOTE_OPTS.get(self.config.triple_quote)),\n            **self.get_offset(col)\n        )", "response": "Add a message for an invalid triple quote."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _invalid_docstring_quote(self, quote, row, col=None):\n        self.add_message(\n            'invalid-docstring-quote',\n            line=row,\n            args=(quote, TRIPLE_QUOTE_OPTS.get(self.config.docstring_quote)),\n            **self.get_offset(col)\n        )", "response": "Add a message for an invalid docstring quote."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the path to the directory where geth data is stored.", "response": "def get_live_data_dir():\n    \"\"\"\n    pygeth needs a base directory to store it's chain data.  By default this is\n    the directory that `geth` uses as it's `datadir`.\n    \"\"\"\n    if sys.platform == 'darwin':\n        data_dir = os.path.expanduser(os.path.join(\n            \"~\",\n            \"Library\",\n            \"Ethereum\",\n        ))\n    elif sys.platform in {'linux', 'linux2', 'linux3'}:\n        data_dir = os.path.expanduser(os.path.join(\n            \"~\",\n            \".ethereum\",\n        ))\n    elif sys.platform == 'win32':\n        data_dir = os.path.expanduser(os.path.join(\n            \"\\\\\",\n            \"~\",\n            \"AppData\",\n            \"Roaming\",\n            \"Ethereum\",\n        ))\n\n    else:\n        raise ValueError((\n            \"Unsupported platform: '{0}'.  Only darwin/linux2/win32 are \"\n            \"supported.  You must specify the geth datadir manually\"\n        ).format(sys.platform))\n    return data_dir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_accounts(data_dir, **geth_kwargs):\n    command, proc = spawn_geth(dict(\n        data_dir=data_dir,\n        suffix_args=['account', 'list'],\n        **geth_kwargs\n    ))\n    stdoutdata, stderrdata = proc.communicate()\n\n    if proc.returncode:\n        if \"no keys in store\" in stderrdata.decode(\"utf-8\"):\n            return tuple()\n        else:\n            raise ValueError(format_error_message(\n                \"Error trying to list accounts\",\n                command,\n                proc.returncode,\n                stdoutdata,\n                stderrdata,\n            ))\n    accounts = parse_geth_accounts(stdoutdata)\n    return accounts", "response": "Returns all geth accounts as tuple of hex encoded strings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_new_account(data_dir, password, **geth_kwargs):\n    if os.path.exists(password):\n        geth_kwargs['password'] = password\n\n    command, proc = spawn_geth(dict(\n        data_dir=data_dir,\n        suffix_args=['account', 'new'],\n        **geth_kwargs\n    ))\n\n    if os.path.exists(password):\n        stdoutdata, stderrdata = proc.communicate()\n    else:\n        stdoutdata, stderrdata = proc.communicate(b\"\\n\".join((password, password)))\n\n    if proc.returncode:\n        raise ValueError(format_error_message(\n            \"Error trying to create a new account\",\n            command,\n            proc.returncode,\n            stdoutdata,\n            stderrdata,\n        ))\n\n    match = account_regex.search(stdoutdata)\n    if not match:\n        raise ValueError(format_error_message(\n            \"Did not find an address in process output\",\n            command,\n            proc.returncode,\n            stdoutdata,\n            stderrdata,\n        ))\n\n    return b'0x' + match.groups()[0]", "response": "Create a new Ethereum account on geth."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_path_exists(dir_path):\n    if not os.path.exists(dir_path):\n        mkdir(dir_path)\n        return True\n    return False", "response": "Ensure that a path exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompares two zip files for EVENT and ENTRY", "response": "def compare(left, right):\n    \"\"\"\n    yields EVENT,ENTRY pairs describing the differences between left\n    and right, which are filenames for a pair of zip files\n    \"\"\"\n\n    with open_zip(left) as l:\n        with open_zip(right) as r:\n            return compare_zips(l, r)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare_zips(left, right):\n\n    ll = set(left.namelist())\n    rl = set(right.namelist())\n\n    for f in ll:\n        if f in rl:\n            rl.remove(f)\n\n            if f[-1] == '/':\n                # it's a directory entry\n                pass\n\n            elif _different(left, right, f):\n                yield DIFF, f\n\n            else:\n                yield SAME, f\n\n        else:\n            yield LEFT, f\n\n    for f in rl:\n        yield RIGHT, f", "response": "Yield EVENT ENTRY pairs describing the differences between left and right ZipFile instances"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _different(left, right, f):\n\n    l = left.getinfo(f)\n    r = right.getinfo(f)\n\n    if (l.file_size == r.file_size) and (l.CRC == r.CRC):\n        # ok, they seem passibly similar, let's deep check them.\n        return _deep_different(left, right, f)\n\n    else:\n        # yup, they're different\n        return True", "response": "True if entry f is different between left and right ZipFile\n    instances otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _deep_different(left, right, entry):\n\n    left = chunk_zip_entry(left, entry)\n    right = chunk_zip_entry(right, entry)\n\n    for ldata, rdata in zip_longest(left, right):\n        if ldata != rdata:\n            return True\n    return False", "response": "Checks that entry is identical between ZipFile instances left and right."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect_compare_into(left, right, added, removed, altered, same):\n\n    with open_zip(left) as l:\n        with open_zip(right) as r:\n            return collect_compare_zips_into(l, r,\n                                             added, removed,\n                                             altered, same)", "response": "Collect differences between two zip files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef collect_compare_zips_into(left, right, added, removed, altered, same):\n\n    for event, filename in compare_zips(left, right):\n        if event == LEFT:\n            group = removed\n        elif event == RIGHT:\n            group = added\n        elif event == DIFF:\n            group = altered\n        elif event == SAME:\n            group = same\n        else:\n            assert False\n\n        if group is not None:\n            group.append(filename)\n\n    return added, removed, altered, same", "response": "Collects the differences between left and right ZipFile instances and returns a tuple containing added removed altered same."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_zipstream(data):\n\n    if isinstance(data, (str, buffer)):\n        data = BytesIO(data)\n\n    if hasattr(data, \"read\"):\n        tell = 0\n        if hasattr(data, \"tell\"):\n            tell = data.tell()\n\n        try:\n            result = bool(_EndRecData(data))\n        except IOError:\n            result = False\n\n        if hasattr(data, \"seek\"):\n            data.seek(tell)\n\n    else:\n        raise TypeError(\"requies str, buffer, or stream-like object\")\n\n    return result", "response": "A function to check whether a file - like object is a valid ZipFile."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the CRC32 of the contents of filename", "response": "def file_crc32(filename, chunksize=_CHUNKSIZE):\n    \"\"\"\n    calculate the CRC32 of the contents of filename\n    \"\"\"\n\n    check = 0\n    with open(filename, 'rb') as fd:\n        for data in iter(lambda: fd.read(chunksize), \"\"):\n            check = crc32(data, check)\n    return check"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _collect_infos(dirname):\n\n    \"\"\" Utility function used by ExplodedZipFile to generate ZipInfo\n    entries for all of the files and directories under dirname \"\"\"\n\n    for r, _ds, fs in walk(dirname):\n        if not islink(r) and r != dirname:\n            i = ZipInfo()\n            i.filename = join(relpath(r, dirname), \"\")\n            i.file_size = 0\n            i.compress_size = 0\n            i.CRC = 0\n            yield i.filename, i\n\n        for f in fs:\n            df = join(r, f)\n            relfn = relpath(join(r, f), dirname)\n\n            if islink(df):\n                pass\n\n            elif isfile(df):\n                i = ZipInfo()\n                i.filename = relfn\n                i.file_size = getsize(df)\n                i.compress_size = i.file_size\n                i.CRC = file_crc32(df)\n                yield i.filename, i\n\n            else:\n                # TODO: is there any more special treatment?\n                pass", "response": "Utility function used by ExplodedZipFile to generate ZipInfo objects for all of the files and directories under dirname."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns either a zipfile. ZipFile instance or an ExplodedZipFile instance depending on whether fn is a valid zip file or a directory.", "response": "def zip_file(fn, mode=\"r\"):\n    \"\"\"\n    returns either a zipfile.ZipFile instance or an ExplodedZipFile\n    instance, depending on whether fn is the name of a valid zip file,\n    or a directory.\n    \"\"\"\n\n    if isdir(fn):\n        return ExplodedZipFile(fn)\n    elif is_zipfile(fn):\n        return ZipFile(fn, mode)\n    else:\n        raise Exception(\"cannot treat as an archive: %r\" % fn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield data from a zip file entry in sequential chunks.", "response": "def chunk_zip_entry(zipfile, name, chunksize=_CHUNKSIZE):\n    \"\"\"\n    opens an entry from an openex zip file archive and yields\n    sequential chunks of data from the resulting stream.\n    \"\"\"\n\n    with open_zip_entry(zipfile, name, mode='r') as stream:\n        data = stream.read(chunksize)\n        while data:\n            yield data\n            data = stream.read(chunksize)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a tuple of (files, dirs, size_uncompressed, size_compressed). files+dirs will equal len(zipfile.infolist)", "response": "def zip_entry_rollup(zipfile):\n    \"\"\"\n    returns a tuple of (files, dirs, size_uncompressed,\n    size_compressed). files+dirs will equal len(zipfile.infolist)\n    \"\"\"\n\n    files = dirs = 0\n    total_c = total_u = 0\n\n    for i in zipfile.infolist():\n        if i.filename[-1] == '/':\n            # I wonder if there's a better detection method than this\n            dirs += 1\n        else:\n            files += 1\n            total_c += i.compress_size\n            total_u += i.file_size\n\n    return files, dirs, total_c, total_u"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect_by_typename(obj_sequence, cache=None):\n\n    if cache is None:\n        cache = {}\n\n    for val in obj_sequence:\n        key = type(val).__name__\n\n        bucket = cache.get(key, None)\n        if bucket is not None:\n            bucket.append(val)\n        else:\n            cache[key] = [val]\n\n    return cache", "response": "Collects objects from obj_sequence and stores them into buckets by type name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collect_by_type(obj_sequence, cache=None):\n\n    if cache is None:\n        cache = {}\n\n    for val in obj_sequence:\n        key = type(val)\n\n        bucket = cache.get(key, None)\n        if bucket is not None:\n            bucket.append(val)\n        else:\n            cache[key] = [val]\n\n    return cache", "response": "Collects objects from obj_sequence and stores them into buckets by type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over the objects in the object list in the order of the objects in typelist.", "response": "def iterate_by_type(objs, typelist):\n    \"\"\"\n    collects a sequence of objs into buckets by type, then re-emits\n    objs from the buckets, sorting through the buckets in the order\n    specified by typelist. Any objects of a type not specified in\n    typelist will be emitted last in no guaranteed order (but still\n    grouped by type).\n    \"\"\"\n\n    cache = collect_by_type(objs)\n\n    for t in typelist:\n        for val in cache.pop(t, tuple()):\n            yield val\n\n    for tl in cache.values():\n        for val in tl:\n            yield val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef squash(change, is_ignored=False, options=None):\n\n    if options:\n        is_ignored = change.is_ignored(options)\n\n    if isinstance(change, Removal):\n        result = SquashedRemoval(change, is_ignored)\n    elif isinstance(change, Addition):\n        result = SquashedAddition(change, is_ignored)\n    else:\n        result = SquashedChange(change, is_ignored)\n\n    return result", "response": "Squashes the in - depth information of a change to a simplified and\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dict describing a simple snapshot of this change and its children if any.", "response": "def simplify(self, options=None):\n        \"\"\"\n        returns a dict describing a simple snapshot of this change, and\n        its children if any.\n        \"\"\"\n\n        simple = {\n            \"class\": type(self).__name__,\n            \"is_change\": self.is_change(),\n            \"description\": self.get_description(),\n            \"label\": self.label,\n        }\n\n        if options:\n            simple[\"is_ignored\"] = self.is_ignored(options)\n\n        if isinstance(self, Addition):\n            simple[\"is_addition\"] = True\n\n        if isinstance(self, Removal):\n            simple[\"is_removal\"] = True\n\n        if self.entry:\n            simple[\"entry\"] = self.entry\n\n        return simple"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tuple of is_change description of the stored entry in self. changed and self. description", "response": "def check_impl(self):\n        \"\"\"\n        returns a tuple of (is_change,description) which are then stored\n        in self.changed and self.description\n\n        The default implementation will get the data from the left and\n        right sides by calling self.fn_data, then compare them via\n        self.fn_differ. If they do differ, a message will be\n        constructed using self.fn_pretty to create human-readable\n        versions of the data that changed.\n        \"\"\"\n\n        if self.fn_differ(self.get_ldata(), self.get_rdata()):\n\n            left = self.pretty_ldata_desc()\n            right = self.pretty_rdata_desc()\n            msg = \"%s changed: %s to %s\" % (self.label, left, right)\n\n            return True, msg\n\n        else:\n            return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simplify(self, options=None):\n\n        # TODO: we might want to get rid of this method and just move\n        # it into the JSONEncoder in report.py\n\n        simple = super(GenericChange, self).simplify(options)\n\n        ld = self.pretty_ldata()\n        if ld is not None:\n            simple[\"old_data\"] = ld\n\n        rd = self.pretty_rdata()\n        if rd is not None:\n            simple[\"new_data\"] = rd\n\n        return simple", "response": "provide a simple representation of this change as a dictionary\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear(self):\n\n        super(SuperChange, self).clear()\n\n        for c in self.changes:\n            c.clear()\n        self.changes = tuple()", "response": "Clears all child changes and drops the reference to them"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects the entries in the overriden change_types field with the left and right data.", "response": "def collect_impl(self):\n        \"\"\"\n        instantiates each of the entries in in the overriden change_types\n        field with the left and right data\n        \"\"\"\n\n        ldata = self.get_ldata()\n        rdata = self.get_rdata()\n\n        for change_type in self.change_types:\n            yield change_type(ldata, rdata)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collect(self, force=False):\n\n        if force or not self.changes:\n            self.changes = tuple(self.collect_impl())\n        return self.changes", "response": "Calls collect_impl and stores the results as the child changes of\n        this super - change."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if any of the items in self. changes_impl is a change.", "response": "def check_impl(self):\n        \"\"\"\n        sets self.changes to the result of self.changes_impl, then if any\n        member of those checks shows as a change, will return\n        True,None\n        \"\"\"\n\n        c = False\n        for change in self.collect():\n            change.check()\n            c = c or change.is_change()\n        return c, None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if this entry is ignored.", "response": "def is_ignored(self, options):\n        \"\"\"\n        If we have changed children and all the children which are changes\n        are ignored, then we are ignored. Otherwise, we are not\n        ignored\n        \"\"\"\n\n        if not self.is_change():\n            return False\n\n        changes = self.collect()\n        if not changes:\n            return False\n\n        for change in changes:\n            if change.is_change() and not change.is_ignored(options):\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a simple dict representing this change and its children and their children", "response": "def simplify(self, options=None):\n        \"\"\"\n        generate a simple dict representing this change data, and\n        collecting all of the sub-change instances (which are NOT\n        immediately simplified themselves)\n        \"\"\"\n\n        data = super(SuperChange, self).simplify(options)\n\n        show_ignored = False\n        show_unchanged = False\n\n        if options:\n            show_ignored = getattr(options, \"show_ignored\", show_ignored)\n            show_unchanged = getattr(options, \"show_unchanged\", show_unchanged)\n\n        # build a list of sub-changes honoring show-ignored and\n        # show-unchanged\n\n        subs = list()\n        for s in self.collect():\n            if s.is_change():\n                if show_ignored or not s.is_ignored(options):\n                    subs.append(s)\n            elif show_unchanged:\n                subs.append(s)\n\n        data[\"children\"] = subs\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsquashes all child changes into squashed changes", "response": "def squash_children(self, options):\n        \"\"\"\n        reduces the memory footprint of this super-change by converting\n        all child changes into squashed changes\n        \"\"\"\n\n        oldsubs = self.collect()\n        self.changes = tuple(squash(c, options=options) for c in oldsubs)\n\n        for change in oldsubs:\n            change.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noption group specific to the tests in jardiff", "response": "def add_jardiff_optgroup(parser):\n    \"\"\"\n    option group specific to the tests in jardiff\n    \"\"\"\n\n    og = parser.add_argument_group(\"JAR Checking Options\")\n\n    og.add_argument(\"--ignore-jar-entry\", action=\"append\", default=[])\n\n    og.add_argument(\"--ignore-jar-signature\",\n                    action=\"store_true\", default=False,\n                    help=\"Ignore JAR signing changes\")\n\n    og.add_argument(\"--ignore-manifest\",\n                    action=\"store_true\", default=False,\n                    help=\"Ignore changes to manifests\")\n\n    og.add_argument(\"--ignore-manifest-subsections\",\n                    action=\"store_true\", default=False,\n                    help=\"Ignore changes to manifest subsections\")\n\n    og.add_argument(\"--ignore-manifest-key\",\n                    action=\"append\", default=[],\n                    help=\"case-insensitive manifest keys to ignore\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_jardiff_options(updates=None):\n\n    parser = create_optparser()\n    options, _args = parser.parse_args(list())\n\n    if updates:\n        # pylint: disable=W0212\n        options._update_careful(updates)\n\n    return options", "response": "Generate an options object with the appropriate default values in\n    place for API usage of jardiff features."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(args=sys.argv):\n\n    parser = create_optparser(args[0])\n    return cli(parser.parse_args(args[1:]))", "response": "The main entry point for the jardiff CLI\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding check_impl to open the left and right zipfiles and set the ldata and rdata attributes to the ldata and rdata attributes.", "response": "def check_impl(self):\n        \"\"\"\n        Overridden to open the left and right zipfiles and to provide all\n        subchecks with an open ZipFile instance rather than having\n        them all open and close the ZipFile individually. For the\n        duration of the check (which calls collect_impl), the\n        attributes self.lzip and self.rzip will be available and used\n        as the ldata and rdata of all subchecks.\n        \"\"\"\n\n        with open_zip(self.ldata) as lzip:\n            with open_zip(self.rdata) as rzip:\n                self.lzip = lzip\n                self.rzip = rzip\n                ret = super(JarContentsChange, self).check_impl()\n\n        self.lzip = None\n        self.rzip = None\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_json_report_optgroup(parser):\n\n    g = parser.add_argument_group(\"JSON Report Options\")\n\n    g.add_argument(\"--json-indent\", action=\"store\", default=2, type=int)", "response": "Adds the JSON Report Options to the given parser."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _indent_change(change, out, options, indent):\n\n    show_unchanged = getattr(options, \"show_unchanged\", False)\n    show_ignored = getattr(options, \"show_ignored\", False)\n\n    show = False\n    desc = change.get_description()\n\n    if change.is_change():\n        if change.is_ignored(options):\n            if show_ignored:\n                show = True\n                _indent(out, indent, desc, \" [IGNORED]\")\n        else:\n            show = True\n            _indent(out, indent, desc)\n    elif show_unchanged:\n        show = True\n        _indent(out, indent, desc)\n\n    if show:\n        indent += 1\n        for sub in change.collect():\n            _indent_change(sub, out, options, indent)", "response": "recursive function to print indented change descriptions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a message to a text stream with indentation.", "response": "def _indent(stream, indent, *msgs):\n\n    \"\"\" write a message to a text stream, with indentation. Also ensures that\n    the output encoding of the messages is safe for writing. \"\"\"\n\n    for x in range(0, indent):\n        stream.write(\"  \")\n    for x in msgs:\n        # Any nicer way? In Py2 x can be 'str' or 'unicode'.\n        stream.write(x.encode(\"ascii\", \"backslashreplace\").decode(\"ascii\"))\n    stream.write(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compose_cheetah_template_map(cache):\n\n    from .cheetah import get_templates\n\n    # pylint: disable=W0406\n    # needed for introspection\n    import javatools\n\n    for template_type in get_templates():\n        if \"_\" not in template_type.__name__:\n            # we use the _ to denote package and class names. So any\n            # template without a _ in the name isn't meant to be\n            # matched to a change type.\n            continue\n\n        # get the package and change class names based off of the\n        # template class name\n        tn = template_type.__name__\n        pn, cn = tn.split(\"_\", 1)\n\n        # get the package from javatools\n        pk = getattr(javatools, pn, None)\n        if pk is None:\n            __import__(\"javatools.\" + pn)\n            pk = getattr(javatools, pn, None)\n\n        # get the class from the package\n        cc = getattr(pk, cn, None)\n        if cc is None:\n            raise Exception(\"no change class for template %s\" % tn)\n\n        # associate a Change class with a Template class\n        cache[cc] = template_type\n\n    return cache", "response": "This function is used to compose the cheetah template map into a given cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_cheetah_template(change_type):\n\n    tm = cheetah_template_map()\n\n    # follow the built-in MRO for a type to find the matching\n    # cheetah template\n    for t in change_type.mro():\n        tmpl = tm.get(t)\n        if tmpl:\n            return tmpl\n\n    # this should never happen, since we'll provide a\n    # change_Change template, and all of the changes should be\n    # inheriting from that type\n    raise Exception(\"No template for class %s\" % change_type.__name__)", "response": "Returns the appropriate cheetah template class for the given change type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_html_report_optgroup(parser):\n\n    g = parser.add_argument_group(\"HTML Report Options\")\n\n    g.add_argument(\"--html-stylesheet\", action=\"append\",\n                   dest=\"html_stylesheets\", default=list())\n\n    g.add_argument(\"--html-javascript\", action=\"append\",\n                   dest=\"html_javascripts\", default=list())\n\n    g.add_argument(\"--html-copy-data\", action=\"store\", default=None,\n                   help=\"Copy default resources to the given directory and\"\n                   \" enable them in the template\")", "response": "Adds the HTML Report Options to the given parser."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quick_report(report_type, change, options):\n\n    report = report_type(None, options)\n\n    if options.output:\n        with open(options.output, \"w\") as out:\n            report.run(change, None, out)\n    else:\n        report.run(change, None, sys.stdout)", "response": "Runs a quick report on the current node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the breadcrumbs as relative to the basedir", "response": "def get_relative_breadcrumbs(self):\n        \"\"\"\n        get the breadcrumbs as relative to the basedir\n        \"\"\"\n\n        basedir = self.basedir\n        crumbs = self.breadcrumbs\n\n        return [(relpath(b, basedir), e) for b, e in crumbs]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd formats by short label descriptors such as txt json or htm.", "response": "def add_formats_by_name(self, rfmt_list):\n        \"\"\"\n        adds formats by short label descriptors, such as 'txt', 'json', or\n        'html'\n        \"\"\"\n\n        for fmt in rfmt_list:\n            if fmt == \"json\":\n                self.add_report_format(JSONReportFormat)\n            elif fmt in (\"txt\", \"text\"):\n                self.add_report_format(TextReportFormat)\n            elif fmt in (\"htm\", \"html\"):\n                self.add_report_format(CheetahReportFormat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subreporter(self, subpath, entry):\n\n        newbase = join(self.basedir, subpath)\n        r = Reporter(newbase, entry, self.options)\n\n        crumbs = list(self.breadcrumbs)\n        crumbs.append((self.basedir, self.entry))\n        r.breadcrumbs = crumbs\n\n        r.formats = set(self.formats)\n        return r", "response": "create a sub - reporter for a sub - report"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears the cache of all report format instances created during setup", "response": "def clear(self):\n        \"\"\"\n        calls clear on any report format instances created during setup\n        and drops the cache\n        \"\"\"\n\n        if self._formats:\n            for fmt in self._formats:\n                fmt.clear()\n        self._formats = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _relative(self, uri):\n\n        if uri.startswith(\"http:\") or \\\n           uri.startswith(\"https:\") or \\\n           uri.startswith(\"file:\") or \\\n           uri.startswith(\"/\"):\n            return uri\n\n        elif exists(uri):\n            return relpath(uri, self.basedir)\n\n        else:\n            return uri", "response": "If uri is relative re - relate it to our basedir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of URIs that are relative to our basedir.", "response": "def _relative_uris(self, uri_list):\n        \"\"\"\n        if uris in list are relative, re-relate them to our basedir\n        \"\"\"\n\n        return [u for u in (self._relative(uri) for uri in uri_list) if u]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self):\n\n        from javatools import cheetah\n\n        options = self.options\n        datadir = getattr(options, \"html_copy_data\", None)\n\n        if getattr(options, \"html_data_copied\", False) or not datadir:\n            # either already run by a parent report, or not supposed\n            # to run at all.\n            return\n\n        # this is where we've installed the default media\n        datasrc = join(cheetah.__path__[0], \"data\")\n\n        # record the .js and .css content we copy\n        javascripts = list()\n        stylesheets = list()\n\n        # copy the contents of our data source to datadir\n        for _orig, copied in copydir(datasrc, datadir):\n            if copied.endswith(\".js\"):\n                javascripts.append(copied)\n            elif copied.endswith(\".css\"):\n                stylesheets.append(copied)\n\n        javascripts.extend(getattr(options, \"html_javascripts\", tuple()))\n        stylesheets.extend(getattr(options, \"html_stylesheets\", tuple()))\n\n        options.html_javascripts = javascripts\n        options.html_stylesheets = stylesheets\n\n        # keep from copying again\n        options.html_data_copied = True", "response": "This method is called by the base class to setup the default media cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets up the report directory for an HTML report. Obtains the top-level Cheetah template that is appropriate for the change instance, and runs it. The cheetah templates are supplied the following values: * change - the Change instance to report on * entry - the string name of the entry for this report * options - the cli options object * breadcrumbs - list of backlinks * javascripts - list of .js links * stylesheets - list of .css links The cheetah templates are also given a render_change method which can be called on another Change instance to cause its template to be resolved and run in-line.", "response": "def run_impl(self, change, entry, out):\n        \"\"\"\n        sets up the report directory for an HTML report. Obtains the\n        top-level Cheetah template that is appropriate for the change\n        instance, and runs it.\n\n        The cheetah templates are supplied the following values:\n         * change - the Change instance to report on\n         * entry - the string name of the entry for this report\n         * options - the cli options object\n         * breadcrumbs - list of backlinks\n         * javascripts - list of .js links\n         * stylesheets - list of .css links\n\n        The cheetah templates are also given a render_change method\n        which can be called on another Change instance to cause its\n        template to be resolved and run in-line.\n        \"\"\"\n\n        options = self.options\n\n        # translate relative paths if necessary\n        javascripts = self._relative_uris(options.html_javascripts)\n        stylesheets = self._relative_uris(options.html_stylesheets)\n\n        # map the class of the change to a template class\n        template_class = resolve_cheetah_template(type(change))\n\n        # instantiate and setup the template\n        template = template_class()\n\n        # create a transaction wrapping the output stream\n        template.transaction = DummyTransaction()\n        template.transaction.response(resp=out)\n\n        # inject our values\n        template.change = change\n        template.entry = entry\n        template.options = options\n        template.breadcrumbs = self.breadcrumbs\n        template.javascripts = javascripts\n        template.stylesheets = stylesheets\n\n        # this lets us call render_change from within the template on\n        # a change instance to chain to another template (eg, for\n        # sub-changes)\n        template.render_change = lambda c: self.run_impl(c, entry, out)\n\n        # execute the template, which will write its contents to the\n        # transaction (and the underlying stream)\n        template.respond()\n\n        # clean up the template\n        template.shutdown()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_provides(self, ignored=tuple()):\n\n        if self._provides is None:\n            self._collect_requires_provides()\n\n        d = self._provides\n        if ignored:\n            d = dict((k, v) for k, v in d.items()\n                     if not fnmatches(k, *ignored))\n        return d", "response": "a map of classes and class members and what\n        provides them."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the object and cleans up any temporary files that were created by introspection.", "response": "def close(self):\n        \"\"\" if this was a zip'd distribution, any introspection\n        may have resulted in opening or creating temporary files.\n        Call close in order to clean up. \"\"\"\n\n        if self.tmpdir:\n            rmtree(self.tmpdir)\n            self.tmpdir = None\n\n        self._contents = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprovides sensible defaults for a code, and registers it with the __OPTABLE for lookup.", "response": "def __op(name, val, fmt=None, const=False, consume=0, produce=0):\n    \"\"\"\n    provides sensible defaults for a code, and registers it with the\n    __OPTABLE for lookup.\n    \"\"\"\n\n    name = name.lower()\n\n    # fmt can either be a str representing the struct to unpack, or a\n    # callable to do more complex unpacking. If it's a str, create a\n    # callable for it.\n    if isinstance(fmt, str):\n        fmt = partial(_unpack, compile_struct(fmt))\n\n    operand = (name, val, fmt, consume, produce, const)\n\n    assert(name not in __OPTABLE)\n    assert(val not in __OPTABLE)\n\n    __OPTABLE[name] = operand\n    __OPTABLE[val] = operand\n\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunpack the data tuple from bc and returns the next offset past the unpacked data tuple", "response": "def _unpack(struct, bc, offset=0):\n    \"\"\"\n    returns the unpacked data tuple, and the next offset past the\n    unpacked data\n    \"\"\"\n\n    return struct.unpack_from(bc, offset), offset + struct.size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unpack_lookupswitch(bc, offset):\n\n    jump = (offset % 4)\n    if jump:\n        offset += (4 - jump)\n\n    (default, npairs), offset = _unpack(_struct_ii, bc, offset)\n\n    switches = list()\n    for _index in range(npairs):\n        pair, offset = _unpack(_struct_ii, bc, offset)\n        switches.append(pair)\n\n    return (default, switches), offset", "response": "unpacks the lookupswitch op arguments\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _unpack_tableswitch(bc, offset):\n\n    jump = (offset % 4)\n    if jump:\n        offset += (4 - jump)\n\n    (default, low, high), offset = _unpack(_struct_iii, bc, offset)\n\n    joffs = list()\n    for _index in range((high - low) + 1):\n        j, offset = _unpack(_struct_i, bc, offset)\n        joffs.append(j)\n\n    return (default, low, high, joffs), offset", "response": "unpacks the tableswitch op arguments\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _unpack_wide(bc, offset):\n\n    code = ord(bc[offset])\n\n    if code == OP_iinc:\n        return _unpack(_struct_BHh, bc, offset)\n\n    elif code in (OP_iload, OP_fload, OP_aload, OP_lload, OP_dload,\n                  OP_istore, OP_fstore, OP_astore, OP_lstore,\n                  OP_dstore, OP_ret):\n\n        return _unpack(_struct_BH, bc, offset)\n\n    else:\n        # no other opcodes are valid, so shouldn't have fallen through\n        # to here.\n        assert False", "response": "unpacks a wide version of the tree tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisassemble Java bytecode into a sequence of tuples.", "response": "def disassemble(bytecode):\n    \"\"\"\n    Generator. Disassembles Java bytecode into a sequence of (offset,\n    code, args) tuples\n    :type bytecode: bytes\n    \"\"\"\n\n    offset = 0\n    end = len(bytecode)\n\n    while offset < end:\n        orig_offset = offset\n\n        code = bytecode[offset]\n        if not isinstance(code, int):   # Py3\n            code = ord(code)\n\n        offset += 1\n\n        args = tuple()\n        fmt = get_arg_format(code)\n        if fmt:\n            args, offset = fmt(bytecode, offset)\n\n        yield (orig_offset, code, args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _mp_run_check(tasks, results, options):\n\n    try:\n        for index, change in iter(tasks.get, None):\n            # this is the part that takes up all of our time and\n            # produces side-effects like writing out files for all of\n            # the report formats.\n            change.check()\n\n            # rather than serializing the completed change (which\n            # could be rather large now that it's been realized), we\n            # send back only what we want, which is the squashed\n            # overview, and throw away the used bits.\n            squashed = squash(change, options=options)\n            change.clear()\n\n            results.put((index, squashed))\n\n    except KeyboardInterrupt:\n        # prevent a billion lines of backtrace from hitting the user\n        # in the face\n        return", "response": "run check on all of our user - specified items and put the squashed bits in the results queue"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an option group relating to the use of a DistChange or DistReport", "response": "def add_distdiff_optgroup(parser):\n    \"\"\"\n    Option group relating to the use of a DistChange or DistReport\n    \"\"\"\n\n    # for the --processes default\n    cpus = cpu_count()\n\n    og = parser.add_argument_group(\"Distribution Checking Options\")\n\n    og.add_argument(\"--processes\", type=int, default=cpus,\n                    help=\"Number of child processes to spawn to handle\"\n                    \" sub-reports. Set to 0 to disable multi-processing.\"\n                    \" Defaults to the number of CPUs (%r)\" % cpus)\n\n    og.add_argument(\"--shallow\", action=\"store_true\", default=False,\n                    help=\"Check only that the files of this dist have\"\n                    \"changed, do not infer the meaning\")\n\n    og.add_argument(\"--ignore-filenames\", action=\"append\", default=[],\n                    help=\"file glob to ignore. Can be specified multiple\"\n                    \" times\")\n\n    og.add_argument(\"--ignore-trailing-whitespace\",\n                    action=\"store_true\", default=False,\n                    help=\"ignore trailing whitespace when comparing text\"\n                    \" files\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_optparser(progname=None):\n    from . import report\n\n    parser = ArgumentParser(prog=progname)\n    parser.add_argument(\"dist\", nargs=2,\n                        help=\"distributions to compare\")\n\n    add_general_optgroup(parser)\n    add_distdiff_optgroup(parser)\n    add_jardiff_optgroup(parser)\n    add_classdiff_optgroup(parser)\n\n    report.add_general_report_optgroup(parser)\n    report.add_json_report_optgroup(parser)\n    report.add_html_report_optgroup(parser)\n\n    return parser", "response": "Create an optparser instance filled with options and groups\n    appropriate for use with the distdiff command"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an options object with the appropriate default values in place for API usage of distdiff features.", "response": "def default_distdiff_options(updates=None):\n    \"\"\"\n    generate an options object with the appropriate default values in\n    place for API usage of distdiff features. overrides is an optional\n    dictionary which will be used to update fields on the options\n    object.\n    \"\"\"\n\n    parser = create_optparser()\n    options = parser.parse_args(list())\n\n    if updates:\n        # pylint: disable=W0212\n        options._update_careful(updates)\n\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield the change instances based on the delta of the two distribution directories.", "response": "def collect_impl(self):\n        \"\"\"\n        emits change instances based on the delta of the two distribution\n        directories\n        \"\"\"\n\n        ld = self.ldata\n        rd = self.rdata\n        deep = not self.shallow\n\n        for event, entry in compare(ld, rd):\n            if deep and fnmatches(entry, *JAR_PATTERNS):\n                if event == LEFT:\n                    yield DistJarRemoved(ld, rd, entry)\n                elif event == RIGHT:\n                    yield DistJarAdded(ld, rd, entry)\n                elif event == DIFF:\n                    yield DistJarChange(ld, rd, entry, True)\n                elif event == SAME:\n                    yield DistJarChange(ld, rd, entry, False)\n\n            elif deep and fnmatches(entry, \"*.class\"):\n                if event == LEFT:\n                    yield DistClassRemoved(ld, rd, entry)\n                elif event == RIGHT:\n                    yield DistClassAdded(ld, rd, entry)\n                elif event == DIFF:\n                    yield DistClassChange(ld, rd, entry, True)\n                elif event == SAME:\n                    yield DistClassChange(ld, rd, entry, False)\n\n            elif deep and fnmatches(entry, *TEXT_PATTERNS):\n                if event == LEFT:\n                    yield DistContentRemoved(ld, rd, entry)\n                elif event == RIGHT:\n                    yield DistContentAdded(ld, rd, entry)\n                elif event == DIFF:\n                    yield DistTextChange(ld, rd, entry)\n                elif event == SAME:\n                    yield DistTextChange(ld, rd, entry, False)\n\n            elif deep and fnmatches(entry, \"*/MANIFEST.MF\"):\n                if event == LEFT:\n                    yield DistContentRemoved(ld, rd, entry)\n                elif event == RIGHT:\n                    yield DistContentAdded(ld, rd, entry)\n                elif event == DIFF:\n                    yield DistManifestChange(ld, rd, entry, True)\n                elif event == SAME:\n                    yield DistManifestChange(ld, rd, entry, False)\n\n            else:\n                if event == LEFT:\n                    yield DistContentRemoved(ld, rd, entry)\n                elif event == RIGHT:\n                    yield DistContentAdded(ld, rd, entry)\n                elif event == DIFF:\n                    yield DistContentChange(ld, rd, entry, True)\n                elif event == SAME:\n                    yield DistContentChange(ld, rd, entry, False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects DistJarChange and DistClassChange from the underlying DistChange and DistJarReport instances.", "response": "def collect_impl(self):\n        \"\"\"\n        overrides DistJarChange and DistClassChange from the underlying\n        DistChange with DistJarReport and DistClassReport instances\n        \"\"\"\n\n        for c in DistChange.collect_impl(self):\n            if isinstance(c, DistJarChange):\n                if c.is_change():\n                    ln = DistJarReport.report_name\n                    nr = self.reporter.subreporter(c.entry, ln)\n                    c = DistJarReport(c.ldata, c.rdata, c.entry, nr)\n            elif isinstance(c, DistClassChange):\n                if c.is_change():\n                    ln = DistClassReport.report_name\n                    nr = self.reporter.subreporter(c.entry, ln)\n                    c = DistClassReport(c.ldata, c.rdata, c.entry, nr)\n            yield c"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mp_check_impl(self, process_count):\n\n        from multiprocessing import Process, Queue\n\n        options = self.reporter.options\n\n        # this is the function that will be run in a separate process,\n        # which will handle the tasks queue and feed into the results\n        # queue\n        func = _mp_run_check\n\n        # normally this would happen lazily, but since we'll have\n        # multiple processes all running reports at the same time, we\n        # need to make sure the setup is done before-hand. This is\n        # hackish, but in particular this keeps the HTML reports from\n        # trying to perform the default data copy over and over.\n        self.reporter.setup()\n\n        # enqueue the sub-reports for multi-processing. Other types of\n        # changes can happen sync.\n        changes = list(self.collect_impl())\n\n        task_count = 0\n        tasks = Queue()\n        results = Queue()\n\n        try:\n            # as soon as we start using the tasks queue, we need to be\n            # catching the KeyboardInterrupt event so that we can\n            # drain the queue and lets its underlying thread terminate\n            # happily.\n\n            # TODO: is there a better way to handle this shutdown\n            # gracefully?\n\n            # feed any sub-reports to the tasks queue\n            for index in range(0, len(changes)):\n                change = changes[index]\n                if isinstance(change, (DistJarReport, DistClassReport)):\n                    changes[index] = None\n                    tasks.put((index, change))\n                    task_count += 1\n\n            # infrequent edge case, but don't bother starting more\n            # helpers than we'll ever use\n            process_count = min(process_count, task_count)\n\n            # start the number of helper processes, and make sure\n            # there are that many stop sentinels at the end of the\n            # tasks queue\n            for _i in range(0, process_count):\n                tasks.put(None)\n                process = Process(target=func, args=(tasks, results, options))\n                process.daemon = False\n                process.start()\n\n            # while the helpers are running, perform our checks\n            for change in changes:\n                if change:\n                    change.check()\n\n            # get all of the results and feed them back into our change\n            for _i in range(0, task_count):\n                index, change = results.get()\n                changes[index] = change\n\n        except KeyboardInterrupt:\n            # drain the tasks queue so it will exit gracefully\n            for _change in iter(tasks.get, None):\n                pass\n            raise\n\n        # complete the check by setting our internal collection of\n        # child changes and returning our overall status\n        c = False\n        for change in changes:\n            c = c or change.is_change()\n        self.changes = changes\n        return c, None", "response": "A multiprocessing - enabled check implementation. Will create up to process_count helper processes and use them to perform the check."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compile_struct(fmt, cache=None):\n\n    if cache is None:\n        cache = _struct_cache\n\n    sfmt = cache.get(fmt, None)\n    if not sfmt:\n        sfmt = Struct(fmt)\n        cache[fmt] = sfmt\n    return sfmt", "response": "Compile a struct. Struct instance from a format string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unpack(data):\n\n    if isinstance(data, (bytes, buffer)):\n        return BufferUnpacker(data)\n\n    elif hasattr(data, \"read\"):\n        return StreamUnpacker(data)\n\n    else:\n        raise TypeError(\"unpack requires bytes, buffer, or instance\"\n                        \" supporting the read method\")", "response": "Returns either a BufferUnpacker or StreamUnpacker instance depending upon the type of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads count bytes from the underlying buffer and return them as a str. Raises an UnpackException if there is not enough data in the underlying buffer.", "response": "def read(self, count):\n        \"\"\"\n        read count bytes from the underlying buffer and return them as a\n        str. Raises an UnpackException if there is not enough data in\n        the underlying buffer.\n        \"\"\"\n\n        offset = self.offset\n        if self.data:\n            avail = len(self.data) - offset\n        else:\n            avail = 0\n\n        if avail < count:\n            raise UnpackException(None, count, avail)\n\n        self.offset = offset + count\n        return self.data[offset:self.offset]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n\n        data = self.data\n        self.data = None\n\n        if hasattr(data, \"close\"):\n            data.close()", "response": "close this unpacker and the underlying stream if it supports such\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncompiling the cheetah template into a python file in build_lib", "response": "def build_template(self, template, template_file, package):\n        \"\"\"\n        Compile the cheetah template in src into a python file in build\n        \"\"\"\n\n        try:\n            from Cheetah.Compiler import Compiler\n        except ImportError:\n            self.announce(\"unable to import Cheetah.Compiler, build failed\")\n            raise\n        else:\n            comp = Compiler(file=template_file, moduleName=template)\n\n        # load configuration if it exists\n        conf_fn = DEFAULT_CONFIG\n        if exists(conf_fn):\n            with open(conf_fn, \"rt\") as config:\n                comp.updateSettingsFromConfigFileObj(config)\n\n        # and just why can't I configure these?\n        comp.setShBang(\"\")\n        comp.addModuleHeader(\"pylint: disable=C,W,R,F\")\n\n        outfd = join(self.build_lib, *package.split(\".\"))\n        outfn = join(outfd, template + \".py\")\n\n        if not exists(outfd):\n            makedirs(outfd)\n\n        if newer(template_file, outfn):\n            self.announce(\"compiling %s -> %s\" % (template_file, outfd), 2)\n            with open(outfn, \"w\") as output:\n                output.write(str(comp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields a sequence of tuples containing the index left type right pretty value and type of the constant pool.", "response": "def pretty_merge_constants(left_cpool, right_cpool):\n    \"\"\"\n    sequence of tuples containing (index, left type, left pretty\n    value, right type, right pretty value). If the constant pools are\n    of inequal length, a value of None will be set in place of the\n    type and pretty value for indexes past its end\n    \"\"\"\n\n    lsize = len(left_cpool.consts)\n    rsize = len(right_cpool.consts)\n\n    index = 1\n    for index in range(1, min(lsize, rsize)):\n        lt, lv = left_cpool.pretty_const(index)\n        rt, rv = right_cpool.pretty_const(index)\n        yield (index, lt, lv, rt, rv)\n\n    if lsize > rsize:\n        for index in range(index, lsize):\n            lt, lv = left_cpool.pretty_const(index)\n            yield (index, lt, lv, None, None)\n\n    elif rsize > lsize:\n        for index in range(index, rsize):\n            rt, rv = right_cpool.pretty_const(index)\n            yield (index, None, None, rt, rv)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge two code structures into one.", "response": "def merge_code(left_code, right_code):\n    \"\"\"\n    { relative_line:\n      ((left_abs_line, ((offset, op, args), ...)),\n       (right_abs_line, ((offset, op, args), ...))),\n      ... }\n    \"\"\"\n\n    data = dict()\n\n    code_lines = (left_code and left_code.iter_code_by_lines()) or tuple()\n    for abs_line, rel_line, dis in code_lines:\n        data[rel_line] = [(abs_line, dis), None]\n\n    code_lines = (right_code and right_code.iter_code_by_lines()) or tuple()\n    for abs_line, rel_line, dis in code_lines:\n        found = data.get(rel_line, None)\n        if found is None:\n            found = [None, (abs_line, dis)]\n            data[rel_line] = found\n        else:\n            found[1] = (abs_line, dis)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_classdiff_optgroup(parser):\n\n    g = parser.add_argument_group(\"Class Checking Options\")\n\n    g.add_argument(\"--ignore-version-up\", action=\"store_true\",\n                   default=False)\n    g.add_argument(\"--ignore-version-down\", action=\"store_true\", default=False)\n    g.add_argument(\"--ignore-platform-up\", action=\"store_true\", default=False)\n    g.add_argument(\"--ignore-platform-down\", action=\"store_true\",\n                   default=False)\n    g.add_argument(\"--ignore-absolute-lines\", action=\"store_true\",\n                   default=False)\n    g.add_argument(\"--ignore-relative-lines\", action=\"store_true\",\n                   default=False)\n    g.add_argument(\"--ignore-deprecated\", action=\"store_true\", default=False)\n    g.add_argument(\"--ignore-added\", action=\"store_true\", default=False)\n    g.add_argument(\"--ignore-pool\", action=\"store_true\", default=False)\n\n    g.add_argument(\"--ignore-lines\", nargs=0,\n                   help=\"ignore relative and absolute line-number changes\",\n                   action=_opt_cb_ign_lines)\n\n    g.add_argument(\"--ignore-platform\", nargs=0,\n                   help=\"ignore platform changes\",\n                   action=_opt_cb_ign_platform)\n\n    g.add_argument(\"--ignore-version\", nargs=0,\n                   help=\"ignore version changes\",\n                   action=_opt_cb_ign_version)", "response": "Option group specific to class checking."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the options for the general option group for all javatool CLIs", "response": "def add_general_optgroup(parser):\n    \"\"\"\n    option group for general-use features of all javatool CLIs\n    \"\"\"\n\n    g = parser.add_argument_group(\"General Options\")\n\n    g.add_argument(\"-q\", \"--quiet\", dest=\"silent\",\n                   action=\"store_true\", default=False)\n\n    g.add_argument(\"-v\", \"--verbose\", nargs=0, action=_opt_cb_verbose)\n\n    g.add_argument(\"-o\", \"--output\", dest=\"output\", default=None)\n\n    g.add_argument(\"-j\", \"--json\", dest=\"json\",\n                   action=\"store_true\", default=False)\n\n    g.add_argument(\"--show-ignored\", action=\"store_true\", default=False)\n    g.add_argument(\"--show-unchanged\", action=\"store_true\", default=False)\n\n    g.add_argument(\"--ignore\", action=_opt_cb_ignore,\n                   help=\"comma-separated list of ignores\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields Cheetah templates under this module", "response": "def _iter_templates():\n    \"\"\"\n    uses reflection to yield the Cheetah templates under this module\n    \"\"\"\n\n    # pylint: disable=W0406\n    # needed for introspection\n    import javatools.cheetah\n    from Cheetah.Template import Template\n\n    for _, name, _ in iter_modules(__path__):\n        __import__(\"javatools.cheetah.\" + name)\n        found = getattr(getattr(javatools.cheetah, name), name)\n        if issubclass(found, Template):\n            yield found"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nescapes special characters with their XML entity versions", "response": "def xml_entity_escape(data):\n    \"\"\"\n    replace special characters with their XML entity versions\n    \"\"\"\n\n    data = data.replace(\"&\", \"&amp;\")\n    data = data.replace(\">\", \"&gt;\")\n    data = data.replace(\"<\", \"&lt;\")\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether the given member should be shown by the show option.", "response": "def should_show(options, member):\n    \"\"\"\n    whether to show a member by its access flags and the show\n    option. There's probably a faster and smarter way to do this, but\n    eh.\n    \"\"\"\n\n    show = options.show\n    if show == SHOW_PUBLIC:\n        return member.is_public()\n    elif show == SHOW_PACKAGE:\n        return member.is_public() or member.is_protected()\n    elif show == SHOW_PRIVATE:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nunpacking an annotation value and returns the tag and data tuple of an annotation value.", "response": "def _unpack_annotation_val(unpacker, cpool):\n    \"\"\"\n    tag, data tuple of an annotation\n    \"\"\"\n\n    tag, = unpacker.unpack_struct(_B)\n    tag = chr(tag)\n\n    if tag in 'BCDFIJSZs':\n        data, = unpacker.unpack_struct(_H)\n\n    elif tag == 'e':\n        data = unpacker.unpack_struct(_HH)\n\n    elif tag == 'c':\n        data, = unpacker.unpack_struct(_H)\n\n    elif tag == '@':\n        data = JavaAnnotation(cpool)\n        data.unpack(unpacker)\n\n    elif tag == '[':\n        data = list()\n        count, = unpacker.unpack_struct(_H)\n        for _i in range(0, count):\n            data.append(_unpack_annotation_val(unpacker, cpool))\n\n    else:\n        raise Unimplemented(\"Unknown tag {}\".format(tag))\n\n    return tag, data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pretty_annotation_val(val, cpool):\n\n    tag, data = val\n\n    if tag in 'BCDFIJSZs':\n        data = \"%s#%i\" % (tag, data)\n\n    elif tag == 'e':\n        data = \"e#%i.#%i\" % data\n\n    elif tag == 'c':\n        data = \"c#%i\" % data\n\n    elif tag == '@':\n        data = \"@\" + data.pretty_annotation()\n\n    elif tag == '[':\n        combine = list()\n        for val in data:\n            combine.append(_pretty_annotation_val(val, cpool))\n        data = \"[%s]\" % \", \".join(combine)\n\n    return data", "response": "return a pretty display of a tag and data pair annotation value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef platform_from_version(major, minor):\n\n    v = (major, minor)\n    for low, high, name in _platforms:\n        if low <= v <= high:\n            return name\n    return None", "response": "Returns the minimum platform version that can load the given class\n    version indicated by major. minor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nunpacking a constant pool item which will consist of a type byte a value of the appropriate typecode", "response": "def _unpack_const_item(unpacker):\n    \"\"\"\n    unpack a constant pool item, which will consist of a type byte\n    (see the CONST_ values in this module) and a value of the\n    appropriate type\n    \"\"\"\n\n    (typecode,) = unpacker.unpack_struct(_B)\n\n    if typecode == CONST_Utf8:\n        (slen,) = unpacker.unpack_struct(_H)\n        val = unpacker.read(slen)\n        try:\n            val = val.decode(\"utf8\")\n        except UnicodeDecodeError:\n            # easiest hack to handle java's modified utf-8 encoding\n            val = val.replace(b\"\\xC0\\x80\", b\"\\x00\").decode(\"utf8\")\n\n    elif typecode == CONST_Integer:\n        (val,) = unpacker.unpack(\">i\")\n\n    elif typecode == CONST_Float:\n        (val,) = unpacker.unpack(\">f\")\n\n    elif typecode == CONST_Long:\n        (val,) = unpacker.unpack(\">q\")\n\n    elif typecode == CONST_Double:\n        (val,) = unpacker.unpack(\">d\")\n\n    elif typecode in (CONST_Class, CONST_String, CONST_MethodType):\n        (val,) = unpacker.unpack_struct(_H)\n\n    elif typecode in (CONST_Fieldref, CONST_Methodref,\n                      CONST_InterfaceMethodref, CONST_NameAndType,\n                      CONST_ModuleId, CONST_InvokeDynamic):\n        val = unpacker.unpack_struct(_HH)\n\n    elif typecode == CONST_MethodHandle:\n        val = unpacker.unpack_struct(_BH)\n\n    elif typecode == CONST_MethodType:\n        val = unpacker.unpack_struct(_H)\n\n    elif typecode == CONST_InvokeDynamic:\n        val = unpacker.unpack_struct(_HH)\n\n    else:\n        raise Unimplemented(\"unknown constant type %r\" % typecode)\n\n    return typecode, val"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the appropriate pretty version of a typecode and a value", "response": "def _pretty_const_type_val(typecode, val):\n    \"\"\"\n    given a typecode and a value, returns the appropriate pretty\n    version of that value (not the dereferenced data)\n    \"\"\"\n\n    if typecode == CONST_Utf8:\n        typestr = \"Utf8\"  # formerly Asciz, which was considered Java bug\n        if not isinstance(val, str):  # Py2, val is 'unicode'\n            val = repr(val)[2:-1]  # trim off the surrounding u\"\" (HACK)\n        else:\n            val = repr(val)[1:-1]  # trim off the surrounding \"\" (HACK)\n    elif typecode == CONST_Integer:\n        typestr = \"int\"\n    elif typecode == CONST_Float:\n        typestr = \"float\"\n        val = \"%ff\" % val\n    elif typecode == CONST_Long:\n        typestr = \"long\"\n        val = \"%il\" % val\n    elif typecode == CONST_Double:\n        typestr = \"double\"\n        val = \"%fd\" % val\n    elif typecode == CONST_Class:\n        typestr = \"class\"\n        val = \"#%i\" % val\n    elif typecode == CONST_String:\n        typestr = \"String\"\n        val = \"#%i\" % val\n    elif typecode == CONST_Fieldref:\n        typestr = \"Field\"\n        val = \"#%i.#%i\" % val\n    elif typecode == CONST_Methodref:\n        typestr = \"Method\"\n        val = \"#%i.#%i\" % val\n    elif typecode == CONST_InterfaceMethodref:\n        typestr = \"InterfaceMethod\"\n        val = \"#%i.#%i\" % val\n    elif typecode == CONST_NameAndType:\n        typestr = \"NameAndType\"\n        val = \"#%i:#%i\" % val\n    elif typecode == CONST_ModuleId:\n        typestr = \"ModuleId\"\n        val = \"#%i@#%i\" % val\n    elif typecode == CONST_MethodHandle:\n        typestr = \"MethodHandle\"\n        val = repr(val)\n    elif typecode == CONST_MethodType:\n        typestr = \"MethodType\"\n        val = repr(val)\n    elif typecode == CONST_InvokeDynamic:\n        typestr = \"InvokeDynamic\"\n        val = repr(val)\n    else:\n        raise Unimplemented(\"unknown constant type %r\" % typecode)\n\n    return typestr, val"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _next_argsig(s):\n\n    c = s[0]\n\n    if c in \"BCDFIJSVZ\":\n        result = (c, s[1:])\n\n    elif c == \"[\":\n        d, s = _next_argsig(s[1:])\n        result = (c + d, s[len(d) + 1:])\n\n    elif c == \"L\":\n        i = s.find(';') + 1\n        result = (s[:i], s[i + 1:])\n\n    elif c == \"(\":\n        i = s.find(')') + 1\n        result = (s[:i], s[i:])\n\n    else:\n        raise Unimplemented(\"_next_argsig is %r in %r\" % (c, s))\n\n    return result", "response": "given a string find the next complete argument signature and\n    return it and a new string advanced past that point\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate through all of the type signatures in a sequence", "response": "def _typeseq_iter(s):\n    \"\"\"\n    iterate through all of the type signatures in a sequence\n    \"\"\"\n\n    s = str(s)\n    while s:\n        t, s = _next_argsig(s)\n        yield t"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a pretty version of a type code.", "response": "def _pretty_type(s, offset=0):\n    # pylint: disable=R0911, R0912\n    # too many returns, too many branches. Not converting this to a\n    # dict lookup. Waiving instead.\n\n    \"\"\"\n    returns the pretty version of a type code\n    \"\"\"\n\n    tc = s[offset]\n\n    if tc == \"V\":\n        return \"void\"\n\n    elif tc == \"Z\":\n        return \"boolean\"\n\n    elif tc == \"C\":\n        return \"char\"\n\n    elif tc == \"B\":\n        return \"byte\"\n\n    elif tc == \"S\":\n        return \"short\"\n\n    elif tc == \"I\":\n        return \"int\"\n\n    elif tc == \"J\":\n        return \"long\"\n\n    elif tc == \"D\":\n        return \"double\"\n\n    elif tc == \"F\":\n        return \"float\"\n\n    elif tc == \"L\":\n        return _pretty_class(s[offset + 1:-1])\n\n    elif tc == \"[\":\n        return \"%s[]\" % _pretty_type(s, offset + 1)\n\n    elif tc == \"(\":\n        return \"(%s)\" % \",\".join(_pretty_typeseq(s[offset + 1:-1]))\n\n    elif tc == \"T\":\n        return \"generic \" + s[offset + 1:]\n\n    else:\n        raise Unimplemented(\"unknown type, %r\" % tc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_class(data):\n\n    try:\n        with unpack(data) as up:\n            magic = up.unpack_struct(_BBBB)\n\n        return magic == JAVA_CLASS_MAGIC\n\n    except UnpackException:\n        return False", "response": "Checks that the data is a Java class file. Returns True if the magic numbers are equal False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether the given file is a Java class file by opening it and checking for the magic header.", "response": "def is_class_file(filename):\n    \"\"\"\n    checks whether the given file is a Java class file, by opening it\n    and checking for the magic header\n    \"\"\"\n\n    with open(filename, \"rb\") as fd:\n        c = fd.read(len(JAVA_CLASS_MAGIC))\n        if isinstance(c, str):      # Python 2\n            c = map(ord, c)\n        return tuple(c) == JAVA_CLASS_MAGIC"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nunpacking a Java class from data.", "response": "def unpack_class(data, magic=None):\n    \"\"\"\n    unpacks a Java class from data, which can be a string, a buffer,\n    or a stream supporting the read method. Returns a populated\n    JavaClassInfo instance.\n\n    If data is a stream which has already been confirmed to be a java\n    class, it may have had the first four bytes read from it already.\n    In this case, pass those magic bytes as a str or tuple and the\n    unpacker will not attempt to read them again.\n\n    Raises a ClassUnpackException or an UnpackException if the class\n    data is malformed. Raises Unimplemented if a feature is discovered\n    which isn't understood by javatools yet.\n    \"\"\"\n\n    with unpack(data) as up:\n        magic = magic or up.unpack_struct(_BBBB)\n        if magic != JAVA_CLASS_MAGIC:\n            raise ClassUnpackException(\"Not a Java class file\")\n\n        o = JavaClassInfo()\n        o.unpack(up, magic=magic)\n\n    return o"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack(self, unpacker):\n\n        (count, ) = unpacker.unpack_struct(_H)\n\n        # first item is never present in the actual data buffer, but\n        # the count number acts like it would be.\n        items = [(None, None), ]\n        count -= 1\n\n        # Long and Double const types will \"consume\" an item count,\n        # but not data\n        hackpass = False\n\n        for _i in range(0, count):\n\n            if hackpass:\n                # previous item was a long or double\n                hackpass = False\n                items.append((None, None))\n\n            else:\n                item = _unpack_const_item(unpacker)\n                items.append(item)\n\n                # if this item was a long or double, skip the next\n                # counter.\n                if item[0] in (CONST_Long, CONST_Double):\n                    hackpass = True\n\n        self.consts = items", "response": "Unpacks the constant pool from an unpacker stream containing a count of constant pool items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deref_const(self, index):\n\n        if not index:\n            raise IndexError(\"Requested const 0\")\n\n        t, v = self.consts[index]\n\n        if t in (CONST_Utf8, CONST_Integer, CONST_Float,\n                 CONST_Long, CONST_Double):\n            return v\n\n        elif t in (CONST_Class, CONST_String, CONST_MethodType):\n            return self.deref_const(v)\n\n        elif t in (CONST_Fieldref, CONST_Methodref,\n                   CONST_InterfaceMethodref, CONST_NameAndType,\n                   CONST_ModuleId):\n            return tuple(self.deref_const(i) for i in v)\n\n        else:\n            raise Unimplemented(\"Unknown constant pool type %r\" % t)", "response": "Returns the dereferenced value from the const pool at the given index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding a sequence of tuples index type dereferenced value of the constant pool entries.", "response": "def constants(self):\n        \"\"\"\n        sequence of tuples (index, type, dereferenced value) of the\n        constant pool entries.\n        \"\"\"\n\n        for i in range(1, len(self.consts)):\n            t, _v = self.consts[i]\n            if t:\n                yield (i, t, self.deref_const(i))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretty_constants(self):\n\n        for i in range(1, len(self.consts)):\n            t, v = self.pretty_const(i)\n            if t:\n                yield (i, t, v)", "response": "Returns a generator of tuples describing the constant pool entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretty_const(self, index):\n\n        t, v = self.consts[index]\n        if not t:\n            return None, None\n        else:\n            return _pretty_const_type_val(t, v)", "response": "Returns a tuple of the pretty type and value of a constant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pretty_deref_const(self, index):\n\n        t, v = self.consts[index]\n\n        if t == CONST_String:\n            result = self.deref_const(v)\n\n        elif t in (CONST_Utf8,\n                   CONST_Integer, CONST_Float,\n                   CONST_Long, CONST_Double):\n            result = v\n\n        elif t == CONST_Class:\n            result = _pretty_class(self.deref_const(v))\n\n        elif t == CONST_Fieldref:\n            cn = self.deref_const(v[0])\n            cn = _pretty_class(cn)\n\n            n, t = self.deref_const(v[1])\n\n            result = \"%s.%s:%s\" % (cn, n, _pretty_type(t))\n\n        elif t in (CONST_Methodref,\n                   CONST_InterfaceMethodref):\n\n            cn = self.deref_const(v[0])\n            cn = _pretty_class(cn)\n\n            n, t = self.deref_const(v[1])\n\n            args, ret = tuple(_pretty_typeseq(t))\n\n            result = \"%s.%s%s:%s\" % (cn, n, args, ret)\n\n        elif t == CONST_NameAndType:\n            a, b = (self.deref_const(i) for i in v)\n            b = \"\".join(_pretty_typeseq(b))\n            result = \"%s:%s\" % (a, b)\n\n        elif t == CONST_ModuleId:\n            a, b = (self.deref_const(i) for i in v)\n            result = \"%s@%s\" % (a, b)\n\n        elif not t:\n            # the skipped-type, meaning the prior index was a\n            # two-slotter.\n            result = \"\"\n\n        else:\n            raise Unimplemented(\"No pretty for const type %r\" % t)\n\n        return result", "response": "Returns a string representation of the constant at the given index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpack(self, unpacker):\n\n        # bound method for dereferencing constants\n        cval = self.cpool.deref_const\n\n        (count,) = unpacker.unpack_struct(_H)\n        for _i in range(0, count):\n            (name, size) = unpacker.unpack_struct(_HI)\n            self[cval(name)] = unpacker.read(size)", "response": "Unpack an attributes table from an unpacker stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unpack(self, unpacker, magic=None):\n\n        # only unpack the magic bytes if it wasn't specified\n        magic = magic or unpacker.unpack_struct(_BBBB)\n\n        if isinstance(magic, (str, buffer)):\n            magic = tuple(ord(m) for m in magic)\n        else:\n            magic = tuple(magic)\n\n        if magic != JAVA_CLASS_MAGIC:\n            raise ClassUnpackException(\"Not a Java class file\")\n\n        self.magic = magic\n\n        # unpack (minor, major), store as (major, minor)\n        self.version = unpacker.unpack_struct(_HH)[::-1]\n\n        # unpack constant pool\n        self.cpool.unpack(unpacker)\n\n        (a, b, c) = unpacker.unpack_struct(_HHH)\n        self.access_flags = a\n        self.this_ref = b\n        self.super_ref = c\n\n        # unpack interfaces\n        (count,) = unpacker.unpack_struct(_H)\n        self.interfaces = unpacker.unpack(\">%iH\" % count)\n\n        uobjs = unpacker.unpack_objects\n\n        # unpack fields\n        self.fields = tuple(uobjs(JavaMemberInfo,\n                                  self.cpool, is_method=False))\n\n        # unpack methods\n        self.methods = tuple(uobjs(JavaMemberInfo,\n                                   self.cpool, is_method=True))\n\n        # unpack attributes\n        self.attribs.unpack(unpacker)", "response": "Unpacks a Java class from an unpacker stream. Updates the internal structure of this instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the field with the given name or None if no such field is found.", "response": "def get_field_by_name(self, name):\n        \"\"\"\n        the field member matching name, or None if no such field is found\n        \"\"\"\n\n        for f in self.fields:\n            if f.get_name() == name:\n                return f\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_methods_by_name(self, name):\n\n        return (m for m in self.methods if m.get_name() == name)", "response": "Returns a generator of methods that match the given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_method(self, name, arg_types=()):\n\n        # ensure any lists or iterables are converted to tuple for\n        # comparison against get_arg_type_descriptors()\n        arg_types = tuple(arg_types)\n\n        for m in self.get_methods_by_name(name):\n            if (((not m.is_bridge()) and\n                 m.get_arg_type_descriptors() == arg_types)):\n                return m\n        return None", "response": "Searches for the method with the given name and having argument type descriptors matching those in arg_types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_method_bridges(self, name, arg_types=()):\n\n        for m in self.get_methods_by_name(name):\n            if ((m.is_bridge() and\n                 m.get_arg_type_descriptors() == arg_types)):\n                yield m", "response": "generator of bridge methods found that adapt the return types of the named method and having argument type descriptors matching arg_types."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the name of the source file this class was compiled from or None if the class was not compiled from.", "response": "def get_sourcefile(self):\n        \"\"\"\n        the name of thie file this class was compiled from, or None if not\n        indicated\n\n        reference: http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7.10\n        \"\"\"  # noqa\n\n        buff = self.get_attribute(\"SourceFile\")\n        if buff is None:\n            return None\n\n        with unpack(buff) as up:\n            (ref,) = up.unpack_struct(_H)\n\n        return self.deref_const(ref)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a tuple of JavaInnerClassInfo instances describing the innerclasses of this class.", "response": "def get_innerclasses(self):\n        \"\"\"\n        sequence of JavaInnerClassInfo instances describing the inner\n        classes of this class definition\n\n        reference: http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7.6\n        \"\"\"  # noqa\n\n        buff = self.get_attribute(\"InnerClasses\")\n        if buff is None:\n            return tuple()\n\n        with unpack(buff) as up:\n            return tuple(up.unpack_objects(JavaInnerClassInfo, self.cpool))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_enclosingmethod(self):\n\n        buff = self.get_attribute(\"EnclosingMethod\")\n\n        # TODO:\n        # Running across classes with data in this attribute like\n        # 00 06 00 00\n        # which would be the 6th const for the class name, and the\n        # zero-th (INVALID) const for method. Maybe this is static\n        # inner classes?\n\n        if buff is None:\n            return None\n\n        # class index, method index\n        with unpack(buff) as up:\n            ci, mi = up.unpack_struct(_HH)\n\n        result = None\n\n        if ci and mi:\n            enc_class = self.deref_const(ci)\n            enc_meth, enc_type = self.deref_const(mi)\n            result = \"%s.%s%s\" % (enc_class, enc_meth, enc_type)\n\n        elif ci:\n            result = self.deref_const(ci)\n\n        return result", "response": "Returns the name of the enclosing method of the class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pretty_access_flags_gen(self):\n\n        if self.is_public():\n            yield \"public\"\n        if self.is_final():\n            yield \"final\"\n        if self.is_abstract():\n            yield \"abstract\"\n        if self.is_interface():\n            if self.is_annotation():\n                yield \"@interface\"\n            else:\n                yield \"interface\"\n        if self.is_enum():\n            yield \"enum\"", "response": "generator of the pretty access flags"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pretty_descriptor(self):\n\n        f = \" \".join(self.pretty_access_flags())\n        if not self.is_interface():\n            f += \" class\"\n\n        n = self.pretty_this()\n        e = self.pretty_super()\n        i = \",\".join(self.pretty_interfaces())\n\n        if i:\n            return \"%s %s extends %s implements %s\" % (f, n, e, i)\n        else:\n            return \"%s %s extends %s\" % (f, n, e)", "response": "Returns a pretty descriptor of the class or interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_requires(self):\n\n        provided = set(self.get_provides(private=True))\n        cpool = self.cpool\n\n        # loop through the constant pool for API types\n        for i, t, _v in cpool.constants():\n\n            if t in (CONST_Class, CONST_Fieldref,\n                     CONST_Methodref, CONST_InterfaceMethodref):\n\n                # convert this away from unicode so we can\n                pv = str(cpool.pretty_deref_const(i))\n\n                if pv[0] == \"[\":\n                    # sometimes when calling operations on an array\n                    # the type embeded in the cpool will be the array\n                    # type, not just the class type. Let's only gather\n                    # the types themselves and ignore the fact that\n                    # the class really wanted an array of them.  In\n                    # the event that this was a method or field on the\n                    # array, we'll throw away that as well, and just\n                    # emit the type contained in the array.\n                    t = _typeseq(pv)\n                    if t[1] == \"L\":\n                        pv = _pretty_type(t[1:])\n                    else:\n                        pv = None\n\n                if pv and (pv not in provided):\n                    yield pv", "response": "get_requires yields the required classes fields methods determined my\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of the API s provides.", "response": "def get_provides(self, ignored=tuple(), private=False):\n        \"\"\"\n        The provided API, including the class itself, its fields, and its\n        methods.\n        \"\"\"\n\n        if private:\n            if self._provides_private is None:\n                self._provides_private = set(self._get_provides(True))\n            provides = self._provides_private\n        else:\n            if self._provides is None:\n                self._provides = set(self._get_provides(False))\n            provides = self._provides\n\n        return [prov for prov in provides if not fnmatches(prov, *ignored)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_requires(self, ignored=tuple()):\n\n        if self._requires is None:\n            self._requires = set(self._get_requires())\n\n        requires = self._requires\n        return [req for req in requires if not fnmatches(req, *ignored)]", "response": "Returns a list of all required API methods that this class references."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unpack(self, unpacker):\n\n        (a, b, c) = unpacker.unpack_struct(_HHH)\n\n        self.access_flags = a\n        self.name_ref = b\n        self.descriptor_ref = c\n        self.attribs.unpack(unpacker)", "response": "unpack the contents of this instance from the contents in unpacker"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of the AnnotationDefault attribute of the object.", "response": "def get_annotationdefault(self):\n        \"\"\"\n        The AnnotationDefault attribute, only present upon fields in an\n        annotaion.\n\n        reference: http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7.20\n        \"\"\"  # noqa\n\n        buff = self.get_attribute(\"AnnotationDefault\")\n        if buff is None:\n            return None\n\n        with unpack(buff) as up:\n            (ti, ) = up.unpack_struct(_H)\n\n        return ti"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_code(self):\n\n        buff = self.get_attribute(\"Code\")\n        if buff is None:\n            return None\n\n        with unpack(buff) as up:\n            code = JavaCodeInfo(self.cpool)\n            code.unpack(up)\n\n        return code", "response": "Returns the JavaCodeInfo of this member or None if the member is a abstract method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a tuple of class names for the exception types this method may raise or None if this method may not raise.", "response": "def get_exceptions(self):\n        \"\"\"\n        a tuple of class names for the exception types this method may\n        raise, or None if this is not a method\n\n        reference: http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7.5\n        \"\"\"  # noqa\n\n        buff = self.get_attribute(\"Exceptions\")\n        if buff is None:\n            return ()\n\n        with unpack(buff) as up:\n            return tuple(self.deref_const(e[0]) for e\n                         in up.unpack_struct_array(_H))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the constant value for this field or None if this field is not a stateful field.", "response": "def get_constantvalue(self):\n        \"\"\"\n        the constant pool index for this field, or None if this is not a\n        contant field\n\n        reference: http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7.2\n        \"\"\"  # noqa\n\n        buff = self.get_attribute(\"ConstantValue\")\n        if buff is None:\n            return None\n\n        with unpack(buff) as up:\n            (cval_ref, ) = up.unpack_struct(_H)\n\n        return cval_ref"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the type descriptors for this field.", "response": "def get_arg_type_descriptors(self):\n        \"\"\"\n        The parameter type descriptor list for a method, or None for a\n        field.  Type descriptors are shorthand identifiers for the\n        builtin java types.\n        \"\"\"\n\n        if not self.is_method:\n            return tuple()\n\n        tp = _typeseq(self.get_descriptor())\n        tp = _typeseq(tp[0][1:-1])\n\n        return tp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a tuple of pretty argument types.", "response": "def pretty_arg_types(self):\n        \"\"\"\n        Sequence of pretty argument types.\n        \"\"\"\n\n        if self.is_method:\n            types = self.get_arg_type_descriptors()\n            return (_pretty_type(t) for t in types)\n        else:\n            return tuple()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pretty_descriptor(self):\n\n        f = \" \".join(self.pretty_access_flags())\n        p = self.pretty_type()\n        n = self.get_name()\n        t = \",\".join(self.pretty_exceptions())\n\n        if n == \"<init>\":\n            # we pretend that there's no return type, even though it's\n            # V for constructors\n            p = None\n\n        if self.is_method:\n            # stick the name and args together so there's no space\n            n = \"%s(%s)\" % (n, \",\".join(self.pretty_arg_types()))\n\n        if t:\n            # assemble any throws as necessary\n            t = \"throws \" + t\n\n        return \" \".join(z for z in (f, p, n, t) if z)", "response": "return a string representation of the object as a long member name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the identifier for this entry.", "response": "def get_identifier(self):\n        \"\"\"\n        For methods this is the return type, the name and the (non-pretty)\n        argument descriptor. For fields it is simply the name.\n\n        The return-type of methods is attached to the identifier when\n        it is a bridge method, which can technically allow two methods\n        with the same name and argument type list, but with different\n        return type.\n        \"\"\"\n\n        ident = self.get_name()\n\n        if self.is_method:\n            args = \",\".join(self.get_arg_type_descriptors())\n            if self.is_bridge():\n                ident = \"%s(%s):%s\" % (ident, args, self.get_descriptor())\n            else:\n                ident = \"%s(%s)\" % (ident, args)\n\n        return ident"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a pretty version of get_identifier that is suitable for use in a log message.", "response": "def pretty_identifier(self):\n        \"\"\"\n        The pretty version of get_identifier\n        \"\"\"\n\n        ident = self.get_name()\n        if self.is_method:\n            args = \",\".join(self.pretty_arg_types())\n            ident = \"%s(%s)\" % (ident, args)\n\n        return \"%s:%s\" % (ident, self.pretty_type())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unpack(self, unpacker):\n\n        (a, b, c) = unpacker.unpack_struct(_HHI)\n\n        self.max_stack = a\n        self.max_locals = b\n        self.code = unpacker.read(c)\n\n        uobjs = unpacker.unpack_objects\n        self.exceptions = tuple(uobjs(JavaExceptionInfo, self))\n\n        self.attribs.unpack(unpacker)", "response": "Reads the code block from a buffer and updates the internal structure\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_linenumbertable(self):\n\n        lnt = self._lnt\n        if lnt is None:\n            buff = self.get_attribute(\"LineNumberTable\")\n            if buff is None:\n                lnt = tuple()\n            else:\n                with unpack(buff) as up:\n                    lnt = tuple(up.unpack_struct_array(_HH))\n            self._lnt = lnt\n        return lnt", "response": "get_linenumbertable - Returns a tuple of the Line Number Table of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a sequence of tuples where the first element is the code offset and the second is the line number.", "response": "def get_relativelinenumbertable(self):\n        \"\"\"\n        a sequence of (code_offset, line_number) pairs. Similar to the\n        get_linenumbertable method, but the line numbers start at 0\n        (they are relative to the method, not to the class file)\n        \"\"\"\n\n        lnt = self.get_linenumbertable()\n        if lnt:\n            lineoff = lnt[0][1]\n            return tuple((o, l - lineoff) for (o, l) in lnt)\n        else:\n            return tuple()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_localvariabletable(self):\n\n        buff = self.get_attribute(\"LocalVariableTable\")\n        if buff is None:\n            return tuple()\n\n        with unpack(buff) as up:\n            return tuple(up.unpack_struct_array(_HHHHH))", "response": "Returns a tuple of the localvariabletable values of the attribute LocalVariableTable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_line_for_offset(self, code_offset):\n\n        prev_line = 0\n\n        for (offset, line) in self.get_linenumbertable():\n            if offset < code_offset:\n                prev_line = line\n            elif offset == code_offset:\n                return line\n            else:\n                return prev_line\n\n        return prev_line", "response": "returns the line number given a code offset"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_code_by_lines(self):\n\n        lnt = self.get_linenumbertable()\n        if not lnt:\n            yield (None, None, self.disassemble())\n            return\n\n        lnt_offset = lnt[0][1]\n\n        cur_line = None\n        current = None\n\n        for codelet in self.disassemble():\n            abs_line = self.get_line_for_offset(codelet[0])\n\n            if cur_line == abs_line:\n                current.append(codelet)\n\n            else:\n                if cur_line is not None:\n                    yield (cur_line, cur_line - lnt_offset, current)\n                cur_line = abs_line\n                current = [codelet]\n\n        if current:\n            yield (cur_line, cur_line - lnt_offset, current)", "response": "Iterate over code by line."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef disassemble(self):\n\n        dis = self._dis_code\n        if dis is None:\n            dis = tuple(disassemble(self.code))\n            self._dis_code = dis\n\n        return dis", "response": "Disassemble the underlying bytecode instructions and generates a\n            sequence of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the internal structure of the exception table entry in an exception table entry.", "response": "def unpack(self, unpacker):\n        \"\"\"\n        unpacks an exception handler entry in an exception table. Updates\n        the internal structure of this instance\n        \"\"\"\n\n        (a, b, c, d) = unpacker.unpack_struct(_HHHH)\n\n        self.start_pc = a\n        self.end_pc = b\n        self.handler_pc = c\n        self.catch_type_ref = d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef info(self):\n\n        return (self.start_pc, self.end_pc,\n                self.handler_pc, self.get_catch_type())", "response": "return a tuple of start end handler pc catch_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unpack(self, unpacker):\n\n        (a, b, c, d) = unpacker.unpack_struct(_HHHH)\n\n        self.inner_info_ref = a\n        self.outer_info_ref = b\n        self.name_ref = c\n        self.access_flags = d", "response": "unpack this instance with data from unpacker"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _b64encode_to_str(data):\n    ret = b64encode(data)\n    if not isinstance(ret, str):  # Python3\n        return ret.decode('ascii')\n    else:\n        return ret", "response": "Wrapper around b64encode which takes and returns same - named types\n    on both Python 2 and Python 3."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef b64_encoded_digest(data, algorithm):\n    h = algorithm()\n    h.update(data)\n    return _b64encode_to_str(h.digest())", "response": "Returns a base64 encoded digest of the given data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the manifest section data.", "response": "def parse_sections(data):\n    \"\"\"\n    yields one section at a time in the form\n\n    [ (key, [val, ...]), ... ]\n\n    where key is a string and val is a string representing a single\n    line of any value associated with the key. Multiple vals may be\n    present if the value is long enough to require line continuations\n    in which case simply concatenate the vals together to get the full\n    value.\n    :type data: unicode in Py2, str in Py3\n    \"\"\"\n\n    if not data:\n        return\n\n    # our current section\n    curr = None\n\n    for lineno, line in enumerate(data.splitlines()):\n        # Clean up the line\n        cleanline = line.replace('\\x00', '')\n\n        if not cleanline:\n            # blank line means end of current section (if any)\n            if curr:\n                yield curr\n                curr = None\n\n        elif cleanline[0] == ' ':\n            # line beginning with a space means a continuation\n            if curr is None:\n                raise MalformedManifest(\"bad line continuation, \"\n                                        \" line: %i\" % lineno)\n            else:\n                # pylint: disable=unsubscriptable-object\n                curr[-1][1].append(cleanline[1:])\n\n        else:\n            # otherwise the beginning of a new k:v pair\n            if curr is None:\n                curr = list()\n\n            try:\n                key, val = cleanline.split(':', 1)\n                curr.append((key, [val[1:]]))\n            except ValueError:\n                raise MalformedManifest(\n                    \"Invalid manifest line: %i; line contents: %s\"\n                    % (lineno, cleanline))\n\n\n    # yield and leftovers\n    if curr:\n        yield curr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_key_val(stream, key, val, linesep=os.linesep):\n\n    key = key.encode('utf-8') or \"\"\n    val = val.encode('utf-8') or \"\"\n    linesep = linesep.encode('utf-8')\n\n    if not (0 < len(key) < 69):\n        raise ManifestKeyException(\"bad key length\", key)\n\n    if len(key) + len(val) > 68:\n        kvbuffer = BytesIO(b\": \".join((key, val)))\n\n        # first grab 70 (which is 72 after the trailing newline)\n        stream.write(kvbuffer.read(70))\n\n        # now only 69 at a time, because we need a leading space and a\n        # trailing \\n\n        part = kvbuffer.read(69)\n        while part:\n            stream.write(linesep + b\" \")\n            stream.write(part)\n            part = kvbuffer.read(69)\n        kvbuffer.close()\n\n    else:\n        stream.write(key)\n        stream.write(b\": \")\n        stream.write(val)\n\n    stream.write(linesep)", "response": "Writes a key - value pair to a binary MANIFEST file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef digest_chunks(chunks, algorithms=(hashlib.md5, hashlib.sha1)):\n\n    hashes = [algorithm() for algorithm in algorithms]\n\n    for chunk in chunks:\n        for h in hashes:\n            h.update(chunk)\n\n    return [_b64encode_to_str(h.digest()) for h in hashes]", "response": "Returns a base64 rep of the given digest algorithms from the given chunks of data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef file_chunk(filename, size=_BUFFERING):\n\n    def chunks():\n        with open(filename, \"rb\", _BUFFERING) as fd:\n            buf = fd.read(size)\n            while buf:\n                yield buf\n                buf = fd.read(size)\n    return chunks", "response": "returns a generator function which will emit x - sized\nInviteables chunks of filename s contents"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a generator function which will emit x - sized Inviteables chunks of the named entry in the zipfile.", "response": "def zipentry_chunk(zipfile, name, size=_BUFFERING):\n    \"\"\"\n    returns a generator function which when called will emit x-sized\n    chunks of the named entry in the zipfile object\n    \"\"\"\n\n    def chunks():\n        with zipfile.open(name) as fd:\n            buf = fd.read(size)\n            while buf:\n                yield buf\n                buf = fd.read(size)\n    return chunks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield a tuple of (relative filename, chunking function). The chunking function can be called to open and iterate over the contents of the filename.", "response": "def directory_generator(dirname, trim=0):\n    \"\"\"\n    yields a tuple of (relative filename, chunking function). The\n    chunking function can be called to open and iterate over the\n    contents of the filename.\n    \"\"\"\n\n    def gather(collect, dirname, fnames):\n        for fname in fnames:\n            df = join(dirname, fname)\n            if not isdir(df):\n                collect.append(df)\n\n    collect = list()\n    walk(dirname, gather, collect)\n    for fname in collect:\n        yield fname[trim:], file_chunk(fname)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef multi_path_generator(pathnames):\n\n    for pathname in pathnames:\n        if isdir(pathname):\n            for entry in directory_generator(pathname):\n                yield entry\n        else:\n            yield pathname, file_chunk(pathname)", "response": "This is a generator that yields all of the files found under the list of pathnames."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef single_path_generator(pathname):\n\n    if isdir(pathname):\n        trim = len(pathname)\n        if pathname[-1] != sep:\n            trim += 1\n        for entry in directory_generator(pathname, trim):\n            yield entry\n\n    else:\n        zf = ZipFile(pathname)\n        for f in zf.namelist():\n            if f[-1] != '/':\n                yield f, zipentry_chunk(zf, f)\n        zf.close()", "response": "Generator that yields name chunkgen pairs for the given file at pathname."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncommands - line call to create a manifest from a JAR file or a directory", "response": "def cli_create(argument_list):\n    \"\"\"\n    command-line call to create a manifest from a JAR file or a\n    directory\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"content\", help=\"file or directory\")\n    # TODO: shouldn't we always process directories recursively?\n    parser.add_argument(\"-r\", \"--recursive\",\n                        help=\"process directories recursively\")\n    parser.add_argument(\"-i\", \"--ignore\", nargs=\"+\", action=\"append\",\n                        help=\"patterns to ignore \"\n                             \"(can be given more than once)\")\n    parser.add_argument(\"-m\", \"--manifest\", default=None,\n                        help=\"output file (default is stdout)\")\n    parser.add_argument(\"-d\", \"--digest\",\n                        help=\"digest(s) to use, comma-separated\")\n\n    args = parser.parse_args(argument_list)\n\n    # TODO: remove digest from here, they are created when signing!\n    if args.digest is None:\n        args.digest = \"MD5,SHA1\"\n    requested_digests = args.digest.split(\",\")\n    use_digests = [_get_digest(digest) for digest in requested_digests]\n\n    if args.recursive:\n        entries = multi_path_generator(args.content)\n    else:\n        entries = single_path_generator(args.content)\n\n    mf = Manifest()\n\n    ignores = [\"META-INF/*\"]\n    if args.ignore:\n        ignores.extend(*args.ignore)\n\n    for name, chunks in entries:\n        # skip the stuff that we were told to ignore\n        if ignores and fnmatches(name, *ignores):\n            continue\n\n        sec = mf.create_section(name)\n\n        digests = zip(requested_digests, digest_chunks(chunks(), use_digests))\n        for digest_name, digest_value in digests:\n            sec[digest_name + \"-Digest\"] = digest_value\n\n    if args.manifest:\n        # we'll output to the manifest file if specified, and we'll\n        # even create parent directories for it, if necessary\n        makedirsp(split(args.manifest)[0])\n        output = open(args.manifest, \"wb\")\n    else:\n        output = sys.stdout\n\n    mf.store(output)\n\n    if args.manifest:\n        output.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(args=sys.argv):\n\n    if len(args) < 2:\n        return usage(\"Command expected\")\n\n    command = args[1]\n    rest = args[2:]\n\n    if \"create\".startswith(command):\n        return cli_create(rest)\n    elif \"query\".startswith(command):\n        return cli_query(rest)\n    elif \"verify\".startswith(command):\n        return cli_verify(rest)\n    else:\n        return usage(\"Unknown command: %s\" % command)", "response": "Entry point for the manifest CLI\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates this section from an iteration of the parse_items", "response": "def load(self, items):\n        \"\"\"\n        Populate this section from an iteration of the parse_items call\n        \"\"\"\n\n        for k, vals in items:\n            self[k] = \"\".join(vals)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store(self, stream, linesep=os.linesep):\n\n        for k, v in self.items():\n            write_key_val(stream, k, v, linesep)\n\n        stream.write(linesep.encode('utf-8'))", "response": "Serialize this section and write it to a binary stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserializes the section and return it as bytes", "response": "def get_data(self, linesep=os.linesep):\n        \"\"\"\n        Serialize the section and return it as bytes\n        :return bytes\n        \"\"\"\n\n        stream = BytesIO()\n        self.store(stream, linesep)\n        return stream.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of keys ending with given suffix.", "response": "def keys_with_suffix(self, suffix):\n        \"\"\"\n        :return: list of keys ending with given :suffix:.\n        \"\"\"\n        return [k.rstrip(suffix) for k in self.keys() if k.endswith(suffix)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates and return a new sub - section with the given Name attribute.", "response": "def create_section(self, name, overwrite=True):\n        \"\"\"\n        create and return a new sub-section of this manifest, with the\n        given Name attribute. If a sub-section already exists with\n        that name, it will be lost unless overwrite is False in which\n        case the existing sub-section will be returned.\n        \"\"\"\n\n        if overwrite:\n            sect = ManifestSection(name)\n            self.sub_sections[name] = sect\n\n        else:\n            sect = self.sub_sections.get(name, None)\n            if sect is None:\n                sect = ManifestSection(name)\n                self.sub_sections[name] = sect\n\n        return sect"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_file(self, filename):\n\n        with open(filename, \"rb\", _BUFFERING) as stream:\n            self.parse(stream.read())", "response": "Load the object from the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the manifest and populate instance with values and sub - sections.", "response": "def parse(self, data):\n        \"\"\"\n        populate instance with values and sub-sections\n        :param data: UTF-8 encoded manifest\n        :type data: bytes\n        \"\"\"\n\n        data = data.decode('utf-8')\n        self.linesep = detect_linesep(data)\n\n        # the first section is the main one for the manifest. It's\n        # also where we will check for our newline separator\n        sections = parse_sections(data)\n        self.load(next(sections))\n\n        # and all following sections are considered sub-sections\n        for section in sections:\n            next_section = ManifestSection(None)\n            next_section.load(section)\n            self.sub_sections[next_section.primary()] = next_section"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store(self, stream, linesep=None):\n\n        # either specified here, specified on the instance, or the OS\n        # default\n        linesep = linesep or self.linesep or os.linesep\n\n        ManifestSection.store(self, stream, linesep)\n        for sect in sorted(self.sub_sections.values()):\n            sect.store(stream, linesep)", "response": "Serialize the manifest to a binary stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_main_section(self, linesep=None):\n\n        linesep = linesep or self.linesep or os.linesep\n\n        stream = BytesIO()\n        ManifestSection.store(self, stream, linesep)\n        return stream.getvalue()", "response": "Serialize just the main section of the manifest and return it as bytes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying checksums present in the jar file against the jar content.", "response": "def verify_jar_checksums(self, jar_file, strict=True):\n        \"\"\"\n        Verify checksums, present in the manifest, against the JAR content.\n        :return: list of entries for which verification has failed\n        \"\"\"\n\n        verify_failures = []\n\n        zip_file = ZipFile(jar_file)\n        for filename in zip_file.namelist():\n            if file_skips_verification(filename):\n                continue\n\n            file_section = self.create_section(filename, overwrite=False)\n\n            digests = file_section.keys_with_suffix(\"-Digest\")\n            if not digests and strict:\n                verify_failures.append(filename)\n                continue\n\n            for java_digest in digests:\n                read_digest = file_section.get(java_digest + \"-Digest\")\n                calculated_digest = b64_encoded_digest(\n                    zip_file.read(filename),\n                    _get_digest(java_digest))\n\n                if calculated_digest == read_digest:\n                    # found a match\n                    break\n            else:\n                # for all the digests, not one of them matched. Add\n                # this filename to the error list\n                verify_failures.append(filename)\n\n        return verify_failures"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd all but signature - related entries in jar_file to the manifest.", "response": "def add_jar_entries(self, jar_file, digest_name=\"SHA-256\"):\n        \"\"\"\n        Add manifest sections for all but signature-related entries\n        of :param jar_file.\n        :param digest_name The digest algorithm to use\n        :return None\n        TODO: join code with cli_create\n        \"\"\"\n\n        key_digest = digest_name + \"-Digest\"\n        digest = _get_digest(digest_name)\n\n        with ZipFile(jar_file, 'r') as jar:\n            for entry in jar.namelist():\n                if file_skips_verification(entry):\n                    continue\n                section = self.create_section(entry)\n                section[key_digest] = b64_encoded_digest(jar.read(entry),\n                                                         digest)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all items from this manifest and clears all sub - sections and all items from this manifest and all sub - sections.", "response": "def clear(self):\n        \"\"\"\n        removes all items from this manifest, and clears and removes all\n        sub-sections\n        \"\"\"\n\n        for sub in self.sub_sections.values():\n            sub.clear()\n        self.sub_sections.clear()\n\n        ManifestSection.clear(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef digest_manifest(self, manifest, java_algorithm=\"SHA-256\"):\n\n        # pick a line separator for creating checksums of the manifest\n        # contents. We want to use either the one from the given\n        # manifest, or the OS default if it hasn't specified one.\n        linesep = manifest.linesep or os.linesep\n\n        all_key = java_algorithm + \"-Digest-Manifest\"\n        main_key = java_algorithm + \"-Digest-Manifest-Main-Attributes\"\n        sect_key = java_algorithm + \"-Digest\"\n\n        digest = _get_digest(java_algorithm)\n        accum = manifest.get_main_section()\n        self[main_key] = b64_encoded_digest(accum, digest)\n\n        for sub_section in manifest.sub_sections.values():\n            sub_data = sub_section.get_data(linesep)\n            sf_sect = self.create_section(sub_section.primary())\n            sf_sect[sect_key] = b64_encoded_digest(sub_data, digest)\n            accum += sub_data\n\n        self[all_key] = b64_encoded_digest(accum, digest)", "response": "Create a digest based off of the contents of an existing manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_manifest_main_checksum(self, manifest):\n\n        # NOTE: JAR spec does not state whether there can be >1 digest used,\n        # and should the validator require any or all digests to match.\n        # We allow mismatching digests and require just one to be correct.\n        # We see no security risk: it is the signer of the .SF file\n        # who shall check, what is being signed.\n        for java_digest in self.keys_with_suffix(\"-Digest-Manifest\"):\n            whole_mf_digest = b64_encoded_digest(\n                manifest.get_data(),\n                _get_digest(java_digest))\n\n            # It is enough for at least one digest to be correct\n            if whole_mf_digest == self.get(java_digest + \"-Digest-Manifest\"):\n                return True\n\n        return False", "response": "Verify the checksum over the manifest main section."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_manifest_entry_checksums(self, manifest, strict=True):\n\n        # TODO: this behavior is probably wrong -- surely if there are\n        # multiple digests, they should ALL match, or verification would\n        # fail?\n\n        failures = []\n\n        for s in manifest.sub_sections.values():\n            sf_section = self.create_section(s.primary(), overwrite=False)\n\n            digests = s.keys_with_suffix(\"-Digest\")\n            if not digests and strict:\n                failures.append(s.primary())\n                continue\n\n            for java_digest in digests:\n                section_digest = b64_encoded_digest(\n                    s.get_data(manifest.linesep),\n                    _get_digest(java_digest))\n\n                if section_digest == sf_section.get(java_digest + \"-Digest\"):\n                    # found a match, verified\n                    break\n            else:\n                # no matches found for the digests present\n                failures.append(s.primary())\n\n        return failures", "response": "Verifies the checksums over the manifest entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if entry matches any of the glob patterns false otherwise.", "response": "def fnmatches(entry, *pattern_list):\n    \"\"\"\n    returns true if entry matches any of the glob patterns, false\n    otherwise\n    \"\"\"\n\n    for pattern in pattern_list:\n        if pattern and fnmatch(entry, pattern):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copydir(orig, dest):\n\n    copied = list()\n\n    makedirsp(dest)\n\n    for root, dirs, files in walk(orig):\n        for d in dirs:\n            # ensure directories exist\n            makedirsp(join(dest, d))\n\n        for f in files:\n            root_f = join(root, f)\n            dest_f = join(dest, relpath(root_f, orig))\n            copy(root_f, dest_f)\n            copied.append((root_f, dest_f))\n\n    return copied", "response": "Copies a directory orig to dest. Returns a list of tuples of tuples of relative filenames which were copied from orig to dest. Returns a list of tuples of relative filenames which were copied from orig to dest."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare(left, right):\n\n    dc = dircmp(left, right, ignore=[])\n    return _gen_from_dircmp(dc, left, right)", "response": "Returns a generator that yields pairs indicating the contents of the left and right directories."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _gen_from_dircmp(dc, lpath, rpath):\n\n    left_only = dc.left_only\n    left_only.sort()\n\n    for f in left_only:\n        fp = join(dc.left, f)\n        if isdir(fp):\n            for r, _ds, fs in walk(fp):\n                r = relpath(r, lpath)\n                for f in fs:\n                    yield(LEFT, join(r, f))\n        else:\n            yield (LEFT, relpath(fp, lpath))\n\n    right_only = dc.right_only\n    right_only.sort()\n\n    for f in right_only:\n        fp = join(dc.right, f)\n        if isdir(fp):\n            for r, _ds, fs in walk(fp):\n                r = relpath(r, rpath)\n                for f in fs:\n                    yield(RIGHT, join(r, f))\n        else:\n            yield (RIGHT, relpath(fp, rpath))\n\n    diff_files = dc.diff_files\n    diff_files.sort()\n\n    for f in diff_files:\n        yield (DIFF, join(relpath(dc.right, rpath), f))\n\n    same_files = dc.same_files\n    same_files.sort()\n\n    for f in same_files:\n        yield (BOTH, join(relpath(dc.left, lpath), f))\n\n    subdirs = dc.subdirs.values()\n    subdirs = sorted(subdirs)\n    for sub in subdirs:\n        for event in _gen_from_dircmp(sub, lpath, rpath):\n            yield event", "response": "Generate the list of all the events that are in the dircmp tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncollecting the results of compare into the given lists", "response": "def collect_compare_into(left, right, added, removed, altered, same):\n    \"\"\"\n    collect the results of compare into the given lists (or None if\n    you do not wish to collect results of that type. Returns a tuple\n    of (added, removed, altered, same)\n    \"\"\"\n\n    for event, filename in compare(left, right):\n        if event == LEFT:\n            group = removed\n\n        elif event == RIGHT:\n            group = added\n\n        elif event == DIFF:\n            group = altered\n\n        elif event == BOTH:\n            group = same\n\n        else:\n            assert False\n\n        if group is not None:\n            group.append(filename)\n\n    return added, removed, altered, same"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify(certificate, jar_file, sf_name=None):\n\n    # Step 0: get the \"key alias\", used also for naming of sig-related files.\n    zip_file = ZipFile(jar_file)\n    sf_files = [f for f in zip_file.namelist() if file_matches_sigfile(f)]\n\n    if len(sf_files) == 0:\n        raise JarSignatureMissingError(\"No .SF file in %s\" % jar_file)\n\n    elif len(sf_files) > 1:\n        if sf_name is None:\n            msg = \"Multiple .SF files in %s, but SF_NAME.SF not specified\" \\\n                % jar_file\n            raise VerificationError(msg)\n\n        elif ('META-INF/' + sf_name) in sf_files:\n            sf_filename = 'META-INF/' + sf_name\n\n        else:\n            msg = \"No .SF file in %s named META-INF/%s (found %d .SF files)\" \\\n                % (jar_file, sf_name, len(sf_files))\n            raise VerificationError(msg)\n\n    elif len(sf_files) == 1:\n        if sf_name is None:\n            sf_filename = sf_files[0]\n\n        elif sf_files[0] == 'META-INF/' + sf_name:\n            sf_filename = sf_files[0]\n\n        else:\n            msg = \"No .SF file in %s named META-INF/%s\" % (jar_file, sf_name)\n            raise VerificationError(msg)\n\n    key_alias = sf_filename[9:-3]  # \"META-INF/%s.SF\"\n    sf_data = zip_file.read(sf_filename)\n\n    # Step 1: check the crypto part.\n    file_list = zip_file.namelist()\n    sig_block_filename = None\n\n    # JAR specification mentions only RSA and DSA; jarsigner also has EC\n    # TODO: what about \"SIG-*\"?\n    signature_extensions = (\"RSA\", \"DSA\", \"EC\")\n    for extension in signature_extensions:\n        candidate_filename = \"META-INF/%s.%s\" % (key_alias, extension)\n        if candidate_filename in file_list:\n            sig_block_filename = candidate_filename\n            break\n\n    if sig_block_filename is None:\n        msg = \"None of %s found in JAR\" % \\\n              \", \".join(key_alias + \".\" + x for x in signature_extensions)\n        raise JarSignatureMissingError(msg)\n\n    sig_block_data = zip_file.read(sig_block_filename)\n    try:\n        verify_signature_block(certificate, sf_data, sig_block_data)\n\n    except SignatureBlockVerificationError as message:\n        message = \"Signature block verification failed: %s\" % message\n        raise SignatureBlockFileVerificationError(message)\n\n    # KEYALIAS.SF is correctly signed.\n    # Step 2: Check that it contains correct checksum of the manifest.\n    signature_manifest = SignatureManifest()\n    signature_manifest.parse(sf_data)\n\n    jar_manifest = Manifest()\n    jar_manifest.load_from_jar(jar_file)\n\n    errors = signature_manifest.verify_manifest(jar_manifest)\n    if len(errors) > 0:\n        msg = \"%s: in .SF file, section checksum(s) failed for: %s\" \\\n              % (jar_file, \",\".join(errors))\n        raise ManifestChecksumError(msg)\n\n    # Checksums of MANIFEST.MF itself are correct.\n\n    # Step 3: Check that it contains valid checksums for each file\n    # from the JAR.  NOTE: the check is done for JAR entries. If some\n    # JAR entries are deleted after signing, the verification still\n    # succeeds.  This seems to not follow the reference specification,\n    # but that's what jarsigner does.\n    errors = jar_manifest.verify_jar_checksums(jar_file)\n    if len(errors) > 0:\n        msg = \"Checksum(s) for jar entries of jar file %s failed for: %s\" \\\n              % (jar_file, \",\".join(errors))\n        raise JarChecksumError(msg)\n\n    return None", "response": "Verify a JAR file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sign(jar_file, cert_file, key_file, key_alias,\n         extra_certs=None, digest=\"SHA-256\", output=None):\n    \"\"\"\n    Signs the jar (almost) identically to jarsigner.\n    :exception ManifestNotFoundError, CannotFindKeyTypeError\n    :return None\n    \"\"\"\n\n    mf = Manifest()\n    mf.load_from_jar(jar_file)\n    mf.add_jar_entries(jar_file, digest)\n\n    # create a signature manifest, and make it match the line separator\n    # style of the manifest it'll be digesting.\n    sf = SignatureManifest(linesep=mf.linesep)\n\n    sf_digest_algorithm = \"SHA-256\" if digest is None else digest\n    sf.digest_manifest(mf, sf_digest_algorithm)\n\n    sig_digest_algorithm = digest  # No point to make it different\n    sig_block_extension = private_key_type(key_file)\n\n    sigdata = sf.get_signature(cert_file, key_file,\n                               extra_certs, sig_digest_algorithm)\n\n    # We might just add new entries to the original JAR, but jarsigner puts\n    # all META-INF/ to the beginning of the archive. Let's do the same.\n    with NamedTemporaryFile() as new_jar_file:\n        new_jar = ZipFile(new_jar_file, \"w\", ZIP_DEFLATED)\n        new_jar.writestr(\"META-INF/MANIFEST.MF\", mf.get_data())\n        new_jar.writestr(\"META-INF/%s.SF\" % key_alias, sf.get_data())\n        new_jar.writestr(\"META-INF/%s.%s\" % (key_alias, sig_block_extension),\n                         sigdata)\n        jar = ZipFile(jar_file, \"a\")\n        for entry in jar.namelist():\n            # TODO: In Py2, namelist() can be of type unicode\n            if not entry.upper() == \"META-INF/MANIFEST.MF\":\n                new_jar.writestr(entry, jar.read(entry))\n\n        new_jar.close()\n        new_jar_file.flush()\n        copyfile(new_jar_file.name, jar_file if output is None else output)", "response": "Signs the jar file identically to jarsigner."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a JAR from a list of entries.", "response": "def create_jar(jar_file, entries):\n    \"\"\"\n    Create JAR from given entries.\n    :param jar_file: filename of the created JAR\n    :type jar_file: str\n    :param entries: files to put into the JAR\n    :type entries: list[str]\n    :return: None\n    \"\"\"\n\n    # 'jar' adds separate entries for directories, also for empty ones.\n    with ZipFile(jar_file, \"w\") as jar:\n        jar.writestr(\"META-INF/\", \"\")\n        jar.writestr(\"META-INF/MANIFEST.MF\", Manifest().get_data())\n        for entry in entries:\n            jar.write(entry)\n            if os.path.isdir(entry):\n                for root, dirs, files in os.walk(entry):\n                    for filename in dirs + files:\n                        jar.write(os.path.join(root, filename))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli_sign_jar(argument_list=None):\n\n    usage_message = \"jarutil s [OPTIONS] jar_file cert_file \" \\\n                    \"key_file key_alias\"\n\n    parser = argparse.ArgumentParser(usage=usage_message)\n\n    parser.add_argument(\"jar_file\", type=str,\n                        help=\"JAR file to sign\")\n    parser.add_argument(\"cert_file\", type=str,\n                        help=\"Certificate file (PEM format)\")\n    parser.add_argument(\"key_file\", type=str,\n                        help=\"Private key file (PEM format)\")\n    parser.add_argument(\"key_alias\", type=str,\n                        help=\"JAR \\\"key alias\\\" to use\")\n\n    parser.add_argument(\"-d\", \"--digest\", action=\"store\", default=\"SHA-256\",\n                        # TODO: factor to a constant\n                        help=\"Digest algorithm used for signing\")\n\n    parser.add_argument(\"-c\", \"--chain\", action=\"append\",\n                        dest=\"extra_certs\", default=[],\n                        help=\"Additional certificates to embed into the\"\n                        \" signature (PEM format). More than one \\\"-c\\\" option\"\n                        \" can be provided.\")\n\n    parser.add_argument(\"-o\", \"--output\", action=\"store\", default=None,\n                        help=\"Filename to put signed jar. If not provided, the\"\n                        \" signature is added to the original jar file.\")\n\n    args = parser.parse_args(argument_list)\n\n    try:\n        sign(args.jar_file, args.cert_file, args.key_file, args.key_alias,\n             args.extra_certs, args.digest, args.output)\n\n    except CannotFindKeyTypeError:\n        print(\"Cannot determine private key type in %s\" % args.key_file)\n        return 1\n\n    except MissingManifestError:\n        print(\"Manifest missing in jar file %s\" % args.jar_file)\n        return 2\n\n    return 0", "response": "Command - line wrapper around sign"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli_verify_jar_signature(argument_list):\n\n    usage_message = \"jarutil v file.jar trusted_certificate.pem [SF_NAME.SF]\"\n    if len(argument_list) < 2 or len(argument_list) > 3:\n        print(usage_message)\n        return 1\n\n    jar_file, certificate, sf_name = (argument_list + [None])[:3]\n    try:\n        verify(certificate, jar_file, sf_name)\n\n    except VerificationError as error_message:\n        print(error_message)\n        return 1\n\n    else:\n        print(\"Jar verified.\")\n        return 0", "response": "Command - line wrapper around verify"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef private_key_type(key_file):\n\n    keytypes = ((\"RSA\", RSA), (\"DSA\", DSA), (\"EC\", EC))\n\n    for key, ktype in keytypes:\n        try:\n            ktype.load_key(key_file)\n        except (RSA.RSAError, DSA.DSAError, ValueError):\n            continue\n        else:\n            return key\n    else:\n        raise CannotFindKeyTypeError()", "response": "Determines the type of the private key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_signature_block(openssl_digest, certificate, private_key,\n                           extra_certs, data):\n\n    \"\"\"\n    Produces a signature block for the data.\n\n    Reference\n    ---------\n    http://docs.oracle.com/javase/7/docs/technotes/guides/jar/jar.html#Digital_Signatures\n\n    Note: Oracle does not specify the content of the \"signature\n    file block\", friendly saying that \"These are binary files\n    not intended to be interpreted by humans\".\n\n    :param openssl_digest: alrogithm known to OpenSSL used to digest the data\n    :type openssl_digest: str\n    :param certificate: filename of the certificate file (PEM format)\n    :type certificate: str\n    :param private_key:filename of private key used to sign (PEM format)\n    :type private_key: str\n    :param extra_certs: additional certificates to embed into the signature (PEM format)\n    :type extra_certs: array of filenames\n    :param data: the content to be signed\n    :type data: bytes\n    :returns: content of the signature block file as produced by jarsigner\n    :rtype: bytes\n    \"\"\"  # noqa\n\n    smime = SMIME.SMIME()\n    with BIO.openfile(private_key) as k, BIO.openfile(certificate) as c:\n        smime.load_key_bio(k, c)\n\n    if extra_certs is not None:\n        # Could we use just X509.new_stack_from_der() instead?\n        stack = X509.X509_Stack()\n        for cert in extra_certs:\n            stack.push(X509.load_cert(cert))\n        smime.set_x509_stack(stack)\n\n    pkcs7 = smime.sign(BIO.MemoryBuffer(data),\n                       algo=openssl_digest,\n                       flags=(SMIME.PKCS7_BINARY |\n                              SMIME.PKCS7_DETACHED |\n                              SMIME.PKCS7_NOATTR))\n    tmp = BIO.MemoryBuffer()\n    pkcs7.write_der(tmp)\n    return tmp.read()", "response": "Create a signature block for the data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies the signature of the content over the certificate_file.", "response": "def verify_signature_block(certificate_file, content, signature):\n    \"\"\"\n    Verifies the 'signature' over the 'content', trusting the\n    'certificate'.\n\n    :param certificate_file: the trusted certificate (PEM format)\n    :type certificate_file: str\n    :param content: The signature should match this content\n    :type content: str\n    :param signature: data (DER format) subject to check\n    :type signature: str\n    :return None if the signature validates.\n    :exception SignatureBlockVerificationError\n    \"\"\"\n\n    sig_bio = BIO.MemoryBuffer(signature)\n    pkcs7 = SMIME.PKCS7(m2.pkcs7_read_bio_der(sig_bio._ptr()), 1)\n    signers_cert_stack = pkcs7.get0_signers(X509.X509_Stack())\n    trusted_cert_store = X509.X509_Store()\n    trusted_cert_store.set_verify_cb(ignore_missing_email_protection_eku_cb)\n    trusted_cert_store.load_info(certificate_file)\n    smime = SMIME.SMIME()\n    smime.set_x509_stack(signers_cert_stack)\n    smime.set_x509_store(trusted_cert_store)\n    data_bio = BIO.MemoryBuffer(content)\n\n    try:\n        smime.verify(pkcs7, data_bio)\n    except SMIME.PKCS7_Error as message:\n        raise SignatureBlockVerificationError(message)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search(self, search_term, column=\"title\", number_results=25):\n        request={\"req\":search_term,\"column\":column}\n        self.__choose_mirror()\n        if sys.version_info[0] < 3:\n            url = self.__selected_mirror+\"/search.php?\"+ \\\n                    urllib.urlencode(request)\n        else:\n            url = self.__selected_mirror+\"/search.php?\"+ \\\n                    urllib.parse.urlencode(request)\n        self.grabber.go(url)\n        search_result = []\n        nbooks = re.search(r'([0-9]*) (books|files)',\n                           self.grabber.doc.select(\n                               \"/html/body/table[2]/tr/td[1]/font\").text())\n        nbooks = int(nbooks.group(1))\n        pages_to_load = int(math.ceil(number_results/25.0)) # Pages needed to be loaded\n        # Check if the pages needed to be loaded are more than the pages available\n        if pages_to_load > int(math.ceil(nbooks/25.0)):\n            pages_to_load = int(math.ceil(nbooks/25.0))\n        for page in range(1, pages_to_load+1):\n            if len(search_result) > number_results:  # Check if we got all the results\n                break\n            url = \"\"\n            request.update({\"page\":page})\n            if sys.version_info[0] < 3:\n                url = self.__selected_mirror+\"/search.php?\"+ \\\n                      urllib.urlencode(request)\n            else:\n                url = self.__selected_mirror+\"/search.php?\"+ \\\n                      urllib.parse.urlencode(request)\n            self.grabber.go(url)\n            search_result += self.__parse_books()\n            if page != pages_to_load:\n                # Random delay because if you ask a lot of pages,your ip might get blocked.\n                time.sleep(random.randint(250, 1000)/1000.0)\n        return search_result[:number_results]", "response": "Search for a term in the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending email to the assigned lists", "response": "def send(self, obj_id):\n        \"\"\"\n        Send email to the assigned lists\n\n        :param obj_id: int\n        :return: dict|str\n        \"\"\"\n        response = self._client.session.post(\n            '{url}/{id}/send'.format(\n                url=self.endpoint_url, id=obj_id\n            )\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends email to a specific object", "response": "def send_to_contact(self, obj_id, contact_id):\n        \"\"\"\n        Send email to a specific contact\n\n        :param obj_id: int\n        :param contact_id: int\n        :return: dict|str\n        \"\"\"\n        response = self._client.session.post(\n            '{url}/{id}/send/contact/{contact_id}'.format(\n                url=self.endpoint_url, id=obj_id, contact_id=contact_id\n            )\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_owners(self):\n        response = self._client.session.get(\n            '{url}/list/owners'.format(url=self.endpoint_url)\n        )\n        return self.process_response(response)", "response": "Get a list of users available as contact owners"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_events(\n        self,\n        obj_id,\n        search='',\n        include_events=None,\n        exclude_events=None,\n        order_by='',\n        order_by_dir='ASC',\n        page=1\n    ):\n        \"\"\"\n        Get a list of a contact's engagement events\n\n        :param obj_id: int Contact ID\n        :param search: str\n        :param include_events: list|tuple\n        :param exclude_events: list|tuple\n        :param order_by: str\n        :param order_by_dir: str\n        :param page: int\n        :return: dict|str\n        \"\"\"\n        if include_events is None:\n            include_events = []\n        if exclude_events is None:\n            exclude_events = []\n\n        parameters = {\n            'search': search,\n            'includeEvents': include_events,\n            'excludeEvents': exclude_events,\n            'orderBy': order_by,\n            'orderByDir': order_by_dir,\n            'page': page\n        }\n        response = self._client.session.get(\n            '{url}/{id}/events'.format(\n                url=self.endpoint_url, id=obj_id\n            ),\n            params=parameters\n        )\n        return self.process_response(response)", "response": "Get a list of a contact s engagement events."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_contact_notes(\n        self,\n        obj_id,\n        search='',\n        start=0,\n        limit=0,\n        order_by='',\n        order_by_dir='ASC'\n    ):\n        \"\"\"\n        Get a list of a contact's notes\n\n        :param obj_id: int Contact ID\n        :param search: str\n        :param start: int\n        :param limit: int\n        :param order_by: str\n        :param order_by_dir: str\n        :return: dict|str\n        \"\"\"\n\n        parameters = {\n            'search': search,\n            'start': start,\n            'limit': limit,\n            'orderBy': order_by,\n            'orderByDir': order_by_dir,\n        }\n        response = self._client.session.get(\n            '{url}/{id}/notes'.format(\n                url=self.endpoint_url, id=obj_id\n            ),\n            params=parameters\n        )\n        return self.process_response(response)", "response": "Get a list of a specific entry in a specific entry in the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the points to a contact in the cluster.", "response": "def add_points(self, obj_id, points, **kwargs):\n        \"\"\"\n        Add the points to a contact\n\n        :param obj_id: int\n        :param points: int\n        :param kwargs: dict 'eventname' and 'actionname'\n        :return: dict|str\n        \"\"\"\n\n        response = self._client.session.post(\n            '{url}/{id}/points/plus/{points}'.format(\n                url=self.endpoint_url, id=obj_id, points=points\n            ),\n            data=kwargs\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_dnc(\n        self,\n        obj_id,\n        channel='email',\n        reason=MANUAL,\n        channel_id=None,\n        comments='via API'\n    ):\n        \"\"\"\n        Adds Do Not Contact\n\n        :param obj_id: int\n        :param channel: str\n        :param reason: str\n        :param channel_id: int\n        :param comments: str\n        :return: dict|str\n        \"\"\"\n        data = {\n            'reason': reason,\n            'channelId': channel_id,\n            'comments': comments\n        }\n        response = self._client.session.post(\n            '{url}/{id}/dnc/add/{channel}'.format(\n                url=self.endpoint_url, id=obj_id, channel=channel\n            ),\n            data=data\n        )\n        return self.process_response(response)", "response": "Adds a do not contact to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves Do Not Contact related object.", "response": "def remove_dnc(self, obj_id, channel):\n        \"\"\"\n        Removes Do Not Contact\n\n        :param obj_id: int\n        :param channel: str\n        :return: dict|str\n        \"\"\"\n        response = self._client.session.post(\n            '{url}/{id}/dnc/remove/{channel}'.format(\n                url=self.endpoint_url, id=obj_id, channel=channel\n            )\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, obj_id):\n        response = self._client.session.get(\n            '{url}/{id}'.format(\n                url=self.endpoint_url, id=obj_id\n            )\n        )\n        return self.process_response(response)", "response": "Get a single item from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_list(\n        self,\n        search='',\n        start=0,\n        limit=0,\n        order_by='',\n        order_by_dir='ASC',\n        published_only=False,\n        minimal=False\n    ):\n        \"\"\"\n        Get a list of items\n\n        :param search: str\n        :param start: int\n        :param limit: int\n        :param order_by: str\n        :param order_by_dir: str\n        :param published_only: bool\n        :param minimal: bool\n        :return: dict|str\n        \"\"\"\n\n        parameters = {}\n        args = ['search', 'start', 'limit', 'minimal']\n        for arg in args:\n            if arg in locals() and locals()[arg]:\n                parameters[arg] = locals()[arg]\n        if order_by:\n            parameters['orderBy'] = order_by\n        if order_by_dir:\n            parameters['orderByDir'] = order_by_dir\n        if published_only:\n            parameters['publishedOnly'] = 'true'\n        response = self._client.session.get(\n            self.endpoint_url, params=parameters\n        )\n        return self.process_response(response)", "response": "Get a list of items in the nova cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets published list of all the available catalnite elements", "response": "def get_published_list(\n        self, search='', start=0, limit=0, order_by='', order_by_dir='ASC'\n    ):\n        \"\"\"\n        Proxy function to get_list with published_only set to True\n        :param search: str\n        :param start: int\n        :param limit: int\n        :param order_by: str\n        :param order_by_dir: str\n        :return: dict|str\n        \"\"\"\n        return self.get_list(\n            search=search,\n            start=start,\n            limit=limit,\n            order_by=order_by,\n            order_by_dir=order_by_dir,\n            published_only=True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new item in the nova user s cache.", "response": "def create(self, parameters):\n        \"\"\"\n        Create a new item (if supported)\n\n        :param parameters: dict\n        :return: dict|str\n        \"\"\"\n        response = self._client.session.post(\n            '{url}/new'.format(url=self.endpoint_url), data=parameters\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef edit(self, obj_id, parameters, create_if_not_exists=False):\n        if create_if_not_exists:\n            response = self._client.session.put(\n                '{url}/{id}/edit'.format(\n                    url=self.endpoint_url, id=obj_id\n                ),\n                data=parameters\n            )\n        else:\n            response = self._client.session.patch(\n                '{url}/{id}/edit'.format(\n                    url=self.endpoint_url, id=obj_id\n                ),\n                data=parameters\n            )\n        return self.process_response(response)", "response": "Edit an item with option to create if it doesn t exist"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a single item from the nova.", "response": "def get(self, data_type, options=None):\n        \"\"\"\n        Get a single item\n        :param data_type: str\n        :param options: dict\n        :return: dict|str\n        \"\"\"\n        if options is None:\n            options = {}\n        response = self._client.session.get(\n            '{url}/{type}'.format(\n                url=self.endpoint_url, type=data_type\n            ),\n            params=options\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of stat items in a table.", "response": "def get(self, table='', start=0, limit=0, order=None, where=None):\n        \"\"\"\n        Get a list of stat items\n\n        :param table: str database table name\n        :param start: int\n        :param limit: int\n        :param order: list|tuple\n        :param where: list|tuple\n        :return:\n        \"\"\"\n        parameters = {}\n\n        args = ['start', 'limit', 'order', 'where']\n        for arg in args:\n            if arg in locals() and locals()[arg]:\n                parameters[arg] = locals()[arg]\n\n        response = self._client.session.get(\n            '{url}/{table}'.format(\n                url=self.endpoint_url, table=table\n            ),\n            params=parameters\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a contact to the segment", "response": "def add_contact(self, segment_id, contact_id):\n        \"\"\"\n        Add a contact to the segment\n\n        :param segment_id: int Segment ID\n        :param contact_id: int Contact ID\n        :return: dict|str\n        \"\"\"\n\n        response = self._client.session.post(\n            '{url}/{segment_id}/contact/add/{contact_id}'.format(\n                url=self.endpoint_url,\n                segment_id=segment_id,\n                contact_id=contact_id\n            )\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck permissions for a user in the nova user.", "response": "def check_permission(self, obj_id, permissions):\n        \"\"\"\n        Get list of permissions for a user\n\n        :param obj_id: int\n        :param permissions: str|list|tuple\n        :return: dict|str\n        \"\"\"\n        response = self._client.session.post(\n            '{url}/{id}/permissioncheck'.format(\n                url=self.endpoint_url, id=obj_id\n            ),\n            data={'permissions': permissions}\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_trigger_events(self, trigger_id, event_ids):\n\n        response = self._client.session.delete(\n            '{url}/{trigger_id}/events/delete'.format(\n                url=self.endpoint_url, trigger_id=trigger_id\n            ),\n            params={'events': event_ids}\n        )\n        return self.process_response(response)", "response": "Remove events from a point trigger"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_fields(self, form_id, field_ids):\n\n        response = self._client.session.delete(\n            '{url}/{form_id}/fields/delete'.format(\n                url=self.endpoint_url, form_id=form_id\n            ),\n            params={'fields': field_ids}\n        )\n        return self.process_response(response)", "response": "Remove fields from a form."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove actions from a form in a nova moderation.", "response": "def delete_actions(self, form_id, action_ids):\n        \"\"\"\n        Remove actions from a form\n\n        :param form_id: int\n        :param action_ids: list|tuple\n        :return: dict|str\n        \"\"\"\n\n        response = self._client.session.delete(\n            '{url}/{form_id}/actions/delete'.format(\n                url=self.endpoint_url, form_id=form_id\n            ),\n            params={'actions': action_ids}\n        )\n        return self.process_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the file with the given token", "response": "def update_token_tempfile(token):\n    \"\"\"\n    Example of function for token update\n    \"\"\"\n    with open(tmp, 'w') as f:\n        f.write(json.dumps(token, indent=4))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the folder to look at", "response": "def set_folder(self, folder='assets'):\n        \"\"\"\n        Changes the file folder to look at\n\n        :param folder: str [images, assets]\n        :return: None\n        \"\"\"\n        folder = folder.replace('/', '.')\n        self._endpoint = 'files/{folder}'.format(folder=folder)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _hexplot(matrix, fig, colormap):\n    umatrix_min = matrix.min()\n    umatrix_max = matrix.max()\n    n_rows, n_columns = matrix.shape\n    cmap = plt.get_cmap(colormap)\n    offsets = np.zeros((n_columns * n_rows, 2))\n    facecolors = []\n    for row in range(n_rows):\n        for col in range(n_columns):\n            if row % 2 == 0:\n                offsets[row * n_columns + col] = [col +\n                                                  0.5, 2 * n_rows - 2 * row]\n                facecolors.append(cmap((matrix[row, col] - umatrix_min) /\n                                       (umatrix_max) * 255))\n            else:\n                offsets[row * n_columns + col] = [col, 2 * n_rows - 2 * row]\n                facecolors.append(cmap((matrix[row, col] - umatrix_min) /\n                                       (umatrix_max) * 255))\n    polygon = np.zeros((6, 2), float)\n    polygon[:, 0] = 1.1 * np.array([0.5, 0.5, 0.0, -0.5, -0.5, 0.0])\n    polygon[:, 1] = 1.1 * np.array([-np.sqrt(3) / 6, np.sqrt(3) / 6,\n                                    np.sqrt(3) / 2 + np.sqrt(3) / 6,\n                                    np.sqrt(3) / 6, -np.sqrt(3) / 6,\n                                    -np.sqrt(3) / 2 - np.sqrt(3) / 6])\n    polygons = np.expand_dims(polygon, 0) + np.expand_dims(offsets, 1)\n    ax = fig.gca()\n    collection = mcoll.PolyCollection(\n        polygons,\n        offsets=offsets,\n        facecolors=facecolors,\n        edgecolors=facecolors,\n        linewidths=1.0,\n        offset_position=\"data\")\n    ax.add_collection(collection, autolim=False)\n    corners = ((-0.5, -0.5), (n_columns + 0.5, 2 * n_rows + 0.5))\n    ax.update_datalim(corners)\n    ax.autoscale_view(tight=True)\n    return offsets", "response": "Internal function to plot a hexagonal map."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the best matching units from a file to the Somoclu object.", "response": "def load_bmus(self, filename):\n        \"\"\"Load the best matching units from a file to the Somoclu object.\n\n        :param filename: The name of the file.\n        :type filename: str.\n        \"\"\"\n        self.bmus = np.loadtxt(filename, comments='%', usecols=(1, 2))\n        if self.n_vectors != 0 and len(self.bmus) != self.n_vectors:\n            raise Exception(\"The number of best matching units does not match \"\n                            \"the number of data instances\")\n        else:\n            self.n_vectors = len(self.bmus)\n        tmp = self.bmus[:, 0].copy()\n        self.bmus[:, 0] = self.bmus[:, 1].copy()\n        self.bmus[:, 1] = tmp\n        if max(self.bmus[:, 0]) > self._n_columns - 1 or \\\n                max(self.bmus[:, 1]) > self._n_rows - 1:\n            raise Exception(\"The dimensions of the best matching units do not \"\n                            \"match that of the map\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the U - matrix from a file to the Somoclu object.", "response": "def load_umatrix(self, filename):\n        \"\"\"Load the umatrix from a file to the Somoclu object.\n\n        :param filename: The name of the file.\n        :type filename: str.\n        \"\"\"\n\n        self.umatrix = np.loadtxt(filename, comments='%')\n        if self.umatrix.shape != (self._n_rows, self._n_columns):\n            raise Exception(\"The dimensions of the U-matrix do not \"\n                            \"match that of the map\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the codebook from a file to the Somoclu object.", "response": "def load_codebook(self, filename):\n        \"\"\"Load the codebook from a file to the Somoclu object.\n\n        :param filename: The name of the file.\n        :type filename: str.\n        \"\"\"\n        self.codebook = np.loadtxt(filename, comments='%')\n        if self.n_dim == 0:\n            self.n_dim = self.codebook.shape[1]\n        if self.codebook.shape != (self._n_rows * self._n_columns,\n                                   self.n_dim):\n            raise Exception(\"The dimensions of the codebook do not \"\n                            \"match that of the map\")\n        self.codebook.shape = (self._n_rows, self._n_columns, self.n_dim)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train(self, data=None, epochs=10, radius0=0, radiusN=1,\n              radiuscooling=\"linear\",\n              scale0=0.1, scaleN=0.01, scalecooling=\"linear\"):\n        \"\"\"Train the map on the current data in the Somoclu object.\n        :param data: Optional parameter to provide training data. It is not\n                     necessary if the data was added via the method\n                     `update_data`.\n        :type data: 2D numpy.array of float32.\n        :param epochs: The number of epochs to train the map for.\n        :type epochs: int.\n        :param radius0: The initial radius on the map where the update happens\n                        around a best matching unit. Default value of 0 will\n                        trigger a value of min(n_columns, n_rows)/2.\n        :type radius0: float.\n        :param radiusN: The radius on the map where the update happens around a\n                        best matching unit in the final epoch. Default: 1.\n        :type radiusN: float.\n        :param radiuscooling: The cooling strategy between radius0 and radiusN:\n\n                                   * \"linear\": Linear interpolation (default)\n                                   * \"exponential\": Exponential decay\n        :param scale0: The initial learning scale. Default value: 0.1.\n        :type scale0: float.\n        :param scaleN: The learning scale in the final epoch. Default: 0.01.\n        :type scaleN: float.\n        :param scalecooling: The cooling strategy between scale0 and scaleN:\n\n                                   * \"linear\": Linear interpolation (default)\n                                   * \"exponential\": Exponential decay\n        :type scalecooling: str.\n        \"\"\"\n        _check_cooling_parameters(radiuscooling, scalecooling)\n        if self._data is None and data is None:\n            raise Exception(\"No data was provided!\")\n        elif data is not None:\n            self.update_data(data)\n        self._init_codebook()\n        self.umatrix.shape = (self._n_rows * self._n_columns, )\n        self.bmus.shape = (self.n_vectors * 2, )\n        wrap_train(np.ravel(self._data), epochs, self._n_columns, self._n_rows,\n                   self.n_dim, self.n_vectors, radius0, radiusN,\n                   radiuscooling, scale0, scaleN, scalecooling,\n                   self._kernel_type, self._map_type, self._grid_type,\n                   self._compact_support, self._neighborhood == \"gaussian\",\n                   self._std_coeff, self._verbose, self.codebook, self.bmus,\n                   self.umatrix)\n        self.umatrix.shape = (self._n_rows, self._n_columns)\n        self.bmus.shape = (self.n_vectors, 2)\n        self.codebook.shape = (self._n_rows, self._n_columns, self.n_dim)", "response": "Train the map on the current data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the data set in the Somoclu object.", "response": "def update_data(self, data):\n        \"\"\"Change the data set in the Somoclu object. It is useful when the\n        data is updated and the training should continue on the new data.\n\n        :param data: The training data.\n        :type data: 2D numpy.array of float32.\n        \"\"\"\n        oldn_dim = self.n_dim\n        if data.dtype != np.float32:\n            print(\"Warning: data was not float32. A 32-bit copy was made\")\n            self._data = np.float32(data)\n        else:\n            self._data = data\n        self.n_vectors, self.n_dim = data.shape\n        if self.n_dim != oldn_dim and oldn_dim != 0:\n            raise Exception(\"The dimension of the new data does not match!\")\n        self.bmus = np.zeros(self.n_vectors * 2, dtype=np.intc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef view_component_planes(self, dimensions=None, figsize=None,\n                              colormap=cm.Spectral_r, colorbar=False,\n                              bestmatches=False, bestmatchcolors=None,\n                              labels=None, zoom=None, filename=None):\n        \"\"\"Observe the component planes in the codebook of the SOM.\n\n        :param dimensions: Optional parameter to specify along which dimension\n                           or dimensions should the plotting happen. By\n                           default, each dimension is plotted in a sequence of\n                           plots.\n        :type dimension: int or list of int.\n        :param figsize: Optional parameter to specify the size of the figure.\n        :type figsize: (int, int)\n        :param colormap: Optional parameter to specify the color map to be\n                         used.\n        :type colormap: matplotlib.colors.Colormap\n        :param colorbar: Optional parameter to include a colormap as legend.\n        :type colorbar: bool.\n        :param bestmatches: Optional parameter to plot best matching units.\n        :type bestmatches: bool.\n        :param bestmatchcolors: Optional parameter to specify the color of each\n                                best matching unit.\n        :type bestmatchcolors: list of int.\n        :param labels: Optional parameter to specify the label of each point.\n        :type labels: list of str.\n        :param zoom: Optional parameter to zoom into a region on the map. The\n                     first two coordinates of the tuple are the row limits, the\n                     second tuple contains the column limits.\n        :type zoom: ((int, int), (int, int))\n        :param filename: If specified, the plot will not be shown but saved to\n                         this file.\n        :type filename: str.\n        \"\"\"\n        if self.codebook is None:\n            raise Exception(\"The codebook is not available. Either train a map\"\n                            \" or load a codebook from a file\")\n        if dimensions is None:\n            dimensions = range(self.n_dim)\n        for i in dimensions:\n            plt = self._view_matrix(self.codebook[:, :, i], figsize, colormap,\n                                    colorbar, bestmatches, bestmatchcolors,\n                                    labels, zoom, filename)\n        return plt", "response": "This function plots the component planes of the SOM."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef view_umatrix(self, figsize=None, colormap=cm.Spectral_r,\n                     colorbar=False, bestmatches=False, bestmatchcolors=None,\n                     labels=None, zoom=None, filename=None):\n        \"\"\"Plot the U-matrix of the trained map.\n\n        :param figsize: Optional parameter to specify the size of the figure.\n        :type figsize: (int, int)\n        :param colormap: Optional parameter to specify the color map to be\n                         used.\n        :type colormap: matplotlib.colors.Colormap\n        :param colorbar: Optional parameter to include a colormap as legend.\n        :type colorbar: bool.\n        :param bestmatches: Optional parameter to plot best matching units.\n        :type bestmatches: bool.\n        :param bestmatchcolors: Optional parameter to specify the color of each\n                                best matching unit.\n        :type bestmatchcolors: list of int.\n        :param labels: Optional parameter to specify the label of each point.\n        :type labels: list of str.\n        :param zoom: Optional parameter to zoom into a region on the map. The\n                     first two coordinates of the tuple are the row limits, the\n                     second tuple contains the column limits.\n        :type zoom: ((int, int), (int, int))\n        :param filename: If specified, the plot will not be shown but saved to\n                         this file.\n        :type filename: str.\n        \"\"\"\n        if self.umatrix is None:\n            raise Exception(\"The U-matrix is not available. Either train a map\"\n                            \" or load a U-matrix from a file\")\n        return self._view_matrix(self.umatrix, figsize, colormap, colorbar,\n                                 bestmatches, bestmatchcolors, labels, zoom,\n                                 filename)", "response": "Plot the U - matrix of the trained map."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the activation map of a given data instance or a new data vector :param data_vector: Optional parameter for a new vector :type data_vector: numpy.array :param data_index: Optional parameter for the index of the data instance :type data_index: int. :param activation_map: Optional parameter to pass the an activation map :type activation_map: numpy.array :param figsize: Optional parameter to specify the size of the figure. :type figsize: (int, int) :param colormap: Optional parameter to specify the color map to be used. :type colormap: matplotlib.colors.Colormap :param colorbar: Optional parameter to include a colormap as legend. :type colorbar: bool. :param bestmatches: Optional parameter to plot best matching units. :type bestmatches: bool. :param bestmatchcolors: Optional parameter to specify the color of each best matching unit. :type bestmatchcolors: list of int. :param labels: Optional parameter to specify the label of each point. :type labels: list of str. :param zoom: Optional parameter to zoom into a region on the map. The first two coordinates of the tuple are the row limits, the second tuple contains the column limits. :type zoom: ((int, int), (int, int)) :param filename: If specified, the plot will not be shown but saved to this file. :type filename: str.", "response": "def view_activation_map(self, data_vector=None, data_index=None,\n                            activation_map=None, figsize=None,\n                            colormap=cm.Spectral_r, colorbar=False,\n                            bestmatches=False, bestmatchcolors=None,\n                            labels=None, zoom=None, filename=None):\n        \"\"\"Plot the activation map of a given data instance or a new data\n        vector\n\n        :param data_vector: Optional parameter for a new vector\n        :type data_vector: numpy.array\n        :param data_index: Optional parameter for the index of the data instance\n        :type data_index: int.\n        :param activation_map: Optional parameter to pass the an activation map\n        :type activation_map: numpy.array\n        :param figsize: Optional parameter to specify the size of the figure.\n        :type figsize: (int, int)\n        :param colormap: Optional parameter to specify the color map to be\n                         used.\n        :type colormap: matplotlib.colors.Colormap\n        :param colorbar: Optional parameter to include a colormap as legend.\n        :type colorbar: bool.\n        :param bestmatches: Optional parameter to plot best matching units.\n        :type bestmatches: bool.\n        :param bestmatchcolors: Optional parameter to specify the color of each\n                                best matching unit.\n        :type bestmatchcolors: list of int.\n        :param labels: Optional parameter to specify the label of each point.\n        :type labels: list of str.\n        :param zoom: Optional parameter to zoom into a region on the map. The\n                     first two coordinates of the tuple are the row limits, the\n                     second tuple contains the column limits.\n        :type zoom: ((int, int), (int, int))\n        :param filename: If specified, the plot will not be shown but saved to\n                         this file.\n        :type filename: str.\n        \"\"\"\n        if data_vector is None and data_index is None:\n            raise Exception(\"Either specify a vector to see its activation \"\n                            \"or give an index of the training data instances\")\n        if data_vector is not None and data_index is not None:\n            raise Exception(\"You cannot specify both a data vector and the \"\n                            \"index of a training data instance\")\n        if data_vector is not None and activation_map is not None:\n            raise Exception(\"You cannot pass a previously computated\"\n                            \"activation map with a data vector\")\n        if data_vector is not None:\n            try:\n                d1, _ = data_vector.shape\n                w = data_vector.copy()\n            except ValueError:\n                d1, _ = data_vector.shape\n                w = data_vector.reshape(1, d1)\n            if w.shape[1] == 1:\n                w = w.T\n            matrix = cdist(self.codebook.reshape((self.codebook.shape[0] *\n                                                  self.codebook.shape[1],\n                                                  self.codebook.shape[2])),\n                           w, 'euclidean').T\n            matrix.shape = (self.codebook.shape[0], self.codebook.shape[1])\n        else:\n            if activation_map is None and self.activation_map is None:\n                self.get_surface_state()\n            if activation_map is None:\n                activation_map = self.activation_map\n            matrix = activation_map[data_index].reshape((self.codebook.shape[0],\n                                                         self.codebook.shape[1]))\n        return self._view_matrix(matrix, figsize, colormap, colorbar,\n                                 bestmatches, bestmatchcolors, labels, zoom,\n                                 filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _view_matrix(self, matrix, figsize, colormap, colorbar, bestmatches,\n                     bestmatchcolors, labels, zoom, filename):\n        \"\"\"Internal function to plot a map with best matching units and labels.\n        \"\"\"\n        if zoom is None:\n            zoom = ((0, self._n_rows), (0, self._n_columns))\n        if figsize is None:\n            figsize = (8, 8 / float(zoom[1][1] / zoom[0][1]))\n        fig = plt.figure(figsize=figsize)\n        if self._grid_type == \"hexagonal\":\n            offsets = _hexplot(matrix[zoom[0][0]:zoom[0][1],\n                                      zoom[1][0]:zoom[1][1]], fig, colormap)\n            filtered_bmus = self._filter_array(self.bmus, zoom)\n            filtered_bmus[:, 0] = filtered_bmus[:, 0] - zoom[1][0]\n            filtered_bmus[:, 1] = filtered_bmus[:, 1] - zoom[0][0]\n            bmu_coords = np.zeros(filtered_bmus.shape)\n            for i, (row, col) in enumerate(filtered_bmus):\n                bmu_coords[i] = offsets[col * zoom[1][1] + row]\n        else:\n            plt.imshow(matrix[zoom[0][0]:zoom[0][1], zoom[1][0]:zoom[1][1]],\n                       aspect='auto', interpolation='bicubic')\n            plt.set_cmap(colormap)\n            bmu_coords = self._filter_array(self.bmus, zoom)\n            bmu_coords[:, 0] = bmu_coords[:, 0] - zoom[1][0]\n            bmu_coords[:, 1] = bmu_coords[:, 1] - zoom[0][0]\n        if colorbar:\n            cmap = cm.ScalarMappable(cmap=colormap)\n            cmap.set_array(matrix)\n            plt.colorbar(cmap, orientation='horizontal', shrink=0.5)\n\n        if bestmatches:\n            if bestmatchcolors is None:\n                if self.clusters is None:\n                    colors = \"white\"\n                else:\n                    colors = []\n                    for bm in self.bmus:\n                        colors.append(self.clusters[bm[1], bm[0]])\n                    colors = self._filter_array(colors, zoom)\n            else:\n                colors = self._filter_array(bestmatchcolors, zoom)\n            plt.scatter(bmu_coords[:, 0], bmu_coords[:, 1], c=colors)\n\n        if labels is not None:\n            for label, col, row in zip(self._filter_array(labels, zoom),\n                                       bmu_coords[:, 0], bmu_coords[:, 1]):\n                if label is not None:\n                    plt.annotate(label, xy=(col, row), xytext=(10, -5),\n                                 textcoords='offset points', ha='left',\n                                 va='bottom',\n                                 bbox=dict(boxstyle='round,pad=0.3',\n                                           fc='white', alpha=0.8))\n        plt.axis('off')\n        if filename is not None:\n            plt.savefig(filename)\n        else:\n            plt.show()\n        return plt", "response": "Internal function to plot a map with best matching units and labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_parameters(self):\n        if self._map_type != \"planar\" and self._map_type != \"toroid\":\n            raise Exception(\"Invalid parameter for _map_type: \" +\n                            self._map_type)\n        if self._grid_type != \"rectangular\" and self._grid_type != \"hexagonal\":\n            raise Exception(\"Invalid parameter for _grid_type: \" +\n                            self._grid_type)\n        if self._neighborhood != \"gaussian\" and self._neighborhood != \"bubble\":\n            raise Exception(\"Invalid parameter for neighborhood: \" +\n                            self._neighborhood)\n        if self._kernel_type != 0 and self._kernel_type != 1:\n            raise Exception(\"Invalid parameter for kernelTye: \" +\n                            self._kernel_type)\n        if self._verbose < 0 and self._verbose > 2:\n            raise Exception(\"Invalid parameter for verbose: \" +\n                            self._kernel_type)", "response": "Internal function to verify the basic parameters of the SOM."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncluster the codebook. The clusters of the data instances can be assigned based on the BMUs. The method populates the class variable Somoclu.clusters. If viewing methods are called after clustering, but without colors for best matching units, colors will be automatically assigned based on cluster membership. :param algorithm: Optional parameter to specify a scikit-learn clustering algorithm. The default is K-means with eight clusters. :type filename: sklearn.base.ClusterMixin.", "response": "def cluster(self, algorithm=None):\n        \"\"\"Cluster the codebook. The clusters of the data instances can be\n        assigned based on the BMUs. The method populates the class variable\n        Somoclu.clusters. If viewing methods are called after clustering, but\n        without colors for best matching units, colors will be automatically\n        assigned based on cluster membership.\n\n        :param algorithm: Optional parameter to specify a scikit-learn\n                          clustering algorithm. The default is K-means with\n                          eight clusters.\n        :type filename: sklearn.base.ClusterMixin.\n        \"\"\"\n\n        import sklearn.base\n        if algorithm is None:\n            import sklearn.cluster\n            algorithm = sklearn.cluster.KMeans()\n        elif not isinstance(algorithm, sklearn.base.ClusterMixin):\n            raise Exception(\"Cannot use algorithm of type \" + type(algorithm))\n        original_shape = self.codebook.shape\n        self.codebook.shape = (self._n_columns * self._n_rows, self.n_dim)\n        linear_clusters = algorithm.fit_predict(self.codebook)\n        self.codebook.shape = original_shape\n        self.clusters = np.zeros((self._n_rows, self._n_columns), dtype=int)\n        for i, c in enumerate(linear_clusters):\n            self.clusters[i // self._n_columns, i % self._n_columns] = c"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Euclidean distance between the codebook and data.", "response": "def get_surface_state(self, data=None):\n        \"\"\"Return the Euclidean distance between codebook and data.\n\n        :param data: Optional parameter to specify data, otherwise the\n                     data used previously to train the SOM is used.\n        :type data: 2D numpy.array of float32.\n\n        :returns: The the dot product of the codebook and the data.\n        :rtype: 2D numpy.array\n        \"\"\"\n\n        if data is None:\n            d = self._data\n        else:\n            d = data\n\n        codebookReshaped = self.codebook.reshape(self.codebook.shape[0] * self.codebook.shape[1], self.codebook.shape[2])\n        parts = np.array_split(d, 200, axis=0)\n        am = np.empty((0, (self._n_columns * self._n_rows)), dtype=\"float64\")\n        for part in parts:\n            am = np.concatenate((am, (cdist((part), codebookReshaped, 'euclidean'))), axis=0)\n\n        if data is None:\n            self.activation_map = am\n        return am"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bmus(self, activation_map):\n\n        Y, X = np.unravel_index(activation_map.argmin(axis=1),\n                                (self._n_rows, self._n_columns))\n        return np.vstack((X, Y)).T", "response": "Returns the best matching units indexes of the activation map."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the similarity matrix according to the activation map and the data points.", "response": "def view_similarity_matrix(self, data=None, labels=None, figsize=None,\n                               filename=None):\n        \"\"\"Plot the similarity map according to the activation map\n\n        :param data: Optional parameter for data points to calculate the\n                     similarity with\n        :type data: numpy.array\n        :param figsize: Optional parameter to specify the size of the figure.\n        :type figsize: (int, int)\n        :param labels: Optional parameter to specify the label of each point.\n        :type labels: list of str.\n        :param filename: If specified, the plot will not be shown but saved to\n                         this file.\n        :type filename: str.\n        \"\"\"\n\n        if not have_heatmap:\n            raise Exception(\"Import dependencies missing for viewing \"\n                            \"similarity matrix. You must have seaborn and \"\n                            \"scikit-learn\")\n        if data is None and self.activation_map is None:\n            self.get_surface_state()\n        if data is None:\n            X = self.activation_map\n        else:\n            X = data\n        # Calculate the pairwise correlations as a metric for similarity\n        corrmat = 1 - pairwise_distances(X, metric=\"correlation\")\n\n        # Set up the matplotlib figure\n        if figsize is None:\n            figsize = (12, 9)\n        f, ax = plt.subplots(figsize=figsize)\n\n        # Y axis has inverted labels (seaborn default, no idea why)\n        if labels is None:\n            xticklabels = []\n            yticklabels = []\n        else:\n            xticklabels = labels\n            yticklabels = labels\n\n        # Draw the heatmap using seaborn\n        sns.heatmap(corrmat, vmax=1, vmin=-1, square=True,\n                    xticklabels=xticklabels, yticklabels=yticklabels,\n                    cmap=\"RdBu_r\", center=0)\n        f.tight_layout()\n\n        # This sets the ticks to a readable angle\n        plt.yticks(rotation=0)\n        plt.xticks(rotation=90)\n\n        # This sets the labels for the two axes\n        ax.set_yticklabels(yticklabels, ha='right', va='center', size=8)\n        ax.set_xticklabels(xticklabels, ha='center', va='top', size=8)\n\n        # Save and close the figure\n        if filename is not None:\n            plt.savefig(filename, bbox_inches='tight')\n        else:\n            plt.show()\n        return plt"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncustomize the compiler extension for the NVCC.", "response": "def customize_compiler_for_nvcc(self):\n    '''This is a verbatim copy of the NVCC compiler extension from\n    https://github.com/rmcgibbo/npcuda-example\n    '''\n    self.src_extensions.append('.cu')\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == '.cu':\n            self.set_executable('compiler_so', CUDA['nvcc'])\n            postargs = extra_postargs['nvcc']\n        else:\n            postargs = extra_postargs['cc']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        self.compiler_so = default_compiler_so\n    self._compile = _compile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_child_element(elm, child_local_name):\n    for n in range(len(elm)):\n        child_elm = elm[n]\n        tag = etree.QName(child_elm)\n        if tag.localname == child_local_name:\n            return child_elm\n    return None", "response": "Find an XML child element by local tag name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_service_url(self):\n        cas_service_url = self.authenticator.cas_service_url\n        if cas_service_url is None:\n            cas_service_url = self.request.protocol + \"://\" + self.request.host + self.request.uri\n        return cas_service_url", "response": "Make the service URL for this CAS service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_service_ticket(self, ticket):\n        app_log = logging.getLogger(\"tornado.application\")\n        http_client = AsyncHTTPClient()\n        service = self.make_service_url()\n        qs_dict = dict(service=service, ticket=ticket)\n        qs = urllib.parse.urlencode(qs_dict)\n        cas_validate_url = self.authenticator.cas_service_validate_url + \"?\" + qs\n        response = None\n        app_log.debug(\"Validate URL: {0}\".format(cas_validate_url))\n        try:\n            response = yield http_client.fetch(\n                cas_validate_url,\n                method=\"GET\",\n                ca_certs=self.authenticator.cas_client_ca_certs)\n            app_log.debug(\"Response was successful: {0}\".format(response))\n        except HTTPError as ex:\n            app_log.debug(\"Response was unsuccessful: {0}\".format(response))\n            return (False, None, None)\n        parser = etree.XMLParser()\n        root = etree.fromstring(response.body, parser=parser)\n        auth_result_elm = root[0]\n        is_success = (etree.QName(auth_result_elm).localname == 'authenticationSuccess')\n        if not is_success:\n            return (False, None, None)\n        user_elm = find_child_element(auth_result_elm, \"user\")\n        user = user_elm.text.lower()\n        attrib_results = set([])\n        attribs = find_child_element(auth_result_elm, \"attributes\")\n        if attribs is None:\n            attribs = []\n        for attrib in attribs:\n            name = etree.QName(attrib).localname\n            value = attrib.text\n            attrib_results.add((name, value))\n        return (True, user, attrib_results)", "response": "Validate a CAS service ticket."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexit all processes attempting to finish uncommitted active work before exit.", "response": "def clean_exit(signum, frame=None):\n    \"\"\"Exit all processes attempting to finish uncommitted active work before exit.\n         Can be called on an os signal or no zookeeper losing connection.\n    \"\"\"\n    global exiting\n    if exiting:\n        # Since this is set up as a handler for SIGCHLD when this kills one\n        # child it gets another signal, the global exiting avoids this running\n        # multiple times.\n        LOG.debug('Exit in progress clean_exit received additional signal %s' % signum)\n        return\n\n    LOG.info('Received signal %s, beginning graceful shutdown.' % signum)\n    exiting = True\n    wait_for_exit = False\n\n    for process in processors:\n        try:\n            if process.is_alive():\n                # Sends sigterm which any processes after a notification is sent attempt to handle\n                process.terminate()\n                wait_for_exit = True\n        except Exception:  # nosec\n            # There is really nothing to do if the kill fails, so just go on.\n            # The # nosec keeps bandit from reporting this as a security issue\n            pass\n\n    # wait for a couple seconds to give the subprocesses a chance to shut down correctly.\n    if wait_for_exit:\n        time.sleep(2)\n\n    # Kill everything, that didn't already die\n    for child in multiprocessing.active_children():\n        LOG.debug('Killing pid %s' % child.pid)\n        try:\n            os.kill(child.pid, signal.SIGKILL)\n        except Exception:  # nosec\n            # There is really nothing to do if the kill fails, so just go on.\n            # The # nosec keeps bandit from reporting this as a security issue\n            pass\n\n    if signum == signal.SIGTERM:\n        sys.exit(0)\n\n    sys.exit(signum)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n\n    config.parse_args()\n\n    # Add processors for metrics topic\n    if cfg.CONF.kafka_metrics.enabled:\n        prepare_processes(cfg.CONF.kafka_metrics,\n                          cfg.CONF.repositories.metrics_driver)\n    # Add processors for alarm history topic\n    if cfg.CONF.kafka_alarm_history.enabled:\n        prepare_processes(cfg.CONF.kafka_alarm_history,\n                          cfg.CONF.repositories.alarm_state_history_driver)\n    # Add processors for events topic\n    if cfg.CONF.kafka_events.enabled:\n        prepare_processes(cfg.CONF.kafka_events,\n                          cfg.CONF.repositories.events_driver)\n\n    # Start\n    try:\n        LOG.info('''\n\n               _____\n              /     \\   ____   ____ _____    ______ ____ _____\n             /  \\ /  \\ /  _ \\ /    \\\\\\__  \\  /  ___// ___\\\\\\__  \\\\\n            /    Y    (  <_> )   |  \\/ __ \\_\\___ \\\\  \\___ / __  \\\\_\n            \\____|__  /\\____/|___|  (____  /____  >\\___  >____  /\n                    \\/            \\/     \\/     \\/     \\/     \\/\n            __________                    .__          __\n            \\______   \\ ___________  _____|__| _______/  |_  ___________\n             |     ___// __ \\_  __ \\/  ___/  |/  ___/\\   __\\/ __ \\_  __ \\\\\n             |    |   \\  ___/|  | \\/\\___ \\|  |\\___ \\  |  | \\  ___/|  | \\/\n             |____|    \\___  >__|  /____  >__/____  > |__|  \\___  >__|\n                           \\/           \\/        \\/            \\/\n\n        ''')\n        for process in processors:\n            process.start()\n\n        # The signal handlers must be added after the processes start otherwise\n        # they run on all processes\n        signal.signal(signal.SIGCHLD, clean_exit)\n        signal.signal(signal.SIGINT, clean_exit)\n        signal.signal(signal.SIGTERM, clean_exit)\n\n        while True:\n            time.sleep(10)\n\n    except Exception:\n        LOG.exception('Error! Exiting.')\n        clean_exit(signal.SIGKILL)", "response": "Main entry point for the crawler."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_config_files():\n    # default files\n    conf_files = cfg.find_config_files(project='monasca',\n                                       prog='monasca-persister')\n    # deprecated config files (only used if standard config files are not there)\n    if len(conf_files) == 0:\n        old_conf_files = cfg.find_config_files(project='monasca',\n                                               prog='persister')\n        if len(old_conf_files) > 0:\n            LOG.warning('Found deprecated old location \"{}\" '\n                        'of main configuration file'.format(old_conf_files))\n            conf_files += old_conf_files\n    return conf_files", "response": "Get the possible configuration files accepted by oslo. config\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_conf_modules():\n    for modname in _list_module_names():\n        mod = importutils.import_module('monasca_persister.conf.' + modname)\n        required_funcs = ['register_opts', 'list_opts']\n        for func in required_funcs:\n            if hasattr(mod, func):\n                yield mod", "response": "Load all modules that contain configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_opts():\n    for mod in load_conf_modules():\n        mod_opts = mod.list_opts()\n        if type(mod_opts) is list:\n            for single_mod_opts in mod_opts:\n                yield single_mod_opts[0], single_mod_opts[1]\n        else:\n            yield mod_opts[0], mod_opts[1]", "response": "List all conf modules opts.\n    Goes through all conf modules and yields their opts.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays a simple message inside a window", "response": "def message(title=\"\", text=\"\", width=DEFAULT_WIDTH,\n            height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a simple message\n\n    :param text: text inside the window\n    :type text: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    \"\"\"\n    return _simple_dialog(Gtk.MessageType.INFO,\n                          text, title, width, height, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a simple error dialog", "response": "def error(title=\"\", text=\"\", width=DEFAULT_WIDTH,\n          height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a simple error\n\n    :param text: text inside the window\n    :type text: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    \"\"\"\n    return _simple_dialog(Gtk.MessageType.ERROR,\n                          text, title, width, height, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a simple warning dialog.", "response": "def warning(title=\"\", text=\"\", width=DEFAULT_WIDTH,\n            height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a simple warning\n\n    :param text: text inside the window\n    :type text: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    \"\"\"\n    return _simple_dialog(Gtk.MessageType.WARNING,\n                          text, title, width, height, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay a question in a window.", "response": "def question(title=\"\", text=\"\", width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a question, possible answer are yes/no.\n\n    :param text: text inside the window\n    :type text: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: The answer as a boolean\n    :rtype: bool\n    \"\"\"\n    response = _simple_dialog(Gtk.MessageType.QUESTION, text, title, width, height, timeout)\n    if response == Gtk.ResponseType.YES:\n        return True\n    elif response == Gtk.ResponseType.NO:\n        return False\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef entry(text=\"\", placeholder=\"\", title=\"\",\n          width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a text input\n\n    :param text: text inside the window\n    :type text: str\n    :param placeholder: placeholder for the input\n    :type placeholder: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: The content of the text input\n    :rtype: str\n    \"\"\"\n    dialog = ZEntryMessage(text, placeholder, title,\n                           width, height, timeout)\n    dialog.run()\n    return dialog.response", "response": "Display a text inside a window and return the content of the text."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay a text input with hidden characters", "response": "def password(text=\"\", placeholder=\"\", title=\"\",\n             width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a text input with hidden characters\n\n    :param text: text inside the window\n    :type text: str\n    :param placeholder: placeholder for the input\n    :type placeholder: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: The content of the text input\n    :rtype: str\n    \"\"\"\n    dialog = ZEntryPassword(text, placeholder, title,\n                            width, height, timeout)\n    dialog.run()\n    return dialog.response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef zlist(columns, items, print_columns=None,\n          text=\"\", title=\"\", width=DEFAULT_WIDTH,\n          height=ZLIST_HEIGHT, timeout=None):\n    \"\"\"\n    Display a list of values\n\n    :param columns: a list of columns name\n    :type columns: list of strings\n    :param items: a list of values\n    :type items: list of strings\n    :param print_columns: index of a column (return just the values from this column)\n    :type print_columns: int (None if all the columns)\n    :param text: text inside the window\n    :type text: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: A row of values from the table\n    :rtype: list\n    \"\"\"\n    dialog = ZList(columns, items, print_columns,\n                   text, title, width, height, timeout)\n    dialog.run()\n    return dialog.response", "response": "Display a list of values from a list of columns."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef file_selection(multiple=False, directory=False, save=False,\n                   confirm_overwrite=False, filename=None,\n                   title=\"\", width=DEFAULT_WIDTH,\n                   height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Open a file selection window\n\n    :param multiple: allow multiple file selection\n    :type multiple: bool\n    :param directory: only directory selection\n    :type directory: bool\n    :param save: save mode\n    :type save: bool\n    :param confirm_overwrite: confirm when a file is overwritten\n    :type confirm_overwrite: bool\n    :param filename: placeholder for the filename\n    :type filename: str\n    :param text: text inside the window\n    :type text: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: path of files selected.\n    :rtype: string or list if multiple enabled\n    \"\"\"\n    dialog = ZFileSelection(multiple, directory, save,\n                            confirm_overwrite, filename,\n                            title, width, height, timeout)\n    dialog.run()\n    return dialog.response", "response": "Open a file selection window and return the path of the selected files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calendar(text=\"\", day=None, month=None, title=\"\",\n             width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a calendar\n\n    :param text: text inside the window\n    :type text: str\n    :param day: default day\n    :type day: int\n    :param month: default month\n    :type month: int\n    :param text: text inside the window\n    :type text: str\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: (year, month, day)\n    :rtype: tuple\n    \"\"\"\n    dialog = ZCalendar(text, day, month,\n                       title, width, height, timeout)\n    dialog.run()\n    return dialog.response", "response": "Display a calendar of a ZCalendar object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef color_selection(show_palette=False, opacity_control=False, title=\"\",\n                    width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Display a color selection dialog\n\n    :param show_palette: hide/show the palette with preselected colors\n    :type show_palette: bool\n    :param opacity_control: allow to control opacity\n    :type opacity_control: bool\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: the color selected by the user\n    :rtype: str\n    \"\"\"\n    dialog = ZColorSelection(show_palette, opacity_control, title, width, height, timeout)\n    dialog.run()\n    return dialog.response", "response": "Display a color selection dialog"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scale(text=\"\", value=0, min=0 ,max=100, step=1, draw_value=True, title=\"\",\n          width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT, timeout=None):\n    \"\"\"\n    Select a number with a range widget\n\n    :param text: text inside window\n    :type text: str\n    :param value: current value\n    :type value: int\n    :param min: minimum value\n    :type min: int\n    :param max: maximum value\n    :type max: int\n    :param step: incrementation value\n    :type step: int\n    :param draw_value: hide/show cursor value\n    :type draw_value: bool\n    :param title: title of the window\n    :type title: str\n    :param width: window width\n    :type width: int\n    :param height: window height\n    :type height: int\n    :param timeout: close the window after n seconds\n    :type timeout: int\n    :return: The value selected by the user\n    :rtype: float\n    \"\"\"\n    dialog = ZScale(text, value, min, max, step,\n                    draw_value, title, width, height, timeout)\n    dialog.run()\n    return dialog.response", "response": "Select a number with a range widget"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the port and serial number for a microbit device.", "response": "def find_microbit():\n    \"\"\"\n    Returns a tuple representation of the port and serial number for a\n    connected micro:bit device. If no device is connected the tuple will be\n    (None, None).\n    \"\"\"\n    ports = list_serial_ports()\n    for port in ports:\n        if \"VID:PID=0D28:0204\" in port[2].upper():\n            return (port[0], port.serial_number)\n    return (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raw_on(serial):\n\n    def flush_to_msg(serial, msg):\n        \"\"\"Read the rx serial data until we reach an expected message.\"\"\"\n        data = serial.read_until(msg)\n        if not data.endswith(msg):\n            if COMMAND_LINE_FLAG:\n                print(data)\n            raise IOError('Could not enter raw REPL.')\n\n    def flush(serial):\n        \"\"\"Flush all rx input without relying on serial.flushInput().\"\"\"\n        n = serial.inWaiting()\n        while n > 0:\n            serial.read(n)\n            n = serial.inWaiting()\n\n    raw_repl_msg = b'raw REPL; CTRL-B to exit\\r\\n>'\n    # Send CTRL-B to end raw mode if required.\n    serial.write(b'\\x02')\n    # Send CTRL-C three times between pauses to break out of loop.\n    for i in range(3):\n        serial.write(b'\\r\\x03')\n        time.sleep(0.01)\n    flush(serial)\n    # Go into raw mode with CTRL-A.\n    serial.write(b'\\r\\x01')\n    flush_to_msg(serial, raw_repl_msg)\n    # Soft Reset with CTRL-D\n    serial.write(b'\\x04')\n    flush_to_msg(serial, b'soft reboot\\r\\n')\n    # Some MicroPython versions/ports/forks provide a different message after\n    # a Soft Reset, check if we are in raw REPL, if not send a CTRL-A again\n    data = serial.read_until(raw_repl_msg)\n    if not data.endswith(raw_repl_msg):\n        serial.write(b'\\r\\x01')\n        flush_to_msg(serial, raw_repl_msg)\n    flush(serial)", "response": "Put the device into raw mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetect if a micro : bit is connected and return a serial object to talk to it.", "response": "def get_serial():\n    \"\"\"\n    Detect if a micro:bit is connected and return a serial object to talk to\n    it.\n    \"\"\"\n    port, serial_number = find_microbit()\n    if port is None:\n        raise IOError('Could not find micro:bit.')\n    return Serial(port, SERIAL_BAUD_RATE, timeout=1, parity='N')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a command to the connected micro : bit via serial and returns the stdout and stderr output from the serial.", "response": "def execute(commands, serial=None):\n    \"\"\"\n    Sends the command to the connected micro:bit via serial and returns the\n    result. If no serial connection is provided, attempts to autodetect the\n    device.\n\n    For this to work correctly, a particular sequence of commands needs to be\n    sent to put the device into a good state to process the incoming command.\n\n    Returns the stdout and stderr output from the micro:bit.\n    \"\"\"\n    close_serial = False\n    if serial is None:\n        serial = get_serial()\n        close_serial = True\n        time.sleep(0.1)\n    result = b''\n    raw_on(serial)\n    time.sleep(0.1)\n    # Write the actual command and send CTRL-D to evaluate.\n    for command in commands:\n        command_bytes = command.encode('utf-8')\n        for i in range(0, len(command_bytes), 32):\n            serial.write(command_bytes[i:min(i + 32, len(command_bytes))])\n            time.sleep(0.01)\n        serial.write(b'\\x04')\n        response = serial.read_until(b'\\x04>')       # Read until prompt.\n        out, err = response[2:-2].split(b'\\x04', 1)  # Split stdout, stderr\n        result += out\n        if err:\n            return b'', err\n    time.sleep(0.1)\n    raw_off(serial)\n    if close_serial:\n        serial.close()\n        time.sleep(0.1)\n    return result, err"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_error(err):\n    if err:\n        decoded = err.decode('utf-8')\n        try:\n            return decoded.split('\\r\\n')[-2]\n        except Exception:\n            return decoded\n    return 'There was an error.'", "response": "Take stderr bytes returned from MicroPython and attempt to create a\n    non - verbose error message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ls(serial=None):\n    out, err = execute([\n        'import os',\n        'print(os.listdir())',\n    ], serial)\n    if err:\n        raise IOError(clean_error(err))\n    return ast.literal_eval(out.decode('utf-8'))", "response": "List the files on the micro : bit device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rm(filename, serial=None):\n    commands = [\n        \"import os\",\n        \"os.remove('{}')\".format(filename),\n    ]\n    out, err = execute(commands, serial)\n    if err:\n        raise IOError(clean_error(err))\n    return True", "response": "Removes a referenced file on the micro : bit system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put(filename, target=None, serial=None):\n    if not os.path.isfile(filename):\n        raise IOError('No such file.')\n    with open(filename, 'rb') as local:\n        content = local.read()\n    filename = os.path.basename(filename)\n    if target is None:\n        target = filename\n    commands = [\n        \"fd = open('{}', 'wb')\".format(target),\n        \"f = fd.write\",\n    ]\n    while content:\n        line = content[:64]\n        if PY2:\n            commands.append('f(b' + repr(line) + ')')\n        else:\n            commands.append('f(' + repr(line) + ')')\n        content = content[64:]\n    commands.append('fd.close()')\n    out, err = execute(commands, serial)\n    if err:\n        raise IOError(clean_error(err))\n    return True", "response": "Put a referenced file on the local file system on the BBC micro : bit."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a referenced file on the device s file system and copies it to the target directory.", "response": "def get(filename, target=None, serial=None):\n    \"\"\"\n    Gets a referenced file on the device's file system and copies it to the\n    target (or current working directory if unspecified).\n\n    If no serial object is supplied, microfs will attempt to detect the\n    connection itself.\n\n    Returns True for success or raises an IOError if there's a problem.\n    \"\"\"\n    if target is None:\n        target = filename\n    commands = [\n        \"\\n\".join([\n            \"try:\",\n            \" from microbit import uart as u\",\n            \"except ImportError:\",\n            \" try:\",\n            \"  from machine import UART\",\n            \"  u = UART(0, {})\".format(SERIAL_BAUD_RATE),\n            \" except Exception:\",\n            \"  try:\",\n            \"   from sys import stdout as u\",\n            \"  except Exception:\",\n            \"   raise Exception('Could not find UART module in device.')\"]),\n        \"f = open('{}', 'rb')\".format(filename),\n        \"r = f.read\",\n        \"result = True\",\n        \"while result:\\n result = r(32)\\n if result:\\n  u.write(result)\\n\",\n        \"f.close()\",\n    ]\n    out, err = execute(commands, serial)\n    if err:\n        raise IOError(clean_error(err))\n    # Recombine the bytes while removing \"b'\" from start and \"'\" from end.\n    with open(target, 'wb') as f:\n        f.write(out)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef version(serial=None):\n    try:\n        out, err = execute([\n            'import os',\n            'print(os.uname())',\n        ], serial)\n        if err:\n            raise ValueError(clean_error(err))\n    except ValueError:\n        # Re-raise any errors from stderr raised in the try block.\n        raise\n    except Exception:\n        # Raise a value error to indicate unable to find something on the\n        # microbit that will return parseable information about the version.\n        # It doesn't matter what the error is, we just need to indicate a\n        # failure with the expected ValueError exception.\n        raise ValueError()\n    raw = out.decode('utf-8').strip()\n    raw = raw[1:-1]\n    items = raw.split(', ')\n    result = {}\n    for item in items:\n        key, value = item.split('=')\n        result[key] = value[1:-1]\n    return result", "response": "Returns the version information for the connectedMicroPython device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef http_context(self, worker_ctx):\n        http = {}\n        if isinstance(worker_ctx.entrypoint, HttpRequestHandler):\n            try:\n                request = worker_ctx.args[0]\n                try:\n                    if request.mimetype == 'application/json':\n                        data = request.data\n                    else:\n                        data = request.form\n                except ClientDisconnected:\n                    data = {}\n\n                urlparts = urlsplit(request.url)\n                http.update({\n                    'url': '{}://{}{}'.format(\n                        urlparts.scheme, urlparts.netloc, urlparts.path\n                    ),\n                    'query_string': urlparts.query,\n                    'method': request.method,\n                    'data': data,\n                    'headers': dict(get_headers(request.environ)),\n                    'env': dict(get_environ(request.environ)),\n                })\n            except:\n                pass  # probably not a compatible entrypoint\n\n        self.client.http_context(http)", "response": "Extract the HTTP context from the worker_ctx."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge any user context to include in the sentry payload.", "response": "def user_context(self, worker_ctx, exc_info):\n        \"\"\" Merge any user context to include in the sentry payload.\n\n        Extracts user identifiers from the worker context data by matching\n        context keys with\n        \"\"\"\n        user = {}\n        for key in worker_ctx.context_data:\n            for matcher in self.user_type_context_keys:\n                if re.search(matcher, key):\n                    user[key] = worker_ctx.context_data[key]\n                    break\n\n        self.client.user_context(user)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tags_context(self, worker_ctx, exc_info):\n        tags = {\n            'call_id': worker_ctx.call_id,\n            'parent_call_id': worker_ctx.immediate_parent_call_id,\n            'service_name': worker_ctx.container.service_name,\n            'method_name': worker_ctx.entrypoint.method_name\n        }\n        for key in worker_ctx.context_data:\n            for matcher in self.tag_type_context_keys:\n                if re.search(matcher, key):\n                    tags[key] = worker_ctx.context_data[key]\n                    break\n\n        self.client.tags_context(tags)", "response": "Merge any tags to include in the sentry payload."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extra_context(self, worker_ctx, exc_info):\n        extra = {}\n        extra.update(worker_ctx.context_data)\n\n        self.client.extra_context(extra)", "response": "Merge any extra context to include in the sentry payload."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_serving_nowait(self,\n                             connection_config: ConnectionConfig,\n                             loop: Optional[asyncio.AbstractEventLoop] = None) -> None:\n        \"\"\"\n        Start serving this :class:`~lahja.endpoint.Endpoint` so that it can receive events. It is\n        not guaranteed that the :class:`~lahja.endpoint.Endpoint` is fully ready after this method\n        returns. Use :meth:`~lahja.endpoint.Endpoint.start_serving` or combine with\n        :meth:`~lahja.endpoint.Endpoint.wait_until_serving`\n        \"\"\"\n        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        self._name = connection_config.name\n        self._ipc_path = connection_config.path\n        self._create_external_api(self._ipc_path)\n        self._loop = loop\n        self._internal_loop_running = asyncio.Event(loop=self.event_loop)\n        self._receiving_loop_running = asyncio.Event(loop=self.event_loop)\n        self._internal_queue = asyncio.Queue(loop=self.event_loop)\n        self._receiving_queue = asyncio.Queue(loop=self.event_loop)\n\n        # Using `gather` (over e.g. `wait` or plain `ensure_future`) ensures that the inner futures\n        # are automatically cancelled as soon as the parent task is cancelled\n        asyncio.gather(\n            asyncio.ensure_future(self._connect_receiving_queue(), loop=self.event_loop),\n            asyncio.ensure_future(self._connect_internal_queue(), loop=self.event_loop),\n            loop=self.event_loop\n        )\n\n        self._running = True", "response": "Start serving this object for the given connection_config and loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def start_serving(self,\n                            connection_config: ConnectionConfig,\n                            loop: Optional[asyncio.AbstractEventLoop] = None) -> None:\n        \"\"\"\n        Start serving this :class:`~lahja.endpoint.Endpoint` so that it can receive events. Await\n        until the :class:`~lahja.endpoint.Endpoint` is ready.\n        \"\"\"\n        self.start_serving_nowait(connection_config, loop)\n        await self.wait_until_serving()", "response": "Start serving this : class : ~lahja. endpoint. Endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nawaits until the endpoint is ready to receive events.", "response": "async def wait_until_serving(self) -> None:\n        \"\"\"\n        Await until the ``Endpoint`` is ready to receive events.\n        \"\"\"\n        await asyncio.gather(\n            self._receiving_loop_running.wait(),\n            self._internal_loop_running.wait(),\n            loop=self.event_loop\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect_to_endpoints_blocking(self, *endpoints: ConnectionConfig, timeout: int=30) -> None:\n        self._throw_if_already_connected(*endpoints)\n        for endpoint in endpoints:\n            wait_for_path_blocking(endpoint.path, timeout)\n            self._connect_if_not_already_connected(endpoint)", "response": "Connect to the given endpoints and block until the connection is established."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def connect_to_endpoints(self, *endpoints: ConnectionConfig) -> None:\n        self._throw_if_already_connected(*endpoints)\n        await asyncio.gather(\n            *(self._await_connect_to_endpoint(endpoint) for endpoint in endpoints),\n            loop=self.event_loop\n        )", "response": "Connect to the given endpoints and await until all connections are established."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect to the given endpoints asynchronously.", "response": "def connect_to_endpoints_nowait(self, *endpoints: ConnectionConfig) -> None:\n        \"\"\"\n        Connect to the given endpoints as soon as they become available but do not block.\n        \"\"\"\n        self._throw_if_already_connected(*endpoints)\n        for endpoint in endpoints:\n            asyncio.ensure_future(self._await_connect_to_endpoint(endpoint))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(self) -> None:\n        if not self._running:\n            return\n\n        self._running = False\n        self._receiving_queue.put_nowait((TRANSPARENT_EVENT, None))\n        self._internal_queue.put_nowait((TRANSPARENT_EVENT, None))", "response": "Stop the internal and receiver threads."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbroadcasts an event to all connected Endpoints.", "response": "def broadcast(self, item: BaseEvent, config: Optional[BroadcastConfig] = None) -> None:\n        \"\"\"\n        Broadcast an instance of :class:`~lahja.misc.BaseEvent` on the event bus. Takes\n        an optional second parameter of :class:`~lahja.misc.BroadcastConfig` to decide\n        where this event should be broadcasted to. By default, events are broadcasted across\n        all connected endpoints with their consuming call sites.\n        \"\"\"\n        item._origin = self.name\n        if config is not None and config.internal:\n            # Internal events simply bypass going through the central event bus\n            # and are directly put into the local receiving queue instead.\n            self._internal_queue.put_nowait((item, config))\n        else:\n            # Broadcast to every connected Endpoint that is allowed to receive the event\n            compressed_item = self._compress_event(item)\n            for name, connector in self._connected_endpoints.items():\n                allowed = (config is None) or config.allowed_to_receive(name)\n                if allowed:\n                    connector.put_nowait((compressed_item, config))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a request to the broker and wait on an expected response.", "response": "async def request(self,\n                      item: BaseRequestResponseEvent[TResponse],\n                      config: Optional[BroadcastConfig] = None) -> TResponse:\n        \"\"\"\n        Broadcast an instance of :class:`~lahja.misc.BaseRequestResponseEvent` on the event bus and\n        immediately wait on an expected answer of type :class:`~lahja.misc.BaseEvent`. Optionally\n        pass a second parameter of :class:`~lahja.misc.BroadcastConfig` to decide where the request\n        should be broadcasted to. By default, requests are broadcasted across all connected\n        endpoints with their consuming call sites.\n        \"\"\"\n        item._origin = self.name\n        item._id = str(uuid.uuid4())\n\n        future: asyncio.Future = asyncio.Future(loop=self.event_loop)\n        self._futures[item._id] = future\n\n        self.broadcast(item, config)\n\n        future.add_done_callback(functools.partial(self._remove_cancelled_future, item._id))\n\n        result = await future\n\n        expected_response_type = item.expected_response_type()\n        if not isinstance(result, expected_response_type):\n            raise UnexpectedResponse(\n                f\"The type of the response is {type(result)}, expected: {expected_response_type}\"\n            )\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subscribe(self,\n                  event_type: Type[TSubscribeEvent],\n                  handler: Callable[[TSubscribeEvent], None]) -> Subscription:\n        \"\"\"\n        Subscribe to receive updates for any event that matches the specified event type.\n        A handler is passed as a second argument an :class:`~lahja.misc.Subscription` is returned\n        to unsubscribe from the event if needed.\n        \"\"\"\n        if event_type not in self._handler:\n            self._handler[event_type] = []\n\n        casted_handler = cast(Callable[[BaseEvent], Any], handler)\n\n        self._handler[event_type].append(casted_handler)\n\n        return Subscription(lambda: self._handler[event_type].remove(casted_handler))", "response": "Subscribe to receive updates for any event that matches the specified event type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def stream(self,\n                     event_type: Type[TStreamEvent],\n                     num_events: Optional[int] = None) -> AsyncGenerator[TStreamEvent, None]:\n        \"\"\"\n        Stream all events that match the specified event type. This returns an\n        ``AsyncIterable[BaseEvent]`` which can be consumed through an ``async for`` loop.\n        An optional ``num_events`` parameter can be passed to stop streaming after a maximum amount\n        of events was received.\n        \"\"\"\n        queue: asyncio.Queue = asyncio.Queue()\n\n        if event_type not in self._queues:\n            self._queues[event_type] = []\n\n        self._queues[event_type].append(queue)\n        i = None if num_events is None else 0\n        while True:\n            try:\n                yield await queue.get()\n            except GeneratorExit:\n                self._queues[event_type].remove(queue)\n                break\n            except asyncio.CancelledError:\n                self._queues[event_type].remove(queue)\n                break\n            else:\n                if i is None:\n                    continue\n\n                i += 1\n\n                if i >= cast(int, num_events):\n                    self._queues[event_type].remove(queue)\n                    break", "response": "Streams all events that match the event type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for a single instance of an event that matches the specified event type.", "response": "async def wait_for(self, event_type: Type[TWaitForEvent]) -> TWaitForEvent:  # type: ignore\n        \"\"\"\n        Wait for a single instance of an event that matches the specified event type.\n        \"\"\"\n        # mypy thinks we are missing a return statement but this seems fair to do\n        async for event in self.stream(event_type, num_events=1):\n            return event"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwaits up to timeout seconds for the given path to appear at path otherwise raises a TimeoutError.", "response": "def wait_for_path_blocking(path: pathlib.Path, timeout: int=30) -> None:\n    \"\"\"\n    Waits up to ``timeout`` seconds for the path to appear at path\n    ``path`` otherwise raises :exc:`TimeoutError`.\n    \"\"\"\n    start_at = time.monotonic()\n    while time.monotonic() - start_at < timeout:\n        if path.exists():\n            return\n        else:\n            time.sleep(0.05)\n\n    raise TimeoutError(f\"IPC socket file {path} has not appeared in {timeout} seconds\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_to_heap(self, heap, descriptors='stale', data='stale'):\n        if descriptors not in ['stale', 'all', 'none']:\n            raise ValueError(\"descriptors must be one of 'stale', 'all', 'none'\")\n        if data not in ['stale', 'all', 'none']:\n            raise ValueError(\"data must be one of 'stale', 'all', 'none'\")\n        for item in self._item_group.values():\n            info = self._get_info(item)\n            if (descriptors == 'all') or (descriptors == 'stale'\n                                          and self._descriptor_stale(item, info)):\n                heap.add_descriptor(item)\n                info.descriptor_cnt = self._descriptor_cnt\n            if item.value is not None:\n                if (data == 'all') or (data == 'stale' and info.version != item.version):\n                    heap.add_item(item)\n                    info.version = item.version\n        self._descriptor_cnt += 1\n        return heap", "response": "Adds all the new items and descriptors to the heap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_heap(self, *args, **kwargs):\n        heap = Heap(self._flavour)\n        self.add_to_heap(heap, *args, **kwargs)\n        return heap", "response": "Return a new heap which contains all the new items and item\n        descriptors since the last call."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a connection to the broker.", "response": "def connect(cls, *args, **kwargs):\n        \"\"\"Open a connection.\n\n        The arguments are the same as for the constructor of\n        :py:class:`spead2.send.TcpStream`.\n        \"\"\"\n        loop = kwargs.get('loop')\n        if loop is None:\n            loop = trollius.get_event_loop()\n        future = trollius.Future(loop=loop)\n\n        def callback(arg):\n            if not future.done():\n                if isinstance(arg, Exception):\n                    loop.call_soon_threadsafe(future.set_exception, arg)\n                else:\n                    loop.call_soon_threadsafe(future.set_result, arg)\n\n        stream = cls(callback, *args, **kwargs)\n        yield From(future)\n        raise Return(stream)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove all waiters that are done.", "response": "def _clear_done_waiters(self):\n        \"\"\"Remove waiters that are done (should only happen if they are cancelled)\"\"\"\n        while self._waiters and self._waiters[0].done():\n            self._waiters.popleft()\n        if not self._waiters:\n            self._stop_listening()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_range_list(ranges):\n    if not ranges:\n        return []\n    parts = ranges.split(',')\n    out = []\n    for part in parts:\n        fields = part.split('-', 1)\n        if len(fields) == 2:\n            start = int(fields[0])\n            end = int(fields[1])\n            out.extend(range(start, end + 1))\n        else:\n            out.append(int(fields[0]))\n    return out", "response": "Split a string like 2 3 5 8 9 - 11 into a list of integers. This is intended to ease adding command - line options for dealing with affinity."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattempts to convert a SPEAD format specification to a numpy dtype.", "response": "def _parse_format(cls, fmt):\n        \"\"\"Attempt to convert a SPEAD format specification to a numpy dtype.\n        Where necessary, `O` is used.\n\n        Raises\n        ------\n        ValueError\n            If the format is illegal\n        \"\"\"\n        fields = []\n        if not fmt:\n            raise ValueError('empty format')\n        for code, length in fmt:\n            if length == 0:\n                raise ValueError('zero-length field (bug_compat mismatch?)')\n            if ((code in ('u', 'i') and length in (8, 16, 32, 64)) or\n                    (code == 'f' and length in (32, 64))):\n                fields.append('>' + code + str(length // 8))\n            elif code == 'b' and length == 8:\n                fields.append('?')\n            elif code == 'c' and length == 8:\n                fields.append('S1')\n            else:\n                if code not in ['u', 'i', 'b']:\n                    raise ValueError('illegal format ({}, {})'.format(code, length))\n                fields.append('O')\n        return _np.dtype(','.join(fields))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef itemsize_bits(self):\n        if self.dtype is not None:\n            return self.dtype.itemsize * 8\n        else:\n            return sum(x[1] for x in self.format)", "response": "Number of bits per element in the array"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the dynamic shape given incoming data that is big enough to hold max_elements elements.", "response": "def dynamic_shape(self, max_elements):\n        \"\"\"Determine the dynamic shape, given incoming data that is big enough\n        to hold `max_elements` elements.\n        \"\"\"\n        known = 1\n        unknown_pos = -1\n        for i, x in enumerate(self.shape):\n            if x is not None:\n                known *= x\n            else:\n                assert unknown_pos == -1, 'Shape has multiple unknown dimensions'\n                unknown_pos = i\n        if unknown_pos == -1:\n            return self.shape\n        else:\n            shape = list(self.shape)\n            if known == 0:\n                shape[unknown_pos] = 0\n            else:\n                shape[unknown_pos] = max_elements // known\n            return shape"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine whether the given shape is compatible with the variable - sized shape for this descriptor.", "response": "def compatible_shape(self, shape):\n        \"\"\"Determine whether `shape` is compatible with the (possibly\n        variable-sized) shape for this descriptor\"\"\"\n        if len(shape) != len(self.shape):\n            return False\n        for x, y in zip(self.shape, shape):\n            if x is not None and x != y:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _transform_value(self):\n        value = self.value\n        if value is None:\n            raise ValueError('Cannot send a value of None')\n        if (isinstance(value, (six.binary_type, six.text_type)) and\n                len(self.shape) == 1):\n            # This is complicated by Python 3 not providing a simple way to\n            # turn a bytes object into a list of one-byte objects, the way\n            # list(str) does.\n            value = [self.value[i : i + 1] for i in range(len(self.value))]\n        value = _np.array(value, dtype=self._internal_dtype, order=self.order, copy=False)\n        if not self.compatible_shape(value.shape):\n            raise ValueError('Value has shape {}, expected {}'.format(value.shape, self.shape))\n        return value", "response": "Mangle the value into a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an object that implements the buffer protocol for the value.", "response": "def to_buffer(self):\n        \"\"\"Returns an object that implements the buffer protocol for the value.\n        It can be either the original value (on the numpy fast path), or a new\n        temporary object.\n        \"\"\"\n        value = self._transform_value()\n        if self._fastpath != _FASTPATH_NUMPY:\n            bit_length = self.itemsize_bits * self._num_elements()\n            out = bytearray((bit_length + 7) // 8)\n            gen = self._write_bits(out)\n            gen.send(None)  # Initialise the generator\n            # If it's a scalar, unpack it. That way, the input to the\n            # final level of recursion in _store_recursive is always\n            # the scalar rather than the 0D array.\n            if len(self.shape) == 0:\n                value = value[()]\n            self._store_recursive(len(self.shape), value, gen)\n            gen.close()\n            return out\n        else:\n            if self.order == 'F':\n                # numpy doesn't allow buffer protocol to be used on arrays that\n                # aren't C-contiguous, but transposition just fiddles the\n                # strides of the view without creating a new array.\n                value = value.transpose()\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a new item to the group. The parameters are used to construct an item.", "response": "def add_item(self, *args, **kwargs):\n        \"\"\"Add a new item to the group. The parameters are used to construct an\n        :py:class:`Item`. If `id` is `None`, it will be automatically populated\n        with an ID that is not already in use.\n\n        See the class documentation for the behaviour when the name or ID\n        collides with an existing one. In addition, if the item descriptor is\n        identical to an existing one and a value, this value is assigned to\n        the existing item.\n        \"\"\"\n        item = Item(*args, **kwargs)\n        if item.id is None:\n            item.id = _UNRESERVED_ID\n            while item.id in self._by_id:\n                item.id += 1\n        self._add_item(item)\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the items in the internal list with the items from an incoming heap.", "response": "def update(self, heap):\n        \"\"\"Update the item descriptors and items from an incoming heap.\n\n        Parameters\n        ----------\n        heap : :class:`spead2.recv.Heap`\n            Incoming heap\n\n        Returns\n        -------\n        dict\n            Items that have been updated from this heap, indexed by name\n        \"\"\"\n        for descriptor in heap.get_descriptors():\n            item = Item.from_raw(descriptor, flavour=heap.flavour)\n            self._add_item(item)\n        updated_items = {}\n        for raw_item in heap.get_items():\n            if raw_item.id <= STREAM_CTRL_ID:\n                continue     # Special fields, not real items\n            try:\n                item = self._by_id[raw_item.id]\n            except KeyError:\n                _logger.warning('Item with ID %#x received but there is no descriptor', raw_item.id)\n            else:\n                item.set_from_raw(raw_item)\n                item.version += 1\n                updated_items[item.name] = item\n        return updated_items"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_params(self):\n        params = super().get_params()\n        params.update({'knn': self.knn,\n                       'decay': self.decay,\n                       'bandwidth': self.bandwidth,\n                       'bandwidth_scale': self.bandwidth_scale,\n                       'distance': self.distance,\n                       'thresh': self.thresh,\n                       'n_jobs': self.n_jobs,\n                       'random_state': self.random_state,\n                       'verbose': self.verbose})\n        return params", "response": "Get parameters from this object\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_params(self, **params):\n        if 'knn' in params and params['knn'] != self.knn:\n            raise ValueError(\"Cannot update knn. Please create a new graph\")\n        if 'decay' in params and params['decay'] != self.decay:\n            raise ValueError(\"Cannot update decay. Please create a new graph\")\n        if 'bandwidth' in params and params['bandwidth'] != self.bandwidth:\n            raise ValueError(\n                \"Cannot update bandwidth. Please create a new graph\")\n        if 'bandwidth_scale' in params and \\\n                params['bandwidth_scale'] != self.bandwidth_scale:\n            raise ValueError(\n                \"Cannot update bandwidth_scale. Please create a new graph\")\n        if 'distance' in params and params['distance'] != self.distance:\n            raise ValueError(\"Cannot update distance. \"\n                             \"Please create a new graph\")\n        if 'thresh' in params and params['thresh'] != self.thresh \\\n                and self.decay != 0:\n            raise ValueError(\"Cannot update thresh. Please create a new graph\")\n        if 'n_jobs' in params:\n            self.n_jobs = params['n_jobs']\n            if hasattr(self, \"_knn_tree\"):\n                self.knn_tree.set_params(n_jobs=self.n_jobs)\n        if 'random_state' in params:\n            self.random_state = params['random_state']\n        if 'verbose' in params:\n            self.verbose = params['verbose']\n        # update superclass parameters\n        super().set_params(**params)\n        return self", "response": "Set parameters on the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef knn_tree(self):\n        try:\n            return self._knn_tree\n        except AttributeError:\n            try:\n                self._knn_tree = NearestNeighbors(\n                    n_neighbors=self.knn + 1,\n                    algorithm='ball_tree',\n                    metric=self.distance,\n                    n_jobs=self.n_jobs).fit(self.data_nu)\n            except ValueError:\n                # invalid metric\n                warnings.warn(\n                    \"Metric {} not valid for `sklearn.neighbors.BallTree`. \"\n                    \"Graph instantiation may be slower than normal.\".format(\n                        self.distance),\n                    UserWarning)\n                self._knn_tree = NearestNeighbors(\n                    n_neighbors=self.knn + 1,\n                    algorithm='auto',\n                    metric=self.distance,\n                    n_jobs=self.n_jobs).fit(self.data_nu)\n            return self._knn_tree", "response": "Builds or returns the fitted KNN tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a k nearest neighbors kernel.", "response": "def build_kernel(self):\n        \"\"\"Build the KNN kernel.\n\n        Build a k nearest neighbors kernel, optionally with alpha decay.\n        Must return a symmetric matrix\n\n        Returns\n        -------\n        K : kernel matrix, shape=[n_samples, n_samples]\n            symmetric matrix with ones down the diagonal\n            with no non-negative entries.\n        \"\"\"\n        K = self.build_kernel_to_data(self.data_nu, knn=self.knn + 1)\n        return K"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a kernel from new input data Y to the existing data.", "response": "def build_kernel_to_data(self, Y, knn=None, bandwidth=None,\n                             bandwidth_scale=None):\n        \"\"\"Build a kernel from new input data `Y` to the `self.data`\n\n        Parameters\n        ----------\n\n        Y: array-like, [n_samples_y, n_features]\n            new data for which an affinity matrix is calculated\n            to the existing data. `n_features` must match\n            either the ambient or PCA dimensions\n\n        knn : `int` or `None`, optional (default: `None`)\n            If `None`, defaults to `self.knn`\n\n        bandwidth : `float`, `callable`, or `None`, optional (default: `None`)\n            If `None`, defaults to `self.bandwidth`\n\n        bandwidth_scale : `float`, optional (default : `None`)\n            Rescaling factor for bandwidth.\n            If `None`, defaults to self.bandwidth_scale\n\n        Returns\n        -------\n\n        K_yx: array-like, [n_samples_y, n_samples]\n            kernel matrix where each row represents affinities of a single\n            sample in `Y` to all samples in `self.data`.\n\n        Raises\n        ------\n\n        ValueError: if the supplied data is the wrong shape\n        \"\"\"\n        if knn is None:\n            knn = self.knn\n        if bandwidth is None:\n            bandwidth = self.bandwidth\n        if bandwidth_scale is None:\n            bandwidth_scale = self.bandwidth_scale\n        if knn > self.data.shape[0]:\n            warnings.warn(\"Cannot set knn ({k}) to be greater than \"\n                          \"n_samples ({n}). Setting knn={n}\".format(\n                              k=knn, n=self.data.shape[0]))\n\n        Y = self._check_extension_shape(Y)\n        tasklogger.log_start(\"KNN search\")\n        if self.decay is None or self.thresh == 1:\n            # binary connectivity matrix\n            K = self.knn_tree.kneighbors_graph(\n                Y, n_neighbors=knn,\n                mode='connectivity')\n            tasklogger.log_complete(\"KNN search\")\n        else:\n            # sparse fast alpha decay\n            knn_tree = self.knn_tree\n            search_knn = min(knn * 20, self.data_nu.shape[0])\n            distances, indices = knn_tree.kneighbors(\n                Y, n_neighbors=search_knn)\n            self._check_duplicates(distances, indices)\n            tasklogger.log_complete(\"KNN search\")\n            tasklogger.log_start(\"affinities\")\n            if bandwidth is None:\n                bandwidth = distances[:, knn - 1]\n\n            bandwidth = bandwidth * bandwidth_scale\n\n            radius = bandwidth * np.power(-1 * np.log(self.thresh),\n                                          1 / self.decay)\n            update_idx = np.argwhere(\n                np.max(distances, axis=1) < radius).reshape(-1)\n            tasklogger.log_debug(\"search_knn = {}; {} remaining\".format(\n                search_knn, len(update_idx)))\n            if len(update_idx) > 0:\n                distances = [d for d in distances]\n                indices = [i for i in indices]\n            while len(update_idx) > Y.shape[0] // 10 and \\\n                    search_knn < self.data_nu.shape[0] / 2:\n                # increase the knn search\n                search_knn = min(search_knn * 20, self.data_nu.shape[0])\n                dist_new, ind_new = knn_tree.kneighbors(\n                    Y[update_idx], n_neighbors=search_knn)\n                for i, idx in enumerate(update_idx):\n                    distances[idx] = dist_new[i]\n                    indices[idx] = ind_new[i]\n                update_idx = [i for i, d in enumerate(distances) if np.max(d) <\n                              (radius if isinstance(bandwidth, numbers.Number)\n                               else radius[i])]\n                tasklogger.log_debug(\"search_knn = {}; {} remaining\".format(\n                    search_knn,\n                    len(update_idx)))\n            if search_knn > self.data_nu.shape[0] / 2:\n                knn_tree = NearestNeighbors(\n                    search_knn, algorithm='brute',\n                    n_jobs=self.n_jobs).fit(self.data_nu)\n            if len(update_idx) > 0:\n                tasklogger.log_debug(\n                    \"radius search on {}\".format(len(update_idx)))\n                # give up - radius search\n                dist_new, ind_new = knn_tree.radius_neighbors(\n                    Y[update_idx, :],\n                    radius=radius\n                    if isinstance(bandwidth, numbers.Number)\n                    else np.max(radius[update_idx]))\n                for i, idx in enumerate(update_idx):\n                    distances[idx] = dist_new[i]\n                    indices[idx] = ind_new[i]\n            if isinstance(bandwidth, numbers.Number):\n                data = np.concatenate(distances) / bandwidth\n            else:\n                data = np.concatenate([distances[i] / bandwidth[i]\n                                       for i in range(len(distances))])\n\n            indices = np.concatenate(indices)\n            indptr = np.concatenate(\n                [[0], np.cumsum([len(d) for d in distances])])\n            K = sparse.csr_matrix((data, indices, indptr),\n                                  shape=(Y.shape[0], self.data_nu.shape[0]))\n            K.data = np.exp(-1 * np.power(K.data, self.decay))\n            # handle nan\n            K.data = np.where(np.isnan(K.data), 1, K.data)\n            # TODO: should we zero values that are below thresh?\n            K.data[K.data < self.thresh] = 0\n            K = K.tocoo()\n            K.eliminate_zeros()\n            K = K.tocsr()\n            tasklogger.log_complete(\"affinities\")\n        return K"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_params(self):\n        params = super().get_params()\n        params.update({'n_landmark': self.n_landmark,\n                       'n_pca': self.n_pca})\n        return params", "response": "Get parameters from this object\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset parameters on this object returning self.", "response": "def set_params(self, **params):\n        \"\"\"Set parameters on this object\n\n        Safe setter method - attributes should not be modified directly as some\n        changes are not valid.\n        Valid parameters:\n        - n_landmark\n        - n_svd\n\n        Parameters\n        ----------\n        params : key-value pairs of parameter name and new values\n\n        Returns\n        -------\n        self\n        \"\"\"\n        # update parameters\n        reset_landmarks = False\n        if 'n_landmark' in params and params['n_landmark'] != self.n_landmark:\n            self.n_landmark = params['n_landmark']\n            reset_landmarks = True\n        if 'n_svd' in params and params['n_svd'] != self.n_svd:\n            self.n_svd = params['n_svd']\n            reset_landmarks = True\n        # update superclass parameters\n        super().set_params(**params)\n        # reset things that changed\n        if reset_landmarks:\n            self._reset_landmarks()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the landmark operator for the current state of the object.", "response": "def build_landmark_op(self):\n        \"\"\"Build the landmark operator\n\n        Calculates spectral clusters on the kernel, and calculates transition\n        probabilities between cluster centers by using transition probabilities\n        between samples assigned to each cluster.\n        \"\"\"\n        tasklogger.log_start(\"landmark operator\")\n        is_sparse = sparse.issparse(self.kernel)\n        # spectral clustering\n        tasklogger.log_start(\"SVD\")\n        _, _, VT = randomized_svd(self.diff_aff,\n                                  n_components=self.n_svd,\n                                  random_state=self.random_state)\n        tasklogger.log_complete(\"SVD\")\n        tasklogger.log_start(\"KMeans\")\n        kmeans = MiniBatchKMeans(\n            self.n_landmark,\n            init_size=3 * self.n_landmark,\n            batch_size=10000,\n            random_state=self.random_state)\n        self._clusters = kmeans.fit_predict(\n            self.diff_op.dot(VT.T))\n        # some clusters are not assigned\n        tasklogger.log_complete(\"KMeans\")\n\n        # transition matrices\n        pmn = self._landmarks_to_data()\n\n        # row normalize\n        pnm = pmn.transpose()\n        pmn = normalize(pmn, norm='l1', axis=1)\n        pnm = normalize(pnm, norm='l1', axis=1)\n        landmark_op = pmn.dot(pnm)  # sparsity agnostic matrix multiplication\n        if is_sparse:\n            # no need to have a sparse landmark operator\n            landmark_op = landmark_op.toarray()\n        # store output\n        self._landmark_op = landmark_op\n        self._transitions = pnm\n        tasklogger.log_complete(\"landmark operator\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds transition matrix from new data to the graph", "response": "def extend_to_data(self, data, **kwargs):\n        \"\"\"Build transition matrix from new data to the graph\n\n        Creates a transition matrix such that `Y` can be approximated by\n        a linear combination of landmarks. Any\n        transformation of the landmarks can be trivially applied to `Y` by\n        performing\n\n        `transform_Y = transitions.dot(transform)`\n\n        Parameters\n        ----------\n\n        Y: array-like, [n_samples_y, n_features]\n            new data for which an affinity matrix is calculated\n            to the existing data. `n_features` must match\n            either the ambient or PCA dimensions\n\n        Returns\n        -------\n\n        transitions : array-like, [n_samples_y, self.data.shape[0]]\n            Transition matrix from `Y` to `self.data`\n        \"\"\"\n        kernel = self.build_kernel_to_data(data, **kwargs)\n        if sparse.issparse(kernel):\n            pnm = sparse.hstack(\n                [sparse.csr_matrix(kernel[:, self.clusters == i].sum(\n                    axis=1)) for i in np.unique(self.clusters)])\n        else:\n            pnm = np.array([np.sum(\n                kernel[:, self.clusters == i],\n                axis=1).T for i in np.unique(self.clusters)]).transpose()\n        pnm = normalize(pnm, norm='l1', axis=1)\n        return pnm"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interpolate(self, transform, transitions=None, Y=None):\n        if transitions is None and Y is None:\n            # assume Y is self.data and use standard landmark transitions\n            transitions = self.transitions\n        return super().interpolate(transform, transitions=transitions, Y=Y)", "response": "Interpolate new data onto a transformation of the graph data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets parameters from this object", "response": "def get_params(self):\n        \"\"\"Get parameters from this object\n        \"\"\"\n        params = super().get_params()\n        params.update({'knn': self.knn,\n                       'decay': self.decay,\n                       'bandwidth': self.bandwidth,\n                       'bandwidth_scale': self.bandwidth_scale,\n                       'distance': self.distance,\n                       'precomputed': self.precomputed})\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_params(self, **params):\n        if 'precomputed' in params and \\\n                params['precomputed'] != self.precomputed:\n            raise ValueError(\"Cannot update precomputed. \"\n                             \"Please create a new graph\")\n        if 'distance' in params and params['distance'] != self.distance and \\\n                self.precomputed is None:\n            raise ValueError(\"Cannot update distance. \"\n                             \"Please create a new graph\")\n        if 'knn' in params and params['knn'] != self.knn and \\\n                self.precomputed is None:\n            raise ValueError(\"Cannot update knn. Please create a new graph\")\n        if 'decay' in params and params['decay'] != self.decay and \\\n                self.precomputed is None:\n            raise ValueError(\"Cannot update decay. Please create a new graph\")\n        if 'bandwidth' in params and \\\n            params['bandwidth'] != self.bandwidth and \\\n                self.precomputed is None:\n            raise ValueError(\n                \"Cannot update bandwidth. Please create a new graph\")\n        if 'bandwidth_scale' in params and \\\n                params['bandwidth_scale'] != self.bandwidth_scale:\n            raise ValueError(\n                \"Cannot update bandwidth_scale. Please create a new graph\")\n        # update superclass parameters\n        super().set_params(**params)\n        return self", "response": "Set parameters on the object holding the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a kernel matrix for the nearest neighbors of the current instance of the class.", "response": "def build_kernel(self):\n        \"\"\"Build the KNN kernel.\n\n        Build a k nearest neighbors kernel, optionally with alpha decay.\n        If `precomputed` is not `None`, the appropriate steps in the kernel\n        building process are skipped.\n        Must return a symmetric matrix\n\n        Returns\n        -------\n        K : kernel matrix, shape=[n_samples, n_samples]\n            symmetric matrix with ones down the diagonal\n            with no non-negative entries.\n\n        Raises\n        ------\n        ValueError: if `precomputed` is not an acceptable value\n        \"\"\"\n        if self.precomputed == \"affinity\":\n            # already done\n            # TODO: should we check that precomputed matrices look okay?\n            # e.g. check the diagonal\n            K = self.data_nu\n        elif self.precomputed == \"adjacency\":\n            # need to set diagonal to one to make it an affinity matrix\n            K = self.data_nu\n            if sparse.issparse(K) and \\\n                not (isinstance(K, sparse.dok_matrix) or\n                     isinstance(K, sparse.lil_matrix)):\n                K = K.tolil()\n            K = set_diagonal(K, 1)\n        else:\n            tasklogger.log_start(\"affinities\")\n            if sparse.issparse(self.data_nu):\n                self.data_nu = self.data_nu.toarray()\n            if self.precomputed == \"distance\":\n                pdx = self.data_nu\n            elif self.precomputed is None:\n                pdx = pdist(self.data_nu, metric=self.distance)\n                if np.any(pdx == 0):\n                    pdx = squareform(pdx)\n                    duplicate_ids = np.array(\n                        [i for i in np.argwhere(pdx == 0)\n                         if i[1] > i[0]])\n                    duplicate_names = \", \".join([\"{} and {}\".format(i[0], i[1])\n                                                 for i in duplicate_ids])\n                    warnings.warn(\n                        \"Detected zero distance between samples {}. \"\n                        \"Consider removing duplicates to avoid errors in \"\n                        \"downstream processing.\".format(duplicate_names),\n                        RuntimeWarning)\n                else:\n                    pdx = squareform(pdx)\n            else:\n                raise ValueError(\n                    \"precomputed='{}' not recognized. \"\n                    \"Choose from ['affinity', 'adjacency', 'distance', \"\n                    \"None]\".format(self.precomputed))\n            if self.bandwidth is None:\n                knn_dist = np.partition(\n                    pdx, self.knn + 1, axis=1)[:, :self.knn + 1]\n                bandwidth = np.max(knn_dist, axis=1)\n            elif callable(self.bandwidth):\n                bandwidth = self.bandwidth(pdx)\n            else:\n                bandwidth = self.bandwidth\n            bandwidth = bandwidth * self.bandwidth_scale\n            pdx = (pdx.T / bandwidth).T\n            K = np.exp(-1 * np.power(pdx, self.decay))\n            # handle nan\n            K = np.where(np.isnan(K), 1, K)\n            tasklogger.log_complete(\"affinities\")\n        # truncate\n        if sparse.issparse(K):\n            if not (isinstance(K, sparse.csr_matrix) or\n                    isinstance(K, sparse.csc_matrix) or\n                    isinstance(K, sparse.bsr_matrix)):\n                K = K.tocsr()\n            K.data[K.data < self.thresh] = 0\n            K = K.tocoo()\n            K.eliminate_zeros()\n            K = K.tocsr()\n        else:\n            K[K < self.thresh] = 0\n        return K"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_kernel_to_data(self, Y, knn=None, bandwidth=None, bandwidth_scale=None):\n        if knn is None:\n            knn = self.knn\n        if bandwidth is None:\n            bandwidth = self.bandwidth\n        if bandwidth_scale is None:\n            bandwidth_scale = self.bandwidth_scale\n        if self.precomputed is not None:\n            raise ValueError(\"Cannot extend kernel on precomputed graph\")\n        else:\n            tasklogger.log_start(\"affinities\")\n            Y = self._check_extension_shape(Y)\n            pdx = cdist(Y, self.data_nu, metric=self.distance)\n            if bandwidth is None:\n                knn_dist = np.partition(pdx, knn, axis=1)[:, :knn]\n                bandwidth = np.max(knn_dist, axis=1)\n            elif callable(bandwidth):\n                bandwidth = bandwidth(pdx)\n            bandwidth = bandwidth_scale * bandwidth\n            pdx = (pdx.T / bandwidth).T\n            K = np.exp(-1 * pdx**self.decay)\n            # handle nan\n            K = np.where(np.isnan(K), 1, K)\n            K[K < self.thresh] = 0\n            tasklogger.log_complete(\"affinities\")\n        return K", "response": "Builds a transition matrix from new data to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets parameters from this object", "response": "def get_params(self):\n        \"\"\"Get parameters from this object\n        \"\"\"\n        params = super().get_params()\n        params.update({'beta': self.beta,\n                       'knn': self.knn,\n                       'decay': self.decay,\n                       'bandwidth': self.bandwidth,\n                       'distance': self.distance,\n                       'thresh': self.thresh,\n                       'n_jobs': self.n_jobs})\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets parameters on this object.", "response": "def set_params(self, **params):\n        \"\"\"Set parameters on this object\n\n        Safe setter method - attributes should not be modified directly as some\n        changes are not valid.\n        Valid parameters:\n        - n_jobs\n        - random_state\n        - verbose\n        Invalid parameters: (these would require modifying the kernel matrix)\n        - knn\n        - adaptive_k\n        - decay\n        - distance\n        - thresh\n        - beta\n\n        Parameters\n        ----------\n        params : key-value pairs of parameter name and new values\n\n        Returns\n        -------\n        self\n        \"\"\"\n        # mnn specific arguments\n        if 'beta' in params and params['beta'] != self.beta:\n            raise ValueError(\"Cannot update beta. Please create a new graph\")\n\n        # knn arguments\n        knn_kernel_args = ['knn', 'decay', 'distance', 'thresh', 'bandwidth']\n        knn_other_args = ['n_jobs', 'random_state', 'verbose']\n        for arg in knn_kernel_args:\n            if arg in params and params[arg] != getattr(self, arg):\n                raise ValueError(\"Cannot update {}. \"\n                                 \"Please create a new graph\".format(arg))\n        for arg in knn_other_args:\n            if arg in params:\n                self.__setattr__(arg, params[arg])\n                for g in self.subgraphs:\n                    g.set_params(**{arg: params[arg]})\n\n        # update superclass parameters\n        super().set_params(**params)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the MNN kernel.", "response": "def build_kernel(self):\n        \"\"\"Build the MNN kernel.\n\n        Build a mutual nearest neighbors kernel.\n\n        Returns\n        -------\n        K : kernel matrix, shape=[n_samples, n_samples]\n            symmetric matrix with ones down the diagonal\n            with no non-negative entries.\n        \"\"\"\n        tasklogger.log_start(\"subgraphs\")\n        self.subgraphs = []\n        from .api import Graph\n        # iterate through sample ids\n        for i, idx in enumerate(self.samples):\n            tasklogger.log_debug(\"subgraph {}: sample {}, \"\n                                 \"n = {}, knn = {}\".format(\n                                     i, idx, np.sum(self.sample_idx == idx),\n                                     self.knn))\n            # select data for sample\n            data = self.data_nu[self.sample_idx == idx]\n            # build a kNN graph for cells within sample\n            graph = Graph(data, n_pca=None,\n                          knn=self.knn,\n                          decay=self.decay,\n                          bandwidth=self.bandwidth,\n                          distance=self.distance,\n                          thresh=self.thresh,\n                          verbose=self.verbose,\n                          random_state=self.random_state,\n                          n_jobs=self.n_jobs,\n                          kernel_symm='+',\n                          initialize=True)\n            self.subgraphs.append(graph)  # append to list of subgraphs\n        tasklogger.log_complete(\"subgraphs\")\n\n        tasklogger.log_start(\"MNN kernel\")\n        if self.thresh > 0 or self.decay is None:\n            K = sparse.lil_matrix(\n                (self.data_nu.shape[0], self.data_nu.shape[0]))\n        else:\n            K = np.zeros([self.data_nu.shape[0], self.data_nu.shape[0]])\n        for i, X in enumerate(self.subgraphs):\n            K = set_submatrix(K, self.sample_idx == self.samples[i],\n                              self.sample_idx == self.samples[i], X.K)\n            within_batch_norm = np.array(np.sum(X.K, 1)).flatten()\n            for j, Y in enumerate(self.subgraphs):\n                if i == j:\n                    continue\n                tasklogger.log_start(\n                    \"kernel from sample {} to {}\".format(self.samples[i],\n                                                         self.samples[j]))\n                Kij = Y.build_kernel_to_data(\n                    X.data_nu,\n                    knn=self.knn)\n                between_batch_norm = np.array(np.sum(Kij, 1)).flatten()\n                scale = np.minimum(1, within_batch_norm /\n                                   between_batch_norm) * self.beta\n                if sparse.issparse(Kij):\n                    Kij = Kij.multiply(scale[:, None])\n                else:\n                    Kij = Kij * scale[:, None]\n                K = set_submatrix(K, self.sample_idx == self.samples[i],\n                                  self.sample_idx == self.samples[j], Kij)\n                tasklogger.log_complete(\n                    \"kernel from sample {} to {}\".format(self.samples[i],\n                                                         self.samples[j]))\n        tasklogger.log_complete(\"MNN kernel\")\n        return K"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Graph(data,\n          n_pca=None,\n          sample_idx=None,\n          adaptive_k=None,\n          precomputed=None,\n          knn=5,\n          decay=10,\n          bandwidth=None,\n          bandwidth_scale=1.0,\n          anisotropy=0,\n          distance='euclidean',\n          thresh=1e-4,\n          kernel_symm='+',\n          theta=None,\n          n_landmark=None,\n          n_svd=100,\n          beta=1,\n          n_jobs=-1,\n          verbose=False,\n          random_state=None,\n          graphtype='auto',\n          use_pygsp=False,\n          initialize=True,\n          **kwargs):\n    \"\"\"Create a graph built on data.\n\n    Automatically selects the appropriate DataGraph subclass based on\n    chosen parameters.\n    Selection criteria:\n    - if `graphtype` is given, this will be respected\n    - otherwise:\n    -- if `sample_idx` is given, an MNNGraph will be created\n    -- if `precomputed` is not given, and either `decay` is `None` or `thresh`\n    is given, a kNNGraph will be created\n    - otherwise, a TraditionalGraph will be created.\n\n    Incompatibilities:\n    - MNNGraph and kNNGraph cannot be precomputed\n    - kNNGraph and TraditionalGraph do not accept sample indices\n\n    Parameters\n    ----------\n    data : array-like, shape=[n_samples,n_features]\n        accepted types: `numpy.ndarray`, `scipy.sparse.spmatrix`.\n        TODO: accept pandas dataframes\n\n    n_pca : `int` or `None`, optional (default: `None`)\n        number of PC dimensions to retain for graph building.\n        If `None`, uses the original data.\n        Note: if data is sparse, uses SVD instead of PCA\n        TODO: should we subtract and store the mean?\n\n    knn : `int`, optional (default: 5)\n        Number of nearest neighbors (including self) to use to build the graph\n\n    decay : `int` or `None`, optional (default: 10)\n        Rate of alpha decay to use. If `None`, alpha decay is not used.\n\n    bandwidth : `float`, list-like,`callable`, or `None`, optional (default: `None`)\n        Fixed bandwidth to use. If given, overrides `knn`. Can be a single\n        bandwidth, list-like (shape=[n_samples]) of bandwidths for each\n        sample, or a `callable` that takes in an `n x n` distance matrix and returns a\n        a single value or list-like of length n (shape=[n_samples])\n\n    bandwidth_scale : `float`, optional (default : 1.0)\n        Rescaling factor for bandwidth.\n\n    anisotropy : float, optional (default: 0)\n        Level of anisotropy between 0 and 1\n        (alpha in Coifman & Lafon, 2006)\n\n    distance : `str`, optional (default: `'euclidean'`)\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph.\n        TODO: actually sklearn.neighbors has even more choices\n\n    thresh : `float`, optional (default: `1e-4`)\n        Threshold above which to calculate alpha decay kernel.\n        All affinities below `thresh` will be set to zero in order to save\n        on time and memory constraints.\n\n    kernel_symm : string, optional (default: '+')\n        Defines method of MNN symmetrization.\n        '+'  : additive\n        '*'  : multiplicative\n        'theta' : min-max\n        'none' : no symmetrization\n\n    theta: float (default: None)\n        Min-max symmetrization constant or matrix. Only used if kernel_symm='theta'.\n        K = `theta * min(K, K.T) + (1 - theta) * max(K, K.T)`\n\n    precomputed : {'distance', 'affinity', 'adjacency', `None`}, optional (default: `None`)\n        If the graph is precomputed, this variable denotes which graph\n        matrix is provided as `data`.\n        Only one of `precomputed` and `n_pca` can be set.\n\n    beta: float, optional(default: 1)\n        Multiply between - batch connections by beta\n\n    sample_idx: array-like\n        Batch index for MNN kernel\n\n    adaptive_k : `{'min', 'mean', 'sqrt', 'none'}` (default: None)\n        Weights MNN kernel adaptively using the number of cells in\n        each sample according to the selected method.\n\n    n_landmark : `int`, optional (default: 2000)\n        number of landmarks to use\n\n    n_svd : `int`, optional (default: 100)\n        number of SVD components to use for spectral clustering\n\n    random_state : `int` or `None`, optional (default: `None`)\n        Random state for random PCA\n\n    verbose : `bool`, optional (default: `True`)\n        Verbosity.\n        TODO: should this be an integer instead to allow multiple\n        levels of verbosity?\n\n    n_jobs : `int`, optional (default : 1)\n        The number of jobs to use for the computation.\n        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n        used at all, which is useful for debugging.\n        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for\n        n_jobs = -2, all CPUs but one are used\n\n    graphtype : {'exact', 'knn', 'mnn', 'auto'} (Default: 'auto')\n        Manually selects graph type. Only recommended for expert users\n\n    use_pygsp : `bool` (Default: `False`)\n        If true, inherits from `pygsp.graphs.Graph`.\n\n    initialize : `bool` (Default: `True`)\n        If True, initialize the kernel matrix on instantiation\n\n    **kwargs : extra arguments for `pygsp.graphs.Graph`\n\n    Returns\n    -------\n    G : `DataGraph`\n\n    Raises\n    ------\n    ValueError : if selected parameters are incompatible.\n    \"\"\"\n    tasklogger.set_level(verbose)\n    if sample_idx is not None and len(np.unique(sample_idx)) == 1:\n        warnings.warn(\"Only one unique sample. \"\n                      \"Not using MNNGraph\")\n        sample_idx = None\n        if graphtype == 'mnn':\n            graphtype = 'auto'\n    if graphtype == 'auto':\n        # automatic graph selection\n        if sample_idx is not None:\n            # only mnn does batch correction\n            graphtype = \"mnn\"\n        elif precomputed is not None:\n            # precomputed requires exact graph\n            graphtype = \"exact\"\n        elif decay is None:\n            # knn kernel\n            graphtype = \"knn\"\n        elif thresh == 0 or callable(bandwidth):\n            # compute full distance matrix\n            graphtype = \"exact\"\n        else:\n            # decay kernel with nonzero threshold - knn is more efficient\n            graphtype = \"knn\"\n\n    # set base graph type\n    if graphtype == \"knn\":\n        basegraph = graphs.kNNGraph\n        if precomputed is not None:\n            raise ValueError(\"kNNGraph does not support precomputed \"\n                             \"values. Use `graphtype='exact'` or \"\n                             \"`precomputed=None`\")\n        if sample_idx is not None:\n            raise ValueError(\"kNNGraph does not support batch \"\n                             \"correction. Use `graphtype='mnn'` or \"\n                             \"`sample_idx=None`\")\n\n    elif graphtype == \"mnn\":\n        basegraph = graphs.MNNGraph\n        if precomputed is not None:\n            raise ValueError(\"MNNGraph does not support precomputed \"\n                             \"values. Use `graphtype='exact'` and \"\n                             \"`sample_idx=None` or `precomputed=None`\")\n    elif graphtype == \"exact\":\n        basegraph = graphs.TraditionalGraph\n        if sample_idx is not None:\n            raise ValueError(\"TraditionalGraph does not support batch \"\n                             \"correction. Use `graphtype='mnn'` or \"\n                             \"`sample_idx=None`\")\n    else:\n        raise ValueError(\"graphtype '{}' not recognized. Choose from \"\n                         \"['knn', 'mnn', 'exact', 'auto']\")\n\n    # set add landmarks if necessary\n    parent_classes = [basegraph]\n    msg = \"Building {} graph\".format(graphtype)\n    if n_landmark is not None:\n        parent_classes.append(graphs.LandmarkGraph)\n        msg = msg + \" with landmarks\"\n    if use_pygsp:\n        parent_classes.append(base.PyGSPGraph)\n        if len(parent_classes) > 2:\n            msg = msg + \" with PyGSP inheritance\"\n        else:\n            msg = msg + \" and PyGSP inheritance\"\n\n    tasklogger.log_debug(msg)\n\n    class_names = [p.__name__.replace(\"Graph\", \"\") for p in parent_classes]\n    try:\n        Graph = eval(\"graphs.\" + \"\".join(class_names) + \"Graph\")\n    except NameError:\n        raise RuntimeError(\"unknown graph classes {}\".format(parent_classes))\n\n    params = kwargs\n    for parent_class in parent_classes:\n        for param in parent_class._get_param_names():\n            try:\n                params[param] = eval(param)\n            except NameError:\n                # keyword argument not specified above - no problem\n                pass\n\n    # build graph and return\n    tasklogger.log_debug(\"Initializing {} with arguments {}\".format(\n        parent_classes,\n        \", \".join([\"{}='{}'\".format(key, value)\n                   for key, value in params.items()\n                   if key != \"data\"])))\n    return Graph(**params)", "response": "Create a new MNNGraph and TraditionalGraph from data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an igraph. Graph to a graphtools. TraditionalGraph", "response": "def from_igraph(G, attribute=\"weight\", **kwargs):\n    \"\"\"Convert an igraph.Graph to a graphtools.Graph\n\n    Creates a graphtools.graphs.TraditionalGraph with a\n    precomputed adjacency matrix\n\n    Parameters\n    ----------\n    G : igraph.Graph\n        Graph to be converted\n    attribute : str, optional (default: \"weight\")\n        attribute containing edge weights, if any.\n        If None, unweighted graph is built\n    kwargs\n        keyword arguments for graphtools.Graph\n\n    Returns\n    -------\n    G : graphtools.graphs.TraditionalGraph\n    \"\"\"\n    if 'precomputed' in kwargs:\n        if kwargs['precomputed'] != 'adjacency':\n            warnings.warn(\n                \"Cannot build graph from igraph with precomputed={}. \"\n                \"Use 'adjacency' instead.\".format(kwargs['precomputed']),\n                UserWarning)\n        del kwargs['precomputed']\n    try:\n        K = G.get_adjacency(attribute=attribute).data\n    except ValueError as e:\n        if str(e) == \"Attribute does not exist\":\n            warnings.warn(\"Edge attribute {} not found. \"\n                          \"Returning unweighted graph\".format(attribute),\n                          UserWarning)\n        K = G.get_adjacency(attribute=None).data\n    return Graph(sparse.coo_matrix(K),\n                 precomputed='adjacency', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_pickle(path):\n    with open(path, 'rb') as f:\n        G = pickle.load(f)\n\n    if not isinstance(G, base.BaseGraph):\n        warnings.warn(\n            'Returning object that is not a graphtools.base.BaseGraph')\n    elif isinstance(G, base.PyGSPGraph) and isinstance(G.logger, str):\n        G.logger = pygsp.utils.build_logger(G.logger)\n    return G", "response": "Load pickled Graphtools object from file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_param_names(cls):\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [p for p in init_signature.parameters.values()\n                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n        # Extract and sort argument names excluding 'self'\n        parameters = set([p.name for p in parameters])\n\n        # recurse\n        for superclass in cls.__bases__:\n            try:\n                parameters.update(superclass._get_param_names())\n            except AttributeError:\n                # object and pygsp.graphs.Graph don't have this method\n                pass\n\n        return parameters", "response": "Get the parameter names for the estimator class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reduce_data(self):\n        if self.n_pca is not None and self.n_pca < self.data.shape[1]:\n            tasklogger.log_start(\"PCA\")\n            if sparse.issparse(self.data):\n                if isinstance(self.data, sparse.coo_matrix) or \\\n                        isinstance(self.data, sparse.lil_matrix) or \\\n                        isinstance(self.data, sparse.dok_matrix):\n                    self.data = self.data.tocsr()\n                self.data_pca = TruncatedSVD(self.n_pca,\n                                             random_state=self.random_state)\n            else:\n                self.data_pca = PCA(self.n_pca,\n                                    svd_solver='randomized',\n                                    random_state=self.random_state)\n            self.data_pca.fit(self.data)\n            data_nu = self.data_pca.transform(self.data)\n            tasklogger.log_complete(\"PCA\")\n            return data_nu\n        else:\n            data_nu = self.data\n            if sparse.issparse(data_nu) and not isinstance(\n                    data_nu, (sparse.csr_matrix,\n                              sparse.csc_matrix,\n                              sparse.bsr_matrix)):\n                data_nu = data_nu.tocsr()\n            return data_nu", "response": "Private method to reduce the data dimension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_params(self, **params):\n        if 'n_pca' in params and params['n_pca'] != self.n_pca:\n            raise ValueError(\"Cannot update n_pca. Please create a new graph\")\n        if 'random_state' in params:\n            self.random_state = params['random_state']\n        super().set_params(**params)\n        return self", "response": "Set parameters on this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms input data Y to reduced data space defined by self. data.", "response": "def transform(self, Y):\n        \"\"\"Transform input data `Y` to reduced data space defined by `self.data`\n\n        Takes data in the same ambient space as `self.data` and transforms it\n        to be in the same reduced space as `self.data_nu`.\n\n        Parameters\n        ----------\n        Y : array-like, shape=[n_samples_y, n_features]\n            n_features must be the same as `self.data`.\n\n        Returns\n        -------\n        Transformed data, shape=[n_samples_y, n_pca]\n\n        Raises\n        ------\n        ValueError : if Y.shape[1] != self.data.shape[1]\n        \"\"\"\n        try:\n            # try PCA first\n\n            return self.data_pca.transform(Y)\n        except AttributeError:  # no pca, try to return data\n            try:\n                if Y.shape[1] != self.data.shape[1]:\n                    # shape is wrong\n                    raise ValueError\n                return Y\n            except IndexError:\n                # len(Y.shape) < 2\n                raise ValueError\n        except ValueError:\n            # more informative error\n            raise ValueError(\"data of shape {} cannot be transformed\"\n                             \" to graph built on data of shape {}\".format(\n                                 Y.shape, self.data.shape))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_kernel(self):\n        kernel = self.build_kernel()\n        kernel = self.symmetrize_kernel(kernel)\n        kernel = self.apply_anisotropy(kernel)\n        if (kernel - kernel.T).max() > 1e-5:\n            warnings.warn(\"K should be symmetric\", RuntimeWarning)\n        if np.any(kernel.diagonal == 0):\n            warnings.warn(\"K should have a non-zero diagonal\", RuntimeWarning)\n        return kernel", "response": "Private method to build kernel matrix and runs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_params(self, **params):\n        if 'theta' in params and params['theta'] != self.theta:\n            raise ValueError(\"Cannot update theta. Please create a new graph\")\n        if 'anisotropy' in params and params['anisotropy'] != self.anisotropy:\n            raise ValueError(\n                \"Cannot update anisotropy. Please create a new graph\")\n        if 'kernel_symm' in params and \\\n                params['kernel_symm'] != self.kernel_symm:\n            raise ValueError(\n                \"Cannot update kernel_symm. Please create a new graph\")\n        super().set_params(**params)\n        return self", "response": "Set parameters on this object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef P(self):\n        try:\n            return self._diff_op\n        except AttributeError:\n            self._diff_op = normalize(self.kernel, 'l1', axis=1)\n            return self._diff_op", "response": "Diffusion operator of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef K(self):\n        try:\n            return self._kernel\n        except AttributeError:\n            self._kernel = self._build_kernel()\n            return self._kernel", "response": "Kernel matrix of the n_samples with the diagonal matrix with\n ones down the diagonal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_pygsp(self, **kwargs):\n        from . import api\n        if 'precomputed' in kwargs:\n            if kwargs['precomputed'] != 'affinity':\n                warnings.warn(\n                    \"Cannot build PyGSPGraph with precomputed={}. \"\n                    \"Using 'affinity' instead.\".format(kwargs['precomputed']),\n                    UserWarning)\n            del kwargs['precomputed']\n        if 'use_pygsp' in kwargs:\n            if kwargs['use_pygsp'] is not True:\n                warnings.warn(\n                    \"Cannot build PyGSPGraph with use_pygsp={}. \"\n                    \"Use True instead.\".format(kwargs['use_pygsp']),\n                    UserWarning)\n            del kwargs['use_pygsp']\n        return api.Graph(self.K,\n                         precomputed=\"affinity\", use_pygsp=True,\n                         **kwargs)", "response": "Convert to a PyGSP Graph object for use only when the user has not yet recompute the kernel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the object to an igraph Graph. Weighted_Adjacency object.", "response": "def to_igraph(self, attribute=\"weight\", **kwargs):\n        \"\"\"Convert to an igraph Graph\n\n        Uses the igraph.Graph.Weighted_Adjacency constructor\n\n        Parameters\n        ----------\n        attribute : str, optional (default: \"weight\")\n        kwargs : additional arguments for igraph.Graph.Weighted_Adjacency\n        \"\"\"\n        try:\n            import igraph as ig\n        except ImportError:\n            raise ImportError(\"Please install igraph with \"\n                              \"`pip install --user python-igraph`.\")\n        try:\n            W = self.W\n        except AttributeError:\n            # not a pygsp graph\n            W = self.K.copy()\n            W = utils.set_diagonal(W, 0)\n        return ig.Graph.Weighted_Adjacency(utils.to_dense(W).tolist(),\n                                           attr=attribute, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_pickle(self, path):\n        if int(sys.version.split(\".\")[1]) < 7 and isinstance(self, pygsp.graphs.Graph):\n            # python 3.5, 3.6\n            logger = self.logger\n            self.logger = logger.name\n        with open(path, 'wb') as f:\n            pickle.dump(self, f)\n        if int(sys.version.split(\".\")[1]) < 7 and isinstance(self, pygsp.graphs.Graph):\n            self.logger = logger", "response": "Save the current Graph to a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_params(self):\n        params = Data.get_params(self)\n        params.update(BaseGraph.get_params(self))\n        return params", "response": "Get parameters from this object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_extension_shape(self, Y):\n        if len(Y.shape) != 2:\n            raise ValueError(\"Expected a 2D matrix. Y has shape {}\".format(\n                Y.shape))\n        if not Y.shape[1] == self.data_nu.shape[1]:\n            # try PCA transform\n            if Y.shape[1] == self.data.shape[1]:\n                Y = self.transform(Y)\n            else:\n                # wrong shape\n                if self.data.shape[1] != self.data_nu.shape[1]:\n                    # PCA is possible\n                    msg = (\"Y must be of shape either \"\n                           \"(n, {}) or (n, {})\").format(\n                        self.data.shape[1], self.data_nu.shape[1])\n                else:\n                    # no PCA, only one choice of shape\n                    msg = \"Y must be of shape (n, {})\".format(\n                        self.data.shape[1])\n                raise ValueError(msg)\n        return Y", "response": "Private method to check if new data matches self. data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild transition matrix from new data to the graph", "response": "def extend_to_data(self, Y):\n        \"\"\"Build transition matrix from new data to the graph\n\n        Creates a transition matrix such that `Y` can be approximated by\n        a linear combination of samples in `self.data`. Any\n        transformation of `self.data` can be trivially applied to `Y` by\n        performing\n\n        `transform_Y = self.interpolate(transform, transitions)`\n\n        Parameters\n        ----------\n\n        Y: array-like, [n_samples_y, n_dimensions]\n            new data for which an affinity matrix is calculated\n            to the existing data. `n_features` must match\n            either the ambient or PCA dimensions\n\n        Returns\n        -------\n\n        transitions : array-like, shape=[n_samples_y, self.data.shape[0]]\n            Transition matrix from `Y` to `self.data`\n        \"\"\"\n        Y = self._check_extension_shape(Y)\n        kernel = self.build_kernel_to_data(Y)\n        transitions = normalize(kernel, norm='l1', axis=1)\n        return transitions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interpolate(self, transform, transitions=None, Y=None):\n        if transitions is None:\n            if Y is None:\n                raise ValueError(\n                    \"Either `transitions` or `Y` must be provided.\")\n            else:\n                transitions = self.extend_to_data(Y)\n        Y_transform = transitions.dot(transform)\n        return Y_transform", "response": "Interpolate new data onto a transformation of the graph data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting parameters on this object returning self", "response": "def set_params(self, **params):\n        \"\"\"Set parameters on this object\n\n        Safe setter method - attributes should not be modified directly as some\n        changes are not valid.\n        Valid parameters:\n        - n_jobs\n        - verbose\n\n        Parameters\n        ----------\n        params : key-value pairs of parameter name and new values\n\n        Returns\n        -------\n        self\n        \"\"\"\n        if 'n_jobs' in params:\n            self.n_jobs = params['n_jobs']\n        if 'verbose' in params:\n            self.verbose = params['verbose']\n            tasklogger.set_level(self.verbose)\n        super().set_params(**params)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a subset of feed. stop_times.", "response": "def get_stop_times(feed: \"Feed\", date: Optional[str] = None) -> DataFrame:\n    \"\"\"\n    Return a subset of ``feed.stop_times``.\n\n    Parameters\n    ----------\n    feed : Feed\n    date : string\n        YYYYMMDD date string restricting the output to trips active\n        on the date\n\n    Returns\n    -------\n    DataFrame\n        Subset of ``feed.stop_times``\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.stop_times``\n    - Those used in :func:`.trips.get_trips`\n\n    \"\"\"\n    f = feed.stop_times.copy()\n    if date is None:\n        return f\n\n    g = feed.get_trips(date)\n    return f[f[\"trip_id\"].isin(g[\"trip_id\"])]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend the shape_dist_traveled field to feed. stop_times.", "response": "def append_dist_to_stop_times(feed: \"Feed\", trip_stats: DataFrame) -> \"Feed\":\n    \"\"\"\n    Calculate and append the optional ``shape_dist_traveled`` field in\n    ``feed.stop_times`` in terms of the distance units\n    ``feed.dist_units``.\n    Need trip stats in the form output by\n    :func:`.trips.compute_trip_stats` for this.\n    Return the resulting Feed.\n\n    Notes\n    -----\n    - Does not always give accurate results, as described below.\n    - The algorithm works as follows.\n      Compute the ``shape_dist_traveled`` field by using Shapely to\n      measure the distance of a stop along its trip linestring.\n      If for a given trip this process produces a non-monotonically\n      increasing, hence incorrect, list of (cumulative) distances, then\n      fall back to estimating the distances as follows.\n\n      Get the average speed of the trip via ``trip_stats`` and use is to\n      linearly interpolate distances for stop times, assuming that the\n      first stop is at shape_dist_traveled = 0 (the start of the shape)\n      and the last stop is at shape_dist_traveled = the length of the trip\n      (taken from trip_stats and equal to the length of the shape, unless\n      ``trip_stats`` was called with ``get_dist_from_shapes == False``).\n      This fallback method usually kicks in on trips with\n      self-intersecting linestrings.\n      Unfortunately, this fallback method will produce incorrect results\n      when the first stop does not start at the start of its shape\n      (so shape_dist_traveled != 0).\n      This is the case for several trips in `this Portland feed\n      <https://transitfeeds.com/p/trimet/43/1400947517>`_, for example.\n    - Assume the following feed attributes are not ``None``:\n\n        * ``feed.stop_times``\n        * Those used in :func:`.shapes.build_geometry_by_shape`\n        * Those used in :func:`.stops.build_geometry_by_stop`\n\n    \"\"\"\n    feed = feed.copy()\n    geometry_by_shape = feed.build_geometry_by_shape(use_utm=True)\n    geometry_by_stop = feed.build_geometry_by_stop(use_utm=True)\n\n    # Initialize DataFrame\n    f = pd.merge(\n        feed.stop_times,\n        trip_stats[[\"trip_id\", \"shape_id\", \"distance\", \"duration\"]],\n    ).sort_values([\"trip_id\", \"stop_sequence\"])\n\n    # Convert departure times to seconds past midnight to ease calculations\n    f[\"departure_time\"] = f[\"departure_time\"].map(hp.timestr_to_seconds)\n    dist_by_stop_by_shape = {shape: {} for shape in geometry_by_shape}\n    m_to_dist = hp.get_convert_dist(\"m\", feed.dist_units)\n\n    def compute_dist(group):\n        # Compute the distances of the stops along this trip\n        shape = group[\"shape_id\"].iat[0]\n        if not isinstance(shape, str):\n            group[\"shape_dist_traveled\"] = np.nan\n            return group\n        elif np.isnan(group[\"distance\"].iat[0]):\n            group[\"shape_dist_traveled\"] = np.nan\n            return group\n        linestring = geometry_by_shape[shape]\n        distances = []\n        for stop in group[\"stop_id\"].values:\n            if stop in dist_by_stop_by_shape[shape]:\n                d = dist_by_stop_by_shape[shape][stop]\n            else:\n                d = m_to_dist(\n                    hp.get_segment_length(linestring, geometry_by_stop[stop])\n                )\n                dist_by_stop_by_shape[shape][stop] = d\n            distances.append(d)\n        s = sorted(distances)\n        D = linestring.length\n        distances_are_reasonable = all([d < D + 100 for d in distances])\n        if distances_are_reasonable and s == distances:\n            # Good\n            pass\n        elif distances_are_reasonable and s == distances[::-1]:\n            # Reverse. This happens when the direction of a linestring\n            # opposes the direction of the bus trip.\n            distances = distances[::-1]\n        else:\n            # Totally redo using trip length, first and last stop times,\n            # and linear interpolation\n            dt = group[\"departure_time\"]\n            times = dt.values  # seconds\n            t0, t1 = times[0], times[-1]\n            d0, d1 = 0, group[\"distance\"].iat[0]\n            # Get indices of nan departure times and\n            # temporarily forward fill them\n            # for the purposes of using np.interp smoothly\n            nan_indices = np.where(dt.isnull())[0]\n            dt.fillna(method=\"ffill\")\n            # Interpolate\n            distances = np.interp(times, [t0, t1], [d0, d1])\n            # Nullify distances with nan departure times\n            for i in nan_indices:\n                distances[i] = np.nan\n\n        group[\"shape_dist_traveled\"] = distances\n        return group\n\n    g = f.groupby(\"trip_id\", group_keys=False).apply(compute_dist)\n    # Convert departure times back to time strings\n    g[\"departure_time\"] = g[\"departure_time\"].map(\n        lambda x: hp.timestr_to_seconds(x, inverse=True)\n    )\n    g = g.drop([\"shape_id\", \"distance\", \"duration\"], axis=1)\n    feed.stop_times = g\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the first departure time and last arrival time listed in feed. stop_times respectively.", "response": "def get_start_and_end_times(\n    feed: \"Feed\", date: Optional[str] = None\n) -> List[str]:\n    \"\"\"\n    Return the first departure time and last arrival time\n    (HH:MM:SS time strings) listed in ``feed.stop_times``, respectively.\n    Restrict to the given date (YYYYMMDD string) if specified.\n    \"\"\"\n    st = feed.get_stop_times(date)\n    return (\n        st[\"departure_time\"].dropna().min(),\n        st[\"arrival_time\"].dropna().max(),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef valid_str(x: str) -> bool:\n    if isinstance(x, str) and x.strip():\n        return True\n    else:\n        return False", "response": "Return True if x is a non - blank string ; otherwise return False."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if x is a valid H : MM : SS time ; otherwise return False.", "response": "def valid_time(x: str) -> bool:\n    \"\"\"\n    Return ``True`` if ``x`` is a valid H:MM:SS or HH:MM:SS time;\n    otherwise return ``False``.\n    \"\"\"\n    if isinstance(x, str) and (\n        re.match(TIME_PATTERN1, x) or re.match(TIME_PATTERN2, x)\n    ):\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef valid_date(x: str) -> bool:\n    try:\n        if x != dt.datetime.strptime(x, DATE_FORMAT).strftime(DATE_FORMAT):\n            raise ValueError\n        return True\n    except ValueError:\n        return False", "response": "Return True if x is a valid YYYYMMDD date ; otherwise return False."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef valid_url(x: str) -> bool:\n    if isinstance(x, str) and re.match(URL_PATTERN, x):\n        return True\n    else:\n        return False", "response": "Return True if x is a valid URL otherwise return False."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef valid_email(x: str) -> bool:\n    if isinstance(x, str) and re.match(EMAIL_PATTERN, x):\n        return True\n    else:\n        return False", "response": "Return True if x is a valid email address ; otherwise return False."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if x is a valid hexadecimal color string without the leading hash.", "response": "def valid_color(x: str) -> bool:\n    \"\"\"\n    Return ``True`` if ``x`` a valid hexadecimal color string without\n    the leading hash; otherwise return ``False``.\n    \"\"\"\n    if isinstance(x, str) and re.match(COLOR_PATTERN, x):\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_for_required_columns(\n    problems: List, table: str, df: DataFrame\n) -> List:\n    \"\"\"\n    Check that the given GTFS table has the required columns.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Check that the DataFrame contains the colums required by GTFS\n        and append to the problems list one error for each column\n        missing.\n\n    \"\"\"\n    r = cs.GTFS_REF\n    req_columns = r.loc[\n        (r[\"table\"] == table) & r[\"column_required\"], \"column\"\n    ].values\n    for col in req_columns:\n        if col not in df.columns:\n            problems.append([\"error\", f\"Missing column {col}\", table, []])\n\n    return problems", "response": "Checks that the given GTFS table has the required columns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the given GTFS DataFrame contains invalid columns.", "response": "def check_for_invalid_columns(\n    problems: List, table: str, df: DataFrame\n) -> List:\n    \"\"\"\n    Check for invalid columns in the given GTFS DataFrame.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Check whether the DataFrame contains extra columns not in the\n        GTFS and append to the problems list one warning for each extra\n        column.\n\n    \"\"\"\n    r = cs.GTFS_REF\n    valid_columns = r.loc[r[\"table\"] == table, \"column\"].values\n    for col in df.columns:\n        if col not in valid_columns:\n            problems.append(\n                [\"warning\", f\"Unrecognized column {col}\", table, []]\n            )\n\n    return problems"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_table(\n    problems: List,\n    table: str,\n    df: DataFrame,\n    condition,\n    message: str,\n    type_: str = \"error\",\n) -> List:\n    \"\"\"\n    Check the given GTFS table for the given problem condition.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n    condition : boolean expression\n        One involving ``df``, e.g.`df['route_id'].map(is_valid_str)``\n    message : string\n        Problem message, e.g. ``'Invalid route_id'``\n    type_ : string\n        ``'error'`` or ``'warning'`` indicating the type of problem\n        encountered\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Record the indices of ``df`` that statisfy the condition.\n        If the list of indices is nonempty, append to the\n        problems the item ``[type_, message, table, indices]``;\n        otherwise do not append anything.\n\n    \"\"\"\n    indices = df.loc[condition].index.tolist()\n    if indices:\n        problems.append([type_, message, table, indices])\n\n    return problems", "response": "Checks the given GTFS table for the given condition."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_column(\n    problems: List,\n    table: str,\n    df: DataFrame,\n    column: str,\n    checker,\n    type_: str = \"error\",\n    *,\n    column_required: bool = True,\n) -> List:\n    \"\"\"\n    Check the given column of the given GTFS with the given problem\n    checker.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n    column : string\n        A column of ``df``\n    column_required : boolean\n        ``True`` if and only if ``column`` is required\n        (and not optional) by the GTFS\n    checker : boolean valued unary function\n        Returns ``True`` if and only if no problem is encountered\n    type_ : string\n        ``'error'`` or ``'warning'`` indicating the type of problem\n        encountered\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Apply the checker to the column entries and record the indices\n        of ``df`` where the checker returns ``False``.\n        If the list of indices of is nonempty, append to the problems the\n        item ``[type_, problem, table, indices]``; otherwise do not\n        append anything.\n\n        If not ``column_required``, then NaN entries will be ignored\n        before applying the checker.\n\n    \"\"\"\n    f = df.copy()\n    if not column_required:\n        if column not in f.columns:\n            f[column] = np.nan\n        f = f.dropna(subset=[column])\n\n    cond = ~f[column].map(checker)\n    problems = check_table(\n        problems,\n        table,\n        f,\n        cond,\n        f\"Invalid {column}; maybe has extra space characters\",\n        type_,\n    )\n\n    return problems", "response": "Checks the given column of the given GTFS with the given checker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_column_id(\n    problems: List,\n    table: str,\n    df: DataFrame,\n    column: str,\n    *,\n    column_required: bool = True,\n) -> List:\n    \"\"\"\n    A specialization of :func:`check_column`.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n    column : string\n        A column of ``df``\n    column_required : boolean\n        ``True`` if and only if ``column`` is required\n        (and not optional) by the GTFS\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Record the indices of ``df`` where the given column has\n        duplicated entry or an invalid strings.\n        If the list of indices is nonempty, append to the problems the\n        item ``[type_, problem, table, indices]``; otherwise do not\n        append anything.\n\n        If not ``column_required``, then NaN entries will be ignored\n        in the checking.\n\n    \"\"\"\n    f = df.copy()\n    if not column_required:\n        if column not in f.columns:\n            f[column] = np.nan\n        f = f.dropna(subset=[column])\n\n    cond = ~f[column].map(valid_str)\n    problems = check_table(\n        problems,\n        table,\n        f,\n        cond,\n        f\"Invalid {column}; maybe has extra space characters\",\n    )\n\n    cond = f[column].duplicated()\n    problems = check_table(problems, table, f, cond, f\"Repeated {column}\")\n\n    return problems", "response": "Checks that a column ID is valid for the GTFS table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_column_linked_id(\n    problems: List,\n    table: str,\n    df: DataFrame,\n    column: str,\n    target_df: DataFrame,\n    target_column: Optional[str] = None,\n    *,\n    column_required: bool = True,\n) -> List:\n    \"\"\"\n    A modified version of :func:`check_column_id`.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n    column : string\n        A column of ``df``\n    column_required : boolean\n        ``True`` if and only if ``column`` is required\n        (and not optional) by the GTFS\n    target_df : DataFrame\n        A GTFS table\n    target_column : string\n        A column of ``target_df``; defaults to ``column_name``\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Record indices of ``df`` where the following condition is\n        violated: ``column`` contain IDs that are valid strings and are\n        present in ``target_df`` under the ``target_column`` name.\n        If the list of indices is nonempty, append to the problems the\n        item ``[type_, problem, table, indices]``; otherwise do not\n        append anything.\n\n        If not ``column_required``, then NaN entries will be ignored\n        in the checking.\n\n    \"\"\"\n    if target_column is None:\n        target_column = column\n\n    f = df.copy()\n\n    if target_df is None:\n        g = pd.DataFrame()\n        g[target_column] = np.nan\n    else:\n        g = target_df.copy()\n\n    if target_column not in g.columns:\n        g[target_column] = np.nan\n\n    if not column_required:\n        if column not in f.columns:\n            f[column] = np.nan\n        f = f.dropna(subset=[column])\n        g = g.dropna(subset=[target_column])\n\n    cond = ~f[column].isin(g[target_column])\n    problems = check_table(problems, table, f, cond, f\"Undefined {column}\")\n\n    return problems", "response": "Checks that the column is linked to a specific entry in the GTFS table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat the given list of problems as a DataFrame.", "response": "def format_problems(\n    problems: List, *, as_df: bool = False\n) -> Union[List, DataFrame]:\n    \"\"\"\n    Format the given problems list as a DataFrame.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    as_df : boolean\n\n    Returns\n    -------\n    list or DataFrame\n        Return ``problems`` if not ``as_df``; otherwise return a\n        DataFrame with the problems as rows and the columns\n        ``['type', 'message', 'table', 'rows']``.\n\n    \"\"\"\n    if as_df:\n        problems = pd.DataFrame(\n            problems, columns=[\"type\", \"message\", \"table\", \"rows\"]\n        ).sort_values([\"type\", \"table\"])\n    return problems"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that feed. agency follows the GTFS.", "response": "def check_agency(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Check that ``feed.agency`` follows the GTFS.\n    Return a list of problems of the form described in\n    :func:`check_table`;\n    the list will be empty if no problems are found.\n    \"\"\"\n    table = \"agency\"\n    problems = []\n\n    # Preliminary checks\n    if feed.agency is None:\n        problems.append([\"error\", \"Missing table\", table, []])\n    else:\n        f = feed.agency.copy()\n        problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check service_id\n    problems = check_column_id(\n        problems, table, f, \"agency_id\", column_required=False\n    )\n\n    # Check agency_name\n    problems = check_column(problems, table, f, \"agency_name\", valid_str)\n\n    # Check agency_url\n    problems = check_column(problems, table, f, \"agency_url\", valid_url)\n\n    # Check agency_timezone\n    problems = check_column(\n        problems, table, f, \"agency_timezone\", valid_timezone\n    )\n\n    # Check agency_fare_url\n    problems = check_column(\n        problems, table, f, \"agency_fare_url\", valid_url, column_required=False\n    )\n\n    # Check agency_lang\n    problems = check_column(\n        problems, table, f, \"agency_lang\", valid_lang, column_required=False\n    )\n\n    # Check agency_phone\n    problems = check_column(\n        problems, table, f, \"agency_phone\", valid_str, column_required=False\n    )\n\n    # Check agency_email\n    problems = check_column(\n        problems, table, f, \"agency_email\", valid_email, column_required=False\n    )\n\n    return format_problems(problems, as_df=as_df)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking the feed. calendar for required and invalid columns.", "response": "def check_calendar(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.calendar``.\n    \"\"\"\n    table = \"calendar\"\n    problems = []\n\n    # Preliminary checks\n    if feed.calendar is None:\n        return problems\n\n    f = feed.calendar.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check service_id\n    problems = check_column_id(problems, table, f, \"service_id\")\n\n    # Check weekday columns\n    v = lambda x: x in range(2)\n    for col in [\n        \"monday\",\n        \"tuesday\",\n        \"wednesday\",\n        \"thursday\",\n        \"friday\",\n        \"saturday\",\n        \"sunday\",\n    ]:\n        problems = check_column(problems, table, f, col, v)\n\n    # Check start_date and end_date\n    for col in [\"start_date\", \"end_date\"]:\n        problems = check_column(problems, table, f, col, valid_date)\n\n    if include_warnings:\n        # Check if feed has expired\n        d = f[\"end_date\"].max()\n        if feed.calendar_dates is not None and not feed.calendar_dates.empty:\n            table += \"/calendar_dates\"\n            d = max(d, feed.calendar_dates[\"date\"].max())\n        if d < dt.datetime.today().strftime(DATE_FORMAT):\n            problems.append([\"warning\", \"Feed expired\", table, []])\n\n    return format_problems(problems, as_df=as_df)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_calendar_dates(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.calendar_dates``.\n    \"\"\"\n    table = \"calendar_dates\"\n    problems = []\n\n    # Preliminary checks\n    if feed.calendar_dates is None:\n        return problems\n\n    f = feed.calendar_dates.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check service_id\n    problems = check_column(problems, table, f, \"service_id\", valid_str)\n\n    # Check date\n    problems = check_column(problems, table, f, \"date\", valid_date)\n\n    # No duplicate (service_id, date) pairs allowed\n    cond = f[[\"service_id\", \"date\"]].duplicated()\n    problems = check_table(\n        problems, table, f, cond, \"Repeated pair (service_id, date)\"\n    )\n\n    # Check exception_type\n    v = lambda x: x in [1, 2]\n    problems = check_column(problems, table, f, \"exception_type\", v)\n\n    return format_problems(problems, as_df=as_df)", "response": "Check that the feed. calendar_dates is well - formed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that the Fare Attributes are well - formed.", "response": "def check_fare_attributes(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.calendar_dates``.\n    \"\"\"\n    table = \"fare_attributes\"\n    problems = []\n\n    # Preliminary checks\n    if feed.fare_attributes is None:\n        return problems\n\n    f = feed.fare_attributes.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check fare_id\n    problems = check_column_id(problems, table, f, \"fare_id\")\n\n    # Check currency_type\n    problems = check_column(\n        problems, table, f, \"currency_type\", valid_currency\n    )\n\n    # Check payment_method\n    v = lambda x: x in range(2)\n    problems = check_column(problems, table, f, \"payment_method\", v)\n\n    # Check transfers\n    v = lambda x: pd.isnull(x) or x in range(3)\n    problems = check_column(problems, table, f, \"transfers\", v)\n\n    # Check transfer_duration\n    v = lambda x: x >= 0\n    problems = check_column(\n        problems, table, f, \"transfer_duration\", v, column_required=False\n    )\n\n    return format_problems(problems, as_df=as_df)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the Faire Rules of a feed.", "response": "def check_fare_rules(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.calendar_dates``.\n    \"\"\"\n    table = \"fare_rules\"\n    problems = []\n\n    # Preliminary checks\n    if feed.fare_rules is None:\n        return problems\n\n    f = feed.fare_rules.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check fare_id\n    problems = check_column_linked_id(\n        problems, table, f, \"fare_id\", feed.fare_attributes\n    )\n\n    # Check route_id\n    problems = check_column_linked_id(\n        problems, table, f, \"route_id\", feed.routes, column_required=False\n    )\n\n    # Check origin_id, destination_id, contains_id\n    for col in [\"origin_id\", \"destination_id\", \"contains_id\"]:\n        problems = check_column_linked_id(\n            problems,\n            table,\n            f,\n            col,\n            feed.stops,\n            \"zone_id\",\n            column_required=False,\n        )\n\n    return format_problems(problems, as_df=as_df)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_feed_info(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.feed_info``.\n    \"\"\"\n    table = \"feed_info\"\n    problems = []\n\n    # Preliminary checks\n    if feed.feed_info is None:\n        return problems\n\n    f = feed.feed_info.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check feed_publisher_name\n    problems = check_column(\n        problems, table, f, \"feed_publisher_name\", valid_str\n    )\n\n    # Check feed_publisher_url\n    problems = check_column(\n        problems, table, f, \"feed_publisher_url\", valid_url\n    )\n\n    # Check feed_lang\n    problems = check_column(problems, table, f, \"feed_lang\", valid_lang)\n\n    # Check feed_start_date and feed_end_date\n    cols = [\"feed_start_date\", \"feed_end_date\"]\n    for col in cols:\n        problems = check_column(\n            problems, table, f, col, valid_date, column_required=False\n        )\n\n    if set(cols) <= set(f.columns):\n        d1, d2 = f.loc[0, [\"feed_start_date\", \"feed_end_date\"]].values\n        if pd.notnull(d1) and pd.notnull(d2) and d1 > d1:\n            problems.append(\n                [\n                    \"error\",\n                    \"feed_start_date later than feed_end_date\",\n                    table,\n                    [0],\n                ]\n            )\n\n    # Check feed_version\n    problems = check_column(\n        problems, table, f, \"feed_version\", valid_str, column_required=False\n    )\n\n    return format_problems(problems, as_df=as_df)", "response": "Check the feed. feed_info column for valid values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the feed. frequencies is well - formed and returns a list of problems.", "response": "def check_frequencies(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.frequencies``.\n    \"\"\"\n    table = \"frequencies\"\n    problems = []\n\n    # Preliminary checks\n    if feed.frequencies is None:\n        return problems\n\n    f = feed.frequencies.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check trip_id\n    problems = check_column_linked_id(\n        problems, table, f, \"trip_id\", feed.trips\n    )\n\n    # Check start_time and end_time\n    time_cols = [\"start_time\", \"end_time\"]\n    for col in time_cols:\n        problems = check_column(problems, table, f, col, valid_time)\n\n    for col in time_cols:\n        f[col] = f[col].map(hp.timestr_to_seconds)\n\n    # Start_time should be earlier than end_time\n    cond = f[\"start_time\"] >= f[\"end_time\"]\n    problems = check_table(\n        problems, table, f, cond, \"start_time not earlier than end_time\"\n    )\n\n    # Headway periods should not overlap\n    f = f.sort_values([\"trip_id\", \"start_time\"])\n    for __, group in f.groupby(\"trip_id\"):\n        a = group[\"start_time\"].values\n        b = group[\"end_time\"].values\n        indices = np.flatnonzero(a[1:] < b[:-1]).tolist()\n        if indices:\n            problems.append(\n                [\n                    \"error\",\n                    \"Headway periods for the same trip overlap\",\n                    table,\n                    indices,\n                ]\n            )\n\n    # Check headway_secs\n    v = lambda x: x >= 0\n    problems = check_column(problems, table, f, \"headway_secs\", v)\n\n    # Check exact_times\n    v = lambda x: x in range(2)\n    problems = check_column(\n        problems, table, f, \"exact_times\", v, column_required=False\n    )\n\n    return format_problems(problems, as_df=as_df)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_routes(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.routes``.\n    \"\"\"\n    table = \"routes\"\n    problems = []\n\n    # Preliminary checks\n    if feed.routes is None:\n        problems.append([\"error\", \"Missing table\", table, []])\n    else:\n        f = feed.routes.copy()\n        problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check route_id\n    problems = check_column_id(problems, table, f, \"route_id\")\n\n    # Check agency_id\n    if \"agency_id\" in f:\n        if \"agency_id\" not in feed.agency.columns:\n            problems.append(\n                [\n                    \"error\",\n                    \"agency_id column present in routes but not in agency\",\n                    table,\n                    [],\n                ]\n            )\n        else:\n            g = f.dropna(subset=[\"agency_id\"])\n            cond = ~g[\"agency_id\"].isin(feed.agency[\"agency_id\"])\n            problems = check_table(\n                problems, table, g, cond, \"Undefined agency_id\"\n            )\n\n    # Check route_short_name and route_long_name\n    for column in [\"route_short_name\", \"route_long_name\"]:\n        problems = check_column(\n            problems, table, f, column, valid_str, column_required=False\n        )\n\n    cond = ~(f[\"route_short_name\"].notnull() | f[\"route_long_name\"].notnull())\n    problems = check_table(\n        problems,\n        table,\n        f,\n        cond,\n        \"route_short_name and route_long_name both empty\",\n    )\n\n    # Check route_type\n    v = lambda x: x in range(8)\n    problems = check_column(problems, table, f, \"route_type\", v)\n\n    # Check route_url\n    problems = check_column(\n        problems, table, f, \"route_url\", valid_url, column_required=False\n    )\n\n    # Check route_color and route_text_color\n    for col in [\"route_color\", \"route_text_color\"]:\n        problems = check_column(\n            problems, table, f, col, valid_color, column_required=False\n        )\n\n    if include_warnings:\n        # Check for duplicated (route_short_name, route_long_name) pairs\n        cond = f[[\"route_short_name\", \"route_long_name\"]].duplicated()\n        problems = check_table(\n            problems,\n            table,\n            f,\n            cond,\n            \"Repeated pair (route_short_name, route_long_name)\",\n            \"warning\",\n        )\n\n        # Check for routes without trips\n        s = feed.trips[\"route_id\"]\n        cond = ~f[\"route_id\"].isin(s)\n        problems = check_table(\n            problems, table, f, cond, \"Route has no trips\", \"warning\"\n        )\n\n    return format_problems(problems, as_df=as_df)", "response": "Checks that the feed. routes contains all of the required columns and agency_id columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_shapes(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.shapes``.\n    \"\"\"\n    table = \"shapes\"\n    problems = []\n\n    # Preliminary checks\n    if feed.shapes is None:\n        return problems\n\n    f = feed.shapes.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check shape_id\n    problems = check_column(problems, table, f, \"shape_id\", valid_str)\n\n    # Check shape_pt_lon and shape_pt_lat\n    for column, bound in [(\"shape_pt_lon\", 180), (\"shape_pt_lat\", 90)]:\n        v = lambda x: pd.notnull(x) and -bound <= x <= bound\n        cond = ~f[column].map(v)\n        problems = check_table(\n            problems,\n            table,\n            f,\n            cond,\n            f\"{column} out of bounds {[-bound, bound]}\",\n        )\n\n    # Check for duplicated (shape_id, shape_pt_sequence) pairs\n    cond = f[[\"shape_id\", \"shape_pt_sequence\"]].duplicated()\n    problems = check_table(\n        problems, table, f, cond, \"Repeated pair (shape_id, shape_pt_sequence)\"\n    )\n\n    # Check if shape_dist_traveled does decreases on a trip\n    if \"shape_dist_traveled\" in f.columns:\n        g = f.dropna(subset=[\"shape_dist_traveled\"])\n        indices = []\n        prev_sid = None\n        prev_dist = -1\n        cols = [\"shape_id\", \"shape_dist_traveled\"]\n        for i, sid, dist in g[cols].itertuples():\n            if sid == prev_sid and dist < prev_dist:\n                indices.append(i)\n\n            prev_sid = sid\n            prev_dist = dist\n\n        if indices:\n            problems.append(\n                [\n                    \"error\",\n                    \"shape_dist_traveled decreases on a trip\",\n                    table,\n                    indices,\n                ]\n            )\n\n    return format_problems(problems, as_df=as_df)", "response": "Checks that the feed. shapes is well - formed and returns a list of lists of problems."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_stops(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.stops``.\n    \"\"\"\n    table = \"stops\"\n    problems = []\n\n    # Preliminary checks\n    if feed.stops is None:\n        problems.append([\"error\", \"Missing table\", table, []])\n    else:\n        f = feed.stops.copy()\n        problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check stop_id\n    problems = check_column_id(problems, table, f, \"stop_id\")\n\n    # Check stop_code, stop_desc, zone_id, parent_station\n    for column in [\"stop_code\", \"stop_desc\", \"zone_id\", \"parent_station\"]:\n        problems = check_column(\n            problems, table, f, column, valid_str, column_required=False\n        )\n\n    # Check stop_name\n    problems = check_column(problems, table, f, \"stop_name\", valid_str)\n\n    # Check stop_lon and stop_lat\n    if \"location_type\" in f.columns:\n        requires_location = f[\"location_type\"].isin([0, 1, 2])\n    else:\n        requires_location = True\n    for column, bound in [(\"stop_lon\", 180), (\"stop_lat\", 90)]:\n        v = lambda x: pd.notnull(x) and -bound <= x <= bound\n        cond = requires_location & ~f[column].map(v)\n        problems = check_table(\n            problems,\n            table,\n            f,\n            cond,\n            f\"{column} out of bounds {[-bound, bound]}\",\n        )\n\n    # Check stop_url\n    problems = check_column(\n        problems, table, f, \"stop_url\", valid_url, column_required=False\n    )\n\n    # Check location_type\n    v = lambda x: x in range(5)\n    problems = check_column(\n        problems, table, f, \"location_type\", v, column_required=False\n    )\n\n    # Check stop_timezone\n    problems = check_column(\n        problems,\n        table,\n        f,\n        \"stop_timezone\",\n        valid_timezone,\n        column_required=False,\n    )\n\n    # Check wheelchair_boarding\n    v = lambda x: x in range(3)\n    problems = check_column(\n        problems, table, f, \"wheelchair_boarding\", v, column_required=False\n    )\n\n    # Check further location_type and parent_station\n    if \"parent_station\" in f.columns:\n        if \"location_type\" not in f.columns:\n            problems.append(\n                [\n                    \"error\",\n                    \"parent_station column present but location_type column missing\",\n                    table,\n                    [],\n                ]\n            )\n        else:\n            # Stations must have location type 1\n            station_ids = f.loc[\n                f[\"parent_station\"].notnull(), \"parent_station\"\n            ]\n            cond = f[\"stop_id\"].isin(station_ids) & (f[\"location_type\"] != 1)\n            problems = check_table(\n                problems, table, f, cond, \"A station must have location_type 1\"\n            )\n\n            # Stations must not lie in stations\n            cond = (f[\"location_type\"] == 1) & f[\"parent_station\"].notnull()\n            problems = check_table(\n                problems,\n                table,\n                f,\n                cond,\n                \"A station must not lie in another station\",\n            )\n\n            # Entrances (type 2), generic nodes (type 3) and boarding areas (type 4) need to be part of a parent\n            cond = (\n                f[\"location_type\"].isin([2, 3, 4])\n                & f[\"parent_station\"].isnull()\n            )\n            problems = check_table(\n                problems,\n                table,\n                f,\n                cond,\n                \"Entrances, nodes, and boarding areas must be part of a parent station\",\n            )\n\n    if include_warnings:\n        # Check for stops without trips\n        s = feed.stop_times[\"stop_id\"]\n        cond = ~feed.stops[\"stop_id\"].isin(s)\n        problems = check_table(\n            problems, table, f, cond, \"Stop has no stop times\", \"warning\"\n        )\n\n    return format_problems(problems, as_df=as_df)", "response": "Check the stops of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that the feed. stop_times columns exist and that they are not missing.", "response": "def check_stop_times(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.stop_times``.\n    \"\"\"\n    table = \"stop_times\"\n    problems = []\n\n    # Preliminary checks\n    if feed.stop_times is None:\n        problems.append([\"error\", \"Missing table\", table, []])\n    else:\n        f = feed.stop_times.copy().sort_values([\"trip_id\", \"stop_sequence\"])\n        problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check trip_id\n    problems = check_column_linked_id(\n        problems, table, f, \"trip_id\", feed.trips\n    )\n\n    # Check arrival_time and departure_time\n    v = lambda x: pd.isnull(x) or valid_time(x)\n    for col in [\"arrival_time\", \"departure_time\"]:\n        problems = check_column(problems, table, f, col, v)\n\n    # Check that arrival and departure times exist for the first and last\n    # stop of each trip and for each timepoint.\n    # For feeds with many trips, iterating through the stop time rows is\n    # faster than uisg groupby.\n    if \"timepoint\" not in f.columns:\n        f[\"timepoint\"] = np.nan  # This will not mess up later timepoint check\n\n    indices = []\n    prev_tid = None\n    prev_atime = 1\n    prev_dtime = 1\n    for i, tid, atime, dtime, tp in f[\n        [\"trip_id\", \"arrival_time\", \"departure_time\", \"timepoint\"]\n    ].itertuples():\n        if tid != prev_tid:\n            # Check last stop of previous trip\n            if pd.isnull(prev_atime) or pd.isnull(prev_dtime):\n                indices.append(i - 1)\n            # Check first stop of current trip\n            if pd.isnull(atime) or pd.isnull(dtime):\n                indices.append(i)\n        elif tp == 1 and (pd.isnull(atime) or pd.isnull(dtime)):\n            # Failure at timepoint\n            indices.append(i)\n\n        prev_tid = tid\n        prev_atime = atime\n        prev_dtime = dtime\n\n    if indices:\n        problems.append(\n            [\n                \"error\",\n                \"First/last/time point arrival/departure time missing\",\n                table,\n                indices,\n            ]\n        )\n\n    # Check stop_id\n    problems = check_column_linked_id(\n        problems, table, f, \"stop_id\", feed.stops\n    )\n\n    # Check for duplicated (trip_id, stop_sequence) pairs\n    cond = f[[\"trip_id\", \"stop_sequence\"]].dropna().duplicated()\n    problems = check_table(\n        problems, table, f, cond, \"Repeated pair (trip_id, stop_sequence)\"\n    )\n\n    # Check stop_headsign\n    problems = check_column(\n        problems, table, f, \"stop_headsign\", valid_str, column_required=False\n    )\n\n    # Check pickup_type and drop_off_type\n    for col in [\"pickup_type\", \"drop_off_type\"]:\n        v = lambda x: x in range(4)\n        problems = check_column(\n            problems, table, f, col, v, column_required=False\n        )\n\n    # Check if shape_dist_traveled decreases on a trip\n    if \"shape_dist_traveled\" in f.columns:\n        g = f.dropna(subset=[\"shape_dist_traveled\"])\n        indices = []\n        prev_tid = None\n        prev_dist = -1\n        for i, tid, dist in g[[\"trip_id\", \"shape_dist_traveled\"]].itertuples():\n            if tid == prev_tid and dist < prev_dist:\n                indices.append(i)\n\n            prev_tid = tid\n            prev_dist = dist\n\n        if indices:\n            problems.append(\n                [\n                    \"error\",\n                    \"shape_dist_traveled decreases on a trip\",\n                    table,\n                    indices,\n                ]\n            )\n\n    # Check timepoint\n    v = lambda x: x in range(2)\n    problems = check_column(\n        problems, table, f, \"timepoint\", v, column_required=False\n    )\n\n    if include_warnings:\n        # Check for duplicated (trip_id, departure_time) pairs\n        cond = f[[\"trip_id\", \"departure_time\"]].duplicated()\n        problems = check_table(\n            problems,\n            table,\n            f,\n            cond,\n            \"Repeated pair (trip_id, departure_time)\",\n            \"warning\",\n        )\n\n    return format_problems(problems, as_df=as_df)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_transfers(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.transfers``.\n    \"\"\"\n    table = \"transfers\"\n    problems = []\n\n    # Preliminary checks\n    if feed.transfers is None:\n        return problems\n\n    f = feed.transfers.copy()\n    problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check from_stop_id and to_stop_id\n    for col in [\"from_stop_id\", \"to_stop_id\"]:\n        problems = check_column_linked_id(\n            problems, table, f, col, feed.stops, \"stop_id\"\n        )\n\n    # Check transfer_type\n    v = lambda x: pd.isnull(x) or x in range(5)\n    problems = check_column(\n        problems, table, f, \"transfer_type\", v, column_required=False\n    )\n\n    # Check min_transfer_time\n    v = lambda x: x >= 0\n    problems = check_column(\n        problems, table, f, \"min_transfer_time\", v, column_required=False\n    )\n\n    return format_problems(problems, as_df=as_df)", "response": "Check that the transfers in the feed are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_trips(\n    feed: \"Feed\", *, as_df: bool = False, include_warnings: bool = False\n) -> List:\n    \"\"\"\n    Analog of :func:`check_agency` for ``feed.trips``.\n    \"\"\"\n    table = \"trips\"\n    problems = []\n\n    # Preliminary checks\n    if feed.trips is None:\n        problems.append([\"error\", \"Missing table\", table, []])\n    else:\n        f = feed.trips.copy()\n        problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check trip_id\n    problems = check_column_id(problems, table, f, \"trip_id\")\n\n    # Check route_id\n    problems = check_column_linked_id(\n        problems, table, f, \"route_id\", feed.routes\n    )\n\n    # Check service_id\n    g = pd.DataFrame()\n    if feed.calendar is not None:\n        g = pd.concat([g, feed.calendar], sort=False)\n    if feed.calendar_dates is not None:\n        g = pd.concat([g, feed.calendar_dates], sort=False)\n    problems = check_column_linked_id(problems, table, f, \"service_id\", g)\n\n    # Check direction_id\n    v = lambda x: x in range(2)\n    problems = check_column(\n        problems, table, f, \"direction_id\", v, column_required=False\n    )\n\n    # Check block_id\n    if \"block_id\" in f.columns:\n        v = lambda x: pd.isnull(x) or valid_str(x)\n        cond = ~f[\"block_id\"].map(v)\n        problems = check_table(problems, table, f, cond, \"Blank block_id\")\n\n    # Check shape_id\n    problems = check_column_linked_id(\n        problems, table, f, \"shape_id\", feed.shapes, column_required=False\n    )\n\n    # Check wheelchair_accessible and bikes_allowed\n    v = lambda x: x in range(3)\n    for column in [\"wheelchair_accessible\", \"bikes_allowed\"]:\n        problems = check_column(\n            problems, table, f, column, v, column_required=False\n        )\n\n    # Check for trips with no stop times\n    if include_warnings:\n        s = feed.stop_times[\"trip_id\"]\n        cond = ~f[\"trip_id\"].isin(s)\n        problems = check_table(\n            problems, table, f, cond, \"Trip has no stop times\", \"warning\"\n        )\n\n    return format_problems(problems, as_df=as_df)", "response": "Check the traps of a feed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(\n    feed: \"Feed\", *, as_df: bool = True, include_warnings: bool = True\n) -> Union[List, DataFrame]:\n    \"\"\"\n    Check whether the given feed satisfies the GTFS.\n\n    Parameters\n    ----------\n    feed : Feed\n    as_df : boolean\n        If ``True``, then return the resulting report as a DataFrame;\n        otherwise return the result as a list\n    include_warnings : boolean\n        If ``True``, then include problems of types ``'error'`` and\n        ``'warning'``; otherwise, only return problems of type\n        ``'error'``\n\n    Returns\n    -------\n    list or DataFrame\n        Run all the table-checking functions: :func:`check_agency`,\n        :func:`check_calendar`, etc.\n        This yields a possibly empty list of items\n        [problem type, message, table, rows].\n        If ``as_df``, then format the error list as a DataFrame with the\n        columns\n\n        - ``'type'``: 'error' or 'warning'; 'error' means the GTFS is\n          violated; 'warning' means there is a problem but it's not a\n          GTFS violation\n        - ``'message'``: description of the problem\n        - ``'table'``: table in which problem occurs, e.g. 'routes'\n        - ``'rows'``: rows of the table's DataFrame where problem occurs\n\n        Return early if the feed is missing required tables or required\n        columns.\n\n    Notes\n    -----\n    - This function interprets the GTFS liberally, classifying problems\n      as warnings rather than errors where the GTFS is unclear.\n      For example if a trip_id listed in the trips table is not listed\n      in the stop times table (a trip with no stop times),\n      then that's a warning and not an error.\n    - Timing benchmark: on a 2.80 GHz processor machine with 16 GB of\n      memory, this function checks `this 31 MB Southeast Queensland feed\n      <http://transitfeeds.com/p/translink/21/20170310>`_\n      in 22 seconds, including warnings.\n\n    \"\"\"\n    problems = []\n\n    # Check for invalid columns and check the required tables\n    checkers = [\n        \"check_agency\",\n        \"check_calendar\",\n        \"check_calendar_dates\",\n        \"check_fare_attributes\",\n        \"check_fare_rules\",\n        \"check_feed_info\",\n        \"check_frequencies\",\n        \"check_routes\",\n        \"check_shapes\",\n        \"check_stops\",\n        \"check_stop_times\",\n        \"check_transfers\",\n        \"check_trips\",\n    ]\n    for checker in checkers:\n        problems.extend(\n            globals()[checker](feed, include_warnings=include_warnings)\n        )\n\n    # Check calendar/calendar_dates combo\n    if feed.calendar is None and feed.calendar_dates is None:\n        problems.append(\n            [\"error\", \"Missing both tables\", \"calendar & calendar_dates\", []]\n        )\n\n    return format_problems(problems, as_df=as_df)", "response": "Checks whether the given feed satisfies the GTFS."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a DataFrame summarizing all GTFS tables in the given feed or in the given table if specified.", "response": "def summarize(feed: \"Feed\", table: str = None) -> DataFrame:\n    \"\"\"\n    Return a DataFrame summarizing all GTFS tables in the given feed\n    or in the given table if specified.\n\n    Parameters\n    ----------\n    feed : Feed\n    table : string\n        A GTFS table name, e.g. ``'stop_times'``\n\n    Returns\n    -------\n    DataFrame\n        Columns are\n\n        - ``'table'``: name of the GTFS table, e.g. ``'stops'``\n        - ``'column'``: name of a column in the table,\n          e.g. ``'stop_id'``\n        - ``'num_values'``: number of values in the column\n        - ``'num_nonnull_values'``: number of nonnull values in the\n          column\n        - ``'num_unique_values'``: number of unique values in the\n          column, excluding null values\n        - ``'min_value'``: minimum value in the column\n        - ``'max_value'``: maximum value in the column\n\n    Notes\n    -----\n    - If the table is not in the feed, then return an empty DataFrame\n    - If the table is not valid, raise a ValueError\n\n    \"\"\"\n    gtfs_tables = cs.GTFS_REF.table.unique()\n\n    if table is not None:\n        if table not in gtfs_tables:\n            raise ValueError(f\"{table} is not a GTFS table\")\n        else:\n            tables = [table]\n    else:\n        tables = gtfs_tables\n\n    frames = []\n    for table in tables:\n        f = getattr(feed, table)\n        if f is None:\n            continue\n\n        def my_agg(col):\n            d = {}\n            d[\"column\"] = col.name\n            d[\"num_values\"] = col.size\n            d[\"num_nonnull_values\"] = col.count()\n            d[\"num_unique_values\"] = col.nunique()\n            d[\"min_value\"] = col.dropna().min()\n            d[\"max_value\"] = col.dropna().max()\n            return pd.Series(d)\n\n        g = f.apply(my_agg).T.reset_index(drop=True)\n        g[\"table\"] = table\n        frames.append(g)\n\n    cols = [\n        \"table\",\n        \"column\",\n        \"num_values\",\n        \"num_nonnull_values\",\n        \"num_unique_values\",\n        \"min_value\",\n        \"max_value\",\n    ]\n\n    if not frames:\n        f = pd.DataFrame()\n    else:\n        f = pd.concat(frames)\n        # Rearrange columns\n        f = f[cols].copy()\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a DataFrame of various basic information about a single node.", "response": "def describe(feed: \"Feed\", sample_date: Optional[str] = None) -> DataFrame:\n    \"\"\"\n    Return a DataFrame of various feed indicators and values,\n    e.g. number of routes.\n    Specialize some those indicators to the given sample date,\n    e.g. number of routes active on the date.\n\n    Parameters\n    ----------\n    feed : Feed\n    sample_date : string\n        YYYYMMDD date string specifying the date to compute sample\n        stats; defaults to the first Thursday of the Feed's period\n\n    Returns\n    -------\n    DataFrame\n        The columns are\n\n        - ``'indicator'``: string; name of an indicator, e.g. 'num_routes'\n        - ``'value'``: value of the indicator, e.g. 27\n\n    \"\"\"\n    from . import calendar as cl\n\n    d = OrderedDict()\n    dates = cl.get_dates(feed)\n    d[\"agencies\"] = feed.agency[\"agency_name\"].tolist()\n    d[\"timezone\"] = feed.agency[\"agency_timezone\"].iat[0]\n    d[\"start_date\"] = dates[0]\n    d[\"end_date\"] = dates[-1]\n    d[\"num_routes\"] = feed.routes.shape[0]\n    d[\"num_trips\"] = feed.trips.shape[0]\n    d[\"num_stops\"] = feed.stops.shape[0]\n    if feed.shapes is not None:\n        d[\"num_shapes\"] = feed.shapes[\"shape_id\"].nunique()\n    else:\n        d[\"num_shapes\"] = 0\n\n    if sample_date is None or sample_date not in feed.get_dates():\n        sample_date = cl.get_first_week(feed)[3]\n    d[\"sample_date\"] = sample_date\n    d[\"num_routes_active_on_sample_date\"] = feed.get_routes(sample_date).shape[\n        0\n    ]\n    trips = feed.get_trips(sample_date)\n    d[\"num_trips_active_on_sample_date\"] = trips.shape[0]\n    d[\"num_stops_active_on_sample_date\"] = feed.get_stops(sample_date).shape[0]\n    f = pd.DataFrame(list(d.items()), columns=[\"indicator\", \"value\"])\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassess the quality of a feed.", "response": "def assess_quality(feed: \"Feed\") -> DataFrame:\n    \"\"\"\n    Return a DataFrame of various feed indicators and values,\n    e.g. number of trips missing shapes.\n\n    Parameters\n    ----------\n    feed : Feed\n\n    Returns\n    -------\n    DataFrame\n        The columns are\n\n        - ``'indicator'``: string; name of an indicator, e.g. 'num_routes'\n        - ``'value'``: value of the indicator, e.g. 27\n\n    Notes\n    -----\n    - An odd function, but useful to see roughly how broken a feed is\n    - Not a GTFS validator\n\n    \"\"\"\n    d = OrderedDict()\n\n    # Count duplicate route short names\n    r = feed.routes\n    dup = r.duplicated(subset=[\"route_short_name\"])\n    n = dup[dup].count()\n    d[\"num_route_short_names_duplicated\"] = n\n    d[\"frac_route_short_names_duplicated\"] = n / r.shape[0]\n\n    # Count stop times missing shape_dist_traveled values\n    st = feed.stop_times.sort_values([\"trip_id\", \"stop_sequence\"])\n    if \"shape_dist_traveled\" in st.columns:\n        # Count missing distances\n        n = st[st[\"shape_dist_traveled\"].isnull()].shape[0]\n        d[\"num_stop_time_dists_missing\"] = n\n        d[\"frac_stop_time_dists_missing\"] = n / st.shape[0]\n    else:\n        d[\"num_stop_time_dists_missing\"] = st.shape[0]\n        d[\"frac_stop_time_dists_missing\"] = 1\n\n    # Count direction_ids missing\n    t = feed.trips\n    if \"direction_id\" in t.columns:\n        n = t[t[\"direction_id\"].isnull()].shape[0]\n        d[\"num_direction_ids_missing\"] = n\n        d[\"frac_direction_ids_missing\"] = n / t.shape[0]\n    else:\n        d[\"num_direction_ids_missing\"] = t.shape[0]\n        d[\"frac_direction_ids_missing\"] = 1\n\n    # Count trips missing shapes\n    if feed.shapes is not None:\n        n = t[t[\"shape_id\"].isnull()].shape[0]\n    else:\n        n = t.shape[0]\n    d[\"num_trips_missing_shapes\"] = n\n    d[\"frac_trips_missing_shapes\"] = n / t.shape[0]\n\n    # Count missing departure times\n    n = st[st[\"departure_time\"].isnull()].shape[0]\n    d[\"num_departure_times_missing\"] = n\n    d[\"frac_departure_times_missing\"] = n / st.shape[0]\n\n    # Count missing first departure times missing\n    g = st.groupby(\"trip_id\").first().reset_index()\n    n = g[g[\"departure_time\"].isnull()].shape[0]\n    d[\"num_first_departure_times_missing\"] = n\n    d[\"frac_first_departure_times_missing\"] = n / st.shape[0]\n\n    # Count missing last departure times\n    g = st.groupby(\"trip_id\").last().reset_index()\n    n = g[g[\"departure_time\"].isnull()].shape[0]\n    d[\"num_last_departure_times_missing\"] = n\n    d[\"frac_last_departure_times_missing\"] = n / st.shape[0]\n\n    # Opine\n    if (\n        (d[\"frac_first_departure_times_missing\"] >= 0.1)\n        or (d[\"frac_last_departure_times_missing\"] >= 0.1)\n        or d[\"frac_trips_missing_shapes\"] >= 0.8\n    ):\n        d[\"assessment\"] = \"bad feed\"\n    elif (\n        d[\"frac_direction_ids_missing\"]\n        or d[\"frac_stop_time_dists_missing\"]\n        or d[\"num_route_short_names_duplicated\"]\n    ):\n        d[\"assessment\"] = \"probably a fixable feed\"\n    else:\n        d[\"assessment\"] = \"good feed\"\n\n    f = pd.DataFrame(list(d.items()), columns=[\"indicator\", \"value\"])\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_dist(feed: \"Feed\", new_dist_units: str) -> \"Feed\":\n    feed = feed.copy()\n\n    if feed.dist_units == new_dist_units:\n        # Nothing to do\n        return feed\n\n    old_dist_units = feed.dist_units\n    feed.dist_units = new_dist_units\n\n    converter = hp.get_convert_dist(old_dist_units, new_dist_units)\n\n    if hp.is_not_null(feed.stop_times, \"shape_dist_traveled\"):\n        feed.stop_times[\"shape_dist_traveled\"] = feed.stop_times[\n            \"shape_dist_traveled\"\n        ].map(converter)\n\n    if hp.is_not_null(feed.shapes, \"shape_dist_traveled\"):\n        feed.shapes[\"shape_dist_traveled\"] = feed.shapes[\n            \"shape_dist_traveled\"\n        ].map(converter)\n\n    return feed", "response": "Convert the distances recorded in the shape_dist_traveled columns of the given Feed to the given distance units."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute some feed stats for the given dates and trip stats.", "response": "def compute_feed_stats(\n    feed: \"Feed\", trip_stats: DataFrame, dates: List[str]\n) -> DataFrame:\n    \"\"\"\n    Compute some feed stats for the given dates and trip stats.\n\n    Parameters\n    ----------\n    feed : Feed\n    trip_stats : DataFrame\n        Trip stats to consider in the format output by\n        :func:`.trips.compute_trip_stats`\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute stats\n\n    Returns\n    -------\n    DataFrame\n        The columns are\n\n        - ``'date'``\n        - ``'num_stops'``: number of stops active on the date\n        - ``'num_routes'``: number of routes active on the date\n        - ``'num_trips'``: number of trips that start on the date\n        - ``'num_trip_starts'``: number of trips with nonnull start\n          times on the date\n        - ``'num_trip_ends'``: number of trips with nonnull start times\n          and nonnull end times on the date, ignoring trips that end\n          after 23:59:59 on the date\n        - ``'peak_num_trips'``: maximum number of simultaneous trips in\n          service on the date\n        - ``'peak_start_time'``: start time of first longest period\n          during which the peak number of trips occurs on the date\n        - ``'peak_end_time'``: end time of first longest period during\n          which the peak number of trips occurs on the date\n        - ``'service_distance'``: sum of the service distances for the\n          active routes on the date\n        - ``'service_duration'``: sum of the service durations for the\n          active routes on the date\n        - ``'service_speed'``: service_distance/service_duration on the\n          date\n\n        Dates with no trip activity will have null stats.\n        Exclude dates that lie outside of the Feed's date range.\n        If all the dates given lie outside of the Feed's date range,\n        then return an empty DataFrame.\n\n    Notes\n    -----\n    - The route and trip stats for date d contain stats for trips that\n      start on date d only and ignore trips that start on date d-1 and\n      end on date d\n    - Assume the following feed attributes are not ``None``:\n\n        * Those used in :func:`.trips.get_trips`\n        * Those used in :func:`.routes.get_routes`\n        * Those used in :func:`.stops.get_stops`\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    ts = trip_stats.copy()\n    activity = feed.compute_trip_activity(dates)\n    stop_times = feed.stop_times.copy()\n\n    # Convert timestrings to seconds for quicker calculations\n    ts[[\"start_time\", \"end_time\"]] = ts[[\"start_time\", \"end_time\"]].applymap(\n        hp.timestr_to_seconds\n    )\n\n    # Collect stats for each date, memoizing stats by trip ID sequence\n    # to avoid unnecessary recomputations.\n    # Store in dictionary of the form\n    # trip ID sequence ->\n    # [stats dictionary, date list that stats apply]\n    stats_and_dates_by_ids = {}\n    cols = [\n        \"num_stops\",\n        \"num_routes\",\n        \"num_trips\",\n        \"num_trip_starts\",\n        \"num_trip_ends\",\n        \"peak_num_trips\",\n        \"peak_start_time\",\n        \"peak_end_time\",\n        \"service_distance\",\n        \"service_duration\",\n        \"service_speed\",\n    ]\n    null_stats = {c: np.nan for c in cols}\n    for date in dates:\n        stats = {}\n        ids = tuple(activity.loc[activity[date] > 0, \"trip_id\"])\n        if ids in stats_and_dates_by_ids:\n            # Append date to date list\n            stats_and_dates_by_ids[ids][1].append(date)\n        elif not ids:\n            # Null stats\n            stats_and_dates_by_ids[ids] = [null_stats, [date]]\n        else:\n            # Compute stats\n            f = ts[ts[\"trip_id\"].isin(ids)].copy()\n            stats[\"num_stops\"] = stop_times.loc[\n                stop_times[\"trip_id\"].isin(ids), \"stop_id\"\n            ].nunique()\n            stats[\"num_routes\"] = f[\"route_id\"].nunique()\n            stats[\"num_trips\"] = f.shape[0]\n            stats[\"num_trip_starts\"] = f[\"start_time\"].count()\n            stats[\"num_trip_ends\"] = f.loc[\n                f[\"end_time\"] < 24 * 3600, \"end_time\"\n            ].count()\n            stats[\"service_distance\"] = f[\"distance\"].sum()\n            stats[\"service_duration\"] = f[\"duration\"].sum()\n            stats[\"service_speed\"] = (\n                stats[\"service_distance\"] / stats[\"service_duration\"]\n            )\n\n            # Compute peak stats, which is the slowest part\n            active_trips = hp.get_active_trips_df(\n                f[[\"start_time\", \"end_time\"]]\n            )\n            times, counts = active_trips.index.values, active_trips.values\n            start, end = hp.get_peak_indices(times, counts)\n            stats[\"peak_num_trips\"] = counts[start]\n            stats[\"peak_start_time\"] = times[start]\n            stats[\"peak_end_time\"] = times[end]\n\n            # Record stats\n            stats_and_dates_by_ids[ids] = [stats, [date]]\n\n    # Assemble stats into DataFrame\n    rows = []\n    for stats, dates_ in stats_and_dates_by_ids.values():\n        for date in dates_:\n            s = copy.copy(stats)\n            s[\"date\"] = date\n            rows.append(s)\n    f = pd.DataFrame(rows).sort_values(\"date\")\n\n    # Convert seconds back to timestrings\n    times = [\"peak_start_time\", \"peak_end_time\"]\n    f[times] = f[times].applymap(\n        lambda t: hp.timestr_to_seconds(t, inverse=True)\n    )\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_feed_time_series(\n    feed: \"Feed\", trip_stats: DataFrame, dates: List[str], freq: str = \"5Min\"\n) -> DataFrame:\n    \"\"\"\n    Compute some feed stats in time series form for the given dates\n    and trip stats.\n\n    Parameters\n    ----------\n    feed : Feed\n    trip_stats : DataFrame\n        Trip stats to consider in the format output by\n        :func:`.trips.compute_trip_stats`\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute stats\n    freq : string\n        Pandas frequency string specifying the frequency of the\n        resulting time series, e.g. '5Min'; highest frequency allowable\n        is one minute ('Min').\n\n    Returns\n    -------\n    DataFrame\n        A time series with a timestamp index across the given dates\n        sampled at the given frequency.\n        The maximum allowable frequency is 1 minute.\n\n        The columns are\n\n        - ``'num_trips'``: number of trips in service during during the\n          time period\n        - ``'num_trip_starts'``: number of trips with starting during the\n          time period\n        - ``'num_trip_ends'``: number of trips ending during the\n          time period, ignoring the trips the end past midnight\n        - ``'service_distance'``: distance traveled during the time\n          period by all trips active during the time period\n        - ``'service_duration'``: duration traveled during the time\n          period by all trips active during the time period\n        - ``'service_speed'``: ``service_distance/service_duration``\n\n        Exclude dates that lie outside of the Feed's date range.\n        If all the dates given lie outside of the Feed's date range,\n        then return an empty DataFrame with the specified columns.\n\n    Notes\n    -----\n    - See the notes for :func:`.routes.compute_route_time_series_base`\n    - If all dates lie outside the Feed's date range, then return an\n      empty DataFrame\n    - Assume the following feed attributes are not ``None``:\n\n       * Those used in :func:`.routes.compute_route_time_series`\n\n    \"\"\"\n    rts = feed.compute_route_time_series(trip_stats, dates, freq=freq)\n    if rts.empty:\n        return pd.DataFrame()\n\n    cols = [\n        \"num_trip_starts\",\n        \"num_trip_ends\",\n        \"num_trips\",\n        \"service_distance\",\n        \"service_duration\",\n        \"service_speed\",\n    ]\n    f = pd.concat(\n        [rts[col].sum(axis=1, min_count=1) for col in cols], axis=1, keys=cols\n    )\n    f[\"service_speed\"] = f[\"service_distance\"] / f[\"service_duration\"]\n\n    return f.sort_index(axis=1)", "response": "Compute some feed time series for the given dates and trip stats."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a feed create a new shape ID for every trip that is missing a shape ID.", "response": "def create_shapes(feed: \"Feed\", *, all_trips: bool = False) -> \"Feed\":\n    \"\"\"\n    Given a feed, create a shape for every trip that is missing a\n    shape ID.\n    Do this by connecting the stops on the trip with straight lines.\n    Return the resulting feed which has updated shapes and trips\n    tables.\n\n    If ``all_trips``, then create new shapes for all trips by\n    connecting stops, and remove the old shapes.\n\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.stop_times``\n    - ``feed.trips``\n    - ``feed.stops``\n\n    \"\"\"\n    feed = feed.copy()\n\n    if all_trips:\n        trip_ids = feed.trips[\"trip_id\"]\n    else:\n        trip_ids = feed.trips[feed.trips[\"shape_id\"].isnull()][\"trip_id\"]\n\n    # Get stop times for given trips\n    f = feed.stop_times[feed.stop_times[\"trip_id\"].isin(trip_ids)][\n        [\"trip_id\", \"stop_sequence\", \"stop_id\"]\n    ]\n    f = f.sort_values([\"trip_id\", \"stop_sequence\"])\n\n    if f.empty:\n        # Nothing to do\n        return feed\n\n    # Create new shape IDs for given trips.\n    # To do this, collect unique stop sequences,\n    # sort them to impose a canonical order, and\n    # assign shape IDs to them\n    stop_seqs = sorted(\n        set(\n            tuple(group[\"stop_id\"].values)\n            for trip, group in f.groupby(\"trip_id\")\n        )\n    )\n    k = int(math.log10(len(stop_seqs))) + 1  # Digits for padding shape IDs\n    shape_by_stop_seq = {\n        seq: f\"shape_{i:0{k}d}\" for i, seq in enumerate(stop_seqs)\n    }\n\n    # Assign these new shape IDs to given trips\n    shape_by_trip = {\n        trip: shape_by_stop_seq[tuple(group[\"stop_id\"].values)]\n        for trip, group in f.groupby(\"trip_id\")\n    }\n    trip_cond = feed.trips[\"trip_id\"].isin(trip_ids)\n    feed.trips.loc[trip_cond, \"shape_id\"] = feed.trips.loc[\n        trip_cond, \"trip_id\"\n    ].map(lambda x: shape_by_trip[x])\n\n    # Build new shapes for given trips\n    G = [\n        [shape, i, stop]\n        for stop_seq, shape in shape_by_stop_seq.items()\n        for i, stop in enumerate(stop_seq)\n    ]\n    g = pd.DataFrame(G, columns=[\"shape_id\", \"shape_pt_sequence\", \"stop_id\"])\n    g = g.merge(feed.stops[[\"stop_id\", \"stop_lon\", \"stop_lat\"]]).sort_values(\n        [\"shape_id\", \"shape_pt_sequence\"]\n    )\n    g = g.drop([\"stop_id\"], axis=1)\n    g = g.rename(\n        columns={\"stop_lon\": \"shape_pt_lon\", \"stop_lat\": \"shape_pt_lat\"}\n    )\n\n    if feed.shapes is not None and not all_trips:\n        # Update feed shapes with new shapes\n        feed.shapes = pd.concat([feed.shapes, g], sort=False)\n    else:\n        # Create all new shapes\n        feed.shapes = g\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_bounds(feed: \"Feed\") -> Tuple:\n    lons, lats = feed.stops[\"stop_lon\"], feed.stops[\"stop_lat\"]\n    return lons.min(), lats.min(), lons.max(), lats.max()", "response": "Compute the bounds of the feed s stop coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the convex hull of the given Feed.", "response": "def compute_convex_hull(feed: \"Feed\") -> Polygon:\n    \"\"\"\n    Return a Shapely Polygon representing the convex hull formed by\n    the stops of the given Feed.\n    \"\"\"\n    m = sg.MultiPoint(feed.stops[[\"stop_lon\", \"stop_lat\"]].values)\n    return m.convex_hull"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the center of the convex of the given Feed.", "response": "def compute_center(\n    feed: \"Feed\", num_busiest_stops: Optional[int] = None\n) -> Tuple:\n    \"\"\"\n    Return the centroid (WGS84 longitude-latitude pair) of the convex\n    hull of the stops of the given Feed.\n    If ``num_busiest_stops`` (integer) is given,\n    then compute the ``num_busiest_stops`` busiest stops in the feed\n    on the first Monday of the feed and return the mean of the\n    longitudes and the mean of the latitudes of these stops,\n    respectively.\n    \"\"\"\n    s = feed.stops.copy()\n    if num_busiest_stops is None:\n        hull = compute_convex_hull(feed)\n        lon, lat = list(hull.centroid.coords)[0]\n    else:\n        date = feed.get_first_week()[0]\n        ss = feed.compute_stop_stats([date]).sort_values(\n            \"num_trips\", ascending=False\n        )\n        if ss.stop_id.isnull().all():\n            # No stats, which could happen with a crappy feed.\n            # Fall back to all stops.\n            hull = compute_convex_hull(feed)\n            lon, lat = list(hull.centroid.coords)[0]\n        else:\n            f = ss.head(num_busiest_stops)\n            f = s.merge(f)\n            lon = f[\"stop_lon\"].mean()\n            lat = f[\"stop_lat\"].mean()\n\n    return lon, lat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrestrict a feed to only the stops trips shapes etc. active on at least one of the given dates.", "response": "def restrict_to_dates(feed: \"Feed\", dates: List[str]) -> \"Feed\":\n    \"\"\"\n    Build a new feed by restricting this one to only the stops,\n    trips, shapes, etc. active on at least one of the given dates\n    (YYYYMMDD strings).\n    Return the resulting feed, which will have empty non-agency tables\n    if no trip is active on any of the given dates.\n    \"\"\"\n    # Initialize the new feed as the old feed.\n    # Restrict its DataFrames below.\n    feed = feed.copy()\n\n    # Get every trip that is active on at least one of the dates\n    try:\n        trip_ids = feed.compute_trip_activity(dates).loc[\n            lambda x: x[[c for c in x.columns if c != \"trip_id\"]].sum(axis=1)\n            > 0,\n            \"trip_id\",\n        ]\n    except KeyError:\n        # No trips\n        trip_ids = []\n\n    # Slice trips\n    feed.trips = feed.trips.loc[lambda x: x.trip_id.isin(trip_ids)]\n\n    # Slice routes\n    feed.routes = feed.routes.loc[\n        lambda x: x.route_id.isin(feed.trips.route_id)\n    ]\n\n    # Slice stop times\n    feed.stop_times = feed.stop_times.loc[lambda x: x.trip_id.isin(trip_ids)]\n\n    # Slice stops\n    stop_ids = feed.stop_times.stop_id.unique()\n    feed.stops = feed.stops.loc[lambda x: x.stop_id.isin(stop_ids)]\n\n    # Slice calendar\n    service_ids = feed.trips.service_id\n    if feed.calendar is not None:\n        feed.calendar = feed.calendar.loc[\n            lambda x: x.service_id.isin(service_ids)\n        ]\n\n    # Get agency for trips\n    if \"agency_id\" in feed.routes.columns:\n        agency_ids = feed.routes.agency_id\n        if len(agency_ids):\n            feed.agency = feed.agency.loc[\n                lambda x: x.agency_id.isin(agency_ids)\n            ]\n\n    # Now for the optional files.\n    # Get calendar dates for trips.\n    if feed.calendar_dates is not None:\n        feed.calendar_dates = feed.calendar_dates.loc[\n            lambda x: x.service_id.isin(service_ids)\n        ]\n\n    # Get frequencies for trips\n    if feed.frequencies is not None:\n        feed.frequencies = feed.frequencies.loc[\n            lambda x: x.trip_id.isin(trip_ids)\n        ]\n\n    # Get shapes for trips\n    if feed.shapes is not None:\n        shape_ids = feed.trips.shape_id\n        feed.shapes = feed.shapes.loc[lambda x: x.shape_id.isin(shape_ids)]\n\n    # Get transfers for stops\n    if feed.transfers is not None:\n        feed.transfers = feed.transfers.loc[\n            lambda x: x.from_stop_id.isin(stop_ids)\n            | x.to_stop_id.isin(stop_ids)\n        ]\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restrict_to_routes(feed: \"Feed\", route_ids: List[str]) -> \"Feed\":\n    # Initialize the new feed as the old feed.\n    # Restrict its DataFrames below.\n    feed = feed.copy()\n\n    # Slice routes\n    feed.routes = feed.routes[feed.routes[\"route_id\"].isin(route_ids)].copy()\n\n    # Slice trips\n    feed.trips = feed.trips[feed.trips[\"route_id\"].isin(route_ids)].copy()\n\n    # Slice stop times\n    trip_ids = feed.trips[\"trip_id\"]\n    feed.stop_times = feed.stop_times[\n        feed.stop_times[\"trip_id\"].isin(trip_ids)\n    ].copy()\n\n    # Slice stops\n    stop_ids = feed.stop_times[\"stop_id\"].unique()\n    feed.stops = feed.stops[feed.stops[\"stop_id\"].isin(stop_ids)].copy()\n\n    # Slice calendar\n    service_ids = feed.trips[\"service_id\"]\n    if feed.calendar is not None:\n        feed.calendar = feed.calendar[\n            feed.calendar[\"service_id\"].isin(service_ids)\n        ].copy()\n\n    # Get agency for trips\n    if \"agency_id\" in feed.routes.columns:\n        agency_ids = feed.routes[\"agency_id\"]\n        if len(agency_ids):\n            feed.agency = feed.agency[\n                feed.agency[\"agency_id\"].isin(agency_ids)\n            ].copy()\n\n    # Now for the optional files.\n    # Get calendar dates for trips.\n    if feed.calendar_dates is not None:\n        feed.calendar_dates = feed.calendar_dates[\n            feed.calendar_dates[\"service_id\"].isin(service_ids)\n        ].copy()\n\n    # Get frequencies for trips\n    if feed.frequencies is not None:\n        feed.frequencies = feed.frequencies[\n            feed.frequencies[\"trip_id\"].isin(trip_ids)\n        ].copy()\n\n    # Get shapes for trips\n    if feed.shapes is not None:\n        shape_ids = feed.trips[\"shape_id\"]\n        feed.shapes = feed.shapes[\n            feed.shapes[\"shape_id\"].isin(shape_ids)\n        ].copy()\n\n    # Get transfers for stops\n    if feed.transfers is not None:\n        feed.transfers = feed.transfers[\n            feed.transfers[\"from_stop_id\"].isin(stop_ids)\n            | feed.transfers[\"to_stop_id\"].isin(stop_ids)\n        ].copy()\n\n    return feed", "response": "Restrict a feed to only the stops trips shapes etc. used by the routes with the given list of route IDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restrict_to_polygon(feed: \"Feed\", polygon: Polygon) -> \"Feed\":\n    # Initialize the new feed as the old feed.\n    # Restrict its DataFrames below.\n    feed = feed.copy()\n\n    # Get IDs of stops within the polygon\n    stop_ids = feed.get_stops_in_polygon(polygon)[\"stop_id\"]\n\n    # Get all trips that stop at at least one of those stops\n    st = feed.stop_times.copy()\n    trip_ids = st[st[\"stop_id\"].isin(stop_ids)][\"trip_id\"]\n    feed.trips = feed.trips[feed.trips[\"trip_id\"].isin(trip_ids)].copy()\n\n    # Get stop times for trips\n    feed.stop_times = st[st[\"trip_id\"].isin(trip_ids)].copy()\n\n    # Get stops for trips\n    stop_ids = feed.stop_times[\"stop_id\"]\n    feed.stops = feed.stops[feed.stops[\"stop_id\"].isin(stop_ids)].copy()\n\n    # Get routes for trips\n    route_ids = feed.trips[\"route_id\"]\n    feed.routes = feed.routes[feed.routes[\"route_id\"].isin(route_ids)].copy()\n\n    # Get calendar for trips\n    service_ids = feed.trips[\"service_id\"]\n    if feed.calendar is not None:\n        feed.calendar = feed.calendar[\n            feed.calendar[\"service_id\"].isin(service_ids)\n        ].copy()\n\n    # Get agency for trips\n    if \"agency_id\" in feed.routes.columns:\n        agency_ids = feed.routes[\"agency_id\"]\n        if len(agency_ids):\n            feed.agency = feed.agency[\n                feed.agency[\"agency_id\"].isin(agency_ids)\n            ].copy()\n\n    # Now for the optional files.\n    # Get calendar dates for trips.\n    cd = feed.calendar_dates\n    if cd is not None:\n        feed.calendar_dates = cd[cd[\"service_id\"].isin(service_ids)].copy()\n\n    # Get frequencies for trips\n    if feed.frequencies is not None:\n        feed.frequencies = feed.frequencies[\n            feed.frequencies[\"trip_id\"].isin(trip_ids)\n        ].copy()\n\n    # Get shapes for trips\n    if feed.shapes is not None:\n        shape_ids = feed.trips[\"shape_id\"]\n        feed.shapes = feed.shapes[\n            feed.shapes[\"shape_id\"].isin(shape_ids)\n        ].copy()\n\n    # Get transfers for stops\n    if feed.transfers is not None:\n        t = feed.transfers\n        feed.transfers = t[\n            t[\"from_stop_id\"].isin(stop_ids) | t[\"to_stop_id\"].isin(stop_ids)\n        ].copy()\n\n    return feed", "response": "Returns a new feed by restricting all trips that intersect the given Shapely polygon to those that intersect the given Shapely polygon."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_screen_line_counts(\n    feed: \"Feed\", linestring: LineString, dates: List[str], geo_shapes=None\n) -> DataFrame:\n    \"\"\"\n    Find all the Feed trips active on the given dates\n    that intersect the given Shapely LineString (with WGS84\n    longitude-latitude coordinates).\n\n    Parameters\n    ----------\n    feed : Feed\n    linestring : Shapely LineString\n    dates : list\n        YYYYMMDD date strings\n\n    Returns\n    -------\n    DataFrame\n        The columns are\n\n        - ``'date'``\n        - ``'trip_id'``\n        - ``'route_id'``\n        - ``'route_short_name'``\n        - ``'crossing_time'``: time that the trip's vehicle crosses\n          the linestring; one trip could cross multiple times\n        - ``'orientation'``: 1 or -1; 1 indicates trip travel from the\n          left side to the right side of the screen line;\n          -1 indicates trip travel in the  opposite direction\n\n    Notes\n    -----\n    - Requires GeoPandas\n    - The first step is to geometrize ``feed.shapes`` via\n      :func:`.shapes.geometrize_shapes`. Alternatively, use the\n      ``geo_shapes`` GeoDataFrame, if given.\n    - Assume ``feed.stop_times`` has an accurate\n      ``shape_dist_traveled`` column.\n    - Assume that trips travel in the same direction as their\n      shapes. That restriction is part of GTFS, by the way.\n      To calculate direction quickly and accurately, assume that\n      the screen line is straight and doesn't double back on itself.\n    - Probably does not give correct results for trips with\n      self-intersecting shapes.\n    - The algorithm works as follows\n\n        1. Compute all the shapes that intersect the linestring\n        2. For each such shape, compute the intersection points\n        3. For each point p, scan through all the trips in the feed\n           that have that shape\n        4. For each date in ``dates``, restrict to trips active on the\n           date and interpolate a stop time for p by assuming that the\n           feed has the shape_dist_traveled field in stop times\n        5. Use that interpolated time as the crossing time of the trip\n           vehicle, and compute the trip orientation to the screen line\n           via a cross product of a vector in the direction of the\n           screen line and a tiny vector in the direction of trip travel\n\n    - Assume the following feed attributes are not ``None``:\n         * ``feed.shapes``, if ``geo_shapes`` is not given\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    # Get all shapes that intersect the screen line\n    shapes = feed.get_shapes_intersecting_geometry(\n        linestring, geo_shapes, geometrized=True\n    )\n\n    # Convert shapes to UTM\n    lat, lon = feed.shapes.loc[0, [\"shape_pt_lat\", \"shape_pt_lon\"]].values\n    crs = hp.get_utm_crs(lat, lon)\n    shapes = shapes.to_crs(crs)\n\n    # Convert linestring to UTM\n    linestring = hp.linestring_to_utm(linestring)\n\n    # Get all intersection points of shapes and linestring\n    shapes[\"intersection\"] = shapes.intersection(linestring)\n\n    # Make a vector in the direction of the screen line\n    # to later calculate trip orientation.\n    # Does not work in case of a bent screen line.\n    p1 = sg.Point(linestring.coords[0])\n    p2 = sg.Point(linestring.coords[-1])\n    w = np.array([p2.x - p1.x, p2.y - p1.y])\n\n    # Build a dictionary from the shapes DataFrame of the form\n    # shape ID -> list of pairs (d, v), one for each intersection point,\n    # where d is the distance of the intersection point along shape,\n    # and v is a tiny vectors from the point in direction of shape.\n    # Assume here that trips travel in the same direction as their shapes.\n    dv_by_shape = {}\n    eps = 1\n    convert_dist = hp.get_convert_dist(\"m\", feed.dist_units)\n    for __, sid, geom, intersection in shapes.itertuples():\n        # Get distances along shape of intersection points (in meters)\n        distances = [geom.project(p) for p in intersection]\n        # Build tiny vectors\n        vectors = []\n        for i, p in enumerate(intersection):\n            q = geom.interpolate(distances[i] + eps)\n            vector = np.array([q.x - p.x, q.y - p.y])\n            vectors.append(vector)\n        # Convert distances to units used in feed\n        distances = [convert_dist(d) for d in distances]\n        dv_by_shape[sid] = list(zip(distances, vectors))\n\n    # Get trips with those shapes\n    t = feed.trips\n    t = t[t[\"shape_id\"].isin(dv_by_shape.keys())].copy()\n\n    # Merge in route short names and stop times\n    t = t.merge(feed.routes[[\"route_id\", \"route_short_name\"]]).merge(\n        feed.stop_times\n    )\n\n    # Drop NaN departure times and convert to seconds past midnight\n    t = t[t[\"departure_time\"].notnull()].copy()\n    t[\"departure_time\"] = t[\"departure_time\"].map(hp.timestr_to_seconds)\n\n    # Compile crossings by date\n    a = feed.compute_trip_activity(dates)\n    rows = []\n    for date in dates:\n        # Slice to trips active on date\n        ids = a.loc[a[date] == 1, \"trip_id\"]\n        f = t[t[\"trip_id\"].isin(ids)].copy()\n\n        # For each shape find the trips that cross the screen line\n        # and get crossing times and orientation\n        f = f.sort_values([\"trip_id\", \"stop_sequence\"])\n        for tid, group in f.groupby(\"trip_id\"):\n            sid = group[\"shape_id\"].iat[0]\n            rid = group[\"route_id\"].iat[0]\n            rsn = group[\"route_short_name\"].iat[0]\n            stop_times = group[\"departure_time\"].values\n            stop_distances = group[\"shape_dist_traveled\"].values\n            for d, v in dv_by_shape[sid]:\n                # Interpolate crossing time\n                time = np.interp(d, stop_distances, stop_times)\n                # Compute direction of trip travel relative to\n                # screen line by looking at the sign of the cross\n                # product of tiny shape vector and screen line vector\n                det = np.linalg.det(np.array([v, w]))\n                if det >= 0:\n                    orientation = 1\n                else:\n                    orientation = -1\n                # Update rows\n                rows.append([date, tid, rid, rsn, time, orientation])\n\n    # Create DataFrame\n    cols = [\n        \"date\",\n        \"trip_id\",\n        \"route_id\",\n        \"route_short_name\",\n        \"crossing_time\",\n        \"orientation\",\n    ]\n    g = pd.DataFrame(rows, columns=cols).sort_values([\"date\", \"crossing_time\"])\n\n    # Convert departure times back to time strings\n    g[\"crossing_time\"] = g[\"crossing_time\"].map(\n        lambda x: hp.timestr_to_seconds(x, inverse=True)\n    )\n\n    return g", "response": "Compute the screen line counts for a given feed and linestring."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the given trip is active on the given date.", "response": "def is_active_trip(feed: \"Feed\", trip_id: str, date: str) -> bool:\n    \"\"\"\n    Return ``True`` if the ``feed.calendar`` or ``feed.calendar_dates``\n    says that the trip runs on the given date; return ``False``\n    otherwise.\n\n    Note that a trip that starts on date d, ends after 23:59:59, and\n    does not start again on date d+1 is considered active on date d and\n    not active on date d+1.\n    This subtle point, which is a side effect of the GTFS, can\n    lead to confusion.\n\n    Parameters\n    ----------\n    feed : Feed\n    trip_id : string\n        ID of a trip in ``feed.trips``\n    date : string\n        YYYYMMDD date string\n\n    Returns\n    -------\n    boolean\n        ``True`` if and only if the given trip starts on the given\n        date.\n\n    Notes\n    -----\n    - This function is key for getting all trips, routes, etc. that are\n      active on a given date, so the function needs to be fast\n    - Assume the following feed attributes are not ``None``:\n\n        * ``feed.trips``\n\n    \"\"\"\n    service = feed._trips_i.at[trip_id, \"service_id\"]\n    # Check feed._calendar_dates_g.\n    caldg = feed._calendar_dates_g\n    if caldg is not None:\n        if (service, date) in caldg.groups:\n            et = caldg.get_group((service, date))[\"exception_type\"].iat[0]\n            if et == 1:\n                return True\n            else:\n                # Exception type is 2\n                return False\n    # Check feed._calendar_i\n    cali = feed._calendar_i\n    if cali is not None:\n        if service in cali.index:\n            weekday_str = hp.weekday_to_str(hp.datestr_to_date(date).weekday())\n            if (\n                cali.at[service, \"start_date\"]\n                <= date\n                <= cali.at[service, \"end_date\"]\n                and cali.at[service, weekday_str] == 1\n            ):\n                return True\n            else:\n                return False\n    # If you made it here, then something went wrong\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a subset of feed. trips that are active at a given date and at a given time.", "response": "def get_trips(\n    feed: \"Feed\", date: Optional[str] = None, time: Optional[str] = None\n) -> DataFrame:\n    \"\"\"\n    Return a subset of ``feed.trips``.\n\n    Parameters\n    ----------\n    feed : Feed\n    date : string\n        YYYYMMDD date string\n    time : string\n        HH:MM:SS time string, possibly with HH > 23\n\n    Returns\n    -------\n    DataFrame\n        The subset of ``feed.trips`` containing trips active (starting)\n        on the given date at the given time.\n        If no date or time are specified, then return the entire\n        ``feed.trips``.\n\n    \"\"\"\n    if feed.trips is None or date is None:\n        return feed.trips\n\n    f = feed.trips.copy()\n    f[\"is_active\"] = f[\"trip_id\"].map(\n        lambda trip_id: feed.is_active_trip(trip_id, date)\n    )\n    f = f[f[\"is_active\"]].copy()\n    del f[\"is_active\"]\n\n    if time is not None:\n        # Get trips active during given time\n        g = pd.merge(f, feed.stop_times[[\"trip_id\", \"departure_time\"]])\n\n        def F(group):\n            d = {}\n            start = group[\"departure_time\"].dropna().min()\n            end = group[\"departure_time\"].dropna().max()\n            try:\n                result = start <= time <= end\n            except TypeError:\n                result = False\n            d[\"is_active\"] = result\n            return pd.Series(d)\n\n        h = g.groupby(\"trip_id\").apply(F).reset_index()\n        f = pd.merge(f, h[h[\"is_active\"]])\n        del f[\"is_active\"]\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the activity of a given trip on a given list of dates.", "response": "def compute_trip_activity(feed: \"Feed\", dates: List[str]) -> DataFrame:\n    \"\"\"\n    Mark trip as active or inactive on the given dates as computed\n    by :func:`is_active_trip`.\n\n    Parameters\n    ----------\n    feed : Feed\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute activity\n\n    Returns\n    -------\n    DataFrame\n        Columns are\n\n        - ``'trip_id'``\n        - ``dates[0]``: 1 if the trip is active on ``dates[0]``;\n          0 otherwise\n        - ``dates[1]``: 1 if the trip is active on ``dates[1]``;\n          0 otherwise\n        - etc.\n        - ``dates[-1]``: 1 if the trip is active on ``dates[-1]``;\n          0 otherwise\n\n        If ``dates`` is ``None`` or the empty list, then return an\n        empty DataFrame.\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.trips``\n    - Those used in :func:`is_active_trip`\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    f = feed.trips.copy()\n    for date in dates:\n        f[date] = f[\"trip_id\"].map(\n            lambda trip_id: int(feed.is_active_trip(trip_id, date))\n        )\n    return f[[\"trip_id\"] + list(dates)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_busiest_date(feed: \"Feed\", dates: List[str]) -> str:\n    f = feed.compute_trip_activity(dates)\n    s = [(f[c].sum(), c) for c in f.columns if c != \"trip_id\"]\n    return max(s)[1]", "response": "Given a list of dates return the first date that has the maximum number of active trips."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the trip stats for a given feed.", "response": "def compute_trip_stats(\n    feed: \"Feed\",\n    route_ids: Optional[List[str]] = None,\n    *,\n    compute_dist_from_shapes: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Return a DataFrame with the following columns:\n\n    - ``'trip_id'``\n    - ``'route_id'``\n    - ``'route_short_name'``\n    - ``'route_type'``\n    - ``'direction_id'``: NaN if missing from feed\n    - ``'shape_id'``: NaN if missing from feed\n    - ``'num_stops'``: number of stops on trip\n    - ``'start_time'``: first departure time of the trip\n    - ``'end_time'``: last departure time of the trip\n    - ``'start_stop_id'``: stop ID of the first stop of the trip\n    - ``'end_stop_id'``: stop ID of the last stop of the trip\n    - ``'is_loop'``: 1 if the start and end stop are less than 400m apart and\n      0 otherwise\n    - ``'distance'``: distance of the trip in ``feed.dist_units``;\n      contains all ``np.nan`` entries if ``feed.shapes is None``\n    - ``'duration'``: duration of the trip in hours\n    - ``'speed'``: distance/duration\n\n    If ``feed.stop_times`` has a ``shape_dist_traveled`` column with at\n    least one non-NaN value and ``compute_dist_from_shapes == False``,\n    then use that column to compute the distance column.\n    Else if ``feed.shapes is not None``, then compute the distance\n    column using the shapes and Shapely.\n    Otherwise, set the distances to NaN.\n\n    If route IDs are given, then restrict to trips on those routes.\n\n    Notes\n    -----\n    - Assume the following feed attributes are not ``None``:\n\n        * ``feed.trips``\n        * ``feed.routes``\n        * ``feed.stop_times``\n        * ``feed.shapes`` (optionally)\n        * Those used in :func:`.stops.build_geometry_by_stop`\n\n    - Calculating trip distances with ``compute_dist_from_shapes=True``\n      seems pretty accurate.  For example, calculating trip distances on\n      `this Portland feed\n      <https://transitfeeds.com/p/trimet/43/1400947517>`_\n      using ``compute_dist_from_shapes=False`` and\n      ``compute_dist_from_shapes=True``,\n      yields a difference of at most 0.83km from the original values.\n\n    \"\"\"\n    f = feed.trips.copy()\n\n    # Restrict to given route IDs\n    if route_ids is not None:\n        f = f[f[\"route_id\"].isin(route_ids)].copy()\n\n    # Merge with stop times and extra trip info.\n    # Convert departure times to seconds past midnight to\n    # compute trip durations later.\n    if \"direction_id\" not in f.columns:\n        f[\"direction_id\"] = np.nan\n    if \"shape_id\" not in f.columns:\n        f[\"shape_id\"] = np.nan\n\n    f = (\n        f[[\"route_id\", \"trip_id\", \"direction_id\", \"shape_id\"]]\n        .merge(feed.routes[[\"route_id\", \"route_short_name\", \"route_type\"]])\n        .merge(feed.stop_times)\n        .sort_values([\"trip_id\", \"stop_sequence\"])\n        .assign(\n            departure_time=lambda x: x[\"departure_time\"].map(\n                hp.timestr_to_seconds\n            )\n        )\n    )\n\n    # Compute all trips stats except distance,\n    # which is possibly more involved\n    geometry_by_stop = feed.build_geometry_by_stop(use_utm=True)\n    g = f.groupby(\"trip_id\")\n\n    def my_agg(group):\n        d = OrderedDict()\n        d[\"route_id\"] = group[\"route_id\"].iat[0]\n        d[\"route_short_name\"] = group[\"route_short_name\"].iat[0]\n        d[\"route_type\"] = group[\"route_type\"].iat[0]\n        d[\"direction_id\"] = group[\"direction_id\"].iat[0]\n        d[\"shape_id\"] = group[\"shape_id\"].iat[0]\n        d[\"num_stops\"] = group.shape[0]\n        d[\"start_time\"] = group[\"departure_time\"].iat[0]\n        d[\"end_time\"] = group[\"departure_time\"].iat[-1]\n        d[\"start_stop_id\"] = group[\"stop_id\"].iat[0]\n        d[\"end_stop_id\"] = group[\"stop_id\"].iat[-1]\n        dist = geometry_by_stop[d[\"start_stop_id\"]].distance(\n            geometry_by_stop[d[\"end_stop_id\"]]\n        )\n        d[\"is_loop\"] = int(dist < 400)\n        d[\"duration\"] = (d[\"end_time\"] - d[\"start_time\"]) / 3600\n        return pd.Series(d)\n\n    # Apply my_agg, but don't reset index yet.\n    # Need trip ID as index to line up the results of the\n    # forthcoming distance calculation\n    h = g.apply(my_agg)\n\n    # Compute distance\n    if (\n        hp.is_not_null(f, \"shape_dist_traveled\")\n        and not compute_dist_from_shapes\n    ):\n        # Compute distances using shape_dist_traveled column\n        h[\"distance\"] = g.apply(\n            lambda group: group[\"shape_dist_traveled\"].max()\n        )\n    elif feed.shapes is not None:\n        # Compute distances using the shapes and Shapely\n        geometry_by_shape = feed.build_geometry_by_shape(use_utm=True)\n        geometry_by_stop = feed.build_geometry_by_stop(use_utm=True)\n        m_to_dist = hp.get_convert_dist(\"m\", feed.dist_units)\n\n        def compute_dist(group):\n            \"\"\"\n            Return the distance traveled along the trip between the\n            first and last stops.\n            If that distance is negative or if the trip's linestring\n            intersects itfeed, then return the length of the trip's\n            linestring instead.\n            \"\"\"\n            shape = group[\"shape_id\"].iat[0]\n            try:\n                # Get the linestring for this trip\n                linestring = geometry_by_shape[shape]\n            except KeyError:\n                # Shape ID is NaN or doesn't exist in shapes.\n                # No can do.\n                return np.nan\n\n            # If the linestring intersects itfeed, then that can cause\n            # errors in the computation below, so just\n            # return the length of the linestring as a good approximation\n            D = linestring.length\n            if not linestring.is_simple:\n                return D\n\n            # Otherwise, return the difference of the distances along\n            # the linestring of the first and last stop\n            start_stop = group[\"stop_id\"].iat[0]\n            end_stop = group[\"stop_id\"].iat[-1]\n            try:\n                start_point = geometry_by_stop[start_stop]\n                end_point = geometry_by_stop[end_stop]\n            except KeyError:\n                # One of the two stop IDs is NaN, so just\n                # return the length of the linestring\n                return D\n            d1 = linestring.project(start_point)\n            d2 = linestring.project(end_point)\n            d = d2 - d1\n            if 0 < d < D + 100:\n                return d\n            else:\n                # Something is probably wrong, so just\n                # return the length of the linestring\n                return D\n\n        h[\"distance\"] = g.apply(compute_dist)\n        # Convert from meters\n        h[\"distance\"] = h[\"distance\"].map(m_to_dist)\n    else:\n        h[\"distance\"] = np.nan\n\n    # Reset index and compute final stats\n    h = h.reset_index()\n    h[\"speed\"] = h[\"distance\"] / h[\"duration\"]\n    h[[\"start_time\", \"end_time\"]] = h[[\"start_time\", \"end_time\"]].applymap(\n        lambda x: hp.timestr_to_seconds(x, inverse=True)\n    )\n\n    return h.sort_values([\"route_id\", \"direction_id\", \"start_time\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef locate_trips(feed: \"Feed\", date: str, times: List[str]) -> DataFrame:\n    if not hp.is_not_null(feed.stop_times, \"shape_dist_traveled\"):\n        raise ValueError(\n            \"feed.stop_times needs to have a non-null shape_dist_traveled \"\n            \"column. You can create it, possibly with some inaccuracies, \"\n            \"via feed2 = feed.append_dist_to_stop_times().\"\n        )\n\n    if \"shape_id\" not in feed.trips.columns:\n        raise ValueError(\"feed.trips.shape_id must exist.\")\n\n    # Start with stop times active on date\n    f = feed.get_stop_times(date)\n    f[\"departure_time\"] = f[\"departure_time\"].map(hp.timestr_to_seconds)\n\n    # Compute relative distance of each trip along its path\n    # at the given time times.\n    # Use linear interpolation based on stop departure times and\n    # shape distance traveled.\n    geometry_by_shape = feed.build_geometry_by_shape(use_utm=False)\n    sample_times = np.array([hp.timestr_to_seconds(s) for s in times])\n\n    def compute_rel_dist(group):\n        dists = sorted(group[\"shape_dist_traveled\"].values)\n        times = sorted(group[\"departure_time\"].values)\n        ts = sample_times[\n            (sample_times >= times[0]) & (sample_times <= times[-1])\n        ]\n        ds = np.interp(ts, times, dists)\n        return pd.DataFrame({\"time\": ts, \"rel_dist\": ds / dists[-1]})\n\n    # return f.groupby('trip_id', group_keys=False).\\\n    #   apply(compute_rel_dist).reset_index()\n    g = f.groupby(\"trip_id\").apply(compute_rel_dist).reset_index()\n\n    # Delete extraneous multi-index column\n    del g[\"level_1\"]\n\n    # Convert times back to time strings\n    g[\"time\"] = g[\"time\"].map(lambda x: hp.timestr_to_seconds(x, inverse=True))\n\n    # Merge in more trip info and\n    # compute longitude and latitude of trip from relative distance\n    t = feed.trips.copy()\n    if \"direction_id\" not in t.columns:\n        t[\"direction_id\"] = np.nan\n\n    h = pd.merge(g, t[[\"trip_id\", \"route_id\", \"direction_id\", \"shape_id\"]])\n    if not h.shape[0]:\n        # Return a DataFrame with the promised headers but no data.\n        # Without this check, result below could be an empty DataFrame.\n        h[\"lon\"] = pd.Series()\n        h[\"lat\"] = pd.Series()\n        return h\n\n    def get_lonlat(group):\n        shape = group[\"shape_id\"].iat[0]\n        linestring = geometry_by_shape[shape]\n        lonlats = [\n            linestring.interpolate(d, normalized=True).coords[0]\n            for d in group[\"rel_dist\"].values\n        ]\n        group[\"lon\"], group[\"lat\"] = zip(*lonlats)\n        return group\n\n    return h.groupby(\"shape_id\").apply(get_lonlat)", "response": "Locate all trips in a given date and times."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trip_to_geojson(\n    feed: \"Feed\", trip_id: str, *, include_stops: bool = False\n) -> Dict:\n    \"\"\"\n    Return a GeoJSON representation of the given trip, optionally with\n    its stops.\n\n    Parameters\n    ----------\n    feed : Feed\n    trip_id : string\n        ID of trip in ``feed.trips``\n    include_stops : boolean\n\n    Returns\n    -------\n    dictionary\n        A (decoded) GeoJSON FeatureCollection comprising a Linestring\n        feature representing the trip's shape.\n        If ``include_stops``, then also include one Point feature for\n        each stop  visited by the trip.\n        The Linestring feature will contain as properties all the\n        columns in ``feed.trips`` pertaining to the given trip,\n        and each Point feature will contain as properties all the\n        columns in ``feed.stops`` pertaining to the stop,\n        except the ``stop_lat`` and ``stop_lon`` properties.\n\n        Return the empty dictionary if the trip has no shape.\n\n    \"\"\"\n    # Get the relevant shapes\n    t = feed.trips.copy()\n    t = t[t[\"trip_id\"] == trip_id].copy()\n    shid = t[\"shape_id\"].iat[0]\n    geometry_by_shape = feed.build_geometry_by_shape(\n        use_utm=False, shape_ids=[shid]\n    )\n\n    if not geometry_by_shape:\n        return {}\n\n    features = [\n        {\n            \"type\": \"Feature\",\n            \"properties\": json.loads(t.to_json(orient=\"records\"))[0],\n            \"geometry\": sg.mapping(sg.LineString(geometry_by_shape[shid])),\n        }\n    ]\n\n    if include_stops:\n        # Get relevant stops and geometrys\n        s = feed.get_stops(trip_id=trip_id)\n        cols = set(s.columns) - set([\"stop_lon\", \"stop_lat\"])\n        s = s[list(cols)].copy()\n        stop_ids = s[\"stop_id\"].tolist()\n        geometry_by_stop = feed.build_geometry_by_stop(stop_ids=stop_ids)\n        features.extend(\n            [\n                {\n                    \"type\": \"Feature\",\n                    \"properties\": json.loads(\n                        s[s[\"stop_id\"] == stop_id].to_json(orient=\"records\")\n                    )[0],\n                    \"geometry\": sg.mapping(geometry_by_stop[stop_id]),\n                }\n                for stop_id in stop_ids\n            ]\n        )\n\n    return {\"type\": \"FeatureCollection\", \"features\": features}", "response": "Return a GeoJSON representation of the given trip."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Folium map showing the given trips and their stops.", "response": "def map_trips(\n    feed: \"Feed\",\n    trip_ids: List[str],\n    color_palette: List[str] = cs.COLORS_SET2,\n    *,\n    include_stops: bool = True,\n):\n    \"\"\"\n    Return a Folium map showing the given trips and (optionally)\n    their stops.\n\n    Parameters\n    ----------\n    feed : Feed\n    trip_ids : list\n        IDs of trips in ``feed.trips``\n    color_palette : list\n        Palette to use to color the routes. If more routes than colors,\n        then colors will be recycled.\n    include_stops : boolean\n        If ``True``, then include stops in the map\n\n    Returns\n    -------\n    dictionary\n        A Folium Map depicting the shapes of the trips.\n        If ``include_stops``, then include the stops for each trip.\n\n    Notes\n    ------\n    - Requires Folium\n\n    \"\"\"\n    import folium as fl\n    import folium.plugins as fp\n\n    # Get routes slice and convert to dictionary\n    trips = (\n        feed.trips.loc[lambda x: x[\"trip_id\"].isin(trip_ids)]\n        .fillna(\"n/a\")\n        .to_dict(orient=\"records\")\n    )\n\n    # Create colors\n    n = len(trips)\n    colors = [color_palette[i % len(color_palette)] for i in range(n)]\n\n    # Initialize map\n    my_map = fl.Map(tiles=\"cartodbpositron\")\n\n    # Collect route bounding boxes to set map zoom later\n    bboxes = []\n\n    # Create a feature group for each route and add it to the map\n    for i, trip in enumerate(trips):\n        collection = feed.trip_to_geojson(\n            trip_id=trip[\"trip_id\"], include_stops=include_stops\n        )\n        group = fl.FeatureGroup(name=\"Trip \" + trip[\"trip_id\"])\n        color = colors[i]\n\n        for f in collection[\"features\"]:\n            prop = f[\"properties\"]\n\n            # Add stop\n            if f[\"geometry\"][\"type\"] == \"Point\":\n                lon, lat = f[\"geometry\"][\"coordinates\"]\n                fl.CircleMarker(\n                    location=[lat, lon],\n                    radius=8,\n                    fill=True,\n                    color=color,\n                    weight=1,\n                    popup=fl.Popup(hp.make_html(prop)),\n                ).add_to(group)\n\n            # Add path\n            else:\n                # Path\n                prop[\"color\"] = color\n                path = fl.GeoJson(\n                    f,\n                    name=trip,\n                    style_function=lambda x: {\n                        \"color\": x[\"properties\"][\"color\"]\n                    },\n                )\n                path.add_child(fl.Popup(hp.make_html(prop)))\n                path.add_to(group)\n\n                # Direction arrows, assuming, as GTFS does, that\n                # trip direction equals LineString direction\n                fp.PolyLineTextPath(\n                    path,\n                    \"        \\u27A4        \",\n                    repeat=True,\n                    offset=5.5,\n                    attributes={\"fill\": color, \"font-size\": \"18\"},\n                ).add_to(group)\n\n                bboxes.append(sg.box(*sg.shape(f[\"geometry\"]).bounds))\n\n        group.add_to(my_map)\n\n    fl.LayerControl().add_to(my_map)\n\n    # Fit map to bounds\n    bounds = so.unary_union(bboxes).bounds\n    bounds2 = [bounds[1::-1], bounds[3:1:-1]]  # Folium expects this ordering\n    my_map.fit_bounds(bounds2)\n\n    return my_map"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstrips whitespace from all column names in the given DataFrame and return the resulting DataFrame.", "response": "def clean_column_names(df: DataFrame) -> DataFrame:\n    \"\"\"\n    Strip the whitespace from all column names in the given DataFrame\n    and return the result.\n    \"\"\"\n    f = df.copy()\n    f.columns = [col.strip() for col in f.columns]\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndropping stops trips routes and calendar dates from the given Feed.", "response": "def drop_zombies(feed: \"Feed\") -> \"Feed\":\n    \"\"\"\n    In the given \"Feed\", drop stops with no stop times,\n    trips with no stop times, shapes with no trips,\n    routes with no trips, and services with no trips, in that order.\n    Return the resulting \"Feed\".\n    \"\"\"\n    feed = feed.copy()\n\n    # Drop stops of location type 0 that lack stop times\n    ids = feed.stop_times[\"stop_id\"].unique()\n    f = feed.stops\n    cond = f[\"stop_id\"].isin(ids)\n    if \"location_type\" in f.columns:\n        cond |= f[\"location_type\"] != 0\n    feed.stops = f[cond].copy()\n\n    # Drop trips with no stop times\n    ids = feed.stop_times[\"trip_id\"].unique()\n    f = feed.trips\n    feed.trips = f[f[\"trip_id\"].isin(ids)]\n\n    # Drop shapes with no trips\n    ids = feed.trips[\"shape_id\"].unique()\n    f = feed.shapes\n    if f is not None:\n        feed.shapes = f[f[\"shape_id\"].isin(ids)]\n\n    # Drop routes with no trips\n    ids = feed.trips[\"route_id\"].unique()\n    f = feed.routes\n    feed.routes = f[f[\"route_id\"].isin(ids)]\n\n    # Drop services with no trips\n    ids = feed.trips[\"service_id\"].unique()\n    if feed.calendar is not None:\n        f = feed.calendar\n        feed.calendar = f[f[\"service_id\"].isin(ids)]\n    if feed.calendar_dates is not None:\n        f = feed.calendar_dates\n        feed.calendar_dates = f[f[\"service_id\"].isin(ids)]\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_ids(feed: \"Feed\") -> \"Feed\":\n    # Alter feed inputs only, and build a new feed from them.\n    # The derived feed attributes, such as feed.trips_i,\n    # will be automatically handled when creating the new feed.\n    feed = feed.copy()\n\n    for table in cs.GTFS_REF[\"table\"].unique():\n        f = getattr(feed, table)\n        if f is None:\n            continue\n        for column in cs.GTFS_REF.loc[cs.GTFS_REF[\"table\"] == table, \"column\"]:\n            if column in f.columns and column.endswith(\"_id\"):\n                try:\n                    f[column] = f[column].str.strip().str.replace(r\"\\s+\", \"_\")\n                    setattr(feed, table, f)\n                except AttributeError:\n                    # Column is not of string type\n                    continue\n\n    return feed", "response": "Removes all whitespace from all string IDs and replace every remaining whitespace chunk with an underscore."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans the time columns of the given Feed.", "response": "def clean_times(feed: \"Feed\") -> \"Feed\":\n    \"\"\"\n    In the given \"Feed\", convert H:MM:SS time strings to HH:MM:SS time\n    strings to make sorting by time work as expected.\n    Return the resulting \"Feed\".\n    \"\"\"\n\n    def reformat(t):\n        if pd.isnull(t):\n            return t\n        t = t.strip()\n        if len(t) == 7:\n            t = \"0\" + t\n        return t\n\n    feed = feed.copy()\n    tables_and_columns = [\n        (\"stop_times\", [\"arrival_time\", \"departure_time\"]),\n        (\"frequencies\", [\"start_time\", \"end_time\"]),\n    ]\n    for table, columns in tables_and_columns:\n        f = getattr(feed, table)\n        if f is not None:\n            f[columns] = f[columns].applymap(reformat)\n        setattr(feed, table, f)\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_route_short_names(feed: \"Feed\") -> \"Feed\":\n    feed = feed.copy()\n    r = feed.routes\n    if r is None:\n        return feed\n\n    # Fill NaNs and strip whitespace\n    r[\"route_short_name\"] = r[\"route_short_name\"].fillna(\"n/a\").str.strip()\n\n    # Disambiguate\n    def disambiguate(row):\n        rsn, rid = row\n        return rsn + \"-\" + rid\n\n    r[\"dup\"] = r[\"route_short_name\"].duplicated(keep=False)\n    r.loc[r[\"dup\"], \"route_short_name\"] = r.loc[\n        r[\"dup\"], [\"route_short_name\", \"route_id\"]\n    ].apply(disambiguate, axis=1)\n    del r[\"dup\"]\n\n    feed.routes = r\n    return feed", "response": "Clean up missing route short names in a feed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naggregate routes by route short name say and assign new route IDs.", "response": "def aggregate_routes(\n    feed: \"Feed\", by: str = \"route_short_name\", route_id_prefix: str = \"route_\"\n) -> \"Feed\":\n    \"\"\"\n    Aggregate routes by route short name, say, and assign new route IDs.\n\n    Parameters\n    ----------\n    feed : \"Feed\"\n    by : string\n        A column of ``feed.routes``\n    route_id_prefix : string\n        Prefix to use when creating new route IDs\n\n    Returns\n    -------\n    \"Feed\"\n        The result is built from the given \"Feed\" as follows.\n        Group ``feed.routes`` by the ``by`` column, and for each group\n\n        1. Choose the first route in the group\n        2. Assign a new route ID based on the given ``route_id_prefix``\n           string and a running count, e.g. ``'route_013'``\n        3. Assign all the trips associated with routes in the group to\n           that first route\n        4. Update the route IDs in the other \"Feed\" tables\n\n    \"\"\"\n    if by not in feed.routes.columns:\n        raise ValueError(f\"Column {by} not in feed.routes\")\n\n    feed = feed.copy()\n\n    # Create new route IDs\n    routes = feed.routes\n    n = routes.groupby(by).ngroups\n    k = int(math.log10(n)) + 1  # Number of digits for padding IDs\n    nrid_by_orid = dict()\n    i = 1\n    for col, group in routes.groupby(by):\n        nrid = f\"route_{i:0{k}d}\"\n        d = {orid: nrid for orid in group[\"route_id\"].values}\n        nrid_by_orid.update(d)\n        i += 1\n\n    routes[\"route_id\"] = routes[\"route_id\"].map(lambda x: nrid_by_orid[x])\n    routes = routes.groupby(by).first().reset_index()\n    feed.routes = routes\n\n    # Update route IDs of trips\n    trips = feed.trips\n    trips[\"route_id\"] = trips[\"route_id\"].map(lambda x: nrid_by_orid[x])\n    feed.trips = trips\n\n    # Update route IDs of transfers\n    if feed.transfers is not None:\n        transfers = feed.transfers\n        transfers[\"route_id\"] = transfers[\"route_id\"].map(\n            lambda x: nrid_by_orid[x]\n        )\n        feed.transfers = transfers\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all expired entries from the given Feed.", "response": "def clean(feed: \"Feed\") -> \"Feed\":\n    \"\"\"\n    Apply\n\n    #. :func:`drop_zombies`\n    #. :func:`clean_ids`\n    #. :func:`clean_times`\n    #. :func:`clean_route_short_names`\n\n    to the given \"Feed\" in that order.\n    Return the resulting \"Feed\".\n    \"\"\"\n    feed = feed.copy()\n    ops = [\n        \"clean_ids\",\n        \"clean_times\",\n        \"clean_route_short_names\",\n        \"drop_zombies\",\n    ]\n    for op in ops:\n        feed = globals()[op](feed)\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndropping all DataFrame columns that are not in the GTFS.", "response": "def drop_invalid_columns(feed: \"Feed\") -> \"Feed\":\n    \"\"\"\n    Drop all DataFrame columns of the given \"Feed\" that are not\n    listed in the GTFS.\n    Return the resulting new \"Feed\".\n    \"\"\"\n    feed = feed.copy()\n    for table, group in cs.GTFS_REF.groupby(\"table\"):\n        f = getattr(feed, table)\n        if f is None:\n            continue\n        valid_columns = group[\"column\"].values\n        for col in f.columns:\n            if col not in valid_columns:\n                print(f\"{table}: dropping invalid column {col}\")\n                del f[col]\n        setattr(feed, table, f)\n\n    return feed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_route_stats_base(\n    trip_stats_subset: DataFrame,\n    headway_start_time: str = \"07:00:00\",\n    headway_end_time: str = \"19:00:00\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Compute stats for the given subset of trips stats.\n\n    Parameters\n    ----------\n    trip_stats_subset : DataFrame\n        Subset of the output of :func:`.trips.compute_trip_stats`\n    split_directions : boolean\n        If ``True``, then separate the stats by trip direction (0 or 1);\n        otherwise aggregate trips visiting from both directions\n    headway_start_time : string\n        HH:MM:SS time string indicating the start time for computing\n        headway stats\n    headway_end_time : string\n        HH:MM:SS time string indicating the end time for computing\n        headway stats\n\n    Returns\n    -------\n    DataFrame\n        Columns are\n\n        - ``'route_id'``\n        - ``'route_short_name'``\n        - ``'route_type'``\n        - ``'direction_id'``\n        - ``'num_trips'``: number of trips on the route in the subset\n        - ``'num_trip_starts'``: number of trips on the route with\n          nonnull start times\n        - ``'num_trip_ends'``: number of trips on the route with nonnull\n          end times that end before 23:59:59\n        - ``'is_loop'``: 1 if at least one of the trips on the route has\n          its ``is_loop`` field equal to 1; 0 otherwise\n        - ``'is_bidirectional'``: 1 if the route has trips in both\n          directions; 0 otherwise\n        - ``'start_time'``: start time of the earliest trip on the route\n        - ``'end_time'``: end time of latest trip on the route\n        - ``'max_headway'``: maximum of the durations (in minutes)\n          between trip starts on the route between\n          ``headway_start_time`` and ``headway_end_time`` on the given\n          dates\n        - ``'min_headway'``: minimum of the durations (in minutes)\n          mentioned above\n        - ``'mean_headway'``: mean of the durations (in minutes)\n          mentioned above\n        - ``'peak_num_trips'``: maximum number of simultaneous trips in\n          service (for the given direction, or for both directions when\n          ``split_directions==False``)\n        - ``'peak_start_time'``: start time of first longest period\n          during which the peak number of trips occurs\n        - ``'peak_end_time'``: end time of first longest period during\n          which the peak number of trips occurs\n        - ``'service_duration'``: total of the duration of each trip on\n          the route in the given subset of trips; measured in hours\n        - ``'service_distance'``: total of the distance traveled by each\n          trip on the route in the given subset of trips; measured in\n          whatever distance units are present in ``trip_stats_subset``;\n          contains all ``np.nan`` entries if ``feed.shapes is None``\n        - ``'service_speed'``: service_distance/service_duration;\n          measured in distance units per hour\n        - ``'mean_trip_distance'``: service_distance/num_trips\n        - ``'mean_trip_duration'``: service_duration/num_trips\n\n    If not ``split_directions``, then remove the\n    direction_id column and compute each route's stats,\n    except for headways, using\n    its trips running in both directions.\n    In this case, (1) compute max headway by taking the max of the\n    max headways in both directions; (2) compute mean headway by\n    taking the weighted mean of the mean headways in both\n    directions.\n\n    If ``trip_stats_subset`` is empty, return an empty DataFrame.\n\n    Raise a ValueError if ``split_directions`` and no non-NaN\n    direction ID values present\n    \"\"\"\n    if trip_stats_subset.empty:\n        return pd.DataFrame()\n\n    # Convert trip start and end times to seconds to ease calculations below\n    f = trip_stats_subset.copy()\n    f[[\"start_time\", \"end_time\"]] = f[[\"start_time\", \"end_time\"]].applymap(\n        hp.timestr_to_seconds\n    )\n\n    headway_start = hp.timestr_to_seconds(headway_start_time)\n    headway_end = hp.timestr_to_seconds(headway_end_time)\n\n    def compute_route_stats_split_directions(group):\n        # Take this group of all trips stats for a single route\n        # and compute route-level stats.\n        d = OrderedDict()\n        d[\"route_short_name\"] = group[\"route_short_name\"].iat[0]\n        d[\"route_type\"] = group[\"route_type\"].iat[0]\n        d[\"num_trips\"] = group.shape[0]\n        d[\"num_trip_starts\"] = group[\"start_time\"].count()\n        d[\"num_trip_ends\"] = group.loc[\n            group[\"end_time\"] < 24 * 3600, \"end_time\"\n        ].count()\n        d[\"is_loop\"] = int(group[\"is_loop\"].any())\n        d[\"start_time\"] = group[\"start_time\"].min()\n        d[\"end_time\"] = group[\"end_time\"].max()\n\n        # Compute max and mean headway\n        stimes = group[\"start_time\"].values\n        stimes = sorted(\n            [\n                stime\n                for stime in stimes\n                if headway_start <= stime <= headway_end\n            ]\n        )\n        headways = np.diff(stimes)\n        if headways.size:\n            d[\"max_headway\"] = np.max(headways) / 60  # minutes\n            d[\"min_headway\"] = np.min(headways) / 60  # minutes\n            d[\"mean_headway\"] = np.mean(headways) / 60  # minutes\n        else:\n            d[\"max_headway\"] = np.nan\n            d[\"min_headway\"] = np.nan\n            d[\"mean_headway\"] = np.nan\n\n        # Compute peak num trips\n        active_trips = hp.get_active_trips_df(\n            group[[\"start_time\", \"end_time\"]]\n        )\n        times, counts = active_trips.index.values, active_trips.values\n        start, end = hp.get_peak_indices(times, counts)\n        d[\"peak_num_trips\"] = counts[start]\n        d[\"peak_start_time\"] = times[start]\n        d[\"peak_end_time\"] = times[end]\n\n        d[\"service_distance\"] = group[\"distance\"].sum()\n        d[\"service_duration\"] = group[\"duration\"].sum()\n\n        return pd.Series(d)\n\n    def compute_route_stats(group):\n        d = OrderedDict()\n        d[\"route_short_name\"] = group[\"route_short_name\"].iat[0]\n        d[\"route_type\"] = group[\"route_type\"].iat[0]\n        d[\"num_trips\"] = group.shape[0]\n        d[\"num_trip_starts\"] = group[\"start_time\"].count()\n        d[\"num_trip_ends\"] = group.loc[\n            group[\"end_time\"] < 24 * 3600, \"end_time\"\n        ].count()\n        d[\"is_loop\"] = int(group[\"is_loop\"].any())\n        d[\"is_bidirectional\"] = int(group[\"direction_id\"].unique().size > 1)\n        d[\"start_time\"] = group[\"start_time\"].min()\n        d[\"end_time\"] = group[\"end_time\"].max()\n\n        # Compute headway stats\n        headways = np.array([])\n        for direction in [0, 1]:\n            stimes = group[group[\"direction_id\"] == direction][\n                \"start_time\"\n            ].values\n            stimes = sorted(\n                [\n                    stime\n                    for stime in stimes\n                    if headway_start <= stime <= headway_end\n                ]\n            )\n            headways = np.concatenate([headways, np.diff(stimes)])\n        if headways.size:\n            d[\"max_headway\"] = np.max(headways) / 60  # minutes\n            d[\"min_headway\"] = np.min(headways) / 60  # minutes\n            d[\"mean_headway\"] = np.mean(headways) / 60  # minutes\n        else:\n            d[\"max_headway\"] = np.nan\n            d[\"min_headway\"] = np.nan\n            d[\"mean_headway\"] = np.nan\n\n        # Compute peak num trips\n        active_trips = hp.get_active_trips_df(\n            group[[\"start_time\", \"end_time\"]]\n        )\n        times, counts = active_trips.index.values, active_trips.values\n        start, end = hp.get_peak_indices(times, counts)\n        d[\"peak_num_trips\"] = counts[start]\n        d[\"peak_start_time\"] = times[start]\n        d[\"peak_end_time\"] = times[end]\n\n        d[\"service_distance\"] = group[\"distance\"].sum()\n        d[\"service_duration\"] = group[\"duration\"].sum()\n\n        return pd.Series(d)\n\n    if split_directions:\n        f = f.loc[lambda x: x.direction_id.notnull()].assign(\n            direction_id=lambda x: x.direction_id.astype(int)\n        )\n        if f.empty:\n            raise ValueError(\n                \"At least one trip stats direction ID value \"\n                \"must be non-NaN.\"\n            )\n\n        g = (\n            f.groupby([\"route_id\", \"direction_id\"])\n            .apply(compute_route_stats_split_directions)\n            .reset_index()\n        )\n\n        # Add the is_bidirectional column\n        def is_bidirectional(group):\n            d = {}\n            d[\"is_bidirectional\"] = int(\n                group[\"direction_id\"].unique().size > 1\n            )\n            return pd.Series(d)\n\n        gg = g.groupby(\"route_id\").apply(is_bidirectional).reset_index()\n        g = g.merge(gg)\n    else:\n        g = f.groupby(\"route_id\").apply(compute_route_stats).reset_index()\n\n    # Compute a few more stats\n    g[\"service_speed\"] = g[\"service_distance\"] / g[\"service_duration\"]\n    g[\"mean_trip_distance\"] = g[\"service_distance\"] / g[\"num_trips\"]\n    g[\"mean_trip_duration\"] = g[\"service_duration\"] / g[\"num_trips\"]\n\n    # Convert route times to time strings\n    g[[\"start_time\", \"end_time\", \"peak_start_time\", \"peak_end_time\"]] = g[\n        [\"start_time\", \"end_time\", \"peak_start_time\", \"peak_end_time\"]\n    ].applymap(lambda x: hp.timestr_to_seconds(x, inverse=True))\n\n    return g", "response": "Compute stats for the given subset of trips stats."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_route_time_series_base(\n    trip_stats_subset: DataFrame,\n    date_label: str = \"20010101\",\n    freq: str = \"5Min\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Compute stats in a 24-hour time series form for the given subset of trips.\n\n    Parameters\n    ----------\n    trip_stats_subset : DataFrame\n        A subset of the output of :func:`.trips.compute_trip_stats`\n    split_directions : boolean\n        If ``True``, then separate each routes's stats by trip direction;\n        otherwise aggregate trips in both directions\n    freq : Pandas frequency string\n        Specifices the frequency with which to resample the time series;\n        max frequency is one minute ('Min')\n    date_label : string\n        YYYYMMDD date string used as the date in the time series index\n\n    Returns\n    -------\n    DataFrame\n        A time series version of the following route stats for each\n        route.\n\n        - ``num_trips``: number of trips in service on the route\n          at any time within the time bin\n        - ``num_trip_starts``: number of trips that start within\n          the time bin\n        - ``num_trip_ends``: number of trips that end within the\n          time bin, ignoring trips that end past midnight\n        - ``service_distance``: sum of the service duration accrued\n          during the time bin across all trips on the route;\n          measured in hours\n        - ``service_distance``: sum of the service distance accrued\n          during the time bin across all trips on the route; measured\n          in kilometers\n        - ``service_speed``: ``service_distance/service_duration``\n          for the route\n\n        The columns are hierarchical (multi-indexed) with\n\n        - top level: name is ``'indicator'``; values are\n          ``'num_trip_starts'``, ``'num_trip_ends'``, ``'num_trips'``,\n          ``'service_distance'``, ``'service_duration'``, and\n          ``'service_speed'``\n        - middle level: name is ``'route_id'``;\n          values are the active routes\n        - bottom level: name is ``'direction_id'``; values are 0s and 1s\n\n        If not ``split_directions``, then don't include the bottom level.\n\n        The time series has a timestamp index for a 24-hour period\n        sampled at the given frequency.\n        The maximum allowable frequency is 1 minute.\n        If ``trip_stats_subset`` is empty, then return an empty\n        DataFrame with the columns ``'num_trip_starts'``,\n        ``'num_trip_ends'``, ``'num_trips'``, ``'service_distance'``,\n        ``'service_duration'``, and ``'service_speed'``.\n\n    Notes\n    -----\n    - The time series is computed at a one-minute frequency, then\n      resampled at the end to the given frequency\n    - Trips that lack start or end times are ignored, so the the\n      aggregate ``num_trips`` across the day could be less than the\n      ``num_trips`` column of :func:`compute_route_stats_base`\n    - All trip departure times are taken modulo 24 hours.\n      So routes with trips that end past 23:59:59 will have all\n      their stats wrap around to the early morning of the time series,\n      except for their ``num_trip_ends`` indicator.\n      Trip endings past 23:59:59 not binned so that resampling the\n      ``num_trips`` indicator works efficiently.\n    - Note that the total number of trips for two consecutive time bins\n      t1 < t2 is the sum of the number of trips in bin t2 plus the\n      number of trip endings in bin t1.\n      Thus we can downsample the ``num_trips`` indicator by keeping\n      track of only one extra count, ``num_trip_ends``, and can avoid\n      recording individual trip IDs.\n    - All other indicators are downsampled by summing.\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    if trip_stats_subset.empty:\n        return pd.DataFrame()\n\n    tss = trip_stats_subset.copy()\n    if split_directions:\n        tss = tss.loc[lambda x: x.direction_id.notnull()].assign(\n            direction_id=lambda x: x.direction_id.astype(int)\n        )\n        if tss.empty:\n            raise ValueError(\n                \"At least one trip stats direction ID value \"\n                \"must be non-NaN.\"\n            )\n\n        # Alter route IDs to encode direction:\n        # <route ID>-0 and <route ID>-1 or <route ID>-NA\n        tss[\"route_id\"] = (\n            tss[\"route_id\"]\n            + \"-\"\n            + tss[\"direction_id\"].map(lambda x: str(int(x)))\n        )\n\n    routes = tss[\"route_id\"].unique()\n    # Build a dictionary of time series and then merge them all\n    # at the end.\n    # Assign a uniform generic date for the index\n    date_str = date_label\n    day_start = pd.to_datetime(date_str + \" 00:00:00\")\n    day_end = pd.to_datetime(date_str + \" 23:59:00\")\n    rng = pd.period_range(day_start, day_end, freq=\"Min\")\n    indicators = [\n        \"num_trip_starts\",\n        \"num_trip_ends\",\n        \"num_trips\",\n        \"service_duration\",\n        \"service_distance\",\n    ]\n\n    bins = [i for i in range(24 * 60)]  # One bin for each minute\n    num_bins = len(bins)\n\n    # Bin start and end times\n    def F(x):\n        return (hp.timestr_to_seconds(x) // 60) % (24 * 60)\n\n    tss[[\"start_index\", \"end_index\"]] = tss[\n        [\"start_time\", \"end_time\"]\n    ].applymap(F)\n    routes = sorted(set(tss[\"route_id\"].values))\n\n    # Bin each trip according to its start and end time and weight\n    series_by_route_by_indicator = {\n        indicator: {route: [0 for i in range(num_bins)] for route in routes}\n        for indicator in indicators\n    }\n    for index, row in tss.iterrows():\n        route = row[\"route_id\"]\n        start = row[\"start_index\"]\n        end = row[\"end_index\"]\n        distance = row[\"distance\"]\n\n        if start is None or np.isnan(start) or start == end:\n            continue\n\n        # Get bins to fill\n        if start <= end:\n            bins_to_fill = bins[start:end]\n        else:\n            bins_to_fill = bins[start:] + bins[:end]\n\n        # Bin trip\n        # Do num trip starts\n        series_by_route_by_indicator[\"num_trip_starts\"][route][start] += 1\n        # Don't mark trip ends for trips that run past midnight;\n        # allows for easy resampling of num_trips later\n        if start <= end:\n            series_by_route_by_indicator[\"num_trip_ends\"][route][end] += 1\n        # Do rest of indicators\n        for indicator in indicators[2:]:\n            if indicator == \"num_trips\":\n                weight = 1\n            elif indicator == \"service_duration\":\n                weight = 1 / 60\n            else:\n                weight = distance / len(bins_to_fill)\n            for bin in bins_to_fill:\n                series_by_route_by_indicator[indicator][route][bin] += weight\n\n    # Create one time series per indicator\n    rng = pd.date_range(date_str, periods=24 * 60, freq=\"Min\")\n    series_by_indicator = {\n        indicator: pd.DataFrame(\n            series_by_route_by_indicator[indicator], index=rng\n        ).fillna(0)\n        for indicator in indicators\n    }\n\n    # Combine all time series into one time series\n    g = hp.combine_time_series(\n        series_by_indicator, kind=\"route\", split_directions=split_directions\n    )\n\n    return hp.downsample(g, freq=freq)", "response": "Compute stats in a 24 - hour time series form for the given subset of trips."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a subset of feed. routes", "response": "def get_routes(\n    feed: \"Feed\", date: Optional[str] = None, time: Optional[str] = None\n) -> DataFrame:\n    \"\"\"\n    Return a subset of ``feed.routes``\n\n    Parameters\n    -----------\n    feed : Feed\n    date : string\n        YYYYMMDD date string restricting routes to only those active on\n        the date\n    time : string\n        HH:MM:SS time string, possibly with HH > 23, restricting routes\n        to only those active during the time\n\n    Returns\n    -------\n    DataFrame\n        A subset of ``feed.routes``\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.routes``\n    - Those used in :func:`.trips.get_trips`.\n\n    \"\"\"\n    if date is None:\n        return feed.routes.copy()\n\n    trips = feed.get_trips(date, time)\n    R = trips[\"route_id\"].unique()\n    return feed.routes[feed.routes[\"route_id\"].isin(R)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_route_stats(\n    feed: \"Feed\",\n    trip_stats_subset: DataFrame,\n    dates: List[str],\n    headway_start_time: str = \"07:00:00\",\n    headway_end_time: str = \"19:00:00\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Compute route stats for all the trips that lie in the given subset\n    of trip stats and that start on the given dates.\n\n    Parameters\n    ----------\n    feed : Feed\n    trip_stats_subset : DataFrame\n        Slice of the output of :func:`.trips.compute_trip_stats`\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute stats\n    split_directions : boolean\n        If ``True``, then separate the stats by trip direction (0 or 1);\n        otherwise aggregate trips visiting from both directions\n    headway_start_time : string\n        HH:MM:SS time string indicating the start time for computing\n        headway stats\n    headway_end_time : string\n        HH:MM:SS time string indicating the end time for computing\n        headway stats\n\n    Returns\n    -------\n    DataFrame\n        Columns are\n\n        - ``'date'``\n        - the columns listed in :func:``compute_route_stats_base``\n\n        Dates with no trip activity will have null stats.\n        Exclude dates that lie outside of the Feed's date range.\n        If all the dates given lie outside of the Feed's date range,\n        then return an empty DataFrame.\n\n    Notes\n    -----\n    - The route stats for date d contain stats for trips that start on\n      date d only and ignore trips that start on date d-1 and end on\n      date d\n    - Assume the following feed attributes are not ``None``:\n\n        * Those used in :func:`.helpers.compute_route_stats_base`\n\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    ts = trip_stats_subset.copy()\n    activity = feed.compute_trip_activity(dates)\n\n    # Collect stats for each date, memoizing stats by trip ID sequence\n    # to avoid unnecessary recomputations.\n    # Store in dictionary of the form\n    # trip ID sequence ->\n    # [stats DataFarme, date list that stats apply]\n    stats_and_dates_by_ids = {}\n    cols = [\n        \"route_id\",\n        \"route_short_name\",\n        \"route_type\",\n        \"num_trips\",\n        \"num_trip_ends\",\n        \"num_trip_starts\",\n        \"is_bidirectional\",\n        \"is_loop\",\n        \"start_time\",\n        \"end_time\",\n        \"max_headway\",\n        \"min_headway\",\n        \"mean_headway\",\n        \"peak_num_trips\",\n        \"peak_start_time\",\n        \"peak_end_time\",\n        \"service_duration\",\n        \"service_distance\",\n        \"service_speed\",\n        \"mean_trip_distance\",\n        \"mean_trip_duration\",\n    ]\n    if split_directions:\n        cols.append(\"direction_id\")\n    null_stats = pd.DataFrame(\n        OrderedDict([(c, np.nan) for c in cols]), index=[0]\n    )\n    for date in dates:\n        ids = tuple(activity.loc[activity[date] > 0, \"trip_id\"])\n        if ids in stats_and_dates_by_ids:\n            # Append date to date list\n            stats_and_dates_by_ids[ids][1].append(date)\n        elif not ids:\n            # Null stats\n            stats_and_dates_by_ids[ids] = [null_stats, [date]]\n        else:\n            # Compute stats\n            t = ts[ts[\"trip_id\"].isin(ids)].copy()\n            stats = compute_route_stats_base(\n                t,\n                split_directions=split_directions,\n                headway_start_time=headway_start_time,\n                headway_end_time=headway_end_time,\n            )\n\n            # Remember stats\n            stats_and_dates_by_ids[ids] = [stats, [date]]\n\n    # Assemble stats into DataFrame\n    frames = []\n    for stats, dates_ in stats_and_dates_by_ids.values():\n        for date in dates_:\n            f = stats.copy()\n            f[\"date\"] = date\n            frames.append(f)\n    f = (\n        pd.concat(frames)\n        .sort_values([\"date\", \"route_id\"])\n        .reset_index(drop=True)\n    )\n\n    return f", "response": "Compute route stats for all trips in a given subset of trips and dates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a DataFrame with the same index and hierarchical columns as output by compute_route_time_series_base.", "response": "def build_null_route_time_series(\n    feed: \"Feed\",\n    date_label: str = \"20010101\",\n    freq: str = \"5Min\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Return a route time series with the same index and hierarchical columns\n    as output by :func:`compute_route_time_series_base`,\n    but fill it full of null values.\n    \"\"\"\n    start = date_label\n    end = pd.to_datetime(date_label + \" 23:59:00\")\n    rng = pd.date_range(start, end, freq=freq)\n    inds = [\n        \"num_trip_starts\",\n        \"num_trip_ends\",\n        \"num_trips\",\n        \"service_duration\",\n        \"service_distance\",\n        \"service_speed\",\n    ]\n    rids = feed.routes.route_id\n    if split_directions:\n        product = [inds, rids, [0, 1]]\n        names = [\"indicator\", \"route_id\", \"direction_id\"]\n    else:\n        product = [inds, rids]\n        names = [\"indicator\", \"route_id\"]\n    cols = pd.MultiIndex.from_product(product, names=names)\n    return pd.DataFrame([], index=rng, columns=cols).sort_index(\n        axis=1, sort_remaining=True\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute route stats for each date in the given time series subset.", "response": "def compute_route_time_series(\n    feed: \"Feed\",\n    trip_stats_subset: DataFrame,\n    dates: List[str],\n    freq: str = \"5Min\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Compute route stats in time series form for the trips that lie in\n    the trip stats subset and that start on the given dates.\n\n    Parameters\n    ----------\n    feed : Feed\n    trip_stats_subset : DataFrame\n        Slice of the output of :func:`.trips.compute_trip_stats`\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute stats\n    split_directions : boolean\n        If ``True``, then separate each routes's stats by trip direction;\n        otherwise aggregate trips in both directions\n    freq : Pandas frequency string\n        Specifices the frequency with which to resample the time series;\n        max frequency is one minute ('Min')\n\n    Returns\n    -------\n    DataFrame\n        Same format as output by :func:`compute_route_time_series_base`\n        but with multiple dates\n\n        Exclude dates that lie outside of the Feed's date range.\n        If all dates lie outside the Feed's date range, then return an\n        empty DataFrame.\n\n    Notes\n    -----\n    - See the notes for :func:`compute_route_time_series_base`\n    - Assume the following feed attributes are not ``None``:\n\n        * Those used in :func:`.trips.get_trips`\n\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    activity = feed.compute_trip_activity(dates)\n    ts = trip_stats_subset.copy()\n\n    # Collect stats for each date, memoizing stats by trip ID sequence\n    # to avoid unnecessary re-computations.\n    # Store in dictionary of the form\n    # trip ID sequence ->\n    # [stats DataFarme, date list that stats apply]\n    stats_and_dates_by_ids = {}\n    null_stats = build_null_route_time_series(\n        feed, split_directions=split_directions, freq=freq\n    )\n    for date in dates:\n        ids = tuple(activity.loc[activity[date] > 0, \"trip_id\"])\n        if ids in stats_and_dates_by_ids:\n            # Append date to date list\n            stats_and_dates_by_ids[ids][1].append(date)\n        elif not ids:\n            # Null stats\n            stats_and_dates_by_ids[ids] = [null_stats, [date]]\n        else:\n            # Compute stats\n            t = ts[ts[\"trip_id\"].isin(ids)].copy()\n            stats = compute_route_time_series_base(\n                t,\n                split_directions=split_directions,\n                freq=freq,\n                date_label=date,\n            )\n\n            # Remember stats\n            stats_and_dates_by_ids[ids] = [stats, [date]]\n\n    # Assemble stats into DataFrame\n    frames = []\n    for stats, dates_ in stats_and_dates_by_ids.values():\n        for date in dates_:\n            f = stats.copy()\n            # Replace date\n            d = hp.datestr_to_date(date)\n            f.index = f.index.map(\n                lambda t: t.replace(year=d.year, month=d.month, day=d.day)\n            )\n            frames.append(f)\n\n    f = pd.concat(frames).sort_index().sort_index(axis=1, sort_remaining=True)\n\n    if len(dates) > 1:\n        # Insert missing dates and NaNs to complete series index\n        end_datetime = pd.to_datetime(dates[-1] + \" 23:59:59\")\n        new_index = pd.date_range(dates[0], end_datetime, freq=freq)\n        f = f.reindex(new_index)\n    else:\n        # Set frequency\n        f.index.freq = pd.tseries.frequencies.to_offset(freq)\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a timetable for the given route and dates.", "response": "def build_route_timetable(\n    feed: \"Feed\", route_id: str, dates: List[str]\n) -> DataFrame:\n    \"\"\"\n    Return a timetable for the given route and dates.\n\n    Parameters\n    ----------\n    feed : Feed\n    route_id : string\n        ID of a route in ``feed.routes``\n    dates : string or list\n        A YYYYMMDD date string or list thereof\n\n    Returns\n    -------\n    DataFrame\n        The columns are all those in ``feed.trips`` plus those in\n        ``feed.stop_times`` plus ``'date'``, and the trip IDs\n        are restricted to the given route ID.\n        The result is sorted first by date and then by grouping by\n        trip ID and sorting the groups by their first departure time.\n\n        Skip dates outside of the Feed's dates.\n\n        If there is no route activity on the given dates, then return\n        an empty DataFrame.\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.stop_times``\n    - Those used in :func:`.trips.get_trips`\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    t = pd.merge(feed.trips, feed.stop_times)\n    t = t[t[\"route_id\"] == route_id].copy()\n    a = feed.compute_trip_activity(dates)\n\n    frames = []\n    for date in dates:\n        # Slice to trips active on date\n        ids = a.loc[a[date] == 1, \"trip_id\"]\n        f = t[t[\"trip_id\"].isin(ids)].copy()\n        f[\"date\"] = date\n        # Groupby trip ID and sort groups by their minimum departure time.\n        # For some reason NaN departure times mess up the transform below.\n        # So temporarily fill NaN departure times as a workaround.\n        f[\"dt\"] = f[\"departure_time\"].fillna(method=\"ffill\")\n        f[\"min_dt\"] = f.groupby(\"trip_id\")[\"dt\"].transform(min)\n        frames.append(f)\n\n    f = pd.concat(frames)\n    return f.sort_values([\"date\", \"min_dt\", \"stop_sequence\"]).drop(\n        [\"min_dt\", \"dt\"], axis=1\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a GeoJSON representation of the route and optionally its stops.", "response": "def route_to_geojson(\n    feed: \"Feed\",\n    route_id: str,\n    date: Optional[str] = None,\n    *,\n    include_stops: bool = False,\n) -> Dict:\n    \"\"\"\n    Return a GeoJSON rendering of the route and, optionally, its stops.\n\n    Parameters\n    ----------\n    feed : Feed\n    route_id : string\n        ID of a route in ``feed.routes``\n    date : string\n        YYYYMMDD date string restricting the output to trips active\n        on the date\n    include_stops : boolean\n        If ``True``, then include stop features in the result\n\n    Returns\n    -------\n    dictionary\n        A decoded GeoJSON feature collection comprising a\n        LineString features of the distinct shapes of the trips on the\n        route.\n        If ``include_stops``, then include one Point feature for\n        each stop on the route.\n\n    \"\"\"\n    # Get set of unique trip shapes for route\n    shapes = (\n        feed.get_trips(date=date)\n        .loc[lambda x: x[\"route_id\"] == route_id, \"shape_id\"]\n        .unique()\n    )\n    if not shapes.size:\n        return {\"type\": \"FeatureCollection\", \"features\": []}\n\n    geom_by_shape = feed.build_geometry_by_shape(shape_ids=shapes)\n\n    # Get route properties\n    route = (\n        feed.get_routes(date=date)\n        .loc[lambda x: x[\"route_id\"] == route_id]\n        .fillna(\"n/a\")\n        .to_dict(orient=\"records\", into=OrderedDict)\n    )[0]\n\n    # Build route shape features\n    features = [\n        {\n            \"type\": \"Feature\",\n            \"properties\": route,\n            \"geometry\": sg.mapping(sg.LineString(geom)),\n        }\n        for geom in geom_by_shape.values()\n    ]\n\n    # Build stop features if desired\n    if include_stops:\n        stops = (\n            feed.get_stops(route_id=route_id)\n            .fillna(\"n/a\")\n            .to_dict(orient=\"records\", into=OrderedDict)\n        )\n        features.extend(\n            [\n                {\n                    \"type\": \"Feature\",\n                    \"geometry\": {\n                        \"type\": \"Point\",\n                        \"coordinates\": [stop[\"stop_lon\"], stop[\"stop_lat\"]],\n                    },\n                    \"properties\": stop,\n                }\n                for stop in stops\n            ]\n        )\n\n    return {\"type\": \"FeatureCollection\", \"features\": features}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Folium map showing the given routes and their stops.", "response": "def map_routes(\n    feed: \"Feed\",\n    route_ids: List[str],\n    date: Optional[str] = None,\n    color_palette: List[str] = cs.COLORS_SET2,\n    *,\n    include_stops: bool = True,\n):\n    \"\"\"\n    Return a Folium map showing the given routes and (optionally)\n    their stops.\n\n    Parameters\n    ----------\n    feed : Feed\n    route_ids : list\n        IDs of routes in ``feed.routes``\n    date : string\n        YYYYMMDD date string restricting the output to trips active\n        on the date\n    color_palette : list\n        Palette to use to color the routes. If more routes than colors,\n        then colors will be recycled.\n    include_stops : boolean\n        If ``True``, then include stops in the map\n\n    Returns\n    -------\n    dictionary\n        A Folium Map depicting the distinct shapes of the trips on\n        each route.\n        If ``include_stops``, then include the stops for each route.\n\n    Notes\n    ------\n    - Requires Folium\n\n    \"\"\"\n    import folium as fl\n\n    # Get routes slice and convert to dictionary\n    routes = (\n        feed.routes.loc[lambda x: x[\"route_id\"].isin(route_ids)]\n        .fillna(\"n/a\")\n        .to_dict(orient=\"records\")\n    )\n\n    # Create route colors\n    n = len(routes)\n    colors = [color_palette[i % len(color_palette)] for i in range(n)]\n\n    # Initialize map\n    my_map = fl.Map(tiles=\"cartodbpositron\")\n\n    # Collect route bounding boxes to set map zoom later\n    bboxes = []\n\n    # Create a feature group for each route and add it to the map\n    for i, route in enumerate(routes):\n        collection = feed.route_to_geojson(\n            route_id=route[\"route_id\"], date=date, include_stops=include_stops\n        )\n        group = fl.FeatureGroup(name=\"Route \" + route[\"route_short_name\"])\n        color = colors[i]\n\n        for f in collection[\"features\"]:\n            prop = f[\"properties\"]\n\n            # Add stop\n            if f[\"geometry\"][\"type\"] == \"Point\":\n                lon, lat = f[\"geometry\"][\"coordinates\"]\n                fl.CircleMarker(\n                    location=[lat, lon],\n                    radius=8,\n                    fill=True,\n                    color=color,\n                    weight=1,\n                    popup=fl.Popup(hp.make_html(prop)),\n                ).add_to(group)\n\n            # Add path\n            else:\n                prop[\"color\"] = color\n                path = fl.GeoJson(\n                    f,\n                    name=route,\n                    style_function=lambda x: {\n                        \"color\": x[\"properties\"][\"color\"]\n                    },\n                )\n                path.add_child(fl.Popup(hp.make_html(prop)))\n                path.add_to(group)\n                bboxes.append(sg.box(*sg.shape(f[\"geometry\"]).bounds))\n\n        group.add_to(my_map)\n\n    fl.LayerControl().add_to(my_map)\n\n    # Fit map to bounds\n    bounds = so.unary_union(bboxes).bounds\n    bounds2 = [bounds[1::-1], bounds[3:1:-1]]  # Folium expects this ordering\n    my_map.fit_bounds(bounds2)\n\n    return my_map"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_geometry_by_shape(\n    feed: \"Feed\",\n    shape_ids: Optional[List[str]] = None,\n    *,\n    use_utm: bool = False,\n) -> Dict:\n    \"\"\"\n    Return a dictionary with structure shape_id -> Shapely LineString\n    of shape.\n\n    Parameters\n    ----------\n    feed : Feed\n    shape_ids : list\n        IDs of shapes in ``feed.shapes`` to restrict output to; return\n        all shapes if ``None``.\n    use_utm : boolean\n        If ``True``, then use local UTM coordinates; otherwise, use\n        WGS84 coordinates\n\n    Returns\n    -------\n    dictionary\n        Has the structure\n        shape_id -> Shapely LineString of shape.\n        If ``feed.shapes is None``, then return ``None``.\n\n        Return the empty dictionary if ``feed.shapes is None``.\n\n    \"\"\"\n    if feed.shapes is None:\n        return {}\n\n    # Note the output for conversion to UTM with the utm package:\n    # >>> u = utm.from_latlon(47.9941214, 7.8509671)\n    # >>> print u\n    # (414278, 5316285, 32, 'T')\n    d = {}\n    shapes = feed.shapes.copy()\n    if shape_ids is not None:\n        shapes = shapes[shapes[\"shape_id\"].isin(shape_ids)]\n\n    if use_utm:\n        for shape, group in shapes.groupby(\"shape_id\"):\n            lons = group[\"shape_pt_lon\"].values\n            lats = group[\"shape_pt_lat\"].values\n            xys = [\n                utm.from_latlon(lat, lon)[:2] for lat, lon in zip(lats, lons)\n            ]\n            d[shape] = sg.LineString(xys)\n    else:\n        for shape, group in shapes.groupby(\"shape_id\"):\n            lons = group[\"shape_pt_lon\"].values\n            lats = group[\"shape_pt_lat\"].values\n            lonlats = zip(lons, lats)\n            d[shape] = sg.LineString(lonlats)\n    return d", "response": "Builds a dictionary with structure shape_id - > Shapely LineString of shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shapes_to_geojson(\n    feed: \"Feed\", shape_ids: Optional[List[str]] = None\n) -> Dict:\n    \"\"\"\n    Return a (decoded) GeoJSON FeatureCollection of LineString features\n    representing ``feed.shapes``.\n    Each feature will have a ``shape_id`` property.\n    The coordinates reference system is the default one for GeoJSON,\n    namely WGS84.\n\n    If a list of shape IDs is given, then return only the LineString\n    features corresponding to those shape IDS.\n    Return the empty dictionary if ``feed.shapes is None``\n    \"\"\"\n    geometry_by_shape = feed.build_geometry_by_shape(shape_ids=shape_ids)\n    if geometry_by_shape:\n        fc = {\n            \"type\": \"FeatureCollection\",\n            \"features\": [\n                {\n                    \"properties\": {\"shape_id\": shape},\n                    \"type\": \"Feature\",\n                    \"geometry\": sg.mapping(linestring),\n                }\n                for shape, linestring in geometry_by_shape.items()\n            ],\n        }\n    else:\n        fc = {}\n    return fc", "response": "Returns a ( decoded ) GeoJSON FeatureCollection of LineString features containing the shapes of the given ones."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_shapes_intersecting_geometry(\n    feed: \"Feed\", geometry, geo_shapes=None, *, geometrized: bool = False\n) -> DataFrame:\n    \"\"\"\n    Return the slice of ``feed.shapes`` that contains all shapes that\n    intersect the given Shapely geometry, e.g. a Polygon or LineString.\n\n    Parameters\n    ----------\n    feed : Feed\n    geometry : Shapley geometry, e.g. a Polygon\n        Specified in WGS84 coordinates\n    geo_shapes : GeoPandas GeoDataFrame\n        The output of :func:`geometrize_shapes`\n    geometrize : boolean\n        If ``True``, then return the shapes DataFrame as a GeoDataFrame\n        of the form output by :func:`geometrize_shapes`\n\n    Returns\n    -------\n    DataFrame or GeoDataFrame\n\n    Notes\n    -----\n    - Requires GeoPandas\n    - Specifying ``geo_shapes`` will skip the first step of the\n      algorithm, namely, geometrizing ``feed.shapes``\n    - Assume the following feed attributes are not ``None``:\n\n        * ``feed.shapes``, if ``geo_shapes`` is not given\n\n    \"\"\"\n    if geo_shapes is not None:\n        f = geo_shapes.copy()\n    else:\n        f = geometrize_shapes(feed.shapes)\n\n    cols = f.columns\n    f[\"hit\"] = f[\"geometry\"].intersects(geometry)\n    f = f[f[\"hit\"]][cols]\n\n    if geometrized:\n        return f\n    else:\n        return ungeometrize_shapes(f)", "response": "Returns a slice of feed. shapes that contains all shapes that intersect the given Shapely geometry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append_dist_to_shapes(feed: \"Feed\") -> \"Feed\":\n    if feed.shapes is None:\n        raise ValueError(\n            \"This function requires the feed to have a shapes.txt file\"\n        )\n\n    feed = feed.copy()\n    f = feed.shapes\n    m_to_dist = hp.get_convert_dist(\"m\", feed.dist_units)\n\n    def compute_dist(group):\n        # Compute the distances of the stops along this trip\n        group = group.sort_values(\"shape_pt_sequence\")\n        shape = group[\"shape_id\"].iat[0]\n        if not isinstance(shape, str):\n            group[\"shape_dist_traveled\"] = np.nan\n            return group\n        points = [\n            sg.Point(utm.from_latlon(lat, lon)[:2])\n            for lon, lat in group[[\"shape_pt_lon\", \"shape_pt_lat\"]].values\n        ]\n        p_prev = points[0]\n        d = 0\n        distances = [0]\n        for p in points[1:]:\n            d += p.distance(p_prev)\n            distances.append(d)\n            p_prev = p\n        group[\"shape_dist_traveled\"] = distances\n        return group\n\n    g = f.groupby(\"shape_id\", group_keys=False).apply(compute_dist)\n    # Convert from meters\n    g[\"shape_dist_traveled\"] = g[\"shape_dist_traveled\"].map(m_to_dist)\n\n    feed.shapes = g\n    return feed", "response": "Append the shape_dist_traveled field to the feed. shapes. txt file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef geometrize_shapes(\n    shapes: DataFrame, *, use_utm: bool = False\n) -> DataFrame:\n    \"\"\"\n    Given a GTFS shapes DataFrame, convert it to a GeoPandas\n    GeoDataFrame and return the result.\n    The result has a ``'geometry'`` column of WGS84 LineStrings\n    instead of the columns ``'shape_pt_sequence'``, ``'shape_pt_lon'``,\n    ``'shape_pt_lat'``, and ``'shape_dist_traveled'``.\n    If ``use_utm``, then use local UTM coordinates for the geometries.\n\n    Notes\n    ------\n    Requires GeoPandas.\n    \"\"\"\n    import geopandas as gpd\n\n    f = shapes.copy().sort_values([\"shape_id\", \"shape_pt_sequence\"])\n\n    def my_agg(group):\n        d = {}\n        d[\"geometry\"] = sg.LineString(\n            group[[\"shape_pt_lon\", \"shape_pt_lat\"]].values\n        )\n        return pd.Series(d)\n\n    g = f.groupby(\"shape_id\").apply(my_agg).reset_index()\n    g = gpd.GeoDataFrame(g, crs=cs.WGS84)\n\n    if use_utm:\n        lat, lon = f.loc[0, [\"shape_pt_lat\", \"shape_pt_lon\"]].values\n        crs = hp.get_utm_crs(lat, lon)\n        g = g.to_crs(crs)\n\n    return g", "response": "Given a GTFS shapes DataFrame convert it to a GeoPandas\n    GeoDataFrame and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ungeometrize_shapes(geo_shapes) -> DataFrame:\n    geo_shapes = geo_shapes.to_crs(cs.WGS84)\n\n    F = []\n    for index, row in geo_shapes.iterrows():\n        F.extend(\n            [\n                [row[\"shape_id\"], i, x, y]\n                for i, (x, y) in enumerate(row[\"geometry\"].coords)\n            ]\n        )\n\n    return pd.DataFrame(\n        F,\n        columns=[\n            \"shape_id\",\n            \"shape_pt_sequence\",\n            \"shape_pt_lon\",\n            \"shape_pt_lat\",\n        ],\n    )", "response": "Returns a DataFrame that contains the ungeometrized shape tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a time string x to a number of seconds.", "response": "def timestr_to_seconds(\n    x: Union[dt.date, str], *, inverse: bool = False, mod24: bool = False\n) -> int:\n    \"\"\"\n    Given an HH:MM:SS time string ``x``, return the number of seconds\n    past midnight that it represents.\n    In keeping with GTFS standards, the hours entry may be greater than\n    23.\n    If ``mod24``, then return the number of seconds modulo ``24*3600``.\n    If ``inverse``, then do the inverse operation.\n    In this case, if ``mod24`` also, then first take the number of\n    seconds modulo ``24*3600``.\n    \"\"\"\n    if not inverse:\n        try:\n            hours, mins, seconds = x.split(\":\")\n            result = int(hours) * 3600 + int(mins) * 60 + int(seconds)\n            if mod24:\n                result %= 24 * 3600\n        except:\n            result = np.nan\n    else:\n        try:\n            seconds = int(x)\n            if mod24:\n                seconds %= 24 * 3600\n            hours, remainder = divmod(seconds, 3600)\n            mins, secs = divmod(remainder, 60)\n            result = f\"{hours:02d}:{mins:02d}:{secs:02d}\"\n        except:\n            result = np.nan\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a GTFS HH : MM : SS time string return a timestring in the same format but with the hours taken modulo 24.", "response": "def timestr_mod24(timestr: str) -> int:\n    \"\"\"\n    Given a GTFS HH:MM:SS time string, return a timestring in the same\n    format but with the hours taken modulo 24.\n    \"\"\"\n    try:\n        hours, mins, secs = [int(x) for x in timestr.split(\":\")]\n        hours %= 24\n        result = f\"{hours:02d}:{mins:02d}:{secs:02d}\"\n    except:\n        result = None\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef weekday_to_str(\n    weekday: Union[int, str], *, inverse: bool = False\n) -> Union[int, str]:\n    \"\"\"\n    Given a weekday number (integer in the range 0, 1, ..., 6),\n    return its corresponding weekday name as a lowercase string.\n    Here 0 -> 'monday', 1 -> 'tuesday', and so on.\n    If ``inverse``, then perform the inverse operation.\n    \"\"\"\n    s = [\n        \"monday\",\n        \"tuesday\",\n        \"wednesday\",\n        \"thursday\",\n        \"friday\",\n        \"saturday\",\n        \"sunday\",\n    ]\n    if not inverse:\n        try:\n            return s[weekday]\n        except:\n            return\n    else:\n        try:\n            return s.index(weekday)\n        except:\n            return", "response": "Given a weekday number returns its corresponding weekday name as a lowercase string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_segment_length(\n    linestring: LineString, p: Point, q: Optional[Point] = None\n) -> float:\n    \"\"\"\n    Given a Shapely linestring and two Shapely points,\n    project the points onto the linestring, and return the distance\n    along the linestring between the two points.\n    If ``q is None``, then return the distance from the start of the\n    linestring to the projection of ``p``.\n    The distance is measured in the native coordinates of the linestring.\n    \"\"\"\n    # Get projected distances\n    d_p = linestring.project(p)\n    if q is not None:\n        d_q = linestring.project(q)\n        d = abs(d_p - d_q)\n    else:\n        d = d_p\n    return d", "response": "Given a Shapely linestring and two points project the points onto the linestring and return the distance along the linestring along the two points."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_max_runs(x) -> np.array:\n    # Get 0-1 array where 1 marks the max values of x\n    x = np.array(x)\n    m = np.max(x)\n    y = (x == m) * 1\n    # Bound y by zeros to detect runs properly\n    bounded = np.hstack(([0], y, [0]))\n    # Get 1 at run starts and -1 at run ends\n    diffs = np.diff(bounded)\n    run_starts = np.where(diffs > 0)[0]\n    run_ends = np.where(diffs < 0)[0]\n    return np.array([run_starts, run_ends]).T", "response": "Given a list of numbers return a NumPy array of pairs of start index end index + 1."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an increasing list of times as seconds past midnight and counts at those respective times return a pair of indices i j such that times[i] < times[j ]", "response": "def get_peak_indices(times: List, counts: List) -> np.array:\n    \"\"\"\n    Given an increasing list of times as seconds past midnight and a\n    list of trip counts at those respective times,\n    return a pair of indices i, j such that times[i] to times[j] is\n    the first longest time period such that for all i <= x < j,\n    counts[x] is the max of counts.\n    Assume times and counts have the same nonzero length.\n    \"\"\"\n    max_runs = get_max_runs(counts)\n\n    def get_duration(a):\n        return times[a[1]] - times[a[0]]\n\n    index = np.argmax(np.apply_along_axis(get_duration, 1, max_runs))\n    return max_runs[index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_convert_dist(\n    dist_units_in: str, dist_units_out: str\n) -> Callable[[float], float]:\n    \"\"\"\n    Return a function of the form\n\n      distance in the units ``dist_units_in`` ->\n      distance in the units ``dist_units_out``\n\n    Only supports distance units in :const:`constants.DIST_UNITS`.\n    \"\"\"\n    di, do = dist_units_in, dist_units_out\n    DU = cs.DIST_UNITS\n    if not (di in DU and do in DU):\n        raise ValueError(f\"Distance units must lie in {DU}\")\n\n    d = {\n        \"ft\": {\"ft\": 1, \"m\": 0.3048, \"mi\": 1 / 5280, \"km\": 0.0003048},\n        \"m\": {\"ft\": 1 / 0.3048, \"m\": 1, \"mi\": 1 / 1609.344, \"km\": 1 / 1000},\n        \"mi\": {\"ft\": 5280, \"m\": 1609.344, \"mi\": 1, \"km\": 1.609344},\n        \"km\": {\"ft\": 1 / 0.0003048, \"m\": 1000, \"mi\": 1 / 1.609344, \"km\": 1},\n    }\n    return lambda x: d[di][do] * x", "response": "Returns a function that converts distance in the units dist_units_in - > dist_units_out - > dist_units_in - > dist_units_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef almost_equal(f: DataFrame, g: DataFrame) -> bool:\n    if f.empty or g.empty:\n        return f.equals(g)\n    else:\n        # Put in canonical order\n        F = (\n            f.sort_index(axis=1)\n            .sort_values(list(f.columns))\n            .reset_index(drop=True)\n        )\n        G = (\n            g.sort_index(axis=1)\n            .sort_values(list(g.columns))\n            .reset_index(drop=True)\n        )\n        return F.equals(G)", "response": "Return True if the given DataFrames are almost equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_not_null(df: DataFrame, col_name: str) -> bool:\n    if (\n        isinstance(df, pd.DataFrame)\n        and col_name in df.columns\n        and df[col_name].notnull().any()\n    ):\n        return True\n    else:\n        return False", "response": "Return True if the given DataFrame has a column of the given name and there is at least one non - NaN value in that column ; return False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_utm_crs(lat: float, lon: float) -> Dict:\n    zone = utm.from_latlon(lat, lon)[2]\n    south = lat < 0\n    return {\n        \"proj\": \"utm\",\n        \"zone\": zone,\n        \"south\": south,\n        \"ellps\": \"WGS84\",\n        \"datum\": \"WGS84\",\n        \"units\": \"m\",\n        \"no_defs\": True,\n    }", "response": "Returns a dictionary containing the UTM projection corresponding to the given WGS84 longitude and latitude."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef linestring_to_utm(linestring: LineString) -> LineString:\n    proj = lambda x, y: utm.from_latlon(y, x)[:2]\n    return transform(proj, linestring)", "response": "Given a Shapely LineString in WGS84 coordinates convert it to the appropriate UTM coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_active_trips_df(trip_times: DataFrame) -> DataFrame:\n    active_trips = (\n        pd.concat(\n            [\n                pd.Series(1, trip_times.start_time),  # departed add 1\n                pd.Series(-1, trip_times.end_time),  # arrived subtract 1\n            ]\n        )\n        .groupby(level=0, sort=True)\n        .sum()\n        .cumsum()\n        .ffill()\n    )\n    return active_trips", "response": "Returns a DataFrame containing the number of active trips in the specified trip_times."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef combine_time_series(\n    time_series_dict: Dict, kind: str, *, split_directions: bool = False\n) -> DataFrame:\n    \"\"\"\n    Combine the many time series DataFrames in the given dictionary\n    into one time series DataFrame with hierarchical columns.\n\n    Parameters\n    ----------\n    time_series_dict : dictionary\n        Has the form string -> time series\n    kind : string\n        ``'route'`` or ``'stop'``\n    split_directions : boolean\n        If ``True``, then assume the original time series contains data\n        separated by trip direction; otherwise, assume not.\n        The separation is indicated by a suffix ``'-0'`` (direction 0)\n        or ``'-1'`` (direction 1) in the route ID or stop ID column\n        values.\n\n    Returns\n    -------\n    DataFrame\n        Columns are hierarchical (multi-index).\n        The top level columns are the keys of the dictionary and\n        the second level columns are ``'route_id'`` and\n        ``'direction_id'``, if ``kind == 'route'``, or 'stop_id' and\n        ``'direction_id'``, if ``kind == 'stop'``.\n        If ``split_directions``, then third column is\n        ``'direction_id'``; otherwise, there is no ``'direction_id'``\n        column.\n\n    \"\"\"\n    if kind not in [\"stop\", \"route\"]:\n        raise ValueError(\"kind must be 'stop' or 'route'\")\n\n    names = [\"indicator\"]\n    if kind == \"stop\":\n        names.append(\"stop_id\")\n    else:\n        names.append(\"route_id\")\n\n    if split_directions:\n        names.append(\"direction_id\")\n\n    def process_index(k):\n        a, b = k.rsplit(\"-\", 1)\n        return a, int(b)\n\n    frames = list(time_series_dict.values())\n    new_frames = []\n    if split_directions:\n        for f in frames:\n            ft = f.T\n            ft.index = pd.MultiIndex.from_tuples(\n                [process_index(k) for (k, __) in ft.iterrows()]\n            )\n            new_frames.append(ft.T)\n    else:\n        new_frames = frames\n    result = pd.concat(\n        new_frames, axis=1, keys=list(time_series_dict.keys()), names=names\n    )\n\n    return result", "response": "Combine the many time series DataFrames in the given dictionary into one time series DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef downsample(time_series: DataFrame, freq: str) -> DataFrame:\n\n    f = time_series.copy()\n\n    # Can't downsample to a shorter frequency\n    if f.empty or pd.tseries.frequencies.to_offset(freq) < f.index.freq:\n        return f\n\n    result = None\n    if \"stop_id\" in time_series.columns.names:\n        # It's a stops time series\n        result = f.resample(freq).sum()\n    else:\n        # It's a route or feed time series.\n        inds = [\n            \"num_trips\",\n            \"num_trip_starts\",\n            \"num_trip_ends\",\n            \"service_distance\",\n            \"service_duration\",\n        ]\n        frames = []\n\n        # Resample num_trips in a custom way that depends on\n        # num_trips and num_trip_ends\n        def agg_num_trips(group):\n            return (\n                group[\"num_trips\"].iloc[-1]\n                + group[\"num_trip_ends\"].iloc[:-1].sum()\n            )\n\n        num_trips = f.groupby(pd.Grouper(freq=freq)).apply(agg_num_trips)\n        frames.append(num_trips)\n\n        # Resample the rest of the indicators via summing\n        frames.extend([f[ind].resample(freq).agg(\"sum\") for ind in inds[1:]])\n\n        g = pd.concat(frames, axis=1, keys=inds)\n\n        # Calculate speed and add it to f. Can't resample it.\n        speed = g[\"service_distance\"] / g[\"service_duration\"]\n        speed = pd.concat({\"service_speed\": speed}, axis=1)\n        result = pd.concat([g, speed], axis=1)\n\n    # Reset column names and sort the hierarchical columns to allow slicing;\n    # see http://pandas.pydata.org/pandas-docs/stable/advanced.html#sorting-a-multiindex\n    result.columns.names = f.columns.names\n    result = result.sort_index(axis=1, sort_remaining=True)\n\n    return result", "response": "Downsamples the given time series to the given Pandas frequency string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a subset of a stop times DataFrame and a subset of a trips DataFrame return a DataFrame that provides summary stats about the stops in the inner join of the two DataFrames.", "response": "def compute_stop_stats_base(\n    stop_times_subset: DataFrame,\n    trip_subset: DataFrame,\n    headway_start_time: str = \"07:00:00\",\n    headway_end_time: str = \"19:00:00\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Given a subset of a stop times DataFrame and a subset of a trips\n    DataFrame, return a DataFrame that provides summary stats about the\n    stops in the inner join of the two DataFrames.\n\n    Parameters\n    ----------\n    stop_times_subset : DataFrame\n        A valid GTFS stop times table\n    trip_subset : DataFrame\n        A valid GTFS trips table\n    split_directions : boolean\n        If ``True``, then separate the stop stats by direction (0 or 1)\n        of the trips visiting the stops; otherwise aggregate trips\n        visiting from both directions\n    headway_start_time : string\n        HH:MM:SS time string indicating the start time for computing\n        headway stats\n    headway_end_time : string\n        HH:MM:SS time string indicating the end time for computing\n        headway stats\n\n    Returns\n    -------\n    DataFrame\n        The columns are\n\n        - stop_id\n        - direction_id: present if and only if ``split_directions``\n        - num_routes: number of routes visiting stop\n          (in the given direction)\n        - num_trips: number of trips visiting stop\n          (in the givin direction)\n        - max_headway: maximum of the durations (in minutes)\n          between trip departures at the stop between\n          ``headway_start_time`` and ``headway_end_time``\n        - min_headway: minimum of the durations (in minutes) mentioned\n          above\n        - mean_headway: mean of the durations (in minutes) mentioned\n          above\n        - start_time: earliest departure time of a trip from this stop\n        - end_time: latest departure time of a trip from this stop\n\n\n    Notes\n    -----\n    - If ``trip_subset`` is empty, then return an empty DataFrame.\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present.\n\n    \"\"\"\n    if trip_subset.empty:\n        return pd.DataFrame()\n\n    f = pd.merge(stop_times_subset, trip_subset)\n\n    # Convert departure times to seconds to ease headway calculations\n    f[\"departure_time\"] = f[\"departure_time\"].map(hp.timestr_to_seconds)\n\n    headway_start = hp.timestr_to_seconds(headway_start_time)\n    headway_end = hp.timestr_to_seconds(headway_end_time)\n\n    # Compute stats for each stop\n    def compute_stop_stats(group):\n        # Operate on the group of all stop times for an individual stop\n        d = OrderedDict()\n        d[\"num_routes\"] = group[\"route_id\"].unique().size\n        d[\"num_trips\"] = group.shape[0]\n        d[\"start_time\"] = group[\"departure_time\"].min()\n        d[\"end_time\"] = group[\"departure_time\"].max()\n        headways = []\n        dtimes = sorted(\n            [\n                dtime\n                for dtime in group[\"departure_time\"].values\n                if headway_start <= dtime <= headway_end\n            ]\n        )\n        headways.extend(\n            [dtimes[i + 1] - dtimes[i] for i in range(len(dtimes) - 1)]\n        )\n        if headways:\n            d[\"max_headway\"] = np.max(headways) / 60  # minutes\n            d[\"min_headway\"] = np.min(headways) / 60  # minutes\n            d[\"mean_headway\"] = np.mean(headways) / 60  # minutes\n        else:\n            d[\"max_headway\"] = np.nan\n            d[\"min_headway\"] = np.nan\n            d[\"mean_headway\"] = np.nan\n        return pd.Series(d)\n\n    if split_directions:\n        if \"direction_id\" not in f.columns:\n            f[\"direction_id\"] = np.nan\n        f = f.loc[lambda x: x.direction_id.notnull()].assign(\n            direction_id=lambda x: x.direction_id.astype(int)\n        )\n        if f.empty:\n            raise ValueError(\n                \"At least one trip direction ID value \" \"must be non-NaN.\"\n            )\n        g = f.groupby([\"stop_id\", \"direction_id\"])\n    else:\n        g = f.groupby(\"stop_id\")\n\n    result = g.apply(compute_stop_stats).reset_index()\n\n    # Convert start and end times to time strings\n    result[[\"start_time\", \"end_time\"]] = result[\n        [\"start_time\", \"end_time\"]\n    ].applymap(lambda x: hp.timestr_to_seconds(x, inverse=True))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a subset of a stop times DataFrame and a subset of a trips DataFrame, return a DataFrame that provides a summary time series about the stops in the inner join of the two DataFrames. Parameters ---------- stop_times_subset : DataFrame A valid GTFS stop times table trip_subset : DataFrame A valid GTFS trips table split_directions : boolean If ``True``, then separate each stop's stats by trip direction; otherwise aggregate trips visiting from both directions freq : Pandas frequency string Specifices the frequency with which to resample the time series; max frequency is one minute ('Min') date_label : string YYYYMMDD date string used as the date in the time series index Returns ------- DataFrame A time series with a timestamp index for a 24-hour period sampled at the given frequency. The only indicator variable for each stop is - ``num_trips``: the number of trips that visit the stop and have a nonnull departure time from the stop The maximum allowable frequency is 1 minute. The columns are hierarchical (multi-indexed) with - top level: name = 'indicator', values = ['num_trips'] - middle level: name = 'stop_id', values = the active stop IDs - bottom level: name = 'direction_id', values = 0s and 1s If not ``split_directions``, then don't include the bottom level. Notes ----- - The time series is computed at a one-minute frequency, then resampled at the end to the given frequency - Stop times with null departure times are ignored, so the aggregate of ``num_trips`` across the day could be less than the ``num_trips`` column in :func:`compute_stop_stats_base` - All trip departure times are taken modulo 24 hours, so routes with trips that end past 23:59:59 will have all their stats wrap around to the early morning of the time series. - 'num_trips' should be resampled with ``how=np.sum`` - If ``trip_subset`` is empty, then return an empty DataFrame - Raise a ValueError if ``split_directions`` and no non-NaN direction ID values present", "response": "def compute_stop_time_series_base(\n    stop_times_subset: DataFrame,\n    trip_subset: DataFrame,\n    freq: str = \"5Min\",\n    date_label: str = \"20010101\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Given a subset of a stop times DataFrame and a subset of a trips\n    DataFrame, return a DataFrame that provides a summary time series\n    about the stops in the inner join of the two DataFrames.\n\n    Parameters\n    ----------\n    stop_times_subset : DataFrame\n        A valid GTFS stop times table\n    trip_subset : DataFrame\n        A valid GTFS trips table\n    split_directions : boolean\n        If ``True``, then separate each stop's stats by trip direction;\n        otherwise aggregate trips visiting from both directions\n    freq : Pandas frequency string\n        Specifices the frequency with which to resample the time series;\n        max frequency is one minute ('Min')\n    date_label : string\n        YYYYMMDD date string used as the date in the time series index\n\n    Returns\n    -------\n    DataFrame\n        A time series with a timestamp index for a 24-hour period\n        sampled at the given frequency.\n        The only indicator variable for each stop is\n\n        - ``num_trips``: the number of trips that visit the stop and\n          have a nonnull departure time from the stop\n\n        The maximum allowable frequency is 1 minute.\n\n        The columns are hierarchical (multi-indexed) with\n\n        - top level: name = 'indicator', values = ['num_trips']\n        - middle level: name = 'stop_id', values = the active stop IDs\n        - bottom level: name = 'direction_id', values = 0s and 1s\n\n        If not ``split_directions``, then don't include the bottom level.\n\n    Notes\n    -----\n    - The time series is computed at a one-minute frequency, then\n      resampled at the end to the given frequency\n    - Stop times with null departure times are ignored, so the aggregate\n      of ``num_trips`` across the day could be less than the\n      ``num_trips`` column in :func:`compute_stop_stats_base`\n    - All trip departure times are taken modulo 24 hours,\n      so routes with trips that end past 23:59:59 will have all\n      their stats wrap around to the early morning of the time series.\n    - 'num_trips' should be resampled with ``how=np.sum``\n    - If ``trip_subset`` is empty, then return an empty DataFrame\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    if trip_subset.empty:\n        return pd.DataFrame()\n\n    f = pd.merge(stop_times_subset, trip_subset)\n\n    if split_directions:\n        if \"direction_id\" not in f.columns:\n            f[\"direction_id\"] = np.nan\n        f = f.loc[lambda x: x.direction_id.notnull()].assign(\n            direction_id=lambda x: x.direction_id.astype(int)\n        )\n        if f.empty:\n            raise ValueError(\n                \"At least one trip direction ID value \" \"must be non-NaN.\"\n            )\n\n        # Alter stop IDs to encode trip direction:\n        # <stop ID>-0 and <stop ID>-1\n        f[\"stop_id\"] = f[\"stop_id\"] + \"-\" + f[\"direction_id\"].map(str)\n    stops = f[\"stop_id\"].unique()\n\n    # Bin each stop departure time\n    bins = [i for i in range(24 * 60)]  # One bin for each minute\n    num_bins = len(bins)\n\n    def F(x):\n        return (hp.timestr_to_seconds(x) // 60) % (24 * 60)\n\n    f[\"departure_index\"] = f[\"departure_time\"].map(F)\n\n    # Create one time series for each stop\n    series_by_stop = {stop: [0 for i in range(num_bins)] for stop in stops}\n\n    for stop, group in f.groupby(\"stop_id\"):\n        counts = Counter((bin, 0) for bin in bins) + Counter(\n            group[\"departure_index\"].values\n        )\n        series_by_stop[stop] = [counts[bin] for bin in bins]\n\n    # Combine lists into dictionary of form indicator -> time series.\n    # Only one indicator in this case, but could add more\n    # in the future as was done with route time series.\n    rng = pd.date_range(date_label, periods=24 * 60, freq=\"Min\")\n    series_by_indicator = {\n        \"num_trips\": pd.DataFrame(series_by_stop, index=rng).fillna(0)\n    }\n\n    # Combine all time series into one time series\n    g = hp.combine_time_series(\n        series_by_indicator, kind=\"stop\", split_directions=split_directions\n    )\n    return hp.downsample(g, freq=freq)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_stops(\n    feed: \"Feed\",\n    date: Optional[str] = None,\n    trip_id: Optional[str] = None,\n    route_id: Optional[str] = None,\n    *,\n    in_stations: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Return a section of ``feed.stops``.\n\n    Parameters\n    -----------\n    feed : Feed\n    date : string\n        YYYYMMDD string; restricts the output to stops active\n        (visited by trips) on the date\n    trip_id : string\n        ID of a trip in ``feed.trips``; restricts output to stops\n        visited by the trip\n    route_id : string\n        ID of route in ``feed.routes``; restricts output to stops\n        visited by the route\n    in_stations : boolean\n        If ``True``, then restricts output to stops in stations if\n        station data is available in ``feed.stops``\n\n    Returns\n    -------\n    DataFrame\n        A subset of ``feed.stops`` defined by the parameters above\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.stops``\n    - Those used in :func:`.stop_times.get_stop_times`\n\n    \"\"\"\n    s = feed.stops.copy()\n    if date is not None:\n        A = feed.get_stop_times(date)[\"stop_id\"]\n        s = s[s[\"stop_id\"].isin(A)].copy()\n    if trip_id is not None:\n        st = feed.stop_times.copy()\n        B = st[st[\"trip_id\"] == trip_id][\"stop_id\"]\n        s = s[s[\"stop_id\"].isin(B)].copy()\n    elif route_id is not None:\n        A = feed.trips[feed.trips[\"route_id\"] == route_id][\"trip_id\"]\n        st = feed.stop_times.copy()\n        B = st[st[\"trip_id\"].isin(A)][\"stop_id\"]\n        s = s[s[\"stop_id\"].isin(B)].copy()\n    if in_stations and set([\"location_type\", \"parent_station\"]) <= set(\n        s.columns\n    ):\n        s = s[(s[\"location_type\"] != 1) & (s[\"parent_station\"].notnull())]\n\n    return s", "response": "Returns a subset of feed. stops that are visited by a trip or route."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_geometry_by_stop(\n    feed: \"Feed\",\n    stop_ids: Optional[List[str]] = None,\n    *,\n    use_utm: bool = False,\n) -> Dict:\n    \"\"\"\n    Return a dictionary with the structure\n    stop_id -> Shapely Point with coordinates of the stop.\n\n    Parameters\n    ----------\n    feed : Feed\n    use_utm : boolean\n        If ``True``, then return each point in UTM coordinates\n        appropriate to the region; otherwise use the default WGS84\n        coordinates\n    stop_ids : list\n        Stop IDs (strings) from ``feed.stops`` to restrict output to\n\n    Returns\n    -------\n    dictionary\n        Each key is a stop ID and each value is a Shapely Point with\n        coordinates of the stop\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.stops``\n\n    \"\"\"\n    d = {}\n    stops = feed.stops.copy()\n    if stop_ids is not None:\n        stops = stops[stops[\"stop_id\"].isin(stop_ids)]\n\n    stops = stops[stops.stop_lat.notna() & stops.stop_lon.notna()]\n\n    if use_utm:\n        for stop, group in stops.groupby(\"stop_id\"):\n            lat, lon = group[[\"stop_lat\", \"stop_lon\"]].values[0]\n            d[stop] = sg.Point(utm.from_latlon(lat, lon)[:2])\n    else:\n        for stop, group in stops.groupby(\"stop_id\"):\n            lat, lon = group[[\"stop_lat\", \"stop_lon\"]].values[0]\n            d[stop] = sg.Point([lon, lat])\n    return d", "response": "Build a dictionary with the structure\n    stop_id - Stop ID of the stop in the feed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the stop activity for a given list of dates.", "response": "def compute_stop_activity(feed: \"Feed\", dates: List[str]) -> DataFrame:\n    \"\"\"\n    Mark stops as active or inactive on the given dates.\n    A stop is *active* on a given date if some trips that starts on the\n    date visits the stop (possibly after midnight).\n\n    Parameters\n    ----------\n    feed : Feed\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute activity\n\n    Returns\n    -------\n    DataFrame\n        Columns are\n\n        - stop_id\n        - ``dates[0]``: 1 if the stop has at least one trip visiting it\n          on ``dates[0]``; 0 otherwise\n        - ``dates[1]``: 1 if the stop has at least one trip visiting it\n          on ``dates[1]``; 0 otherwise\n        - etc.\n        - ``dates[-1]``: 1 if the stop has at least one trip visiting it\n          on ``dates[-1]``; 0 otherwise\n\n    Notes\n    -----\n    - If all dates lie outside the Feed period, then return an empty\n      DataFrame\n    - Assume the following feed attributes are not ``None``:\n\n        * ``feed.stop_times``\n        * Those used in :func:`.trips.compute_trip_activity`\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    trip_activity = feed.compute_trip_activity(dates)\n    g = pd.merge(trip_activity, feed.stop_times).groupby(\"stop_id\")\n    # Pandas won't allow me to simply return g[dates].max().reset_index().\n    # I get ``TypeError: unorderable types: datetime.date() < str()``.\n    # So here's a workaround.\n    for (i, date) in enumerate(dates):\n        if i == 0:\n            f = g[date].max().reset_index()\n        else:\n            f = f.merge(g[date].max().reset_index())\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute stats for all stops in the given dates.", "response": "def compute_stop_stats(\n    feed: \"Feed\",\n    dates: List[str],\n    stop_ids: Optional[List[str]] = None,\n    headway_start_time: str = \"07:00:00\",\n    headway_end_time: str = \"19:00:00\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Compute stats for all stops for the given dates.\n    Optionally, restrict to the stop IDs given.\n\n    Parameters\n    ----------\n    feed : Feed\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute stats\n    stop_ids : list\n        Optional list of stop IDs to restrict stats to\n    headway_start_time : string\n        HH:MM:SS time string indicating the start time for computing\n        headway stats\n    headway_end_time : string\n        HH:MM:SS time string indicating the end time for computing\n        headway stats\n    split_directions : boolean\n        If ``True``, then separate the stop stats by direction (0 or 1)\n        of the trips visiting the stops; otherwise aggregate trips\n        visiting from both directions\n\n    Returns\n    -------\n    DataFrame\n        Columns are\n\n        - ``'date'``\n        - ``'stop_id'``\n        - ``'direction_id'``: present if and only if ``split_directions``\n        - ``'num_routes'``: number of routes visiting the stop\n          (in the given direction) on the date\n        - ``'num_trips'``: number of trips visiting stop\n          (in the givin direction) on the date\n        - ``'max_headway'``: maximum of the durations (in minutes)\n          between trip departures at the stop between\n          ``headway_start_time`` and ``headway_end_time`` on the date\n        - ``'min_headway'``: minimum of the durations (in minutes) mentioned\n          above\n        - ``'mean_headway'``: mean of the durations (in minutes) mentioned\n          above\n        - ``'start_time'``: earliest departure time of a trip from this stop\n          on the date\n        - ``'end_time'``: latest departure time of a trip from this stop on\n          the date\n\n        Dates with no trip activity will have null stats.\n        Exclude dates that lie outside of the Feed's date range.\n        If all the dates given lie outside of the Feed's date range,\n        then return an empty DataFrame.\n\n    Notes\n    -----\n    - Assume the following feed attributes are not ``None``:\n        * ``feed.stop_times``\n        * Those used in :func:`.trips.get_trips`\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    # Restrict stop times to stop IDs if specified\n    if stop_ids is not None:\n        stop_times_subset = feed.stop_times.loc[\n            lambda x: x[\"stop_id\"].isin(stop_ids)\n        ].copy()\n    else:\n        stop_times_subset = feed.stop_times.copy()\n\n    activity = feed.compute_trip_activity(dates)\n\n    # Collect stats for each date, memoizing stats by trip ID sequence\n    # to avoid unnecessary recomputations.\n    # Store in dictionary of the form\n    # trip ID sequence ->\n    # [stats DataFarme, date list that stats apply]\n    stats_and_dates_by_ids = {}\n    cols = [\n        \"stop_id\",\n        \"num_routes\",\n        \"num_trips\",\n        \"max_headway\",\n        \"min_headway\",\n        \"mean_headway\",\n        \"start_time\",\n        \"end_time\",\n    ]\n    if split_directions:\n        cols.append(\"direction_id\")\n    null_stats = pd.DataFrame({c: np.nan for c in cols}, index=[0])\n    for date in dates:\n        ids = tuple(activity.loc[activity[date] > 0, \"trip_id\"])\n        if ids in stats_and_dates_by_ids:\n            # Append date to date list\n            stats_and_dates_by_ids[ids][1].append(date)\n        elif not ids:\n            # Null stats\n            stats_and_dates_by_ids[ids] = [null_stats, [date]]\n        else:\n            # Compute stats\n            t = feed.trips\n            trips = t[t[\"trip_id\"].isin(ids)].copy()\n            stats = compute_stop_stats_base(\n                stop_times_subset,\n                trips,\n                split_directions=split_directions,\n                headway_start_time=headway_start_time,\n                headway_end_time=headway_end_time,\n            )\n\n            # Remember stats\n            stats_and_dates_by_ids[ids] = [stats, [date]]\n\n    # Assemble stats into DataFrame\n    frames = []\n    for stats, dates_ in stats_and_dates_by_ids.values():\n        for date in dates_:\n            f = stats.copy()\n            f[\"date\"] = date\n            frames.append(f)\n    f = (\n        pd.concat(frames)\n        .sort_values([\"date\", \"stop_id\"])\n        .reset_index(drop=True)\n    )\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a DataFrame with the same index and hierarchical columns as output by compute_stop_time_series_base.", "response": "def build_null_stop_time_series(\n    feed: \"Feed\",\n    date_label: str = \"20010101\",\n    freq: str = \"5Min\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Return a stop time series with the same index and hierarchical columns\n    as output by :func:`compute_stop_time_series_base`,\n    but fill it full of null values.\n    \"\"\"\n    start = date_label\n    end = pd.to_datetime(date_label + \" 23:59:00\")\n    rng = pd.date_range(start, end, freq=freq)\n    inds = [\"num_trips\"]\n    sids = feed.stops.stop_id\n    if split_directions:\n        product = [inds, sids, [0, 1]]\n        names = [\"indicator\", \"stop_id\", \"direction_id\"]\n    else:\n        product = [inds, sids]\n        names = [\"indicator\", \"stop_id\"]\n    cols = pd.MultiIndex.from_product(product, names=names)\n    return pd.DataFrame([], index=rng, columns=cols).sort_index(\n        axis=1, sort_remaining=True\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_stop_time_series(\n    feed: \"Feed\",\n    dates: List[str],\n    stop_ids: Optional[List[str]] = None,\n    freq: str = \"5Min\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Compute time series for the stops on the given dates at the\n    given frequency and return the result as a DataFrame of the same\n    form as output by :func:`.stop_times.compute_stop_time_series_base`.\n    Optionally restrict to stops in the given list of stop IDs.\n\n    Parameters\n    ----------\n    feed : Feed\n    dates : string or list\n        A YYYYMMDD date string or list thereof indicating the date(s)\n        for which to compute stats\n    stop_ids : list\n        Optional list of stop IDs to restrict to\n    split_directions : boolean\n        If ``True``, then separate the stop stats by direction (0 or 1)\n        of the trips visiting the stops; otherwise aggregate trips\n        visiting from both directions\n    freq : Pandas frequency string\n        Specifices the frequency with which to resample the time series;\n        max frequency is one minute ('Min')\n\n    Returns\n    -------\n    DataFrame\n        A time series with a timestamp index across the given dates\n        sampled at the given frequency.\n        The maximum allowable frequency is 1 minute.\n\n        The columns are the same as in\n        :func:`compute_stop_time_series_base`.\n\n        Exclude dates that lie outside of the Feed's date range.\n        If all dates lie outside the Feed's date range, then return an\n        empty DataFrame\n\n    Notes\n    -----\n    - See the notes for :func:`compute_stop_time_series_base`\n    - Assume the following feed attributes are not ``None``:\n\n        * ``feed.stop_times``\n        * Those used in :func:`.trips.get_trips`\n\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    activity = feed.compute_trip_activity(dates)\n\n    # Restrict stop times to stop IDs if specified\n    if stop_ids is not None:\n        stop_times_subset = feed.stop_times.loc[\n            lambda x: x[\"stop_id\"].isin(stop_ids)\n        ].copy()\n    else:\n        stop_times_subset = feed.stop_times.copy()\n\n    # Collect stats for each date, memoizing stats by trip ID sequence\n    # to avoid unnecessary recomputations.\n    # Store in dictionary of the form\n    # trip ID sequence ->\n    # [stats DataFarme, date list that stats apply]\n    stats_and_dates_by_ids = {}\n    null_stats = build_null_stop_time_series(\n        feed, split_directions=split_directions, freq=freq\n    )\n    for date in dates:\n        ids = tuple(activity.loc[activity[date] > 0, \"trip_id\"])\n        if ids in stats_and_dates_by_ids:\n            # Append date to date list\n            stats_and_dates_by_ids[ids][1].append(date)\n        elif not ids:\n            # Null stats\n            stats_and_dates_by_ids[ids] = [null_stats, [date]]\n        else:\n            # Compute stats\n            t = feed.trips\n            trips = t[t[\"trip_id\"].isin(ids)].copy()\n            stats = compute_stop_time_series_base(\n                stop_times_subset,\n                trips,\n                split_directions=split_directions,\n                freq=freq,\n                date_label=date,\n            )\n\n            # Remember stats\n            stats_and_dates_by_ids[ids] = [stats, [date]]\n\n    # Assemble stats into DataFrame\n    frames = []\n    for stats, dates_ in stats_and_dates_by_ids.values():\n        for date in dates_:\n            f = stats.copy()\n            # Replace date\n            d = hp.datestr_to_date(date)\n            f.index = f.index.map(\n                lambda t: t.replace(year=d.year, month=d.month, day=d.day)\n            )\n            frames.append(f)\n\n    f = pd.concat(frames).sort_index().sort_index(axis=1, sort_remaining=True)\n\n    if len(dates) > 1:\n        # Insert missing dates and NaNs to complete series index\n        end_datetime = pd.to_datetime(dates[-1] + \" 23:59:59\")\n        new_index = pd.date_range(f.index[0], end_datetime, freq=freq)\n        f = f.reindex(new_index)\n    else:\n        # Set frequency\n        f.index.freq = pd.tseries.frequencies.to_offset(freq)\n\n    return f", "response": "Compute the time series for the given dates at the given frequency and return the result as a DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a DataFrame containing the timetable for the given stop ID and list of dates.", "response": "def build_stop_timetable(\n    feed: \"Feed\", stop_id: str, dates: List[str]\n) -> DataFrame:\n    \"\"\"\n    Return a DataFrame containing the timetable for the given stop ID\n    and dates.\n\n    Parameters\n    ----------\n    feed : Feed\n    stop_id : string\n        ID of the stop for which to build the timetable\n    dates : string or list\n        A YYYYMMDD date string or list thereof\n\n    Returns\n    -------\n    DataFrame\n        The columns are all those in ``feed.trips`` plus those in\n        ``feed.stop_times`` plus ``'date'``, and the stop IDs are\n        restricted to the given stop ID.\n        The result is sorted by date then departure time.\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.trips``\n    - Those used in :func:`.stop_times.get_stop_times`\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    t = pd.merge(feed.trips, feed.stop_times)\n    t = t[t[\"stop_id\"] == stop_id].copy()\n    a = feed.compute_trip_activity(dates)\n\n    frames = []\n    for date in dates:\n        # Slice to stops active on date\n        ids = a.loc[a[date] == 1, \"trip_id\"]\n        f = t[t[\"trip_id\"].isin(ids)].copy()\n        f[\"date\"] = date\n        frames.append(f)\n\n    f = pd.concat(frames)\n    return f.sort_values([\"date\", \"departure_time\"])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a slice of feed. stops that lie within the given Shapely Polygon object.", "response": "def get_stops_in_polygon(\n    feed: \"Feed\", polygon: Polygon, geo_stops=None\n) -> DataFrame:\n    \"\"\"\n    Return the slice of ``feed.stops`` that contains all stops that lie\n    within the given Shapely Polygon object that is specified in\n    WGS84 coordinates.\n\n    Parameters\n    ----------\n    feed : Feed\n    polygon : Shapely Polygon\n        Specified in WGS84 coordinates\n    geo_stops : Geopandas GeoDataFrame\n        A geographic version of ``feed.stops`` which will be computed\n        if not given.\n        Specify this parameter in batch jobs to avoid unnecessary\n        computation.\n\n    Returns\n    -------\n    DataFrame\n        Subset of ``feed.stops``\n\n    Notes\n    -----\n    - Requires GeoPandas\n    - Assume the following feed attributes are not ``None``:\n\n        * ``feed.stops``, if ``geo_stops`` is not given\n\n    \"\"\"\n    if geo_stops is not None:\n        f = geo_stops.copy()\n    else:\n        f = geometrize_stops(feed.stops)\n\n    cols = f.columns\n    f[\"hit\"] = f[\"geometry\"].within(polygon)\n    f = f[f[\"hit\"]][cols]\n    return ungeometrize_stops(f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef geometrize_stops(stops: List[str], *, use_utm: bool = False) -> DataFrame:\n    import geopandas as gpd\n\n    g = (\n        stops.assign(\n            geometry=lambda x: [\n                sg.Point(p) for p in x[[\"stop_lon\", \"stop_lat\"]].values\n            ]\n        )\n        .drop([\"stop_lon\", \"stop_lat\"], axis=1)\n        .pipe(lambda x: gpd.GeoDataFrame(x, crs=cs.WGS84))\n    )\n\n    if use_utm:\n        lat, lon = stops.loc[0, [\"stop_lat\", \"stop_lon\"]].values\n        crs = hp.get_utm_crs(lat, lon)\n        g = g.to_crs(crs)\n\n    return g", "response": "Given a stops DataFrame convert it to a GeoPandas GeoDataFrame\n    and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ungeometrize_stops(geo_stops: DataFrame) -> DataFrame:\n    f = geo_stops.copy().to_crs(cs.WGS84)\n    f[\"stop_lon\"], f[\"stop_lat\"] = zip(\n        *f[\"geometry\"].map(lambda p: [p.x, p.y])\n    )\n    del f[\"geometry\"]\n    return f", "response": "Returns a copy of the given GeoDataFrame with the ungeometrized stops."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Folium map showing the given stops.", "response": "def map_stops(\n    feed: \"Feed\", stop_ids: List[str], stop_style: Dict = STOP_STYLE\n):\n    \"\"\"\n    Return a Folium map showing the given stops.\n\n    Parameters\n    ----------\n    feed : Feed\n    stop_ids : list\n        IDs of trips in ``feed.stops``\n    stop_style: dictionary\n        Folium CircleMarker parameters to use for styling stops.\n\n    Returns\n    -------\n    dictionary\n        A Folium Map depicting the stops as CircleMarkers.\n\n    Notes\n    ------\n    - Requires Folium\n\n    \"\"\"\n    import folium as fl\n\n    # Initialize map\n    my_map = fl.Map(tiles=\"cartodbpositron\")\n\n    # Create a feature group for the stops and add it to the map\n    group = fl.FeatureGroup(name=\"Stops\")\n\n    # Add stops to feature group\n    stops = feed.stops.loc[lambda x: x.stop_id.isin(stop_ids)].fillna(\"n/a\")\n    for prop in stops.to_dict(orient=\"records\"):\n        # Add stop\n        lon = prop[\"stop_lon\"]\n        lat = prop[\"stop_lat\"]\n        fl.CircleMarker(\n            location=[lat, lon],\n            popup=fl.Popup(hp.make_html(prop)),\n            **stop_style,\n        ).add_to(group)\n\n    group.add_to(my_map)\n\n    # Add layer control\n    fl.LayerControl().add_to(my_map)\n\n    # Fit map to stop bounds\n    bounds = [\n        (stops.stop_lat.min(), stops.stop_lon.min()),\n        (stops.stop_lat.max(), stops.stop_lon.max()),\n    ]\n    my_map.fit_bounds(bounds, padding=[1, 1])\n\n    return my_map"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dates(feed: \"Feed\", *, as_date_obj: bool = False) -> List[str]:\n    dates = []\n    if feed.calendar is not None and not feed.calendar.empty:\n        if \"start_date\" in feed.calendar.columns:\n            dates.append(feed.calendar[\"start_date\"].min())\n        if \"end_date\" in feed.calendar.columns:\n            dates.append(feed.calendar[\"end_date\"].max())\n    if feed.calendar_dates is not None and not feed.calendar_dates.empty:\n        if \"date\" in feed.calendar_dates.columns:\n            start = feed.calendar_dates[\"date\"].min()\n            end = feed.calendar_dates[\"date\"].max()\n            dates.extend([start, end])\n    if not dates:\n        return []\n\n    start_date, end_date = min(dates), max(dates)\n    start_date, end_date = map(hp.datestr_to_date, [start_date, end_date])\n    num_days = (end_date - start_date).days\n    result = [\n        start_date + rd.relativedelta(days=+d) for d in range(num_days + 1)\n    ]\n\n    # Convert dates back to strings if required\n    if not as_date_obj:\n        result = [hp.datestr_to_date(x, inverse=True) for x in result]\n\n    return result", "response": "Returns a list of dates for which the given Feed is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_first_week(feed: \"Feed\", *, as_date_obj: bool = False) -> List[str]:\n    dates = feed.get_dates(as_date_obj=True)\n    if not dates:\n        return []\n\n    # Get first Monday\n    monday_index = None\n    for (i, date) in enumerate(dates):\n        if date.weekday() == 0:\n            monday_index = i\n            break\n    if monday_index is None:\n        return []\n\n    result = []\n    for j in range(7):\n        try:\n            result.append(dates[monday_index + j])\n        except:\n            break\n\n    # Convert to date strings if requested\n    if not as_date_obj:\n        result = [hp.datestr_to_date(x, inverse=True) for x in result]\n\n    return result", "response": "Returns a list of dates corresponding to the first Monday--Sunday week for which this feed is valid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restrict_dates(feed: \"Feed\", dates: List[str]) -> List[str]:\n    # Coerce string to set\n    if isinstance(dates, str):\n        dates = [dates]\n\n    # Restrict\n    return [d for d in dates if d in feed.get_dates()]", "response": "Restrict the dates in a base order of the dates in a base order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_gtfs(path: Path) -> DataFrame:\n    path = Path(path)\n    if not path.exists():\n        raise ValueError(f\"Path {path} does not exist\")\n\n    # Collect rows of DataFrame\n    rows = []\n    if path.is_file():\n        # Zip file\n        with zipfile.ZipFile(str(path)) as src:\n            for x in src.infolist():\n                if x.filename == \"./\":\n                    continue\n                d = {}\n                d[\"file_name\"] = x.filename\n                d[\"file_size\"] = x.file_size\n                rows.append(d)\n    else:\n        # Directory\n        for x in path.iterdir():\n            d = {}\n            d[\"file_name\"] = x.name\n            d[\"file_size\"] = x.stat().st_size\n            rows.append(d)\n\n    return pd.DataFrame(rows)", "response": "Returns a DataFrame with the columns file_name file_size and file_size of the contents of the contents of the files in the GTFS zip file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading a GTFS file into a feed dictionary.", "response": "def read_gtfs(path: Path, dist_units: str) -> \"Feed\":\n    \"\"\"\n    Create a Feed instance from the given path and given distance units.\n    The path should be a directory containing GTFS text files or a\n    zip file that unzips as a collection of GTFS text files\n    (and not as a directory containing GTFS text files).\n    The distance units given must lie in :const:`constants.dist_units`\n\n    Notes\n    -----\n    - Ignore non-GTFS files\n    - Automatically strip whitespace from the column names in GTFS files\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise ValueError(f\"Path {path} does not exist\")\n\n    # Unzip path to temporary directory if necessary\n    if path.is_file():\n        zipped = True\n        tmp_dir = tempfile.TemporaryDirectory()\n        src_path = Path(tmp_dir.name)\n        shutil.unpack_archive(str(path), tmp_dir.name, \"zip\")\n    else:\n        zipped = False\n        src_path = path\n\n    # Read files into feed dictionary of DataFrames\n    feed_dict = {table: None for table in cs.GTFS_REF[\"table\"]}\n    for p in src_path.iterdir():\n        table = p.stem\n        # Skip empty files, irrelevant files, and files with no data\n        if p.is_file() and p.stat().st_size and table in feed_dict:\n            # utf-8-sig gets rid of the byte order mark (BOM);\n            # see http://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n            df = pd.read_csv(p, dtype=cs.DTYPE, encoding=\"utf-8-sig\")\n            if not df.empty:\n                feed_dict[table] = cn.clean_column_names(df)\n\n    feed_dict[\"dist_units\"] = dist_units\n\n    # Delete temporary directory\n    if zipped:\n        tmp_dir.cleanup()\n\n    # Create feed\n    return Feed(**feed_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports the given feed to the given path.", "response": "def write_gtfs(feed: \"Feed\", path: Path, ndigits: int = 6) -> None:\n    \"\"\"\n    Export the given feed to the given path.\n    If the path end in '.zip', then write the feed as a zip archive.\n    Otherwise assume the path is a directory, and write the feed as a\n    collection of CSV files to that directory, creating the directory\n    if it does not exist.\n    Round all decimals to ``ndigits`` decimal places.\n    All distances will be the distance units ``feed.dist_units``.\n    \"\"\"\n    path = Path(path)\n\n    if path.suffix == \".zip\":\n        # Write to temporary directory before zipping\n        zipped = True\n        tmp_dir = tempfile.TemporaryDirectory()\n        new_path = Path(tmp_dir.name)\n    else:\n        zipped = False\n        if not path.exists():\n            path.mkdir()\n        new_path = path\n\n    for table in cs.GTFS_REF[\"table\"].unique():\n        f = getattr(feed, table)\n        if f is None:\n            continue\n\n        f = f.copy()\n        # Some columns need to be output as integers.\n        # If there are NaNs in any such column,\n        # then Pandas will format the column as float, which we don't want.\n        f_int_cols = set(cs.INT_COLS) & set(f.columns)\n        for s in f_int_cols:\n            f[s] = f[s].fillna(-1).astype(int).astype(str).replace(\"-1\", \"\")\n        p = new_path / (table + \".txt\")\n        f.to_csv(str(p), index=False, float_format=f\"%.{ndigits}f\")\n\n    # Zip directory\n    if zipped:\n        basename = str(path.parent / path.stem)\n        shutil.make_archive(basename, format=\"zip\", root_dir=tmp_dir.name)\n        tmp_dir.cleanup()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trips(self, val):\n        self._trips = val\n        if val is not None and not val.empty:\n            self._trips_i = self._trips.set_index(\"trip_id\")\n        else:\n            self._trips_i = None", "response": "Update self. _trips_i and self. _trips_i based on the given values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate self. _calendar_i and self. calendar.", "response": "def calendar(self, val):\n        \"\"\"\n        Update ``self._calendar_i``if ``self.calendar`` changes.\n        \"\"\"\n        self._calendar = val\n        if val is not None and not val.empty:\n            self._calendar_i = self._calendar.set_index(\"service_id\")\n        else:\n            self._calendar_i = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates self. _calendar_dates_g with new values.", "response": "def calendar_dates(self, val):\n        \"\"\"\n        Update ``self._calendar_dates_g``\n        if ``self.calendar_dates`` changes.\n        \"\"\"\n        self._calendar_dates = val\n        if val is not None and not val.empty:\n            self._calendar_dates_g = self._calendar_dates.groupby(\n                [\"service_id\", \"date\"]\n            )\n        else:\n            self._calendar_dates_g = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self) -> \"Feed\":\n        other = Feed(dist_units=self.dist_units)\n        for key in set(cs.FEED_ATTRS) - set([\"dist_units\"]):\n            value = getattr(self, key)\n            if isinstance(value, pd.DataFrame):\n                # Pandas copy DataFrame\n                value = value.copy()\n            elif isinstance(value, pd.core.groupby.DataFrameGroupBy):\n                # Pandas does not have a copy method for groupby objects\n                # as far as i know\n                value = deepcopy(value)\n            setattr(other, key, value)\n\n        return other", "response": "Returns a copy of this feed with all the same\n        attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _encode_content(self, uri, content):\n        if content is None:\n            content = self.NONE\n        return smart_str('|'.join([six.text_type(uri), content]))", "response": "Encode the content of a node uri and content as bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _decode_content(self, content):\n        content = smart_unicode(content)\n        uri, _, content = content.partition(u'|')\n        if content == self.NONE:\n            content = None\n        return uri or None, content", "response": "Split node string to uri and content and convert back to unicode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering node as html for templates with edit tagging.", "response": "def render_node(node, context=None, edit=True):\n    \"\"\"\n    Render node as html for templates, with edit tagging.\n    \"\"\"\n    output = node.render(**context or {}) or u''\n    if edit:\n        return u'<span data-i18n=\"{0}\">{1}</span>'.format(node.uri.clone(scheme=None, ext=None, version=None), output)\n    else:\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef node(key, default=None, edit=True):\n    node = cio.get(key, default=default or u'')\n    return lambda _: render_node(node, edit=edit)", "response": "Get a node from the cio dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render_to_json(self, context, **response_kwargs):\n        response_kwargs['content_type'] = 'application/json'\n        return self.response_class(\n            self.convert_context_to_json(context),\n            **response_kwargs\n        )", "response": "Returns a JSON response transforming the context to make the payload."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_post_data(self, request):\n        params = dict(request.POST)\n        params.update(request.FILES)\n\n        data = defaultdict(dict)\n\n        # Split data and meta parameters\n        for param in sorted(params.keys()):\n            value = params[param]\n\n            if isinstance(value, list) and len(value) <= 1:\n                value = value[0] if value else None\n\n            prefix, _, field = param.partition('[')\n            if field:\n                field = field[:-1]\n                try:\n                    data[prefix][field] = value\n                except TypeError:\n                    raise InvalidNodeData('Got both reserved parameter \"data\" and plugin specific parameters.')\n            else:\n                data[prefix] = value\n\n        return data['data'], data['meta']", "response": "Collect and merge post parameters with multipart files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, request, uri):\n        uri = self.decode_uri(uri)\n        node = cio.get(uri, lazy=False)\n\n        if node.content is None:\n            raise Http404\n\n        return self.render_to_json({\n            'uri': node.uri,\n            'content': node.content\n        })", "response": "Return published node or specified version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset node data for uri return rendered content.", "response": "def post(self, request, uri):\n        \"\"\"\n        Set node data for uri, return rendered content.\n\n        JSON Response:\n            {uri: x, content: y}\n        \"\"\"\n        uri = self.decode_uri(uri)\n        data, meta = self.get_post_data(request)\n        meta['author'] = auth.get_username(request)\n        node = cio.set(uri, data, publish=False, **meta)\n        return self.render_to_json(node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting versioned uri and return empty text response on success.", "response": "def delete(self, request, uri):\n        \"\"\"\n        Delete versioned uri and return empty text response on success.\n        \"\"\"\n        uri = self.decode_uri(uri)\n        uris = cio.delete(uri)\n\n        if uri not in uris:\n            raise Http404\n\n        return self.render_to_response()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, request, uri):\n        uri = self.decode_uri(uri)\n        node = cio.publish(uri)\n\n        if not node:\n            raise Http404\n\n        return self.render_to_json(node)", "response": "Publish versioned uri.\n\n        JSON Response:\n            {uri: x, content: y}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, request, uri):\n        uri = self.decode_uri(uri)\n        revisions = cio.revisions(uri)\n        revisions = [list(revision) for revision in revisions]  # Convert tuples to lists\n        return self.render_to_json(revisions)", "response": "Return a list of all the available cisco related objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload raw node source from storage.", "response": "def get(self, request, uri):\n        \"\"\"\n        Load raw node source from storage.\n\n        JSON Response:\n            {uri: x, data: y}\n        \"\"\"\n        uri = self.decode_uri(uri)\n        node = cio.load(uri)\n        return self.render_to_json(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders data for plugin and return text response.", "response": "def post(self, request, ext):\n        \"\"\"\n        Render data for plugin and return text response.\n        \"\"\"\n        try:\n            plugin = plugins.get(ext)\n            data, meta = self.get_post_data(request)\n            data = plugin.load(data)\n        except UnknownPlugin:\n            raise Http404\n        else:\n            content = plugin.render(data)\n            return self.render_to_response(content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the authentication data from the AUTH_CONF file.", "response": "def load_auth(configfile):\n    \"\"\"Get authentication data from the AUTH_CONF file.\"\"\"\n\n    logging.debug('Loading habitica auth data from %s' % configfile)\n\n    try:\n        cf = open(configfile)\n    except IOError:\n        logging.error(\"Unable to find '%s'.\" % configfile)\n        exit(1)\n\n    config = configparser.SafeConfigParser({'checklists': False})\n    config.readfp(cf)\n\n    cf.close()\n\n    # Get data from config\n    rv = {}\n    try:\n        rv = {'url': config.get('Habitica', 'url'),\n              'checklists': config.get('Habitica', 'checklists'),\n              'x-api-user': config.get('Habitica', 'login'),\n              'x-api-key': config.get('Habitica', 'password')}\n\n    except configparser.NoSectionError:\n        logging.error(\"No 'Habitica' section in '%s'\" % configfile)\n        exit(1)\n\n    except configparser.NoOptionError as e:\n        logging.error(\"Missing option in auth file '%s': %s\"\n                      % (configfile, e.message))\n        exit(1)\n\n    # Return auth data as a dictionnary\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_task_ids(tids):\n    logging.debug('raw task ids: %s' % tids)\n    task_ids = []\n    for raw_arg in tids:\n        for bit in raw_arg.split(','):\n            if '-' in bit:\n                start, stop = [int(e) for e in bit.split('-')]\n                task_ids.extend(range(start, stop + 1))\n            else:\n                task_ids.append(int(bit))\n    return [e - 1 for e in set(task_ids)]", "response": "Convert a list of task - id strings into a list of integers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset display_checklist status toggling from CLI flag", "response": "def set_checklists_status(auth, args):\n    \"\"\"Set display_checklist status, toggling from cli flag\"\"\"\n    global checklists_on\n\n    if auth['checklists'] == \"true\":\n        checklists_on = True\n    else:\n        checklists_on = False\n\n    # reverse the config setting if specified by the CLI option\n    if args['--checklists']:\n        checklists_on = not checklists_on\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli():\n\n    # set up args\n    args = docopt(cli.__doc__, version=VERSION)\n\n    # set up logging\n    if args['--verbose']:\n        logging.basicConfig(level=logging.INFO)\n    if args['--debug']:\n        logging.basicConfig(level=logging.DEBUG)\n\n    logging.debug('Command line args: {%s}' %\n                  ', '.join(\"'%s': '%s'\" % (k, v) for k, v in args.items()))\n\n    # Set up auth\n    auth = load_auth(AUTH_CONF)\n\n    # Prepare cache\n    cache = load_cache(CACHE_CONF)\n\n    # instantiate api service\n    hbt = api.Habitica(auth=auth)\n\n    # Flag checklists as on if true in the config\n    set_checklists_status(auth, args)\n\n    # GET server status\n    if args['<command>'] == 'server':\n        server = hbt.status()\n        if server['status'] == 'up':\n            print('Habitica server is up')\n        else:\n            print('Habitica server down... or your computer cannot connect')\n\n    # open HABITICA_TASKS_PAGE\n    elif args['<command>'] == 'home':\n        home_url = '%s%s' % (auth['url'], HABITICA_TASKS_PAGE)\n        print('Opening %s' % home_url)\n        open_new_tab(home_url)\n\n    # GET user\n    elif args['<command>'] == 'status':\n\n        # gather status info\n        user = hbt.user()\n        stats = user.get('stats', '')\n        items = user.get('items', '')\n        food_count = sum(items['food'].values())\n        group = hbt.groups(type='party')\n        party = DEFAULT_PARTY\n        quest = DEFAULT_QUEST\n        mount = DEFAULT_MOUNT\n\n        # if in a party, grab party info\n        if group:\n            party_id = group[0]['id']\n            party_title = group[0]['name']\n\n            # if on a quest with the party, grab quest info\n            quest_data = getattr(hbt.groups, party_id)()['quest']\n            if quest_data and quest_data['active']:\n                quest_key = quest_data['key']\n\n                if cache.get(SECTION_CACHE_QUEST, 'quest_key') != quest_key:\n                    # we're on a new quest, update quest key\n                    logging.info('Updating quest information...')\n                    content = hbt.content()\n                    quest_type = ''\n                    quest_max = '-1'\n                    quest_title = content['quests'][quest_key]['text']\n\n                    # if there's a content/quests/<quest_key/collect,\n                    # then drill into .../collect/<whatever>/count and\n                    # .../collect/<whatever>/text and get those values\n                    if content.get('quests', {}).get(quest_key,\n                                                     {}).get('collect'):\n                        logging.debug(\"\\tOn a collection type of quest\")\n                        qt = 'collect'\n                        clct = content['quests'][quest_key][qt].values()[0]\n                        quest_max = clct['count']\n                    # else if it's a boss, then hit up\n                    # content/quests/<quest_key>/boss/hp\n                    elif content.get('quests', {}).get(quest_key,\n                                                       {}).get('boss'):\n                        logging.debug(\"\\tOn a boss/hp type of quest\")\n                        qt = 'hp'\n                        quest_max = content['quests'][quest_key]['boss'][qt]\n\n                    # store repr of quest info from /content\n                    cache = update_quest_cache(CACHE_CONF,\n                                               quest_key=str(quest_key),\n                                               quest_type=str(qt),\n                                               quest_max=str(quest_max),\n                                               quest_title=str(quest_title))\n\n                # now we use /party and quest_type to figure out our progress!\n                quest_type = cache.get(SECTION_CACHE_QUEST, 'quest_type')\n                if quest_type == 'collect':\n                    qp_tmp = quest_data['progress']['collect']\n                    quest_progress = qp_tmp.values()[0]\n                else:\n                    quest_progress = quest_data['progress']['hp']\n\n                quest = '%s/%s \"%s\"' % (\n                        str(int(quest_progress)),\n                        cache.get(SECTION_CACHE_QUEST, 'quest_max'),\n                        cache.get(SECTION_CACHE_QUEST, 'quest_title'))\n\n        # prepare and print status strings\n        title = 'Level %d %s' % (stats['lvl'], stats['class'].capitalize())\n        health = '%d/%d' % (stats['hp'], stats['maxHealth'])\n        xp = '%d/%d' % (int(stats['exp']), stats['toNextLevel'])\n        mana = '%d/%d' % (int(stats['mp']), stats['maxMP'])\n        currentPet = items.get('currentPet', '')\n        if not currentPet:\n            currentPet = DEFAULT_PET\n        pet = '%s (%d food items)' % (currentPet, food_count)\n        mount = items.get('currentMount', '')\n        if not mount:\n            mount = DEFAULT_MOUNT\n        summary_items = ('health', 'xp', 'mana', 'quest', 'pet', 'mount')\n        len_ljust = max(map(len, summary_items)) + 1\n        print('-' * len(title))\n        print(title)\n        print('-' * len(title))\n        print('%s %s' % ('Health:'.rjust(len_ljust, ' '), health))\n        print('%s %s' % ('XP:'.rjust(len_ljust, ' '), xp))\n        print('%s %s' % ('Mana:'.rjust(len_ljust, ' '), mana))\n        print('%s %s' % ('Pet:'.rjust(len_ljust, ' '), pet))\n        print('%s %s' % ('Mount:'.rjust(len_ljust, ' '), mount))\n        print('%s %s' % ('Party:'.rjust(len_ljust, ' '), party))\n        print('%s %s' % ('Quest:'.rjust(len_ljust, ' '), quest))\n\n    # GET/POST habits\n    elif args['<command>'] == 'habits':\n        habits = hbt.user.tasks(type='habits')\n        if 'up' in args['<args>']:\n            tids = get_task_ids(args['<args>'][1:])\n            for tid in tids:\n                tval = habits[tid]['value']\n                hbt.user.tasks(_id=habits[tid]['id'],\n                               _direction='up', _method='post')\n                print('incremented task \\'%s\\''\n                      % habits[tid]['text'].encode('utf8'))\n                habits[tid]['value'] = tval + (TASK_VALUE_BASE ** tval)\n                sleep(HABITICA_REQUEST_WAIT_TIME)\n        elif 'down' in args['<args>']:\n            tids = get_task_ids(args['<args>'][1:])\n            for tid in tids:\n                tval = habits[tid]['value']\n                hbt.user.tasks(_id=habits[tid]['id'],\n                               _direction='down', _method='post')\n                print('decremented task \\'%s\\''\n                      % habits[tid]['text'].encode('utf8'))\n                habits[tid]['value'] = tval - (TASK_VALUE_BASE ** tval)\n                sleep(HABITICA_REQUEST_WAIT_TIME)\n        for i, task in enumerate(habits):\n            score = qualitative_task_score_from_value(task['value'])\n            print('[%s] %s %s' % (score, i + 1, task['text'].encode('utf8')))\n\n    # GET/PUT tasks:daily\n    elif args['<command>'] == 'dailies':\n        dailies = hbt.user.tasks(type='dailys')\n        if 'done' in args['<args>']:\n            tids = get_task_ids(args['<args>'][1:])\n            for tid in tids:\n                hbt.user.tasks(_id=dailies[tid]['id'],\n                               _direction='up', _method='post')\n                print('marked daily \\'%s\\' completed'\n                      % dailies[tid]['text'].encode('utf8'))\n                dailies[tid]['completed'] = True\n                sleep(HABITICA_REQUEST_WAIT_TIME)\n        elif 'undo' in args['<args>']:\n            tids = get_task_ids(args['<args>'][1:])\n            for tid in tids:\n                hbt.user.tasks(_id=dailies[tid]['id'],\n                               _method='put', completed=False)\n                print('marked daily \\'%s\\' incomplete'\n                      % dailies[tid]['text'].encode('utf8'))\n                dailies[tid]['completed'] = False\n                sleep(HABITICA_REQUEST_WAIT_TIME)\n        print_task_list(dailies)\n\n    # GET tasks:todo\n    elif args['<command>'] == 'todos':\n        todos = [e for e in hbt.user.tasks(type='todos')\n                 if not e['completed']]\n        if 'done' in args['<args>']:\n            tids = get_task_ids(args['<args>'][1:])\n            for tid in tids:\n                hbt.user.tasks(_id=todos[tid]['id'],\n                               _direction='up', _method='post')\n                print('marked todo \\'%s\\' complete'\n                      % todos[tid]['text'].encode('utf8'))\n                sleep(HABITICA_REQUEST_WAIT_TIME)\n            todos = updated_task_list(todos, tids)\n        elif 'add' in args['<args>']:\n            ttext = ' '.join(args['<args>'][1:])\n            hbt.user.tasks(type='todo',\n                           text=ttext,\n                           priority=PRIORITY[args['--difficulty']],\n                           _method='post')\n            todos.insert(0, {'completed': False, 'text': ttext})\n            print('added new todo \\'%s\\'' % ttext)\n        elif 'delete' in args['<args>']:\n            tids = get_task_ids(args['<args>'][1:])\n            for tid in tids:\n                hbt.user.tasks(_id=todos[tid]['id'],\n                               _method='delete')\n                print('deleted todo \\'%s\\''\n                      % todos[tid]['text'])\n                sleep(HABITICA_REQUEST_WAIT_TIME)\n            todos = updated_task_list(todos, tids)\n        print_task_list(todos)", "response": "Habitica command - line interface."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_wsgi_environ_from_event(event):\n    params = event.get('queryStringParameters')\n    environ = EnvironBuilder(method=event.get('httpMethod') or 'GET',\n                             path=event.get('path') or '/',\n                             headers=event.get('headers') or {},\n                             data=event.get('body') or b'',\n                             query_string=params or {}).get_environ()\n    environ['SERVER_PORT'] = 443\n    if 'execute-api' in environ['HTTP_HOST']:\n        # this is the API-Gateway hostname, which takes the stage as the first\n        # script path component\n        environ['SCRIPT_NAME'] = '/' + event['requestContext'].get('stage')\n    else:\n        # we are using our own hostname, nothing gets added to the script path\n        environ['SCRIPT_NAME'] = ''\n    environ['wsgi.url_scheme'] = 'https'\n    environ['lambda.event'] = event\n    return environ", "response": "Create a WSGI environment from the proxy integration event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a WSGI environment dict from a Lambda API Gateway request event.", "response": "def _build_wsgi_env(event, app_name):\n    \"\"\"Turn the Lambda/API Gateway request event into a WSGI environment dict.\n\n    :param dict event:\n        The event parameters passed to the Lambda function entrypoint.\n    :param str app_name:\n        Name of the API application.\n    \"\"\"\n    gateway = event['parameters']['gateway']\n    request = event['parameters']['request']\n    ctx = event['rawContext']\n    headers = request['header']\n    body = six.text_type(json.dumps(request['body']))\n\n    # Render the path correctly so connexion/flask will pass the path params to\n    # the handler function correctly.\n    # Basically, this replaces \"/foo/{param1}/bar/{param2}\" with\n    # \"/foo/123/bar/456\".\n    path = gateway['resource-path'].format(\n        **event['parameters']['request']['path']\n    )\n    environ = {\n        'PATH_INFO': path,\n        'QUERY_STRING': urlencode(request['querystring']),\n        'REMOTE_ADDR': ctx['identity']['sourceIp'],\n        'REQUEST_METHOD': ctx['httpMethod'],\n        'SCRIPT_NAME': app_name,\n        'SERVER_NAME': app_name,\n        'SERVER_PORT': headers.get('X-Forwarded-Port', '80'),\n        'SERVER_PROTOCOL': 'HTTP/1.1',\n        'wsgi.version': (1, 0),\n        'wsgi.url_scheme': headers.get('X-Forwarded-Proto', 'http'),\n        'wsgi.input': StringIO(body),\n        'wsgi.errors': StringIO(),\n        'wsgi.multiprocess': False,\n        'wsgi.multithread': False,\n        'wsgi.run_once': False,\n        'CONTENT_TYPE': headers.get('Content-Type', 'application/json'),\n    }\n    if ctx['httpMethod'] in ['POST', 'PUT', 'PATCH']:\n        environ['CONTENT_LENGTH'] = str(len(body))\n\n    for header_name, header_value in headers.items():\n        wsgi_name = 'HTTP_{}'.format(header_name.upper().replace('-', '_'))\n        environ[wsgi_name] = str(header_value)\n\n    return environ"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call_api(event, app_name, app_swagger_path, logger, strict_validation=True,\n             validate_responses=True, cache_app=True):\n    \"\"\"Wire up the incoming Lambda/API Gateway request to an application.\n\n    :param dict event:\n        Dictionary containing the entire request template. This can vary wildly\n        depending on the template structure and contents.\n    :param str app_name:\n        Name of the API application.\n    :param str app_swagger_path:\n        Local path to the Swagger API YAML file.\n    :param logging.Logger logger:\n        A Logger instance returned by `fleece.log.get_logger()` to be used for\n        capturing details about errors.\n    :param bool strict_validation:\n        Toggle to enable/disable Connexion's parameter validation.\n    :param bool validate_responses:\n        Toggle to enable/disable Connexion's response validation.\n    :param bool cache_app:\n        Toggle to enable/disable the caching of the Connextion/Flask app\n        instance. It's on by default, because it provides a significant\n        performance improvement in the Lambda runtime environment.\n    \"\"\"\n    app = get_connexion_app(\n        app_name=app_name,\n        app_swagger_path=app_swagger_path,\n        strict_validation=strict_validation,\n        validate_responses=validate_responses,\n        cache_app=cache_app,\n    )\n    return app.call_api(event)", "response": "Wire up the incoming Lambda API Gateway request to an application."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a request against the API defined by this app.", "response": "def call_api(self, event):\n        \"\"\"Make a request against the API defined by this app.\n\n        Return any result from the API call as a JSON object/dict. In the event\n        of a client or server error response from the API endpoint handler (4xx\n        or 5xx), raise a :class:`fleece.httperror.HTTPError` containing some\n        context about the error. Any unexpected exception encountered while\n        executing an API endpoint handler will be appropriately raised as a\n        generic 500 error with no error context (in order to not expose too\n        much of the internals of the application).\n\n        :param dict event:\n            Dictionary containing the entire request payload passed to the\n            Lambda function handler.\n        :returns:\n            JSON object (as a `list` or `dict`) containing the response data\n            from the API endpoint.\n        :raises:\n            :class:`fleece.httperror.HTTPError` on 4xx or 5xx responses from\n            the API endpoint handler, or a 500 response on an unexpected\n            failure (due to a bug in handler code, for example).\n        \"\"\"\n        try:\n            environ = _build_wsgi_env(event, self.import_name)\n            response = werkzeug.wrappers.Response.from_app(self, environ)\n            response_dict = json.loads(response.get_data())\n\n            if 400 <= response.status_code < 500:\n                if ('error' in response_dict and\n                        'message' in response_dict['error']):\n                    # FIXME(larsbutler): If 'error' is not a collection\n                    # (list/dict) and is a scalar such as an int, the check\n                    # `message in response_dict['error']` will blow up and\n                    # result in a generic 500 error. This check assumes too\n                    # much about the format of error responses given by the\n                    # handler. It might be good to allow this handling to\n                    # support custom structures.\n                    msg = response_dict['error']['message']\n                elif 'detail' in response_dict:\n                    # Likely this is a generated 400 response from Connexion.\n                    # Reveal the 'detail' of the message to the user.\n                    # NOTE(larsbutler): If your API handler explicitly returns\n                    # a 'detail' key in in the response, be aware that this\n                    # will be exposed to the client.\n                    msg = response_dict['detail']\n                else:\n                    # Respond with a generic client error.\n                    msg = 'Client Error'\n                # FIXME(larsbutler): The logic above still assumes a lot about\n                # the API response. That's not great. It would be nice to make\n                # this more flexible and explicit.\n                self.logger.error(\n                    'Raising 4xx error',\n                    http_status=response.status_code,\n                    message=msg,\n                )\n                raise httperror.HTTPError(\n                    status=response.status_code,\n                    message=msg,\n                )\n            elif 500 <= response.status_code < 600:\n                if response_dict['title'] == RESPONSE_CONTRACT_VIOLATION:\n                    # This case is generally enountered if the API endpoint\n                    # handler code does not conform to the contract dictated by\n                    # the Swagger specification.\n                    self.logger.error(\n                        RESPONSE_CONTRACT_VIOLATION,\n                        detail=response_dict['detail']\n                    )\n                else:\n                    # This case will trigger if\n                    # a) the handler code raises an unexpected exception\n                    # or\n                    # b) the handler code explicitly returns a 5xx error.\n                    self.logger.error(\n                        'Raising 5xx error',\n                        response=response_dict,\n                        http_status=response.status_code,\n                    )\n                raise httperror.HTTPError(status=response.status_code)\n            else:\n                return response_dict\n        except httperror.HTTPError:\n            self.logger.exception('HTTPError')\n            raise\n        except Exception:\n            self.logger.exception('Unhandled exception')\n            raise httperror.HTTPError(status=500)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a request against the API defined by this app and return any result from the API call as a dict that is expected by the AWS_PROXY type integration in API Gateway.", "response": "def call_proxy_api(self, event):\n        \"\"\"Make a request against the API defined by this app.\n\n        Return any result from the API call as a dict that is expected by the\n        AWS_PROXY type integration in API Gateway.\n\n        :param dict event:\n            Dictionary containing the entire request payload passed to the\n            Lambda function handler.\n        :returns:\n            Dictionary containing the response data from the API endpoint.\n        :raises:\n            :class:`fleece.httperror.HTTPError` on 4xx or 5xx responses from\n            the API endpoint handler, or a 500 response on an unexpected\n            failure (due to a bug in handler code, for example).\n        \"\"\"\n        try:\n            environ = build_wsgi_environ_from_event(event)\n            response = werkzeug.wrappers.Response.from_app(self, environ)\n            return {\n                'statusCode': response.status_code,\n                'headers': {\n                    header: value for header, value in response.headers.items()\n                },\n                'body': response.get_data(as_text=True),\n            }\n        except Exception:\n            # We should actually never get here, because exceptions raised\n            # within the application are handled by the error handlers, and the\n            # default one will return a clean 500 error during the happy path\n            # above.\n            self.logger.exception('Unhandled exception')\n            return {\n                'statusCode': 500,\n                'headers': {},\n                'body': json.dumps({\n                    'error': {\n                        'message': 'Internal server error.',\n                    },\n                }),\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses FAWS provided credentials to assume defined role.", "response": "def assume_role(credentials, account, role):\n    \"\"\"Use FAWS provided credentials to assume defined role.\"\"\"\n    sts = boto3.client(\n        'sts',\n        aws_access_key_id=credentials['accessKeyId'],\n        aws_secret_access_key=credentials['secretAccessKey'],\n        aws_session_token=credentials['sessionToken'],\n    )\n    resp = sts.assume_role(\n        RoleArn='arn:aws:sts::{}:role/{}'.format(account, role),\n        RoleSessionName='fleece_assumed_role'\n    )\n    return {\n        'accessKeyId': resp['Credentials']['AccessKeyId'],\n        'secretAccessKey': resp['Credentials']['SecretAccessKey'],\n        'sessionToken': resp['Credentials']['SessionToken'],\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_environment(config, stage):\n    stage_data = get_stage_data(stage, config.get('stages', {}))\n    if not stage_data:\n        sys.exit(NO_STAGE_DATA.format(stage))\n    try:\n        return stage_data['environment']\n    except KeyError:\n        sys.exit(NO_ENV_IN_STAGE.format(stage))", "response": "Find default environment name in stage."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding environment name in config object and return AWS account.", "response": "def get_account(config, environment, stage=None):\n    \"\"\"Find environment name in config object and return AWS account.\"\"\"\n    if environment is None and stage:\n        environment = get_environment(config, stage)\n    account = None\n    for env in config.get('environments', []):\n        if env.get('name') == environment:\n            account = env.get('account')\n            role = env.get('role')\n            username = os.environ.get(env.get('rs_username_var')) \\\n                if env.get('rs_username_var') else None\n            apikey = os.environ.get(env.get('rs_apikey_var')) \\\n                if env.get('rs_apikey_var') else None\n    if not account:\n        sys.exit(ACCT_NOT_FOUND_ERROR.format(environment))\n    return account, role, username, apikey"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets AWS account credentials to enable access to AWS.", "response": "def get_aws_creds(account, tenant, token):\n    \"\"\"Get AWS account credentials to enable access to AWS.\n\n    Returns a time bound set of AWS credentials.\n    \"\"\"\n    url = (FAWS_API_URL.format(account))\n    headers = {\n        'X-Auth-Token': token,\n        'X-Tenant-Id': tenant,\n    }\n    response = requests.post(url, headers=headers,\n                             json={'credential': {'duration': '3600'}})\n\n    if not response.ok:\n        sys.exit(FAWS_API_ERROR.format(response.status_code, response.text))\n    return response.json()['credential']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget config file and parse YAML into dict.", "response": "def get_config(config_file):\n    \"\"\"Get config file and parse YAML into dict.\"\"\"\n    config_path = os.path.abspath(config_file)\n\n    try:\n        with open(config_path, 'r') as data:\n            config = yaml.safe_load(data)\n    except IOError as exc:\n        sys.exit(str(exc))\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_rackspace_token(username, apikey):\n    auth_params = {\n        \"auth\": {\n            \"RAX-KSKEY:apiKeyCredentials\": {\n                \"username\": username,\n                \"apiKey\": apikey,\n            }\n        }\n    }\n    response = requests.post(RS_IDENTITY_URL, json=auth_params)\n    if not response.ok:\n        sys.exit(RS_AUTH_ERROR.format(response.status_code, response.text))\n\n    identity = response.json()\n    return (identity['access']['token']['id'],\n            identity['access']['token']['tenant']['id'])", "response": "Get Rackspace Identity token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate command - line arguments.", "response": "def validate_args(args):\n    \"\"\"Validate command-line arguments.\"\"\"\n    if not any([args.environment, args.stage, args.account]):\n        sys.exit(NO_ACCT_OR_ENV_ERROR)\n    if args.environment and args.account:\n        sys.exit(ENV_AND_ACCT_ERROR)\n    if args.environment and args.role:\n        sys.exit(ENV_AND_ROLE_ERROR)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap for requests. request function.", "response": "def request(method, url, **kwargs):\n    \"\"\"\n    Wrapper for the `requests.request()` function.\n    It accepts the same arguments as the original, plus an optional `retries`\n    that overrides the default retry mechanism.\n    \"\"\"\n    retries = kwargs.pop('retries', None)\n    with Session(retries=retries) as session:\n        return session.request(method=method, url=url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a request to the specified URL.", "response": "def request(self, method, url, **kwargs):\n        \"\"\"\n        Send a request.\n        If timeout is not explicitly given, use the default timeouts.\n        \"\"\"\n        if 'timeout' not in kwargs:\n            if self.timeout is not None:\n                kwargs['timeout'] = self.timeout\n            else:\n                kwargs['timeout'] = (DEFAULT_CONNECT_TIMEOUT,\n                                     DEFAULT_READ_TIMEOUT)\n        return super(Session, self).request(method=method, url=url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_default_timeout(timeout=None, connect_timeout=None, read_timeout=None):\n    global DEFAULT_TIMEOUT\n    global DEFAULT_CONNECT_TIMEOUT\n    global DEFAULT_READ_TIMEOUT\n    DEFAULT_TIMEOUT = timeout\n    DEFAULT_CONNECT_TIMEOUT = connect_timeout\n    DEFAULT_READ_TIMEOUT = read_timeout", "response": "This function sets the default socket timeouts for the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef client(*args, **kwargs):\n    timeout = kwargs.pop('timeout', DEFAULT_TIMEOUT)\n    connect_timeout = kwargs.pop('connect_timeout',\n                                 DEFAULT_CONNECT_TIMEOUT or timeout)\n    read_timeout = kwargs.pop('read_timeout', DEFAULT_READ_TIMEOUT or timeout)\n\n    config = Config(connect_timeout=connect_timeout,\n                    read_timeout=read_timeout)\n    return real_boto3.client(*args, config=config, **kwargs)", "response": "Create a low - level service client by name using the default session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _has_streamhandler(logger, level=None, fmt=LOG_FORMAT,\n                       stream=DEFAULT_STREAM):\n    \"\"\"Check the named logger for an appropriate existing StreamHandler.\n\n    This only returns True if a StreamHandler that exaclty matches\n    our specification is found. If other StreamHandlers are seen,\n    we assume they were added for a different purpose.\n    \"\"\"\n    # Ensure we are talking the same type of logging levels\n    # if they passed in a string we need to convert it to a number\n    if isinstance(level, basestring):\n        level = logging.getLevelName(level)\n\n    for handler in logger.handlers:\n        if not isinstance(handler, logging.StreamHandler):\n            continue\n        if handler.stream is not stream:\n            continue\n        if handler.level != level:\n            continue\n        if not handler.formatter or handler.formatter._fmt != fmt:\n            continue\n        return True\n    return False", "response": "Check the named logger for an appropriate existing StreamHandler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inject_request_ids_into_environment(func):\n\n    @wraps(func)\n    def wrapper(event, context):\n        # This might not always be an API Gateway event, so only log the\n        # request ID, if it looks like to be coming from there.\n        if 'requestContext' in event:\n            os.environ[ENV_APIG_REQUEST_ID] = event['requestContext'].get(\n                'requestId', 'N/A')\n        os.environ[ENV_LAMBDA_REQUEST_ID] = context.aws_request_id\n        return func(event, context)\n\n    return wrapper", "response": "Decorator for the Lambda handler to inject request IDs into the environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconfigure and return a logger with structlog and stdlib.", "response": "def get_logger(name=None, level=None, stream=DEFAULT_STREAM,\n               clobber_root_handler=True, logger_factory=None,\n               wrapper_class=None):\n    \"\"\"Configure and return a logger with structlog and stdlib.\"\"\"\n    _configure_logger(\n        logger_factory=logger_factory,\n        wrapper_class=wrapper_class)\n    log = structlog.get_logger(name)\n    root_logger = logging.root\n    if log == root_logger:\n        if not _has_streamhandler(root_logger, level=level, stream=stream):\n            stream_handler = logging.StreamHandler(stream)\n            stream_handler.setLevel(level)\n            stream_handler.setFormatter(logging.Formatter(fmt=LOG_FORMAT))\n            root_logger.addHandler(stream_handler)\n        else:\n            if clobber_root_handler:\n                for handler in root_logger.handlers:\n                    handler.setFormatter(logging.Formatter(fmt=LOG_FORMAT))\n    if level:\n        log.setLevel(level)\n    return log"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates token and return auth context.", "response": "def validate(token):\n    \"\"\"Validate token and return auth context.\"\"\"\n    token_url = TOKEN_URL_FMT.format(token=token)\n    headers = {\n        'x-auth-token': token,\n        'accept': 'application/json',\n    }\n    resp = requests.get(token_url, headers=headers)\n\n    if not resp.status_code == 200:\n        raise HTTPError(status=401)\n    return resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fullmatch(pattern, text, *args, **kwargs):\n    match = re.match(pattern, text, *args, **kwargs)\n    return match if match.group(0) == text else None", "response": "Match a pattern with a text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing X - Ray Trace ID environment variable and return an XRayTraceID instance.", "response": "def get_trace_id():\n    \"\"\"Parse X-Ray Trace ID environment variable.\n\n    The value looks something like this:\n\n        Root=1-5901e3bc-8da3814a5f3ccbc864b66ecc;Parent=328f72132deac0ce;Sampled=1\n\n    `Root` is the main X-Ray Trace ID, `Parent` points to the top-level\n    segment, and `Sampled` shows whether the current request should be traced\n    or not.\n\n    If the environment variable doesn't exist, just return an `XRayTraceID`\n    instance with default values, which means that tracing will be skipped\n    due to `sampled` being set to `False`.\n    \"\"\"\n    raw_trace_id = os.environ.get('_X_AMZN_TRACE_ID', '')\n    trace_id_parts = raw_trace_id.split(';')\n    trace_kwargs = {\n        'trace_id': None,\n        'parent_id': None,\n        'sampled': False,\n    }\n    if trace_id_parts[0] != '':\n        # This means the trace ID environment variable is not empty\n        for part in trace_id_parts:\n            name, value = part.split('=')\n            if name == 'Root':\n                trace_kwargs['trace_id'] = value\n            elif name == 'Parent':\n                trace_kwargs['parent_id'] = value\n            elif name == 'Sampled':\n                trace_kwargs['sampled'] = bool(int(value))\n\n    return XRayTraceID(**trace_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse X - Ray Daemon address environment variable.", "response": "def get_xray_daemon():\n    \"\"\"Parse X-Ray Daemon address environment variable.\n\n    If the environment variable is not set, raise an exception to signal that\n    we're unable to send data to X-Ray.\n    \"\"\"\n    env_value = os.environ.get('AWS_XRAY_DAEMON_ADDRESS')\n    if env_value is None:\n        raise XRayDaemonNotFoundError()\n\n    xray_ip, xray_port = env_value.split(':')\n    return XRayDaemon(ip_address=xray_ip, port=int(xray_port))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_data_on_udp(ip_address, port, data):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        sock.sendto(data.encode('utf-8'), (ip_address, port))\n    except:\n        LOGGER.exception('Failed to send trace to X-Ray Daemon')\n    finally:\n        sock.close()", "response": "Helper function to send a string over UDP to a specific IP and port."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_segment_document_to_xray_daemon(segment_document):\n    try:\n        xray_daemon = get_xray_daemon()\n    except XRayDaemonNotFoundError:\n        LOGGER.error('X-Ray Daemon not running, skipping send')\n        return\n\n    message = u'{header}\\n{document}'.format(\n        header=json.dumps(XRAY_DAEMON_HEADER),\n        document=json.dumps(\n            segment_document,\n            ensure_ascii=False,\n            cls=StringJSONEncoder,\n        ),\n    )\n\n    send_data_on_udp(\n        ip_address=xray_daemon.ip_address,\n        port=xray_daemon.port,\n        data=message,\n    )", "response": "Format and send document to the X - Ray Daemon."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_subsegment_to_xray_daemon(subsegment_id, parent_id,\n                                   start_time, end_time=None, name=None,\n                                   namespace='remote', extra_data=None):\n    \"\"\"High level function to send data to the X-Ray Daemon.\n\n    If `end_time` is set to `None` (which is the default), a partial subsegment\n    will be sent to X-Ray, which has to be completed by a subsequent call to\n    this function after the underlying operation finishes.\n\n    The `namespace` argument can either be `aws` or `remote` (`local` is used\n    as a default, but  it's not officially supported by X-Ray).\n\n    The `extra_data` argument must be a `dict` that is used for updating the\n    segment document with arbitrary data.\n    \"\"\"\n    extra_data = extra_data or {}\n    trace_id = get_trace_id()\n    segment_document = {\n        'type': 'subsegment',\n        'id': subsegment_id,\n        'trace_id': trace_id.trace_id,\n        'parent_id': parent_id,\n        'start_time': start_time,\n    }\n    if end_time is None:\n        segment_document['in_progress'] = True\n    else:\n        segment_document.update({\n            'end_time': end_time,\n            'name': name,\n            'namespace': namespace,\n        })\n        segment_document.update(extra_data)\n\n    LOGGER.debug(\n        'Prepared segment document for X-Ray Daemon',\n        segment_document=segment_document,\n    )\n    send_segment_document_to_xray_daemon(segment_document)", "response": "High level function to send data to the X - Ray Daemon."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generic_xray_wrapper(wrapped, instance, args, kwargs, name, namespace,\n                         metadata_extractor,\n                         error_handling_type=ERROR_HANDLING_GENERIC):\n    \"\"\"Wrapper function around existing calls to send traces to X-Ray.\n\n    `wrapped` is the original function, `instance` is the original instance,\n    if the function is an instance method (otherwise it'll be `None`), `args`\n    and `kwargs` are the positional and keyword arguments the original function\n    was called with.\n\n    The `name` argument can either be a `string` or a callable. In the latter\n    case the function must accept the following arguments:\n\n        def callback(wrapped, instance, args, kwargs):\n            pass\n\n    The `metadata_extractor` argument has to be a function with the following\n    definition:\n\n        def callback(wrapped, instance, args, kwargs, return_value):\n            pass\n\n    It has to return a `dict` that will be used to extend the segment document.\n\n    The `error_handling_type` determines how exceptions raised by the wrapped\n    function are handled. Currently `botocore` requires some special care.\n    \"\"\"\n    if not get_trace_id().sampled:\n        # Request not sampled by X-Ray, let's get to the call\n        # immediately.\n        LOGGER.debug('Request not sampled by X-Ray, skipping trace')\n        return wrapped(*args, **kwargs)\n\n    start_time = time.time()\n    error = False\n    cause = None\n\n    # Fetch the parent ID from the current thread, and set it to the current\n    # subsegment, so that downstream subsegments will be correctly nested.\n    original_parent_id = get_parent_id()\n    # If not parent ID exists in the thread-local storage, it means we're at\n    # the topmost level, so we have to retrieve the parent ID from the trace ID\n    # environment variable.\n    parent_id = original_parent_id or get_parent_id_from_trace_id()\n    subsegment_id = generate_subsegment_id()\n    set_parent_id(subsegment_id)\n    # Send partial subsegment to X-Ray, so that it'll know about the relations\n    # upfront (otherwise we'll lose data, since downstream subsegments will\n    # have invalid parent IDs).\n    send_subsegment_to_xray_daemon(\n        subsegment_id=subsegment_id,\n        parent_id=parent_id,\n        start_time=start_time,\n    )\n    try:\n        return_value = wrapped(*args, **kwargs)\n    except Exception as exc:\n        error = True\n        cause = {\n            'exceptions': [\n                {\n                    'message': str(exc),\n                    'type': '{}.{}'.format(\n                        type(exc).__module__,\n                        type(exc).__name__,\n                    ),\n                }\n            ]\n        }\n        if error_handling_type == ERROR_HANDLING_GENERIC:\n            return_value = None\n        elif error_handling_type == ERROR_HANDLING_BOTOCORE:\n            if isinstance(exc, ClientError):\n                return_value = exc.response\n            else:\n                return_value = {}\n        raise\n    finally:\n        end_time = time.time()\n        extra_data = metadata_extractor(\n            wrapped=wrapped,\n            instance=instance,\n            args=args,\n            kwargs=kwargs,\n            return_value=return_value,\n        )\n        extra_data['error'] = error\n        if error:\n            extra_data['cause'] = cause\n        # We allow the name to be determined dynamically when a function is\n        # passed in as the `name` argument.\n        if callable(name):\n            name = name(wrapped, instance, args, kwargs)\n        send_subsegment_to_xray_daemon(\n            subsegment_id=subsegment_id,\n            parent_id=parent_id,\n            start_time=start_time,\n            end_time=end_time,\n            name=name,\n            namespace=namespace,\n            extra_data=extra_data,\n        )\n        # After done with reporting the current subsegment, reset parent\n        # ID to the original one.\n        set_parent_id(original_parent_id)\n\n    return return_value", "response": "A generic wrapper function around existing calls to send traces to X - Ray."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstashing the args and kwargs into the metadata of the subsegment.", "response": "def extract_function_metadata(wrapped, instance, args, kwargs, return_value):\n    \"\"\"Stash the `args` and `kwargs` into the metadata of the subsegment.\"\"\"\n    LOGGER.debug(\n        'Extracting function call metadata',\n        args=args,\n        kwargs=kwargs,\n    )\n    return {\n        'metadata': {\n            'args': args,\n            'kwargs': kwargs,\n        },\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_service_name(wrapped, instance, args, kwargs):\n    if 'serviceAbbreviation' not in instance._service_model.metadata:\n        return instance._service_model.metadata['endpointPrefix']\n    return instance._service_model.metadata['serviceAbbreviation']", "response": "Return the AWS service name the client is communicating with."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract AWS metadata from the response.", "response": "def extract_aws_metadata(wrapped, instance, args, kwargs, return_value):\n    \"\"\"Provide AWS metadata for improved visualization.\n\n    See documentation for this data structure:\n    http://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-aws\n    \"\"\"\n    response = return_value\n    LOGGER.debug(\n        'Extracting AWS metadata',\n        args=args,\n        kwargs=kwargs,\n    )\n    if 'operation_name' in kwargs:\n        operation_name = kwargs['operation_name']\n    else:\n        operation_name = args[0]\n\n    # Most of the time the actual keyword arguments to the client call are\n    # passed in as a positial argument after the operation name.\n    if len(kwargs) == 0 and len(args) == 2:\n        kwargs = args[1]\n\n    region_name = instance._client_config.region_name\n\n    response_metadata = response.get('ResponseMetadata')\n\n    metadata = {\n        'aws': {\n            'operation': operation_name,\n            'region': region_name,\n        }\n    }\n\n    if 'TableName' in kwargs:\n        metadata['aws']['table_name'] = kwargs['TableName']\n    if 'QueueUrl' in kwargs:\n        metadata['aws']['queue_url'] = kwargs['QueueUrl']\n\n    if response_metadata is not None:\n        metadata['http'] = {\n            'response': {\n                'status': response_metadata['HTTPStatusCode'],\n            },\n        }\n        metadata['aws']['request_id'] = response_metadata['RequestId']\n\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap around botocore s base API call method.", "response": "def xray_botocore_api_call(wrapped, instance, args, kwargs):\n    \"\"\"Wrapper around botocore's base client API call method.\"\"\"\n    return generic_xray_wrapper(\n        wrapped, instance, args, kwargs,\n        name=get_service_name,\n        namespace='aws',\n        metadata_extractor=extract_aws_metadata,\n        error_handling_type=ERROR_HANDLING_BOTOCORE,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract HTTP metadata for improved visualization.", "response": "def extract_http_metadata(wrapped, instance, args, kwargs, return_value):\n    \"\"\"Provide HTTP request metadata for improved visualization.\n\n    See documentation for this data structure:\n    http://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-http\n    \"\"\"\n    response = return_value\n    LOGGER.debug(\n        'Extracting HTTP metadata',\n        args=args,\n        kwargs=kwargs,\n    )\n    if 'request' in kwargs:\n        request = kwargs['request']\n    else:\n        request = args[0]\n\n    metadata = {\n        'http': {\n            'request': {\n                'method': request.method.upper(),\n                'url': request.url,\n            },\n        },\n    }\n    if response is not None:\n        metadata['http']['response'] = {\n            'status': response.status_code,\n        }\n\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps around the requests library s low - level send method.", "response": "def xray_requests_send(wrapped, instance, args, kwargs):\n    \"\"\"Wrapper around the requests library's low-level send method.\"\"\"\n    return generic_xray_wrapper(\n        wrapped, instance, args, kwargs,\n        name='requests',\n        namespace='remote',\n        metadata_extractor=extract_http_metadata,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_recording(recording, save_path):\n        '''\n        Save recording extractor to MEArec format.\n        Parameters\n        ----------\n        recording: RecordingExtractor\n            Recording extractor object to be saved\n        save_path: str\n            .h5 or .hdf5 path\n        '''\n        assert HAVE_MREX, \"To use the MEArec extractors, install MEArec: \\n\\n pip install MEArec\\n\\n\"\n        save_path = Path(save_path)\n        if save_path.is_dir():\n            print(\"The file will be saved as recording.h5 in the provided folder\")\n            save_path = save_path / 'recording.h5'\n        if save_path.suffix == '.h5' or save_path.suffix == '.hdf5':\n            info = {'recordings': {'fs': recording.get_sampling_frequency()}}\n            rec_dict = {'recordings': recording.get_traces()}\n            if 'location' in recording.get_channel_property_names():\n                positions = np.array([recording.get_channel_property(chan, 'location')\n                                      for chan in recording.get_channel_ids()])\n                rec_dict['channel_positions'] = positions\n            recgen = mr.RecordingGenerator(rec_dict=rec_dict, info=info)\n            mr.save_recording_generator(recgen, str(save_path), verbose=False)\n        else:\n            raise Exception(\"Provide a folder or an .h5/.hdf5 as 'save_path'\")", "response": "Save a recording extractor object to MEArec format."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves sorting extractor object to MEArec format.", "response": "def write_sorting(sorting, save_path, sampling_frequency):\n        '''\n        Save sorting extractor to MEArec format.\n        Parameters\n        ----------\n        sorting: SortingExtractor\n            Sorting extractor object to be saved\n        save_path: str\n            .h5 or .hdf5 path\n        sampling_frequency: int\n            Sampling frequency in Hz\n\n        '''\n        assert HAVE_MREX, \"To use the MEArec extractors, install MEArec: \\n\\n pip install MEArec\\n\\n\"\n        save_path = Path(save_path)\n        if save_path.is_dir():\n            print(\"The file will be saved as sorting.h5 in the provided folder\")\n            save_path = save_path / 'sorting.h5'\n        if save_path.suffix == '.h5' or save_path.suffix == '.hdf5':\n            # create neo spike trains\n            spiketrains = []\n            for u in sorting.get_unit_ids():\n                st = neo.SpikeTrain(times=sorting.get_unit_spike_train(u) / float(sampling_frequency) * pq.s,\n                                    t_start=np.min(sorting.get_unit_spike_train(u) / float(sampling_frequency)) * pq.s,\n                                    t_stop=np.max(sorting.get_unit_spike_train(u) / float(sampling_frequency)) * pq.s)\n                st.annotate(unit_id=u)\n                spiketrains.append(st)\n\n            duration = np.max([st.t_stop.magnitude for st in spiketrains])\n            info = {'recordings': {'fs': sampling_frequency}, 'spiketrains': {'duration': duration}}\n            rec_dict = {'spiketrains': spiketrains}\n            recgen = mr.RecordingGenerator(rec_dict=rec_dict, info=info)\n            mr.save_recording_generator(recgen, str(save_path), verbose=False)\n        else:\n            raise Exception(\"Provide a folder or an .h5/.hdf5 as 'save_path'\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_unit_property(self, unit_id, property_name, value):\n        '''This function adds a unit property data set under the given property\n        name to the given unit.\n\n        Parameters\n        ----------\n        unit_id: int\n            The unit id for which the property will be set\n        property_name: str\n            The name of the property to be stored\n        value\n            The data associated with the given property name. Could be many\n            formats as specified by the user.\n        '''\n        if isinstance(unit_id, (int, np.integer)):\n            if unit_id in self.get_unit_ids():\n                if unit_id not in self._unit_properties:\n                    self._unit_properties[unit_id] = {}\n                if isinstance(property_name, str):\n                    self._unit_properties[unit_id][property_name] = value\n                else:\n                    raise ValueError(str(property_name) + \" must be a string\")\n            else:\n                raise ValueError(str(unit_id) + \" is not a valid unit_id\")\n        else:\n            raise ValueError(str(unit_id) + \" must be an int\")", "response": "This function adds a property to the given unit property data set under the given property name under the given unit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the property value for a list of units in the current object", "response": "def set_units_property(self, *, unit_ids=None, property_name, values):\n        '''Sets unit property data for a list of units\n\n        Parameters\n        ----------\n        unit_ids: list\n            The list of unit ids for which the property will be set\n            Defaults to get_unit_ids()\n        property_name: str\n            The name of the property\n        value: list\n            The list of values to be set\n        '''\n        if unit_ids is None:\n            unit_ids = self.get_unit_ids()\n        for i, unit in enumerate(unit_ids):\n            self.set_unit_property(unit_id=unit, property_name=property_name, value=values[i])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of values stored under the property name corresponding to a list of units", "response": "def get_units_property(self, *, unit_ids=None, property_name):\n        '''Returns a list of values stored under the property name corresponding\n        to a list of units\n\n        Parameters\n        ----------\n        unit_ids: list\n            The unit ids for which the property will be returned\n            Defaults to get_unit_ids()\n        property_name: str\n            The name of the property\n        Returns\n        ----------\n        values\n            The list of values\n        '''\n        if unit_ids is None:\n            unit_ids = self.get_unit_ids()\n        values = [self.get_unit_property(unit_id=unit, property_name=property_name) for unit in unit_ids]\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_unit_property_names(self, unit_id=None):\n        '''Get a list of property names for a given unit, or for all units if unit_id is None\n\n        Parameters\n        ----------\n        unit_id: int\n            The unit id for which the property names will be returned\n            If None (default), will return property names for all units\n        Returns\n        ----------\n        property_names\n            The list of property names from the specified unit(s)\n        '''\n        if unit_id is None:\n            property_names = []\n            for unit_id in self.get_unit_ids():\n                curr_property_names = self.get_unit_property_names(unit_id)\n                for curr_property_name in curr_property_names:\n                    property_names.append(curr_property_name)\n            property_names = sorted(list(set(property_names)))\n            return property_names\n        if isinstance(unit_id, (int, np.integer)):\n            if unit_id in self.get_unit_ids():\n                if unit_id not in self._unit_properties:\n                    self._unit_properties[unit_id] = {}\n                property_names = sorted(self._unit_properties[unit_id].keys())\n                return property_names\n            else:\n                raise ValueError(str(unit_id) + \" is not a valid unit_id\")\n        else:\n            raise ValueError(str(unit_id) + \" must be an int\")", "response": "Get a list of property names for a given unit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy_unit_properties(self, sorting, unit_ids=None):\n        '''Copy unit properties from another sorting extractor to the current\n        sorting extractor.\n\n        Parameters\n        ----------\n        sorting: SortingExtractor\n            The sorting extractor from which the properties will be copied\n        unit_ids: (array_like, int)\n            The list (or single value) of unit_ids for which the properties will be copied.\n        '''\n        if unit_ids is None:\n            unit_ids = sorting.get_unit_ids()\n        if isinstance(unit_ids, int):\n            curr_property_names = sorting.get_unit_property_names(unit_id=unit_ids)\n            for curr_property_name in curr_property_names:\n                value = sorting.get_unit_property(unit_id=unit_ids, property_name=curr_property_name)\n                self.set_unit_property(unit_id=unit_ids, property_name=curr_property_name, value=value)\n        else:\n            for unit_id in unit_ids:\n                curr_property_names = sorting.get_unit_property_names(unit_id=unit_id)\n                for curr_property_name in curr_property_names:\n                    value = sorting.get_unit_property(unit_id=unit_id, property_name=curr_property_name)\n                    self.set_unit_property(unit_id=unit_id, property_name=curr_property_name, value=value)", "response": "Copy unit properties from another sorting extractor to the current sorting extractor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy_unit_spike_features(self, sorting, unit_ids=None):\n        '''Copy unit spike features from another sorting extractor to the current\n        sorting extractor.\n\n        Parameters\n        ----------\n        sorting: SortingExtractor\n            The sorting extractor from which the spike features will be copied\n        unit_ids: (array_like, int)\n            The list (or single value) of unit_ids for which the spike features will be copied.\n        def get_unit_spike_features(self, unit_id, feature_name, start_frame=None, end_frame=None):\n        '''\n        if unit_ids is None:\n            unit_ids = sorting.get_unit_ids()\n        if isinstance(unit_ids, int):\n            curr_feature_names = sorting.get_unit_spike_feature_names(unit_id=unit_ids)\n            for curr_feature_name in curr_feature_names:\n                value = sorting.get_unit_spike_features(unit_id=unit_ids, feature_name=curr_feature_name)\n                self.set_unit_spike_features(unit_id=unit_ids, feature_name=curr_feature_name, value=value)\n        else:\n            for unit_id in unit_ids:\n                curr_feature_names = sorting.get_unit_spike_feature_names(unit_id=unit_id)\n                for curr_feature_name in curr_feature_names:\n                    value = sorting.get_unit_spike_features(unit_id=unit_id, feature_name=curr_feature_name)\n                    self.set_unit_spike_features(unit_id=unit_id, feature_name=curr_feature_name, value=value)", "response": "Copy unit spike features from another sorting extractor to the current sorting extractor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_snippets(self, *, reference_frames, snippet_len, channel_ids=None):\n        '''This function returns data snippets from the given channels that\n        are starting on the given frames and are the length of the given snippet\n        lengths before and after.\n\n        Parameters\n        ----------\n        snippet_len: int or tuple\n            If int, the snippet will be centered at the reference frame and\n            and return half before and half after of the length. If tuple,\n            it will return the first value of before frames and the second value\n            of after frames around the reference frame (allows for asymmetry)\n        reference_frames: array_like\n            A list or array of frames that will be used as the reference frame of\n            each snippet\n        channel_ids: array_like\n            A list or array of channel ids (ints) from which each trace will be\n            extracted.\n\n        Returns\n        ----------\n        snippets: numpy.ndarray\n            Returns a list of the snippets as numpy arrays.\n            The length of the list is len(reference_frames)\n            Each array has dimensions: (num_channels x snippet_len)\n            Out-of-bounds cases should be handled by filling in zeros in the snippet.\n        '''\n        # Default implementation\n        if isinstance(snippet_len, (tuple, list, np.ndarray)):\n            snippet_len_before = snippet_len[0]\n            snippet_len_after = snippet_len[1]\n        else:\n            snippet_len_before = int((snippet_len + 1) / 2)\n            snippet_len_after = snippet_len - snippet_len_before\n\n        if channel_ids is None:\n            channel_ids = self.get_channel_ids()\n\n        num_snippets = len(reference_frames)\n        num_channels = len(channel_ids)\n        num_frames = self.get_num_frames()\n        snippet_len_total = snippet_len_before + snippet_len_after\n        # snippets = []\n        snippets = np.zeros((num_snippets, num_channels, snippet_len_total))\n        #TODO extract all waveforms in a chunk\n        pad_first = False\n        pad_last = False\n        pad_samples_first = 0\n        pad_samples_last = 0\n        snippet_idxs = np.array([], dtype=int)\n        for i in range(num_snippets):\n            snippet_chunk = np.zeros((num_channels, snippet_len_total))\n            if (0 <= reference_frames[i]) and (reference_frames[i] < num_frames):\n                snippet_range = np.array(\n                    [int(reference_frames[i]) - snippet_len_before, int(reference_frames[i]) + snippet_len_after])\n                snippet_buffer = np.array([0, snippet_len_total])\n                # The following handles the out-of-bounds cases\n                if snippet_range[0] < 0:\n                    snippet_buffer[0] -= snippet_range[0]\n                    snippet_range[0] -= snippet_range[0]\n                if snippet_range[1] >= num_frames:\n                    snippet_buffer[1] -= snippet_range[1] - num_frames\n                    snippet_range[1] -= snippet_range[1] - num_frames\n                snippet_chunk[:, snippet_buffer[0]:snippet_buffer[1]] = self.get_traces(channel_ids=channel_ids,\n                                                                                       start_frame=snippet_range[0],\n                                                                                       end_frame=snippet_range[1])\n            snippets[i] = snippet_chunk\n        return snippets", "response": "This function returns the data snippets from the given channels that are starting on the given frames and are the length of the given snippet_len before and after. The length of the snippet_len before and after are set to the length of the reference frame that the channel is used to extract the data from the given channel_ids."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_channel_locations(self, channel_ids, locations):\n        '''This function sets the location properties of each specified channel\n        id with the corresponding locations of the passed in locations list.\n\n        Parameters\n        ----------\n        channel_ids: array_like\n            The channel ids (ints) for which the locations will be specified\n        locations: array_like\n            A list of corresonding locations (array_like) for the given channel_ids\n        '''\n        if len(channel_ids) == len(locations):\n            for i in range(len(channel_ids)):\n                if isinstance(locations[i],(list,np.ndarray)):\n                    location = np.asarray(locations[i])\n                    self.set_channel_property(channel_ids[i], 'location', location.astype(float))\n                else:\n                    raise ValueError(str(locations[i]) + \" must be an array_like\")\n        else:\n            raise ValueError(\"channel_ids and locations must have same length\")", "response": "This function sets the location properties of each specified channel_id with the corresponding locations of the passed in locations list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_channel_groups(self, channel_ids):\n        '''This function returns the group of each channel specifed by\n        channel_ids\n\n        Parameters\n        ----------\n        channel_ids: array_like\n            The channel ids (ints) for which the groups will be returned\n\n        Returns\n        ----------\n        groups: array_like\n            Returns a list of corresonding groups (ints) for the given\n            channel_ids\n        '''\n        groups = []\n        for channel_id in channel_ids:\n            group = self.get_channel_property(channel_id, 'group')\n            groups.append(group)\n        return groups", "response": "This function returns the group of each channel specifed by\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_channel_property_names(self, channel_id=None):\n        '''Get a list of property names for a given channel, or for all channels if channel_id is None\n         Parameters\n        ----------\n        channel_id: int\n            The channel id for which the property names will be returned\n            If None (default), will return property names for all channels\n        Returns\n        ----------\n        property_names\n            The list of property names\n        '''\n        if channel_id is None:\n            property_names = []\n            for channel_id in self.get_channel_ids():\n                curr_property_names = self.get_channel_property_names(channel_id=channel_id)\n                for curr_property_name in curr_property_names:\n                    property_names.append(curr_property_name)\n            property_names = sorted(list(set(property_names)))\n            return property_names\n        if isinstance(channel_id, (int, np.integer)):\n            if channel_id in self.get_channel_ids():\n                if channel_id not in self._channel_properties:\n                    self._channel_properties[channel_id] = {}\n                property_names = sorted(self._channel_properties[channel_id].keys())\n                return property_names\n            else:\n                raise ValueError(str(channel_id) + \" is not a valid channel_id\")\n        else:\n            raise ValueError(str(channel_id) + \" must be an int\")", "response": "Get a list of property names for a given channel or for all channels."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_epoch(self, epoch_name, start_frame, end_frame):\n        '''This function adds an epoch to your recording extractor that tracks\n        a certain time period in your recording. It is stored in an internal\n        dictionary of start and end frame tuples.\n\n        Parameters\n        ----------\n        epoch_name: str\n            The name of the epoch to be added\n        start_frame: int\n            The start frame of the epoch to be added (inclusive)\n        end_frame: int\n            The end frame of the epoch to be added (exclusive)\n\n        '''\n        # Default implementation only allows for frame info. Can override to put more info\n        if isinstance(epoch_name, str):\n            self._epochs[epoch_name] = {'start_frame': int(start_frame), 'end_frame': int(end_frame)}\n        else:\n            raise ValueError(\"epoch_name must be a string\")", "response": "This function adds an epoch to your recording extractor that tracks\n            a certain time period in your recording. It is stored in an internal dictionary of start and end frame tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_epoch_names(self):\n        '''This function returns a list of all the epoch names in your recording\n\n        Returns\n        ----------\n        epoch_names: list\n            List of epoch names in the recording extractor\n        '''\n        epoch_names = list(self._epochs.keys())\n        if not epoch_names:\n            pass\n        else:\n            epoch_start_frames = []\n            for epoch_name in epoch_names:\n                epoch_info = self.get_epoch_info(epoch_name)\n                start_frame = epoch_info['start_frame']\n                epoch_start_frames.append(start_frame)\n            epoch_names = [epoch_name for _, epoch_name in sorted(zip(epoch_start_frames, epoch_names))]\n        return epoch_names", "response": "This function returns a list of all the epoch names in your recording\n        Returns a list of all the epoch names in your recording\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_epoch_info(self, epoch_name):\n        '''This function returns the start frame and end frame of the epoch\n        in a dict.\n\n        Parameters\n        ----------\n        epoch_name: str\n            The name of the epoch to be returned\n\n        Returns\n        ----------\n        epoch_info: dict\n            A dict containing the start frame and end frame of the epoch\n        '''\n        # Default (Can add more information into each epoch in subclass)\n        if isinstance(epoch_name, str):\n            if epoch_name in list(self._epochs.keys()):\n                epoch_info = self._epochs[epoch_name]\n                return epoch_info\n            else:\n                raise ValueError(\"This epoch has not been added\")\n        else:\n            raise ValueError(\"epoch_name must be a string\")", "response": "This function returns the start frame and end frame of the epoch in a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_epoch(self, epoch_name):\n        '''This function returns a SubRecordingExtractor which is a view to the\n        given epoch\n\n        Parameters\n        ----------\n        epoch_name: str\n            The name of the epoch to be returned\n\n        Returns\n        ----------\n        epoch_extractor: SubRecordingExtractor\n            A SubRecordingExtractor which is a view to the given epoch\n        '''\n        epoch_info = self.get_epoch_info(epoch_name)\n        start_frame = epoch_info['start_frame']\n        end_frame = epoch_info['end_frame']\n        from .SubRecordingExtractor import SubRecordingExtractor\n        return SubRecordingExtractor(parent_recording=self, start_frame=start_frame,\n                                     end_frame=end_frame)", "response": "This function returns a SubRecordingExtractor which is a view to the given epoch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_curation_tree(self, unit_id):\n        '''This function prints the current curation tree for the unit_id (roots are current unit ids).\n\n        Parameters\n        ----------\n        unit_id: in\n            The unit id whose curation history will be printed.\n        '''\n        root_ids = []\n        for i in range(len(self._roots)):\n            root_id = self._roots[i].unit_id\n            root_ids.append(root_id)\n        if(unit_id in root_ids):\n            root_index = root_ids.index(unit_id)\n            print(self._roots[root_index])\n        else:\n            raise ValueError(\"invalid unit id\")", "response": "This function prints the current curation tree for the unit_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exclude_units(self, unit_ids):\n        '''This function deletes roots from the curation tree according to the given unit_ids\n\n        Parameters\n        ----------\n        unit_ids: list\n            The unit ids to be excluded\n        '''\n        root_ids = []\n        for i in range(len(self._roots)):\n            root_id = self._roots[i].unit_id\n            root_ids.append(root_id)\n\n        if(set(unit_ids).issubset(set(root_ids)) and len(unit_ids) > 0):\n            indices_to_be_deleted = []\n            for unit_id in unit_ids:\n                root_index = root_ids.index(unit_id)\n                indices_to_be_deleted.append(root_index)\n                if unit_id in self._unit_features:\n                    del self._unit_features[unit_id]\n            self._roots = [self._roots[i] for i,_ in enumerate(root_ids) if i not in indices_to_be_deleted]\n        else:\n            raise ValueError(str(unit_ids) + \" has one or more invalid unit ids\")", "response": "This function deletes roots from the curation tree according to the given unit_ids."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing python scripts in a dictionary containing parsed file", "response": "def read_python(path):\n    '''Parses python scripts in a dictionary\n\n    Parameters\n    ----------\n    path: str\n        Path to file to parse\n\n    Returns\n    -------\n    metadata:\n        dictionary containing parsed file\n\n    '''\n    from six import exec_\n    path = Path(path).absolute()\n    assert path.is_file()\n    with path.open('r') as f:\n        contents = f.read()\n    metadata = {}\n    exec_(contents, {}, metadata)\n    metadata = {k.lower(): v for (k, v) in metadata.items()}\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the probe file into the recording extractor.", "response": "def load_probe_file(recording, probe_file, channel_map=None, channel_groups=None):\n    '''Loads channel information into recording extractor. If a .prb file is given,\n    then 'location' and 'group' information for each channel is stored. If a .csv\n    file is given, then it will only store 'location'\n\n    Parameters\n    ----------\n    recording: RecordingExtractor\n        The recording extractor to channel information\n    probe_file: str\n        Path to probe file. Either .prb or .csv\n    Returns\n    ---------\n    subRecordingExtractor\n    '''\n    probe_file = Path(probe_file)\n    if probe_file.suffix == '.prb':\n        probe_dict = read_python(probe_file)\n        if 'channel_groups' in probe_dict.keys():\n            ordered_channels = np.array([], dtype=int)\n            groups = sorted(probe_dict['channel_groups'].keys())\n            for cgroup_id in groups:\n                cgroup = probe_dict['channel_groups'][cgroup_id]\n                for key_prop, prop_val in cgroup.items():\n                    if key_prop == 'channels':\n                        ordered_channels = np.concatenate((ordered_channels, prop_val))\n\n            if list(ordered_channels) == recording.get_channel_ids():\n                subrecording = recording\n            else:\n                if not np.all([chan in recording.get_channel_ids() for chan in ordered_channels]):\n                    print('Some channel in PRB file are in original recording')\n                present_ordered_channels = [chan for chan in ordered_channels if chan in recording.get_channel_ids()]\n                subrecording = SubRecordingExtractor(recording, channel_ids=present_ordered_channels)\n            for cgroup_id in groups:\n                cgroup = probe_dict['channel_groups'][cgroup_id]\n                if 'channels' not in cgroup.keys() and len(groups) > 1:\n                    raise Exception(\"If more than one 'channel_group' is in the probe file, the 'channels' field\"\n                                    \"for each channel group is required\")\n                elif 'channels' not in cgroup.keys():\n                    channels_in_group = subrecording.get_num_channels()\n                else:\n                    channels_in_group = len(cgroup['channels'])\n                for key_prop, prop_val in cgroup.items():\n                    if key_prop == 'channels':\n                        for i_ch, prop in enumerate(prop_val):\n                            if prop in subrecording.get_channel_ids():\n                                subrecording.set_channel_property(prop, 'group', int(cgroup_id))\n                    elif key_prop == 'geometry' or key_prop == 'location':\n                        if isinstance(prop_val, dict):\n                            if len(prop_val.keys()) == channels_in_group:\n                                print('geometry in PRB have not the same length as channel in group')\n                            for (i_ch, prop) in prop_val.items():\n                                if i_ch in subrecording.get_channel_ids():\n                                    subrecording.set_channel_property(i_ch, 'location', prop)\n                        elif isinstance(prop_val, (list, np.ndarray)) and len(prop_val) == channels_in_group:\n                            for (i_ch, prop) in zip(subrecording.get_channel_ids(), prop_val):\n                                if i_ch in subrecording.get_channel_ids():\n                                    subrecording.set_channel_property(i_ch, 'location', prop)\n                    else:\n                        if isinstance(prop_val, dict) and len(prop_val.keys()) == channels_in_group:\n                            for (i_ch, prop) in prop_val.items():\n                                if i_ch in subrecording.get_channel_ids():\n                                    subrecording.set_channel_property(i_ch, key_prop, prop)\n                        elif isinstance(prop_val, (list, np.ndarray)) and len(prop_val) == channels_in_group:\n                            for (i_ch, prop) in zip(subrecording.get_channel_ids(), prop_val):\n                                if i_ch in subrecording.get_channel_ids():\n                                    subrecording.set_channel_property(i_ch, key_prop, prop)\n                # create dummy locations\n                if 'geometry' not in cgroup.keys() and 'location' not in cgroup.keys():\n                    for i, chan in enumerate(subrecording.get_channel_ids()):\n                        subrecording.set_channel_property(chan, 'location', [i, 0])\n        else:\n            raise AttributeError(\"'.prb' file should contain the 'channel_groups' field\")\n\n    elif probe_file.suffix == '.csv':\n        if channel_map is not None:\n            assert np.all([chan in channel_map for chan in recording.get_channel_ids()]), \\\n                \"all channel_ids in 'channel_map' must be in the original recording channel ids\"\n            subrecording = SubRecordingExtractor(recording, channel_ids=channel_map)\n        else:\n            subrecording = recording\n        with probe_file.open() as csvfile:\n            posreader = csv.reader(csvfile)\n            row_count = 0\n            loaded_pos = []\n            for pos in (posreader):\n                row_count += 1\n                loaded_pos.append(pos)\n            assert len(subrecording.get_channel_ids()) == row_count, \"The .csv file must contain as many \" \\\n                                                                   \"rows as the number of channels in the recordings\"\n            for i_ch, pos in zip(subrecording.get_channel_ids(), loaded_pos):\n                if i_ch in subrecording.get_channel_ids():\n                    subrecording.set_channel_property(i_ch, 'location', list(np.array(pos).astype(float)))\n            if channel_groups is not None and len(channel_groups) == len(subrecording.get_channel_ids()):\n                for i_ch, chg in zip(subrecording.get_channel_ids(), channel_groups):\n                    if i_ch in subrecording.get_channel_ids():\n                        subrecording.set_channel_property(i_ch, 'group', chg)\n    else:\n        raise NotImplementedError(\"Only .csv and .prb probe files can be loaded.\")\n\n    return subrecording"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_probe_file(recording, probe_file, format=None, radius=100, dimensions=None):\n    '''Saves probe file from the channel information of the given recording\n    extractor\n\n    Parameters\n    ----------\n    recording: RecordingExtractor\n        The recording extractor to save probe file from\n    probe_file: str\n        file name of .prb or .csv file to save probe information to\n    format: str (optional)\n        Format for .prb file. It can be either 'klusta' or 'spyking_circus'. Default is None.\n    '''\n    probe_file = Path(probe_file)\n    if not probe_file.parent.is_dir():\n        probe_file.parent.mkdir()\n\n    if probe_file.suffix == '.csv':\n        # write csv probe file\n        with probe_file.open('w') as f:\n            if 'location' in recording.get_channel_property_names():\n                for chan in recording.get_channel_ids():\n                    loc = recording.get_channel_property(chan, 'location')\n                    if len(loc) == 2:\n                        f.write(str(loc[0]))\n                        f.write(',')\n                        f.write(str(loc[1]))\n                        f.write('\\n')\n                    elif len(loc) == 3:\n                        f.write(str(loc[0]))\n                        f.write(',')\n                        f.write(str(loc[1]))\n                        f.write(',')\n                        f.write(str(loc[2]))\n                        f.write('\\n')\n            else:\n                raise AttributeError(\"Recording extractor needs to have \"\n                                     \"'location' property to save .csv probe file\")\n    elif probe_file.suffix == '.prb':\n        _export_prb_file(recording, probe_file, format, radius=radius, dimensions=dimensions)\n    else:\n        raise NotImplementedError(\"Only .csv and .prb probe files can be saved.\")", "response": "Saves probe file from the channel information of the given recording extractor to the given probe file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_binary_dat_format(recording, save_path, time_axis=0, dtype=None, chunksize=None):\n    '''Saves the traces of a recording extractor in binary .dat format.\n\n    Parameters\n    ----------\n    recording: RecordingExtractor\n        The recording extractor object to be saved in .dat format\n    save_path: str\n        The path to the file.\n    time_axis: 0 (default) or 1\n        If 0 then traces are transposed to ensure (nb_sample, nb_channel) in the file.\n        If 1, the traces shape (nb_channel, nb_sample) is kept in the file.\n    dtype: dtype\n        Type of the saved data. Default float32\n    chunksize: None or int\n        If not None then the copy done by chunk size.\n        Thi avoid to much memory consumption for big files.\n    Returns\n    -------\n    '''\n    save_path = Path(save_path)\n    if save_path.suffix == '':\n        # when suffix is already raw/bin/dat do not change it.\n        save_path = save_path.parent / (save_path.name + '.dat')\n\n    if chunksize is None:\n        traces = recording.get_traces()\n        if dtype is not None:\n            traces = traces.astype(dtype)\n        if time_axis == 0:\n            traces = traces.T\n        with save_path.open('wb') as f:\n            traces.tofile(f)\n    else:\n        assert time_axis ==0, 'chunked writting work only with time_axis 0'\n        n_sample = recording.get_num_frames()\n        n_chan = recording.get_num_channels()\n        n_chunk = n_sample // chunksize\n        if n_sample % chunksize > 0:\n            n_chunk += 1\n        with save_path.open('wb') as f:\n            for i in range(n_chunk):\n                traces = recording.get_traces(start_frame=i*chunksize,\n                                            end_frame=min((i+1)*chunksize, n_sample))\n                if dtype is not None:\n                    traces = traces.astype(dtype)\n                if time_axis == 0:\n                    traces = traces.T\n                f.write(traces.tobytes())\n\n    return save_path", "response": "Saves the traces of a recording extractor in binary. dat format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sub_extractors_by_property(extractor, property_name, return_property_list=False):\n    '''Divides Recording or Sorting Extractor based on the property_name (e.g. group)\n\n    Parameters\n    ----------\n    extractor: RecordingExtractor or SortingExtractor\n        The extractor to be subdivided in subextractors\n    property_name: str\n        The property used to subdivide the extractor\n    return_property_list: bool\n        If True the property list is returned\n\n    Returns\n    -------\n    List of subextractors\n\n    '''\n    if isinstance(extractor, RecordingExtractor):\n        if property_name not in extractor.get_channel_property_names():\n            raise ValueError(\"'property_name' must be must be a property of the recording channels\")\n        else:\n            sub_list = []\n            recording = extractor\n            properties = np.array([recording.get_channel_property(chan, property_name)\n                                   for chan in recording.get_channel_ids()])\n            prop_list = np.unique(properties)\n            for prop in prop_list:\n                prop_idx = np.where(prop == properties)\n                chan_idx = list(np.array(recording.get_channel_ids())[prop_idx])\n                sub_list.append(SubRecordingExtractor(recording, channel_ids=chan_idx))\n            if return_property_list:\n                return sub_list, prop_list\n            else:\n                return sub_list\n    elif isinstance(extractor, SortingExtractor):\n        if property_name not in extractor.get_unit_property_names():\n            raise ValueError(\"'property_name' must be must be a property of the units\")\n        else:\n            sub_list = []\n            sorting = extractor\n            properties = np.array([sorting.get_unit_property(unit, property_name)\n                                   for unit in sorting.get_unit_ids()])\n            prop_list = np.unique(properties)\n            for prop in prop_list:\n                prop_idx = np.where(prop == properties)\n                unit_idx = list(np.array(sorting.get_unit_ids())[prop_idx])\n                sub_list.append(SubSortingExtractor(sorting, unit_ids=unit_idx))\n            if return_property_list:\n                return sub_list, prop_list\n            else:\n                return sub_list\n    else:\n        raise ValueError(\"'extractor' must be a RecordingExtractor or a SortingExtractor\")", "response": "Divides Recording or SortingExtractor based on the property_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _export_prb_file(recording, file_name, format=None, adjacency_distance=None, graph=False, geometry=True, radius=100,\n                     dimensions='all'):\n    '''Exports .prb file\n\n    Parameters\n    ----------\n    recording: RecordingExtractor\n        The recording extractor to save probe file from\n    file_name: str\n        probe filename to be exported to\n    format: str\n        'klusta' | 'spiking_circus' (defualt=None)\n    adjacency_distance: float\n        distance to consider 2 channels adjacent (if 'location' is a property)\n    graph: bool\n        if True graph information is extracted and saved\n    geometry:\n        if True geometry is saved\n    radius: float\n        radius for template-matching (if format is 'spyking_circus')\n    '''\n    if format == 'klusta':\n        graph = True\n        geometry = False\n    elif format == 'spyking_circus':\n        graph = False\n        geometry = True\n    else:\n        graph = True\n        geometry = True\n\n    file_name = Path(file_name)\n    assert file_name is not None\n    abspath = file_name.absolute()\n\n    if geometry:\n        if 'location' in recording.get_channel_property_names():\n            positions = np.array([recording.get_channel_property(chan, 'location')\n                                  for chan in recording.get_channel_ids()])\n            if dimensions is not None:\n                positions = positions[:, dimensions]\n        else:\n            print(\"'location' property is not available and it will not be saved.\")\n            positions = None\n            geometry = False\n    else:\n        positions = None\n\n    if 'group' in recording.get_channel_property_names():\n        groups = np.array([recording.get_channel_property(chan, 'group') for chan in recording.get_channel_ids()])\n        channel_groups = np.unique([groups])\n    else:\n        print(\"'group' property is not available and it will not be saved.\")\n        channel_groups = [0]\n        groups = np.array([0] * recording.get_num_channels())\n\n    n_elec = recording.get_num_channels()\n\n    # find adjacency graph\n    if graph:\n        if positions is not None and adjacency_distance is not None:\n            adj_graph = []\n            for chg in channel_groups:\n                group_graph = []\n                elecs = list(np.where(groups == chg)[0])\n                for i in range(len(elecs)):\n                    for j in range(i, len(elecs)):\n                        if elecs[i] != elecs[j]:\n                            if np.linalg.norm(positions[elecs[i]] - positions[elecs[j]]) < adjacency_distance:\n                                group_graph.append((elecs[i], elecs[j]))\n                adj_graph.append(group_graph)\n        else:\n            # all connected by group\n            adj_graph = []\n            for chg in channel_groups:\n                group_graph = []\n                elecs = list(np.where(groups == chg)[0])\n                for i in range(len(elecs)):\n                    for j in range(i, len(elecs)):\n                        if elecs[i] != elecs[j]:\n                            group_graph.append((elecs[i], elecs[j]))\n                adj_graph.append(group_graph)\n\n    with abspath.open('w') as f:\n        f.write('\\n')\n        if format == 'spyking_circus':\n            f.write('total_nb_channels = ' + str(n_elec) + '\\n')\n            f.write('radius = ' + str(radius) + '\\n')\n        f.write('channel_groups = {\\n')\n        if len(channel_groups) > 0:\n            for i_chg, chg in enumerate(channel_groups):\n                f.write(\"     \" + str(int(chg)) + \": \")\n                elecs = list(np.where(groups == chg)[0])\n                f.write(\"\\n        {\\n\")\n                f.write(\"           'channels': \" + str(elecs) + ',\\n')\n                if graph:\n                    if len(adj_graph) == 1:\n                        f.write(\"           'graph':  \" + str(adj_graph[0]) + ',\\n')\n                    else:\n                        f.write(\"           'graph':  \" + str(adj_graph[i_chg]) + ',\\n')\n                else:\n                    f.write(\"           'graph':  [],\\n\")\n                if geometry:\n                    f.write(\"           'geometry':  {\\n\")\n                    for i, pos in enumerate(positions[elecs]):\n                        f.write('               ' + str(elecs[i]) + ': ' + str(list(pos)) + ',\\n')\n                    f.write('           }\\n')\n                f.write('       },\\n')\n            f.write('}\\n')\n        else:\n            for elec in range(n_elec):\n                f.write('    ' + str(elec) + ': ')\n                f.write(\"\\n        {\\n\")\n                f.write(\"           'channels': [\" + str(elec) + '],\\n')\n                f.write(\"           'graph':  [],\\n\")\n                f.write('        },\\n')\n            f.write('}\\n')", "response": "Exports. prb file from a recording extractor to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef openBiocamFile(filename, verbose=False):\n    rf = h5py.File(filename, 'r')\n    # Read recording variables\n    recVars = rf.require_group('3BRecInfo/3BRecVars/')\n    # bitDepth = recVars['BitDepth'].value[0]\n    # maxV = recVars['MaxVolt'].value[0]\n    # minV = recVars['MinVolt'].value[0]\n    nFrames = recVars['NRecFrames'][0]\n    samplingRate = recVars['SamplingRate'][0]\n    signalInv = recVars['SignalInversion'][0]\n    # Read chip variables\n    chipVars = rf.require_group('3BRecInfo/3BMeaChip/')\n    nCols = chipVars['NCols'][0]\n    # Get the actual number of channels used in the recording\n    file_format = rf['3BData'].attrs.get('Version')\n    if file_format == 100:\n        nRecCh = len(rf['3BData/Raw'][0])\n    elif file_format == 101:\n        nRecCh = int(1. * rf['3BData/Raw'].shape[0] / nFrames)\n    else:\n        raise Exception('Unknown data file format.')\n\n    if verbose:\n        print('# 3Brain data format:', file_format, 'signal inversion', signalInv)\n        print('#       signal range: ', recVars['MinVolt'][0], '- ', recVars['MaxVolt'][0])\n        print('# channels: ', nRecCh)\n        print('# frames: ', nFrames)\n        print('# sampling rate: ', samplingRate)\n    # get channel locations\n    r = rf['3BRecInfo/3BMeaStreams/Raw/Chs'][()]['Row']\n    c = rf['3BRecInfo/3BMeaStreams/Raw/Chs'][()]['Col']\n    rawIndices = np.vstack((r, c)).T\n    # assign channel numbers\n    chIndices = np.array([(x - 1) + (y - 1) * nCols for (y, x) in rawIndices])\n    # determine correct function to read data\n    if verbose:\n        print(\"# Signal inversion looks like \" + str(signalInv) + \", guessing correct method for data access.\")\n        print(\"# If your results look wrong, signal polarity is may be wrong.\")\n    if file_format == 100:\n        if signalInv == -1:\n            read_function = readHDF5t_100\n        else:\n            read_function = readHDF5t_100_i\n    else:\n        if signalInv == -1:\n            read_function = readHDF5t_101_i\n        else:\n            read_function = readHDF5t_101\n    return (rf, nFrames, samplingRate, nRecCh, chIndices, file_format, signalInv, rawIndices, read_function)", "response": "Open a Biocam hdf5 file and return the recording info pick te correct method to access raw data and return this to the caller."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef countries():\n    '''\n    get country dictionar from pytz and add some extra.\n    '''\n    global _countries\n    if not _countries:\n        v = {}\n        _countries = v\n        try:\n            from pytz import country_names\n            for k, n in country_names.items():\n                v[k.upper()] = n\n        except Exception:\n            pass\n    return _countries", "response": "get country dictionar from pytz and add some extra."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a dictionary with keys given by countries ISO codes and values given by their currencies", "response": "def countryccys():\n    '''\n    Create a dictionary with keys given by countries ISO codes and values\n    given by their currencies\n    '''\n    global _country_ccys\n    if not _country_ccys:\n        v = {}\n        _country_ccys = v\n        ccys = currencydb()\n        for c in eurozone:\n            v[c] = 'EUR'\n        for c in ccys.values():\n            if c.default_country:\n                v[c.default_country] = c.code\n    return _country_ccys"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a mapping between a country code to another code", "response": "def set_country_map(cfrom, cto, name=None, replace=True):\n    '''\n    Set a mapping between a country code to another code\n    '''\n    global _country_maps\n    cdb = countries()\n    cfrom = str(cfrom).upper()\n    c = cdb.get(cfrom)\n    if c:\n        if name:\n            c = name\n        cto = str(cto).upper()\n        if cto in cdb:\n            raise CountryError('Country %s already in database' % cto)\n        cdb[cto] = c\n        _country_maps[cfrom] = cto\n        ccys = currencydb()\n        cccys = countryccys()\n        ccy = cccys[cfrom]\n        cccys[cto] = ccy\n\n        # If set, remove cfrom from database\n        if replace:\n            ccy = ccys.get(ccy)\n            ccy.default_country = cto\n            cdb.pop(cfrom)\n            cccys.pop(cfrom)\n    else:\n        raise CountryError('Country %s not in database' % c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds new country code to database", "response": "def set_new_country(code, ccy, name):\n    '''\n    Add new country code to database\n    '''\n    code = str(code).upper()\n    cdb = countries()\n    if code in cdb:\n        raise CountryError('Country %s already in database' % code)\n    ccys = currencydb()\n    ccy = str(ccy).upper()\n    if ccy not in ccys:\n        raise CountryError('Currency %s not in database' % ccy)\n    cdb[code] = str(name)\n    cccys = countryccys()\n    cccys[code] = ccy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the mapping for a country code.", "response": "def country_map(code):\n    '''\n    Country mapping\n    '''\n    code = str(code).upper()\n    global _country_maps\n    return _country_maps.get(code, code)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a unix timestamp to a Python datetime object", "response": "def timestamp2date(tstamp):\n    \"Converts a unix timestamp to a Python datetime object\"\n    dt = datetime.fromtimestamp(tstamp)\n    if not dt.hour+dt.minute+dt.second+dt.microsecond:\n        return dt.date()\n    else:\n        return dt"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert from a Julian date or datetime to python date or datetime", "response": "def juldate2date(val):\n    '''Convert from a Julian date/datetime to python date or datetime'''\n    ival = int(val)\n    dec = val - ival\n    try:\n        val4 = 4*ival\n        yd = val4 % 1461\n        st = 1899\n        if yd >= 4:\n            st = 1900\n        yd1 = yd - 241\n        y = val4 // 1461 + st\n        if yd1 >= 0:\n            q = yd1 // 4 * 5 + 308\n            qq = q // 153\n            qr = q % 153\n        else:\n            q = yd // 4 * 5 + 1833\n            qq = q // 153\n            qr = q % 153\n        m = qq % 12 + 1\n        d = qr // 5 + 1\n    except Exception:\n        raise ValueError('Could not convert %s to date' % val)\n    if dec:\n        dec24 = 24*dec\n        hours = int(dec24)\n        minutes = int(60*(dec24 - hours))\n        tot_seconds = 60*(60*(dec24 - hours) - minutes)\n        seconds = int(tot_seconds)\n        microseconds = int(1000000*(tot_seconds-seconds))\n        return datetime(y, m, d, hours, minutes, seconds, microseconds)\n    else:\n        return date(y, m, d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef date2juldate(val):\n    '''Convert from a python date/datetime to a Julian date & time'''\n    f = 12*val.year + val.month - 22803\n    fq = f // 12\n    fr = f % 12\n    dt = (fr*153 + 302)//5 + val.day + fq*1461//4\n    if isinstance(val, datetime):\n        return dt + (val.hour + (val.minute + (\n            val.second + 0.000001*val.microsecond)/60.)/60.)/24.\n    else:\n        return dt", "response": "Convert from a python date to a Julian date & time"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the currency dictionary", "response": "def make_ccys(db):\n    '''\n    Create the currency dictionary\n    '''\n    dfr = 4\n    dollar = r'\\u0024'\n    peso = r'\\u20b1'\n    kr = r'kr'\n    insert = db.insert\n\n    # G10 & SCANDI\n    insert('EUR', '978', 'EU', 1,\n           'Euro', dfr, 'EU', '30/360', 'ACT/360',\n           future='FE', symbol=r'\\u20ac', html='&#x20ac;')\n    insert('GBP', '826', 'BP', 2,\n           'British Pound', dfr, 'GB', 'ACT/365', 'ACT/365',\n           symbol=r'\\u00a3', html='&#xa3;')\n    insert('AUD', '036', 'AD', 3,\n           'Australian Dollar', dfr, 'AU', 'ACT/365', 'ACT/365',\n           symbol=dollar, html='&#x24;')\n    insert('NZD', '554', 'ND', 4,\n           'New-Zealand Dollar', dfr, 'NZ', 'ACT/365', 'ACT/365',\n           symbol=dollar, html='&#x24;')\n    insert('USD', '840', 'UD', 5,\n           'US Dollar', 0, 'US', '30/360', 'ACT/360',\n           future='ED', symbol=dollar, html='&#x24;')\n    insert('CAD', '124', 'CD', 6,\n           'Canadian Dollar', dfr, 'CA', 'ACT/365', 'ACT/365',\n           symbol=dollar, html='&#x24;')\n    insert('CHF', '756', 'SF', 7,\n           'Swiss Franc', dfr, 'CH', '30/360', 'ACT/360',\n           symbol=r'Fr', html='&#x20a3;')\n    insert('NOK', '578', 'NK', 8,\n           'Norwegian Krona', dfr, 'NO', '30/360', 'ACT/360',\n           symbol=kr, html=kr)\n    insert('SEK', '752', 'SK', 9,\n           'Swedish Krona', dfr, 'SE', '30/360', 'ACT/360',\n           symbol=kr, html=kr)\n    insert('DKK', '208', 'DK', 10,\n           'Danish Krona', dfr, 'DK', '30/360', 'ACT/360',\n           symbol=kr, html=kr)\n    insert('JPY', '392', 'JY', 10000,\n           'Japanese Yen', 2,  'JP', 'ACT/365', 'ACT/360',\n           symbol=r'\\u00a5', html='&#xa5;')\n\n    # ASIA\n    insert('CNY', '156', 'CY', 680,\n           'Chinese Renminbi', dfr, 'CN', 'ACT/365', 'ACT/365',\n           symbol=r'\\u00a5', html='&#xa5;')\n    insert('KRW', '410', 'KW', 110000,\n           'South Korean won', 2, 'KR', 'ACT/365', 'ACT/365',\n           symbol=r'\\u20a9', html='&#x20a9;')\n    insert('SGD', '702', 'SD', 15,\n           'Singapore Dollar', dfr, 'SG', 'ACT/365', 'ACT/365',\n           symbol=dollar, html='&#x24;')\n    insert('IDR', '360', 'IH', 970000,\n           'Indonesian Rupiah', 0, 'ID', 'ACT/360', 'ACT/360',\n           symbol=r'Rp', html='Rp')\n    insert('THB', '764', 'TB', 3300,\n           'Thai Baht', 2, 'TH', 'ACT/365', 'ACT/365',\n           symbol=r'\\u0e3f', html='&#xe3f;')\n    insert('TWD', '901', 'TD', 18,\n           'Taiwan Dollar', dfr, 'TW', 'ACT/365', 'ACT/365',\n           symbol=dollar, html='&#x24;')\n    insert('HKD', '344', 'HD', 19,\n           'Hong Kong Dollar', dfr, 'HK', 'ACT/365', 'ACT/365',\n           symbol=r'\\u5713', html='HK&#x24;')\n    insert('PHP', '608', 'PP', 4770,\n           'Philippines Peso', dfr, 'PH', 'ACT/360', 'ACT/360',\n           symbol=peso, html='&#x20b1;')\n    insert('INR', '356', 'IR', 4500,\n           'Indian Rupee', dfr, 'IN', 'ACT/365', 'ACT/365',\n           symbol=r'\\u20a8', html='&#x20a8;')\n    insert('MYR', '458', 'MR', 345,\n           'Malaysian Ringgit', dfr, 'MY', 'ACT/365', 'ACT/365')\n    insert('VND', '704', 'VD', 1700000,\n           'Vietnamese Dong', 0,  'VN', 'ACT/365', 'ACT/365',\n           symbol=r'\\u20ab', html='&#x20ab;')\n\n    # LATIN AMERICA\n    insert('BRL', '986', 'BC', 200,\n           'Brazilian Real', dfr, 'BR', 'BUS/252', 'BUS/252',\n           symbol=r'R$')\n    insert('PEN', '604', 'PS', 220,\n           'Peruvian New Sol', dfr, 'PE', 'ACT/360', 'ACT/360',\n           symbol=r'S/.')\n    insert('ARS', '032', 'AP', 301,\n           'Argentine Peso', dfr, 'AR', '30/360', 'ACT/360',\n           symbol=dollar, html='&#x24;')\n    insert('MXN', '484', 'MP', 1330,\n           'Mexican Peso', dfr, 'MX', 'ACT/360', 'ACT/360',\n           symbol=dollar, html='&#x24;')\n    insert('CLP', '152', 'CH', 54500,\n           'Chilean Peso', 2,  'CL', 'ACT/360', 'ACT/360',\n           symbol=dollar, html='&#x24;')\n    insert('COP', '170', 'CL', 190000,\n           'Colombian Peso', 2,  'CO', 'ACT/360', 'ACT/360',\n           symbol=dollar, html='&#x24;')\n    # TODO: Check towletters code and position\n    insert('JMD', '388', 'JD', 410,\n           'Jamaican Dollar', dfr, 'JM', 'ACT/360', 'ACT/360',\n           symbol=dollar, html='&#x24;')\n    # TODO: Check towletters code and position\n    insert('TTD', '780', 'TT', 410,\n           'Trinidad and Tobago Dollar', dfr, 'TT', 'ACT/360', 'ACT/360',\n           symbol=dollar, html='&#x24;')\n    # TODO: Check towletters code and position\n    insert('BMD', '060', 'BD', 410,\n           'Bermudian Dollar', dfr, 'BM',\n           symbol=dollar, html='&#x24;')\n\n    # EASTERN EUROPE\n    insert('CZK', '203', 'CK', 28,\n           'Czech Koruna', dfr, 'CZ', 'ACT/360', 'ACT/360',\n           symbol=r'\\u004b\\u010d')\n    insert('PLN', '985', 'PZ', 29,\n           'Polish Zloty', dfr, 'PL', 'ACT/ACT', 'ACT/365',\n           symbol=r'\\u0050\\u0142')\n    insert('TRY', '949', 'TY', 30,\n           'Turkish Lira', dfr, 'TR', 'ACT/360', 'ACT/360',\n           symbol=r'\\u0054\\u004c')\n    insert('HUF', '348', 'HF', 32,\n           'Hungarian Forint', dfr, 'HU', 'ACT/365', 'ACT/360',\n           symbol=r'Ft', html='Ft')\n    insert('RON', '946', 'RN', 34,\n           'Romanian Leu', dfr, 'RO', 'ACT/360', 'ACT/360')\n    insert('RUB', '643', 'RR', 36,\n           'Russian Ruble', dfr, 'RU', 'ACT/ACT', 'ACT/ACT',\n           symbol=r'\\u0440\\u0443\\u0431')\n    # TODO: Check towletters code and position\n    insert('HRK', '191', 'HK', 410,\n           'Croatian kuna', dfr, 'HR',\n           symbol=r'kn')\n    # TODO: Check towletters code and position\n    insert('KZT', '398', 'KT', 410,\n           'Tenge',  dfr, 'KZ',\n           symbol=r'\\u20b8', html='&#x20b8;')\n    # TODO: Check towletters code and position\n    insert('BGN', '975', 'BN', 410,\n           'Bulgarian Lev', dfr, 'BG',\n           symbol=r'\\u043b\\u0432.', html='&#1083;&#1074;')\n\n    # MIDDLE EAST & AFRICA\n    insert('ILS', '376', 'IS', 410,\n           'Israeli Shekel', dfr, 'IL', 'ACT/365', 'ACT/365',\n           symbol=r'\\u20aa', html='&#x20aa;')\n    # TODO: Check towletters code and position\n    insert('AED', '784', 'AE', 410,\n           'United Arab Emirates Dirham', dfr, 'AE')\n    # TODO: Check towletters code and position\n    insert('QAR', '634', 'QA', 410,\n           'Qatari Riyal', dfr, 'QA',\n           symbol=r'\\ufdfc', html='&#xfdfc;')\n    # TODO: Check towletters code and position\n    insert('SAR', '682', 'SR', 410,\n           'Saudi Riyal', dfr, 'SA',\n           symbol=r'\\ufdfc', html='&#xfdfc;')\n    insert('EGP', '818', 'EP', 550,\n           'Egyptian Pound', dfr, 'EG',\n           symbol=r'\\u00a3', html='&#xa3;')\n    insert('ZAR', '710', 'SA', 750,\n           'South African Rand', dfr, 'ZA', 'ACT/365', 'ACT/365',\n           symbol=r'R', html='R')\n\n    # BITCOIN\n    insert('XBT', '000', 'BT', -1,\n           'Bitcoin', 8, 'WW',\n           symbol=r'\\u0e3f', html='&#xe3f;')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a : class : ccy_pair from a six letter string.", "response": "def currency_pair(code):\n    '''Construct a :class:`ccy_pair` from a six letter string.'''\n    c = str(code)\n    c1 = currency(c[:3])\n    c2 = currency(c[3:])\n    return ccy_pair(c1, c2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef swap(self, c2):\n        '''\n        put the order of currencies as market standard\n        '''\n        inv = False\n        c1 = self\n        if c1.order > c2.order:\n            ct = c1\n            c1 = c2\n            c2 = ct\n            inv = True\n        return inv, c1, c2", "response": "Swap the order of currencies in self and c2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a cross rate representation with respect USD.", "response": "def as_cross(self, delimiter=''):\n        '''\n        Return a cross rate representation with respect USD.\n        @param delimiter: could be '' or '/' normally\n        '''\n        if self.order > usd_order:\n            return 'USD%s%s' % (delimiter, self.code)\n        else:\n            return '%s%sUSD' % (self.code, delimiter)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new currency pair with the * over* currency as second part of the pair ( Foreign currency.", "response": "def over(self, name='usd'):\n        '''Returns a new currency pair with the *over* currency as\n        second part of the pair (Foreign currency).'''\n        name = name.upper()\n        if self.ccy1.code == name.upper():\n            return ccy_pair(self.ccy2, self.ccy1)\n        else:\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef date(self):\n        if self._date:\n            return self._date\n        return datetime.datetime.now().strftime('%Y-%m-%d')", "response": "Get the date of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload an attribute attachment", "response": "def download(self, attr):\n        \"\"\"\n        Download an attribute attachment\n        (if type is malware-sample or attachment only)\n        :param attr: attribute (should be MispAttribute instance)\n        :returns: value of the attachment\n        \"\"\"\n        if attr.type not in ['malware-sample', 'attachment']:\n            raise ValueError('Only malware-sample and attachment can be downloaded')\n\n        return self.GET('/attributes/downloadAttachment/download/%i' % attr.id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_attribute(attr):\n        assert attr is not MispAttribute\n        prop = MispShadowAttribute()\n        prop.distribution = attr.distribution\n        prop.type = attr.type\n        prop.comment = attr.comment\n        prop.value = attr.value\n        prop.category = attr.category\n        prop.to_ids = attr.to_ids\n        return prop", "response": "Converts an attribute into a shadow attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef active_processes(self, use_cache=True):\n        LOGGER.debug('Checking active processes (cache: %s)', use_cache)\n        if self.can_use_process_cache(use_cache):\n            return self._active_cache[1]\n        active_processes, dead_processes = list(), list()\n        for consumer in self.consumers:\n            processes = list(self.consumers[consumer].processes)\n            for name in processes:\n                child = self.get_consumer_process(consumer, name)\n                if child is None:\n                    dead_processes.append((consumer, name))\n                elif child.pid is None:\n                    dead_processes.append((consumer, name))\n                    continue\n                elif child.pid == self.pid:\n                    continue\n                try:\n                    proc = psutil.Process(child.pid)\n                except psutil.NoSuchProcess:\n                    dead_processes.append((consumer, name))\n                    continue\n\n                if self.unresponsive[name] >= self.MAX_UNRESPONSIVE_COUNT:\n                    LOGGER.info('Killing unresponsive consumer %s (%i): '\n                                '%i misses',\n                                name, proc.pid, self.unresponsive[name])\n                    try:\n                        os.kill(child.pid, signal.SIGABRT)\n                    except OSError:\n                        pass\n                    dead_processes.append((consumer, name))\n                elif self.is_dead(proc, name):\n                    dead_processes.append((consumer, name))\n                else:\n                    active_processes.append(child)\n\n        if dead_processes:\n            LOGGER.debug('Removing %i dead process(es)', len(dead_processes))\n            for proc in dead_processes:\n                self.remove_consumer_process(*proc)\n        self._active_cache = time.time(), active_processes\n        return active_processes", "response": "Return a list of all active processes and dead ones."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the stats data for our process level data.", "response": "def calculate_stats(self, data):\n        \"\"\"Calculate the stats data for our process level data.\n\n        :param data: The collected stats data to report on\n        :type data: dict\n\n        \"\"\"\n        timestamp = data['timestamp']\n        del data['timestamp']\n\n        # Iterate through the last poll results\n        stats = self.consumer_stats_counter()\n        consumer_stats = dict()\n        for name in data.keys():\n            consumer_stats[name] = self.consumer_stats_counter()\n            consumer_stats[name]['processes'] = self.process_count(name)\n            for proc in data[name].keys():\n                for key in stats:\n                    value = data[name][proc]['counts'].get(key, 0)\n                    stats[key] += value\n                    consumer_stats[name][key] += value\n\n        # Return a data structure that can be used in reporting out the stats\n        stats['processes'] = len(self.active_processes())\n        return {\n            'last_poll': timestamp,\n            'consumers': consumer_stats,\n            'process_data': data,\n            'counts': stats\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the process cache can be used.", "response": "def can_use_process_cache(self, use_cache):\n        \"\"\"Returns True if the process cache can be used\n\n        :param bool use_cache: Override the logic to force non-cached values\n        :rtype: bool\n\n        \"\"\"\n        return (use_cache and\n                self._active_cache and\n                self._active_cache[0] > (time.time() - self.poll_interval))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks for the minimum consumer process levels and start up new processes needed.", "response": "def check_process_counts(self):\n        \"\"\"Check for the minimum consumer process levels and start up new\n        processes needed.\n\n        \"\"\"\n        LOGGER.debug('Checking minimum consumer process levels')\n        for name in self.consumers:\n            processes_needed = self.process_spawn_qty(name)\n            if processes_needed:\n                LOGGER.info('Need to spawn %i processes for %s',\n                            processes_needed, name)\n                self.start_processes(name, processes_needed)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collect_results(self, data_values):\n        self.last_poll_results['timestamp'] = self.poll_data['timestamp']\n\n        # Get the name and consumer name and remove it from what is reported\n        consumer_name = data_values['consumer_name']\n        del data_values['consumer_name']\n        process_name = data_values['name']\n        del data_values['name']\n\n        # Add it to our last poll global data\n        if consumer_name not in self.last_poll_results:\n            self.last_poll_results[consumer_name] = dict()\n        self.last_poll_results[consumer_name][process_name] = data_values\n\n        # Calculate the stats\n        self.stats = self.calculate_stats(self.last_poll_results)", "response": "Collect the results from the consumers polled and process it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consumer_stats_counter():\n        return {\n            process.Process.ERROR: 0,\n            process.Process.PROCESSED: 0,\n            process.Process.REDELIVERED: 0\n        }", "response": "Return a new consumer stats counter instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the process object for the specified consumer and process name.", "response": "def get_consumer_process(self, consumer, name):\n        \"\"\"Get the process object for the specified consumer and process name.\n\n        :param str consumer: The consumer name\n        :param str name: The process name\n        :returns: multiprocessing.Process\n\n        \"\"\"\n        return self.consumers[consumer].processes.get(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the consumers config possibly filtering the config if only or qty is set.", "response": "def get_consumer_cfg(config, only, qty):\n        \"\"\"Get the consumers config, possibly filtering the config if only\n        or qty is set.\n\n        :param config: The consumers config section\n        :type config: helper.config.Config\n        :param str only: When set, filter to run only this consumer\n        :param int qty: When set, set the consumer qty to this value\n        :rtype: dict\n\n        \"\"\"\n        consumers = dict(config.application.Consumers or {})\n        if only:\n            for key in list(consumers.keys()):\n                if key != only:\n                    del consumers[key]\n            if qty:\n                consumers[only]['qty'] = qty\n        return consumers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck to see if the specified process is dead.", "response": "def is_dead(self, proc, name):\n        \"\"\"Checks to see if the specified process is dead.\n\n        :param psutil.Process proc: The process to check\n        :param str name: The name of consumer\n        :rtype: bool\n\n        \"\"\"\n        LOGGER.debug('Checking %s (%r)', name, proc)\n        try:\n            status = proc.status()\n        except psutil.NoSuchProcess:\n            LOGGER.debug('NoSuchProcess: %s (%r)', name, proc)\n            return True\n\n        LOGGER.debug('Process %s (%s) status: %r (Unresponsive Count: %s)',\n                     name, proc.pid, status, self.unresponsive[name])\n        if status in _PROCESS_RUNNING:\n            return False\n        elif status == psutil.STATUS_ZOMBIE:\n            try:\n                proc.wait(0.1)\n            except psutil.TimeoutExpired:\n                pass\n            try:\n                proc.terminate()\n                status = proc.status()\n            except psutil.NoSuchProcess:\n                LOGGER.debug('NoSuchProcess: %s (%r)', name, proc)\n                return True\n        return status in _PROCESS_STOPPED_OR_DEAD"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nkills all active processes.", "response": "def kill_processes(self):\n        \"\"\"Gets called on shutdown by the timer when too much time has gone by,\n        calling the terminate method instead of nicely asking for the consumers\n        to stop.\n\n        \"\"\"\n        LOGGER.critical('Max shutdown exceeded, forcibly exiting')\n        processes = self.active_processes(False)\n        while processes:\n            for proc in self.active_processes(False):\n                if int(proc.pid) != int(os.getpid()):\n                    LOGGER.warning('Killing %s (%s)', proc.name, proc.pid)\n                    try:\n                        os.kill(int(proc.pid), signal.SIGKILL)\n                    except OSError:\n                        pass\n                else:\n                    LOGGER.warning('Cowardly refusing kill self (%s, %s)',\n                                   proc.pid, os.getpid())\n            time.sleep(0.5)\n            processes = self.active_processes(False)\n\n        LOGGER.info('Killed all children')\n        return self.set_state(self.STATE_STOPPED)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noutput the stats to the LOGGER.", "response": "def log_stats(self):\n        \"\"\"Output the stats to the LOGGER.\"\"\"\n        if not self.stats.get('counts'):\n            if self.consumers:\n                LOGGER.info('Did not receive any stats data from children')\n            return\n\n        if self.poll_data['processes']:\n            LOGGER.warning('%i process(es) did not respond with stats: %r',\n                           len(self.poll_data['processes']),\n                           self.poll_data['processes'])\n\n        if self.stats['counts']['processes'] > 1:\n            LOGGER.info('%i consumers processed %i messages with %i errors',\n                        self.stats['counts']['processes'],\n                        self.stats['counts']['processed'],\n                        self.stats['counts']['failed'])\n\n        for key in self.stats['consumers'].keys():\n            LOGGER.info('%i %s %s processed %i messages with %i errors',\n                        self.stats['consumers'][key]['processes'], key,\n                        self.consumer_keyword(self.stats['consumers'][key]),\n                        self.stats['consumers'][key]['processed'],\n                        self.stats['consumers'][key]['failed'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new consumer dict for the given name and configuration.", "response": "def new_consumer(self, config, consumer_name):\n        \"\"\"Return a consumer dict for the given name and configuration.\n\n        :param dict config: The consumer configuration\n        :param str consumer_name: The consumer name\n        :rtype: dict\n\n        \"\"\"\n        return Consumer(0,\n                        dict(),\n                        config.get('qty', self.DEFAULT_CONSUMER_QTY),\n                        config.get('queue', consumer_name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new_process(self, consumer_name):\n        process_name = '%s-%s' % (consumer_name,\n                                  self.new_process_number(consumer_name))\n        kwargs = {\n            'config': self.config.application,\n            'consumer_name': consumer_name,\n            'profile': self.profile,\n            'daemon': False,\n            'stats_queue': self.stats_queue,\n            'logging_config': self.config.logging\n        }\n        return process_name, process.Process(name=process_name, kwargs=kwargs)", "response": "Create a new process instance for the given consumer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nincrementing the counter for the process id number for a given consumer name.", "response": "def new_process_number(self, name):\n        \"\"\"Increment the counter for the process id number for a given consumer\n        configuration.\n\n        :param str name: Consumer name\n        :rtype: int\n\n        \"\"\"\n        self.consumers[name].last_proc_num += 1\n        return self.consumers[name].last_proc_num"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninvoke when a child sends up a SIGCHLD signal.", "response": "def on_sigchld(self, _signum, _unused_frame):\n        \"\"\"Invoked when a child sends up an SIGCHLD signal.\n\n        :param int _signum: The signal that was invoked\n        :param frame _unused_frame: The frame that was interrupted\n\n        \"\"\"\n        LOGGER.info('SIGCHLD received from child')\n        if not self.active_processes(False):\n            LOGGER.info('Stopping with no active processes and child error')\n            signal.setitimer(signal.ITIMER_REAL, 0, 0)\n            self.set_state(self.STATE_STOPPED)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling by the Poll timer signal.", "response": "def on_timer(self, _signum, _unused_frame):\n        \"\"\"Invoked by the Poll timer signal.\n\n        :param int _signum: The signal that was invoked\n        :param frame _unused_frame: The frame that was interrupted\n\n        \"\"\"\n        if self.is_shutting_down:\n            LOGGER.debug('Polling timer fired while shutting down')\n            return\n        if not self.polled:\n            self.poll()\n            self.polled = True\n            self.set_timer(5)  # Wait 5 seconds for results\n        else:\n            self.polled = False\n            self.poll_results_check()\n            self.set_timer(self.poll_interval)  # Wait poll interval duration\n\n            # If stats logging is enabled, log the stats\n            if self.log_stats_enabled:\n                self.log_stats()\n\n            # Increment the unresponsive children\n            for proc_name in self.poll_data['processes']:\n                self.unresponsive[proc_name] += 1\n\n            # Remove counters for processes that came back to life\n            for proc_name in list(self.unresponsive.keys()):\n                if proc_name not in self.poll_data['processes']:\n                    del self.unresponsive[proc_name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef poll(self):\n        self.set_state(self.STATE_ACTIVE)\n\n        # If we don't have any active consumers, spawn new ones\n        if not self.total_process_count:\n            LOGGER.debug('Did not find any active consumers in poll')\n            return self.check_process_counts()\n\n        # Start our data collection dict\n        self.poll_data = {'timestamp': time.time(), 'processes': list()}\n\n        # Iterate through all of the consumers\n        for proc in list(self.active_processes()):\n            if proc == multiprocessing.current_process():\n                continue\n\n            # Send the profile signal\n            os.kill(int(proc.pid), signal.SIGPROF)\n            self.poll_data['processes'].append(proc.name)\n\n        # Check if we need to start more processes\n        self.check_process_counts()", "response": "Start the poll process by invoking the get_stats method of the the\n            consumers."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the polling results by checking to see if the stats queue is empty and collect stats.", "response": "def poll_results_check(self):\n        \"\"\"Check the polling results by checking to see if the stats queue is\n        empty. If it is not, try and collect stats. If it is set a timer to\n        call ourselves in _POLL_RESULTS_INTERVAL.\n\n        \"\"\"\n        if not self.consumers:\n            LOGGER.debug('Skipping poll results check, no consumers')\n            return\n\n        LOGGER.debug('Checking for poll results')\n        while True:\n            try:\n                stats = self.stats_queue.get(False)\n            except queue.Empty:\n                break\n            try:\n                self.poll_data['processes'].remove(stats['name'])\n            except ValueError:\n                pass\n            self.collect_results(stats)\n\n        if self.poll_data['processes']:\n            LOGGER.warning('Did not receive results from %r',\n                           self.poll_data['processes'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_spawn_qty(self, name):\n        return self.consumers[name].qty - self.process_count(name)", "response": "Return the number of processes spawned for the given consumer name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove all details for the specified consumer and process name.", "response": "def remove_consumer_process(self, consumer, name):\n        \"\"\"Remove all details for the specified consumer and process name.\n\n        :param str consumer: The consumer name\n        :param str name: The process name\n\n        \"\"\"\n        my_pid = os.getpid()\n        if name in self.consumers[consumer].processes.keys():\n            child = self.consumers[consumer].processes[name]\n            try:\n                alive = child.is_alive()\n            except AssertionError:\n                LOGGER.debug('Tried to test non-child process (%r to %r)',\n                             os.getpid(), child.pid)\n            else:\n                if child.pid == my_pid:\n                    LOGGER.debug('Child has my pid? %r, %r', my_pid, child.pid)\n                elif alive:\n                    try:\n                        child.terminate()\n                    except OSError:\n                        pass\n            try:\n                del self.consumers[consumer].processes[name]\n            except KeyError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        self.set_state(self.STATE_ACTIVE)\n        self.setup_consumers()\n\n        # Set the SIGCHLD handler for child creation errors\n        signal.signal(signal.SIGCHLD, self.on_sigchld)\n\n        # Set the SIGALRM handler for poll interval\n        signal.signal(signal.SIGALRM, self.on_timer)\n\n        # Kick off the poll timer\n        signal.setitimer(signal.ITIMER_REAL, self.poll_interval, 0)\n\n        # Loop for the lifetime of the app, pausing for a signal to pop up\n        while self.is_running:\n            if not self.is_sleeping:\n                self.set_state(self.STATE_SLEEPING)\n            signal.pause()\n\n        # Note we're exiting run\n        LOGGER.info('Exiting Master Control Program')", "response": "This method is called by the Master Control Program when it is ready to start running."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the process name for the top level process so that it shows up in logs in a more trackable fashion.", "response": "def set_process_name():\n        \"\"\"Set the process name for the top level process so that it shows up\n        in logs in a more trackable fashion.\n\n        \"\"\"\n        proc = multiprocessing.current_process()\n        for offset in range(0, len(sys.argv)):\n            if sys.argv[offset] == '-c':\n                name = sys.argv[offset + 1].split('/')[-1]\n                proc.name = name.split('.')[0]\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup_consumers(self):\n        if not self.consumer_cfg:\n            LOGGER.warning('No consumers are configured')\n        for name in self.consumer_cfg.keys():\n            self.consumers[name] = self.new_consumer(\n                self.consumer_cfg[name], name)\n            self.start_processes(name, self.consumers[name].qty)", "response": "Iterate through each consumer in the configuration and kick off the minimal amount of processes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts a new consumer process for the given consumer name.", "response": "def start_process(self, name):\n        \"\"\"Start a new consumer process for the given consumer name\n\n        :param str name: The consumer name\n\n        \"\"\"\n        process_name, proc = self.new_process(name)\n        LOGGER.info('Spawning %s process for %s', process_name, name)\n\n        # Append the process to the consumer process list\n        self.consumers[name].processes[process_name] = proc\n\n        # Start the process\n        try:\n            proc.start()\n        except IOError as error:\n            LOGGER.critical('Failed to start %s for %s: %r',\n                            process_name, name, error)\n            del self.consumers[name].process[process_name]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_processes(self, name, quantity):\n        [self.start_process(name) for i in range(0, quantity or 0)]", "response": "Start the specified quantity of consumer processes for the given consumer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop_processes(self):\n        self.set_state(self.STATE_SHUTTING_DOWN)\n        LOGGER.info('Stopping consumer processes')\n\n        signal.signal(signal.SIGABRT, signal.SIG_IGN)\n        signal.signal(signal.SIGALRM, signal.SIG_IGN)\n        signal.signal(signal.SIGCHLD, signal.SIG_IGN)\n        signal.signal(signal.SIGPROF, signal.SIG_IGN)\n        signal.setitimer(signal.ITIMER_REAL, 0, 0)\n\n        # Send SIGABRT\n        LOGGER.info('Sending SIGABRT to active children')\n        for proc in multiprocessing.active_children():\n            if int(proc.pid) != os.getpid():\n                try:\n                    os.kill(int(proc.pid), signal.SIGABRT)\n                except OSError:\n                    pass\n\n        # Wait for them to finish up to MAX_SHUTDOWN_WAIT\n        for iteration in range(0, self.MAX_SHUTDOWN_WAIT):\n            processes = len(self.active_processes(False))\n            if not processes:\n                break\n\n            LOGGER.info('Waiting on %i active processes to shut down (%i/%i)',\n                        processes, iteration, self.MAX_SHUTDOWN_WAIT)\n            try:\n                time.sleep(0.5)\n            except KeyboardInterrupt:\n                break\n\n        if len(self.active_processes(False)):\n            self.kill_processes()\n\n        LOGGER.debug('All consumer processes stopped')\n        self.set_state(self.STATE_STOPPED)", "response": "Iterate through all of the consumer processes shutting them down."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds command line options to the parser", "response": "def add_parser_arguments():\n    \"\"\"Add options to the parser\"\"\"\n    argparser = parser.get()\n    argparser.add_argument('-P', '--profile',\n                           action='store',\n                           default=None,\n                           dest='profile',\n                           help='Profile the consumer modules, specifying '\n                           'the output directory.')\n    argparser.add_argument('-o', '--only',\n                           action='store',\n                           default=None,\n                           dest='consumer',\n                           help='Only run the consumer specified')\n    argparser.add_argument('-p', '--prepend-path',\n                           action='store',\n                           default=None,\n                           dest='prepend_path',\n                           help='Prepend the python path with the value.')\n    argparser.add_argument('-q', '--qty',\n                           action='store',\n                           type=int,\n                           default=None,\n                           dest='quantity',\n                           help='Run the specified quantity of consumer '\n                           'processes when used in conjunction with -o')\n    argparser.add_argument('--version', action='version',\n                           version='%(prog)s {}'.format(__version__))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an instance of the MasterControlProgram.", "response": "def _master_control_program(self):\n        \"\"\"Return an instance of the MasterControlProgram.\n\n        :rtype: rejected.mcp.MasterControlProgram\n\n        \"\"\"\n        return mcp.MasterControlProgram(self.config,\n                                        consumer=self.args.consumer,\n                                        profile=self.args.profile,\n                                        quantity=self.args.quantity)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(self):\n        # If the app was invoked to specified to prepend the path, do so now\n        if self.args.prepend_path:\n            self._prepend_python_path(self.args.prepend_path)", "response": "Continue the run process blocking on MasterControlProgram. run"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop the MCP and child processes cleanly", "response": "def stop(self):\n        \"\"\"Shutdown the MCP and child processes cleanly\"\"\"\n        LOGGER.info('Shutting down controller')\n        self.set_state(self.STATE_STOP_REQUESTED)\n\n        # Clear out the timer\n        signal.setitimer(signal.ITIMER_PROF, 0, 0)\n\n        self._mcp.stop_processes()\n\n        if self._mcp.is_running:\n            LOGGER.info('Waiting up to 3 seconds for MCP to shut things down')\n            signal.setitimer(signal.ITIMER_REAL, 3, 0)\n            signal.pause()\n            LOGGER.info('Post pause')\n\n        # Force MCP to stop\n        if self._mcp.is_running:\n            LOGGER.warning('MCP is taking too long, requesting process kills')\n            self._mcp.stop_processes()\n            del self._mcp\n        else:\n            LOGGER.info('MCP exited cleanly')\n\n        # Change our state\n        self._stopped()\n        LOGGER.info('Shutdown complete')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        self.setup()\n        self._mcp = self._master_control_program()\n        try:\n            self._mcp.run()\n        except KeyboardInterrupt:\n            LOGGER.info('Caught CTRL-C, shutting down')\n        except Exception:\n            exc_info = sys.exc_info()\n            kwargs = {'logger': 'rejected.controller'}\n            LOGGER.debug('Sending exception to sentry: %r', kwargs)\n            if self._sentry_client:\n                self._sentry_client.captureException(exc_info, **kwargs)\n            raise\n        if self.is_running:\n            self.stop()", "response": "Run the rejected Application"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_confirmation_future(self, exchange, routing_key, properties,\n                                future):\n        \"\"\"Invoked by :class:`~rejected.consumer.Consumer` when publisher\n        confirmations are enabled, containing a stack of futures to finish,\n        by delivery tag, when RabbitMQ confirms the delivery.\n\n        :param str exchange: The exchange the message was published to\n        :param str routing_key: The routing key that was used\n        :param properties: AMQP message properties of published message\n        :type properties: pika.spec.Basic.Properties\n        :param tornado.concurrent.Future future: The future to resolve\n\n        \"\"\"\n        self.delivery_tag += 1\n        self.published_messages.append(\n            Published(self.delivery_tag, properties.message_id, exchange,\n                      routing_key, future))", "response": "Invoked by RabbitMQ when publisher\n        confirmations are enabled."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(self):\n        self.set_state(self.STATE_CONNECTING)\n        self.handle = tornado_connection.TornadoConnection(\n            self._connection_parameters,\n            on_open_callback=self.on_open,\n            on_open_error_callback=self.on_open_error,\n            stop_ioloop_on_close=False,\n            custom_ioloop=self.io_loop)", "response": "Create the low - level AMQP connection to RabbitMQ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shutdown(self):\n        if self.is_shutting_down:\n            self.logger.debug('Already shutting down')\n            return\n\n        self.set_state(self.STATE_SHUTTING_DOWN)\n        self.logger.debug('Shutting down connection')\n        if not self.is_active:\n            return self.channel.close()\n        self.logger.debug('Sending a Basic.Cancel to RabbitMQ')\n        self.channel.basic_cancel(self.on_consumer_cancelled,\n                                  self.consumer_tag)", "response": "Start the connection shutdown process cancelling any active\n        consuming and closing the channel if the connection is not active."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvoking when the connection is opened", "response": "def on_open(self, _handle):\n        \"\"\"Invoked when the connection is opened\n\n        :type _handle: pika.adapters.tornado_connection.TornadoConnection\n\n        \"\"\"\n        self.logger.debug('Connection opened')\n        self.handle.add_on_connection_blocked_callback(self.on_blocked)\n        self.handle.add_on_connection_unblocked_callback(self.on_unblocked)\n        self.handle.add_on_close_callback(self.on_closed)\n        self.handle.channel(self.on_channel_open)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_channel_closed(self, _channel, reply_code, reply_text):\n        self.logger.warning('Channel was closed: (%s) %s - %s',\n                            reply_code, reply_text, self.state_description)\n        if not (400 <= reply_code <= 499):\n            self.set_state(self.STATE_CLOSED)\n            return\n\n        if self.is_shutting_down:\n            self.logger.debug('Closing connection')\n            self.handle.close()\n            return\n\n        self.set_state(self.STATE_CONNECTING)\n        self.handle.channel(self.on_channel_open)\n\n        pending = self.pending_confirmations()\n        if not pending:\n            raise errors.RabbitMQException(self.name, reply_code, reply_text)\n        pending[0][1].future.set_exception(\n                errors.RabbitMQException(self.name, reply_code, reply_text))", "response": "Invoked by pika when RabbitMQ unexpectedly closes the channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef consume(self, queue_name, no_ack, prefetch_count):\n        if self.state == self.STATE_ACTIVE:\n            self.logger.debug('%s already consuming', self.name)\n            return\n        self.set_state(self.STATE_ACTIVE)\n        self.channel.basic_qos(self.on_qos_set, 0, prefetch_count, False)\n        self.channel.basic_consume(\n            consumer_callback=self.on_delivery, queue=queue_name,\n            no_ack=no_ack, consumer_tag=self.consumer_tag)", "response": "Consume messages from RabbitMQ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninvoking by pika when a Basic. Cancel or Basic. CancelOk is received.", "response": "def on_consumer_cancelled(self, _frame):\n        \"\"\"Invoked by pika when a ``Basic.Cancel`` or ``Basic.CancelOk``\n        is received.\n\n        :param _frame: The Basic.Cancel or Basic.CancelOk frame\n        :type _frame: pika.frame.Frame\n\n        \"\"\"\n        self.logger.debug('Consumer has been cancelled')\n        if self.is_shutting_down:\n            self.channel.close()\n        else:\n            self.set_state(self.STATE_CONNECTED)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_confirmation(self, frame):\n        delivered = frame.method.NAME.split('.')[1].lower() == 'ack'\n        self.logger.debug('Received publisher confirmation (Delivered: %s)',\n                          delivered)\n        if frame.method.multiple:\n            for index in range(self.last_confirmation + 1,\n                               frame.method.delivery_tag):\n                self.confirm_delivery(index, delivered)\n        self.confirm_delivery(frame.method.delivery_tag, delivered)\n        self.last_confirmation = frame.method.delivery_tag", "response": "Invoked by pika when RabbitMQ responds to a Basic. Publish RPC\n        command passing in either a Basic. Ack or Basic. Nack frame with\n        the delivery tag of the message that was published."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninvokes by RabbitMQ when a message is confirmed by a Basic. Ack .", "response": "def confirm_delivery(self, delivery_tag, delivered):\n        \"\"\"Invoked by RabbitMQ when it is confirming delivery via a Basic.Ack\n\n        :param\n        :param int delivery_tag: The message # being confirmed\n        :param bool delivered: Was the message delivered\n\n        \"\"\"\n        for offset, msg in self.pending_confirmations():\n            if delivery_tag == msg.delivery_tag:\n                self.published_messages[offset].future.set_result(delivered)\n                return\n        for msg in self.published_messages:\n            if msg.delivery_tag == delivery_tag and msg.future.done():\n                return\n        self.logger.warning('Attempted to confirm publish without future: %r',\n                            delivery_tag)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninvoking by pika when RabbitMQ delivers a message from a queue.", "response": "def on_delivery(self, channel, method, properties, body):\n        \"\"\"Invoked by pika when RabbitMQ delivers a message from a queue.\n\n        :param channel: The channel the message was delivered on\n        :type channel: pika.channel.Channel\n        :param method: The AMQP method frame\n        :type method: pika.frame.Frame\n        :param properties: The AMQP message properties\n        :type properties: pika.spec.Basic.Properties\n        :param bytes body: The message body\n\n        \"\"\"\n        self.callbacks.on_delivery(\n            self.name, channel, method, properties, body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvoke by RabbitMQ when it returns a message that was published.", "response": "def on_return(self, channel, method, properties, body):\n        \"\"\"Invoked by RabbitMQ when it returns a message that was published.\n\n        :param channel: The channel the message was delivered on\n        :type channel: pika.channel.Channel\n        :param method: The AMQP method frame\n        :type method: pika.frame.Frame\n        :param properties: The AMQP message properties\n        :type properties: pika.spec.Basic.Properties\n        :param bytes body: The message body\n\n        \"\"\"\n        pending = self.pending_confirmations()\n        if not pending:  # Exit early if there are no pending messages\n            self.logger.warning('RabbitMQ returned message %s and no pending '\n                                'messages are unconfirmed',\n                                utils.message_info(method.exchange,\n                                                   method.routing_key,\n                                                   properties))\n            return\n\n        self.logger.warning('RabbitMQ returned message %s: (%s) %s',\n                            utils.message_info(method.exchange,\n                                               method.routing_key, properties),\n                            method.reply_code, method.reply_text)\n\n        # Try and match the exact message or first message published that\n        # matches the exchange and routing key\n        for offset, msg in pending:\n            if (msg.message_id == properties.message_id or\n                    (msg.exchange == method.exchange and\n                     msg.routing_key == method.routing_key)):\n                self.published_messages[offset].future.set_result(False)\n                return\n\n        # Handle the case where we only can go on message ordering\n        self.published_messages[0].future.set_result(False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pending_confirmations(self):\n        return sorted([(idx, msg)\n                       for idx, msg in enumerate(self.published_messages)\n                       if not msg.future.done()],\n                      key=lambda x: x[1].delivery_tag)", "response": "Return all published messages that have yet to be acked nacked or\n        returned."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _connection_parameters(self):\n        return pika.ConnectionParameters(\n            self.config.get('host', 'localhost'),\n            self.config.get('port', 5672),\n            self.config.get('vhost', '/'),\n            pika.PlainCredentials(\n                self.config.get('user', 'guest'),\n                self.config.get('password', self.config.get('pass', 'guest'))),\n            ssl=self.config.get('ssl', False),\n            frame_max=self.config.get('frame_max', spec.FRAME_MAX_SIZE),\n            socket_timeout=self.config.get('socket_timeout', 10),\n            heartbeat_interval=self.config.get(\n                'heartbeat_interval', self.HB_INTERVAL))", "response": "Return a pika. ConnectionParameters object for a pika connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npublishing a message to RabbitMQ on the same channel.", "response": "def publish_message(self,\n                        exchange,\n                        routing_key,\n                        properties,\n                        body,\n                        no_serialization=False,\n                        no_encoding=False,\n                        channel=None,\n                        connection=None):\n        \"\"\"Publish a message to RabbitMQ on the same channel the original\n        message was received on.\n\n        By default, if you pass a non-string object to the body and the\n        properties have a supported ``content_type`` set, the body will be\n        auto-serialized in the specified ``content_type``.\n\n        If the properties do not have a timestamp set, it will be set to the\n        current time.\n\n        If you specify a ``content_encoding`` in the properties and the\n        encoding is supported, the body will be auto-encoded.\n\n        Both of these behaviors can be disabled by setting\n        ``no_serialization`` or ``no_encoding`` to ``True``.\n\n        If you pass an unsupported content-type or content-encoding when using\n        the auto-serialization and auto-encoding features, a :exc:`ValueError`\n        will be raised.\n\n        .. versionchanged:: 4.0.0\n           The method returns a :py:class:`~tornado.concurrent.Future` if\n           `publisher confirmations <https://www.rabbitmq.com/confirms.html>`_\n           are enabled on for the connection. In addition, The ``channel``\n           parameter is deprecated and will be removed in a future release.\n\n        :param str exchange: The exchange to publish to\n        :param str routing_key: The routing key to publish with\n        :param dict properties: The message properties\n        :param mixed body: The message body to publish\n        :param bool no_serialization: Turn off auto-serialization of the body\n        :param bool no_encoding: Turn off auto-encoding of the body\n        :param str channel: **Deprecated in 4.0.0** Specify the connection\n            parameter instead.\n        :param str connection: The connection to use. If it is not\n            specified, the channel that the message was delivered on is used.\n        :rtype: tornado.concurrent.Future or None\n        :raises: ValueError\n\n        \"\"\"\n        # Auto-serialize the content if needed\n        is_string = (isinstance(body, str) or isinstance(body, bytes)\n                     or isinstance(body, unicode))\n        if not no_serialization and not is_string and \\\n                properties.get('content_type'):\n            body = self._serialize(\n                body, headers.parse_content_type(properties['content_type']))\n\n        # Auto-encode the message body if needed\n        if not no_encoding and \\\n                properties.get('content_encoding') in self._CODEC_MAP.keys():\n            body = self._compress(\n                body, self._CODEC_MAP[properties['content_encoding']])\n\n        return super(SmartConsumer, self).publish_message(\n            exchange, routing_key, properties, body, channel or connection)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef body(self):\n        if self._message_body:  # If already set, return it\n            return self._message_body\n        self._message_body = self._maybe_decompress_body()\n        self._message_body = self._maybe_deserialize_body()\n        return self._message_body", "response": "Return the message body unencoded if needed deserialized if possible."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compress(self, value, module_name):\n        self.logger.debug('Decompressing with %s', module_name)\n        if not isinstance(value, bytes):\n            value = value.encode('utf-8')\n        return self._maybe_import(module_name).compress(value)", "response": "Compress the value passed in using the named compression module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a list of dicts and return it as a CSV value. The .. versionchanged :: 4. 0. 0", "response": "def _dump_csv(self, rows):\n        \"\"\"Take a list of dicts and return it as a CSV value. The\n\n        .. versionchanged:: 4.0.0\n\n        :param list rows: A list of lists to return as a CSV\n        :rtype: str\n\n        \"\"\"\n        self.logger.debug('Writing %r', rows)\n        csv = self._maybe_import('csv')\n        buff = io.StringIO() if _PYTHON3 else io.BytesIO()\n        writer = csv.DictWriter(\n            buff,\n            sorted(set([k for r in rows for k in r.keys()])),\n            dialect='excel')\n        writer.writeheader()\n        writer.writerows(rows)\n        value = buff.getvalue()\n        buff.close()\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a class : csv. DictReader instance for value passed in.", "response": "def _load_csv(self, value):\n        \"\"\"Return a class:`csv.DictReader` instance for value passed in.\n\n        :param str value: The CSV value\n        :rtype: csv.DictReader\n\n        \"\"\"\n        csv = self._maybe_import('csv')\n        buff = io.StringIO() if _PYTHON3 else io.BytesIO()\n        buff.write(value)\n        buff.seek(0)\n        dialect = csv.Sniffer().sniff(buff.read(1024))\n        buff.seek(0)\n        return csv.DictReader(buff, dialect=dialect)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _maybe_decode(self, value, encoding='utf-8'):\n        if _PYTHON3 and isinstance(value, bytes):\n            try:\n                return value.decode(encoding)\n            except Exception as err:\n                self.logger.exception('Error decoding value: %s', err)\n                raise MessageException(\n                    str(err), 'decoding-{}'.format(encoding))\n        return value", "response": "If a bytes object is passed in decode it using the specified encoding to turn it into a str instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _maybe_decompress_body(self):\n        if self.content_encoding:\n            if self.content_encoding in self._CODEC_MAP.keys():\n                module_name = self._CODEC_MAP[self.content_encoding]\n                self.logger.debug('Decompressing with %s', module_name)\n                module = self._maybe_import(module_name)\n                return module.decompress(self._message.body)\n            self.logger.debug('Unsupported content-encoding: %s',\n                              self.content_encoding)\n        return self._message.body", "response": "Attempt to decompress the message body passed in using the named\nMimeType compression module if specified."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to deserialize the message body based upon the content - type.", "response": "def _maybe_deserialize_body(self):\n        \"\"\"Attempt to deserialize the message body based upon the content-type.\n\n        :rtype: mixed\n\n        \"\"\"\n        if not self.content_type:\n            return self._message_body\n        ct = headers.parse_content_type(self.content_type)\n        key = '{}/{}'.format(ct.content_type, ct.content_subtype)\n        if key not in self._SERIALIZATION_MAP:\n            if key not in self._IGNORE_TYPES:\n                self.logger.debug('Unsupported content-type: %s',\n                                  self.content_type)\n            return self._message_body\n        elif not self._SERIALIZATION_MAP[key].get('enabled', True):\n            self.logger.debug('%s is not enabled in the serialization map',\n                              key)\n            return self._message_body\n        value = self._message_body\n        if not self._SERIALIZATION_MAP[key].get('binary'):\n            value = self._maybe_decode(\n                self._message_body, ct.parameters.get('charset', 'utf-8'))\n        return self._maybe_invoke_serialization(value, 'load', key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the number of messages to process before invoking gc. collect.", "response": "def collection_cycle(self, value):\n        \"\"\"Set the number of messages to process before invoking ``gc.collect``\n\n        :param int value: Cycle size\n\n        \"\"\"\n        if value is not None:\n            self._collection_cycle = value\n            self._cycles_left = min(self._cycles_left, self._collection_cycle)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_finish(self, exc=None):\n        super(GarbageCollector, self).on_finish(exc)\n        self._cycles_left -= 1\n        if self._cycles_left <= 0:\n            num_collected = gc.collect()\n            self._cycles_left = self.collection_cycle\n            LOGGER.debug('garbage collection run, %d objects evicted',\n                         num_collected)", "response": "Used to initiate the garbage collection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef io_loop(self):\n        if self._message and self._message.connection:\n            return self._connections[self._message.connection].io_loop", "response": "Access the tornado. ioloop. IOLoop instance for the current message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nraise an exception if the given app setting is not defined.", "response": "def require_setting(self, name, feature='this feature'):\n        \"\"\"Raises an exception if the given app setting is not defined.\n\n        As a generalization, this method should called from a Consumer's\n        :py:meth:`~rejected.consumer.Consumer.initialize` method. If a required\n        setting is not found, this method will cause the\n        consumer to shutdown prior to receiving any messages from RabbitMQ.\n\n        :param name: The parameter name\n        :type name: :class:`str`\n        :param feature: A friendly name for the setting feature\n        :type feature: :class:`str`\n        :raises: :exc:`~rejected.errors.ConfigurationException`\n\n        \"\"\"\n        if name not in self.settings:\n            raise ConfigurationException(\n                \"You must define the '{}' setting in your \"\n                \"application to use {}\".format(name, feature))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npublish a message to RabbitMQ on the same channel.", "response": "def publish_message(self,\n                        exchange,\n                        routing_key,\n                        properties,\n                        body,\n                        connection=None):\n        \"\"\"Publish a message to RabbitMQ on the same channel the original\n        message was received on. If\n        `publisher confirmations <https://www.rabbitmq.com/confirms.html>`_\n        are enabled, the method will return a\n        :class:`~tornado.concurrent.Future` that will resolve a :class:`bool`\n        that indicates if the publishing was successful.\n\n        .. versionchanged:: 4.0.0\n            Return a :class:`~tornado.concurrent.Future` if\n            `publisher confirmations <https://www.rabbitmq.com/confirms.html>`_\n            are enabled. Removed the ``channel`` parameter.\n\n        :param exchange: The exchange to publish to\n        :type exchange: :class:`str`\n        :param routing_key: The routing key to publish with\n        :type routing_key: :class:`str`\n        :param properties: The message properties\n        :type properties: :class:`dict`\n        :param body: The message body\n        :type body: :class:`bytes` or :class:`str`\n        :param connection: The connection to use. If it is not\n            specified, the channel that the message was delivered on is used.\n        :type connection: :class:`str`\n\n        :rtype: :class:`tornado.concurrent.Future` or :data:`None`\n\n        \"\"\"\n        conn = self._publish_connection(connection)\n        self.logger.debug('Publishing message to %s:%s (%s)', exchange,\n                          routing_key, conn.name)\n        basic_properties = self._get_pika_properties(properties)\n        with self._measurement.track_duration('publish.{}.{}'.format(\n                exchange, routing_key)):\n            conn.channel.basic_publish(\n                exchange=exchange,\n                routing_key=routing_key,\n                properties=basic_properties,\n                body=body,\n                mandatory=conn.publisher_confirmations)\n            return self._publisher_confirmation_future(\n                conn.name, exchange, routing_key, basic_properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rpc_reply(self,\n                  body,\n                  properties=None,\n                  exchange=None,\n                  reply_to=None,\n                  connection=None):\n        \"\"\"Reply to the message that is currently being processed.\n\n        .. versionadded:: 4.0.0\n\n        If the exchange is not specified, the exchange of the message that is\n        currently being processed by the Consumer will be used.\n\n        If ``reply_to`` is not provided, it will attempt to use the\n        ``reply_to`` property of the message that is currently being processed\n        by the consumer. If both are not set, a :exc:`ValueError` is raised.\n\n        If any of the following message properties are not provided, they will\n        automatically be assigned:\n\n          - ``app_id``\n          - ``correlation_id``\n          - ``message_id``\n          - ``timestamp``\n\n        The ``correlation_id`` will only be automatically assigned if the\n        original message provided a ``message_id``.\n\n        If the connection that the message is being published on has publisher\n        confirmations enabled, a :py:class:`~tornado.concurrent.Future` is\n        returned.\n\n        :param body: The message body\n        :type body: :class:`bytes` or :class:`str`\n        :param properties: The AMQP properties to use for the reply message\n        :type properties: :class:`dict` or :data:`None`\n        :param exchange: The exchange to publish to. Defaults to the exchange\n            of the message that is currently being processed by the Consumer.\n        :type exchange: :class:`str` or :data:`None`\n        :param reply_to: The routing key to send the reply to. Defaults to the\n            reply_to property of the message that is currently being processed\n            by the Consumer. If neither are set, a :exc:`ValueError` is\n            raised.\n        :type reply_to: class:`str` or :data:`None`\n        :param connection: The connection to use. If it is not\n            specified, the channel that the message was delivered on is used.\n        :type connection: :class:`str`\n        :rtype: :class:`tornado.concurrent.Future` or :data:`None`\n        :raises: :exc:`ValueError`\n\n        \"\"\"\n        if reply_to is None and self.reply_to is None:\n            raise ValueError('Missing reply_to routing key')\n        properties = properties or {}\n        if not properties.get('app_id'):\n            properties['app_id'] = self.name\n        if not properties.get('correlation_id') and self.message_id:\n            properties['correlation_id'] = self.message_id\n        if not properties.get('message_id'):\n            properties['message_id'] = str(uuid.uuid4())\n        if not properties.get('timestamp'):\n            properties['timestamp'] = int(time.time())\n        return self.publish_message(exchange or self.exchange, reply_to\n                                    or self.reply_to, properties, body,\n                                    connection)", "response": "This method is used to send a reply to a message that is currently being processed by the consumer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting a context tag in Sentry for the given key and value.", "response": "def set_sentry_context(self, tag, value):\n        \"\"\"Set a context tag in Sentry for the given key and value.\n\n        :param tag: The context tag name\n        :type tag: :class:`str`\n        :param value: The context value\n        :type value: :class:`str`\n\n        \"\"\"\n        if self.sentry_client:\n            self.logger.debug('Setting sentry context for %s to %s', tag,\n                              value)\n            self.sentry_client.tags_context({tag: value})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stats_add_duration(self, key, duration):\n        if not self._measurement:\n            if not self.IGNORE_OOB_STATS:\n                self.logger.warning(\n                    'stats_add_timing invoked outside execution')\n            return\n        self._measurement.add_duration(key, duration)", "response": "Add a duration to the per - message measurements of the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stats_incr(self, key, value=1):\n        if not self._measurement:\n            if not self.IGNORE_OOB_STATS:\n                self.logger.warning('stats_incr invoked outside execution')\n            return\n        self._measurement.incr(key, value)", "response": "Increment the specified key in the per - message measurements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stats_set_tag(self, key, value=1):\n        if not self._measurement:\n            if not self.IGNORE_OOB_STATS:\n                self.logger.warning('stats_set_tag invoked outside execution')\n            return\n        self._measurement.set_tag(key, value)", "response": "Set the specified tag value in the per - message measurements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stats_set_value(self, key, value=1):\n        if not self._measurement:\n            if not self.IGNORE_OOB_STATS:\n                self.logger.warning(\n                    'stats_set_value invoked outside execution')\n            return\n        self._measurement.set_value(key, value)", "response": "Set the specified key value in the per - message measurements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stats_track_duration(self, key):\n        start_time = time.time()\n        try:\n            yield\n        finally:\n            self.stats_add_duration(\n                key, max(start_time, time.time()) - start_time)", "response": "Time around a context and add to the per - message measurements that are being processed by the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a context tag from sentry_client. tags.", "response": "def unset_sentry_context(self, tag):\n        \"\"\"Remove a context tag from sentry\n\n        :param tag: The context tag to remove\n        :type tag: :class:`str`\n\n        \"\"\"\n        if self.sentry_client:\n            self.sentry_client.tags.pop(tag, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction that will allow Rejected to process IOLoop events while in a tight-loop inside an asynchronous consumer. .. code-block:: python :caption: Example Usage class Consumer(consumer.Consumer): @gen.coroutine def process(self): for iteration in range(0, 1000000): yield self.yield_to_ioloop()", "response": "def yield_to_ioloop(self):\n        \"\"\"Function that will allow Rejected to process IOLoop events while\n        in a tight-loop inside an asynchronous consumer.\n\n        .. code-block:: python\n           :caption: Example Usage\n\n           class Consumer(consumer.Consumer):\n\n               @gen.coroutine\n               def process(self):\n                   for iteration in range(0, 1000000):\n                       yield self.yield_to_ioloop()\n\n        \"\"\"\n        try:\n            yield self._yield_condition.wait(\n                self._message.channel.connection.ioloop.time() + 0.001)\n        except gen.TimeoutError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses a message from RabbitMQ and return a generator that yields the result.", "response": "def execute(self, message_in, measurement):\n        \"\"\"Process the message from RabbitMQ. To implement logic for processing\n        a message, extend Consumer._process, not this method.\n\n        This for internal use and should not be extended or used directly.\n\n        :param message_in: The message to process\n        :type message_in: :class:`rejected.data.Message`\n        :param measurement: For collecting per-message instrumentation\n        :type measurement: :class:`rejected.data.Measurement`\n\n        \"\"\"\n        self.logger.debug('Received: %r', message_in)\n\n        try:\n            self._preprocess(message_in, measurement)\n        except DropMessage:\n            raise gen.Return(data.MESSAGE_DROP)\n        except MessageException as exc:\n            raise gen.Return(self._on_message_exception(exc))\n\n        try:\n            yield self._execute()\n        except ExecutionFinished as exc:\n            self.logger.debug('Execution finished early')\n            if not self._finished:\n                self.on_finish(exc)\n\n        except RabbitMQException as exc:\n            self._handle_exception(exc)\n            raise gen.Return(data.RABBITMQ_EXCEPTION)\n\n        except ConfigurationException as exc:\n            self._handle_exception(exc)\n            raise gen.Return(data.CONFIGURATION_EXCEPTION)\n\n        except ConsumerException as exc:\n            self._handle_exception(exc)\n            raise gen.Return(data.CONSUMER_EXCEPTION)\n\n        except MessageException as exc:\n            raise gen.Return(self._on_message_exception(exc))\n\n        except ProcessingException as exc:\n            self._handle_exception(exc)\n            self._republish_processing_error(\n                exc.metric or exc.__class__.__name__)\n            raise gen.Return(data.PROCESSING_EXCEPTION)\n\n        except (NotImplementedError, Exception) as exc:\n            self._handle_exception(exc)\n            raise gen.Return(data.UNHANDLED_EXCEPTION)\n\n        else:\n            self._finished = True\n\n        self._maybe_clear_confirmation_futures()\n        raise gen.Return(data.MESSAGE_ACK)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _clear(self):\n        self._finished = False\n        self._measurement = None\n        self._message = None\n        self._message_body = None", "response": "Resets all assigned data for the current message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninvoking the lifecycle methods for a message .", "response": "def _execute(self):\n        \"\"\"Invoke the lifecycle methods for a message\n        (prepare, process, on_finish), yielding if any are coroutines.\n\n        \"\"\"\n        for call in (self.prepare, self.process, self.on_finish):\n            self.logger.debug('Invoking %r', call)\n            result = call()\n            if concurrent.is_future(result):\n                yield result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_pika_properties(properties_in):\n        properties = pika.BasicProperties()\n        for key in properties_in or {}:\n            if properties_in.get(key) is not None:\n                setattr(properties, key, properties_in.get(key))\n        return properties", "response": "Convert a dictionary of properties to a pika. spec. BasicProperties object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when the message has finished processing.", "response": "def _maybe_clear_confirmation_futures(self):\n        \"\"\"Invoked when the message has finished processing, ensuring there\n        are no confirmation futures pending.\n\n        \"\"\"\n        for name in self._connections.keys():\n            self._connections[name].clear_confirmation_futures()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _maybe_drop_message(self):\n        if self.headers[_PROCESSING_EXCEPTIONS] >= self._error_max_retry:\n            self.logger.warning('Dropping message with %i deaths due to '\n                                'ERROR_MAX_RETRY',\n                                self.headers[_PROCESSING_EXCEPTIONS])\n            if self._drop_exchange:\n                self._republish_dropped_message('max retries ({})'.format(\n                    self.headers[_PROCESSING_EXCEPTIONS]))\n            raise DropMessage", "response": "Check to see if processing this message would exceed the configured error max retry value. If so raise a DropMessage."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _maybe_set_message_age(self):\n        if self._message.properties.timestamp:\n            message_age = float(\n                max(self._message.properties.timestamp, time.time()) -\n                self._message.properties.timestamp)\n            if message_age > 0:\n                self.measurement.add_duration(\n                    self.message_age_key(), message_age)", "response": "If timestamp is set record the age of the message coming in\n            if the relative age is > 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset internal state validating that the message should be processed and not dropped.", "response": "def _preprocess(self, message_in, measurement):\n        \"\"\"Invoked at the start of execution, setting internal state,\n        validating that the message should be processed and not dropped.\n\n        :param message_in: The message to process\n        :type message_in: :class:`rejected.data.Message`\n        :param measurement: For collecting per-message instrumentation\n        :type measurement: :class:`rejected.data.Measurement`\n        :raises: rejected.errors.MessageException\n        :raises: rejected.errors.DropMessage\n\n        \"\"\"\n        self._clear()\n        self._message = message_in\n        self._measurement = measurement\n\n        self._maybe_set_message_age()\n\n        # Ensure there is a correlation ID\n        self._correlation_id = (\n                self._message.properties.correlation_id or\n                self._message.properties.message_id or\n                str(uuid.uuid4()))\n\n        # Set the Correlation ID for the connection for logging\n        self._connections[self._message.connection].correlation_id = \\\n            self._correlation_id\n\n        # Set the sentry context for the message type if set\n        if self.message_type:\n            self.set_sentry_context('type', self.message_type)\n\n        # Validate the message type if validation is turned on\n        if self._message_type:\n            self._validate_message_type()\n\n        # Check the number of ProcessingErrors and possibly drop the message\n        if self._error_max_retry and _PROCESSING_EXCEPTIONS in self.headers:\n            self._maybe_drop_message()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a future that enables the consumers to block on the confirmation of a published message.", "response": "def _publisher_confirmation_future(self, name, exchange, routing_key,\n                                       properties):\n        \"\"\"Return a future a publisher confirmation result that enables\n        consumers to block on the confirmation of a published message.\n\n        Two internal dicts are used for keeping track of state.\n        Consumer._delivery_tags is a dict of connection names that keeps\n        the last delivery tag expectation and is used to correlate the future\n        with the delivery tag that is expected to be confirmed from RabbitMQ.\n\n        This for internal use and should not be extended or used directly.\n\n        :param str name: The connection name for the future\n        :param str exchange: The exchange the message was published to\n        :param str routing_key: The routing key that was used\n        :param properties: The AMQP message properties for the delivery\n        :type properties: pika.spec.Basic.Properties\n        :rtype: concurrent.Future.\n\n        \"\"\"\n        if self._connections[name].publisher_confirmations:\n            future = concurrent.Future()\n            self._connections[name].add_confirmation_future(\n                exchange, routing_key, properties, future)\n            return future"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the connection to publish.", "response": "def _publish_connection(self, name=None):\n        \"\"\"Return the connection to publish. If the name is not specified,\n        the connection associated with the current message is returned.\n\n        :param str name:\n        :rtype: rejected.process.Connection\n\n        \"\"\"\n        try:\n            conn = self._connections[name or self._message.connection]\n        except KeyError:\n            raise ValueError('Channel {} not found'.format(name))\n        if not conn.is_connected or conn.channel.is_closed:\n            raise RabbitMQException(conn.name, 599, 'NOT_CONNECTED')\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _republish_dropped_message(self, reason):\n        self.logger.debug('Republishing due to ProcessingException')\n        properties = dict(self._message.properties) or {}\n        if 'headers' not in properties or not properties['headers']:\n            properties['headers'] = {}\n        properties['headers']['X-Dropped-By'] = self.name\n        properties['headers']['X-Dropped-Reason'] = reason\n        properties['headers']['X-Dropped-Timestamp'] = \\\n            datetime.datetime.utcnow().isoformat()\n        properties['headers']['X-Original-Exchange'] = self._message.exchange\n\n        self._message.channel.basic_publish(\n            self._drop_exchange, self._message.routing_key, self._message.body,\n            pika.BasicProperties(**properties))", "response": "Republish the original message that was received it is being dropped by the consumer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _republish_processing_error(self, error):\n        self.logger.debug('Republishing due to ProcessingException')\n        properties = dict(self._message.properties) or {}\n        if 'headers' not in properties or not properties['headers']:\n            properties['headers'] = {}\n\n        if error:\n            properties['headers']['X-Processing-Exception'] = error\n\n        if _PROCESSING_EXCEPTIONS not in properties['headers']:\n            properties['headers'][_PROCESSING_EXCEPTIONS] = 1\n        else:\n            try:\n                properties['headers'][_PROCESSING_EXCEPTIONS] += 1\n            except TypeError:\n                properties['headers'][_PROCESSING_EXCEPTIONS] = 1\n\n        self._message.channel.basic_publish(\n            self._error_exchange, self._message.routing_key,\n            self._message.body, pika.BasicProperties(**properties))", "response": "Republish the original message that was received because ProcessingException was raised."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _unsupported_message_type(self):\n        if isinstance(self._message_type, (tuple, list, set)):\n            return self.message_type not in self._message_type\n        return self.message_type != self._message_type", "response": "Check if the current message type is not supported by the configured message type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_message_type(self):\n        if self._unsupported_message_type():\n            self.logger.warning(\n                'Received unsupported message type: %s', self.message_type)\n            if self._drop_invalid:\n                if self._drop_exchange:\n                    self._republish_dropped_message('invalid type')\n                raise DropMessage\n            raise MessageException", "response": "Check to see if the current message s AMQP type property is\n        supported and if not raise either a DropMessage or MessageException."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the setting if it is not set", "response": "def _setting(self, key, default):\n        \"\"\"Return the setting, checking config, then the appropriate\n        environment variable, falling back to the default, caching the\n        results.\n\n        :param str key: The key to get\n        :param any default: The default value if not set\n        :return: str\n\n        \"\"\"\n        if key not in self._settings:\n            value = self._settings_in.get(\n                key, os.environ.get('STATSD_{}'.format(key).upper(), default))\n            self._settings[key] = value\n        return self._settings[key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend the specified value to the statsd daemon via UDP without a direct socket connection.", "response": "def _send(self, key, value, metric_type):\n        \"\"\"Send the specified value to the statsd daemon via UDP without a\n        direct socket connection.\n\n        :param str key: The key name to send\n        :param int or float value: The value for the key\n\n        \"\"\"\n        try:\n            payload = self._build_payload(key, value, metric_type)\n            LOGGER.debug('Sending statsd payload: %r', payload)\n            self._socket.sendto(payload.encode('utf-8'), self._address)\n        except socket.error:  # pragma: nocover\n            LOGGER.exception('Error sending statsd metric')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_payload(self, key, value, metric_type):\n        if self._setting('include_hostname', True):\n            return self.PAYLOAD_HOSTNAME.format(\n                self._prefix, self._hostname, self._consumer_name, key, value,\n                metric_type)\n        return self.PAYLOAD_NO_HOSTNAME.format(\n            self._prefix, self._consumer_name, key, value,\n            metric_type)", "response": "Build the payload for the current session."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassign the specified state to this consumer object.", "response": "def set_state(self, new_state):\n        \"\"\"Assign the specified state to this consumer object.\n\n        :param int new_state: The new state of the object\n        :raises: ValueError\n\n        \"\"\"\n        # Make sure it's a valid state\n        if new_state not in self.STATES:\n            raise ValueError('Invalid state value: %r' % new_state)\n\n        # Set the state\n        LOGGER.debug('State changing from %s to %s', self.STATES[self.state],\n                     self.STATES[new_state])\n        self.state = new_state\n        self.state_start = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a bool determining if the process is in a running state or not", "response": "def is_running(self):\n        \"\"\"Returns a bool determining if the process is in a running state or\n        not\n\n        :rtype: bool\n\n        \"\"\"\n        return self.state in [self.STATE_IDLE, self.STATE_ACTIVE,\n                              self.STATE_SLEEPING]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nacknowledges the message on the broker and log the ack", "response": "def ack_message(self, message):\n        \"\"\"Acknowledge the message on the broker and log the ack\n\n        :param message: The message to acknowledge\n        :type message: rejected.data.Message\n\n        \"\"\"\n        if message.channel.is_closed:\n            LOGGER.warning('Can not ack message, channel is closed')\n            self.counters[self.CLOSED_ON_COMPLETE] += 1\n            return\n        message.channel.basic_ack(delivery_tag=message.delivery_tag)\n        self.counters[self.ACKED] += 1\n        self.measurement.set_tag(self.ACKED, True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating RabbitMQ connections and assign the connection object to the connections dict.", "response": "def create_connections(self):\n        \"\"\"Create and start the RabbitMQ connections, assigning the connection\n        object to the connections dict.\n\n        \"\"\"\n        self.set_state(self.STATE_CONNECTING)\n        for conn in self.consumer_config.get('connections', []):\n            name, confirm, consume = conn, False, True\n            if isinstance(conn, dict):\n                name = conn['name']\n                confirm = conn.get('publisher_confirmation', False)\n                consume = conn.get('consume', True)\n\n            if name not in self.config['Connections']:\n                LOGGER.critical('Connection \"%s\" for %s not found',\n                                name, self.consumer_name)\n                continue\n\n            self.connections[name] = connection.Connection(\n                name, self.config['Connections'][name], self.consumer_name,\n                consume, confirm, self.ioloop, self.callbacks)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_config(cfg, number, name, connection_name):\n        return {\n            'connection': cfg['Connections'][connection_name],\n            'consumer_name': name,\n            'process_name': '%s_%i_tag_%i' % (name, os.getpid(), number)\n        }", "response": "Initialize a new consumer thread setting defaults and config values\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_consumer(self, cfg):\n        try:\n            handle, version = utils.import_consumer(cfg['consumer'])\n        except ImportError as error:\n            LOGGER.exception('Error importing the consumer %s: %s',\n                             cfg['consumer'], error)\n            return\n\n        if version:\n            LOGGER.info('Creating consumer %s v%s', cfg['consumer'], version)\n            self.consumer_version = version\n        else:\n            LOGGER.info('Creating consumer %s', cfg['consumer'])\n\n        settings = cfg.get('config', dict())\n        settings['_import_module'] = '.'.join(cfg['consumer'].split('.')[0:-1])\n\n        kwargs = {\n            'settings': settings,\n            'process': self,\n            'drop_exchange': cfg.get('drop_exchange'),\n            'drop_invalid_messages': cfg.get('drop_invalid_messages'),\n            'message_type': cfg.get('message_type'),\n            'error_exchange': cfg.get('error_exchange'),\n            'error_max_retry': cfg.get('error_max_retry')\n        }\n\n        try:\n            return handle(**kwargs)\n        except Exception as error:\n            LOGGER.exception('Error creating the consumer \"%s\": %s',\n                             cfg['consumer'], error)\n            self.send_exception_to_sentry(sys.exc_info())\n            self.on_startup_error('Failed to create consumer {}: {}'.format(\n                self.consumer_name, error))", "response": "Imports and creates a new instance of the configured message consumer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopping the next message on the stack and invoke the consumer with the message.", "response": "def maybe_get_next_message(self):\n        \"\"\"Pop the next message on the stack, adding a callback on the IOLoop\n        to invoke the consumer with the message. This is done so we let the\n        IOLoop perform any pending callbacks before trying to process the\n        next message.\n\n        \"\"\"\n        if self.pending:\n            self.ioloop.add_callback(\n                self.invoke_consumer, self.pending.popleft())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maybe_submit_measurement(self):\n        if self.statsd:\n            self.submit_statsd_measurements()\n        if self.influxdb:\n            self.submit_influxdb_measurement()", "response": "Check for configured instrumentation backends and if found submit the message measurement info."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a message from Rabbit", "response": "def on_delivery(self, name, channel, method, properties, body):\n        \"\"\"Process a message from Rabbit\n\n        :param str name: The connection name\n        :param pika.channel.Channel channel: The message's delivery channel\n        :param pika.frames.MethodFrame method: The method frame\n        :param pika.spec.BasicProperties properties: The message properties\n        :param str body: The message body\n\n        \"\"\"\n        message = data.Message(name, channel, method, properties, body)\n        if self.is_processing:\n            return self.pending.append(message)\n        self.invoke_consumer(message)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_processed(self, message, result, start_time):\n        duration = max(start_time, time.time()) - start_time\n        self.counters[self.TIME_SPENT] += duration\n        self.measurement.add_duration(self.TIME_SPENT, duration)\n\n        if result == data.MESSAGE_DROP:\n            LOGGER.debug('Rejecting message due to drop return from consumer')\n            self.reject(message, False)\n            self.counters[self.DROPPED] += 1\n\n        elif result == data.MESSAGE_EXCEPTION:\n            LOGGER.debug('Rejecting message due to MessageException')\n            self.reject(message, False)\n            self.counters[self.MESSAGE_EXCEPTION] += 1\n\n        elif result == data.PROCESSING_EXCEPTION:\n            LOGGER.debug('Rejecting message due to ProcessingException')\n            self.reject(message, False)\n            self.counters[self.PROCESSING_EXCEPTION] += 1\n\n        elif result == data.CONFIGURATION_EXCEPTION:\n            LOGGER.debug('Rejecting message due to ConfigurationException '\n                         'and shutting down')\n            self.reject(message, False)\n            self.counters[self.CONFIGURATION_EXCEPTION] += 1\n            self.stop_consumer()\n            self.shutdown_connections()\n\n        elif result == data.CONSUMER_EXCEPTION:\n            LOGGER.debug('Re-queueing message due to ConsumerException')\n            self.reject(message, True)\n            self.on_processing_error()\n            self.counters[self.CONSUMER_EXCEPTION] += 1\n\n        elif result == data.RABBITMQ_EXCEPTION:\n            LOGGER.debug('Processing interrupted due to RabbitMQException')\n            self.on_processing_error()\n            self.counters[self.RABBITMQ_EXCEPTION] += 1\n\n        elif result == data.UNHANDLED_EXCEPTION:\n            LOGGER.debug('Re-queueing message due to UnhandledException')\n            self.reject(message, True)\n            self.on_processing_error()\n            self.counters[self.UNHANDLED_EXCEPTION] += 1\n\n        elif result == data.MESSAGE_REQUEUE:\n            LOGGER.debug('Re-queueing message due Consumer request')\n            self.reject(message, True)\n            self.counters[self.REQUEUED] += 1\n\n        elif result == data.MESSAGE_ACK and not self.no_ack:\n            self.ack_message(message)\n\n        self.counters[self.PROCESSED] += 1\n        self.measurement.set_tag(self.PROCESSED, True)\n        self.maybe_submit_measurement()\n        self.reset_state()", "response": "Invoked after a message is processed by the consumer and the result of the processing of the message is received."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_processing_error(self):\n        duration = time.time() - self.last_failure\n        if duration > self.MAX_ERROR_WINDOW:\n            LOGGER.info('Resetting failure window, %i seconds since last',\n                        duration)\n            self.reset_error_counter()\n        self.counters[self.ERROR] += 1\n        self.last_failure = time.time()\n        if self.too_many_errors:\n            LOGGER.critical('Error threshold exceeded (%i), shutting down',\n                            self.counters[self.ERROR])\n            self.shutdown_connections()", "response": "Called when a message processing failure occurs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_ready_to_stop(self):\n\n        # Set the state to shutting down if it wasn't set as that during loop\n        self.set_state(self.STATE_SHUTTING_DOWN)\n\n        # Reset any signal handlers\n        signal.signal(signal.SIGABRT, signal.SIG_IGN)\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        signal.signal(signal.SIGPROF, signal.SIG_IGN)\n        signal.signal(signal.SIGTERM, signal.SIG_IGN)\n\n        # Allow the consumer to gracefully stop and then stop the IOLoop\n        if self.consumer:\n            self.stop_consumer()\n\n        # Clear IOLoop constructs\n        self.consumer_lock = None\n\n        # Stop the IOLoop\n        if self.ioloop:\n            LOGGER.debug('Stopping IOLoop')\n            self.ioloop.stop()\n\n        # Note that shutdown is complete and set the state accordingly\n        self.set_state(self.STATE_STOPPED)\n        LOGGER.info('Shutdown complete')", "response": "Invoked when the consumer is ready to stop."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when SIGPROF is received to the master process will dump the stats and queue them for the master process to get data.", "response": "def on_sigprof(self, _unused_signum, _unused_frame):\n        \"\"\"Called when SIGPROF is sent to the process, will dump the stats, in\n        future versions, queue them for the master process to get data.\n\n        :param int _unused_signum: The signal number\n        :param frame _unused_frame: The python frame the signal was received at\n\n        \"\"\"\n        self.stats_queue.put(self.report_stats(), True)\n        self.last_stats_time = time.time()\n        signal.siginterrupt(signal.SIGPROF, False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_startup_error(self, error):\n        LOGGER.critical('Could not start %s: %s', self.consumer_name, error)\n        self.set_state(self.STATE_STOPPED)", "response": "Invoked when a pre - condition for starting the consumer has failed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrejecting the message on the broker and log it.", "response": "def reject(self, message, requeue=True):\n        \"\"\"Reject the message on the broker and log it.\n\n        :param message: The message to reject\n        :type message: rejected.Data.message\n        :param bool requeue: Specify if the message should be re-queued or not\n\n        \"\"\"\n        if self.no_ack:\n            raise RuntimeError('Can not rejected messages when ack is False')\n\n        if message.channel.is_closed:\n            LOGGER.warning('Can not nack message, disconnected from RabbitMQ')\n            self.counters[self.CLOSED_ON_COMPLETE] += 1\n            return\n\n        LOGGER.warning('Rejecting message %s %s requeue', message.delivery_tag,\n                       'with' if requeue else 'without')\n        message.channel.basic_nack(\n            delivery_tag=message.delivery_tag, requeue=requeue)\n        self.measurement.set_tag(self.NACKED, True)\n        self.measurement.set_tag(self.REQUEUED, requeue)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the dict of stats data for the MCP stats queue", "response": "def report_stats(self):\n        \"\"\"Create the dict of stats data for the MCP stats queue\"\"\"\n        if not self.previous:\n            self.previous = dict()\n            for key in self.counters:\n                self.previous[key] = 0\n        values = {\n            'name': self.name,\n            'consumer_name': self.consumer_name,\n            'counts': dict(self.counters),\n            'previous': dict(self.previous)\n        }\n        self.previous = dict(self.counters)\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset_state(self):\n        self.active_message = None\n        self.measurement = None\n        if self.is_waiting_to_shutdown:\n            self.shutdown_connections()\n        elif self.is_processing:\n            self.set_state(self.STATE_IDLE)\n        elif self.is_idle or self.is_connecting or self.is_shutting_down:\n            pass\n        else:\n            LOGGER.critical('Unexepected state: %s', self.state_description)\n        LOGGER.debug('State reset to %s (%s in pending)',\n                     self.state_description, len(self.pending))", "response": "Reset the internal state of the internal state of the internal state of the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run(self):\n        self.set_state(self.STATE_INITIALIZING)\n        self.ioloop = ioloop.IOLoop.current()\n        self.consumer_lock = locks.Lock()\n\n        self.sentry_client = self.setup_sentry(\n            self._kwargs['config'], self.consumer_name)\n\n        try:\n            self.setup()\n        except (AttributeError, ImportError):\n            return self.on_startup_error(\n                'Failed to import the Python module for {}'.format(\n                    self.consumer_name))\n\n        if not self.is_stopped:\n            try:\n                self.ioloop.start()\n            except KeyboardInterrupt:\n                LOGGER.warning('CTRL-C while waiting for clean shutdown')", "response": "Run method that can be profiled"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_exception_to_sentry(self, exc_info):\n        if not self.sentry_client:\n            LOGGER.debug('No sentry_client, aborting')\n            return\n\n        message = dict(self.active_message)\n        try:\n            duration = math.ceil(time.time() - self.delivery_time) * 1000\n        except TypeError:\n            duration = 0\n        kwargs = {'extra': {\n                      'consumer_name': self.consumer_name,\n                      'env': dict(os.environ),\n                      'message': message},\n                  'time_spent': duration}\n        LOGGER.debug('Sending exception to sentry: %r', kwargs)\n        self.sentry_client.captureException(exc_info, **kwargs)", "response": "Send an exception to Sentry if enabled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the consumer and setup needed attributes and connecting to RabbitMQ.", "response": "def setup(self):\n        \"\"\"Initialize the consumer, setting up needed attributes and connecting\n        to RabbitMQ.\n\n        \"\"\"\n        LOGGER.info('Initializing for %s', self.name)\n\n        if 'consumer' not in self.consumer_config:\n            return self.on_startup_error(\n                '\"consumer\" not specified in configuration')\n\n        self.consumer = self.get_consumer(self.consumer_config)\n\n        if not self.consumer:\n            return self.on_startup_error(\n                'Could not import \"{}\"'.format(\n                    self.consumer_config.get(\n                        'consumer', 'unconfigured consumer')))\n\n        self.setup_instrumentation()\n        self.reset_error_counter()\n        self.setup_sighandlers()\n        self.create_connections()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_influxdb(self, config):\n        base_tags = {\n            'version': self.consumer_version\n        }\n        measurement = self.config.get('influxdb_measurement',\n                                      os.environ.get('SERVICE'))\n        if measurement != self.consumer_name:\n            base_tags['consumer'] = self.consumer_name\n        for key in {'ENVIRONMENT', 'SERVICE'}:\n            if key in os.environ:\n                base_tags[key.lower()] = os.environ[key]\n        influxdb.install(\n            '{}://{}:{}/write'.format(\n                config.get('scheme',\n                           os.environ.get('INFLUXDB_SCHEME', 'http')),\n                config.get('host',\n                           os.environ.get('INFLUXDB_HOST', 'localhost')),\n                config.get('port', os.environ.get('INFLUXDB_PORT', '8086'))\n            ),\n            config.get('user', os.environ.get('INFLUXDB_USER')),\n            config.get('password', os.environ.get('INFLUXDB_PASSWORD')),\n            base_tags=base_tags)\n        return config.get('database', 'rejected'), measurement", "response": "Configure the InfluxDB module for the measurement submission."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures instrumentation for submission per message measurements to statsd and InfluxDB.", "response": "def setup_instrumentation(self):\n        \"\"\"Configure instrumentation for submission per message measurements\n        to statsd and/or InfluxDB.\n\n        \"\"\"\n        if not self.config.get('stats') and not self.config.get('statsd'):\n            return\n\n        if 'stats' not in self.config:\n            self.config['stats'] = {}\n\n        # Backwards compatible statsd config support\n        if self.config.get('statsd'):\n            warnings.warn('Deprecated statsd configuration detected',\n                          DeprecationWarning)\n            self.config['stats'].setdefault('statsd',\n                                            self.config.get('statsd'))\n\n        if self.config['stats'].get('statsd'):\n            if self.config['stats']['statsd'].get('enabled', True):\n                self.statsd = statsd.Client(self.consumer_name,\n                                            self.config['stats']['statsd'])\n            LOGGER.debug('statsd measurements configured')\n\n        # InfluxDB support\n        if influxdb and self.config['stats'].get('influxdb'):\n            if self.config['stats']['influxdb'].get('enabled', True):\n                self.influxdb = self.setup_influxdb(\n                    self.config['stats']['influxdb'])\n            LOGGER.debug('InfluxDB measurements configured: %r', self.influxdb)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_sighandlers(self):\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        signal.signal(signal.SIGTERM, signal.SIG_IGN)\n\n        signal.signal(signal.SIGPROF, self.on_sigprof)\n        signal.signal(signal.SIGABRT, self.stop)\n\n        signal.siginterrupt(signal.SIGPROF, False)\n        signal.siginterrupt(signal.SIGABRT, False)\n        LOGGER.debug('Signal handlers setup')", "response": "Setup the stats and stop signal handlers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shutdown_connections(self):\n        if not self.is_shutting_down:\n            self.set_state(self.STATE_SHUTTING_DOWN)\n        for name in self.connections:\n            if self.connections[name].is_running:\n                self.connections[name].shutdown()", "response": "This method closes the connections to RabbitMQ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop the consumer by calling BasicCancel and setting our state to STOP_REQUESTED.", "response": "def stop(self, signum=None, _unused=None):\n        \"\"\"Stop the consumer from consuming by calling BasicCancel and setting\n        our state.\n\n        :param int signum: The signal received\n        :param frame _unused: The stack frame from when the signal was called\n\n        \"\"\"\n        LOGGER.debug('Stop called in state: %s', self.state_description)\n        if self.is_stopped:\n            LOGGER.warning('Stop requested but consumer is already stopped')\n            return\n        elif self.is_shutting_down:\n            LOGGER.warning('Stop requested, consumer is already shutting down')\n            return\n        elif self.is_waiting_to_shutdown:\n            LOGGER.warning('Stop requested but already waiting to shut down')\n            return\n\n        # Stop consuming and close AMQP connections\n        self.shutdown_connections()\n\n        # Wait until the consumer has finished processing to shutdown\n        if self.is_processing:\n            LOGGER.info('Waiting for consumer to finish processing')\n            self.set_state(self.STATE_STOP_REQUESTED)\n            if signum == signal.SIGTERM:\n                signal.siginterrupt(signal.SIGTERM, False)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop_consumer(self):\n        try:\n            LOGGER.info('Shutting down the consumer')\n            self.consumer.shutdown()\n        except AttributeError:\n            LOGGER.debug('Consumer does not have a shutdown method')", "response": "Stop the consumer and allow it to do clean shutdown if it has the ability to do so."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit a measurement for a message to InfluxDB", "response": "def submit_influxdb_measurement(self):\n        \"\"\"Submit a measurement for a message to InfluxDB\"\"\"\n        measurement = influxdb.Measurement(*self.influxdb)\n        measurement.set_timestamp(time.time())\n        for key, value in self.measurement.counters.items():\n            measurement.set_field(key, value)\n        for key, value in self.measurement.tags.items():\n            measurement.set_tag(key, value)\n        for key, value in self.measurement.values.items():\n            measurement.set_field(key, value)\n\n        for key, values in self.measurement.durations.items():\n            if len(values) == 1:\n                measurement.set_field(key, values[0])\n            elif len(values) > 1:\n                measurement.set_field('{}-average'.format(key),\n                                      sum(values) / len(values))\n                measurement.set_field('{}-max'.format(key), max(values))\n                measurement.set_field('{}-min'.format(key), min(values))\n                measurement.set_field('{}-median'.format(key),\n                                      utils.percentile(values, 50))\n                measurement.set_field('{}-95th'.format(key),\n                                      utils.percentile(values, 95))\n\n        influxdb.add_measurement(measurement)\n        LOGGER.debug('InfluxDB Measurement: %r', measurement.marshall())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsubmit a measurement for a message to statsd as individual items.", "response": "def submit_statsd_measurements(self):\n        \"\"\"Submit a measurement for a message to statsd as individual items.\"\"\"\n        for key, value in self.measurement.counters.items():\n            self.statsd.incr(key, value)\n        for key, values in self.measurement.durations.items():\n            for value in values:\n                self.statsd.add_timing(key, value)\n        for key, value in self.measurement.values.items():\n            self.statsd.set_gauge(key, value)\n        for key, value in self.measurement.tags.items():\n            if isinstance(value, bool):\n                if value:\n                    self.statsd.incr(key)\n            elif isinstance(value, str):\n                if value:\n                    self.statsd.incr('{}.{}'.format(key, value))\n            elif isinstance(value, int):\n                self.statsd.incr(key, value)\n            else:\n                LOGGER.warning('The %s value type of %s is unsupported',\n                               key, type(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef profile_file(self):\n        if 'profile' in self._kwargs and self._kwargs['profile']:\n            profile_path = path.normpath(self._kwargs['profile'])\n            if os.path.exists(profile_path) and os.path.isdir(profile_path):\n                return path.join(\n                    profile_path, '{}-{}.prof'.format(\n                        os.getpid(), self._kwargs['consumer_name']))", "response": "Return the full path to write the cProfile data\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_package_version(module_obj, value):\n    for key in ['version', '__version__']:\n        if hasattr(module_obj, key):\n            return getattr(module_obj, key)\n    parts = value.split('.')\n    for index, part in enumerate(parts):\n        try:\n            return pkg_resources.get_distribution(\n                '.'.join(parts[0:index + 1])).version\n        except (pkg_resources.DistributionNotFound,\n                pkg_resources.RequirementParseError):\n            continue", "response": "Get the version of a package or a module s package."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_consumer(value):\n    parts = value.split('.')\n    module_obj = importlib.import_module('.'.join(parts[0:-1]))\n    return (getattr(module_obj, parts[-1]),\n            get_package_version(module_obj, value))", "response": "Import a consumer class in the format of foo. Bar Baz Qux\n    and return a handle to the class and the version."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef message_info(exchange, routing_key, properties):\n    output = []\n    if properties.message_id:\n        output.append(properties.message_id)\n    if properties.correlation_id:\n        output.append('[correlation_id=\"{}\"]'.format(\n            properties.correlation_id))\n    if exchange:\n        output.append('published to \"{}\"'.format(exchange))\n    if routing_key:\n        output.append('using \"{}\"'.format(routing_key))\n    return ' '.join(output)", "response": "Return info about a message using the same conditional constructs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the percentile of a list of values.", "response": "def percentile(values, k):\n    \"\"\"Find the percentile of a list of values.\n\n    :param list values: The list of values to find the percentile of\n    :param int k: The percentile to find\n    :rtype: float or int\n\n    \"\"\"\n    if not values:\n        return None\n    values.sort()\n    index = (len(values) * (float(k) / 100)) - 1\n    return values[int(math.ceil(index))]"}
