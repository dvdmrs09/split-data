{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self, reason=True):\n        # type: (bool) -> list\n        \"\"\"Return the validation results and include an (optional) reason.\n\n        If reason keyword is true, the validation is returned for each validation\n        the [(<result: bool>, <reason:str>), ...]. If reason is False, only a single list of validation results\n        for each configured validator is returned.\n\n        :param reason: (optional) switch to indicate if the reason of the validation should be provided\n        :type reason: bool\n        :return: list of validation results [bool, bool, ...] or\n                 a list of validation results, reasons [(bool, str), ...]\n        :rtype: list(bool) or list((bool, str))\n        :raises Exception: for incorrect validators or incompatible values\n        \"\"\"\n        self._validation_results = [validator.is_valid(self._value) for validator in getattr(self, '_validators')]\n        self._validation_reasons = [validator.get_reason() for validator in getattr(self, '_validators')]\n\n        if reason:\n            return list(zip(self._validation_results, self._validation_reasons))\n        else:\n            return self._validation_results", "response": "Return the validation results and include an optional reason."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ini_config(config=os.path.join(os.path.expanduser('~'), '.zdeskcfg'),\n        default_section=None, section=None):\n    \"\"\"This is a convenience function for getting the zdesk configuration\n    from an ini file without the need to decorate and call your own function.\n    Handy when using zdesk and zdeskcfg from the interactive prompt.\"\"\"\n    plac_ini.call(__placeholder__, config=config, default_section=default_section)\n    return __placeholder__.getconfig(section)", "response": "This function is used to get the zdesk configuration from an ini file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ecg_band_pass_filter(data, sample_rate):\n    nyquist_sample_rate = sample_rate / 2.\n    normalized_cut_offs = [5/nyquist_sample_rate, 15/nyquist_sample_rate]\n    b_coeff, a_coeff = butter(2, normalized_cut_offs, btype='bandpass')[:2]\n    return filtfilt(b_coeff, a_coeff, data, padlen=150)", "response": "Filter the data with a bandpass setting of 5 to 15 Hz."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _integration(data, sample_rate):\n    wind_size = int(0.080 * sample_rate)\n    int_ecg = numpy.zeros_like(data)\n    cum_sum = data.cumsum()\n    int_ecg[wind_size:] = (cum_sum[wind_size:] - cum_sum[:-wind_size]) / wind_size\n    int_ecg[:wind_size] = cum_sum[:wind_size] / numpy.arange(1, wind_size + 1)\n\n    return int_ecg", "response": "Moving window integration. N is the number of samples in the width of the integration\n    window\n\n    ----------\n    Parameters\n    ----------\n    data : ndarray\n        Samples of the signal where a moving window integration will be applied.\n    sample_rate : int\n        Sampling rate at which the acquisition took place.\n\n    Returns\n    -------\n    out : ndarray\n        Integrated signal samples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the buffer for the given data and sample_rate.", "response": "def _buffer_ini(data, sample_rate):\n    \"\"\"\n    Initializes the buffer with eight 1s intervals\n\n    ----------\n    Parameters\n    ----------\n    data : ndarray\n        Pre-processed ECG signal samples.\n    sample_rate : int\n        Sampling rate at which the acquisition took place.\n\n    Returns\n    -------\n    rr_buffer : list\n        Data structure that stores eight samples (in the future this buffer will store the duration\n        of eight RR intervals instead of the 1 second values defined in initialisation).\n    spk1 : float\n        Initial value of SPK1 parameter defined in Pan-Tompkins real-time R peak detection algorithm\n        (named signal peak).\n    npk1 : int\n        Initial value of NPK1 parameter defined in Pan-Tompkins real-time R peak detection algorithm\n        (named noise peak).\n    threshold : float\n        Initial value of the adaptive threshold level (relevant parameter for the application of\n        specific criteria during the identification of R peaks).\n\n    Sources\n    -------\n    https://www.robots.ox.ac.uk/~gari/teaching/cdt/A3/readings/ECG/Pan+Tompkins.pdf\n\n\n    \"\"\"\n    rr_buffer = [1] * 8\n    spk1 = max(data[sample_rate:2*sample_rate])\n    npk1 = 0\n    threshold = _buffer_update(npk1, spk1)\n\n    return rr_buffer, spk1, npk1, threshold"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _detects_peaks(ecg_integrated, sample_rate):\n\n    # Minimum RR interval = 200 ms\n    min_rr = (sample_rate / 1000) * 200\n\n    # Computes all possible peaks and their amplitudes\n    possible_peaks = [i for i in range(0, len(ecg_integrated)-1)\n                      if ecg_integrated[i-1] < ecg_integrated[i] and\n                      ecg_integrated[i] > ecg_integrated[i+1]]\n\n    possible_amplitudes = [ecg_integrated[k] for k in possible_peaks]\n    chosen_peaks = []\n\n    # Starts with first peak\n    if not possible_peaks:\n        raise Exception(\"No Peaks Detected.\")\n    peak_candidate_i = possible_peaks[0]\n    peak_candidate_amp = possible_amplitudes[0]\n    for peak_i, peak_amp in zip(possible_peaks, possible_amplitudes):\n        if peak_i - peak_candidate_i <= min_rr and peak_amp > peak_candidate_amp:\n            peak_candidate_i = peak_i\n            peak_candidate_amp = peak_amp\n        elif peak_i - peak_candidate_i > min_rr:\n            chosen_peaks += [peak_candidate_i - 6]  # Delay of 6 samples\n            peak_candidate_i = peak_i\n            peak_candidate_amp = peak_amp\n        else:\n            pass\n\n    return chosen_peaks, possible_peaks", "response": "Detects local maximums from the integrated signal and returns the list of local maximums that are not yet in the local maximums."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck each peak according to thresholds and returns a new list of local maximums that are not yet in the R peak.", "response": "def _checkup(peaks, ecg_integrated, sample_rate, rr_buffer, spk1, npk1, threshold):\n    \"\"\"\n    Check each peak according to thresholds\n\n    ----------\n    Parameters\n    ----------\n    peaks : list\n        List of local maximums that pass the first stage of conditions needed to be considered as\n        an R peak.\n    ecg_integrated : ndarray\n        Array that contains the samples of the integrated signal.\n    sample_rate : int\n        Sampling rate at which the acquisition took place.\n    rr_buffer : list\n        Data structure that stores the duration of the last eight RR intervals.\n    spk1 : float\n        Actual value of SPK1 parameter defined in Pan-Tompkins real-time R peak detection algorithm\n        (named signal peak).\n    npk1 : int\n        Actual value of NPK1 parameter defined in Pan-Tompkins real-time R peak detection algorithm\n        (named noise peak).\n    threshold : float\n        Initial value of the adaptive threshold level (relevant parameter for the application of\n        specific criteria during the identification of R peaks).\n\n    Returns\n    -------\n    out : list\n        List with the position of the peaks considered as R peak by the algorithm.\n\n    \"\"\"\n    peaks_amp = [ecg_integrated[peak] for peak in peaks]\n    definitive_peaks = []\n    for i, peak in enumerate(peaks):\n        amp = peaks_amp[i]\n\n        # accept if larger than threshold and slope in raw signal\n        # is +-30% of previous slopes\n        if amp > threshold:\n            definitive_peaks, spk1, rr_buffer = _acceptpeak(peak, amp, definitive_peaks, spk1,\n                                                            rr_buffer)\n\n        # accept as qrs if higher than half threshold,\n        # but is 360 ms after last qrs and next peak\n        # is more than 1.5 rr intervals away\n        # just abandon it if there is no peak before\n        # or after\n        elif amp > threshold / 2 and list(definitive_peaks) and len(peaks) > i + 1:\n            mean_rr = numpy.mean(rr_buffer)\n            last_qrs_ms = (peak - definitive_peaks[-1]) * (1000 / sample_rate)\n            last_qrs_to_next_peak = peaks[i+1] - definitive_peaks[-1]\n\n            if last_qrs_ms > 360 and last_qrs_to_next_peak > 1.5 * mean_rr:\n                definitive_peaks, spk1, rr_buffer = _acceptpeak(peak, amp, definitive_peaks, spk1,\n                                                                rr_buffer)\n            else:\n                npk1 = _noisepeak(amp, npk1)\n        # if not either of these it is noise\n        else:\n            npk1 = _noisepeak(amp, npk1)\n        threshold = _buffer_update(npk1, spk1)\n\n    definitive_peaks = numpy.array(definitive_peaks)\n\n    return definitive_peaks"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tachogram(data, sample_rate, signal=False, in_seconds=False, out_seconds=False):\n\n    if signal is False:  # data is a list of R peaks position.\n        data_copy = data\n        time_axis = numpy.array(data)#.cumsum()\n        if out_seconds is True and in_seconds is False:\n            time_axis = time_axis / sample_rate\n    else:  # data is a ECG signal.\n        # Detection of R peaks.\n        data_copy = detect_r_peaks(data, sample_rate, time_units=out_seconds, volts=False,\n                                   resolution=None, plot_result=False)[0]\n        time_axis = data_copy\n\n    # Generation of Tachogram.\n    tachogram_data = numpy.diff(time_axis)\n    tachogram_time = time_axis[1:]\n\n    return tachogram_data, tachogram_time", "response": "Function for generating of ECG Tachograms."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _thres_norm_reg(threshold_level, signal, pre_smooth_signal):\n    avg_signal = numpy.average(pre_smooth_signal)\n    std_signal = numpy.std(pre_smooth_signal)\n\n    threshold_0_perc_level = (-avg_signal) / float(std_signal)\n    threshold_100_perc_level = (numpy.max(signal) - avg_signal) / float(std_signal)\n\n    slope, b_coeff = linregress([0, 100], [threshold_0_perc_level, threshold_100_perc_level])[:2]\n    return slope * threshold_level + b_coeff", "response": "This function is used to normalize the threshold level of a given set of EMG smoothed signal samples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, message, callback=None):\n        assert isinstance(message, velbus.Message)\n        self._write_queue.put_nowait((message, callback))", "response": "Add message to write queue."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __require_kytos_config(self):\n        if self.__enabled is None:\n            uri = self._kytos_api + 'api/kytos/core/config/'\n            try:\n                options = json.loads(urllib.request.urlopen(uri).read())\n            except urllib.error.URLError:\n                print('Kytos is not running.')\n                sys.exit()\n            self.__enabled = Path(options.get('napps'))\n            self.__installed = Path(options.get('installed_napps'))", "response": "Set path locations from kytosapi API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_napp(self, user, napp, version=None):\n        self.user = user\n        self.napp = napp\n        self.version = version or 'latest'", "response": "Set user napp and version"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_napps(napps_dir):\n        jsons = napps_dir.glob('*/*/kytos.json')\n        return sorted(j.parts[-3:-1] for j in jsons)", "response": "Return a list of napps found in napps_dir."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_disabled(self):\n        installed = set(self.get_installed())\n        enabled = set(self.get_enabled())\n        return sorted(installed - enabled)", "response": "Returns a list of username napp_name of disabled napps."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dependencies(self, user=None, napp=None):\n        napps = self._get_napp_key('napp_dependencies', user, napp)\n        return [tuple(napp.split('/')) for napp in napps]", "response": "Get napp_dependencies from install NApp."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_napp_key(self, key, user=None, napp=None):\n        if user is None:\n            user = self.user\n        if napp is None:\n            napp = self.napp\n        kytos_json = self._installed / user / napp / 'kytos.json'\n        try:\n            with kytos_json.open() as file_descriptor:\n                meta = json.load(file_descriptor)\n                return meta[key]\n        except (FileNotFoundError, json.JSONDecodeError, KeyError):\n            return ''", "response": "Return a value from kytos. json. napp. user. napp. key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisables a NApp if it is enabled.", "response": "def disable(self):\n        \"\"\"Disable a NApp if it is enabled.\"\"\"\n        core_napps_manager = CoreNAppsManager(base_path=self._enabled)\n        core_napps_manager.disable(self.user, self.napp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable(self):\n        core_napps_manager = CoreNAppsManager(base_path=self._enabled)\n        core_napps_manager.enable(self.user, self.napp)", "response": "Enable a NApp if not already enabled."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef uninstall(self):\n        if self.is_installed():\n            installed = self.installed_dir()\n            if installed.is_symlink():\n                installed.unlink()\n            else:\n                shutil.rmtree(str(installed))", "response": "Delete code inside NApp directory if existent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_template(templates_path, template_filename, context):\n        template_env = Environment(\n            autoescape=False, trim_blocks=False,\n            loader=FileSystemLoader(str(templates_path)))\n        return template_env.get_template(str(template_filename)) \\\n            .render(context)", "response": "Render Jinja2 template for a NApp structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search(pattern):\n        def match(napp):\n            \"\"\"Whether a NApp metadata matches the pattern.\"\"\"\n            # WARNING: This will change for future versions, when 'author' will\n            # be removed.\n            username = napp.get('username', napp.get('author'))\n\n            strings = ['{}/{}'.format(username, napp.get('name')),\n                       napp.get('description')] + napp.get('tags')\n            return any(pattern.match(string) for string in strings)\n\n        napps = NAppsClient().get_napps()\n        return [napp for napp in napps if match(napp)]", "response": "Search all server NApps matching pattern."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef install_local(self):\n        folder = self._get_local_folder()\n        installed = self.installed_dir()\n        self._check_module(installed.parent)\n        installed.symlink_to(folder.resolve())", "response": "Make a symlink in install folder to a local NApp."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns local NApp root folder.", "response": "def _get_local_folder(self, root=None):\n        \"\"\"Return local NApp root folder.\n\n        Search for kytos.json in _./_ folder and _./user/napp_.\n\n        Args:\n            root (pathlib.Path): Where to begin searching.\n\n        Return:\n            pathlib.Path: NApp root folder.\n\n        Raises:\n            FileNotFoundError: If there is no such local NApp.\n\n        \"\"\"\n        if root is None:\n            root = Path()\n        for folders in ['.'], [self.user, self.napp]:\n            kytos_json = root / Path(*folders) / 'kytos.json'\n            if kytos_json.exists():\n                with kytos_json.open() as file_descriptor:\n                    meta = json.load(file_descriptor)\n                    # WARNING: This will change in future versions, when\n                    # 'author' will be removed.\n                    username = meta.get('username', meta.get('author'))\n                    if username == self.user and meta.get('name') == self.napp:\n                        return kytos_json.parent\n        raise FileNotFoundError('kytos.json not found.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads extract and install NApp.", "response": "def install_remote(self):\n        \"\"\"Download, extract and install NApp.\"\"\"\n        package, pkg_folder = None, None\n        try:\n            package = self._download()\n            pkg_folder = self._extract(package)\n            napp_folder = self._get_local_folder(pkg_folder)\n            dst = self._installed / self.user / self.napp\n            self._check_module(dst.parent)\n            shutil.move(str(napp_folder), str(dst))\n        finally:\n            # Delete temporary files\n            if package:\n                Path(package).unlink()\n            if pkg_folder and pkg_folder.exists():\n                shutil.rmtree(str(pkg_folder))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _download(self):\n        repo = self._config.get('napps', 'repo')\n        napp_id = '{}/{}-{}.napp'.format(self.user, self.napp, self.version)\n        uri = os.path.join(repo, napp_id)\n        return urllib.request.urlretrieve(uri)[0]", "response": "Download NApp package from server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts package to a temporary folder.", "response": "def _extract(filename):\n        \"\"\"Extract package to a temporary folder.\n\n        Return:\n            pathlib.Path: Temp dir with package contents.\n\n        \"\"\"\n        random_string = '{:0d}'.format(randint(0, 10**6))\n        tmp = '/tmp/kytos-napp-' + Path(filename).stem + '-' + random_string\n        os.mkdir(tmp)\n        with tarfile.open(filename, 'r:xz') as tar:\n            tar.extractall(tmp)\n        return Path(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a basic NApp structure for you to develop your NApp.", "response": "def create_napp(cls, meta_package=False):\n        \"\"\"Bootstrap a basic NApp structure for you to develop your NApp.\n\n        This will create, on the current folder, a clean structure of a NAPP,\n        filling some contents on this structure.\n        \"\"\"\n        templates_path = SKEL_PATH / 'napp-structure/username/napp'\n\n        ui_templates_path = os.path.join(templates_path, 'ui')\n\n        username = None\n        napp_name = None\n\n        print('--------------------------------------------------------------')\n        print('Welcome to the bootstrap process of your NApp.')\n        print('--------------------------------------------------------------')\n        print('In order to answer both the username and the napp name,')\n        print('You must follow this naming rules:')\n        print(' - name starts with a letter')\n        print(' - name contains only letters, numbers or underscores')\n        print(' - at least three characters')\n        print('--------------------------------------------------------------')\n        print('')\n        while not cls.valid_name(username):\n            username = input('Please, insert your NApps Server username: ')\n\n        while not cls.valid_name(napp_name):\n            napp_name = input('Please, insert your NApp name: ')\n\n        description = input('Please, insert a brief description for your'\n                            'NApp [optional]: ')\n        if not description:\n            # pylint: disable=fixme\n            description = '# TODO: <<<< Insert your NApp description here >>>>'\n            # pylint: enable=fixme\n\n        context = {'username': username, 'napp': napp_name,\n                   'description': description}\n\n        #: Creating the directory structure (username/napp_name)\n        os.makedirs(username, exist_ok=True)\n\n        #: Creating ``__init__.py`` files\n        with open(os.path.join(username, '__init__.py'), 'w') as init_file:\n            init_file.write(f'\"\"\"Napps for the user {username}.\"\"\"\"')\n\n        os.makedirs(os.path.join(username, napp_name))\n\n        #: Creating the other files based on the templates\n        templates = os.listdir(templates_path)\n        templates.remove('ui')\n        templates.remove('openapi.yml.template')\n\n        if meta_package:\n            templates.remove('main.py.template')\n            templates.remove('settings.py.template')\n\n        for tmp in templates:\n            fname = os.path.join(username, napp_name,\n                                 tmp.rsplit('.template')[0])\n            with open(fname, 'w') as file:\n                content = cls.render_template(templates_path, tmp, context)\n                file.write(content)\n\n        if not meta_package:\n            NAppsManager.create_ui_structure(username, napp_name,\n                                             ui_templates_path, context)\n\n        print()\n        print(f'Congratulations! Your NApp has been bootstrapped!\\nNow you '\n              'can go to the directory {username}/{napp_name} and begin to '\n              'code your NApp.')\n        print('Have fun!')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the ui directory structure.", "response": "def create_ui_structure(cls, username, napp_name, ui_templates_path,\n                            context):\n        \"\"\"Create the ui directory structure.\"\"\"\n        for section in ['k-info-panel', 'k-toolbar', 'k-action-menu']:\n            os.makedirs(os.path.join(username, napp_name, 'ui', section))\n\n        templates = os.listdir(ui_templates_path)\n\n        for tmp in templates:\n            fname = os.path.join(username, napp_name, 'ui',\n                                 tmp.rsplit('.template')[0])\n\n            with open(fname, 'w') as file:\n                content = cls.render_template(ui_templates_path, tmp,\n                                              context)\n                file.write(content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_module(folder):\n        if not folder.exists():\n            folder.mkdir(parents=True, exist_ok=True, mode=0o755)\n            (folder / '__init__.py').touch()", "response": "Create module folder with empty __init__. py if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the. napp file that will be sent to the napps server.", "response": "def build_napp_package(napp_name):\n        \"\"\"Build the .napp file to be sent to the napps server.\n\n        Args:\n            napp_identifier (str): Identifier formatted as\n                <username>/<napp_name>\n\n        Return:\n            file_payload (binary): The binary representation of the napp\n                package that will be POSTed to the napp server.\n\n        \"\"\"\n        ignored_extensions = ['.swp', '.pyc', '.napp']\n        ignored_dirs = ['__pycache__', '.git', '.tox']\n        files = os.listdir()\n        for filename in files:\n            if os.path.isfile(filename) and '.' in filename and \\\n                    filename.rsplit('.', 1)[1] in ignored_extensions:\n                files.remove(filename)\n            elif os.path.isdir(filename) and filename in ignored_dirs:\n                files.remove(filename)\n\n        # Create the '.napp' package\n        napp_file = tarfile.open(napp_name + '.napp', 'x:xz')\n        for local_f in files:\n            napp_file.add(local_f)\n        napp_file.close()\n\n        # Get the binary payload of the package\n        file_payload = open(napp_name + '.napp', 'rb')\n\n        # remove the created package from the filesystem\n        os.remove(napp_name + '.napp')\n\n        return file_payload"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating the metadata to send the napp package.", "response": "def create_metadata(*args, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"Generate the metadata to send the napp package.\"\"\"\n        json_filename = kwargs.get('json_filename', 'kytos.json')\n        readme_filename = kwargs.get('readme_filename', 'README.rst')\n        ignore_json = kwargs.get('ignore_json', False)\n        metadata = {}\n\n        if not ignore_json:\n            try:\n                with open(json_filename) as json_file:\n                    metadata = json.load(json_file)\n            except FileNotFoundError:\n                print(\"ERROR: Could not access kytos.json file.\")\n                sys.exit(1)\n\n        try:\n            with open(readme_filename) as readme_file:\n                metadata['readme'] = readme_file.read()\n        except FileNotFoundError:\n            metadata['readme'] = ''\n\n        try:\n            yaml = YAML(typ='safe')\n            openapi_dict = yaml.load(Path('openapi.yml').open())\n            openapi = json.dumps(openapi_dict)\n        except FileNotFoundError:\n            openapi = ''\n        metadata['OpenAPI_Spec'] = openapi\n\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates package and upload it to NApps Server.", "response": "def upload(self, *args, **kwargs):\n        \"\"\"Create package and upload it to NApps Server.\n\n        Raises:\n            FileNotFoundError: If kytos.json is not found.\n\n        \"\"\"\n        self.prepare()\n        metadata = self.create_metadata(*args, **kwargs)\n        package = self.build_napp_package(metadata.get('name'))\n\n        NAppsClient().upload_napp(metadata, package)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a NApp. Raises: requests.HTTPError: When there's a server error.", "response": "def delete(self):\n        \"\"\"Delete a NApp.\n\n        Raises:\n            requests.HTTPError: When there's a server error.\n\n        \"\"\"\n        client = NAppsClient(self._config)\n        client.delete(self.user, self.napp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing NApp to be uploaded by creating openAPI skeleton.", "response": "def prepare(cls):\n        \"\"\"Prepare NApp to be uploaded by creating openAPI skeleton.\"\"\"\n        if cls._ask_openapi():\n            napp_path = Path()\n            tpl_path = SKEL_PATH / 'napp-structure/username/napp'\n            OpenAPI(napp_path, tpl_path).render_template()\n            print('Please, update your openapi.yml file.')\n            sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nasks the user if we should create a new skeleton.", "response": "def _ask_openapi():\n        \"\"\"Return whether we should create a (new) skeleton.\"\"\"\n        if Path('openapi.yml').exists():\n            question = 'Override local openapi.yml with a new skeleton? (y/N) '\n            default = False\n        else:\n            question = 'Do you have REST endpoints and wish to create an API' \\\n                ' skeleton in openapi.yml? (Y/n) '\n            default = True\n\n        while True:\n            answer = input(question)\n            if answer == '':\n                return default\n            if answer.lower() in ['y', 'yes']:\n                return True\n            if answer.lower() in ['n', 'no']:\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreloading a NApp or all NApps.", "response": "def reload(self, napps=None):\n        \"\"\"Reload a NApp or all NApps.\n\n        Args:\n            napps (list): NApp list to be reloaded.\n        Raises:\n            requests.HTTPError: When there's a server error.\n\n        \"\"\"\n        client = NAppsClient(self._config)\n        client.reload_napps(napps)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef populate(self, priority, address, rtr, data):\n        assert isinstance(data, bytes)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 6)\n        self.set_attributes(priority, address, rtr)\n        self.rain = (((data[0] << 8)| data[1]) / 32 ) * 0.1\n        self.light = (((data[2] << 8) | data[3]) / 32 )\n        self.wind = (((data[4] << 8) | data[5]) / 32 ) * 0.1", "response": "Populates the internal state of the object with the contents of the data bytes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_json(self):\n        json_dict = self.to_json_basic()\n        json_dict['rain'] = self.rain\n        json_dict['light'] = self.light\n        json_dict['wind'] = self.wind\n        return json.dumps(json_dict)", "response": "Returns a JSON string representation of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive an image return the thumbnail url", "response": "def _get_thumbnail_url(image):\n    \"\"\" Given a large image, return the thumbnail url \"\"\"\n    lhs, rhs = splitext(image.url)\n    lhs += THUMB_EXT\n    thumb_url = f'{lhs}{rhs}'\n    return thumb_url"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a client from environment variables.", "response": "def from_env(cls, env_filename=None):\n        # type: (Optional[str]) -> Client\n        \"\"\"Create a client from environment variable settings.\n\n        :param basestring env_filename: filename of the environment file, defaults to '.env' in the local dir\n                                        (or parent dir)\n        :return: :class:`pykechain.Client`\n\n        Example\n        -------\n\n        Initiates the pykechain client from the contents of an environment file. Authentication information is optional\n        but ensure that you provide this later in your code. Offered are both username/password authentication and\n        user token authentication.\n\n        .. code-block:: none\n           :caption: .env\n           :name: dot-env\n\n            # User token here (required)\n            KECHAIN_TOKEN=...<secret user token>...\n            KECHAIN_URL=https://an_url.ke-chain.com\n\n            # or use Basic Auth with username/password\n            KECHAIN_USERNAME=...\n            KECHAIN_PASSWORD=...\n\n            # optional add a scope name or scope id\n            KECHAIN_SCOPE=...\n            KECHAIN_SCOPE_ID=...\n\n        >>> client = Client().from_env()\n\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UserWarning)\n            env.read_envfile(env_filename)\n        client = cls(url=env(KechainEnv.KECHAIN_URL))\n\n        if env(KechainEnv.KECHAIN_TOKEN, None):\n            client.login(token=env(KechainEnv.KECHAIN_TOKEN))\n        elif env(KechainEnv.KECHAIN_USERNAME, None) and env(KechainEnv.KECHAIN_PASSWORD, None):\n            client.login(username=env(KechainEnv.KECHAIN_USERNAME), password=env(KechainEnv.KECHAIN_PASSWORD))\n\n        return client"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef login(self, username=None, password=None, token=None):\n        # type: (Optional[str], Optional[str], Optional[str]) -> None\n        \"\"\"Login into KE-chain with either username/password or token.\n\n        :param basestring username: username for your user from KE-chain\n        :param basestring password: password for your user from KE-chain\n        :param basestring token: user authentication token retrieved from KE-chain\n\n        Examples\n        --------\n        Using Token Authentication (retrieve user Token from the KE-chain instance)\n\n        >>> client = Client()\n        >>> client.login(token='<some-super-long-secret-token>')\n\n        Using Basic authentications (Username/Password)\n\n        >>> client = Client()\n        >>> client.login(username='user', password='pw')\n\n        >>> client = Client()\n        >>> client.login('username','password')\n\n        \"\"\"\n        if token:\n            self.headers['Authorization'] = 'Token {}'.format(token)\n            self.auth = None\n        elif username and password:\n            self.headers.pop('Authorization', None)\n            self.auth = (username, password)", "response": "Login into KE - chain with either username password or token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_url(self, resource, **kwargs):\n        # type: (str, **str) -> str\n        \"\"\"Build the correct API url.\"\"\"\n        return urljoin(self.api_root, API_PATH[resource].format(**kwargs))", "response": "Build the correct API url."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves user objects of the entire administration.", "response": "def _retrieve_users(self):\n        \"\"\"\n        Retrieve user objects of the entire administration.\n\n        :return: list of dictionary with users information\n        :rtype: list(dict)\n        -------\n\n        \"\"\"\n        users_url = self._build_url('users')\n        response = self._request('GET', users_url)\n        users = response.json()\n        return users"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming the request on the API.", "response": "def _request(self, method, url, **kwargs):\n        # type: (str, str, **Any) -> requests.Response\n        \"\"\"Perform the request on the API.\"\"\"\n        self.last_request = None\n        self.last_response = self.session.request(method, url, auth=self.auth, headers=self.headers, **kwargs)\n        self.last_request = self.last_response.request\n        self.last_url = self.last_response.url\n\n        if self.last_response.status_code == requests.codes.forbidden:\n            raise ForbiddenError(self.last_response.json()['results'][0]['detail'])\n\n        return self.last_response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting of the versions of the internal KE - chain app modules.", "response": "def app_versions(self):\n        \"\"\"List of the versions of the internal KE-chain 'app' modules.\"\"\"\n        if not self._app_versions:\n            app_versions_url = self._build_url('versions')\n\n            response = self._request('GET', app_versions_url)\n\n            if response.status_code == requests.codes.not_found:\n                self._app_versions = []\n            elif response.status_code == requests.codes.forbidden:\n                raise ForbiddenError(response.json()['results'][0]['detail'])\n            elif response.status_code != requests.codes.ok:\n                raise APIError(\"Could not retrieve app versions: {}\".format(response))\n            else:\n                self._app_versions = response.json().get('results')\n\n        return self._app_versions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmatching an app version against a semantic version string.", "response": "def match_app_version(self, app=None, label=None, version=None, default=False):\n        \"\"\"Match app version against a semantic version string.\n\n        Checks if a KE-chain app matches a version comparison. Uses the `semver` matcher to check.\n\n        `match(\"2.0.0\", \">=1.0.0\")` => `True`\n        `match(\"1.0.0\", \">1.0.0\")` => `False`\n\n        Examples\n        --------\n        >>> client.match_app_version(label='wim', version=\">=1.99\")\n        >>> True\n\n        >>> client.match_app_version(app='kechain2.core.pim', version=\">=1.0.0\")\n        >>> True\n\n        :param app: (optional) appname eg. 'kechain.core.wim'\n        :type app: basestring or None\n        :param label: (optional) app label (last part of the app name) eb 'wim'\n        :type label: basestring or None\n        :param version: semantic version string to match appname version against eg '2.0.0' or '>=2.0.0'\n        :type version: basestring\n        :param default: (optional) boolean to return if the version of the app is not set but the app found.\n                        Set to None to return a NotFoundError when a version if not found in the app.\n        :type default: bool or None\n        :return: True if the version of the app matches against the match_version, otherwise False\n        :raises IllegalArgumentError: if no app nor a label is provided\n        :raises NotFoundError: if the app is not found\n        :raises ValueError: if the version provided is not parseable by semver,\n                            should contain (<operand><major>.<minor>.<patch) where <operand> is '>,<,>=,<=,=='\n\n        \"\"\"\n        if not app or not label and not (app and label):\n            target_app = [a for a in self.app_versions if a.get('app') == app or a.get('label') == label]\n            if not target_app and not isinstance(default, bool):\n                raise NotFoundError(\"Could not find the app or label provided\")\n            elif not target_app and isinstance(default, bool):\n                return default\n        else:\n            raise IllegalArgumentError(\"Please provide either app or label\")\n\n        if not version:\n            raise IllegalArgumentError(\"Please provide semantic version string including operand eg: `>=1.0.0`\")\n\n        app_version = target_app[0].get('version')\n\n        if target_app and app_version and version:\n            import semver\n            return semver.match(app_version, version)\n        elif not app_version:\n            if isinstance(default, bool):\n                return default\n            else:\n                raise NotFoundError(\"No version found on the app '{}'\".format(target_app[0].get('app')))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreload an object from server. This method is immutable and will return a new object.", "response": "def reload(self, obj, extra_params=None):\n        \"\"\"Reload an object from server. This method is immutable and will return a new object.\n\n        :param obj: object to reload\n        :type obj: :py:obj:`obj`\n        :param extra_params: additional object specific extra query string params (eg for activity)\n        :type extra_params: dict\n        :return: a new object\n        :raises NotFoundError: if original object is not found or deleted in the mean time\n        \"\"\"\n        if not obj._json_data.get('url'):  # pragma: no cover\n            raise NotFoundError(\"Could not reload object, there is no url for object '{}' configured\".format(obj))\n\n        response = self._request('GET', obj._json_data.get('url'), params=extra_params)\n\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not reload object ({})\".format(response))\n\n        data = response.json()\n\n        return obj.__class__(data['results'][0], client=self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scopes(self, name=None, pk=None, status=ScopeStatus.ACTIVE, **kwargs):\n        # type: (Optional[str], Optional[str], Optional[str], **Any) -> List[Scope]\n        \"\"\"Return all scopes visible / accessible for the logged in user.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param name: if provided, filter the search for a scope/project by name\n        :type name: basestring or None\n        :param pk: if provided, filter the search by scope_id\n        :type pk: basestring or None\n        :param status: if provided, filter the search for the status. eg. 'ACTIVE', 'TEMPLATE', 'LIBRARY'\n        :type status: basestring or None\n        :param kwargs: optional additional search arguments\n        :type kwargs: dict or None\n        :return: list of `Scopes`\n        :rtype: list(:class:`models.Scope`)\n        :raises NotFoundError: if no scopes are not found.\n\n        Example\n        -------\n\n        >>> client = Client(url='https://default.localhost:9443', verify=False)\n        >>> client.login('admin','pass')\n        >>> client.scopes()  # doctest: Ellipsis\n        ...\n\n        >>> client.scopes(name=\"Bike Project\")  # doctest: Ellipsis\n        ...\n\n        >>> last_request = client.last_request  # doctest: Ellipsis\n        ...\n        \"\"\"\n        request_params = {\n            'name': name,\n            'id': pk,\n            'status': status,\n        }\n        if kwargs:\n            request_params.update(**kwargs)\n\n        response = self._request('GET', self._build_url('scopes'), params=request_params)\n\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not retrieve scopes\")\n\n        data = response.json()\n\n        return [Scope(s, client=self) for s in data['results']]", "response": "Return all scopes visible or accessible for the logged in user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scope(self, *args, **kwargs):\n        # type: (*Any, **Any) -> Scope\n        \"\"\"Return a single scope based on the provided name.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :return: a single :class:`models.Scope`\n        :raises NotFoundError: When no `Scope` is found\n        :raises MultipleFoundError: When more than a single `Scope` is found\n        \"\"\"\n        _scopes = self.scopes(*args, **kwargs)\n\n        if len(_scopes) == 0:\n            raise NotFoundError(\"No scope fits criteria\")\n        if len(_scopes) != 1:\n            raise MultipleFoundError(\"Multiple scopes fit criteria\")\n\n        return _scopes[0]", "response": "Returns a single scope based on the provided name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for activities with optional name pk and scope filter.", "response": "def activities(self, name=None, pk=None, scope=None, **kwargs):\n        # type: (Optional[str], Optional[str], Optional[str], **Any) -> List[Activity]\n        \"\"\"Search for activities with optional name, pk and scope filter.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param pk: id (primary key) of the activity to retrieve\n        :type pk: basestring or None\n        :param name: filter the activities by name\n        :type name: basestring or None\n        :param scope: filter by scope id\n        :type scope: basestring or None\n        :return: list of :class:`models.Activity`\n        :raises NotFoundError: If no `Activities` are found\n        \"\"\"\n        request_params = {\n            'id': pk,\n            'name': name,\n            'scope': scope\n        }\n\n        # update the fields query params\n        # for 'kechain.core.wim >= 2.0.0' add additional API params\n        if self.match_app_version(label='wim', version='>=2.0.0', default=False):\n            request_params.update(API_EXTRA_PARAMS['activity'])\n\n        if kwargs:\n            request_params.update(**kwargs)\n\n        response = self._request('GET', self._build_url('activities'), params=request_params)\n\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not retrieve activities. Server responded with {}\".format(str(response)))\n\n        data = response.json()\n\n        # for 'kechain.core.wim >= 2.0.0' we return Activity2, otherwise Activity1\n        if self.match_app_version(label='wim', version='<2.0.0', default=True):\n            # WIM1\n            return [Activity(a, client=self) for a in data['results']]\n        else:\n            # WIM2\n            return [Activity2(a, client=self) for a in data['results']]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef activity(self, *args, **kwargs):\n        # type: (*Any, **Any) -> Activity\n        \"\"\"Search for a single activity.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param pk: id (primary key) of the activity to retrieve\n        :type pk: basestring or None\n        :param name: filter the activities by name\n        :type name: basestring or None\n        :param scope: filter by scope id\n        :type scope: basestring or None\n        :return: a single :class:`models.Activity`\n        :raises NotFoundError: When no `Activity` is found\n        :raises MultipleFoundError: When more than a single `Activity` is found\n        \"\"\"\n        _activities = self.activities(*args, **kwargs)\n\n        if len(_activities) == 0:\n            raise NotFoundError(\"No activity fits criteria\")\n        if len(_activities) != 1:\n            raise MultipleFoundError(\"Multiple activities fit criteria\")\n\n        return _activities[0]", "response": "Search for a single activity."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves multiple KE - chain parts.", "response": "def parts(self,\n              name=None,  # type: Optional[str]\n              pk=None,  # type: Optional[str]\n              model=None,  # type: Optional[Part]\n              category=Category.INSTANCE,  # type: Optional[str]\n              bucket=None,  # type: Optional[str]\n              parent=None,  # type: Optional[str]\n              activity=None,  # type: Optional[str]\n              limit=None,  # type: Optional[int]\n              batch=100,  # type: int\n              **kwargs):\n        # type: (...) -> PartSet\n        \"\"\"Retrieve multiple KE-chain parts.\n\n        If no parameters are provided, all parts are retrieved.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param name: filter on name\n        :type name: basestring or None\n        :param pk: filter on primary key\n        :type pk: basestring or None\n        :param model: filter on model_id\n        :type model: basestring or None\n        :param category: filter on category (INSTANCE, MODEL, None)\n        :type category: basestring or None\n        :param bucket: filter on bucket_id\n        :type bucket: basestring or None\n        :param parent: filter on the parent_id, returns all childrent of the parent_id\n        :type parent: basestring or None\n        :param activity: filter on activity_id\n        :type activity: basestring or None\n        :param limit: limit the return to # items (default unlimited, so return all results)\n        :type limit: int or None\n        :param batch: limit the batch size to # items (defaults to 100 items per batch)\n        :type batch: int or None\n        :param kwargs: additional `keyword=value` arguments for the api\n        :type kwargs: dict or None\n        :return: :class:`models.PartSet` which is an iterator of :class:`models.Part`\n        :raises NotFoundError: If no `Part` is found\n\n        Examples\n        --------\n        Return all parts (defaults to instances) with exact name 'Gears'.\n\n        >>> client = Client(url='https://default.localhost:9443', verify=False)\n        >>> client.login('admin','pass')\n        >>> client.parts(name='Gears')  # doctest:Ellipsis\n        ...\n\n        Return all parts with category is MODEL or category is INSTANCE.\n\n        >>> client.parts(name='Gears', category=None)  # doctest:Ellipsis\n        ...\n\n        Return a maximum of 5 parts\n\n        >>> client.parts(limit=5)  # doctest:Ellipsis\n        ...\n\n        \"\"\"\n        # if limit is provided and the batchsize is bigger than the limit, ensure that the batch size is maximised\n        if limit and limit < batch:\n            batch = limit\n\n        request_params = {\n            'id': pk,\n            'name': name,\n            'model': model.id if model else None,\n            'category': category,\n            'bucket': bucket,\n            'parent': parent,\n            'activity_id': activity,\n            'limit': batch\n        }\n\n        if kwargs:\n            request_params.update(**kwargs)\n\n        response = self._request('GET', self._build_url('parts'), params=request_params)\n\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not retrieve parts\")\n\n        data = response.json()\n\n        part_results = data['results']\n\n        if batch and data.get('next'):\n            while data['next']:\n                # respect the limit if set to > 0\n                if limit and len(part_results) >= limit:\n                    break\n                response = self._request('GET', data['next'])\n                data = response.json()\n                part_results.extend(data['results'])\n\n        return PartSet((Part(p, client=self) for p in part_results))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a single KE - chain Part instance.", "response": "def part(self, *args, **kwargs):\n        # type: (*Any, **Any) -> Part\n        \"\"\"Retrieve single KE-chain part.\n\n        Uses the same interface as the :func:`parts` method but returns only a single pykechain :class:`models.Part`\n        instance.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :return: a single :class:`models.Part`\n        :raises NotFoundError: When no `Part` is found\n        :raises MultipleFoundError: When more than a single `Part` is found\n        \"\"\"\n        _parts = self.parts(*args, **kwargs)\n\n        if len(_parts) == 0:\n            raise NotFoundError(\"No part fits criteria\")\n        if len(_parts) != 1:\n            raise MultipleFoundError(\"Multiple parts fit criteria\")\n\n        return _parts[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef model(self, *args, **kwargs):\n        # type: (*Any, **Any) -> Part\n        \"\"\"Retrieve single KE-chain part model.\n\n        Uses the same interface as the :func:`part` method but returns only a single pykechain\n        :class:`models.Part` instance of category `MODEL`.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :return: a single :class:`models.Part`\n        :raises NotFoundError: When no `Part` is found\n        :raises MultipleFoundError: When more than a single `Part` is found\n        \"\"\"\n        kwargs['category'] = Category.MODEL\n        _parts = self.parts(*args, **kwargs)\n\n        if len(_parts) == 0:\n            raise NotFoundError(\"No model fits criteria\")\n        if len(_parts) != 1:\n            raise MultipleFoundError(\"Multiple models fit criteria\")\n\n        return _parts[0]", "response": "Retrieve a single KE - chain part model. Uses the same interface as the part method but returns only a single KE - chain Part instance of category MODEL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef property(self, *args, **kwargs):\n        # type: (*Any, **Any) -> Property\n        \"\"\"Retrieve single KE-chain Property.\n\n        Uses the same interface as the :func:`properties` method but returns only a single pykechain :class:\n        `models.Property` instance.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :return: a single :class:`models.Property`\n        :raises NotFoundError: When no `Property` is found\n        :raises MultipleFoundError: When more than a single `Property` is found\n        \"\"\"\n        _properties = self.properties(*args, **kwargs)\n\n        if len(_properties) == 0:\n            raise NotFoundError(\"No property fits criteria\")\n        if len(_properties) != 1:\n            raise MultipleFoundError(\"Multiple properties fit criteria\")\n\n        return _properties[0]", "response": "Retrieve a single KE - chain Property instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve properties. If additional `keyword=value` arguments are provided, these are added to the request parameters. Please refer to the documentation of the KE-chain API for additional query parameters. :param name: name to limit the search for. :type name: basestring or None :param pk: primary key or id (UUID) of the property to search for :type pk: basestring or None :param category: filter the properties by category. Defaults to INSTANCE. Other options MODEL or None :type category: basestring or None :param kwargs: (optional) additional search keyword arguments :type kwargs: dict or None :return: list of :class:`models.Property` :raises NotFoundError: When no `Property` is found", "response": "def properties(self, name=None, pk=None, category=Category.INSTANCE, **kwargs):\n        # type: (Optional[str], Optional[str], Optional[str], **Any) -> List[Property]\n        \"\"\"Retrieve properties.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param name: name to limit the search for.\n        :type name: basestring or None\n        :param pk: primary key or id (UUID) of the property to search for\n        :type pk: basestring or None\n        :param category: filter the properties by category. Defaults to INSTANCE. Other options MODEL or None\n        :type category: basestring or None\n        :param kwargs: (optional) additional search keyword arguments\n        :type kwargs: dict or None\n        :return: list of :class:`models.Property`\n        :raises NotFoundError: When no `Property` is found\n        \"\"\"\n        request_params = {\n            'name': name,\n            'id': pk,\n            'category': category\n        }\n        if kwargs:\n            request_params.update(**kwargs)\n\n        response = self._request('GET', self._build_url('properties'), params=request_params)\n\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not retrieve properties\")\n\n        data = response.json()\n\n        return [Property.create(p, client=self) for p in data['results']]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef services(self, name=None, pk=None, scope=None, **kwargs):\n        request_params = {\n            'name': name,\n            'id': pk,\n            'scope': scope\n        }\n        if kwargs:\n            request_params.update(**kwargs)\n\n        response = self._request('GET', self._build_url('services'), params=request_params)\n\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not retrieve services\")\n\n        data = response.json()\n        return [Service(service, client=self) for service in data['results']]", "response": "Retrieve Services.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param name: (optional) name to limit the search for\n        :type name: basestring or None\n        :param pk: (optional) primary key or id (UUID) of the service to search for\n        :type pk: basestring or None\n        :param scope: (optional) id (UUID) of the scope to search in\n        :type scope: basestring or None\n        :param kwargs: (optional) additional search keyword arguments\n        :type kwargs: dict or None\n        :return: list of :class:`models.Service` objects\n        :raises NotFoundError: When no `Service` objects are found"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a single KE - chain Service object.", "response": "def service(self, name=None, pk=None, scope=None, **kwargs):\n        \"\"\"\n        Retrieve single KE-chain Service.\n\n        Uses the same interface as the :func:`services` method but returns only a single pykechain\n        :class:`models.Service` instance.\n\n        :param name: (optional) name to limit the search for\n        :type name: basestring or None\n        :param pk: (optional) primary key or id (UUID) of the service to search for\n        :type pk: basestring or None\n        :param scope: (optional) id (UUID) of the scope to search in\n        :type scope: basestring or None\n        :param kwargs: (optional) additional search keyword arguments\n        :type kwargs: dict or None\n        :return: a single :class:`models.Service` object\n        :raises NotFoundError: When no `Service` object is found\n        :raises MultipleFoundError: When more than a single `Service` object is found\n        \"\"\"\n        _services = self.services(name=name, pk=pk, scope=scope, **kwargs)\n\n        if len(_services) == 0:\n            raise NotFoundError(\"No service fits criteria\")\n        if len(_services) != 1:\n            raise MultipleFoundError(\"Multiple services fit criteria\")\n\n        return _services[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve Service Executions. If additional `keyword=value` arguments are provided, these are added to the request parameters. Please refer to the documentation of the KE-chain API for additional query parameters. :param name: (optional) name to limit the search for :type name: basestring or None :param pk: (optional) primary key or id (UUID) of the service to search for :type pk: basestring or None :param scope: (optional) id (UUID) of the scope to search in :type scope: basestring or None :param service: (optional) service UUID to filter on :type service: basestring or None :param kwargs: (optional) additional search keyword arguments :type kwargs: dict or None :return: a single :class:`models.ServiceExecution` object :raises NotFoundError: When no `ServiceExecution` object is found", "response": "def service_executions(self, name=None, pk=None, scope=None, service=None, **kwargs):\n        \"\"\"\n        Retrieve Service Executions.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param name: (optional) name to limit the search for\n        :type name: basestring or None\n        :param pk: (optional) primary key or id (UUID) of the service to search for\n        :type pk: basestring or None\n        :param scope: (optional) id (UUID) of the scope to search in\n        :type scope: basestring or None\n        :param service: (optional) service UUID to filter on\n        :type service: basestring or None\n        :param kwargs: (optional) additional search keyword arguments\n        :type kwargs: dict or None\n        :return: a single :class:`models.ServiceExecution` object\n        :raises NotFoundError: When no `ServiceExecution` object is found\n        \"\"\"\n        request_params = {\n            'name': name,\n            'id': pk,\n            'service': service,\n            'scope': scope\n        }\n        if kwargs:\n            request_params.update(**kwargs)\n\n        r = self._request('GET', self._build_url('service_executions'), params=request_params)\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not retrieve service executions\")\n\n        data = r.json()\n        return [ServiceExecution(service_exeuction, client=self) for service_exeuction in data['results']]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a single KE - chain ServiceExecution object.", "response": "def service_execution(self, name=None, pk=None, scope=None, service=None, **kwargs):\n        \"\"\"\n        Retrieve single KE-chain ServiceExecution.\n\n        Uses the same interface as the :func:`service_executions` method but returns only a single\n        pykechain :class:`models.ServiceExecution` instance.\n\n        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please\n        refer to the documentation of the KE-chain API for additional query parameters.\n\n        :param name: (optional) name to limit the search for\n        :type name: basestring or None\n        :param pk: (optional) primary key or id (UUID) of the service to search for\n        :type pk: basestring or None\n        :param scope: (optional) id (UUID) of the scope to search in\n        :type scope: basestring or None\n        :param kwargs: (optional) additional search keyword arguments\n        :type kwargs: dict or None\n        :return: a single :class:`models.ServiceExecution` object\n        :raises NotFoundError: When no `ServiceExecution` object is found\n        :raises MultipleFoundError: When more than a single `ServiceExecution` object is found\n        \"\"\"\n        _service_executions = self.service_executions(name=name, pk=pk, scope=scope, service=service, **kwargs)\n\n        if len(_service_executions) == 0:\n            raise NotFoundError(\"No service execution fits criteria\")\n        if len(_service_executions) != 1:\n            raise MultipleFoundError(\"Multiple service executions fit criteria\")\n\n        return _service_executions[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef users(self, username=None, pk=None, **kwargs):\n        request_params = {\n            'username': username,\n            'pk': pk,\n        }\n        if kwargs:\n            request_params.update(**kwargs)\n\n        r = self._request('GET', self._build_url('users'), params=request_params)\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not find users: '{}'\".format(r.json()))\n\n        data = r.json()\n        return [User(user, client=self) for user in data['results']]", "response": "Returns a list of users of KE - chain."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a single user of KE - chain.", "response": "def user(self, username=None, pk=None, **kwargs):\n        \"\"\"\n        User of KE-chain.\n\n        Provides single user of :class:`User` of KE-chain. You can filter on username or id or an advanced filter.\n\n        :param username: (optional) username to filter\n        :type username: basestring or None\n        :param pk: (optional) id of the user to filter\n        :type pk: basestring or None\n        :param kwargs: Additional filtering keyword=value arguments\n        :type kwargs: dict or None\n        :return: List of :class:`User`\n        :raises NotFoundError: when a user could not be found\n        :raises MultipleFoundError: when more than a single user can be found\n        \"\"\"\n        _users = self.users(username=username, pk=pk, **kwargs)\n\n        if len(_users) == 0:\n            raise NotFoundError(\"No user criteria matches\")\n        if len(_users) != 1:\n            raise MultipleFoundError(\"Multiple users fit criteria\")\n\n        return _users[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprovides a team of KE - chain.", "response": "def team(self, name=None, id=None, is_hidden=False, **kwargs):\n        \"\"\"\n        Team of KE-chain.\n\n        Provides a team of :class:`Team` of KE-chain. You can filter on team name or provide id.\n\n        :param name: (optional) team name to filter\n        :type name: basestring or None\n        :param id: (optional) id of the user to filter\n        :type id: basestring or None\n        :param is_hidden: (optional) boolean to show non-hidden or hidden teams or both (None) (default is non-hidden)\n        :type is_hidden: bool or None\n        :param kwargs: Additional filtering keyword=value arguments\n        :type kwargs: dict or None\n        :return: List of :class:`Team`\n        :raises NotFoundError: when a user could not be found\n        :raises MultipleFoundError: when more than a single user can be found\n        \"\"\"\n        _teams = self.teams(name=name, id=id, **kwargs)\n\n        if len(_teams) == 0:\n            raise NotFoundError(\"No team criteria matches\")\n        if len(_teams) != 1:\n            raise MultipleFoundError(\"Multiple teams fit criteria\")\n\n        return _teams[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of teams of KE - chain.", "response": "def teams(self, name=None, id=None, is_hidden=False, **kwargs):\n        \"\"\"\n        Teams of KE-chain.\n\n        Provide a list of :class:`Team`s of KE-chain. You can filter on teamname or id or any other advanced filter.\n\n        :param name: (optional) teamname to filter\n        :type name: basestring or None\n        :param id: (optional) id of the team to filter\n        :type id: basestring or None\n        :param is_hidden: (optional) boolean to show non-hidden or hidden teams or both (None) (default is non-hidden)\n        :type is_hidden: bool or None\n        :param kwargs: Additional filtering keyword=value arguments\n        :type kwargs: dict or None\n        :return: List of :class:`Teams`\n        :raises NotFoundError: when a team could not be found\n        \"\"\"\n        request_params = {\n            'name': name,\n            'id': id,\n            'is_hidden': is_hidden\n        }\n        if kwargs:\n            request_params.update(**kwargs)\n\n        r = self._request('GET', self._build_url('teams'), params=request_params)\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise NotFoundError(\"Could not find teams: '{}'\".format(r.json()))\n\n        data = r.json()\n        return [Team(team, client=self) for team in data['results']]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_activity(self, *args, **kwargs):\n        if self.match_app_version(label='wim', version='<2.0.0', default=True):\n            # for wim1\n            if 'activity_type' in kwargs:\n                warnings.warn('For WIM versions 1, you need to ensure to use `activity_class`. Update your code; '\n                              'This will be deprecated in APR2018.')\n                activity_type = kwargs.pop('activity_type')\n                if activity_type not in ActivityType.values():\n                    raise IllegalArgumentError(\n                        \"Please provide accepted activity_type: '{}' not allowed\".format(activity_type))\n                kwargs['activity_class'] = WIMCompatibleActivityTypes.get(activity_type)\n            if 'parent' in kwargs:\n                warnings.warn('For WIM versions 1, you need to ensure to use `process`. Update your code; '\n                              'This will be deprecated in APR2018.')\n                kwargs['process'] = kwargs.pop('parent')\n            return self._create_activity1(*args, **kwargs)\n        else:\n            # for wim2\n            # make old calls compatible with WIM2\n            if 'activity_class' in kwargs:\n                warnings.warn('For WIM versions 2, you need to ensure to use `activity_type`. Update your code; '\n                              'This will be deprecated in APR2018.')\n                activity_class = kwargs.pop('activity_class')\n                if activity_class not in ActivityType.values():\n                    raise IllegalArgumentError(\n                        \"Please provide accepted activity_type: '{}' not allowed\".format(activity_class))\n                kwargs['activity_type'] = WIMCompatibleActivityTypes.get(activity_class)\n            if 'process' in kwargs:\n                warnings.warn('For WIM versions 2, you need to ensure to use `parent` instead of `process`. Update '\n                              'your code; This will be deprecated in APR2018.')\n                kwargs['parent'] = kwargs.pop('process')\n            return self._create_activity2(*args, **kwargs)", "response": "This method creates a new activity in a new application."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new activity.", "response": "def _create_activity1(self, process, name, activity_class=\"UserTask\"):\n        \"\"\"Create a new activity.\n\n        :param process: parent process id\n        :type process: basestring\n        :param name: new activity name\n        :type name: basestring\n        :param activity_class: type of activity: UserTask (default) or Subprocess\n        :type activity_class: basestring\n        :return: the created :class:`models.Activity`\n        :raises IllegalArgumentError: When the provided arguments are incorrect\n        :raises APIError: When the object could not be created\n        \"\"\"\n        if self.match_app_version(label='wim', version='>=2.0.0', default=False):\n            raise APIError('This method is only compatible with versions of KE-chain where the internal `wim` module '\n                           'has a version <=2.0.0. Use the `Client.create_activity2()` method.')\n\n        if activity_class and activity_class not in ActivityType.values():\n            raise IllegalArgumentError(\n                \"Please provide accepted activity_class (provided:{} accepted:{})\".format(\n                    activity_class, (ActivityType.USERTASK, ActivityType.SUBPROCESS, ActivityType.SERVICETASK)))\n        data = {\n            \"name\": name,\n            \"process\": process,\n            \"activity_class\": activity_class\n        }\n\n        response = self._request('POST', self._build_url('activities'), data=data)\n\n        if response.status_code != requests.codes.created:  # pragma: no cover\n            raise APIError(\"Could not create activity\")\n\n        data = response.json()\n\n        return Activity(data['results'][0], client=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_activity2(self, parent, name, activity_type=ActivityType.TASK):\n        # WIM1: activity_class, WIM2: activity_type\n        if self.match_app_version(label='wim', version='<2.0.0', default=True):\n            raise APIError('This method is only compatible with versions of KE-chain where the internal `wim` module '\n                           'has a version >=2.0.0. Use the `Client.create_activity()` method.')\n\n        if activity_type and activity_type not in ActivityType.values():\n            raise IllegalArgumentError(\"Please provide accepted activity_type (provided:{} accepted:{})\".\n                                       format(activity_type, ActivityType.values()))\n        if isinstance(parent, (Activity, Activity2)):\n            parent = parent.id\n        elif is_uuid(parent):\n            parent = parent\n        else:\n            raise IllegalArgumentError(\"Please provide either an activity object or a UUID\")\n\n        data = {\n            \"name\": name,\n            \"parent_id\": parent,\n            \"activity_type\": activity_type\n        }\n\n        response = self._request('POST', self._build_url('activities'), data=data,\n                                 params=API_EXTRA_PARAMS['activities'])\n\n        if response.status_code != requests.codes.created:  # pragma: no cover\n            raise APIError(\"Could not create activity\")\n\n        data = response.json()\n\n        return Activity2(data['results'][0], client=self)", "response": "Create a new activity."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a part internal core function.", "response": "def _create_part(self, action, data, **kwargs):\n        \"\"\"Create a part internal core function.\"\"\"\n        # suppress_kevents should be in the data (not the query_params)\n        if 'suppress_kevents' in kwargs:\n            data['suppress_kevents'] = kwargs.pop('suppress_kevents')\n\n        # prepare url query parameters\n        query_params = kwargs\n        query_params['select_action'] = action\n\n        response = self._request('POST', self._build_url('parts'),\n                                 params=query_params,  # {\"select_action\": action},\n                                 data=data)\n\n        if response.status_code != requests.codes.created:\n            raise APIError(\"Could not create part, {}: {}\".format(str(response), response.content))\n\n        return Part(response.json()['results'][0], client=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new Part instance from a given model under a given parent.", "response": "def create_part(self, parent, model, name=None, **kwargs):\n        \"\"\"Create a new part instance from a given model under a given parent.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :param parent: parent part instance of the new instance\n        :type parent: :class:`models.Part`\n        :param model: target part model on which the new instance is based\n        :type model: :class:`models.Part`\n        :param name: new part name\n        :type name: basestring\n        :param kwargs: (optional) additional keyword=value arguments\n        :return: Part (category = instance)\n        :return: :class:`models.Part` with category `INSTANCE`\n        :raises IllegalArgumentError: When the provided arguments are incorrect\n        :raises APIError: if the `Part` could not be created\n        \"\"\"\n        if parent.category != Category.INSTANCE:\n            raise IllegalArgumentError(\"The parent should be an category 'INSTANCE'\")\n        if model.category != Category.MODEL:\n            raise IllegalArgumentError(\"The models should be of category 'MODEL'\")\n\n        if not name:\n            name = model.name\n\n        data = {\n            \"name\": name,\n            \"parent\": parent.id,\n            \"model\": model.id\n        }\n\n        return self._create_part(action=\"new_instance\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new child model under a given parent.", "response": "def create_model(self, parent, name, multiplicity='ZERO_MANY', **kwargs):\n        \"\"\"Create a new child model under a given parent.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :param parent: parent model\n        :param name: new model name\n        :param parent: parent part instance\n        :type parent: :class:`models.Part`\n        :param name: new part name\n        :type name: basestring\n        :param multiplicity: choose between ZERO_ONE, ONE, ZERO_MANY, ONE_MANY or M_N\n        :type multiplicity: basestring\n        :param kwargs: (optional) additional keyword=value arguments\n        :type kwargs: dict\n        :return: :class:`models.Part` with category `MODEL`\n        :raises IllegalArgumentError: When the provided arguments are incorrect\n        :raises APIError: if the `Part` could not be created\n        \"\"\"\n        if parent.category != Category.MODEL:\n            raise IllegalArgumentError(\"The parent should be of category 'MODEL'\")\n\n        data = {\n            \"name\": name,\n            \"parent\": parent.id,\n            \"multiplicity\": multiplicity\n        }\n\n        return self._create_part(action=\"create_child_model\", data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_clone(self, parent, part, **kwargs):\n        if part.category == Category.MODEL:\n            select_action = 'clone_model'\n        else:\n            select_action = 'clone_instance'\n\n        data = {\n            \"part\": part.id,\n            \"parent\": parent.id,\n            \"suppress_kevents\": kwargs.pop('suppress_kevents', None)\n        }\n\n        # prepare url query parameters\n        query_params = kwargs\n        query_params['select_action'] = select_action\n\n        response = self._request('POST', self._build_url('parts'),\n                                 params=query_params,\n                                 data=data)\n\n        if response.status_code != requests.codes.created:\n            raise APIError(\"Could not clone part, {}: {}\".format(str(response), response.content))\n\n        return Part(response.json()['results'][0], client=self)", "response": "Create a new Part under the Parent."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_proxy_model(self, model, parent, name, multiplicity='ZERO_MANY', **kwargs):\n        if model.category != Category.MODEL:\n            raise IllegalArgumentError(\"The model should be of category MODEL\")\n        if parent.category != Category.MODEL:\n            raise IllegalArgumentError(\"The parent should be of category MODEL\")\n\n        data = {\n            \"name\": name,\n            \"model\": model.id,\n            \"parent\": parent.id,\n            \"multiplicity\": multiplicity\n        }\n\n        return self._create_part(action='create_proxy_model', data=data, **kwargs)", "response": "Creates a new proxy model for the given model and parent."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_property(self, model, name, description=None, property_type=PropertyType.CHAR_VALUE, default_value=None,\n                        unit=None, options=None):\n        \"\"\"Create a new property model under a given model.\n\n        Use the :class:`enums.PropertyType` to select which property type to create to ensure that you\n        provide the correct values to the KE-chain backend. The default is a `PropertyType.CHAR_VALUE` which is a\n        single line text in KE-chain.\n\n        :param model: parent model\n        :type model: :class:`models.Part`\n        :param name: property model name\n        :type name: basestring\n        :param description: property model description (optional)\n        :type description: basestring or None\n        :param property_type: choose one of the :class:`enums.PropertyType`, defaults to `PropertyType.CHAR_VALUE`.\n        :type property_type: basestring or None\n        :param default_value: (optional) default value used for part instances when creating a model.\n        :type default_value: any\n        :param unit: (optional) unit of the property\n        :type unit: basestring or None\n        :param options: (optional) property options (eg. validators or 'single selectlist choices')\n        :type options: basestring or None\n        :return: a :class:`models.Property` with category `MODEL`\n        :raises IllegalArgumentError: When the provided arguments are incorrect\n        :raises APIError: if the `Property` model could not be created\n        \"\"\"\n        if model.category != Category.MODEL:\n            raise IllegalArgumentError(\"The model should be of category MODEL\")\n\n        if not property_type.endswith('_VALUE'):\n            warnings.warn(\"Please use the `PropertyType` enumeration to ensure providing correct \"\n                          \"values to the backend.\", UserWarning)\n            property_type = '{}_VALUE'.format(property_type.upper())\n\n        if property_type not in PropertyType.values():\n            raise IllegalArgumentError(\"Please provide a valid propertytype, please use one of `enums.PropertyType`. \"\n                                       \"Got: '{}'\".format(property_type))\n\n        # because the references value only accepts a single 'model_id' in the default value, we need to convert this\n        # to a single value from the list of values.\n        if property_type in (PropertyType.REFERENCE_VALUE, PropertyType.REFERENCES_VALUE) and \\\n                isinstance(default_value, (list, tuple)) and default_value:\n            default_value = default_value[0]\n\n        data = {\n            \"name\": name,\n            \"part\": model.id,\n            \"description\": description or '',\n            \"property_type\": property_type.upper(),\n            \"value\": default_value,\n            \"unit\": unit or '',\n            \"options\": options or {}\n        }\n\n        # # We add options after the fact only if they are available, otherwise the options will be set to null in the\n        # # request and that can't be handled by KE-chain.\n        # if options:\n        #     data['options'] = options\n\n        response = self._request('POST', self._build_url('properties'),\n                                 json=data)\n\n        if response.status_code != requests.codes.created:\n            raise APIError(\"Could not create property\")\n\n        prop = Property.create(response.json()['results'][0], client=self)\n\n        model.properties.append(prop)\n\n        return prop", "response": "Create a new property model under a given model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_service(self, name, scope, description=None, version=None,\n                       service_type=ServiceType.PYTHON_SCRIPT,\n                       environment_version=ServiceEnvironmentVersion.PYTHON_3_5,\n                       pkg_path=None):\n        \"\"\"\n        Create a Service.\n\n        A service can be created only providing the name (and scope). Other information can be added later.\n        If you provide a path to the `kecpkg` (or python script) to upload (`pkg_path`) on creation,\n        this `kecpkg` will be uploaded in one go. If the later fails, the service is still there, and the package is\n        not uploaded.\n\n        Permission to upload a script is restricted to a superuser, a user in the `GG:Configurators` group and a Scope\n        Manager of the scope to which you are uploading the script.\n\n        :param name: Name of the service\n        :type name: basestring\n        :param scope: Scope where the create the Service under\n        :type scope: :class:`models.Scope`\n        :param description: (optional) description of the Service\n        :type description: basestring or None\n        :param version: (optional) version information of the Service\n        :type version: basestring or None\n        :param service_type: (optional) service type of the service (refer to :class:`pykechain.enums.ServiceType`),\n                             defaults to `PYTHON_SCRIPT`\n        :type service_type: basestring or None\n        :param environment_version: (optional) execution environment of the service (refer to\n         :class:`pykechain.enums.ServiceEnvironmentVersion`), defaults to `PYTHON_3_5`\n        :type environment_version: basestring or None\n        :param pkg_path: (optional) full path name to the `kecpkg` (or python script) to upload\n        :type pkg_path: basestring or None\n        :return: the created :class:`models.Service`\n        :raises IllegalArgumentError: When the provided arguments are incorrect\n        :raises APIError: In case of failure of the creation or failure to upload the pkg_path\n        :raises OSError: In case of failure to locate the `pkg_path`\n        \"\"\"\n        if service_type not in ServiceType.values():\n            raise IllegalArgumentError(\"The type should be of one of {}\".format(ServiceType.values()))\n\n        if environment_version not in ServiceEnvironmentVersion.values():\n            raise IllegalArgumentError(\"The environment version should be of one of {}\".\n                                       format(ServiceEnvironmentVersion.values()))\n\n        data = {\n            \"name\": name,\n            \"scope\": scope,\n            \"description\": description,\n            \"script_type\": service_type,\n            \"script_version\": version,\n            \"env_version\": environment_version,\n        }\n\n        response = self._request('POST', self._build_url('services'),\n                                 data=data)\n\n        if response.status_code != requests.codes.created:  # pragma: no cover\n            raise APIError(\"Could not create service ({})\".format((response, response.json())))\n\n        service = Service(response.json().get('results')[0], client=self)\n\n        if pkg_path:\n            # upload the package\n            service.upload(pkg_path)\n\n        # refresh service contents in place\n        service.refresh()\n\n        return service", "response": "Creates a new service under the given name scope description version and environment version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new scope.", "response": "def create_scope(self, name, status=ScopeStatus.ACTIVE, description=None, tags=None, start_date=None, due_date=None,\n                     team=None, **kwargs):\n        \"\"\"\n        Create a Scope.\n\n        This will create a scope if the client has the right to do so. Sufficient permissions to create a scope are a\n        superuser, a user in the `GG:Configurators` group or `GG:Managers` group.\n\n        ..versionadded: 2.6\n\n        :param name: Name of the scope\n        :type name: basestring\n        :param status: choose one of the :class:`enums.ScopeStatus`, defaults to `ScopeStatus.ACTIVE`\n        :type status: basestring or None\n        :param description: (optional) Description of the scope\n        :type description: basestring or None\n        :param tags: (optional) List of tags to be added to the new scope\n        :type tags: list or None\n        :param start_date: (optional) start date of the scope. Will default to 'now' if not provided.\n        :type start_date: datetime.datetime or None\n        :param due_date: (optional) due date of the scope\n        :type due_date: datetime.datetime or None\n        :param team: (optional) team_id or Team object to assign membership of scope to a team.\n        :type team: basestring or :class:`models.Team` or None\n        :param kwargs: optional additional search arguments\n        :type kwargs: dict or None\n        :return: the created :class:`models.Scope`\n        :raises APIError: In case of failure of the creation of new Scope\n        \"\"\"\n        if not isinstance(name, (str, text_type)):\n            raise IllegalArgumentError(\"'Name' should be provided as a string, was provided as '{}'\".\n                                       format(type(name)))\n        if status not in ScopeStatus.values():\n            raise IllegalArgumentError(\"Please provide a valid scope status, please use one of `enums.ScopeStatus`. \"\n                                       \"Got: '{}'\".format(status))\n        if description and not isinstance(description, (str, text_type)):\n            raise IllegalArgumentError(\"'Description' should be provided as a string, was provided as '{}'\".\n                                       format(type(description)))\n        if tags and not isinstance(tags, list):\n            raise IllegalArgumentError(\"'Tags' should be provided as a list, was provided as '{}'\".\n                                       format(type(tags)))\n        if tags and not (all([isinstance(t, (str, text_type)) for t in tags])):\n            raise IllegalArgumentError(\"Each tag in the list of tags should be provided as a string\")\n\n        if not start_date:\n            start_date = datetime.datetime.now()\n        if not tags:\n            tags = list()\n\n        data_dict = {\n            'name': name,\n            'status': status,\n            'text': description,\n            'tags': tags,\n        }\n\n        if start_date is not None:\n            if isinstance(start_date, datetime.datetime):\n                if not start_date.tzinfo:\n                    warnings.warn(\"The duedate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(start_date.isoformat(sep=' ')))\n                data_dict['start_date'] = start_date.isoformat(sep='T')\n            else:\n                raise IllegalArgumentError('Start date should be a datetime.datetime() object')\n        else:\n            # defaults to now\n            data_dict['start_date'] = datetime.datetime.now()\n\n        if due_date is not None:\n            if isinstance(due_date, datetime.datetime):\n                if not due_date.tzinfo:\n                    warnings.warn(\"The duedate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(due_date.isoformat(sep=' ')))\n                data_dict['due_date'] = due_date.isoformat(sep='T')\n            else:\n                raise IllegalArgumentError('Due date should be a datetime.datetime() object')\n\n        if team is not None:\n            if isinstance(team, Team):\n                team_id = team.id\n            elif is_uuid(team):\n                team_id = team\n            elif isinstance(team, (text_type, string_types)):\n                team_id = self.team(name=team).id\n            else:\n                raise IllegalArgumentError(\"'Team' should be provided as a `models.Team` object or UUID or team name, \"\n                                           \"was provided as a {}\".format(type(team)))\n            data_dict['team'] = team_id\n\n        # injecting additional kwargs for those cases that you need to add extra options.\n        data_dict.update(kwargs)\n\n        response = self._request('POST', self._build_url('scopes'), data=data_dict)\n\n        if response.status_code != requests.codes.created:  # pragma: no cover\n            raise APIError(\"Could not create scope, {}:\\n\\n{}'\".format(str(response), response.json()))\n\n        return Scope(response.json()['results'][0], client=self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a scope. This will delete a scope if the client has the right to do so. Sufficient permissions to delete a scope are a superuser, a user in the `GG:Configurators` group or a user that is the Scope manager of the scope to be deleted. :param scope: Scope object to be deleted :type scope: :class: `models.Scope` :return: None :raises APIError: in case of failure in the deletion of the scope", "response": "def delete_scope(self, scope):\n        \"\"\"\n        Delete a scope.\n\n        This will delete a scope if the client has the right to do so. Sufficient permissions to delete a scope are a\n        superuser, a user in the `GG:Configurators` group or a user that is the Scope manager of the scope to be\n        deleted.\n\n        :param scope: Scope object to be deleted\n        :type scope: :class: `models.Scope`\n\n        :return: None\n        :raises APIError: in case of failure in the deletion of the scope\n        \"\"\"\n        assert isinstance(scope, Scope), 'Scope \"{}\" is not a scope!'.format(scope.name)\n\n        response = self._request('DELETE', self._build_url('scope', scope_id=str(scope.id)))\n\n        if response.status_code != requests.codes.no_content:  # pragma: no cover\n            raise APIError(\"Could not delete scope, {}: {}\".format(str(response), response.content))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clone_scope(self, source_scope, name=None, status=None, start_date=None, due_date=None,\n                    description=None, tags=None, team=None, asynchronous=False):\n        \"\"\"\n        Clone a Scope.\n\n        This will clone a scope if the client has the right to do so. Sufficient permissions to clone a scope are a\n        superuser, a user in the `GG:Configurators` group and a user that is Scope manager of the scope to be\n        clone and member of the `GG:Managers` group as well.\n\n        If no additional arguments are provided, the values of the `source_scope` are used for the new scope.\n\n        .. versionadded: 2.6\n\n        :param source_scope: Scope object to be cloned itself\n        :type source_scope: :class:`models.Scope`\n        :param name: (optional) new name of the scope\n        :type name: basestring or None\n        :param status: (optional) statis of the new scope\n        :type status: one of :class:`enums.ScopeStatus`\n        :param tags: (optional) list of new scope tags\n        :type tags: list or None\n        :param start_date: (optional) start date of the to be cloned scope\n        :type start_date: datetime or None\n        :param due_date: (optional) due data of the to be cloned scope\n        :type due_date: datetime or None\n        :param description: (optional) description of the new scope\n        :type description: basestring or None\n        :param team: (optional) team_id or Team object to assign membership of scope to a team.\n        :type team: basestring or :class:`models.Team` or None\n        # :param scope_options: (optional) dictionary with scope options (NO EFFECT)\n        # :type scope_options: dict or None\n        :param asynchronous: (optional) option to use asynchronous cloning of the scope, default to False.\n        :type asynchronous: bool or None\n        :return: New scope that is cloned\n        :rtype: :class:`models.Scope`\n        :raises IllegalArgumentError: When the provided arguments are incorrect\n        :raises APIError: When the server is unable to clone the scope (eg. permissions)\n        \"\"\"\n        if not isinstance(source_scope, Scope):\n            raise IllegalArgumentError('`source_scope` should be a `Scope` object')\n\n        data_dict = {'id': source_scope.id, 'async': asynchronous}\n\n        if name is not None:\n            if not isinstance(name, (string_types, text_type)):\n                raise IllegalArgumentError(\"`name` should be a string\")\n            data_dict['name'] = str(name)\n\n        if start_date is not None:\n            if isinstance(start_date, datetime.datetime):\n                if not start_date.tzinfo:\n                    warnings.warn(\"The duedate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(start_date.isoformat(sep=' ')))\n                data_dict['start_date'] = start_date.isoformat(sep='T')\n            else:\n                raise IllegalArgumentError('Start date should be a datetime.datetime() object')\n\n        if due_date is not None:\n            if isinstance(due_date, datetime.datetime):\n                if not due_date.tzinfo:\n                    warnings.warn(\"The duedate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(due_date.isoformat(sep=' ')))\n                data_dict['due_date'] = due_date.isoformat(sep='T')\n            else:\n                raise IllegalArgumentError('Due date should be a datetime.datetime() object')\n\n        if description is not None:\n            if not isinstance(description, (text_type, string_types)):\n                raise IllegalArgumentError(\"`description` should be a string\")\n            else:\n                data_dict['text'] = description\n\n        if status is not None:\n            if status not in ScopeStatus.values():\n                raise IllegalArgumentError(\"`status` should be one of '{}'\".format(ScopeStatus.values()))\n            else:\n                data_dict['status'] = str(status)\n\n        if tags is not None:\n            if not isinstance(tags, list):\n                raise IllegalArgumentError(\"'Tags' should be provided as a list, was provided as '{}'\".\n                                           format(type(tags)))\n            if not (all([isinstance(t, (str, text_type)) for t in tags])):\n                raise IllegalArgumentError(\"Each tag in the list of tags should be provided as a string\")\n            data_dict['tags'] = tags\n\n        if team is not None:\n            if isinstance(team, Team):\n                team_id = team.id\n            elif is_uuid(team):\n                team_id = team\n            elif isinstance(team, (text_type, string_types)):\n                team_id = self.team(name=team).id\n            else:\n                raise IllegalArgumentError(\"`team` should be a name of an existing team or UUID of a team\")\n            data_dict['team'] = team_id\n\n        url = self._build_url('scopes')\n        query_params = dict(select_action='clone')\n        response = self._request('POST', url,\n                                 params=query_params,\n                                 json=data_dict)\n\n        if response.status_code != requests.codes.created:  # pragma: no cover\n            if response.status_code == requests.codes.forbidden:\n                raise ForbiddenError(\"Could not clone scope, {}: {}\".format(str(response), response.content))\n            else:\n                raise APIError(\"Could not clone scope, {}: {}\".format(str(response), response.content))\n\n        return Scope(response.json()['results'][0], client=source_scope._client)", "response": "Clone a Scope.\n\n        This will clone a scope if the client has the right to do so. Sufficient permissions to clone a scope are a\n        superuser, a user in the `GG:Configurators` group and a user that is Scope manager of the scope to be\n        clone and member of the `GG:Managers` group as well.\n\n        If no additional arguments are provided, the values of the `source_scope` are used for the new scope.\n\n        .. versionadded: 2.6\n\n        :param source_scope: Scope object to be cloned itself\n        :type source_scope: :class:`models.Scope`\n        :param name: (optional) new name of the scope\n        :type name: basestring or None\n        :param status: (optional) statis of the new scope\n        :type status: one of :class:`enums.ScopeStatus`\n        :param tags: (optional) list of new scope tags\n        :type tags: list or None\n        :param start_date: (optional) start date of the to be cloned scope\n        :type start_date: datetime or None\n        :param due_date: (optional) due data of the to be cloned scope\n        :type due_date: datetime or None\n        :param description: (optional) description of the new scope\n        :type description: basestring or None\n        :param team: (optional) team_id or Team object to assign membership of scope to a team.\n        :type team: basestring or :class:`models.Team` or None\n        # :param scope_options: (optional) dictionary with scope options (NO EFFECT)\n        # :type scope_options: dict or None\n        :param asynchronous: (optional) option to use asynchronous cloning of the scope, default to False.\n        :type asynchronous: bool or None\n        :return: New scope that is cloned\n        :rtype: :class:`models.Scope`\n        :raises IllegalArgumentError: When the provided arguments are incorrect\n        :raises APIError: When the server is unable to clone the scope (eg. permissions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_querystring(self):\n        to_remove = self.get_querystring_parameter_to_remove()\n        query_string = urlparse(self.request.get_full_path()).query\n        query_dict = parse_qs(query_string.encode('utf-8'))\n        for arg in to_remove:\n            if arg in query_dict:\n                del query_dict[arg]\n        clean_query_string = urlencode(query_dict, doseq=True)\n        return clean_query_string", "response": "Clean existing query string by removing any arguments that we don t want to preserve"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_sort(self, request):\n        # Look for 'sort' in get request. If not available use default.\n        sort_request = request.GET.get(self.sort_parameter, self.default_sort)\n        if sort_request.startswith('-'):\n            sort_order = '-'\n            sort_field = sort_request.split('-')[1]\n        else:\n            sort_order = ''\n            sort_field = sort_request\n        # Invalid sort requests fail silently\n        if not sort_field in self._allowed_sort_fields:\n            sort_order = self.default_sort_order\n            sort_field = self.default_sort_field\n        return (sort_order, sort_field)", "response": "Set the sort parameter in the get request and return the order and field of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the next sort string for the given field.", "response": "def get_next_sort_string(self, field):\n        \"\"\"\n        If we're already sorted by the field then the sort query\n        returned reverses the sort order.\n        \"\"\"\n        # self.sort_field is the currect sort field\n        if field == self.sort_field:\n            next_sort = self.toggle_sort_order() + field\n        else:\n            default_order_for_field = \\\n                self._allowed_sort_fields[field]['default_direction']\n            next_sort = default_order_for_field + field\n        return self.get_sort_string(next_sort)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sort_indicator(self, field):\n        indicator = ''\n        if field == self.sort_field:\n            indicator = 'sort-asc'\n            if self.sort_order == '-':\n                indicator = 'sort-desc'\n        return indicator", "response": "Returns a sort class for the active sort only."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_basic_sort_link(self, request, field):\n        query_string = self.get_querystring()\n        sort_string = self.get_next_sort_string(field)\n        if sort_string:\n            sort_link = request.path + '?' + sort_string\n            if query_string:\n                sort_link += '&' + query_string\n        else:\n            sort_link = request.path\n            if query_string:\n                sort_link += '?' + query_string\n        return sort_link", "response": "This method builds the sort link for the current resource based on the field and the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_thumb_path(self, image):\n        image_file = image.file\n        image_name_w_ext = split(image.name)[-1]\n        image_name, ext = splitext(image_name_w_ext)\n        if not self.in_memory(image_file):\n            # `image_file` is already in disk (not in memory).\n            # `image_name` is the full path, not just the name\n            image_name = image_name.split('/')[-1]\n        upload_to = image.field.upload_to\n        if not upload_to.endswith('/'):\n            upload_to = f'{upload_to}/'\n        path_upload_to = f'{upload_to}{image_name}'\n        return f'{self.storage.location}/{path_upload_to}{THUMB_EXT}{ext}'", "response": "Build the absolute path of the to - be - saved thumbnail."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a 10x10px thumbnail for the image", "response": "def generate_thumb(self, image):\n        \"\"\"\n        Given a (large) image, generate a 10x10px thumbnail\n        with blur effect (in order to keep the size small)\n        \"\"\"\n        image_file = image.file\n        picture = Image.open(image_file).convert('RGB')\n        picture.thumbnail((10, 10))\n        picture.filter(ImageFilter.GaussianBlur(radius=4))\n        absolute_path = self.build_thumb_path(image)\n        self.save_thumb(picture, absolute_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_to_binary(self):\n        return bytes([\n            COMMAND_CODE,\n            self.channels_to_byte(self.led_on),\n            self.channels_to_byte(self.led_slow_blinking),\n            self.channels_to_byte(self.led_fast_blinking)\n        ])", "response": "Converts the data to binary format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, **options):\n        shutdown_message = options.get('shutdown_message', '')\n\n        self.stdout.write(\"Performing system checks...\\n\\n\")\n        self.check(display_num_errors=True)\n        self.check_migrations()\n        now = datetime.datetime.now().strftime(r'%B %d, %Y - %X')\n        if six.PY2:\n            now = now.decode(get_system_encoding())\n        self.stdout.write(now)\n\n        addr, port = self.addr, self.port\n        addr = '[{}]'.format(addr) if self._raw_ipv6 else addr\n\n        runner = GunicornRunner(addr, port, options)\n        try:\n            runner.run()\n        except KeyboardInterrupt:\n            runner.shutdown()\n            if shutdown_message:\n                self.stdout.write(shutdown_message)\n            sys.exit(0)\n        except:\n            runner.shutdown()\n            raise", "response": "Override this method to bring Gunicorn on."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef value(self):\n        if not self._value:\n            return None\n        if not self._cached_values and isinstance(self._value, (list, tuple)):\n            ids = [v.get('id') for v in self._value]\n            self._cached_values = list(self._client.parts(id__in=','.join(ids), category=None))\n        return self._cached_values", "response": "Return the value of a reference property."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetecting peaks in data based on their amplitude and other features.", "response": "def detect_panthomkins_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n                 kpsh=False, valley=False, show=False, ax=None):\n\n    \"\"\"Detect peaks in data based on their amplitude and other features.\n\n    Parameters\n    ----------\n    x : 1D array_like\n        data.\n    mph : {None, number}, optional (default = None)\n        detect peaks that are greater than minimum peak height.\n    mpd : positive integer, optional (default = 1)\n        detect peaks that are at least separated by minimum peak distance (in\n        number of data).\n    threshold : positive number, optional (default = 0)\n        detect peaks (valleys) that are greater (smaller) than `threshold`\n        in relation to their immediate neighbors.\n    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')\n        for a flat peak, keep only the rising edge ('rising'), only the\n        falling edge ('falling'), both edges ('both'), or don't detect a\n        flat peak (None).\n    kpsh : bool, optional (default = False)\n        keep peaks with same height even if they are closer than `mpd`.\n    valley : bool, optional (default = False)\n        if True (1), detect valleys (local minima) instead of peaks.\n    show : bool, optional (default = False)\n        if True (1), plot data in matplotlib figure.\n    ax : a matplotlib.axes.Axes instance, optional (default = None).\n\n    Returns\n    -------\n    ind : 1D array_like\n        indeces of the peaks in `x`.\n\n    Notes\n    -----\n    The detection of valleys instead of peaks is performed internally by simply\n    negating the data: `ind_valleys = detect_peaks(-x)`\n\n    The function can handle NaN's\n\n    See this IPython Notebook [1]_.\n\n    References\n    ----------\n    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\"\"\"\n\n\n    x = np.atleast_1d(x).astype('float64')\n    if x.size < 3:\n        return np.array([], dtype=int)\n    if valley:\n        x = -x\n    # find indices of all peaks\n    dx = x[1:] - x[:-1]\n    # handle NaN's\n    indnan = np.where(np.isnan(x))[0]\n    if indnan.size:\n        x[indnan] = np.inf\n        dx[np.where(np.isnan(dx))[0]] = np.inf\n    ine, ire, ife = np.array([[], [], []], dtype=int)\n    if not edge:\n        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n    else:\n        if edge.lower() in ['rising', 'both']:\n            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n        if edge.lower() in ['falling', 'both']:\n            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n    ind = np.unique(np.hstack((ine, ire, ife)))\n    # handle NaN's\n    if ind.size and indnan.size:\n        # NaN's and values close to NaN's cannot be peaks\n        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n    # first and last values of x cannot be peaks\n    if ind.size and ind[0] == 0:\n        ind = ind[1:]\n    if ind.size and ind[-1] == x.size-1:\n        ind = ind[:-1]\n    # remove peaks < minimum peak height\n    if ind.size and mph is not None:\n        ind = ind[x[ind] >= mph]\n    # remove peaks - neighbors < threshold\n    if ind.size and threshold > 0:\n        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n        ind = np.delete(ind, np.where(dx < threshold)[0])\n    # detect small peaks closer than minimum peak distance\n    if ind.size and mpd > 1:\n        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n        idel = np.zeros(ind.size, dtype=bool)\n        for i in range(ind.size):\n            if not idel[i]:\n                # keep peaks with the same height if kpsh is True\n                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n                    & (x[ind[i]] > x[ind] if kpsh else True)\n                idel[i] = 0  # Keep current peak\n        # remove the small peaks and sort back the indices by their occurrence\n        ind = np.sort(ind[~idel])\n\n    if show:\n        if indnan.size:\n            x[indnan] = np.nan\n        if valley:\n            x = -x\n        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n\n    return ind"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the results of the detect_peaks function.", "response": "def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):\n    \"\"\"Plot results of the detect_peaks function, see its help.\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print('matplotlib is not available.')\n    else:\n        if ax is None:\n            _, ax = plt.subplots(1, 1, figsize=(8, 4))\n\n        ax.plot(x, 'b', lw=1)\n        if ind.size:\n            label = 'valley' if valley else 'peak'\n            label = label + 's' if ind.size > 1 else label\n            ax.plot(ind, x[ind], '+', mfc=None, mec='r', mew=2, ms=8,\n                    label='%d %s' % (ind.size, label))\n            ax.legend(loc='best', framealpha=.5, numpoints=1)\n        ax.set_xlim(-.02*x.size, x.size*1.02-1)\n        ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()\n        yrange = ymax - ymin if ymax > ymin else 1\n        ax.set_ylim(ymin - 0.1*yrange, ymax + 0.1*yrange)\n        ax.set_xlabel('Data #', fontsize=14)\n        ax.set_ylabel('Amplitude', fontsize=14)\n        mode = 'Valley detection' if valley else 'Peak detection'\n        ax.set_title(\"%s (mph=%s, mpd=%d, threshold=%s, edge='%s')\"\n                     % (mode, str(mph), mpd, str(threshold), edge))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrefresh the object in place.", "response": "def refresh(self):\n        # type: () -> None\n        \"\"\"Refresh the object in place.\"\"\"\n        from pykechain.client import API_EXTRA_PARAMS\n        src = self._client.reload(self, extra_params=API_EXTRA_PARAMS['activity'])\n        self.__dict__.update(src.__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assignees(self):\n        if 'assignees' in self._json_data and self._json_data.get('assignees_ids') == list():\n            return []\n        elif 'assignees' in self._json_data and self._json_data.get('assignees_ids'):\n            assignees_ids_str = ','.join([str(id) for id in self._json_data.get('assignees_ids')])\n            return self._client.users(id__in=assignees_ids_str, is_hidden=False)\n        return None", "response": "List of assignees to the activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining if the Activity is at the root level of a project.", "response": "def is_rootlevel(self):\n        \"\"\"\n        Determine if the Activity is at the root level of a project.\n\n        It will look for the name of the parent which should be either ActivityRootNames.WORKFLOW_ROOT or\n        ActivityRootNames.CATALOG_ROOT. If the name of the parent cannot be found an additional API call is made\n        to retrieve the parent object (based on the `parent_id` in the json_data).\n\n        :return: Return True if it is a root level activity, otherwise return False\n        :rtype: bool\n        \"\"\"\n        # when the activity itself is a root, than return False immediately\n        if self.is_root():\n            return False\n\n        parent_name = None\n        parent_dict = self._json_data.get('parent_id_name')\n\n        if parent_dict and 'name' in parent_dict:\n            parent_name = parent_dict.get('name')\n        if not parent_dict:\n            parent_name = self._client.activity(id=self._json_data.get('parent_id')).name\n        if parent_name in ActivityRootNames.values():\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parent(self):\n        parent_id = self._json_data.get('parent_id')\n        if parent_id is None:\n            raise NotFoundError(\"Cannot find subprocess for this task '{}', \"\n                                \"as this task exist on top level.\".format(self.name))\n        return self._client.activity(pk=parent_id, scope=self.scope_id)", "response": "Retrieve the parent of this activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the direct activities of this subprocess.", "response": "def children(self, **kwargs):\n        \"\"\"Retrieve the direct activities of this subprocess.\n\n        It returns a combination of Tasks (a.o. UserTasks) and Subprocesses on the direct descending level.\n        Only when the activity is a Subprocess, otherwise it raises a NotFoundError\n\n        :param kwargs: Additional search arguments, check :func:`pykechain.Client.activities` for additional info\n        :type kwargs: dict or None\n        :return: a list of :class:`Activity2`\n        :raises NotFoundError: when this task is not of type `ActivityType.PROCESS`\n\n        Example\n        -------\n        >>> parent = project.parent('Subprocess')\n        >>> children = subprocess.children()\n\n        Example searching for children of a subprocess which contains a name (icontains searches case insensitive\n\n        >>> parent = project.parent('Subprocess')\n        >>> children = subprocess.children(name__icontains='more work')\n\n        \"\"\"\n        if self.activity_type != ActivityType.PROCESS:\n            raise NotFoundError(\"Only subprocesses can have children, please choose a subprocess instead of a '{}' \"\n                                \"(activity '{}')\".format(self.activity_type, self.name))\n\n        return self._client.activities(parent_id=self.id, scope=self.scope_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the other activities that also belong to the parent.", "response": "def siblings(self, **kwargs):\n        \"\"\"Retrieve the other activities that also belong to the parent.\n\n        It returns a combination of Tasks (a.o. UserTasks) and Subprocesses on the level of the current task, including\n        itself. This also works if the activity is of type `ActivityType.PROCESS`.\n\n        :param kwargs: Additional search arguments, check :func:`pykechain.Client.activities` for additional info\n        :type kwargs: dict or None\n        :return: list of :class:`Activity2`\n        :raises NotFoundError: when it is a task in the top level of a project\n\n        Example\n        -------\n        >>> task = project.activity('Some Task')\n        >>> siblings = task.siblings()\n\n        Example for siblings containing certain words in the task name\n        >>> task = project.activity('Some Task')\n        >>> siblings = task.siblings(name__contains='Another Task')\n\n        \"\"\"\n        parent_id = self._json_data.get('parent_id')\n        if parent_id is None:\n            raise NotFoundError(\"Cannot find subprocess for this task '{}', \"\n                                \"as this task exist on top level.\".format(self.name))\n        return self._client.activities(parent_id=parent_id, scope=self.scope_id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef edit(self, name=None, description=None, start_date=None, due_date=None, assignees=None, assignees_ids=None,\n             status=None):\n        \"\"\"Edit the details of an activity.\n\n        :param name: (optionally) edit the name of the activity\n        :type name: basestring or None\n        :param description: (optionally) edit the description of the activity\n        :type description: basestring or None\n        :param start_date: (optionally) edit the start date of the activity as a datetime object (UTC time/timezone\n                            aware preferred)\n        :type start_date: datetime or None\n        :param due_date: (optionally) edit the due_date of the activity as a datetime object (UTC time/timzeone\n                            aware preferred)\n        :type due_date: datetime or None\n        :param assignees: (optionally) edit the assignees (usernames) of the activity as a list, will overwrite all\n                          assignees\n        :type assignees: list(basestring) or None\n        :param assignees_ids: (optionally) edit the assignees (user id's) of the activity as a list, will overwrite all\n                             assignees\n        :type assignees_ids: list(basestring) or None\n        :param status: (optionally) edit the status of the activity as a string based\n                       on :class:`~pykechain.enums.ActivityType`\n        :type status: basestring or None\n\n        :raises NotFoundError: if a `username` in the list of assignees is not in the list of scope members\n        :raises IllegalArgumentError: if the type of the inputs is not correct\n        :raises APIError: if another Error occurs\n        :warns: UserWarning - When a naive datetime is provided. Defaults to UTC.\n\n        Example\n        -------\n        >>> from datetime import datetime\n        >>> my_task = project.activity('Specify the wheel diameter')\n        >>> my_task.edit(name='Specify wheel diameter and circumference',\n        ...              description='The diameter and circumference are specified in inches',\n        ...              start_date=datetime.utcnow(),  # naive time is interpreted as UTC time\n        ...              assignee='testuser')\n\n        If we want to provide timezone aware datetime objects we can use the 3rd party convenience library :mod:`pytz`.\n        Mind that we need to fetch the timezone first and use `<timezone>.localize(<your datetime>)` to make it\n        work correctly.\n\n        Using `datetime(2017,6,1,23,59,0 tzinfo=<tz>)` does NOT work for most timezones with a\n        daylight saving time. Check the `pytz <http://pythonhosted.org/pytz/#localized-times-and-date-arithmetic>`_\n        documentation.\n\n        To make it work using :mod:`pytz` and timezone aware :mod:`datetime` see the following example::\n\n        >>> import pytz\n        >>> start_date_tzaware = datetime.now(pytz.utc)\n        >>> mytimezone = pytz.timezone('Europe/Amsterdam')\n        >>> due_date_tzaware = mytimezone.localize(datetime(2019, 10, 27, 23, 59, 0))\n        >>> my_task.edit(due_date=due_date_tzaware, start_date=start_date_tzaware)\n\n        \"\"\"\n        update_dict = {'id': self.id}\n        if name:\n            if isinstance(name, (str, text_type)):\n                update_dict.update({'name': name})\n                self.name = name\n            else:\n                raise IllegalArgumentError('Name should be a string')\n\n        if description:\n            if isinstance(description, (str, text_type)):\n                update_dict.update({'description': description})\n                self.description = description\n            else:\n                raise IllegalArgumentError('Description should be a string')\n\n        if start_date:\n            if isinstance(start_date, datetime.datetime):\n                if not start_date.tzinfo:\n                    warnings.warn(\"The startdate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(start_date.isoformat(sep=' ')))\n                update_dict.update({'start_date': start_date.isoformat(sep='T')})\n            else:\n                raise IllegalArgumentError('Start date should be a datetime.datetime() object')\n\n        if due_date:\n            if isinstance(due_date, datetime.datetime):\n                if not due_date.tzinfo:\n                    warnings.warn(\"The duedate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(due_date.isoformat(sep=' ')))\n                update_dict.update({'due_date': due_date.isoformat(sep='T')})\n            else:\n                raise IllegalArgumentError('Due date should be a datetime.datetime() object')\n\n        if isinstance(assignees_ids, (list, tuple)) or isinstance(assignees, (list, tuple)):\n            update_assignees_ids = []\n            if isinstance(assignees_ids, (list, tuple)):\n                users = self._client.users()\n                update_assignees_ids = [u.id for u in users if u.id in assignees_ids]\n                if len(update_assignees_ids) != len(assignees_ids):\n                    raise NotFoundError(\"All assignees should be a member of the project\")\n            elif isinstance(assignees, (list, tuple)):\n                users = self._client.users()\n                update_assignees_ids = [u.id for u in users if u.username in assignees]\n                if len(update_assignees_ids) != len(assignees):\n                    raise NotFoundError(\"All assignees should be a member of the project\")\n            else:\n                raise IllegalArgumentError(\"Provide the usernames either as list of usernames of user id's\")\n\n            if isinstance(update_assignees_ids, list):\n                project = self._client.scope(pk=self.scope_id, status=None)\n                member_ids_list = [member['id'] for member in project._json_data['members']]\n                for assignee_id in update_assignees_ids:\n                    if assignee_id not in member_ids_list:\n                        raise NotFoundError(\"Assignee '{}' should be a member of the project\".format(assignee_id))\n                update_dict.update({'assignees_ids': update_assignees_ids})\n        elif assignees_ids or assignees:\n            raise IllegalArgumentError(\"If assignees_ids or assignees are provided, they should be a list or tuple\")\n\n        if status:\n            if isinstance(status, (str, text_type)) and status in ActivityStatus.values():\n                update_dict.update({'status': status})\n            else:\n                raise IllegalArgumentError('Status should be a string and in the list of acceptable '\n                                           'status strings: {}'.format(ActivityStatus.values()))\n\n        url = self._client._build_url('activity', activity_id=self.id)\n\n        r = self._client._request('PUT', url, json=update_dict)\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not update Activity ({})\".format(r))\n\n        self.refresh()", "response": "Edit the details of an activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring the activity input and output.", "response": "def configure(self, inputs, outputs):\n        \"\"\"Configure activity input and output.\n\n        You need to provide a list of input and output :class:`Property`. Does not work with lists of propery id's.\n\n        :param inputs: iterable of input property models\n        :type inputs: list(:class:`Property`)\n        :param outputs: iterable of output property models\n        :type outputs: list(:class:`Property`)\n        :raises APIError: when unable to configure the activity\n        \"\"\"\n        def _get_propertyset(proplist):\n            \"\"\"Make it into a unique list of properties to configure for either inputs or outputs.\"\"\"\n            from pykechain.models import Property\n            propertyset = []\n            for property in proplist:\n                if isinstance(property, Property):\n                    propertyset.append(property.id)\n                elif is_uuid(property):\n                    propertyset.append(property)\n            return list(set(propertyset))\n\n        url = self._client._build_url('activity', activity_id='{}/update_associations'.format(self.id))\n\n        if not all([p._json_data.get('category') == Category.MODEL for p in inputs]) and \\\n                not all([p._json_data.get('category') == Category.MODEL for p in outputs]):\n            raise IllegalArgumentError('All Properties need to be of category MODEL to configure a task')\n\n        r = self._client._request('PUT', url, json={\n            'inputs': _get_propertyset(inputs),\n            'outputs': _get_propertyset(outputs)\n        })\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not configure activity\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading the activity as a PDF.", "response": "def download_as_pdf(self, target_dir=None, pdf_filename=None, paper_size=PaperSize.A4,\n                        paper_orientation=PaperOrientation.PORTRAIT, include_appendices=False):\n        \"\"\"\n        Retrieve the PDF of the Activity.\n\n        .. versionadded:: 2.1\n\n        :param target_dir: (optional) directory path name where the store the log.txt to.\n        :type target_dir: basestring or None\n        :param pdf_filename: (optional) log filename to write the log to, defaults to `log.txt`.\n        :type pdf_filename: basestring or None\n        :param paper_size: The size of the paper to which the PDF is downloaded:\n                               - a4paper (default): A4 paper size\n                               - a3paper: A3 paper size\n                               - a2paper: A2 paper size\n                               - a1paper: A1 paper size\n                               - a0paper: A0 paper size\n        :type paper_size: basestring (see :class:`enums.PaperSize`)\n        :param paper_orientation: The orientation of the paper to which the PDF is downloaded:\n                               - portrait (default): portrait orientation\n                               - landscape: landscape orientation\n        :type paper_size: basestring (see :class:`enums.PaperOrientation`)\n        :param include_appendices: True if the PDF should contain appendices, False (default) if otherwise.\n        :type include_appendices: bool\n        :raises APIError: if the pdf file could not be found.\n        :raises OSError: if the file could not be written.\n        \"\"\"\n        if not pdf_filename:\n            pdf_filename = self.name + '.pdf'\n        if not pdf_filename.endswith('.pdf'):\n            pdf_filename += '.pdf'\n\n        full_path = os.path.join(target_dir or os.getcwd(), pdf_filename)\n\n        request_params = {\n            'papersize': paper_size,\n            'orientation': paper_orientation,\n            'appendices': include_appendices\n        }\n\n        url = self._client._build_url('activity_export', activity_id=self.id)\n        response = self._client._request('GET', url, params=request_params)\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not download PDF of activity {}\".format(self.name))\n\n        # If appendices are included, the request becomes asynchronous\n\n        if include_appendices:\n            data = response.json()\n\n            # Download the pdf async\n            url = urljoin(self._client.api_root, data['download_url'])\n\n            count = 0\n\n            while count <= ASYNC_TIMEOUT_LIMIT:\n                response = self._client._request('GET', url=url)\n\n                if response.status_code == requests.codes.ok:  # pragma: no cover\n                    with open(full_path, 'wb') as f:\n                        for chunk in response.iter_content(1024):\n                            f.write(chunk)\n                    return\n\n                count += ASYNC_REFRESH_INTERVAL\n                time.sleep(ASYNC_REFRESH_INTERVAL)\n\n            raise APIError(\"Could not download PDF of activity {} within the time-out limit of {} \"\n                           \"seconds\".format(self.name, ASYNC_TIMEOUT_LIMIT))\n\n        with open(full_path, 'wb') as f:\n            for chunk in response.iter_content(1024):\n                f.write(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate(self, priority, address, rtr, data):\n        assert isinstance(data, bytes)\n        self.needs_high_priority(priority)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 4)\n        self.set_attributes(priority, address, rtr)\n        # 00000011 = channel 1\n        # 00001100 = channel 2\n        # so shift 1 bit to the right + and with 03\n        tmp = (data[0] >> 1) & 0x03\n        print(tmp)\n        self.channel = self.byte_to_channel(tmp)\n        self.needs_valid_channel(self.channel, 2)\n        (self.delay_time,) = struct.unpack('>L', bytes([0]) + data[1:])", "response": "Populates the internal state of the object with the data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the data to binary format.", "response": "def data_to_binary(self):\n        \"\"\"\n        :return: bytes\n        \"\"\"\n        if self.channel == 0x01:\n            tmp = 0x03\n        else:\n            tmp = 0x0C\n\n        return bytes([\n            COMMAND_CODE,\n            tmp\n        ]) + struct.pack('>L', self.delay_time)[-3:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the name of the module that this instance is in.", "response": "def module_name(self):\n        \"\"\"\n        :return: str\n        \"\"\"\n        if self.module_type in velbus.MODULE_DIRECTORY.keys():\n            return velbus.MODULE_DIRECTORY[self.module_type]\n        return \"Unknown\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef populate(self, priority, address, rtr, data):\n        assert isinstance(data, bytes)\n        self.needs_low_priority(priority)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 4)\n        self.set_attributes(priority, address, rtr)\n        self.module_type = data[0]\n        (self.serial,) = struct.unpack(\n            '>L', bytes([0, 0, data[1], data[2]]))\n        self.memory_map_version = data[3]\n        if len(data) > 4:\n            self.build_year = data[4]\n            self.build_week = data[5]", "response": "Populates the internal state of the object with the data from the specified memory map."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the ISO - 1452 status to binary format.", "response": "def data_to_binary(self):\n        \"\"\"\n        :return: bytes\n        \"\"\"\n        return bytes([\n            COMMAND_CODE,\n            self.module_type,\n            self.channels_to_byte(self.led_on),\n            self.channels_to_byte(self.led_slow_blinking),\n            self.channels_to_byte(self.led_fast_blinking),\n            self.build_year,\n            self.build_week\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        super().run()\n        call('rm -vrf ./build ./dist ./*.egg-info', shell=True)\n        call('find . -name __pycache__ -type d | xargs rm -rf', shell=True)\n        call('test -d docs && make -C docs/ clean', shell=True)", "response": "Clean build dist pyc and egg from package and docs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the data_files directory in the / etc directory.", "response": "def _create_data_files_directory(symlink=False):\n        \"\"\"Install data_files in the /etc directory.\"\"\"\n        current_directory = os.path.abspath(os.path.dirname(__file__))\n\n        etc_kytos = os.path.join(BASE_ENV, ETC_KYTOS)\n\n        if not os.path.exists(etc_kytos):\n            os.makedirs(etc_kytos)\n\n        src = os.path.join(current_directory, KYTOS_SKEL_PATH)\n        dst = os.path.join(BASE_ENV, KYTOS_SKEL_PATH)\n\n        if os.path.exists(dst):\n            if not os.listdir(dst):\n                # Path already exists but it's empty, so we'll populate it\n                # We remove it first to avoid an exception from copytree\n                os.rmdir(dst)\n                shutil.copytree(src, dst)\n        else:\n            # It doesn't exist yet, so we should symlink or copy contents\n            if symlink:\n                os.symlink(src, dst)\n            else:\n                shutil.copytree(src, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef allow(self):\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            self.find_primary_button().click()", "response": "Allow the add - on to be installed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addon_name(self):\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            el = self.find_description()\n            return el.find_element(By.CSS_SELECTOR, \"b\").text", "response": "Provide access to the add - on name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncancel add - on install.", "response": "def cancel(self):\n        \"\"\"Cancel add-on install.\"\"\"\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            self.find_secondary_button().click()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install(self):\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            self.find_primary_button().click()", "response": "Confirm add - on install."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(file, channels=None, devices=None, get_header=False, remote=False, out_dict=False, signal_sample=False, **kwargs):\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Downloading of file %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    if remote is True:\n        # if not os.path.exists(\"tempOST\"):\n        #     os.makedirs(\"tempOST\")\n\n        # Check if it is a Google Drive link.\n        if \"drive.google\" in file:\n            response = requests.get(file)\n            content_type = response.headers['content-type']\n            extension = mimetypes.guess_extension(content_type)\n        else:\n            extension = \".\" + file.split(\".\")[-1]\n\n        if None not in [TEMP_PATH,\n                        datetime.datetime.now().strftime(\"%Y\" + \"_\" + \"%m\" + \"_\" + \"%d\" +\n                                                                 \"_\" + \"%H_%M_%S\"),\n                        extension]:\n            remote_file_path = (TEMP_PATH + \"file_\" + datetime.datetime.now().strftime(\"%Y\" + \"_\" + \"%m\" + \"_\" + \"%d\" + \"_\" + \"%H_%M_%S\") + extension).replace(\"\\\\\", \"/\")\n            file = wget.download(file, remote_file_path)\n        else:\n            file = wget.download(file)\n\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Verification of file type %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    file_type = _file_type(file)\n\n    # %%%%%%%%%%%%%%% Verification if shape of channels and devices is the same %%%%%%%%%%%%%%%%%%%\n    _check_shape_and_type(devices, channels)\n\n    # =============================================================================================\n    # ================== Load data accordingly to file type (Read of Header) ======================\n    # =============================================================================================\n\n    header = read_header(file)\n\n    # =============================================================================================\n    # ========= Verification if the function inputs (\"devices\" and \"channels\") are valid ==========\n    # =============================================================================================\n\n    dev_list = list(header.keys())  # Device list.\n    if devices is None:\n        # When None is defined as the value of the devices input then this means that all\n        # devices are relevant.\n        devices = dev_list\n        if channels is not None:\n            channels = [channels]\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Devices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    dev_list_standard = _check_dev_type(devices, dev_list)\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Channels %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    chn_dict = _available_channels(devices, header)\n    chn_list_standard = _check_chn_type(channels, chn_dict)\n\n    # =============================================================================================\n    # =========================== Read of Data from acquisition file ==============================\n    # =============================================================================================\n    data = None\n    if file_type in [\"txt\", \"plain\", \"bat\"]:\n        data = _load_txt(file, dev_list_standard, chn_list_standard, header, **kwargs)\n        if remote is True:\n            if extension == None:\n                extension = \".txt\"\n                remote_file_path = \"download_file_name\" + extension\n    elif file_type in [\"h5\", \"x-hdf\", \"a\"]:\n        data = _load_h5(file, dev_list_standard, chn_list_standard)\n        if remote is True:\n            if extension == None:\n                extension = \".h5\"\n                remote_file_path = \"download_file_name\" + extension\n    elif file_type in [\"edf\", \"octet-stream\"]:\n        raise RuntimeWarning(\"In the present package version loading data from .edf files is not \"\n                             \"available yet.\")\n\n    # =============================================================================================\n    # ======================= Clone downloaded file to signal library =============================\n    # =============================================================================================\n    project_dir = \"../../signal_samples\"\n    if remote is True and os.path.isdir(project_dir):\n        devices = list(header.keys())\n\n        # Check the number of devices.\n        nbr_devices = len(devices)\n        if nbr_devices > 1:\n            devices_label = \"multi_hub\"\n        else:\n            devices_label = \"single_hub\"\n\n        # Get the list of used sensors.\n        sensor_list = []\n        channels_dev_str = \"\"\n        resolutions_str = \"\"\n        comments_str = \"\"\n        for mac_i, mac in enumerate(devices):\n            if len(devices) > 1 and mac_i != len(devices) - 1:\n                comment_sep = \"\\n\"\n                other_sep = \"\\t\"\n            else:\n                comment_sep = \"\"\n                other_sep = \"\"\n            sensor_list.append(header[mac][\"sensor\"])\n            channels_dev_str += \"[\" + mac + \"] \" + str(len(sensor_list[-1])) + other_sep\n            resolutions_str += \"[\" + mac + \"] \" + str(header[mac][\"resolution\"][0]) + \" bits\" + other_sep\n            comments_str += \"[\" + mac + \"] \" + str(header[mac][\"comments\"]) + comment_sep\n\n        sensor_list = list(set(numpy.concatenate(sensor_list)))\n        # Check if date and sensor_list is in a bytes format.\n        date = header[mac][\"date\"]\n        if type(date) is bytes:\n            date = date.decode(\"ascii\")\n\n        if type(sensor_list[0]) in [bytes, numpy.bytes_]:\n            sensor_list = [item.decode('ascii') for item in sensor_list]\n\n        date = date.replace(\"-\", \"_\")\n        file_extension = remote_file_path.split(\".\")[-1]\n\n        shutil.copy(remote_file_path, project_dir + \"/\" + \"signal_sample_\" +\n                    devices_label + \"_\" + \"_\".join(sensor_list) + \"_\" + date + \".\" + file_extension)\n\n        # Generation of a json file with relevant metadata.\n        aux_chn = list(data[mac].keys())[0]\n        json_dict = {\"Signal Type\": \" | \".join(sensor_list),\n                     \"Acquisition Time\": time.strftime(\"%H:%M:%S.0\", time.gmtime(len(data[mac][aux_chn]) / int(header[mac][\"sampling rate\"]))),\n                     \"Sample Rate\": str(header[devices[0]][\"sampling rate\"]) + \" Hz\",\n                     \"Number of Hubs\": str(len(devices)),\n                     \"Number of Channels\": channels_dev_str,\n                     \"Resolutions\": resolutions_str,\n                     \"Observations\": comments_str}\n        with open(project_dir + \"/\" + \"signal_sample_\" + devices_label + \"_\" +\n                  \"_\".join(sensor_list) + \"_\" + date + \"_info.json\", 'w') as outfile:\n            json.dump(json_dict, outfile)\n\n    # =============================================================================================\n    # ===================================== Outputs ===============================================\n    # =============================================================================================\n\n    # Simplify the output data format if we are only working with one device.\n    nbr_devices = len(list(header.keys()))\n    if nbr_devices == 1 and out_dict is False:\n        mac_out = list(data.keys())[0]\n        data = data[mac_out]\n        header = header[mac_out]\n\n    # Return data.\n    if get_header is True:\n        out = data, header\n    else:\n        out = data\n\n    # [Internal code for overwrite file, after downloading, if already exists]\n    if os.path.exists(file):\n        if remote is False and signal_sample is True:\n            shutil.copy(file, \"download_file_name\" + \".\" + file.split(\".\")[-1])\n        elif remote is True:\n            shutil.move(file, \"download_file_name\" + extension)\n\n    return out", "response": "This function loads the data from a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the header of a file and returns a dictionary containing the header data.", "response": "def read_header(file):\n    \"\"\"\n    -----\n    Brief\n    -----\n    Universal function for reading the header of .txt, .h5 and .edf files generated by OpenSignals.\n\n    -----------\n    Description\n    -----------\n    Each file generated by the OpenSignals software (available at https://www.biosignalsplux.com/en/software) owns a set\n    of metadata that allows the proper identification and characterization of each acquisition, by the identification of\n    the mac address of the devices, date of acquisition, duration, number of samples, type of the devices and firmware\n    version.\n\n    This function allows to easily access all of this information using only one line of code and outputs a dictionary\n    to easily identify each field of the header of file.\n\n    ----------\n    Parameters\n    ----------\n    file : file path\n        File path.\n\n    Returns\n    -------\n    out : dict\n        Header data read from the input file as dictionary with keys:\n            [mac address]: The key is a string with the mac address of the device;\n            sensor: Sensor(s) used in the acquisition;\n            device name: String with the mac address identifying the device used in the acquisition process;\n            sync interval: Time interval (in seconds) at which a digital signal is sent by a\n                \u201cpacemaker\u201d thread to a single device (used when the sync mode in on OpenSignals for synchronized data\n                acquisition using multiple devices);\n            time: Time of the acquisition;\n            comments: Comments inserted in the OpenSignals software after the acquisition;\n            device connection: Used connection to the device while using it;\n            channels: Used channels;\n            keywords: Keywords inserted in the OpenSignals software after the acquisition;\n            digital IO: Digital channels available in each device (0 is the Input and 1 is the Output);\n            firmware version: Firmware version of the device;\n            device: Type of device used during the acquisition;\n            sampling rate: Sampling rate set prior to the acquisition;\n            resolution: Resolution set prior to the acquisition;\n            date: Date of the acquisition;\n            column labels: Labels of each set of data (e.g. channel 1).\n\n    \"\"\"\n\n    # =============================================================================================\n    # ============================== Identification of File Type ==================================\n    # =============================================================================================\n\n    file_type = _file_type(file)\n\n    # =============================================================================================\n    # ========================= Read Header accordingly to file type ==============================\n    # =============================================================================================\n\n    if file_type in [\"txt\", \"plain\", \"bat\"]:\n        file_temp = open(file, \"r\")\n        header = file_temp.readlines()[1]\n        file_temp.close()\n\n        # -------------------------- Conversion to dictionary. ------------------------------------\n        header = ast.literal_eval(header.split(\"# \")[1].split(\"\\n\")[0])\n\n        # -------------------------- Standardization of Header ------------------------------------\n        macs = header.keys()\n        col_nbr = 0\n        for mac in macs:\n            # ------------ Removal of \"special\", \"sensor\", \"mode\" and \"position\" keys -------------\n            del header[mac][\"special\"]\n            #del header[mac][\"sensor\"]\n            del header[mac][\"position\"]\n            del header[mac][\"mode\"]\n\n            # ---------------- Combination of the information in \"label\" and \"column\" -------------\n            column_labels = {}\n            for chn_nbr, chn in enumerate(header[mac][\"channels\"]):\n                chn_label = header[mac][\"label\"][chn_nbr]\n                column_labels[chn] = col_nbr + numpy.where(numpy.array(header[mac][\"column\"]) ==\n                                                           chn_label)[0][0]\n            header[mac][\"column labels\"] = column_labels\n\n            col_nbr += len(header[mac][\"column\"])\n            del header[mac][\"column\"]\n            del header[mac][\"label\"]\n\n    elif file_type in [\"h5\", \"x-hdf\", \"a\"]:\n        file_temp = h5py.File(file)\n        macs = file_temp.keys()\n\n        header = {}\n        for mac in macs:\n            header[mac] = dict(file_temp.get(mac).attrs.items())\n            header[mac][\"sensor\"] = []\n            # --------- Removal of \"duration\", \"keywords\", \"mode\", \"nsamples\" ... keys ------------\n            for key in [\"duration\", \"mode\", \"keywords\", \"nsamples\", \"forcePlatform values\",\n                        \"macaddress\"]:\n                if key in header[mac].keys():\n                    del header[mac][key]\n                    # del header[mac][\"duration\"]\n                    # del header[mac][\"mode\"]\n                    # del header[mac][\"keywords\"]\n                    # del header[mac][\"nsamples\"]\n                    # del header[mac][\"forcePlatform values\"]\n                    # del header[mac][\"macaddress\"]\n\n            # -------------- Inclusion of a field used in .txt files (Convergence) ----------------\n            column_labels = {}\n            for chn in header[mac][\"channels\"]:\n                chn_label = \"channel_\" + str(chn)\n                column_labels[chn] = chn_label\n                header[mac][\"sensor\"].append(dict(file_temp.get(mac).get(\"raw\").get(\"channel_\" + str(chn)).attrs.items())[\"sensor\"])\n            header[mac][\"column labels\"] = column_labels\n\n        file_temp.close()\n\n    # elif file_type in [\"edf\", \"octet-stream\"]:\n    #\n    #     # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    #     # %%%%%%%%%%%% Code taken from convertEDF function of OpenSignals fileHandler %%%%%%%%%%%%%%\n    #     # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    #\n    #     file_temp = pyedflib.EdfReader(file)\n    #     # nbrSamples = file_temp.getNSamples()[0]\n    #     nbr_signals = file_temp.signals_in_file\n    #     file_header = file_temp.getHeader()\n    #     start_date = file_header[\"startdate\"]\n    #     file_header[\"equipment\"] += \"']\"\n    #     equipment = ast.literal_eval(file_header['equipment'])\n    #     equipment = [n.replace(\" \", \"_\") for n in equipment]\n    #     headers = file_temp.getSignalHeaders()\n    #     header = {}\n    #     mac_address_list = []\n    #\n    #     # ---------------------------------- Mac Address List --------------------------------------\n    #     for signal_nbr in numpy.arange(nbr_signals):\n    #         config = headers[signal_nbr]\n    #         mac_address = config[\"transducer\"].split(\",\")[0]\n    #\n    #         if mac_address not in header.keys():\n    #             mac_address_list.append(mac_address)\n    #             header[mac_address] = {}\n    #             header[mac_address][\"device name\"] = mac_address\n    #             header[mac_address][\"sync interval\"] = 2\n    #             header[mac_address][\"time\"] = start_date.strftime('%H:%M:%S.%f')[:-3]\n    #             header[mac_address][\"comments\"] = \"\"\n    #             header[mac_address][\"device connection\"] = \"\"\n    #             header[mac_address][\"channels\"] = []\n    #             header[mac_address][\"date\"] = start_date.strftime('%Y-%m-%d')\n    #             header[mac_address][\"digital IO\"] = []\n    #\n    #             if \",\" in config['transducer']:\n    #                 header[mac_address][\"firmware version\"] = int(config['transducer'].\n    #                                                               split(\",\")[1])\n    #             else:\n    #                 header[mac_address][\"firmware version\"] = \"\"\n    #\n    #             header[mac_address][\"device\"] = equipment[len(mac_address_list) - 1]\n    #             header[mac_address][\"sampling rate\"] = int(config['sample_rate'])\n    #             header[mac_address][\"resolution\"] = []\n    #             header[mac_address][\"column labels\"] = {}\n    #         if \",\" in config['prefilter']:\n    #             header[mac_address][\"channels\"].append(int(config['prefilter'].split(\",\")[0]))\n    #             header[mac_address][\"resolution\"].append(int(config['prefilter'].split(\",\")[1]))\n\n    else:\n        raise RuntimeError(\"The type of the input file does not correspond to the predefined \"\n                           \"formats of OpenSignals\")\n\n    return header"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction for cleaning the temporary folder inside the package.", "response": "def clean_temp():\n    \"\"\"\n    -----\n    Brief\n    -----\n    Function for cleaning the temporary folder inside the package.\n\n    -----------\n    Description\n    -----------\n    Data processing may generate temporary files that occupy the storage of the computer.\n\n    This function allows the user to clean the temporary files when necessary.\n\n    source:\n    https://stackoverflow.com/questions/185936/how-to-delete-the-contents-of-a-folder-in-python\n    \"\"\"\n\n    folder = 'tempOST'\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n        except Exception as exception:\n            print(exception)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions used for reading. txt files generated by OpenSignals.", "response": "def _load_txt(file, devices, channels, header, **kwargs):\n    \"\"\"\n    Function used for reading .txt files generated by OpenSignals.\n\n    ----------\n    Parameters\n    ----------\n    file : file, str, or pathlib.Path\n        File, filename, or generator to read.  If the filename extension is\n        ``.gz`` or ``.bz2``, the file is first decompressed. Note that\n        generators should return byte strings for Python 3k.\n\n    devices : list [\"mac_address_1\" <str>, \"mac_address_2\" <str>...]\n        List of devices selected by the user.\n\n    channels : list [[mac_address_1_channel_1 <int>, mac_address_1_channel_2 <int>...],\n                    [mac_address_2_channel_1 <int>...]...]\n        From which channels will the data be loaded.\n\n    header : dict\n        File header with relevant metadata for identifying which columns may be read.\n\n    **kwargs : list of variable keyword arguments. The valid keywords are those used by\n               numpy.loadtxt function.\n\n    Returns\n    -------\n    out_dict : dict\n        Data read from the text file.\n    \"\"\"\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%% Exclusion of invalid keywords %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    kwargs_txt = _filter_keywords(numpy.loadtxt, kwargs)\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%% Columns of the selected channels %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    out_dict = {}\n    for dev_nbr, device in enumerate(devices):\n        out_dict[device] = {}\n        columns = []\n        for chn in channels[dev_nbr]:\n            columns.append(header[device][\"column labels\"][chn])\n            # header[device][\"column labels\"] contains the column of .txt file where the data of\n            # channel \"chn\" is located.\n            out_dict[device][\"CH\" + str(chn)] = numpy.loadtxt(fname=file, usecols=header[device][\"column labels\"][chn],\n                                                              **kwargs_txt)\n\n    return out_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions used for reading the H5 file.", "response": "def _load_h5(file, devices, channels):\n    \"\"\"\n    Function used for reading .h5 files generated by OpenSignals.\n\n    ----------\n    Parameters\n    ----------\n    file : file path.\n        File Path.\n\n    devices : list [\"mac_address_1\" <str>, \"mac_address_2\" <str>...]\n        List of devices selected by the user.\n\n    channels : list [[mac_address_1_channel_1 <int>, mac_address_1_channel_2 <int>...],\n                    [mac_address_2_channel_1 <int>...]...]\n        From which channels will the data be loaded.\n\n    Returns\n    -------\n    out_dict : dict\n        Data read from the h5 file.\n    \"\"\"\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%%% Creation of h5py object %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    h5_object = h5py.File(file)\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%% Data of the selected channels %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    out_dict = {}\n    for dev_nbr, device in enumerate(devices):\n        out_dict[device] = {}\n        for chn in channels[dev_nbr]:\n            data_temp = list(h5_object.get(device).get(\"raw\").get(\"channel_\" + str(chn)))\n\n            # Conversion of a nested list to a flatten list by list-comprehension\n            # The following line is equivalent to:\n            # for sublist in h5_data:\n            #    for item in sublist:\n            #        flat_list.append(item)\n            #out_dict[device][\"CH\" + str(chn)] = [item for sublist in data_temp for item in sublist]\n            out_dict[device][\"CH\" + str(chn)] = numpy.concatenate(data_temp)\n\n    return out_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the shape of devices and channels fields of load function are consistent.", "response": "def _check_shape_and_type(devices, channels):\n    \"\"\"\n    Function used for checking if the shape of \"devices\" and \"channels\" fields of load function\n    have the same shape (the number of elements in the list specified in \"devices\" is equal to\n    the number of sublists inside \"channels\" field).\n    Both \"devices\" and \"channels\" must be lists.\n\n    ----------\n    Parameters\n    ----------\n    devices : list [\"mac_address_1\" <str>, \"mac_address_2\" <str>...]\n        List of devices selected by the user.\n\n    channels : list [[mac_address_1_channel_1 <int>, mac_address_1_channel_2 <int>...],\n                    [mac_address_2_channel_1 <int>...]...]\n        From which channels will the data be loaded.\n\n    Returns\n    -------\n    An error message when shape or type of the inputs is not valid.\n    \"\"\"\n\n    if isinstance(devices, type(channels)):  # Comparision of type.\n        dev_chn_type = type(devices)\n        if devices is None:\n            pass\n        elif dev_chn_type == list:\n            # Comparision of the shape.\n            if len(devices) == sum(isinstance(i, list) for i in channels):\n                # ----------- Verification if all mac addresses are in a string format ------------\n                for dev_nbr, device in enumerate(devices):\n                    # List element is a string and is one of the available devices.\n                    if isinstance(device, str):\n                        # ----------- Verification if all specified channels are integers ----------\n                        # Each sublist must be composed by integers.\n                        for channel in channels[dev_nbr]:\n                            if isinstance(channel, int):\n                                continue\n                            else:\n                                raise RuntimeError(\"At least one of the 'channels' elements is not \"\n                                                   \"an integer\")\n                    else:\n                        raise RuntimeError(\"At least one of the 'devices' elements is not a mac \"\n                                           \"address string\")\n            else:\n                raise RuntimeError(\"The shape of devices and channels lists are not the same. The \"\n                                   \"number of sublists in the 'channels' input may be equal to the \"\n                                   \"number of devices specified in 'devices' field.\")\n        else:\n            raise RuntimeError(\"The chosen data type of 'devices' and 'channels' fields is not \"\n                               \"supported.\")\n\n    elif devices is None and _is_instance(int, channels, condition=\"all\"):\n        pass\n\n    else:\n        raise RuntimeError(\"The input 'devices' and 'channels' must be of the same type \"\n                           \"(None or list). When only one device is being used is also possible to \"\n                           \"specify None as 'device' input and a list of integers in the 'channel' \"\n                           \"field\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_chn_type(channels, available_channels):\n\n    # ------------------------ Definition of constants and variables -------------------------------\n    chn_list_standardized = []\n\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%% Fill of \"chn_list_standardized\" %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    devices = list(available_channels.keys())\n    for dev_nbr, device in enumerate(devices):\n        if channels is not None:\n            sub_unit = channels[dev_nbr]\n            for channel in sub_unit:  # Each sublist must be composed by integers.\n                if channel in available_channels[devices[dev_nbr]]:\n                    continue\n                else:\n                    raise RuntimeError(\"At least one of the specified channels is not available in \"\n                                       \"the acquisition file.\")\n            chn_list_standardized.append(sub_unit)\n\n        else:  # By omission all the channels were selected.\n            chn_list_standardized.append(available_channels[device])\n\n    return chn_list_standardized", "response": "Function used for checking weather the elements in channels input are coincident with the available channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions used for determination of the available channels in each device.", "response": "def _available_channels(devices, header):\n    \"\"\"\n    Function used for the determination of the available channels in each device.\n\n    ----------\n    Parameters\n    ----------\n    devices : list [\"mac_address_1\" <str>, \"mac_address_2\" <str>...]\n        List of devices selected by the user.\n\n    header: dict\n        Dictionary that contains auxiliary data of the acquisition.\n\n    Returns\n    -------\n    out : dict\n        Returns a dictionary where each device defines a key and the respective value will be a list\n        of the available channels for the device.\n\n    \"\"\"\n\n    # ------------------------ Definition of constants and variables ------------------------------\n    chn_dict = {}\n\n    # %%%%%%%%%%%%%%%%%%%%%% Access to the relevant data in the header %%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    for dev in devices:\n        chn_dict[dev] = header[dev][\"column labels\"].keys()\n\n    return chn_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_dev_type(devices, dev_list):\n\n    if devices is not None:\n        for device in devices:\n            if device in dev_list:  # List element is one of the available devices.\n                continue\n            else:\n                raise RuntimeError(\"At least one of the specified devices is not available in the \"\n                                   \"acquisition file.\")\n        out = devices\n\n    else:\n        out = dev_list\n\n    return out", "response": "Function used for checking weather the devices field only contains devices used during the acquisition file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning intended for identification of the file type.", "response": "def _file_type(file):\n    \"\"\"\n    Function intended for identification of the file type.\n\n    ----------\n    Parameters\n    ----------\n    file : file path\n        File path.\n\n    Returns\n    -------\n    out : str\n        Identified file type.\n\n    \"\"\"\n    # %%%%%%%%%%%%%%%%%%%%%%%%%%%%% Verification of file type %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    if \".\" in file:  # File with known extension.\n        file_type = file.split(\".\")[-1]\n    else:  # File without known extension.\n        file_type = magic.from_file(file, mime=True).split(\"/\")[-1]\n\n    return file_type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a list of elements of a specific type is an instance of a specific data type.", "response": "def _is_instance(type_to_check, element, condition=\"any\", deep=False):\n    \"\"\"\n    -----\n    Brief\n    -----\n    Function that verifies when \"all\" or \"any\" elements of the list \"element\" have the type\n    specified in \"type_to_check\" input.\n\n    -----------\n    Description\n    -----------\n    In some biosignalsnotebooks functions their implementation is extremely dependent on a specific\n    criterion, i.e., 'all' list entries should be of a specific data type.\n\n    In order to ensure this functionality _is_instance function was implemented.\n\n    For example, when plotting data through 'plot' function of 'visualise' module, 'all' entries\n    of time axis and data samples lists need to be 'Numeric'.\n    In order to this condition be checked _is_instance should be called with the following input\n    values:\n\n    _is_instance(Number, [1, 2, 3, True, ...], 'all')\n\n    Sometimes is also relevant to check if at least one of list entries belongs to a data type, for\n    cases like this, the argument \"condition\" should have value equal to \"any\".\n\n    --------\n    Examples\n    --------\n    >>> _is_instance(Number, [1, 2, 3, True], 'all')\n    False\n    >>> _is_instance(Number, [1, 1.2, 3, 5], 'all')\n    True\n\n    ----------\n    Parameters\n    ----------\n    type_to_check : type element\n        Data type (all or any elements of 'element' list must be of the type specified in the\n        current input).\n\n    element : list\n        List where condition specified in \"condition\" will be checked.\n\n    condition : str\n        String with values \"any\" or \"all\" verifying when \"any\" or \"all\" element entries have the\n        specified type.\n\n    deep : bool\n        Flag that identifies when element is in a matrix format and each of its elements should be\n        verified iteratively.\n\n    Returns\n    -------\n    out : boolean\n        Returns True when the \"condition\" is verified for the entries of \"element\" list.\n    \"\"\"\n\n    out = None\n\n    # Direct check of \"condition\" in \"element\".\n    if deep is False:\n        if condition == \"any\":\n            out = any(isinstance(el, type_to_check) for el in element)\n        elif condition == \"all\":\n            out = all(isinstance(el, type_to_check) for el in element)\n\n    # Since \"element\" is in a matrix format, then it will be necessary to check each dimension.\n    else:\n        for row in range(0, len(element)):\n            for column in range(0, len(element[row])):\n                flag = _is_instance(type_to_check, element[column][row], \"all\", deep=False)\n                if flag is False:\n                    out = flag\n                else:\n                    out = True\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _inv_key(list_keys, valid_keys):\n\n    inv_keys = []\n    bool_out = True\n    for i in list_keys:\n        if i not in valid_keys:\n            bool_out = False\n            inv_keys.append(i)\n\n    return bool_out, inv_keys", "response": "Internal function that returns True if all the keywords in list_keys are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a Bokeh file for each plot.", "response": "def _generate_bokeh_file(file_name):\n    \"\"\"\n    -----\n    Brief\n    -----\n    Auxiliary function responsible for the creation of a directory where Bokeh figures will be\n    stored.\n    The \"active\" output file for Bokeh will also be updated for the new one.\n\n    -----------\n    Description\n    -----------\n    To ensure that Bokeh plots are correctly observed in the HTML version of the Notebooks, it is\n    necessary to embed the plots inside Iframes.\n\n    Taking this into consideration, the source file of the plot is mandatory to use an Iframe, and\n    this function ensures the generation of a Bokeh file for each plot, storing it in an adequate\n    place.\n\n    ----------\n    Parameters\n    ----------\n    file_name : str\n        Name given to the file.\n\n    Returns\n    -------\n    out : str\n        String containing the file name.\n    \"\"\"\n    # Creation of our output file instance.\n    if file_name is None:\n        file_name = \"plot_\" + time_package.strftime(\"%Y_%m_%d_%H_%M_%S.html\")\n    else:\n        file_name += \".html\"\n\n    if not os.path.exists(\"generated_plots\"):\n        os.makedirs(\"generated_plots\")\n\n    output_file(os.getcwd().replace(\"\\\\\", \"/\") + \"/generated_plots/\" + file_name)\n\n    return file_name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_a_url(input_element):\n    if type(input_element) is str:\n        # Check if signal_handler is a url.\n        # [Statements to be executed if signal_handler is a url]\n        if any(mark in input_element for mark in [\"http://\", \"https://\", \"www.\", \".pt\", \".com\",\n                                                  \".org\", \".net\"]):\n            return True\n        else:\n            return False\n    else:\n        return False", "response": "Checks if the input_element is a url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a download google link from a link string.", "response": "def _generate_download_google_link(link):\n    \"\"\"\n    -----\n    Brief\n    -----\n    Function that returns a direct download link of a file stored inside a Google Drive\n    Repository.\n\n    -----------\n    Description\n    -----------\n    Generally a link from a Google Drive file is only for viewing purposes.\n\n    If the user wants to download the file it can be done with Google Drive graphical user\n    interface.\n\n    However if we try to programmatically download the file it cannot be done with the normal url.\n\n    So, the current function converts the \"read-only\" link to a downloadable format.\n\n    ----------\n    Parameters\n    ----------\n    link : str\n        Sharable Google Drive link.\n\n    Returns\n    -------\n    out : str\n        Manipulated link, that ensures a direct download with wget function.\n    \"\"\"\n\n    # Get file id.\n    if \"id=\" not in link:\n        # Split link into segments (split character --> /)\n        split_link = link.split(\"/\")\n\n        file_id = split_link[-2]\n    else:\n        # Split link into segments (split string --> \"id=\")\n        split_link = link.split(\"id=\")\n        file_id = split_link[-1]\n\n    return \"https://drive.google.com/uc?export=download&id=\" + file_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a JSON string representation of the current object.", "response": "def to_json(self):\n        \"\"\"\n        :return: str\n        \"\"\"\n        json_dict = self.to_json_basic()\n        json_dict['channels'] = self.relay_channels\n        return json.dumps(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_time(signal, sample_rate=1000):\n\n    # Download of signal if the input is a url.\n    if _is_a_url(signal):\n        # Check if it is a Google Drive sharable link.\n        if \"drive.google\" in signal:\n            signal = _generate_download_google_link(signal)\n        data = load(signal, remote=True)\n        key_level_1 = list(data.keys())[0]\n        if \"00:\" in key_level_1:\n            mac = key_level_1\n            chn = list(data[mac].keys())[0]\n            signal = data[mac][chn]\n        else:\n            chn = key_level_1\n            signal = data[chn]\n\n\n    nbr_of_samples = len(signal)\n    end_of_time = nbr_of_samples / sample_rate\n\n    # ================================= Generation of the Time Axis ===============================\n    time_axis = numpy.linspace(0, end_of_time, nbr_of_samples)\n\n    return list(time_axis)", "response": "This function generates a time axis for the acquisition process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nteam to which the scope is assigned.", "response": "def team(self):\n        \"\"\"Team to which the scope is assigned.\"\"\"\n        team_dict = self._json_data.get('team')\n        if team_dict and team_dict.get('id'):\n            return self._client.team(id=team_dict.get('id'))\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parts(self, *args, **kwargs):\n        return self._client.parts(*args, bucket=self.bucket.get('id'), **kwargs)", "response": "Retrieve parts belonging to this scope."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a single part belonging to this scope.", "response": "def part(self, *args, **kwargs):\n        \"\"\"Retrieve a single part belonging to this scope.\n\n        See :class:`pykechain.Client.part` for available parameters.\n        \"\"\"\n        return self._client.part(*args, bucket=self.bucket.get('id'), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a single part model in this scope.", "response": "def create_model(self, parent, name, multiplicity=Multiplicity.ZERO_MANY):\n        \"\"\"Create a single part model in this scope.\n\n        See :class:`pykechain.Client.create_model` for available parameters.\n        \"\"\"\n        return self._client.create_model(parent, name, multiplicity=multiplicity)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a single model belonging to this scope.", "response": "def model(self, *args, **kwargs):\n        \"\"\"Retrieve a single model belonging to this scope.\n\n        See :class:`pykechain.Client.model` for available parameters.\n        \"\"\"\n        return self._client.model(*args, bucket=self.bucket.get('id'), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef activities(self, *args, **kwargs):\n        if self._client.match_app_version(label='wim', version='<2.0.0', default=True):\n            return self._client.activities(*args, scope=self.id, **kwargs)\n        else:\n            return self._client.activities(*args, scope_id=self.id, **kwargs)", "response": "Retrieve activities belonging to this scope."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_activity(self, *args, **kwargs):\n        if self._client.match_app_version(label='wim', version='<2.0.0', default=True):\n            return self._client.create_activity(self.process, *args, **kwargs)\n        else:\n            return self._client.create_activity(self.workflow_root, *args, **kwargs)", "response": "Create a new activity belonging to this scope."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve services belonging to this scope.", "response": "def services(self, *args, **kwargs):\n        \"\"\"Retrieve services belonging to this scope.\n\n        See :class:`pykechain.Client.services` for available parameters.\n\n        .. versionadded:: 1.13\n        \"\"\"\n        return self._client.services(*args, scope=self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a service to current scope.", "response": "def create_service(self, *args, **kwargs):\n        \"\"\"Create a service to current scope.\n\n        See :class:`pykechain.Client.create_service` for available parameters.\n\n        .. versionadded:: 1.13\n        \"\"\"\n        return self._client.create_service(*args, scope=self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a single service belonging to this scope.", "response": "def service(self, *args, **kwargs):\n        \"\"\"Retrieve a single service belonging to this scope.\n\n        See :class:`pykechain.Client.service` for available parameters.\n\n        .. versionadded:: 1.13\n        \"\"\"\n        return self._client.service(*args, scope=self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving services belonging to this scope.", "response": "def service_executions(self, *args, **kwargs):\n        \"\"\"Retrieve services belonging to this scope.\n\n        See :class:`pykechain.Client.service_executions` for available parameters.\n\n        .. versionadded:: 1.13\n        \"\"\"\n        return self._client.service_executions(*args, scope=self.id, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef service_execution(self, *args, **kwargs):\n        return self._client.service_execution(*args, scope=self.id, **kwargs)", "response": "Retrieve a single service execution belonging to this scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves members of the scope.", "response": "def members(self, is_manager=None):\n        \"\"\"\n        Retrieve members of the scope.\n\n        :param is_manager: (optional) set to True to return only Scope members that are also managers.\n        :type is_manager: bool\n        :return: List of members (usernames)\n\n        Examples\n        --------\n        >>> members = project.members()\n        >>> managers = project.members(is_manager=True)\n\n        \"\"\"\n        if not is_manager:\n            return [member for member in self._json_data['members'] if member['is_active']]\n        else:\n            return [member for member in self._json_data['members'] if\n                    member.get('is_active', False) and member.get('is_manager', False)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_member(self, member):\n        select_action = 'add_member'\n\n        self._update_scope_project_team(select_action=select_action, user=member, user_type='member')", "response": "Add a single member to the scope list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving a single member from the scope list.", "response": "def remove_member(self, member):\n        \"\"\"\n        Remove a single member to the scope.\n\n        :param member: single username to be removed from the scope list of members\n        :type member: basestring\n        :raises APIError: when unable to update the scope member\n        \"\"\"\n        select_action = 'remove_member'\n\n        self._update_scope_project_team(select_action=select_action, user=member, user_type='member')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_manager(self, manager):\n        select_action = 'add_manager'\n\n        self._update_scope_project_team(select_action=select_action, user=manager, user_type='manager')", "response": "Add a single manager to the scope list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a single manager from the scope list.", "response": "def remove_manager(self, manager):\n        \"\"\"\n        Remove a single manager to the scope.\n\n        :param manager: single username to be added to the scope list of managers\n        :type manager: basestring\n        :raises APIError: when unable to update the scope manager\n        \"\"\"\n        select_action = 'remove_manager'\n\n        self._update_scope_project_team(select_action=select_action, user=manager, user_type='manager')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_scope_project_team(self, select_action, user, user_type):\n        if isinstance(user, str):\n            users = self._client._retrieve_users()\n            manager_object = next((item for item in users['results'] if item[\"username\"] == user), None)\n            if manager_object:\n                url = self._client._build_url('scope', scope_id=self.id)\n                r = self._client._request('PUT', url, params={'select_action': select_action},\n                                          data={\n                                              'user_id': manager_object['pk']\n                                          })\n                if r.status_code != requests.codes.ok:  # pragma: no cover\n                    raise APIError(\"Could not {} {} in Scope\".format(select_action.split('_')[0], user_type))\n            else:\n                raise NotFoundError(\"User {} does not exist\".format(user))\n        else:\n            raise TypeError(\"User {} should be defined as a string\".format(user))", "response": "Updates the scope project team of the Scope."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nediting the details of a scope.", "response": "def edit(self, name=None, description=None, start_date=None, due_date=None, status=None, tags=None, team=None,\n             options=None, **kwargs):\n        \"\"\"Edit the details of a scope.\n\n        :param name: (optionally) edit the name of the scope\n        :type name: basestring or None\n        :param description: (optionally) edit the description of the scope\n        :type description: basestring or None\n        :param start_date: (optionally) edit the start date of the scope as a datetime object (UTC time/timezone\n                            aware preferred)\n        :type start_date: datetime or None\n        :param due_date: (optionally) edit the due_date of the scope as a datetime object (UTC time/timzeone\n                            aware preferred)\n        :type due_date: datetime or None\n        :param status: (optionally) edit the status of the scope as a string based\n        :type status: basestring or None\n        :param tags: (optionally) replace the tags on a scope, which is a list of strings [\"one\",\"two\",\"three\"]\n        :type tags: list of basestring or None\n        :param team: (optionally) add the scope to a team\n        :type team: UUIDstring or None\n        :param options: (optionally) custom options dictionary stored on the scope object\n        :type options: dict or None\n        :raises IllegalArgumentError: if the type of the inputs is not correct\n        :raises APIError: if another Error occurs\n        :warns: UserWarning - When a naive datetime is provided. Defaults to UTC.\n\n        Examples\n        --------\n        >>> from datetime import datetime\n        >>> project.edit(name='New project name',\n        ...              description='Changing the description just because I can',\n        ...              start_date=datetime.utcnow(),  # naive time is interpreted as UTC time\n        ...              status=ScopeStatus.CLOSED)\n\n        If we want to provide timezone aware datetime objects we can use the 3rd party convenience library :mod:`pytz`.\n        Mind that we need to fetch the timezone first and use `<timezone>.localize(<your datetime>)` to make it\n        work correctly.\n\n        Using `datetime(2017,6,1,23,59,0 tzinfo=<tz>)` does NOT work for most timezones with a\n        daylight saving time. Check the `pytz <http://pythonhosted.org/pytz/#localized-times-and-date-arithmetic>`_\n        documentation.\n\n        To make it work using :mod:`pytz` and timezone aware :mod:`datetime` see the following example::\n\n        >>> import pytz\n        >>> start_date_tzaware = datetime.now(pytz.utc)\n        >>> mytimezone = pytz.timezone('Europe/Amsterdam')\n        >>> due_date_tzaware = mytimezone.localize(datetime(2019, 10, 27, 23, 59, 0))\n        >>> project.edit(due_date=due_date_tzaware, start_date=start_date_tzaware)\n\n        To assign a scope to a team see the following example::\n\n        >>> my_team = client.team(name='My own team')\n        >>> project.edit(team=my_team)\n\n        \"\"\"\n        update_dict = {'id': self.id}\n        if name is not None:\n            if isinstance(name, (str, text_type)):\n                update_dict.update({'name': name})\n                self.name = name\n            else:\n                raise IllegalArgumentError('Name should be a string')\n\n        if description is not None:  # isinstance(description, (str, text_type)):\n            if isinstance(description, (str, text_type)):\n                update_dict.update({'text': description})\n                self.text = description\n            else:\n                raise IllegalArgumentError('Description should be a string')\n\n        if start_date is not None:\n            if isinstance(start_date, datetime.datetime):\n                if not start_date.tzinfo:\n                    warnings.warn(\"The startdate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(start_date.isoformat(sep=' ')))\n                update_dict.update({'start_date': start_date.isoformat(sep='T')})\n            else:\n                raise IllegalArgumentError('Start date should be a datetime.datetime() object')\n\n        if due_date is not None:\n            if isinstance(due_date, datetime.datetime):\n                if not due_date.tzinfo:\n                    warnings.warn(\"The duedate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(due_date.isoformat(sep=' ')))\n                update_dict.update({'due_date': due_date.isoformat(sep='T')})\n            else:\n                raise IllegalArgumentError('Due date should be a datetime.datetime() object')\n\n        if status is not None:\n            if isinstance(status, (str, text_type)) and status in ScopeStatus.values():\n                update_dict.update({'status': status})\n            else:\n                raise IllegalArgumentError('Status should be a string and in the list of acceptable '\n                                           'status strings: {}'.format(ScopeStatus.values()))\n\n        if tags is not None:\n            if isinstance(tags, (list, tuple, set)):\n                update_dict.update({'tags': tags})\n            else:\n                raise IllegalArgumentError('tags should be a an array (list, tuple, set) of strings')\n\n        if team is not None:\n            if isinstance(team, (str, text_type)) and is_uuid(team):\n                update_dict.update({'team_id': team})\n            elif isinstance(team, Team):\n                update_dict.update({'team_id': team.id})\n            else:\n                raise IllegalArgumentError(\"team should be the uuid of a team\")\n\n        if options is not None:\n            if isinstance(options, dict):\n                update_dict.update({'options': options})\n            else:\n                raise IllegalArgumentError(\"options should be a dictionary\")\n\n        url = self._client._build_url('scope', scope_id=self.id)\n\n        r = self._client._request('PUT', url, json=update_dict)\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not update Scope ({})\".format(r))\n        else:\n            self._json_data = r.json().get('results') and r.json().get('results')[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncloning current scope. See :class:`pykechain.Client.clone_scope` for available parameters. .. versionadded:: 2.6.0", "response": "def clone(self, *args, **kwargs):\n        \"\"\"\n        Clone current scope.\n\n        See :class:`pykechain.Client.clone_scope` for available parameters.\n\n        .. versionadded:: 2.6.0\n        \"\"\"\n        return self._client.clone_scope(*args, source_scope=self, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, value, _sa_initiator=None):\n\n        key = self.keyfunc(value)\n        # Let self[key] raise if key is not in this collection\n        # testlib.pragma exempt:__ne__\n        if not self.__contains__(key) or value not in self[key]:\n            raise sa_exc.InvalidRequestError(\n                \"Can not remove '%s': collection holds '%s' for key '%s'. \"\n                \"Possible cause: is the MappedCollection key function \"\n                \"based on mutable properties or properties that only obtain \"\n                \"values after flush?\" %\n                (value, self[key], key))\n        self.__getitem__(key, _sa_initiator).remove(value)", "response": "Remove an item by value consulting the keyfunc for the key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating and convert a dict - like object into values for set()ing.", "response": "def _convert(self, dictlike):\n        \"\"\"Validate and convert a dict-like object into values for set()ing.\n\n        This is called behind the scenes when a MappedCollection is replaced\n        entirely by another collection, as in::\n\n          myobj.mappedcollection = {'a':obj1, 'b': obj2} # ...\n\n        Raises a TypeError if the key in any (key, value) pair in the dictlike\n        object does not match the key that this collection's keyfunc would\n        have assigned for that value.\n\n        \"\"\"\n        for incoming_key, valuelist in util.dictlike_iteritems(dictlike):\n            for value in valuelist:\n                new_key = self.keyfunc(value)\n                if incoming_key != new_key:\n                    raise TypeError(\n                        \"Found incompatible key %r for value %r; this \"\n                        \"collection's \"\n                        \"keying function requires a key of %r for this value.\" % (\n                            incoming_key, value, new_key))\n                yield value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a safe HTML chunk that can be used as a Jinja2 filter.", "response": "def progressive(image_field, alt_text=''):\n    \"\"\"\n    Used as a Jinja2 filter, this function returns a safe HTML chunk.\n\n    Usage (in the HTML template):\n\n        {{ obj.image|progressive }}\n\n    :param django.db.models.fields.files.ImageFieldFile image_field: image\n    :param str alt_text: str\n    :return: a safe HTML template ready to be rendered\n    \"\"\"\n    if not isinstance(image_field, ImageFieldFile):\n        raise ValueError('\"image_field\" argument must be an ImageField.')\n\n    for engine in engines.all():\n        if isinstance(engine, BaseEngine) and hasattr(engine, 'env'):\n            env = engine.env\n            if isinstance(env, Environment):\n                context = render_progressive_field(image_field, alt_text)\n                template = env.get_template(\n                    'progressiveimagefield/render_field.html'\n                )\n                rendered = template.render(**context)\n                return Markup(rendered)\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopulating the internal state of the object with the contents of the data.", "response": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        -DB1    last 2 bits   = channel\n        -DB1    first 6 bist  = pulses\n        -DB2-5                = pulse counter\n        -DB6-7                = ms/pulse               \n        :return: None\n        \"\"\"\n        assert isinstance(data, bytes)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 7)\n        self.set_attributes(priority, address, rtr)\n        self.channel = (data[0] & 0x03) +1 \n        self.pulses = (data[0] >> 2) * 100\n        self.counter = (data[1] << 24) + (data[2] << 16) + (data[3] << 8) + data[4]\n        self.kwh = float(float(self.counter)/self.pulses)\n        self.delay = (data[5] << 8) + data[6]\n        self.watt = float((1000 * 1000 * 3600) / (self.delay * self.pulses))\n        if self.watt < 55:\n            self.watt = 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_json(self):\n        json_dict = self.to_json_basic()\n        json_dict['pulses'] = self.pulses\n        json_dict['counter'] = self.counter\n        json_dict['kwh'] = self.kwh\n        json_dict['delay'] = self.delay\n        json_dict['watt'] = self.watt\n        json_dict['channel'] = self.channel\n        return json.dumps(json_dict)", "response": "Returns a string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_form(self, form_class=None):\n        form = super().get_form(form_class)\n        if self._save:\n            make_form_or_formset_fields_not_required(form)\n        return form", "response": "Returns the form for this task."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransitions to save the task and return to ASSIGNED state.", "response": "def save_task(self):\n        \"\"\"Transition to save the task and return to ``ASSIGNED`` state.\"\"\"\n        task = self.request.activation.task\n        task.status = STATUS.ASSIGNED\n        task.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef activation_done(self, *args, **kwargs):\n        if self._save:\n            self.save_task()\n        else:\n            super().activation_done(*args, **kwargs)", "response": "Complete the activation or save only depending on form submit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopulating the object with the data from the given bytes.", "response": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        :return: None\n        \"\"\"\n        assert isinstance(data, bytes)\n        self.needs_low_priority(priority)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 3)\n        self.set_attributes(priority, address, rtr)\n        self._wday = data[0]\n        self._hour = data[1]\n        self._min = data[2]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_json(self):\n        json_dict = self.to_json_basic()\n        json_dict['wday'] = self._wday\n        json_dict['hour'] = self._hour \n        json_dict['min'] = self._min\n        return json.dumps(json_dict)", "response": "Returns a string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_to_binary(self):\n        return bytes([\n            COMMAND_CODE,\n            self._wday,\n            self._hour,\n            self._min\n        ])", "response": "Convert the ISO8601 date to binary format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_sample_rate_compare(data_dict, file_name=None):\n\n    # Generation of the HTML file where the plot will be stored.\n    #file_name = _generate_bokeh_file(file_name)\n\n    nbr_rows = len(list(data_dict.keys()))\n\n    # List that store the figure handler.\n    list_figures = plot([[]] * nbr_rows * 2, [[]] * nbr_rows * 2, y_axis_label=\"Raw Data\",\n                        x_axis_label=\"Time (s)\", grid_lines=nbr_rows, grid_columns=2,\n                        grid_plot=True, get_fig_list=True, show_plot=False)\n\n    # Generation of Bokeh Figures.\n    grid_list = []\n    for iter, sample_rate in enumerate(list(data_dict.keys())):\n        # List of figures in a valid gridplot format (each entry will define a row and each\n        # subentry a column of the gridplot)\n        grid_list += [[]]\n\n        # Plotting of 10 seconds segment.\n        # [Figure tile]\n        title_unzoom = Title()\n        title_unzoom.text = 'Sampling Rate: ' + sample_rate + \" Hz\"\n        list_figures[2 * iter].title = title_unzoom\n\n        # [Plot generation]\n        list_figures[2 * iter].line(data_dict[sample_rate][\"time\"][:10 * int(sample_rate)],\n                                    data_dict[sample_rate][\"data\"][:10 * int(sample_rate)],\n                                    **opensignals_kwargs(\"line\"))\n\n        # Storage of customized figure.\n        grid_list[-1] += [list_figures[2 * iter]]\n\n        # Plotting of a zoomed section with 1 second.\n        # [Figure tile]\n        title_zoom = Title()\n        title_zoom.text = 'Zoomed section @ ' + sample_rate + \" Hz\"\n        list_figures[2 * iter + 1].title = title_zoom\n\n        # [Plot generation]\n        list_figures[2 * iter + 1].line(data_dict[sample_rate][\"time\"][:int(sample_rate)],\n                                        data_dict[sample_rate][\"data\"][:int(sample_rate)],\n                                        **opensignals_kwargs(\"line\"))\n\n        # Storage of customized figure.\n        grid_list[-1] += [list_figures[2 * iter + 1]]\n\n    # Organisation of figures in a gridplot.\n    grid_plot_1 = gridplot(grid_list, **opensignals_kwargs(\"gridplot\"))\n\n    # Show representations.\n    show(grid_plot_1)", "response": "Generates a Bokeh file that will be used to plot the sample rate of the ECG signal at a specific sampling rate."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef acquire_subsamples_gp1(input_data, file_name=None):\n\n    # Generation of the HTML file where the plot will be stored.\n    #file_name = _generate_bokeh_file(file_name)\n\n    # Number of acquired samples (Original sample_rate = 4000 Hz)\n    fs_orig = 4000\n    nbr_samples_orig = len(input_data)\n    data_interp = {\"4000\": {}}\n    data_interp[\"4000\"][\"data\"] = input_data\n    data_interp[\"4000\"][\"time\"] = numpy.linspace(0, nbr_samples_orig / fs_orig, nbr_samples_orig)\n\n    # Constants\n    time_orig = data_interp[\"4000\"][\"time\"]\n    data_orig = data_interp[\"4000\"][\"data\"]\n\n    # ============ Interpolation of data accordingly to the desired sampling frequency ============\n    # sample_rate in [3000, 1000, 500, 200, 100] - Some of the available sample frequencies at Plux\n    # acquisition systems\n    # sample_rate in [50, 20] - Non-functional sampling frequencies (Not available at Plux devices\n    # because of their limited application)\n    for sample_rate in [3000, 1000, 500, 200, 100, 50, 20]:\n        fs_str = str(sample_rate)\n        nbr_samples_interp = int((nbr_samples_orig * sample_rate) / fs_orig)\n        data_interp[fs_str] = {}\n        data_interp[fs_str][\"time\"] = numpy.linspace(0, nbr_samples_orig / fs_orig,\n                                                     nbr_samples_interp)\n        data_interp[fs_str][\"data\"] = numpy.interp(data_interp[fs_str][\"time\"], time_orig,\n                                                   data_orig)\n\n    # List that store the figure handler.\n    list_figures = []\n\n    # Generation of Bokeh Figures.\n    for iter_nbr, sample_rate in enumerate([\"4000\", \"3000\", \"1000\", \"500\", \"200\", \"100\"]):\n        # If figure number is a multiple of 3 or if we are generating the first figure...\n        if iter_nbr == 0 or iter_nbr % 2 == 0:\n            list_figures.append([])\n\n        # Plotting phase.\n        list_figures[-1].append(figure(x_axis_label='Time (s)', y_axis_label='Raw Data',\n                                       title=\"Sampling Frequency: \" + sample_rate + \" Hz\",\n                                       **opensignals_kwargs(\"figure\")))\n        list_figures[-1][-1].line(data_interp[sample_rate][\"time\"][:int(sample_rate)],\n                                  data_interp[sample_rate][\"data\"][:int(sample_rate)],\n                                  **opensignals_kwargs(\"line\"))", "response": "This function is used to acquire subsamples of a Bokeh grid with 3x2 format. It is used to plot a grid - plot with 3x2 format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_ecg_pan_tompkins_steps(time, orig_ecg, pre_process_ecg, sampling_rate, titles):\n\n    if len(titles) == 2:\n        # Representation of the output of Step 1 of Pan-Tompkins R-Peak Detection Algorithm.\n        # List that store the figure handler\n        list_figures_1 = [[]]\n\n        # Plotting of Original Signal\n        list_figures_1[-1].append(figure(x_axis_label='Time (s)', y_axis_label='Raw Data',\n                                         title=titles[0], **opensignals_kwargs(\"figure\")))\n        list_figures_1[-1][-1].line(time[:sampling_rate], orig_ecg[:sampling_rate], **opensignals_kwargs(\"line\"))\n\n        # Plotting of Filtered Signal\n        list_figures_1[-1].append(figure(x_axis_label='Time (s)', y_axis_label='Raw Data',\n                                         title=titles[1], **opensignals_kwargs(\"figure\")))\n        list_figures_1[-1][-1].line(time[:sampling_rate], pre_process_ecg[:sampling_rate], **opensignals_kwargs(\"line\"))\n\n        # Grid-Plot.\n        opensignals_style([item for sublist in list_figures_1 for item in sublist])\n        grid_plot_1 = gridplot(list_figures_1, **opensignals_kwargs(\"gridplot\"))\n        show(grid_plot_1)\n    else:\n        raise RuntimeError(\"The field 'title' must be a list of strings with size 2 !\")", "response": "This function generates a Bokeh figure for the specified time - axis series of values and the time - axis signal of the original signal and the original signal of the filtered signal of the original signal and the original signal of the original signal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_ecg_pan_tompkins_peaks(time, orig_ecg, integrated_ecg, sampling_rate, possible_peaks,\n                                probable_peaks, definitive_peaks):\n    \"\"\"\n    -----\n    Brief\n    -----\n    With this plotting function it will be possible to demonstrate which \"peaks\" are being\n    detected in each stage of Pan-Tompkins Algorithm (possible, probable, definitive).\n\n    -----------\n    Description\n    -----------\n    Function intended to generate a Bokeh figure containing all peaks detected in a temporal\n    segment (of ECG integrated signal) with 2 seconds.\n\n    \"Possible Peaks\" are marked with the smallest circle while \"Probable Peaks\" and \"Definitive\n    Peaks\" are highlighted with medium and large size circles.\n\n    Applied in the Notebook titled \"Event Detection - R Peaks (ECG)\".\n\n    ----------\n    Parameters\n    ----------\n    time : list\n        List containing the time-axis sequence of values.\n\n    orig_ecg : list\n        Sequence of sampled values (Original ECG).\n\n    integrated_ecg : list\n        Sequence of sampled values (Integrated ECG).\n\n    sampling_rate : int\n        Acquisition sampling rate (Hz)\n\n    possible_peaks : list\n        List containing all \"Possible Peaks\" detected from Pan-Tompkins R Peak Detection\n        Algorithm.\n\n    probable_peaks : list\n        List containing all \"Probable Peaks\" detected from Pan-Tompkins R Peak Detection\n        Algorithm.\n\n    definitive_peaks : list\n        List containing all \"Definitive Peaks\" detected from Pan-Tompkins R Peak Detection\n        Algorithm.\n    \"\"\"\n\n    # List that store the figure handler\n    list_figures = []\n\n    # Plotting of a signal segment with 2 seconds\n    segment_data = numpy.array(orig_ecg[:2 * sampling_rate])\n    segment_int = numpy.array(integrated_ecg[:2 * sampling_rate])\n    segment_time = numpy.array(time[:2 * sampling_rate])\n\n    # Peaks list for the 2 seconds window\n    possible_peaks_wind = numpy.array(possible_peaks)[numpy.array(possible_peaks) < len(segment_int)]\n    probable_peaks_wind = numpy.array(probable_peaks)[numpy.array(probable_peaks) < len(segment_int)]\n    definitive_peaks_wind = numpy.array(definitive_peaks)[numpy.array(definitive_peaks) < len(segment_int)]\n\n    list_figures.append(figure(x_axis_label='Time (s)', y_axis_label='Raw Data', **opensignals_kwargs(\"figure\")))\n    list_figures[-1].line(segment_time, segment_int, **opensignals_kwargs(\"line\"))\n    list_figures[-1].circle(segment_time[definitive_peaks_wind], segment_int[definitive_peaks_wind], size=30, color=\"#00893E\", legend=\"Definitive Peaks\")\n    list_figures[-1].circle(segment_time[probable_peaks_wind], segment_int[probable_peaks_wind], size=20, color=\"#009EE3\", legend=\"Probable Peaks\")\n    list_figures[-1].circle(segment_time[possible_peaks_wind], segment_int[possible_peaks_wind], size=10, color=\"#302683\", legend=\"Possible Peaks\")\n\n    # Show figure.\n    opensignals_style(list_figures)\n    show(list_figures[-1])", "response": "This function generates a Bokeh figure containing all peaks detected from a PAN - Tompkins signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_emg_graphical_statistical(time, signal, max_sample_value, min_sample_value, avg_sample_value,\n                                   std_sample_value):\n    \"\"\"\n    -----\n    Brief\n    -----\n    This plotting function ensures a graphical representation of maximum, minimum and average sample\n    values registered on the entire EMG acquisition.\n\n    -----------\n    Description\n    -----------\n    Function intended to generate a single Bokeh figure with graphically describing and identifying\n    some statistical parameters extracted from the analysis of the entire electromyographic (EMG) signal.\n\n    Applied in the Notebook titled \"EMG Analysis - Time and Frequency Parameters\".\n\n    ----------\n    Parameters\n    ----------\n    time : list\n        Time-axis linked to the acquired EMG signal samples.\n\n    signal : list\n        Acquired EMG signal samples.\n\n    max_sample_value : float\n        Maximum value registered in the acquired EMG samples.\n\n    min_sample_value: float\n        Minimum value registered in the acquired EMG samples.\n\n    avg_sample_value : float\n        Average value registered in the acquired EMG samples.\n\n    std_sample_value : int\n        Standard deviation of the acquired EMG sample values relatively to avg_sample_value.\n    \"\"\"\n\n    # List that store the figure handler\n    list_figures = []\n\n    # Plotting of EMG.\n    list_figures.append(figure(x_axis_label='Time (s)', y_axis_label='Electric Tension (mV)', x_range=(0, time[-1] + 0.50 * time[-1]), y_range=[-1.10, 1], **opensignals_kwargs(\"figure\")))\n    list_figures[-1].line(time, signal, legend=\"EMG Signal\", **opensignals_kwargs(\"line\"))\n\n    # Representation of EMG and the determined parameters\n    parameter_list = [\"Maximum\", \"Minimum\", \"Average\", \"Standard Deviation\"]\n    for parameter in parameter_list:\n        find_time_max = numpy.array(time)[numpy.where(numpy.array(signal) == max_sample_value)]\n        find_time_min = numpy.array(time)[numpy.where(numpy.array(signal) == min_sample_value)]\n        if parameter == \"Maximum\":\n            list_figures[-1].circle(find_time_max, max_sample_value, radius = 0.5, fill_color=opensignals_color_pallet(),\n                                    legend=parameter + \" EMG\")\n        elif parameter == \"Minimum\":\n            list_figures[-1].circle(find_time_min, min_sample_value, radius=0.5, fill_color=opensignals_color_pallet(),\n                                    legend=parameter + \" EMG\")\n        elif parameter == \"Average\":\n            list_figures[-1].line([0, time[-1]], [avg_sample_value, avg_sample_value],\n                                  legend=parameter + \" EMG Sample\", **opensignals_kwargs(\"line\"))\n        elif parameter == \"Standard Deviation\":\n            box_annotation = BoxAnnotation(left=0, right=time[-1], top=avg_sample_value + std_sample_value,\n                                           bottom=avg_sample_value - std_sample_value, fill_color=\"black\",\n                                           fill_alpha=0.3)\n            list_figures[-1].rect(find_time_min, std_sample_value, width=0, height=0, fill_color=\"black\", fill_alpha=0.3,\n                                  legend=\"Average + Standard Deviation Zone\")\n            list_figures[-1].add_layout(box_annotation)\n\n    # Show figure.\n    opensignals_style(list_figures)\n    show(list_figures[-1])", "response": "This function generates a single Bokeh figure that shows the maximum minimum and average sample values for the given time and signal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_emg_spect_freq(freq_axis, power_axis, max_freq, median_freq):\n\n    # List that store the figure handler\n    list_figures = []\n\n    # Plotting of EMG Power Spectrum\n    list_figures.append(\n        figure(x_axis_label='Frequency (Hz)', y_axis_label='Relative Power (a.u.)', **opensignals_kwargs(\"figure\")))\n    list_figures[-1].line(freq_axis, power_axis, legend=\"Power Spectrum\", **opensignals_kwargs(\"line\"))\n    list_figures[-1].patch(list(freq_axis) + list(freq_axis)[::-1], list(power_axis) + list(numpy.zeros(len(power_axis))),\n                           fill_color=opensignals_color_pallet(), fill_alpha=0.5, line_alpha=0,\n                           legend=\"Area Under Curve\")\n    list_figures[-1].line([median_freq, median_freq], [0, power_axis[numpy.where(freq_axis == median_freq)[0][0]]],\n                          legend=\"Median Frequency\", **opensignals_kwargs(\"line\"))\n    list_figures[-1].line([max_freq, max_freq], [0, power_axis[numpy.where(freq_axis == max_freq)[0][0]]],\n                          legend=\"Maximum Power Frequency\", **opensignals_kwargs(\"line\"))\n\n    # Show figure.\n    opensignals_style(list_figures)\n    show(list_figures[-1])", "response": "This function generates a single Bokeh figure that shows the frequency and power spectrum of the EMG signal."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the Bokeh plot of the median frequency evolution of the time series.", "response": "def plot_median_freq_evol(time_signal, signal, time_median_freq, median_freq, activations_begin,\n                          activations_end, sample_rate, file_name=None):\n    \"\"\"\n    -----\n    Brief\n    -----\n    Graphical representation of the EMG median power frequency evolution time series.\n\n    -----------\n    Description\n    -----------\n    Function intended to generate a Bokeh figure with 2x1 format, where each muscular activation\n    period is identified through a colored box and the plot that shows the median frequency\n    evolution is also presented.\n\n    In the first cell is presented the EMG signal, highlighting each muscular activation.\n    The second cell has the same time scale as the first one (the two plots are synchronized), being\n    plotted the evolution time series of EMG median frequency.\n\n    Per muscular activation period is extracted a Median Power Frequency value (sample), so, our\n    window is a muscular activation period.\n\n    Median power frequency is a commonly used parameter for evaluating muscular fatigue.\n    It is widely accepted that this parameter decreases as fatigue sets in.\n\n    Applied in the Notebook \"Fatigue Evaluation - Evolution of Median Power Frequency\".\n\n    ----------\n    Parameters\n    ----------\n    time_signal : list\n        List with the time axis samples of EMG signal.\n\n    signal : list\n        List with EMG signal to present.\n\n    time_median_freq : list\n        List with the time axis samples of the median frequency evolution time-series.\n\n    median_freq : list\n        List with the Median Frequency samples.\n\n    activations_begin : list\n        List with the samples where each muscular activation period starts.\n\n    activations_end : list\n        List with the samples where each muscular activation period ends.\n\n    sample_rate : int\n        Sampling rate of acquisition.\n\n    file_name : str\n        Path containing the destination folder where the Bokeh figure will be stored.\n    \"\"\"\n\n    # Generation of the HTML file where the plot will be stored.\n    #file_name = _generate_bokeh_file(file_name)\n\n    list_figures_1 = plot([list(time_signal), list(time_median_freq)],\n                          [list(signal), list(median_freq)],\n                          title=[\"EMG Acquisition highlighting muscular activations\",\n                                 \"Median Frequency Evolution\"], grid_plot=True,\n                          grid_lines=2, grid_columns=1, open_signals_style=True,\n                          x_axis_label=\"Time (s)\",\n                          yAxisLabel=[\"Raw Data\", \"Median Frequency (Hz)\"],\n                          x_range=[0, 125], get_fig_list=True, show_plot=False)\n\n    # Highlighting of each processing window\n    for activation in range(0, len(activations_begin)):\n        color = opensignals_color_pallet()\n        box_annotation = BoxAnnotation(left=activations_begin[activation] / sample_rate,\n                                       right=activations_end[activation] / sample_rate,\n                                       fill_color=color, fill_alpha=0.1)\n        box_annotation_copy = BoxAnnotation(left=activations_begin[activation] / sample_rate,\n                                            right=activations_end[activation] / sample_rate,\n                                            fill_color=color, fill_alpha=0.1)\n        list_figures_1[0].add_layout(box_annotation)\n        list_figures_1[1].add_layout(box_annotation_copy)\n\n    gridplot_1 = gridplot([[list_figures_1[0]], [list_figures_1[1]]],\n                          **opensignals_kwargs(\"gridplot\"))\n    show(gridplot_1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_informational_band(freqs, power, signal, sr, band_begin, band_end,\n                            legend=\"Signal Power Spectrum\", x_lim=[], y_lim=[],\n                            show_plot=False, file_name=None):\n    \"\"\"\n    -----\n    Brief\n    -----\n    With this function it is possible to present a plot containing the FFT Power Spectrum of an ECG\n    signal, highlighting the informative frequency band.\n\n    -----------\n    Description\n    -----------\n    The FFT Power Spectrum, of an input signal, can be generated through plotfft function of\n    novainstrumentation package (or periogram function of scipy package).\n    The x axis (freqs) represents the frequency components of the signal, after decomposition was\n    achieved by applying the Fourier Transform. The y axis (power) defines the relative weight of\n    each frequency component (sinusoidal function) in the process of reconstructing the signal by\n    re-summing of decomposition components.\n\n    Additionally, it is also graphically presented a rectangular box showing which are the frequency\n    components with relevant information for studying our input physiological signal.\n\n    Note that each physiological signal has its own \"informational band\", whose limits should be\n    specified in the input arguments \"band_begin\" and \"band_end\".\n\n    Applied in the Notebook \"Digital Filtering - A Fundamental Pre-Processing Step\".\n\n    ----------\n    Parameters\n    ----------\n    freqs : list\n        Frequency axis of power spectrum, defining which frequency components were used during the\n        Fourier decomposition.\n\n    power : list\n        Power axis of power spectrum, defining the relative weight that each frequency component,\n        inside \"freqs\", will have during the signal reconstruction.\n\n    signal : list\n        List containing the acquired signal samples.\n\n    sr : int\n        Sampling rate.\n\n    band_begin : float\n        Lower frequency inside the signal informational band.\n\n    band_end : float\n        Higher frequency inside the signal informational band.\n\n    legend : str\n        A string containing the legend that defines the power spectrum, for example: \"ECG Power\n        Spectrum\".\n\n    x_lim : list\n        A list with length equal to 2, defining the first and last x value that should be presented.\n\n    y_lim : list\n        A list with length equal to 2, defining the first and last y value that should be presented.\n\n    show_plot : bool\n        If True then the generated figure/plot will be shown to the user.\n\n    file_name : str\n        Path containing the destination folder where the Bokeh figure will be stored.\n\n    Returns\n    -------\n    out : bokeh figure\n        Bokeh figure presenting the signal power spectrum and highlighting the informational band.\n    \"\"\"\n\n    # Generation of the HTML file where the plot will be stored.\n    #file_name = _generate_bokeh_file(file_name)\n\n    # ----------------------------- Verification procedure -----------------------------------------\n    # Check if list is the type of input arguments x_lim and y_lim.\n    if type(x_lim) is list and type(y_lim) is list:\n        if len(x_lim) == 2 and len(y_lim) == 2:\n            if len(x_lim) == 0:\n                x_lim = [freqs[0], freqs[-1]]\n            if len(y_lim) == 0:\n                y_lim = [power[0], power[-1]]\n        else:\n            raise RuntimeError(\"The inputs arguments 'x_lim' and 'y_lim', when explicitly specified, \"\n                               \"must be formed by two elements (defining the lower and upper limits \"\n                               \"of the x and y axis).\")\n    else:\n        raise RuntimeError(\"At least one of the input arguments (x_lim or y_lim) does not have a valid\"\n                           \" type. The inputs must be lists.\")\n\n    # List that store the figure handler\n    list_figures = []\n\n    # Plotting of power spectrum\n    list_figures.append(figure(x_axis_label='Frequency (Hz)', y_axis_label='Relative Weight', x_range=(x_lim[0], x_lim[-1]),\n                                 y_range=(y_lim[0], y_lim[1]), **opensignals_kwargs(\"figure\")))\n    list_figures[-1].line(freqs, power, legend=legend,\n                            **opensignals_kwargs(\"line\"))\n\n    # Highlighting of informational band\n    color = opensignals_color_pallet()\n    box_annotation = BoxAnnotation(left=band_begin, right=band_end, fill_color=color,\n                                   fill_alpha=0.1)\n    list_figures[-1].circle([-100], [0], fill_color=color, fill_alpha=0.1,\n                              legend=\"Informational Band\")\n    list_figures[-1].add_layout(box_annotation)\n\n    # # Determination of the maximum frequency\n    # max_freq = max_frequency(signal, sr)\n    #\n    # # Rejection band(above maximum frequency)\n    # color = \"black\"\n    # box_annotations = BoxAnnotation(left=max_freq, right=max_freq + 5, fill_color=color,\n    #                                 fill_alpha=0.1)\n    #\n    # # Show of the plots with the rejection band\n    # list_figures[-1].circle([-100], [0], fill_color=color, fill_alpha=0.1, legend=\"Rejected Band\")\n    # list_figures[-1].add_layout(box_annotations)\n    # list_figures[-1].add_layout(Arrow(end=VeeHead(size=15, line_color=color, fill_color=color,\n    #                                                 fill_alpha=0.1), line_color=color,\n    #                                     x_start=max_freq + 5, y_start=y_lim[1]/2,\n    #                                     x_end=max_freq + 15, y_end=y_lim[1]/2))\n\n    # Apply opensignals style.\n    if len(numpy.shape(list_figures)) != 1:\n        flat_list = [item for sublist in list_figures for item in sublist]\n        opensignals_style(flat_list)\n    else:\n        opensignals_style(list_figures)\n\n    # Present the generated plots.\n    if show_plot is True:\n        show(list_figures[-1])\n        #HTML('<iframe width=100% height=350 src=\"generated_plots/' + file_name + '\"></iframe>')\n\n    return list_figures[-1]", "response": "This function generates a graphical plot of the informational band of a physiological signal."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_low_pass_filter_response(show_plot=False, file_name=None):\n\n    # Generation of the HTML file where the plot will be stored.\n    #file_name = _generate_bokeh_file(file_name)\n\n    # Frequency list.\n    freqs = numpy.linspace(1, 1200, 100000)\n    cutoff_freq = 40\n\n    # Generation of filter response.\n    gain_functions = []\n    legend_strs = []\n    for order in range(1, 7):\n        gain = 20*numpy.log10(1 / (numpy.sqrt(1 + (freqs / cutoff_freq)**(2*order))))\n\n        # Storage of the determined gain values.\n        gain_functions.append(gain)\n        if order == 1:\n            legend_strs.append(\"1st order filter\")\n        elif order == 2:\n            legend_strs.append(\"2nd order filter\")\n        elif order == 3:\n            legend_strs.append(\"3rd order filter\")\n        else:\n            legend_strs.append(str(order) + \"th order filter\")\n\n\n    # Generation of a Bokeh figure with the opensignals style.\n    fig_list = plot([freqs / cutoff_freq]*len(gain_functions), gain_functions, legend=legend_strs,\n                    title=\"Filter Response\", x_axis_label=\"Normalized Frequency\",\n                    y_axis_label=\"Gain (dB)\", x_axis_type=\"log\", x_range=(0.1, 40),\n                    y_range=(-120, 5), show_plot=True, get_fig_list=True)\n\n    # Inclusion of a colored region showing the ideal behaviour.\n    color=opensignals_color_pallet()\n    box_annotation = BoxAnnotation(left=0.1, right=1, top=0, bottom=-120,\n                                   fill_color=color,\n                                   fill_alpha=0.3)\n    fig_list[0].circle([-100], [0], fill_color=color, fill_alpha=0.3, legend=\"Ideal Filter Response\")\n    fig_list[0].add_layout(box_annotation)\n\n    # Show figure.\n    if show_plot is True:\n        show(fig_list[0])", "response": "Generates a single low - pass filter response for a single Bokeh component."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_resp_slow(signal, rect_signal, sample_rate):\n\n    signal = numpy.array(signal) - numpy.average(signal)\n    rect_signal = numpy.array(rect_signal)\n    time = numpy.linspace(0, len(signal) / sample_rate, len(signal))\n\n    # Inhalation and Exhalation time segments.\n    # [Signal Binarisation]\n    rect_signal_rev = rect_signal - numpy.average(rect_signal)\n    inhal_segments = numpy.where(rect_signal_rev >= 0)[0]\n    exhal_segments = numpy.where(rect_signal_rev < 0)[0]\n    rect_signal_rev[inhal_segments] = numpy.max(rect_signal_rev)\n    rect_signal_rev[exhal_segments] = numpy.min(rect_signal_rev)\n\n    # [Signal Differentiation]\n    diff_rect_signal = numpy.diff(rect_signal_rev)\n    inhal_begin = numpy.where(diff_rect_signal > 0)[0]\n    inhal_end = numpy.where(diff_rect_signal < 0)[0]\n    exhal_begin = inhal_end\n    exhal_end = inhal_begin[1:]\n\n    # Generation of a Bokeh figure where data will be plotted.\n    plot_aux = plot(list([0]), list([0]), showPlot=False)[0]\n\n    # Edition of Bokeh figure (title, axes labels...)\n    # [title]\n    title = Title()\n    title.text = \"RIP Signal with slow cycles\"\n    plot_aux.title = title\n\n    # [plot]\n    plot_aux.line(time, signal, **opensignals_kwargs(\"line\"))\n    inhal_color = opensignals_color_pallet()\n    exhal_color = opensignals_color_pallet()\n    for inhal_exhal in range(0, len(inhal_begin)):\n        if inhal_exhal == 0:\n            legend = [\"Inhalation\", \"Exhalation\"]\n        else:\n            legend = [None, None]\n\n        plot_aux.line(time[inhal_begin[inhal_exhal]:inhal_end[inhal_exhal]],\n                  rect_signal_rev[inhal_begin[inhal_exhal]:inhal_end[inhal_exhal]],\n                  line_width=2, line_color=inhal_color, legend=legend[0])\n\n        if inhal_exhal != len(inhal_begin) - 1:\n            plot_aux.line(time[exhal_begin[inhal_exhal]:exhal_end[inhal_exhal]],\n                      rect_signal_rev[exhal_begin[inhal_exhal]:exhal_end[inhal_exhal]],\n                      line_width=2, line_color=exhal_color, legend=legend[1])\n        else:\n            plot_aux.line(time[exhal_begin[inhal_exhal]:], rect_signal_rev[exhal_begin[inhal_exhal]:],\n                      line_width=2, line_color=exhal_color, legend=legend[1])\n\n    # [axes labels]\n    plot_aux.xaxis.axis_label = \"Time (s)\"\n    plot_aux.yaxis.axis_label = \"Raw Data (without DC component)\"\n\n    show(plot_aux)", "response": "This function generates a Bokeh figure that represents the slow respiration cycle of the RIP signal and rectangular signal."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction design to generate a Bokeh figure containing the evolution of RIP signal and rectangular signal that has the first derivative of the RIP signal.", "response": "def plot_resp_diff(signal, rect_signal, sample_rate):\n    \"\"\"\n    Function design to generate a Bokeh figure containing the evolution of RIP signal, when\n    respiration was suspended for a long period, the rectangular signal that defines the\n    stages of inhalation and exhalation and the first derivative of the RIP signal.\n\n    Applied in the Notebook \"Particularities of Inductive Respiration (RIP) Sensor \".\n\n    ----------\n    Parameters\n    ----------\n    signal : list\n        List with the acquired RIP signal.\n\n    rect_signal : list\n        Data samples of the rectangular signal that identifies inhalation and exhalation\n        segments.\n\n    sample_rate : int\n        Sampling rate of acquisition.\n    \"\"\"\n\n    signal = numpy.array(signal) - numpy.average(signal)\n    rect_signal = numpy.array(rect_signal)\n    time = numpy.linspace(0, len(signal) / sample_rate, len(signal))\n    signal_diff = numpy.diff(signal)\n\n    # Inhalation and Exhalation time segments.\n    # [Signal Binarization]\n    rect_signal_rev = rect_signal - numpy.average(rect_signal)\n    inhal_segments = numpy.where(rect_signal_rev >= 0)[0]\n    exhal_segments = numpy.where(rect_signal_rev < 0)[0]\n    rect_signal_rev[inhal_segments] = numpy.max(rect_signal_rev)\n    rect_signal_rev[exhal_segments] = numpy.min(rect_signal_rev)\n\n    # Normalized Data.\n    norm_signal = signal / numpy.max(signal)\n    norm_rect_signal = rect_signal_rev / numpy.max(rect_signal_rev)\n    norm_signal_diff = signal_diff / numpy.max(signal_diff)\n\n    # Smoothed Data.\n    smooth_diff = smooth(signal_diff, int(sample_rate / 10))\n    smooth_norm_diff = smooth(norm_signal_diff, int(sample_rate / 10))\n\n    # Scaled Rectangular Signal.\n    scaled_rect_signal = (rect_signal_rev * numpy.max(smooth_diff)) / numpy.max(rect_signal_rev)\n\n    # [Signal Differentiation]\n    diff_rect_signal = numpy.diff(rect_signal_rev)\n    inhal_begin = numpy.where(diff_rect_signal > 0)[0]\n    inhal_end = numpy.where(diff_rect_signal < 0)[0]\n    exhal_begin = inhal_end\n    exhal_end = inhal_begin[1:]\n\n    # Generation of a Bokeh figure where data will be plotted.\n    figure_list = plot([list([0]), list([0]), list([0])],\n                                    [list([0]), list([0]), list([0])], gridPlot=True, gridLines=3,\n                                    gridColumns=1, showPlot=False)\n\n    # Edition of Bokeh figure (title, axes labels...)\n    # [Top Figure]\n    title = Title()\n    title.text = \"RIP Signal and Respiration Cycles\"\n    figure_list[0].title = title\n\n    figure_list[0].line(time, signal, **opensignals_kwargs(\"line\"))\n\n    # [Plot of inhalation and exhalation segments]\n    _inhal_exhal_segments(figure_list[0], list(time), list(rect_signal_rev), inhal_begin, inhal_end,\n                          exhal_begin, exhal_end)\n    figure_list[0].yaxis.axis_label = \"Raw Data (without DC component)\"\n\n    # [Middle Figure]\n    title = Title()\n    title.text = \"1st Derivative of RIP Signal and Respiration Cycles\"\n    figure_list[1].title = title\n\n    figure_list[1].line(time[1:], smooth_diff, **opensignals_kwargs(\"line\"))\n\n    # [Plot of inhalation and exhalation segments]\n    _inhal_exhal_segments(figure_list[1], list(time), list(scaled_rect_signal), inhal_begin,\n                          inhal_end, exhal_begin, exhal_end)\n    figure_list[1].yaxis.axis_label = \"Raw Differential Data\"\n\n    # [Bottom Figure]\n    title = Title()\n    title.text = \"RIP Signal and 1st Derivative (Normalized)\"\n    figure_list[2].title = title\n\n    figure_list[2].line(time, norm_signal, **opensignals_kwargs(\"line\"))\n    figure_list[2].line(time[1:], smooth_norm_diff, legend=\"RIP 1st Derivative\", **opensignals_kwargs(\"line\"))\n\n    # [Plot of inhalation and exhalation segments]\n    _inhal_exhal_segments(figure_list[2], list(time), list(norm_rect_signal), inhal_begin,\n                          inhal_end, exhal_begin, exhal_end)\n    figure_list[2].yaxis.axis_label = \"Normalized Data\"\n    figure_list[2].xaxis.axis_label = \"Time (s)\"\n\n    grid_plot_ref = gridplot([[figure_list[0]], [figure_list[1]], [figure_list[2]]],\n                             **opensignals_kwargs(\"gridplot\"))\n\n    show(grid_plot_ref)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download(link, out):\n\n    # [Source: https://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3]\n    r = requests.get(link)\n    with open(out, 'wb') as outfile:\n        outfile.write(r.content)", "response": "Downloads a file from a web site."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _inhal_exhal_segments(fig, time, signal, inhal_begin, inhal_end, exhal_begin, exhal_end):\n\n    inhal_color = opensignals_color_pallet()\n    exhal_color = opensignals_color_pallet()\n    for inhal_exhal in range(0, len(inhal_begin)):\n        if inhal_exhal == 0:\n            legend = [\"Respiration Suspension\", \"Normal Breath\"]\n        else:\n            legend = [None, None]\n\n        fig.line(time[inhal_begin[inhal_exhal]:inhal_end[inhal_exhal]],\n                 signal[inhal_begin[inhal_exhal]:inhal_end[inhal_exhal]], line_width=2,\n                 line_color=inhal_color, legend=legend[0])\n\n        if inhal_exhal != len(inhal_begin) - 1:\n            fig.line(time[exhal_begin[inhal_exhal]:exhal_end[inhal_exhal]],\n                     signal[exhal_begin[inhal_exhal]:exhal_end[inhal_exhal]], line_width=2,\n                     line_color=exhal_color, legend=legend[1])\n            if inhal_exhal == 0:\n                fig.line(time[:inhal_begin[inhal_exhal]], signal[:inhal_begin[inhal_exhal]],\n                         line_width=2, line_color=exhal_color, legend=legend[1])\n        else:\n            fig.line(time[exhal_begin[inhal_exhal]:], signal[exhal_begin[inhal_exhal]:],\n                     line_width=2, line_color=exhal_color, legend=legend[1])", "response": "Plots the inhalation and exhalation segments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _boolrelextrema(data, comparator,\n                  axis=0, order=1, mode='clip'):\n    \"\"\"\n    Calculate the relative extrema of `data`.\n\n    Relative extrema are calculated by finding locations where\n    ``comparator(data[n], data[n+1:n+order+1])`` is True.\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative extrema.\n    comparator : callable\n        Function to use to compare two data points.\n        Should take 2 numbers as arguments.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n,n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.  'wrap' (wrap around) or\n        'clip' (treat overflow as the same as the last (or first) element).\n        Default 'clip'.  See numpy.take\n\n    Returns\n    -------\n    extrema : ndarray\n        Indices of the extrema, as boolean array of same shape as data.\n        True for an extrema, False else.\n\n    See also\n    --------\n    argrelmax, argrelmin\n\n    Examples\n    --------\n    array([False, False,  True, False, False], dtype=bool)\n\n    \"\"\"\n    if((int(order) != order) or (order < 1)):\n        raise ValueError('Order must be an int >= 1')\n\n    datalen = data.shape[axis]\n    locs = np.arange(0, datalen)\n\n    results = np.ones(data.shape, dtype=bool)\n    main = data.take(locs, axis=axis, mode=mode)\n    for shift in iter(range(1, order + 1)):\n        plus = data.take(locs + shift, axis=axis, mode=mode)\n        minus = data.take(locs - shift, axis=axis, mode=mode)\n        results &= comparator(main, plus)\n        results &= comparator(main, minus)\n        if(~results.any()):\n            return results\n    return results", "response": "Calculate the relative extrema of data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the relative minima of data.", "response": "def argrelmin(data, axis=0, order=1, mode='clip'):\n    \"\"\"\n    Calculate the relative minima of `data`.\n\n    .. versionadded:: 0.11.0\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative minima.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n, n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.\n        Available options are 'wrap' (wrap around) or 'clip' (treat overflow\n        as the same as the last (or first) element).\n        Default 'clip'. See numpy.take\n\n    Returns\n    -------\n    extrema : ndarray\n        Indices of the minima, as an array of integers.\n\n    See also\n    --------\n    argrelextrema, argrelmax\n\n    Notes\n    -----\n    This function uses `argrelextrema` with np.less as comparator.\n\n    \"\"\"\n    return argrelextrema(data, np.less, axis, order, mode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef argrelmax(data, axis=0, order=1, mode='clip'):\n    return argrelextrema(data, np.greater, axis, order, mode)", "response": "Calculate the relative maxima of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the relative extrema of data.", "response": "def argrelextrema(data, comparator, axis=0, order=1, mode='clip'):\n    \"\"\"\n    Calculate the relative extrema of `data`.\n\n    .. versionadded:: 0.11.0\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative extrema.\n    comparator : callable\n        Function to use to compare two data points.\n        Should take 2 numbers as arguments.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n, n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.  'wrap' (wrap around) or\n        'clip' (treat overflow as the same as the last (or first) element).\n        Default is 'clip'.  See `numpy.take`.\n\n    Returns\n    -------\n    extrema : ndarray\n        Indices of the extrema, as an array of integers (same format as\n        np.argmin, np.argmax).\n\n    See also\n    --------\n    argrelmin, argrelmax\n\n    \"\"\"\n    results = _boolrelextrema(data, comparator,\n                              axis, order, mode)\n    if ~results.any():\n        return (np.array([]),) * 2\n    else:\n        return np.where(results)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an array with all the peaks of the signal ('peaks') and a distance value ('min_distance') and the signal, by argument, this function erases all the unnecessary peaks and returns an array with only the maximum peak for each period of the signal (the period is given by the min_distance). Parameters ---------- signal: array-like the original signal. peaks: array-like the peaks to filter. min_distance: int the distance value to exclude the unnecessary peaks. Returns ------- fp: array-like the new peaks, after filtering just the maximum peak per period. See also: clean_near_events()", "response": "def clean_near_peaks(signal, peaks_, min_distance):\n    \"\"\" Given an array with all the peaks of the signal ('peaks') and a\n    distance value ('min_distance') and the signal, by argument, this function\n    erases all the unnecessary peaks and returns an array with only the maximum\n    peak for each period of the signal (the period is given by the\n    min_distance).\n\n    Parameters\n    ----------\n    signal: array-like\n      the original signal.\n    peaks: array-like\n      the peaks to filter.\n    min_distance: int\n      the distance value to exclude the unnecessary peaks.\n\n    Returns\n    -------\n    fp: array-like\n      the new peaks, after filtering just the maximum peak per period.\n\n    See also: clean_near_events()\n    \"\"\"\n\n    #order all peaks\n    \n    ars = argsort(signal[peaks_])\n    \n    pp = peaks(ars)\n\n    fp = []\n\n    #clean near peaks\n    while len(pp) > 0:\n        fp += [pp[-1]]\n        pp = pp[abs(pp - pp[-1]) > min_distance]\n\n    return sort(array(fp))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive an array with some specific points of the signal and a distance value, this function erases all the surplus points and returns an array with only one point (the first point) per distance samples values Parameters ---------- points: array-like the events to filter. min_distance: int the distance value to exclude the unnecessary events. Returns ------- fp: array-like the new events, after filtering just one per period. Example ------- >>> clean_near_events([1,3,5,50,65,68,83,88],10) array([ 1, 50, 65, 83]) See also: clean_near_peaks()", "response": "def clean_near_events(points, min_distance):\n    \"\"\" Given an array with some specific points of the signal and a distance\n    value, this function erases all the surplus points and returns an array\n    with only one point (the first point) per distance samples values\n\n    Parameters\n    ----------\n    points: array-like\n      the events to filter.\n    min_distance: int\n      the distance value to exclude the unnecessary events.\n\n    Returns\n    -------\n    fp: array-like\n      the new events, after filtering just one per period.\n    Example\n    -------\n    >>> clean_near_events([1,3,5,50,65,68,83,88],10)\n    array([ 1, 50, 65, 83])\n\n    See also: clean_near_peaks()\n    \"\"\"\n\n    fp = []\n    points = array(points)\n    while len(points) > 0:\n        fp += [points[0]]\n        points = points[abs(points - points[0]) > min_distance]\n\n    return array(fp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve and return a KE - chain project.", "response": "def get_project(url=None, username=None, password=None, token=None, scope=None, scope_id=None,\n                env_filename=None, status=ScopeStatus.ACTIVE):\n    \"\"\"\n    Retrieve and return the KE-chain project to be used throughout an app.\n\n    This helper is made to bootstrap a pykechain enabled python script or an jupyter notebook with the correct\n    project (technically this is a `pykechain.models.Scope` model).\n\n    When no parameters are passed in this function, it will try to retrieve `url`, `token`, `scope` (or `scope_id`)\n    from the environment variables or a neatly placed '.env' file.\n\n    when the environment variable KECHAIN_FORCE_ENV_USE is set to true, (or ok, on, 1, yes) then the use of\n    environmentvariables for the retrieval of the scope are enforced. The following environment variables can be set::\n\n        KECHAIN_URL           - full url of KE-chain where to connect to eg: 'https://<some>.ke-chain.com'\n        KECHAIN_TOKEN         - authentication token for the KE-chain user provided from KE-chain user account control\n        KECHAIN_USERNAME      - the username for the credentials\n        KECHAIN_PASSWORD      - the password for the credentials\n        KECHAIN_SCOPE         - the name of the project / scope. Should be unique, otherwise use scope_id\n        KECHAIN_SCOPE_ID      - the UUID of the project / scope.\n        KECHAIN_FORCE_ENV_USE - set to 'true', '1', 'ok', or 'yes' to always use the environment variables.\n        KECHAIN_SCOPE_STATUS  - the status of the Scope to retrieve, defaults to None to retrieve all scopes\n\n    .. versionadded:: 1.12\n\n    :param url: (optional) url of KE-chain\n    :type url: basestring or None\n    :param username: (optional) username for authentication (together with password, if not token)\n    :type username: basestring or None\n    :param password: (optional) password for username/password authentication (together with username, if not token)\n    :type password: basestring or None\n    :param token: (optional) token for authentication (if not username/password)\n    :type token: basestring or None\n    :param scope: (optional) name of the scope to retrieve from KE-chain.\n    :type scope: basestring or None\n    :param scope_id: (optional) UUID of the scope to retrieve and return from KE-chain\n    :type scope_id: basestring or None\n    :param env_filename: (optional) name of the environment filename to bootstrap the Client\n    :type env_filename: basestring or None\n    :param status: (optional) status of the scope to retrieve, defaults to :attr:`enums.Scopestatus.ACTIVE`\n    :type status: basestring or None\n    :return: pykechain.models.Scope\n    :raises NotFoundError: If the scope could not be found\n    :raises ClientError: If the client connection to KE-chain was unsuccessful\n    :raises APIError: If other Errors occur to retrieve the scope\n\n    Example\n    -------\n    An example with parameters provided\n\n    >>> from pykechain import get_project\n    >>> project = get_project(url='http://localhost:8000',\n    ...     username='foo', password='bar', scope='1st!')\n    >>> print(project.name)\n    1st\n\n    An example with a .env file on disk::\n\n        # This is an .env file on disk.\n        KECHAIN_TOKEN=bd9377793f7e74a29dbb11fce969\n        KECHAIN_URL=http://localhost:8080\n        KECHAIN_SCOPE_ID=c9f0-228e-4d3a-9dc0-ec5a75d7\n\n    >>> project = get_project(env_filename='/path/to/.env')\n    >>> project.id\n    c9f0-228e-4d3a-9dc0-ec5a75d7\n\n    An example for get_project that will extract all from the environment variables\n\n    >>> env_vars = os.environ\n    >>> env_vars.get('KECHAIN_TOKEN')\n    bd9377793f7e74a29dbb11fce969\n    >>> env_vars.get('KECHAIN_URL')\n    http://localhost:8080\n    >>> env_vars.get('KECHAIN_SCOPE')\n    Bike Project\n    >>> project = get_project()\n    >>> project.name\n    Bike Project\n    \"\"\"\n    if env.bool(kecenv.KECHAIN_FORCE_ENV_USE, default=False):\n        if not os.getenv(kecenv.KECHAIN_URL):\n            raise ClientError(\n                \"Error: KECHAIN_URL should be provided as environment variable (use of env vars is enforced)\")\n        if not (os.getenv(kecenv.KECHAIN_TOKEN) or\n                (os.getenv(kecenv.KECHAIN_PASSWORD) and os.getenv(kecenv.KECHAIN_PASSWORD))):\n            raise ClientError(\"Error: KECHAIN_TOKEN or KECHAIN_USERNAME and KECHAIN_PASSWORD should be provided as \"\n                              \"environment variable(s) (use of env vars is enforced)\")\n        if not (os.getenv(kecenv.KECHAIN_SCOPE) or os.getenv(kecenv.KECHAIN_SCOPE_ID)):\n            raise ClientError(\"Error: KECHAIN_SCOPE or KECHAIN_SCOPE_ID should be provided as environment variable \"\n                              \"(use of env vars is enforced)\")\n\n    if env.bool(kecenv.KECHAIN_FORCE_ENV_USE, default=False) or \\\n            not any((url, username, password, token, scope, scope_id)):\n        client = Client.from_env(env_filename=env_filename)\n        scope_id = env(kecenv.KECHAIN_SCOPE_ID, default=None)\n        scope = env(kecenv.KECHAIN_SCOPE, default=None)\n        status = env(kecenv.KECHAIN_SCOPE_STATUS, default=None)\n    elif (url and ((username and password) or (token)) and (scope or scope_id)) and \\\n            not env.bool(kecenv.KECHAIN_FORCE_ENV_USE, default=False):\n        client = Client(url=url)\n        client.login(username=username, password=password, token=token)\n    else:\n        raise ClientError(\"Error: insufficient arguments to connect to KE-chain. \"\n                          \"See documentation of `pykechain.get_project()`\")\n\n    if scope_id:\n        return client.scope(pk=scope_id, status=status)\n    else:\n        return client.scope(name=scope, status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npopulates the object with the data from the given byte array.", "response": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        :return: None\n        \"\"\"\n        assert isinstance(data, bytes)\n        self.needs_high_priority(priority)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 3)\n        self.set_attributes(priority, address, rtr)\n        self.closed = self.byte_to_channels(data[0])\n        self.opened = self.byte_to_channels(data[1])\n        self.closed_long = self.byte_to_channels(data[2])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a JSON string representation of the current object.", "response": "def to_json(self):\n        \"\"\"\n        :return: str\n        \"\"\"\n        json_dict = self.to_json_basic()\n        json_dict['closed_channels'] = self.closed\n        json_dict['opened_channels'] = self.opened\n        json_dict['closed_long_channels'] = self.closed_long\n        return json.dumps(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef data_to_binary(self):\n        return bytes([\n            COMMAND_CODE,\n            self.channels_to_byte(self.closed),\n            self.channels_to_byte(self.opened),\n            self.channels_to_byte(self.closed_long)\n        ])", "response": "Converts the data to binary format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _rebuild_key_ids(self):\n        self._key_ids = collections.defaultdict(list)\n        for i, x in enumerate(self._pairs):\n            self._key_ids[x[0]].append(i)", "response": "Rebuild the internal key to index mapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nforces a given key value pair into a certain form.", "response": "def _conform_pair(self, pair):\n        \"\"\"Force a given key/value pair into a certain form.\n        \n        Override the _conform_key and _conform_value if you want to change\n        the mapping behaviour.\n        \n        \"\"\"\n        pair = tuple(pair)\n        if len(pair) != 2:\n            raise ValueError('MultiMap element must have length 2')\n        return (self._conform_key(pair[0]), self._conform_value(pair[1]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getall(self, key):\n        key = self._conform_key(key)\n        return [self._pairs[i][1] for i in self._key_ids[key]]", "response": "A list of all the values stored under this key. Returns an empty list if there are no values under this key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating over all the non - duplicate keys and their values. Only yields the first key of duplicates.", "response": "def iteritems(self):\n        \"\"\"Iterator across all the non-duplicate keys and their values.\n        \n        Only yields the first key of duplicates.\n        \n        \"\"\"\n        keys_yielded = set()\n        for k, v in self._pairs:\n            if k not in keys_yielded:\n                keys_yielded.add(k)\n                yield k, v"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the pairs identified by the given indices into _pairs.", "response": "def _remove_pairs(self, ids_to_remove):\n        \"\"\"Remove the pairs identified by the given indices into _pairs.\n        \n        Removes the pair, and updates the _key_ids mapping to be accurate.\n        Removing the ids from the _key_ids is your own responsibility.\n        \n        Params:\n            ids_to_remove -- The indices to remove. MUST be sorted.\n        \n        \"\"\"\n        # Remove them.\n        for i in reversed(ids_to_remove):\n            del self._pairs[i]\n        \n        # We use the bisect to tell us how many spots the given index is\n        # shifting up in the list.\n        for ids in self._key_ids.itervalues():\n            for i, id in enumerate(ids):\n                ids[i] -= bisect(ids_to_remove, id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting some new pairs and keep the _key_ids updated.", "response": "def _insert_pairs(self, ids_and_pairs):\n        \"\"\"Insert some new pairs, and keep the _key_ids updated.\n        \n        Params:\n            ids_and_pairs -- A list of (index, (key, value)) tuples.\n        \n        \"\"\"\n        ids_to_insert = [x[0] for x in ids_and_pairs]\n        \n        # We use the bisect to tell us how many spots the given index is\n        # shifting up in the list.\n        for ids in self._key_ids.itervalues():\n            for i, id in enumerate(ids):\n                ids[i] += bisect(ids_to_insert, id)\n        \n        # Do the actual insertion\n        for i, pair in ids_and_pairs:\n            self._pairs.insert(i, pair)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setall(self, key, values):\n        key = self._conform_key(key)\n        values = [self._conform_value(x) for x in values]\n        ids = self._key_ids[key][:]\n        while ids and values:\n            id    = ids.pop(0)\n            value = values.pop(0)\n            self._pairs[id] = (key, value)\n        if ids:\n            self._key_ids[key] = self._key_ids[key][:-len(ids)]\n            self._remove_pairs(ids)\n        for value in values:\n            self._key_ids[key].append(len(self._pairs))\n            self._pairs.append((key, value))", "response": "Set all the values for a given key to a list of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort(self, *args, **kwargs):\n        self._pairs.sort(*args, **kwargs)\n        self._rebuild_key_ids()", "response": "Sort the MultiMap by key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving first of given key and return corresponding value.", "response": "def popone(self, key, *default):\n        \"\"\"Remove first of given key and return corresponding value.\n        \n        If key is not found, default is returned if given.\n        \n        >>> m = MutableMultiMap([('a', 1), ('b', 2), ('b', 3), ('c', 4)])\n        >>> m.popone('b')\n        2\n        >>> m.items()\n        [('a', 1), ('b', 3), ('c', 4)]\n        >>> m.popone('b')\n        3\n        >>> m.popone('b')\n        Traceback (most recent call last):\n        ...\n        KeyError: 'b'\n        >>> m.popone('b', 'default')\n        'default'\n        \n        \"\"\"\n        try:\n            value = self[key]\n        except KeyError:\n            if default:\n                return default[0]\n            raise\n        \n        # Delete this one.\n        self._remove_pairs([self._key_ids[self._conform_key(key)].pop(0)])\n        \n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef popall(self, key):\n        values = self.getall(key)\n        try:\n            del self[key]\n        except KeyError:\n            pass\n        return values", "response": "Remove specified key and return all corresponding values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the members of the team.", "response": "def members(self, role=None):\n        \"\"\"Members of the team.\n\n        You may provide the role in the team, to retrieve only the teammmber with that role. Normally there is a\n        single owner, that has administration rights of the team. Normal team members do not have any rights to\n        administer the team itself such as altering the teamname, team image and team members. Administrators do\n        have the right to administer the the team members.\n\n        :param role: (optional) member belonging to a role :class:`pykechain.enums.TeamRoles` to return.\n        :type role: basestring or None\n        :raises IllegalArgumentError: when providing incorrect roles\n        :return: list of dictionaries with members (pk, username, role, email)\n\n        Example\n        -------\n        >>> my_team = client.team(name='My own team')\n        >>> my_team.members()\n        [{\"pk\":1, \"username\"=\"first user\", \"role\"=\"OWNER\", \"email\":\"email@address.com\"}, ...]\n        \"\"\"\n        if role and role not in TeamRoles.values():\n            raise IllegalArgumentError(\"role should be one of `TeamRoles` {}, got '{}'\".format(TeamRoles.values(),\n                                                                                               role))\n\n        member_list = self._json_data.get('members')\n        if role:\n            return [teammember for teammember in member_list if teammember.get('role') == role]\n        else:\n            return member_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding members to a team.", "response": "def add_members(self, users=None, role=TeamRoles.MEMBER):\n        \"\"\"Members to add to a team.\n\n        :param members: list of members, either `User` objects or usernames\n        :type members: List of `User` or List of pk\n        :param role: (optional) role of the users to add (default `TeamRoles.MEMBER`)\n        :type role: basestring\n        :raises IllegalArgumentError: when providing incorrect roles\n\n        Example\n        -------\n        >>> my_team = client.team(name='My own team')\n        >>> other_user = client.users(name='That other person')\n        >>> myself = client.users(name='myself')\n        >>> my_team.add_members([myself], role=TeamRoles.MANAGER)\n        >>> my_team.add_members([other_user], role=TeamRoles.MEMBER)\n\n        \"\"\"\n        if role and role not in TeamRoles.values():\n            raise IllegalArgumentError(\"role should be one of `TeamRoles` {}, got '{}'\".format(TeamRoles.values(),\n                                                                                               role))\n\n        if not users or not isinstance(users, (list, tuple, set)):\n            raise IllegalArgumentError(\"users should be a list of user_ids or `User` objects, got '{}'\".\n                                       format(users))\n\n        update_dict = dict(role=role)\n\n        if all(isinstance(user, int) for user in users):\n            update_dict['users'] = users\n        elif all(isinstance(user, User) for user in users):\n            update_dict['users'] = [user.id for user in users]\n        else:\n            raise IllegalArgumentError(\"All users should be a list of user_ids or `User` objects, got '{}'\".\n                                       format(users))\n\n        self._update('team_add_members', team_id=self.id, update_dict=update_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove members from a team.", "response": "def remove_members(self, users=None):\n        \"\"\"Members to add to a team.\n\n        :param members: list of members, either `User` objects or usernames\n        :type members: List of `User` or List of pk\n        :raises IllegalArgumentError: when providing incorrect roles\n\n\n        Example\n        -------\n        >>> my_team = client.team(name='My own team')\n        >>> other_user = client.users(name='That other person')\n        >>> my_team.remove_members([other_user]R)\n\n        \"\"\"\n        if not users or not isinstance(users, (list, tuple, set)):\n            raise IllegalArgumentError(\"Member should be a list of user_ids or `User` objects, got '{}'\".\n                                       format(users))\n\n        update_dict = dict()\n\n        if all(isinstance(user, int) for user in users):\n            update_dict['users'] = users\n        elif all(isinstance(user, User) for user in users):\n            update_dict['users'] = [user.id for user in users]\n        else:\n            raise IllegalArgumentError(\"All users should be a list of user_ids or `User` objects, got '{}'\".\n                                       format(users))\n\n        self._update('team_remove_members',\n                     update_dict=update_dict,\n                     team_id=self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scopes(self, **kwargs):\n        return self._client.scopes(team=self.id, **kwargs)", "response": "Returns a list of all scopes associated to the team."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert_hash(path: Path, content: Union[str, bytes], *, hash_length=7, hash_algorithm=hashlib.md5):\n    if isinstance(content, str):\n        content = content.encode()\n    hash_ = hash_algorithm(content).hexdigest()[:hash_length]\n    if '.' in path.name:\n        new_name = re.sub(r'\\.', f'.{hash_}.', path.name, count=1)\n    else:\n        new_name = f'{path.name}.{hash_}'\n    return path.with_name(new_name)", "response": "Insert a hash based on the content into the path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide a sorted list of options.", "response": "def options(cls):\n        \"\"\"Provide a sorted list of options.\"\"\"\n        return sorted((value, name) for (name, value) in cls.__dict__.items() if not name.startswith('__'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef navbar(self):\n        window = BaseWindow(self.selenium, self.selenium.current_window_handle)\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            el = self.selenium.find_element(*self._nav_bar_locator)\n            return NavBar(window, el)", "response": "Provide access to the NavBar object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef notification(self):\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            try:\n                root = self.selenium.find_element(*self._notification_locator)\n                return BaseNotification.create(self, root)\n            except NoSuchElementException:\n                pass\n            try:\n                notifications = self.selenium.find_elements(\n                    *self._app_menu_notification_locator\n                )\n                root = next(n for n in notifications if n.is_displayed())\n                return BaseNotification.create(self, root)\n            except StopIteration:\n                pass\n        return None", "response": "Provide access to the currently displayed notification."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for the specified notification to be displayed.", "response": "def wait_for_notification(self, notification_class=BaseNotification):\n        \"\"\"Wait for the specified notification to be displayed.\n\n        Args:\n            notification_class (:py:class:`BaseNotification`, optional):\n                The notification class to wait for. If `None` is specified it\n                will wait for any notification to be closed. Defaults to\n                `BaseNotification`.\n\n        Returns:\n            :py:class:`BaseNotification`: Firefox notification.\n\n        \"\"\"\n        if notification_class:\n            if notification_class is BaseNotification:\n                message = \"No notification was shown.\"\n            else:\n                message = \"{0} was not shown.\".format(notification_class.__name__)\n            self.wait.until(\n                lambda _: isinstance(self.notification, notification_class),\n                message=message,\n            )\n            return self.notification\n        else:\n            self.wait.until(\n                lambda _: self.notification is None,\n                message=\"Unexpected notification shown.\",\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a new browser window.", "response": "def open_window(self, private=False):\n        \"\"\"Open a new browser window.\n\n        Args:\n            private (bool): Optional parameter to open a private browsing\n                window. Defaults to False.\n\n        Returns:\n            :py:class:`BrowserWindow`: Opened window.\n\n        \"\"\"\n        handles_before = self.selenium.window_handles\n        self.switch_to()\n\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            # Opens private or non-private window\n            self.selenium.find_element(*self._file_menu_button_locator).click()\n            if private:\n                self.selenium.find_element(\n                    *self._file_menu_private_window_locator\n                ).click()\n            else:\n                self.selenium.find_element(\n                    *self._file_menu_new_window_button_locator\n                ).click()\n\n        return self.wait.until(\n            expected.new_browser_window_is_opened(self.selenium, handles_before),\n            message=\"No new browser window opened\",\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_serializable_dict(self, attrs_to_serialize=None,\n                             rels_to_expand=None,\n                             rels_to_serialize=None,\n                             key_modifications=None):\n        \"\"\"\n        An alias for `todict`\n        \"\"\"\n        return self.todict(\n            attrs_to_serialize=attrs_to_serialize,\n            rels_to_expand=rels_to_expand, rels_to_serialize=rels_to_serialize,\n            key_modifications=key_modifications)", "response": "Converts the object to a serializable dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef todict_using_struct(self, dict_struct=None, dict_post_processors=None):\n        # It is important to assign the passed kwarg to a differently named variable.\n        # A dict is passed by reference and using the same kwarg here results in it\n        # getting mutated - causing unforeseen side effects\n        dict_struct_to_use = (\n            self._dict_struct_ if dict_struct is None\n            else dict_struct)\n        if dict_struct_to_use is None and self._autogenerate_dict_struct_if_none_:\n            dict_struct_to_use = self.autogenerated_dict_structure()\n        elif dict_struct.get(\"attrs\") is None:\n            dict_struct_to_use = {}\n            dict_struct_to_use[\"attrs\"] = self.autogenerated_dict_structure()[\"attrs\"]\n            if \"rels\" in dict_struct:\n                dict_struct_to_use[\"rels\"] = dict_struct.get(\"rels\")\n        result = self.serialize_attrs(*dict_struct_to_use.get('attrs', []))\n        for rel, rel_dict_struct in dict_struct_to_use.get('rels', {}).items():\n            rel_obj = getattr(self, rel) if hasattr(self, rel) else None\n            if rel_obj is not None:\n                if is_list_like(rel_obj):\n                    result[rel] = [i.todict_using_struct(dict_struct=rel_dict_struct)\n                                   if hasattr(i, 'todict_using_struct') else i\n                                   for i in rel_obj]\n                elif is_dict_like(rel_obj):\n                    result[rel] = {k: v.todict_using_struct(dict_struct=rel_dict_struct)\n                                   if hasattr(v, 'todict_using_struct') else v\n                                   for k, v in rel_obj.iteritems()}\n                else:\n                    result[rel] = rel_obj.todict_using_struct(\n                        dict_struct=rel_dict_struct) if hasattr(\n                        rel_obj, 'todict_using_struct') else rel_obj\n            else:\n                result[rel] = None\n        if isinstance(dict_post_processors, list):\n            for dict_post_processor in dict_post_processors:\n                if callable(dict_post_processor):\n                    result = dict_post_processor(result, self)\n        return result", "response": "This method takes a dict and returns a dictionary of all the related objects in the structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef todict(self, attrs_to_serialize=None,\n               rels_to_expand=None,\n               rels_to_serialize=None,\n               group_listrels_by=None,\n               key_modifications=None,\n               dict_struct=None,\n               dict_post_processors=None):\n\n        \"\"\"Converts an instance to a dictionary form\n\n        Args:\n\n\n            attrs_to_serialize (list of str):  The columns which should\n                be serialized as a part of the output dictionary\n\n            key_modifications (dict of str,str): A dictionary used to map\n                the display names of columns whose original name we want\n                to be modified in the json\n\n            rels_to_serialize (list of tuple of str):  A list of tuples. The\n                first element of the tuple is the relationship\n                that is to be serialized. The second element it the name of the\n                attribute in the related model, the value of which is to be used\n                as the representation\n\n            rels_to_expand (list of str): A list of relationships to expand.\n                You can specify nested relationships by placing dots.\n\n            group_listrels_by (dict of str, list of str): A dictionary\n                representing how to hierarchially group a list like relationship.\n                The relationship fields are the keys and the list of the attributes\n                based on which they are to be grouped are the values.\n\n\n        \"\"\"\n\n        # Never replace the following code by the (attrs = attrs or\n        # self._attrs_) idiom. Python considers empty list as false. So\n        # even if you pass an empty list, it will take self._x_ value. But\n        # we don't want that as the empty list is what we use to end\n        # the recursion\n        dict_struct = (\n            self._dict_struct_ if dict_struct is None\n            else dict_struct)\n        if dict_struct is None:\n            dict_struct = self.autogenerated_dict_structure()\n        if dict_struct is not None:\n            return self.todict_using_struct(\n                dict_struct=dict_struct,\n                dict_post_processors=dict_post_processors)\n        attrs_to_serialize = (\n            self._attrs_to_serialize_ if attrs_to_serialize is None\n            else attrs_to_serialize)\n        rels_to_serialize = (\n            self._rels_to_serialize_ if rels_to_serialize is None\n            else rels_to_serialize)\n        rels_to_expand = (\n            self._rels_to_expand_ if rels_to_expand is None\n            else rels_to_expand)\n        key_modifications = (\n            self._key_modifications_ if key_modifications is None\n            else key_modifications)\n        group_listrels_by = (\n            self._group_listrels_by_ if group_listrels_by is None\n            else group_listrels_by)\n        # Convert rels_to_expand to a dictionary\n        rels_to_expand_dict = {}\n        for rel in rels_to_expand:\n            partitioned_rels = rel.partition('.')\n            if partitioned_rels[0] not in rels_to_expand_dict:\n                rels_to_expand_dict[partitioned_rels[0]] = (\n                    [partitioned_rels[-1]] if partitioned_rels[-1]\n                    else [])\n            else:\n                if partitioned_rels[-1]:\n                    rels_to_expand_dict[partitioned_rels[0]].append(\n                        partitioned_rels[-1])\n\n        # # Convert grouplistrelsby to a dict\n        # group_listrels_dict = {}\n        # for rel_to_group, grouping_keys in group_listrels_by.iteritems():\n        #     partitioned_rel_to_group = rel_to_group.partition('.')\n        #     if partitioned_rel_to_group[0] not in group_listrels_dict:\n        #         group_listrels_dict[partitioned_rel_to_group[0]] = (\n        #             {partitioned_rel_to_group[-1]: grouping_keys}\n        #             if partitioned_rel_to_group[-1] else grouping_keys)\n        #     else:\n        #         if partitioned_rel_to_group[-1]:\n        #             group_listrels_dict[\n        #                 partitioned_rel_to_group[0]][\n        #                     partitioned_rel_to_group[-1]] = grouping_keys\n\n        # Serialize attrs\n        result = self.serialize_attrs(*attrs_to_serialize)\n\n        # Serialize rels\n        if len(rels_to_serialize) > 0:\n            for rel, id_attr in rels_to_serialize:\n                rel_obj = getattr(self, rel) if hasattr(self, rel) else None\n                if rel_obj is not None:\n                    if is_list_like(rel_obj):\n                        if (group_listrels_by is not None and\n                                rel in group_listrels_by):\n                            result[rel] = deep_group(\n                                rel_obj,\n                                attr_to_show=id_attr,\n                                keys=group_listrels_by[rel]\n                            )\n                        else:\n                            result[rel] = [getattr(item, id_attr)\n                                           for item in rel_obj if hasattr(item, id_attr)]\n                    elif is_dict_like(rel_obj):\n                        result[rel] = {k: getattr(v, id_attr)\n                                       for k, v in rel_obj.iteritems()\n                                       if hasattr(v, id_attr)}\n                    else:\n                        result[rel] = getattr(rel_obj, id_attr) if hasattr(\n                            rel_obj, id_attr) else None\n                else:\n                    result[rel] = None\n\n        # Expand some rels\n        for rel, child_rels in rels_to_expand_dict.iteritems():\n            rel_obj = getattr(self, rel) if hasattr(self, rel) else None\n            if rel_obj is not None:\n                if is_list_like(rel_obj):\n                    if (group_listrels_by is not None and\n                            rel in group_listrels_by):\n                        result[rel] = deep_group(\n                            rel_obj,\n                            keys=group_listrels_by[rel], serializer='todict',\n                            serializer_kwargs={'rels_to_expand': child_rels}\n                        )\n                    else:\n                        result[rel] = [i.todict(rels_to_expand=child_rels)\n                                       if hasattr(i, 'todict') else i\n                                       for i in rel_obj]\n                        # result[rel] = serialized_list(\n                        #     rel_obj, rels_to_expand=child_rels)\n                elif is_dict_like(rel_obj):\n                    result[rel] = {k: v.todict()\n                                   if hasattr(v, 'todict') else v\n                                   for k, v in rel_obj.iteritems()}\n                else:\n                    result[rel] = rel_obj.todict(\n                        rels_to_expand=child_rels) if hasattr(\n                        rel_obj, 'todict') else rel_obj\n\n        for key, mod_key in key_modifications.items():\n            if key in result:\n                result[mod_key] = result.pop(key)\n\n        if isinstance(dict_post_processors, list):\n            for dict_post_processor in dict_post_processors:\n                if callable(dict_post_processor):\n                    result = dict_post_processor(result, self)\n\n        return result", "response": "Converts an instance of a class to a dictionary form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize_attrs(self, *args):\n        # return dict([(a, getattr(self, a)) for a in args])\n        cls = type(self)\n        result = {}\n        # result = {\n        #     a: getattr(self, a)\n        #     for a in args\n        #     if hasattr(cls, a) and\n        #     a not in cls.attrs_forbidden_for_serialization()\n        # }\n        for a in args:\n            if hasattr(cls, a) and a not in cls.attrs_forbidden_for_serialization():\n                val = getattr(self, a)\n                if is_list_like(val):\n                    result[a] = list(val)\n                else:\n                    result[a] = val\n        return result", "response": "Converts and instance to a dictionary with only the specified\n        attributes as keys\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fundamental_frequency(s,FS):\n    # TODO: review fundamental frequency to guarantee that f0 exists \n    # suggestion peak level should be bigger \n    # TODO: explain code\n    \"\"\"Compute fundamental frequency along the specified axes.\n\n    Parameters\n    ----------\n    s: ndarray\n        input from which fundamental frequency is computed.\n    FS: int\n        sampling frequency    \n    Returns\n    -------\n    f0: int\n       its integer multiple best explain the content of the signal spectrum.\n    \"\"\"\n    \n    s = s - mean(s)\n    f, fs = plotfft(s, FS, doplot=False)\n    \n    #fs = smooth(fs, 50.0)\n  \n    fs = fs[1:int(len(fs) / 2)]\n    f = f[1:int(len(f) / 2)]\n    \n    cond = find(f > 0.5)[0]\n    \n    bp = bigPeaks(fs[cond:], 0)\n    \n    if bp==[]:\n        f0=0\n    else:\n        \n        bp = bp + cond\n        \n        f0 = f[min(bp)]\n    \n    return f0", "response": "Compute the fundamental frequency along the specified axes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef max_frequency (sig,FS):\n    \n    f, fs = plotfft(sig, FS, doplot=False)    \n    t = cumsum(fs)\n    \n    ind_mag = find (t>t[-1]*0.95)[0]\n    f_max=f[ind_mag]\n    return f_max", "response": "Compute max frequency along the specified axes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef median_frequency(sig,FS):\n    \n    f, fs = plotfft(sig, FS, doplot=False)    \n    t = cumsum(fs)\n    \n    ind_mag = find (t>t[-1]*0.50)[0]\n    f_median=f[ind_mag]\n    return f_median", "response": "Compute the median frequency along the specified axes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling a subcommand passing the args.", "response": "def call(subcommand, args):\n    \"\"\"Call a subcommand passing the args.\"\"\"\n    args['<napp>'] = parse_napps(args['<napp>'])\n    func = getattr(NAppsAPI, subcommand)\n    func(args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a napp_id in tuple with username napp name and version.", "response": "def parse_napp(napp_id):\n    \"\"\"Convert a napp_id in tuple with username, napp name and version.\n\n    Args:\n        napp_id: String with the form 'username/napp[:version]' (version is\n                  optional). If no version is found, it will be None.\n\n    Returns:\n        tuple: A tuple with (username, napp, version)\n\n    Raises:\n        KytosException: If a NApp has not the form _username/name_.\n\n    \"\"\"\n    # `napp_id` regex, composed by two mandatory parts (username, napp_name)\n    # and one optional (version).\n    # username and napp_name need to start with a letter, are composed of\n    # letters, numbers and uderscores and must have at least three characters.\n    # They are separated by a colon.\n    # version is optional and can take any format. Is is separated by a hyphen,\n    # if a version is defined.\n    regex = r'([a-zA-Z][a-zA-Z0-9_]{2,})/([a-zA-Z][a-zA-Z0-9_]{2,}):?(.+)?'\n    compiled_regex = re.compile(regex)\n\n    matched = compiled_regex.fullmatch(napp_id)\n\n    if not matched:\n        msg = '\"{}\" NApp has not the form username/napp_name[:version].'\n        raise KytosException(msg.format(napp_id))\n\n    return matched.groups()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef opensignals_hierarchy(root=None, update=False, clone=False):\n\n    if root is None:\n        root = os.getcwd()\n\n    categories = list(NOTEBOOK_KEYS.keys())\n\n    # ============================ Creation of the main directory ==================================\n    current_dir = root + \"/biosignalsnotebooks_environment\"\n    if not os.path.isdir(current_dir):\n        os.makedirs(current_dir)\n\n    # ================== Copy of 'images' 'styles' and 'signal_samples' folders ====================\n    package_path = os.path.abspath(__file__).split(os.path.basename(__file__))[0].replace(\"\\\\\", \"/\")\n    for var in [\"images\", \"styles\", \"signal_samples\"]:\n        if not os.path.isdir(root + \"/biosignalsnotebooks_environment/\" + var):\n            src = (package_path + \"notebook_files/osf_files/\" + var).replace(\"\\\\\", \"/\")\n            destination = current_dir + \"/\" + var\n            shutil.copytree(src, destination)\n        elif update is True:\n            shutil.rmtree(root + \"/biosignalsnotebooks_environment/\" + var)\n            src = (package_path + \"notebook_files/osf_files/\" + var).replace(\"\\\\\", \"/\")\n            destination = current_dir + \"/\" + var\n            shutil.copytree(src, destination)\n\n    # =========================== Generation of 'Categories' folder ================================\n    current_dir = root + \"/biosignalsnotebooks_environment/Categories\"\n    if not os.path.isdir(current_dir):\n        os.makedirs(current_dir)\n\n    for category in categories:\n        if not os.path.isdir(current_dir + \"/\" + category):\n            os.makedirs(current_dir + \"/\" + category)\n            if category == \"MainFiles\":\n                # Copy aux folder.\n                src = \"../biosignalsnotebooks_notebooks/Categories/MainFiles/aux_files\"\n                shutil.copytree(src, current_dir + \"/\" + category + \"/aux_files\")\n\n    # if clone is True:\n    #     # Fill each folder inside \"Categories\" directory with the respective notebooks.\n    #     # Each notebook will be created by a specific function.\n    #     dir_path = root + \"/notebook_code\"\n    #     list_of_code_dirs = os.listdir(dir_path)\n    #     for folder in list_of_code_dirs:\n    #         folder_path = root + \"/notebook_code/\" + folder\n    #         if folder != \"MainFiles\" and folder != \"__pycache__\":\n    #             list_of_code_files = os.listdir(folder_path)\n    #             for file in list_of_code_files:\n    #                 if file != \"__pycache__\":\n    #                     spec = importlib.util.spec_from_file_location(file, folder_path +\n    #                                                                   \"/\" + file)\n    #                     foo = importlib.util.module_from_spec(spec)\n    #                     spec.loader.exec_module(foo)\n    #                     foo.run(root + \"/biosignalsnotebooks_environment\")\n    #\n    #     # Generation of biosignalsnotebooks environment main files.\n    #     main_page = notebook(\"Main_Files_By_Category\")\n    #     main_page.write_to_file(root + \"/biosignalsnotebooks_environment\", \"biosignalsnotebooks\",\n    #                             footer=False)\n    #\n    #     by_difficulty = notebook(\"Main_Files_By_Difficulty\", \"Notebooks Grouped by Difficulty\", notebook_description=DESCRIPTION_GROUP_BY)\n    #     by_difficulty.write_to_file(root + \"/biosignalsnotebooks_environment\", \"by_diff\",\n    #                                 footer=False)\n    #\n    #     by_tags = notebook(\"Main_Files_By_Tag\", \"Notebooks Grouped by Tag Values\",\n    #                        notebook_description=DESCRIPTION_GROUP_BY)\n    #     by_tags.write_to_file(root + \"/biosignalsnotebooks_environment\", \"by_tag\",\n    #                           footer=False)\n    #\n    #     by_signal_type = notebook(\"Main_Files_By_Signal_Type\", \"Notebooks Grouped by Signal Type\",\n    #                               notebook_description=DESCRIPTION_GROUP_BY)\n    #     by_signal_type.write_to_file(root + \"/biosignalsnotebooks_environment\",\n    #                                  \"by_signal_type\", footer=False)\n    #\n    #     signal_samples = notebook(\"Main_Files_Signal_Samples\", \"Signal Samples Library\",\n    #                               notebook_description=DESCRIPTION_SIGNAL_SAMPLES)\n    #     signal_samples.write_to_file(root + \"/biosignalsnotebooks_environment\",\n    #                                  \"signal_samples\", footer=False)\n\n    return root + \"/biosignalsnotebooks_environment\"", "response": "This function generates the folder hierarchy of OpenSignalsTools Notebooks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_notebooks_by_category(notebook_object, dict_by_tag):\n\n    # ============================ Insertion of an opening text ====================================\n    markdown_cell = OPEN_IMAGE\n\n    # == Generation of a table that group Notebooks by category the information about each signal ==\n    category_list = list(NOTEBOOK_KEYS.keys())\n    tag_keys = list(dict_by_tag.keys())\n\n    markdown_cell += \"\"\"\\n<table id=\"notebook_list\" width=\"100%\">\n    <tr>\n        <td width=\"20%\" class=\"center_cell group_by_header_grey\"> Category </td>\n        <td width=\"60%\" class=\"center_cell group_by_header\"></td>\n        <td width=\"20%\" class=\"center_cell\"></td>\n    </tr>\"\"\"\n\n    for i, category in enumerate(category_list):\n        if category != \"MainFiles\":\n            if category.lower() in tag_keys:\n                if i == 0:\n                    first_border = \"color1_top\"\n                else:\n                    first_border = \"\"\n\n\n                nbr_notebooks = len(dict_by_tag[category.lower()])\n                markdown_cell += \"\\n\\t<tr>\" \\\n                                 \"\\n\\t\\t<td rowspan='\" + str(nbr_notebooks + 1) + \"' class='center_cell open_cell_border_\" + str(NOTEBOOK_KEYS[category]) + \"'><span style='float:center'><img src='../../images/icons/\" + category + \".png' class='icon' style='vertical-align:middle'></span> <span style='float:center' class='color\" + str(NOTEBOOK_KEYS[category]) + \"'>\" + category + \"</span></td>\" \\\n                                 \"\\n\\t\\t<td class='center_cell color\" + str(NOTEBOOK_KEYS[category]) + \"_cell \" + first_border + \"'><span style='float:center'>\" + category +  \"</span></td>\" \\\n                                 \"\\n\\t\\t<td class='center_cell gradient_color\" + str(NOTEBOOK_KEYS[category]) + \"'></td>\" \\\n                                 \"\\n\\t</tr>\"\n\n                notebook_list = dict_by_tag[category.lower()]\n                for j, notebook_file in enumerate(notebook_list):\n                    if j == len(notebook_list) - 1:\n                        last_border = \"class='border_cell_bottom_white'\"\n                    else:\n                        last_border = \"\"\n\n                    split_path = notebook_file.replace(\"\\\\\", \"/\").split(\"/\")\n                    notebook_name = split_path[-1].split(\"&\")[0]\n                    notebook_title = split_path[-1].split(\"&\")[1]\n                    markdown_cell += \"\\n\\t<tr \" + last_border + \">\" \\\n                                     \"\\n\\t\\t<td class='center_cell open_cell_light'> <a href='../\" + category + \"/\" + notebook_name + \"'>\" + notebook_title + \"</a> </td>\" \\\n                                     \"\\n\\t\\t<td class='center_cell'> <a href='../\" + category + \"/\" + notebook_name + \"'><div class='file_icon'></div></a> </td>\" \\\n                                     \"\\n\\t</tr>\"\n\n    markdown_cell += \"\\n</table>\"\n\n    # ============================ Insertion of an introductory text ===============================\n    markdown_cell += DESCRIPTION_CATEGORY\n\n    # =================== Insertion of the HTML table inside a markdown cell =======================\n    notebook_object[\"cells\"].append(nb.v4.new_markdown_cell(markdown_cell))", "response": "Generates a markdown table that contains the Notebooks that are grouped by category."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the GitHub README file.", "response": "def _generate_github_readme(notebook_object, dict_by_tag):\n    \"\"\"\n    Internal function that is used for generation of the GitHub and PyPI README file.\n\n    ----------\n    Parameters\n    ----------\n    notebook_object : notebook object\n        Object of \"notebook\" class where the body will be created.\n\n    dict_by_tag : dict\n        Dictionary where each key is a tag and the respective value will be a list containing the\n        Notebooks (title and filename) that include this tag.\n\n    \"\"\"\n\n    # ==================== List of urls that contain each Category icon ============================\n    icons = {\"Detect\": \"https://i.ibb.co/rymrvFL/Detect.png\",\n             \"Evaluate\": \"https://i.ibb.co/yfwcy2M/Evaluate.png\",\n             \"Extract\": \"https://i.ibb.co/tchq7Cc/Extract.png\",\n             \"Load\": \"https://i.ibb.co/YPbCnzD/Load.png\",\n             \"Pre-Process\": \"https://i.ibb.co/1rKWccX/Pre-Process.png\",\n             \"Record\": \"https://i.ibb.co/d2jZH1s/Record.png\",\n             \"Train_and_Classify\": \"https://i.ibb.co/CQ4cyGb/Train-and-Classify.png\",\n             \"Understand\": \"https://i.ibb.co/MnhRRQT/Understand.png\",\n             \"Visualise\": \"https://i.ibb.co/wh4HKzf/Visualise.png\",\n             \"Other\": \"https://i.ibb.co/ry9BzhV/Other.png\",\n             \"Install\": \"https://i.ibb.co/4fBR4Q3/Install.png\",\n             \"Connect\": \"https://i.ibb.co/3yDZpxC/Connect.png\"}\n\n    # =========================== biosignalsnotebooks website ======================================\n    biosignalsnotebooks_web = \"http://www.biosignalsplux.com/notebooks/Categories/\"\n\n    # == Generation of a table that group Notebooks by category the information about each signal ==\n    category_list = list(NOTEBOOK_KEYS.keys())\n    tag_keys = list(dict_by_tag.keys())\n\n    markdown_cell = \"\"\"<table width=\"100%\">\n    <tr>\n        <td width=\"20%\" align=\"center\"><strong> Category <strong></td>\n        <td width=\"80%\"></td>\n    </tr>\"\"\"\n\n    for i, category in enumerate(category_list):\n        if category != \"MainFiles\":\n            if category.lower() in tag_keys:\n                nbr_notebooks = len(dict_by_tag[category.lower()])\n                notebook_list = dict_by_tag[category.lower()]\n                split_path = notebook_list[0].replace(\"\\\\\", \"/\").split(\"/\")\n                notebook_name = split_path[-1].split(\"&\")[0]\n                notebook_title = split_path[-1].split(\"&\")[1]\n                markdown_cell += \"\\n\\t<tr>\" \\\n                                 \"\\n\\t\\t<td rowspan='\" + str(nbr_notebooks) + \"'><p align='center'><img src='\" + icons[category] + \"' width='50%' align='center'></p></td>\" \\\n                                 \"\\n\\t\\t<td align='center'> <a href='\" + biosignalsnotebooks_web + category + \"/\" + notebook_name.replace(\".ipynb\", \"_rev.php\") + \"' target='_blank'>\" + notebook_title + \"</a> </td>\" \\\n                                 \"\\n\\t</tr>\"\n\n                for j, notebook_file in enumerate(notebook_list[1:]):\n                    split_path = notebook_file.replace(\"\\\\\", \"/\").split(\"/\")\n                    notebook_name = split_path[-1].split(\"&\")[0]\n                    notebook_title = split_path[-1].split(\"&\")[1]\n                    markdown_cell += \"\\n\\t<tr>\" \\\n                                     \"\\n\\t\\t<td align='center'> <a href='\" + biosignalsnotebooks_web + category + \"/\" + notebook_name.replace(\".ipynb\", \"_rev.php\") + \"'>\" + notebook_title + \"</a> </td>\" \\\n                                     \"\\n\\t</tr>\"\n\n    markdown_cell += \"\\n</table>\"\n\n    # ============================ Generation of README files ======================================\n    # [Open template]\n    template_path = os.path.abspath(__file__).split(os.path.basename(__file__))[0].replace(\"\\\\\", \"/\") + \"/notebook_files/github/README_TEMPLATE.md\"\n    with open(template_path, 'r') as readme:\n        readme_str = readme.read()\n\n    readme_str = readme_str.replace(\"LIST_OF_NOTEBOOKS\", markdown_cell)\n\n    # [Storage of the updated files]\n    for path in [\"../biosignalsnotebooks/README_BSN.md\", \"../README.md\"]:\n        with open(path, 'w') as readme_out:\n            readme_out.write(readme_str)\n        readme_out.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _request(self, method, path, params=None):\n        url = self._base_url + path\n\n        try:\n            if method == 'GET':\n                response = requests.get(url, timeout=TIMEOUT)\n            elif method == \"POST\":\n                response = requests.post(url, params, timeout=TIMEOUT)\n            elif method == \"PUT\":\n                response = requests.put(url, params, timeout=TIMEOUT)\n            elif method == \"DELETE\":\n                response = requests.delete(url, timeout=TIMEOUT)\n\n            if response:\n                return response.json()\n            else:\n                return {'status': 'error'}\n        except requests.exceptions.HTTPError:\n            return {'status': 'error'}\n        except requests.exceptions.Timeout:\n            return {'status': 'offline'}\n        except requests.exceptions.RequestException:\n            return {'status': 'offline'}", "response": "Make the actual request and returns the parsed response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the internal state of the object with the contents of the data bytes.", "response": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        data bytes (high + low)\n            1 + 2   = current temp\n            3 + 4   = min temp\n            5 + 6   = max temp\n        :return: None\n        \"\"\"\n        assert isinstance(data, bytes)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 6)\n        self.set_attributes(priority, address, rtr)\n        self.cur = (((data[0] << 8)| data[1]) / 32 ) * 0.0625\n        self.min = (((data[2] << 8) | data[3]) / 32 ) * 0.0625\n        self.max = (((data[4] << 8) | data[5]) / 32 ) * 0.0625"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(self):\n        json_dict = self.to_json_basic()\n        json_dict['cur'] = self.cur\n        json_dict['min'] = self.min\n        json_dict['max'] = self.max\n        return json.dumps(json_dict)", "response": "Returns a JSON string representation of the current set of cache entries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_worker_init(worker):\n    quit_command = 'CTRL-BREAK' if sys.platform == 'win32' else 'CONTROL-C'\n    sys.stdout.write(\n        \"Django version {djangover}, Gunicorn version {gunicornver}, \"\n        \"using settings {settings!r}\\n\"\n        \"Starting development server at {urls}\\n\"\n        \"Quit the server with {quit_command}.\\n\".format(\n            djangover=django.get_version(),\n            gunicornver=gunicorn.__version__,\n            settings=os.environ.get('DJANGO_SETTINGS_MODULE'),\n            urls=', '.join('http://{0}/'.format(b) for b in worker.cfg.bind),\n            quit_command=quit_command,\n        ),\n    )", "response": "Hook into Gunicorn to display message after launching."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef highpass(s, f, order=2, fs=1000.0, use_filtfilt=False):\n    '''\n    @brief: for a given signal s rejects (attenuates) the frequencies lower\n    then the cuttof frequency f and passes the frequencies higher than that\n    value by applying a Butterworth digital filter\n\n    @params:\n\n    s: array-like\n    signal\n\n    f: int\n    the cutoff frequency\n\n    order: int\n    Butterworth filter order\n\n    fs: float\n    sampling frequency\n\n    @return:\n\n    signal: array-like\n    filtered signal\n\n    '''\n\n    b, a = signal.butter(order, f * 2 / (fs/2), btype='highpass')\n    if use_filtfilt:\n        return filtfilt(b, a, s)\n\n    return signal.lfilter(b, a, s)", "response": "This function accepts a signal s and a frequency f and returns a new signal that is highpassed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the data value of this attachment.", "response": "def value(self):\n        \"\"\"Retrieve the data value of this attachment.\n\n        Will show the filename of the attachment if there is an attachment available otherwise None\n        Use save_as in order to download as a file.\n\n        Example\n        -------\n\n        >>> file_attachment_property = project.part('Bike').property('file_attachment')\n        >>> if file_attachment_property.value:\n        ...     file_attachment_property.save_as('file.ext')\n        ... else:\n        ...     print('file attachment not set, its value is None')\n\n        \"\"\"\n        if 'value' in self._json_data and self._json_data['value']:\n            return \"[Attachment: {}]\".format(self._json_data['value'].split('/')[-1])\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filename(self):\n        if self.value and 'value' in self._json_data and self._json_data['value']:\n            return self._json_data['value'].split('/')[-1]\n        return None", "response": "Filename of the attachment without the full attachment path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload a file to the KE - chain.", "response": "def upload(self, data, **kwargs):\n        \"\"\"Upload a file to the attachment property.\n\n        When providing a :class:`matplotlib.figure.Figure` object as data, the figure is uploaded as PNG.\n        For this, `matplotlib`_ should be installed.\n\n        :param filename: File path\n        :type filename: basestring\n        :raises APIError: When unable to upload the file to KE-chain\n        :raises OSError: When the path to the file is incorrect or file could not be found\n\n        .. _matplotlib: https://matplotlib.org/\n        \"\"\"\n        try:\n            import matplotlib.figure\n\n            if isinstance(data, matplotlib.figure.Figure):\n                self._upload_plot(data, **kwargs)\n                return\n        except ImportError:\n            pass\n\n        if isinstance(data, str):\n            with open(data, 'rb') as fp:\n                self._upload(fp)\n        else:\n            self._upload_json(data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_as(self, filename):\n        with open(filename, 'w+b') as f:\n            for chunk in self._download():\n                f.write(chunk)", "response": "Download the attachment to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef devpiserver_cmdline_run(xom):\n    '''\n    Load theme when `theme` parameter is 'semantic-ui'.\n    '''\n    if xom.config.args.theme == 'semantic-ui':\n        xom.config.args.theme = resource_filename('devpi_semantic_ui', '')\n        xom.log.info(\"Semantic UI Theme loaded\")", "response": "Load theme when theme parameter is semantic - ui."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a switch is turned on", "response": "def is_on(self, channel):\n        \"\"\"\n        Check if a switch is turned on\n\n        :return: bool\n        \"\"\"\n        if channel in self._is_on:\n            return self._is_on[channel]\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nturn on switch. :return: None", "response": "def turn_on(self, channel, callback=None):\n        \"\"\"\n        Turn on switch.\n\n        :return: None\n        \"\"\"\n        if callback is None:\n            def callb():\n                \"\"\"No-op\"\"\"\n                pass\n            callback = callb\n        message = velbus.SwitchRelayOnMessage(self._address)\n        message.relay_channels = [channel]\n        self._controller.send(message, callback)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nturning off switch. :return: None", "response": "def turn_off(self, channel, callback=None):\n        \"\"\"\n        Turn off switch.\n\n        :return: None\n        \"\"\"\n        if callback is None:\n            def callb():\n                \"\"\"No-op\"\"\"\n                pass\n            callback = callb\n        message = velbus.SwitchRelayOffMessage(self._address)\n        message.relay_channels = [channel]\n        self._controller.send(message, callback)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_dew_point(self, t=None, rh=None):\n\t\t'With t and rh provided, does not access the hardware.'\n\t\tif t is None: t, rh = self.read_t(), None\n\t\tif rh is None: rh = self.read_rh(t)\n\t\tt_range = 'water' if t >= 0 else 'ice'\n\t\ttn, m = self.c.tn[t_range], self.c.m[t_range]\n\t\treturn ( # ch 4.4\n\t\t\ttn * (math.log(rh / 100.0) + (m * t) / (tn + t))\n\t\t\t/ (m - math.log(rh / 100.0) - m * t / (tn + t)) )", "response": "With t and rh provided does not access the hardware."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _put_options(self, options_list):\n        new_options = self._options.copy()  # make a full copy of the dict not to only link it and update dict in place\n        new_options.update({\"value_choices\": options_list})\n        validate(new_options, options_json_schema)\n\n        url = self._client._build_url('property', property_id=self.id)\n        response = self._client._request('PUT', url, json={'options': new_options})\n\n        if response.status_code != 200:  # pragma: no cover\n            raise APIError(\"Could not update property value. Response: {}\".format(str(response)))\n        else:\n            self._options = new_options", "response": "Save the options to KE - chain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_form_or_formset_fields_not_required(form_or_formset):\n    if isinstance(form_or_formset, BaseFormSet):\n        for single_form in form_or_formset:\n            make_form_fields_not_required(single_form)\n    else:\n        make_form_fields_not_required(form_or_formset)", "response": "Take a Form or FormSet and set all fields to not required."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopulating the internal state of the object with the contents of the data.", "response": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        -DB1    last 2 bits   = channel\n        -DB1    first 6 bist  = pulses\n        -DB2-5                = pulse counter\n        -DB6-7                = ms/pulse               \n        :return: None\n        \"\"\"\n        assert isinstance(data, bytes)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 7)\n        self.set_attributes(priority, address, rtr)\n        self.channel = (data[0] & 0x03) +1 \n        self.pulses = (data[0] >> 2) * 100\n        self.counter = (data[1] << 24) + (data[2] << 16) + (data[3] << 8) + data[4]\n        self.delay = (data[5] << 8) + data[6]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nalign various waves in a specific point. Given a mode of alignment, this function computes the specific time point of a wave where all the waves would be aligned. With the difference between the time point of the reference wave and the time points of all the other waves, we have the amount of samples the waves will move to align in the specific point computed. Parameters ---------- w: array-like the input signal to use as a reference of alignment (all the other signals will be aligned with this one). signals: array-like or matrix-like the input signals to align. mode: string the mode used in the alignment, from 'max', 'min', 'peak', 'peak_neg', 'infMaxAlign' and 'infMinAlign'. Returns ------- nw: a masked array a new set of aligned signals in a masked array (some cells have NAN values due to the alignment). Example ------- >>> align([6,3,4,5,2,2],[10,30,28,26,13,20],'max') [masked_array(data = [30.0 28.0 26.0 13.0 20.0 --], mask = [False False False False False True], fill_value = 1e+20) ] >>> align([6,3,4,5,2,2],[10,30,28,26,13,20],'peak') [masked_array(data = [-- -- 10.0 30.0 28.0 26.0], mask = [ True True False False False False], fill_value = 1e+20) ] >>> align([34,4,8],[[100,550,278,67,613,120],[10,470,230,189,856,420]],'min') [masked_array(data = [278.0 67.0 613.0 120.0 -- --], mask = [False False False False True True], fill_value = 1e+20) , masked_array(data = [-- 10.0 470.0 230.0 189.0 856.0], mask = [ True False False False False False], fill_value = 1e+20) ]", "response": "def align(w,signals,mode):\n    \"\"\" Align various waves in a specific point.\n    \n    Given a mode of alignment, this function computes the specific time point of \n    a wave where all the waves would be aligned. With the difference between the\n    time point of the reference wave and the time points of all the other waves, \n    we have the amount of samples the waves will move to align in the specific \n    point computed. \n   \n    Parameters\n    ----------\n    w: array-like\n      the input signal to use as a reference of alignment (all the other signals\n      will be aligned with this one).\n    signals: array-like or matrix-like\n      the input signals to align. \n    mode: string\n      the mode used in the alignment, from 'max', 'min', 'peak', 'peak_neg', \n      'infMaxAlign' and 'infMinAlign'. \n\n    Returns\n    -------\n    nw: a masked array\n      a new set of aligned signals in a masked array (some cells have NAN values\n      due to the alignment).\n        \n    Example\n    -------\n    >>> align([6,3,4,5,2,2],[10,30,28,26,13,20],'max')\n    [masked_array(data = [30.0 28.0 26.0 13.0 20.0 --],\n                 mask = [False False False False False  True],\n           fill_value = 1e+20)\n    ]\n    >>> align([6,3,4,5,2,2],[10,30,28,26,13,20],'peak')\n    [masked_array(data = [-- -- 10.0 30.0 28.0 26.0],\n                 mask = [ True  True False False False False],\n           fill_value = 1e+20)\n    ]\n    >>> align([34,4,8],[[100,550,278,67,613,120],[10,470,230,189,856,420]],'min')\n    [masked_array(data = [278.0 67.0 613.0 120.0 -- --],\n                 mask = [False False False False  True  True],\n           fill_value = 1e+20)\n    , masked_array(data = [-- 10.0 470.0 230.0 189.0 856.0],\n                 mask = [ True False False False False False],\n           fill_value = 1e+20)\n    ]\n    \"\"\" \n    \n    nw = []\n    \n    if len(shape(signals))==1:       \n        signals = [signals]\n               \n    for i in range(len(signals)):\n        \n        if (mode == 'max'):\n            al = maxAlign(w,signals[i])\n        elif (mode == 'min'):\n            al = minAlign(w,signals[i])\n        elif (mode == 'peak'):\n            al = peakAlign(w,signals[i])\n        elif (mode == 'peak_neg'):\n            al = peakNegAlign(w,signals[i])\n        elif (mode == 'infMaxAlign'):\n            al = infMaxAlign(w,signals[i])\n        elif (mode == 'infMinAlign'):\n            al = infMinAlign(w,signals[i])\n                \n        nw += [ moveWave(signals[i],al) ]\n          \n    return nw"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove a signal in time.", "response": "def moveWave(w,move):\n    \"\"\" Move a signal in time.\n    \n    This function returns a signal created by a shifting in time on the original\n    signal. \n   \n    Parameters\n    ----------\n    w: array-like\n      the input signal to move.\n    move: int\n      the ammount of samples to shift the signal (if <0 the signal moves back, \n      if >0 the signal moves forward).\n\n    Returns\n    -------\n    nw: a masked array\n      a new aligned signal in a masked array (some cells have NAN values\n      due to the alignment).\n        \n    \n    See also: align()\n    \"\"\" \n    \n    nw = ma.masked_all(len(w))\n    \n    if (move == 0): \n        #don't move\n        nw[0:] = w[0:]  \n    elif (move>0):\n        #forward\n        nw[move:len(w)] = w[:len(w)-move]  \n    else:\n        #backwards\n        nw[:len(w)-abs(move)] = w[abs(move):len(w)]\n   \n    return nw"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the scope id of the activity.", "response": "def scope_id(self):\n        \"\"\"\n        ID of the scope this Activity belongs to.\n\n        This property will always produce a scope_id, even when the scope object was not included in an earlier\n        response.\n\n        When the :class:`Scope` is not included in this task, it will make an additional call to the KE-chain API.\n\n        :return: the scope id (uuid string)\n        :raises NotFoundError: if the scope could not be found\n        \"\"\"\n        if self.scope:\n            scope_id = self.scope and self.scope.get('id')\n        else:\n            pseudo_self = self._client.activity(pk=self.id, fields=\"id,scope\")\n            if pseudo_self.scope and pseudo_self.scope.get('id'):\n                self.scope = pseudo_self.scope\n                scope_id = self.scope.get('id')\n            else:\n                raise NotFoundError(\"This activity '{}'({}) does not belong to a scope, something is weird!\".\n                                    format(self.name, self.id))\n        return scope_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if the activity is at the root level of a project.", "response": "def is_rootlevel(self):\n        \"\"\"\n        Determine if Activity is at the root level of a project.\n\n        :return: Return True if it is a root level activity, otherwise return False\n        :rtype: bool\n        \"\"\"\n        container_id = self._json_data.get('container')\n        if container_id:\n            return container_id == self._json_data.get('root_container')\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if the Activity is configured with input and output properties.", "response": "def is_configured(self):\n        \"\"\"\n        Determine if the Activity is configured with input and output properties.\n\n        Makes an additional lightweight call to the API to determine if any associated models are there.\n\n        :return: Return True if it is configured, otherwise return False\n        :rtype: bool\n        \"\"\"\n        # check configured based on if we get at least 1 part back\n        associated_models = self.parts(category=Category.MODEL, limit=1)\n        if associated_models:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parts(self, *args, **kwargs):\n        return self._client.parts(*args, activity=self.id, **kwargs)", "response": "Retrieve the parts belonging to this activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving models and instances belonging to this activity.", "response": "def associated_parts(self, *args, **kwargs):\n        \"\"\"Retrieve models and instances belonging to this activity.\n\n        This is a convenience method for the :func:`Activity.parts()` method, which is used to retrieve both the\n        `Category.MODEL` as well as the `Category.INSTANCE` in a tuple.\n\n        This call only returns the configured properties in an activity. So properties that are not configured\n        are not in the returned parts.\n\n        If you want to retrieve only the models associated to this task it is better to use:\n            `task.parts(category=Category.MODEL)`.\n\n        See :func:`pykechain.Client.parts` for additional available parameters.\n\n        :returns: a tuple(models of :class:`PartSet`, instances of :class:`PartSet`)\n\n        Example\n        -------\n        >>> task = project.activity('Specify Wheel Diameter')\n        >>> all_models, all_instances = task.associated_parts()\n\n        \"\"\"\n        return (\n            self.parts(category=Category.MODEL, *args, **kwargs),\n            self.parts(category=Category.INSTANCE, *args, **kwargs)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef configure(self, inputs, outputs):\n        url = self._client._build_url('activity', activity_id=self.id)\n\n        r = self._client._request('PUT', url, params={'select_action': 'update_associations'}, json={\n            'inputs': [p.id for p in inputs],\n            'outputs': [p.id for p in outputs]\n        })\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not configure activity\")", "response": "Configure the activity input and output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subprocess(self):\n        subprocess_id = self._json_data.get('container')\n        if subprocess_id == self._json_data.get('root_container'):\n            raise NotFoundError(\"Cannot find subprocess for this task '{}', \"\n                                \"as this task exist on top level.\".format(self.name))\n        return self._client.activity(pk=subprocess_id, scope=self.scope_id)", "response": "Retrieve the subprocess in which this activity is defined."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef children(self, **kwargs):\n        if self.activity_type != ActivityType.SUBPROCESS:\n            raise NotFoundError(\"Only subprocesses can have children, please choose a subprocess instead of a '{}' \"\n                                \"(activity '{}')\".format(self.activity_type, self.name))\n\n        return self._client.activities(container=self.id, scope=self.scope_id, **kwargs)", "response": "Retrieve the direct activities of this subprocess."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef siblings(self, **kwargs):\n        container_id = self._json_data.get('container')\n        return self._client.activities(container=container_id, scope=self.scope_id, **kwargs)", "response": "Retrieve the other activities that also belong to the subprocess. This is a convenience method for retrieving the other activities that also belong to the subprocess."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, *args, **kwargs):\n        if self.activity_type != ActivityType.SUBPROCESS:\n            raise IllegalArgumentError(\"One can only create a task under a subprocess.\")\n        return self._client.create_activity(self.id, *args, **kwargs)", "response": "Create a new activity belonging to this subprocess."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nedits the details of an activity.", "response": "def edit(self, name=None, description=None, start_date=None, due_date=None, assignees=None, status=None):\n        \"\"\"Edit the details of an activity.\n\n        :param name: (optionally) edit the name of the activity\n        :type name: basestring or None\n        :param description: (optionally) edit the description of the activity\n        :type description: basestring or None\n        :param start_date: (optionally) edit the start date of the activity as a datetime object (UTC time/timezone\n                            aware preferred)\n        :type start_date: datetime or None\n        :param due_date: (optionally) edit the due_date of the activity as a datetime object (UTC time/timzeone\n                            aware preferred)\n        :type due_date: datetime or None\n        :param assignees: (optionally) edit the assignees of the activity as a list, will overwrite all assignees\n        :type assignees: list(basestring) or None\n        :param status: (optionally) edit the status of the activity as a string based\n                       on :class:`~pykechain.enums.ActivityType`\n        :type status: basestring or None\n\n        :raises NotFoundError: if a `username` in the list of assignees is not in the list of scope members\n        :raises IllegalArgumentError: if the type of the inputs is not correct\n        :raises APIError: if another Error occurs\n        :warns: UserWarning - When a naive datetime is provided. Defaults to UTC.\n\n        Example\n        -------\n        >>> from datetime import datetime\n        >>> my_task = project.activity('Specify the wheel diameter')\n        >>> my_task.edit(name='Specify wheel diameter and circumference',\n        ...              description='The diameter and circumference are specified in inches',\n        ...              start_date=datetime.utcnow(),  # naive time is interpreted as UTC time\n        ...              assignee='testuser')\n\n        If we want to provide timezone aware datetime objects we can use the 3rd party convenience library :mod:`pytz`.\n        Mind that we need to fetch the timezone first and use `<timezone>.localize(<your datetime>)` to make it\n        work correctly.\n\n        Using `datetime(2017,6,1,23,59,0 tzinfo=<tz>)` does NOT work for most timezones with a\n        daylight saving time. Check the `pytz <http://pythonhosted.org/pytz/#localized-times-and-date-arithmetic>`_\n        documentation.\n\n        To make it work using :mod:`pytz` and timezone aware :mod:`datetime` see the following example::\n\n        >>> import pytz\n        >>> start_date_tzaware = datetime.now(pytz.utc)\n        >>> mytimezone = pytz.timezone('Europe/Amsterdam')\n        >>> due_date_tzaware = mytimezone.localize(datetime(2019, 10, 27, 23, 59, 0))\n        >>> my_task.edit(due_date=due_date_tzaware, start_date=start_date_tzaware)\n\n        \"\"\"\n        update_dict = {'id': self.id}\n        if name:\n            if isinstance(name, (str, text_type)):\n                update_dict.update({'name': name})\n                self.name = name\n            else:\n                raise IllegalArgumentError('Name should be a string')\n\n        if description:\n            if isinstance(description, (str, text_type)):\n                update_dict.update({'description': description})\n                self.description = description\n            else:\n                raise IllegalArgumentError('Description should be a string')\n\n        if start_date:\n            if isinstance(start_date, datetime.datetime):\n                if not start_date.tzinfo:\n                    warnings.warn(\"The startdate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(start_date.isoformat(sep=' ')))\n                update_dict.update({'start_date': start_date.isoformat(sep='T')})\n            else:\n                raise IllegalArgumentError('Start date should be a datetime.datetime() object')\n\n        if due_date:\n            if isinstance(due_date, datetime.datetime):\n                if not due_date.tzinfo:\n                    warnings.warn(\"The duedate '{}' is naive and not timezone aware, use pytz.timezone info. \"\n                                  \"This date is interpreted as UTC time.\".format(due_date.isoformat(sep=' ')))\n                update_dict.update({'due_date': due_date.isoformat(sep='T')})\n            else:\n                raise IllegalArgumentError('Due date should be a datetime.datetime() object')\n\n        if assignees:\n            if isinstance(assignees, list):\n                project = self._client.scope(pk=self.scope_id, status=None)\n                members_list = [member['username'] for member in project._json_data['members']]\n                for assignee in assignees:\n                    if assignee not in members_list:\n                        raise NotFoundError(\"Assignee '{}' should be a member of the scope\".format(assignee))\n                update_dict.update({'assignees': assignees})\n            else:\n                raise IllegalArgumentError('Assignees should be a list')\n\n        if status:\n            if isinstance(status, (str, text_type)) and status in ActivityStatus.values():\n                update_dict.update({'status': status})\n            else:\n                raise IllegalArgumentError('Status should be a string')\n\n        url = self._client._build_url('activity', activity_id=self.id)\n        r = self._client._request('PUT', url, json=update_dict)\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not update Activity ({})\".format(r))\n\n        if status:\n            self._json_data['status'] = str(status)\n        if assignees:\n            self._json_data['assignees'] = assignees\n        if due_date:\n            self._json_data['due_date'] = str(due_date)\n        if start_date:\n            self._json_data['start_date'] = str(start_date)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an ExtCustomization object representing the customization of the activity.", "response": "def customization(self):\n        \"\"\"\n        Get a customization object representing the customization of the activity.\n\n        .. versionadded:: 1.11\n\n        :return: An instance of :class:`customization.ExtCustomization`\n\n        Example\n        -------\n        >>> activity = project.activity(name='Customizable activity')\n        >>> customization = activity.customization()\n        >>> part_to_show = project.part(name='Bike')\n        >>> customization.add_property_grid_widget(part_to_show, custom_title=\"My super bike\"))\n\n        \"\"\"\n        from .customization import ExtCustomization\n\n        # For now, we only allow customization in an Ext JS context\n        return ExtCustomization(activity=self, client=self._client)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate GraphQL query string for a set of keys.", "response": "def get_gql_query(self):\n        \"\"\"Generate GraphQL query\"\"\"\n        template_string = \"\"\"query(\n            $stops: [String],\n            $quays: [String],\n            $whitelist: InputWhiteListed,\n            $numberOfDepartures: Int = 2,\n            $omitNonBoarding: Boolean = true){\\n\"\"\"\n        if self.stops:\n            template_string += GRAPHQL_STOP_TEMPLATE\n        if self.quays:\n            template_string += GRAPHQL_QUAY_TEMPLATE\n        template_string += \"}\"\n        template_string += GRAPHQL_CALL_FRAGMENT\n\n        return template_string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_stop_places_quays(self) -> list:\n        all_places = self.stops.copy()\n        for quay in self.quays:\n            all_places.append(quay)\n        return all_places", "response": "Get all stop places and quays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding all quays from stop places.", "response": "async def expand_all_quays(self) -> None:\n        \"\"\"Find all quays from stop places.\"\"\"\n        if not self.stops:\n            return\n\n        headers = {'ET-Client-Name': self._client_name}\n        request = {\n            'query': GRAPHQL_STOP_TO_QUAY_TEMPLATE,\n            'variables': {\n                'stops': self.stops,\n                'omitNonBoarding': self.omit_non_boarding\n            }\n        }\n\n        with async_timeout.timeout(10):\n            resp = await self.web_session.post(RESOURCE,\n                                               json=request,\n                                               headers=headers)\n\n        if resp.status != 200:\n            _LOGGER.error(\n                \"Error connecting to Entur, response http status code: %s\",\n                resp.status)\n            return None\n        result = await resp.json()\n\n        if 'errors' in result:\n            return\n\n        for stop_place in result['data']['stopPlaces']:\n            if len(stop_place['quays']) > 1:\n                for quay in stop_place['quays']:\n                    if quay['estimatedCalls']:\n                        self.quays.append(quay['id'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the latest data from api. entur. org.", "response": "async def update(self) -> None:\n        \"\"\"Get the latest data from api.entur.org.\"\"\"\n        headers = {'ET-Client-Name': self._client_name}\n        request = {\n            'query': self.get_gql_query(),\n            'variables': {\n                'stops': self.stops,\n                'quays': self.quays,\n                'whitelist': {\n                    'lines': self.line_whitelist\n                },\n                'numberOfDepartures': self.number_of_departures,\n                'omitNonBoarding': self.omit_non_boarding\n            }\n        }\n\n        with async_timeout.timeout(10):\n            resp = await self.web_session.post(RESOURCE,\n                                               json=request,\n                                               headers=headers)\n\n        if resp.status != 200:\n            _LOGGER.error(\n                \"Error connecting to Entur, response http status code: %s\",\n                resp.status)\n            return None\n\n        result = await resp.json()\n\n        if 'errors' in result:\n            _LOGGER.warning(\"Entur API responded with error message: {error}\",\n                            result['errors'])\n            return\n\n        self._data = result['data']\n\n        if 'stopPlaces' in self._data:\n            for stop in self._data['stopPlaces']:\n                self._process_place(stop, False)\n\n        if 'quays' in self._data:\n            for quay in self._data['quays']:\n                self._process_place(quay, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts information from place dictionary.", "response": "def _process_place(self, place: dict, is_platform: bool) -> None:\n        \"\"\"Extract information from place dictionary.\"\"\"\n        place_id = place['id']\n        self.info[place_id] = Place(place, is_platform)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a list of model instances into a list of dictionaries where each element in the list is a dictionary of the related objects that are represented by the base class of the base class.", "response": "def serializable_list(\n        olist, attrs_to_serialize=None, rels_to_expand=None,\n        group_listrels_by=None, rels_to_serialize=None,\n        key_modifications=None, groupby=None, keyvals_to_merge=None,\n        preserve_order=False, dict_struct=None, dict_post_processors=None):\n    \"\"\"\n    Converts a list of model instances to a list of dictionaries\n    using their `todict` method.\n\n    Args:\n        olist (list): The list of instances to convert\n        attrs_to_serialize (list, optional): To be passed as an argument\n            to the `todict` method\n        rels_to_expand (list, optional): To be passed as an argument\n            to the `todict` method\n        group_listrels_by (dict, optional): To be passed as an argument\n            to the `todict` method\n        rels_to_serialize (list, optional): To be passed as an argument\n            to the `todict` method\n        key_modifications (dict, optional): To be passed as an argument\n            to the `todict` method\n\n        groupby (list, optional): An optional list of keys based on which\n            the result list will be hierarchially grouped ( and converted\n                into a dict)\n\n        keyvals_to_merge (list of dicts, optional): A list of parameters\n            to be merged with each dict of the output list\n    \"\"\"\n    if groupby:\n        if preserve_order:\n            result = json_encoder(deep_group(\n                olist, keys=groupby, serializer='todict',\n                preserve_order=preserve_order,\n                serializer_kwargs={\n                    'rels_to_serialize': rels_to_serialize,\n                    'rels_to_expand': rels_to_expand,\n                    'attrs_to_serialize': attrs_to_serialize,\n                    'group_listrels_by': group_listrels_by,\n                    'key_modifications': key_modifications,\n                    'dict_struct': dict_struct,\n                    'dict_post_processors': dict_post_processors\n                }))\n        else:\n            result = deep_group(\n                olist, keys=groupby, serializer='todict',\n                preserve_order=preserve_order,\n                serializer_kwargs={\n                    'rels_to_serialize': rels_to_serialize,\n                    'rels_to_expand': rels_to_expand,\n                    'attrs_to_serialize': attrs_to_serialize,\n                    'group_listrels_by': group_listrels_by,\n                    'key_modifications': key_modifications,\n                    'dict_struct': dict_struct,\n                    'dict_post_processors': dict_post_processors\n                })\n        return result\n    else:\n        result_list = map(\n            lambda o: serialized_obj(\n                o, attrs_to_serialize=attrs_to_serialize,\n                rels_to_expand=rels_to_expand,\n                group_listrels_by=group_listrels_by,\n                rels_to_serialize=rels_to_serialize,\n                key_modifications=key_modifications,\n                dict_struct=dict_struct,\n                dict_post_processors=dict_post_processors),\n            olist)\n        if keyvals_to_merge:\n            result_list = [merge(obj_dict, kvdict)\n                           for obj_dict, kvdict in\n                           zip(result_list, keyvals_to_merge)]\n        return result_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jsoned(struct, wrap=True, meta=None, struct_key='result', pre_render_callback=None):\n    return _json.dumps(\n        structured(\n            struct, wrap=wrap, meta=meta, struct_key=struct_key,\n            pre_render_callback=pre_render_callback),\n        default=json_encoder)", "response": "Provides a json dump of the structure in the order they appear in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_query_using_filters_list(result, filters_dict):\n    if not (isinstance(result, Query) or isinstance(result, QueryBooster)):\n        if isinstance(result, DefaultMeta) and class_mapper(\n                result).polymorphic_on is not None:\n            result = result.query.with_polymorphic('*')\n        else:\n            result = result.query\n    filters = filters_dict['f']\n    connector = filters_dict.get('c') or 'AND'\n    sqfilter = convert_filters_to_sqlalchemy_filter(result, filters, connector)\n    if sqfilter is not None:\n        result = result.filter(sqfilter)\n    return result", "response": "Filter the result query using the filters_dict."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_obj(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        response = func(*args, **kwargs)\n        return render_json_obj_with_requested_structure(response)\n    return wrapper", "response": "A decorator that returns a JSON response with a dict\n        representation of the model instance. It expects the decorated function\n        to return a Model instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef populate(self, priority, address, rtr, data):\n        assert isinstance(data, bytes)\n        self.needs_low_priority(priority)\n        self.needs_no_rtr(rtr)\n        #self.needs_data(data, 6)\n        self.set_attributes(priority, address, rtr)\n        self.module_type = data[0]\n        self.sub_address_1 = data[3]\n        self.sub_address_2 = data[4]\n        self.sub_address_3 = data[5]\n        self.sub_address_4 = data[6]", "response": "Populates the object with the data from the specified entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a JSON string representation of the current object.", "response": "def to_json(self):\n        \"\"\"\n        :return: str\n        \"\"\"\n        json_dict = self.to_json_basic()\n        json_dict['sub_1'] = self.sub_address_1\n        json_dict['sub_2'] = self.sub_address_2\n        json_dict['sub_3'] = self.sub_address_3\n        json_dict['sub_4'] = self.sub_address_4\n        return json.dumps(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(self):\n        json_dict = self.to_json_basic()\n        json_dict['ds'] = self._ds\n        return json.dumps(json_dict)", "response": "Returns a JSON string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrefreshing the object in place.", "response": "def refresh(self):\n        # type: () -> None\n        \"\"\"Refresh the object in place.\"\"\"\n        src = self._client.reload(self)\n        self.__dict__.update(src.__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the object with the data.", "response": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        :return: None\n        \"\"\"\n        self.needs_no_rtr(rtr)\n        self.set_attributes(priority, address, rtr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a JSON string representation of the current object.", "response": "def to_json(self):\n        \"\"\"\n        :return: str\n        \"\"\"\n        json_dict = self.to_json_basic()\n        json_dict['sleep_time'] = self.sleep\n        return json.dumps(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self, interactive=False):\n        url = self._client._build_url('service_execute', service_id=self.id)\n        response = self._client._request('GET', url, params=dict(interactive=interactive, format='json'))\n\n        if response.status_code != requests.codes.accepted:  # pragma: no cover\n            raise APIError(\"Could not execute service '{}': {}\".format(self, (response.status_code, response.json())))\n\n        data = response.json()\n        return ServiceExecution(json=data.get('results')[0], client=self._client)", "response": "Execute the service.\n\n        For interactive (notebook) service execution, set interactive to True, defaults to False.\n\n        .. versionadded:: 1.13\n\n        :param interactive: (optional) True if the notebook service should execute in interactive mode.\n        :type interactive: bool or None\n        :return: ServiceExecution when successful.\n        :raises APIError: when unable to execute"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nedit the details of a service.", "response": "def edit(self, name=None, description=None, version=None, **kwargs):\n        \"\"\"\n        Edit Service details.\n\n        .. versionadded:: 1.13\n\n        :param name: (optional) name of the service to change.\n        :type name: basestring or None\n        :param description: (optional) description of the service.\n        :type description: basestring or None\n        :param version: (optional) version number of the service.\n        :type version: basestring or None\n        :param kwargs: (optional) additional keyword arguments to change.\n        :type kwargs: dict or None\n        :raises IllegalArgumentError: when you provide an illegal argument.\n        :raises APIError: if the service could not be updated.\n        \"\"\"\n        update_dict = {'id': self.id}\n        if name:\n            if not isinstance(name, str):\n                raise IllegalArgumentError(\"name should be provided as a string\")\n            update_dict.update({'name': name})\n        if description:\n            if not isinstance(description, str):\n                raise IllegalArgumentError(\"description should be provided as a string\")\n            update_dict.update({'description': description})\n        if version:\n            if not isinstance(version, str):\n                raise IllegalArgumentError(\"description should be provided as a string\")\n            update_dict.update({'script_version': version})\n\n        if kwargs:  # pragma: no cover\n            update_dict.update(**kwargs)\n        response = self._client._request('PUT',\n                                         self._client._build_url('service', service_id=self.id), json=update_dict)\n\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not update Service ({})\".format(response))\n\n        if name:\n            self.name = name\n        if version:\n            self.version = version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting this service. :raises APIError: if delete was not succesfull.", "response": "def delete(self):\n        # type: () -> None\n        \"\"\"Delete this service.\n\n        :raises APIError: if delete was not succesfull.\n        \"\"\"\n        response = self._client._request('DELETE', self._client._build_url('service', service_id=self.id))\n\n        if response.status_code != requests.codes.no_content:  # pragma: no cover\n            raise APIError(\"Could not delete service: {} with id {}\".format(self.name, self.id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuploads a python script or kecpkg to the service.", "response": "def upload(self, pkg_path):\n        \"\"\"\n        Upload a python script (or kecpkg) to the service.\n\n        .. versionadded:: 1.13\n\n        :param pkg_path: path to the python script or kecpkg to upload.\n        :type pkg_path: basestring\n        :raises APIError: if the python package could not be uploaded.\n        :raises OSError: if the python package could not be located on disk.\n        \"\"\"\n        if os.path.exists(pkg_path):\n            self._upload(pkg_path=pkg_path)\n        else:\n            raise OSError(\"Could not locate python package to upload in '{}'\".format(pkg_path))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the service script as known in KE - chain.", "response": "def save_as(self, target_dir=None):\n        \"\"\"\n        Save the kecpkg service script to an (optional) target dir.\n\n        Retains the filename of the service as known in KE-chain.\n\n        .. versionadded:: 1.13\n\n        :param target_dir: (optional) target dir. If not provided will save to current working directory.\n        :type target_dir: basestring or None\n        :raises APIError: if unable to download the service.\n        :raises OSError: if unable to save the service kecpkg file to disk.\n        \"\"\"\n        full_path = os.path.join(target_dir or os.getcwd(), self.filename)\n\n        url = self._client._build_url('service_download', service_id=self.id)\n        response = self._client._request('GET', url)\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not download service script file ({})\".format(response))\n\n        with open(full_path, 'w+b') as f:\n            for chunk in response:\n                f.write(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_executions(self, **kwargs):\n        return self._client.service_executions(service=self.id, scope=self.scope_id, **kwargs)", "response": "Retrieve the executions related to the current service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef service(self):\n        if not self._service:\n            self._service = self._client.service(id=self.service_id)\n        return self._service", "response": "Retrieve the Service object to which this execution is associated."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef terminate(self):\n        url = self._client._build_url('service_execution_terminate', service_execution_id=self.id)\n        response = self._client._request('GET', url, params=dict(format='json'))\n\n        if response.status_code != requests.codes.accepted:  # pragma: no cover\n            raise APIError(\"Could not execute service '{}': {}\".format(self, response))", "response": "Terminate the service execution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_log(self, target_dir=None, log_filename='log.txt'):\n        full_path = os.path.join(target_dir or os.getcwd(), log_filename)\n\n        url = self._client._build_url('service_execution_log', service_execution_id=self.id)\n        response = self._client._request('GET', url)\n        if response.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not download service execution log\")\n\n        with open(full_path, 'w+b') as f:\n            for chunk in response:\n                f.write(chunk)", "response": "Retrieve the log of the service execution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_notebook_url(self):\n        url = self._client._build_url('service_execution_notebook_url', service_execution_id=self.id)\n        response = self._client._request('GET', url, params=dict(format='json'))\n\n        if response.status_code != requests.codes.ok:\n            raise APIError(\"Could not retrieve notebook url '{}': {}\".format(self, response))\n\n        data = response.json()\n        url = data.get('results')[0].get('url')\n        return url", "response": "Get the full url of the notebook in interactive mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions that returns a list of lists that can be used to plot the future data.", "response": "def _plot_future(time, data, legend=None, title=None, y_axis_label=None, hor_lines=None,\n                hor_lines_leg=None, vert_lines=None, vert_lines_leg=None,\n                apply_opensignals_style=True, show_plot=True, warn_print=False, **kwargs):\n    \"\"\"\n    Plotting function intended for an easy representation of OpenSignals acquired data.\n\n    ----------\n    Parameters\n    ----------\n    time : list or list of lists\n        List that contains the time axis samples or a list of lists, when it is intended to present\n        data in a gridplot format. When the input is a list of lists the following structure must\n        be respected:\n        Gridplot with N rows and M columns\n        [[cell_row_0_column_0, cell_row_0_column_1, ..., cell_row_0_column_M],\n         [cell_row_1_column_0, cell_row_1_column_1, ..., cell_row_1_column_M],\n          ...\n         [cell_row_N_column_0, cell_row_N_column_1, ..., cell_row_N_column_M]]\n\n    data : list or list of lists\n        Should have the same shape of time until the cell_row_n_column_m level. At this stage\n        cell_row_n_column_m can contain a set of lists. Each one of these lists contains give\n        rise to a different plot at the figure located in row n and column m of the grid structure.\n\n    legend : list\n        Input where the legend of each plot is specified. Should have the same shape of data.\n\n    title : list\n        Input where the title of each figure is specified. Should have the same shape of time.\n\n    y_axis_label : list\n        Input where the y label of each figure is specified. Should have the same shape of time.\n\n    hor_lines : list of lists\n        The parent list must have the same shape of time and each of its elements (child lists)\n        must be formed by numbers defining the y axis position of the horizontal lines.\n\n    hor_lines_leg : list of lists\n        Legends of the horizontal lines (same shape of hor_lines).\n\n    vert_lines : list of lists\n        The parent list must have the same shape of time and each of its elements (child lists)\n        must be formed by numbers defining the x axis position of the vertical lines.\n\n    vert_lines_leg : list of lists\n        Legends of the vertical lines (same shape of vert_lines).\n\n    apply_opensignals_style : boolean\n        If True then the OpenSignals style will be applied.\n\n\n    show_plot : boolean\n        If True the generated figures will be shown.\n\n    warn_print : bool\n        If True some warnings about invalid kwargs keys will be prompted.\n\n    **kwargs : dict\n        Keyword values for applying in bokeh figures, lines and gridplots.\n\n    Returns\n    -------\n    out : bokeh figure or gridplot\n        Object that is produced during the execution of the present function.\n\n    \"\"\"\n\n    # -------------------------------- Application of styling options -----------------------------\n    if apply_opensignals_style is True:\n        style_figure = {**opensignals_kwargs(\"figure\"), **_filter_keywords(FigureOptions, kwargs,\n                                                                           is_class=True,\n                                                                           warn_print=warn_print)}\n        style_line = {**opensignals_kwargs(\"line\"), **_filter_keywords(Line, kwargs,\n                                                                       warn_print=warn_print)}\n        style_gridplot = {**opensignals_kwargs(\"gridplot\"),\n                          **_filter_keywords(gridplot, kwargs, warn_print=warn_print)}\n    else:\n        style_figure = _filter_keywords(FigureOptions, kwargs, is_class=True, warn_print=warn_print)\n        style_line = _filter_keywords(Line, kwargs, warn_print=warn_print)\n        style_gridplot = _filter_keywords(gridplot, kwargs, warn_print=warn_print)\n\n    # ---------- Based on the input check if the output should be in the gridplot format ----------\n    if len(list(numpy.shape(data))) == 3 and len(list(numpy.shape(time))) == 3:\n        grid_plot = True\n    elif len(list(numpy.shape(data))) == 1 and len(list(numpy.shape(time))) == 1:\n        grid_plot = False\n    else:\n        raise RuntimeError(\"'time' and 'data' fields must have the same shape, which would be a \"\n                           \"list with 1 dimension or a list of lists with 3 levels, such as [[[\"\n                           \"time_0_0, time_0,1, time_0_2], [time_1_0, time_1_1, time_1_2]]]. In the\"\n                           \" previous example the output will be a gridplot with 2 rows and \"\n                           \"three columns.\")\n\n    # ------------ Verification if the input arguments (title and legend) are valid ---------------\n    # [legend]\n    legend = _check_validity_of_inputs(data, legend, \"legend\", grid_plot, dimension=3)\n\n    # [title]\n    title = _check_validity_of_inputs(data, title, \"title\", grid_plot, dimension=2)\n\n    # [y_axis_label]\n    y_axis_label = _check_validity_of_inputs(data, y_axis_label, \"y_axis_label\", grid_plot,\n                                             dimension=2)\n\n    # Horizontal Lines.\n    # [hor_lines]\n    hor_lines = _check_validity_of_inputs(data, hor_lines, \"hor_lines\", grid_plot, dimension=2)\n    hor_lines_leg = _check_validity_of_inputs(data, hor_lines_leg, \"hor_lines_leg\", grid_plot,\n                                              dimension=2)\n\n    # Vertical Lines.\n    # [vert_lines]\n    vert_lines = _check_validity_of_inputs(data, vert_lines, \"vert_lines\", grid_plot, dimension=2)\n    vert_lines_leg = _check_validity_of_inputs(data, vert_lines_leg, \"vert_lines_leg\", grid_plot,\n                                               dimension=2)\n\n    # --------------------------------------- Plotting Stage --------------------------------------\n    fig_list = []\n    if grid_plot is True:\n        # Each element inside \"data\", \"time\", \"title\", \"legend\" ... matrix cell must be a list.\n        if all(_is_instance(list, el, condition=\"all\", deep=True) for el in [time, data, title,\n                                                                         legend, y_axis_label,\n                                                                         hor_lines, vert_lines,\n                                                                         hor_lines_leg,\n                                                                         vert_lines_leg]):\n            for row in range(0, len(data)):  # Generation of a figure per plot.\n                fig_list.append([])\n                for column in range(0, len(data[row])):\n                    for plt in range(0, len(data[row][column])):\n                        # Verification if all elements inside list are numbers.\n                        if _is_instance(Number, data[row][column][plt], condition=\"all\", deep=True) \\\n                                and not _is_instance(bool, data[row][column][plt], condition=\"any\") \\\n                                and _is_instance(Number, time[row][column][0], condition=\"all\") \\\n                                and not _is_instance(bool, time[row][column][0], condition=\"any\"):\n                            fig_list.append([])\n\n                            # Generation of multiple figures.\n                            fig_list[-1][-1].append(figure(title=title[row][column][0],\n                                                           y_axis_label=y_axis_label[row]\n                                                                                    [column][0],\n                                                           **style_figure))\n\n                            fig_list[-1][-1][-1].line(time[row][column][0], data[row][column][plt],\n                                                      legend=legend[row][column][plt], **style_line)\n                        else:\n                            raise RuntimeError(\"At least one of the list elements, specified in \"\n                                               \"data or time, is not numeric.\")\n\n                    # Representation of horizontal lines.\n                    if hor_lines is not None:\n                        for hor_line_nbr, hor_line in enumerate(hor_lines[row][column]):\n                            if hor_lines_leg is not None:\n                                fig_list[-1][-1][-1].line([time[row][column][0],\n                                                           time[row][column][-1]],\n                                                          [hor_line, hor_line],\n                                                          legend=hor_lines_leg[row][hor_line_nbr],\n                                                          **opensignals_kwargs(\"line\"))\n                            else:\n                                fig_list[-1][-1][-1].line([time[row][column][0],\n                                                           time[row][column][-1]],\n                                                          [hor_line, hor_line],\n                                                          **opensignals_kwargs(\"line\"))\n\n                    # Representation of vertical lines.\n                    if vert_lines is not None:\n                        for vert_line_nbr, vert_line in enumerate(vert_lines[row][column]):\n                            if vert_lines_leg is not None:\n                                fig_list[-1][-1][-1].line([vert_line, vert_line],\n                                                          [numpy.min(data[row][column][0]),\n                                                           numpy.max(data[row][column][0])],\n                                                          legend=vert_lines_leg[row][vert_line_nbr],\n                                                          **opensignals_kwargs(\"line\"))\n                            else:\n                                fig_list[-1][-1][-1].line([vert_line, vert_line],\n                                                          [numpy.min(data[row][column][0]),\n                                                           numpy.max(data[row][column][0])],\n                                                          **opensignals_kwargs(\"line\"))\n\n                    # Update of line style.\n                    if apply_opensignals_style is True:\n                        style_line = {**opensignals_kwargs(\"line\"),\n                                      **_filter_keywords(Line, kwargs, warn_print=warn_print)}\n                    else:\n                        style_line = _filter_keywords(Line, kwargs, warn_print=warn_print)\n\n        else:\n            raise RuntimeError(\"At least one of the list elements, specified in data, \"\n                               \"is not a sublist.\")\n    else:\n        # If this happen, then we receive as input a single list for time and data\n        # (Single plot perspective).\n        if _is_instance(Number, data, condition=\"all\") \\\n                and not _is_instance(bool, data, condition=\"any\") \\\n                and _is_instance(Number, time, condition=\"all\")\\\n                and not _is_instance(bool, time, condition=\"any\"):\n            fig_list.append(figure(title=title, y_axis_label=y_axis_label[0], **style_figure))\n            fig_list[-1].line(time, data, legend=legend, **style_line)\n        else:\n            raise RuntimeError(\"At least one of the list elements, specified in data or time, is \"\n                               \"not numeric.\")\n\n    # Application of the OpenSignals Sytle.\n    if apply_opensignals_style is True:\n        opensignals_style([item for sublist in fig_list for item in sublist])\n\n    # Show of plots.\n    if grid_plot is True:\n        # Generation of the gridplot.\n        grid = gridplot(fig_list, **style_gridplot)\n\n        if show_plot is True:\n            show(grid)\n        else:\n            raise RuntimeError(\"The specified number of lines and columns for the grid plot is not \"\n                               \"compatible.\")\n\n    else:\n        if show_plot is True:\n            show(fig_list[-1])\n\n    return fig_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot(*args, legend=None, title=None, x_axis_label=\"Time (s)\", y_axis_label=None,\n         grid_plot=False, grid_lines=None, grid_columns=None, hor_lines=None, hor_lines_leg=None,\n         vert_lines=None, vert_lines_leg=None, apply_opensignals_style=True, show_plot=True,\n         warn_print=False, get_fig_list=False, file_name=None, **kwargs):\n    \"\"\"\n    -----\n    Brief\n    -----\n    Plotting function intended for an easy representation of OpenSignals acquired data.\n\n    -----------\n    Description\n    -----------\n    This function allows to plot data acquired with resource to OpenSignals, available at\n    https://bitalino.com/en/software, in an easy way using the Bokeh library (https://bokeh.pydata.org/en/latest/).\n    The plot will automatically present the OpenSignals style if the corresponding parameter is set to True\n    (apply_opensignals_style=True).\n\n    This allows to easily use multiple plot elements without the need to know any visualization libraries.\n\n    ----------\n    Parameters\n    ----------\n    *args: list\n        Variable number of arguments with the purpose of giving the user the possibility of\n        defining as an input only the \"data\" axis or both \"time\" and \"data\" axes.\n\n    legend : list\n        Input where the legend of each plot is specified. Should have the same shape of time.\n\n    title : list\n        Input where the title of each figure is specified. Should have the same shape of time.\n\n    x_axis_label : list\n        Input where the x label of each figure is specified. All figures will have the same x label\n        in the current implementation.\n\n    y_axis_label : list\n        Input where the y label of each figure is specified. Should have a length equal to the\n        number of figures.\n\n    grid_plot : boolean\n        If True then the plots will be organized in a grid_plot structure.\n\n    grid_lines : int\n        Number of lines of grid plot.\n\n    grid_columns : int\n        Number of columns of grid plot.\n\n    hor_lines : list of lists\n        The parent list must have the same shape of time and each of its elements (child lists)\n        must be formed by numbers defining the y axis position of the horizontal lines.\n\n    hor_lines_leg : list of lists\n        Legends of the horizontal lines (same shape of hor_lines).\n\n    vert_lines : list of lists\n        The parent list must have the same shape of time and each of its elements (child lists)\n        must be formed by numbers defining the x axis position of the vertical lines.\n\n    vert_lines_leg : list of lists\n        Legends of the vertical lines (same shape of vert_lines).\n\n    apply_opensignals_style : boolean\n        If True then the OpenSignals style will be applied.\n\n\n    show_plot : boolean\n        If True the generated figures will be shown.\n\n    warn_print : bool\n        If True some warnings about invalid kwargs keys will be prompted.\n\n    get_fig_list : bool\n        If True then it will be returned a list containing the figure objects generated during\n        the function execution.\n\n    file_name : str\n        Path containing the destination folder where the Bokeh figure will be stored.\n\n    **kwargs : dict\n        Keyword values for applying in bokeh figures, lines and gridplots.\n\n    Returns\n    -------\n    out : bokeh figure or gridplot\n        Object that is produced during the execution of the present function.\n\n    \"\"\"\n\n    # Generation of the HTML file where the plot will be stored.\n    #file_name = _generate_bokeh_file(file_name)\n\n    # Data conversion for ensuring that the function only works with lists.\n    if len(args) == 1:\n        time = numpy.linspace(1, len(args[0]) + 1, len(args[0]))\n        data = args[0]\n    elif len(args) == 2:\n        time = list(args[0])\n        data = list(args[1])\n    else:\n        raise RuntimeError(\"biosignalsnotebooks plot function only accepts 1 or 2 arguments in *args\"\n                           \" input. If only 1 input is given it should be a list with data samples,\"\n                           \"otherwise if 2 inputs are given then the first one defines the time\"\n                           \"axis and the second one data values.\")\n\n    # This function offers two input mechanisms (easy and complex). The easiest one consists in\n    # the representation of a single plot in a single figure, so, the user only needs to specify as\n    # inputs \"time\" and \"data\" lists. On the other hand, for the complex mechanism, the user can\n    # represent plots in different figures, using for that lists of lists as \"time\" and \"data\"\n    # inputs.\n    # In the following lines is ensured that independently of the input given, the function will\n    # achieve is purpose correctly.\n    if _is_instance(Number, data, condition=\"all\") and not _is_instance(bool, data, condition=\"any\") \\\n            and _is_instance(Number, time, condition=\"all\") \\\n            and not _is_instance(bool, time, condition=\"any\"):\n        time = [time]\n        data = [data]\n        if y_axis_label is not None:\n            y_axis_label = [y_axis_label]\n        if hor_lines is not None:\n            hor_lines = [hor_lines]\n        if hor_lines_leg is not None:\n            hor_lines_leg = [hor_lines_leg]\n        if vert_lines is not None:\n            vert_lines = [vert_lines]\n        if vert_lines_leg is not  None:\n            vert_lines_leg = [vert_lines_leg]\n        if title is not None:\n            title = [title]\n        if legend is not None:\n            legend = [legend]\n    elif _is_instance(numpy.ndarray, data, condition=\"any\") \\\n            or _is_instance(numpy.ndarray, time, condition=\"any\"):\n        time = list(map(list, time))\n        data = list(map(list, data))\n\n    # Ensures the application or not of opensignals graphical style.\n    if apply_opensignals_style is True:\n        style_figure = {**opensignals_kwargs(\"figure\"), **_filter_keywords(FigureOptions, kwargs,\n                                                                           is_class=True,\n                                                                           warn_print=warn_print)}\n        style_line = {**opensignals_kwargs(\"line\"), **_filter_keywords(Line, kwargs,\n                                                                       warn_print=warn_print)}\n        style_gridplot = {**opensignals_kwargs(\"gridplot\"),\n                          **_filter_keywords(gridplot, kwargs, warn_print=warn_print)}\n    else:\n        style_figure = _filter_keywords(FigureOptions, kwargs, is_class=True, warn_print=warn_print)\n        style_line = _filter_keywords(Line, kwargs, warn_print=warn_print)\n        style_gridplot = _filter_keywords(gridplot, kwargs, warn_print=warn_print)\n\n    # ------------------------ Verification if the input arguments are valid ----------------------\n    if legend is not None:\n        if isinstance(legend, list):\n            if len(legend) != len(time) or len(legend) != len(data):\n                raise RuntimeError(\"The shape of legend does not match with time input.\")\n        else:\n            raise RuntimeError(\"The specified data type of legend field is not valid. Input must \"\n                               \"be a list.\")\n    else:\n        legend = [None] * len(time)\n\n    if title is not None:\n        if isinstance(title, list):\n            if len(title) != len(time) or len(title) != len(data):\n                raise RuntimeError(\"The shape of title does not match with time input.\")\n        elif isinstance(title, str):\n            if grid_plot is True:\n                raise RuntimeError(\"Each figure of the gridplot must have a title, i.e., the shape\"\n                                   \" of time, data and title inputs needs to match.\")\n            else:\n                title = [title] * len(time)\n        elif grid_plot is False and len(title) != 1:\n            raise RuntimeError(\"The number of titles is not compatible with the number of figures \"\n                               \"(only one title is needed).\")\n        else:\n            raise RuntimeError(\"The specified data type of title field is not valid. Input must be \"\n                               \"a list.\")\n    else:\n        title = [None] * len(time)\n\n    if y_axis_label is not None:\n        if isinstance(y_axis_label, list):\n            if len(y_axis_label) != len(time) or len(y_axis_label) != len(data):\n                raise RuntimeError(\"The shape of y_axis_label does not match with time input.\")\n        elif isinstance(y_axis_label, str):\n            y_axis_label = [y_axis_label] * len(time)\n        elif grid_plot is False and len(y_axis_label) != 1:\n            raise RuntimeError(\"The number of y axis labels is not compatible with the number of \"\n                               \"figures.\")\n        else:\n            raise RuntimeError(\"The specified data type of y_axis_label field is not valid. Input \"\n                               \"must be a list or a string when grid_plot field is False.\")\n    else:\n        y_axis_label = [None] * len(time)\n\n    # Coherence between grid_plot, grid_lines and grid_columns inputs.\n    if grid_lines is not None or grid_columns is not None:\n        if grid_plot is not True:\n            raise RuntimeError(\"When grid_lines and grid_columns inputs are used the field grid_\"\n                               \"plot must be True.\")\n        else:\n            if not isinstance(grid_lines, int) or not isinstance(grid_columns, int):\n                raise RuntimeError(\"At least one of the grid_lines or grid_columns values is not \"\n                                   \"an integer.\")\n\n    # Horizontal Lines.\n    if hor_lines is not None:\n        if isinstance(hor_lines, list):\n            if len(hor_lines) != len(time) or len(hor_lines) != len(data):\n                raise RuntimeError(\"The shape of hor_lines does not match with time input.\")\n        else:\n            raise RuntimeError(\"The specified data type of hor_lines field is not valid. Input \"\n                               \"must be a list of lists.\")\n\n        # Each sublist entry must be numeric.\n        for cell in hor_lines:\n            if not _is_instance(Number, cell, condition=\"all\") \\\n                    or _is_instance(bool, cell, condition=\"any\"):\n                raise RuntimeError(\"At least one of the list elements, specified in hor_lines, \"\n                                   \"is not numeric.\")\n            elif vert_lines_leg is not None:\n                if len(hor_lines) != len(hor_lines_leg):\n                    raise RuntimeError(\"The shape of hor_lines and hor_lines_leg is not the same.\")\n\n    # Vertical Lines.\n    if vert_lines is not None:\n        if isinstance(vert_lines, list):\n            if len(vert_lines) != len(time) or len(vert_lines) != len(data):\n                raise RuntimeError(\"The shape of vert_lines does not match with time input.\")\n        else:\n            raise RuntimeError(\"The specified data type of vert_lines field is not valid. \"\n                               \"Input must be a list of lists.\")\n\n        # Each sublist entry must be numeric.\n        for cell in vert_lines:\n            if not _is_instance(Number, cell, condition=\"all\") \\\n                    or _is_instance(bool, cell, condition=\"any\"):\n                raise RuntimeError(\"At least one of the list elements, specified in vert_lines, \"\n                                   \"is not numeric.\")\n            elif vert_lines_leg is not None:\n                if len(vert_lines) != len(vert_lines_leg):\n                    raise RuntimeError(\"The shape of vert_lines and vert_lines_leg is not \"\n                                       \"the same.\")\n\n    # --------------------------------------- Plotting Stage --------------------------------------\n    fig_list = []\n    # If all data entries are lists, then it is considered that we are in a multiplot situation.\n    if _is_instance(list, data, condition=\"all\") and _is_instance(list, time, condition=\"all\"):\n        for list_entry in range(0, len(time)):  # Generation of a figure per plot.\n            # Verification if all elements inside list are numbers.\n            if _is_instance(Number, data[list_entry], condition=\"all\") \\\n                    and not _is_instance(bool, data[list_entry], condition=\"any\") \\\n                    and _is_instance(Number, time[list_entry], condition=\"all\") \\\n                    and not _is_instance(bool, time[list_entry], condition=\"any\"):\n                if len(time[list_entry]) == len(data[list_entry]):  # Shape verification\n                    if grid_plot is True:  # Generation of multiple figures.\n                        fig_list.append(figure(title=title[list_entry],\n                                               y_axis_label=y_axis_label[list_entry],\n                                               x_axis_label=x_axis_label,\n                                               **style_figure))\n                    elif grid_plot is False and list_entry == 0:\n                        fig_list.append(figure(title=title[list_entry],\n                                               y_axis_label=y_axis_label[list_entry],\n                                               x_axis_label=x_axis_label,\n                                               sizing_mode='scale_both',\n                                               **style_figure))\n\n                    fig_list[-1].line(time[list_entry], data[list_entry], legend=legend[list_entry],\n                                      **style_line)\n\n                    # Representation of horizontal lines.\n                    if hor_lines is not None:\n                        for hor_line_nbr, hor_line in enumerate(hor_lines[list_entry]):\n                            if hor_lines_leg is not None:\n                                fig_list[-1].line([time[list_entry][0], time[list_entry][-1]],\n                                                  [hor_line, hor_line],\n                                                  legend=hor_lines_leg[list_entry][hor_line_nbr],\n                                                  **opensignals_kwargs(\"line\"))\n                            else:\n                                fig_list[-1].line([time[list_entry][0], time[list_entry][-1]],\n                                                  [hor_line, hor_line],\n                                                  **opensignals_kwargs(\"line\"))\n\n                    # Representation of vertical lines.\n                    if vert_lines is not None:\n                        for vert_line_nbr, vert_line in enumerate(vert_lines[list_entry]):\n                            if vert_lines_leg is not None:\n                                fig_list[-1].line([vert_line, vert_line],\n                                                  [numpy.min(data[list_entry]),\n                                                   numpy.max(data[list_entry])],\n                                                  legend=vert_lines_leg[list_entry][vert_line_nbr],\n                                                  **opensignals_kwargs(\"line\"))\n                            else:\n                                fig_list[-1].line([vert_line, vert_line],\n                                                  [numpy.min(data[list_entry]),\n                                                   numpy.max(data[list_entry])],\n                                                  **opensignals_kwargs(\"line\"))\n\n                    # Update of line style.\n                    if apply_opensignals_style is True:\n                        style_line = {**opensignals_kwargs(\"line\"),\n                                      **_filter_keywords(Line, kwargs, warn_print=warn_print)}\n                    else:\n                        style_line = _filter_keywords(Line, kwargs, warn_print=warn_print)\n\n                else:\n                    raise RuntimeError(\"The shape of time and data inputs does not match.\")\n            else:\n                raise RuntimeError(\"At least one of the list elements, specified in data or time, \"\n                                   \"is not numeric.\")\n\n    # If this happen, then we receive as input a single list for time and data\n    # (Single plot perspective).\n    elif _is_instance(Number, data, condition=\"all\") \\\n            and not _is_instance(bool, data, condition=\"any\") \\\n            and _is_instance(Number, time, condition=\"all\") \\\n            and not _is_instance(bool, time, condition=\"any\"):\n        grid_plot = False\n\n        # Verification if all elements inside list are numbers.\n        if _is_instance(Number, data, condition=\"all\") \\\n                and not _is_instance(bool, data, condition=\"any\") \\\n                and _is_instance(Number, time, condition=\"all\") \\\n                and not _is_instance(bool, time, condition=\"any\"):\n            if len(time) == len(data):  # Shape verification\n                fig_list.append(figure(title=title[0], y_axis_label=y_axis_label[0],\n                                       x_axis_label=x_axis_label, **style_figure))\n                fig_list[-1].line(time, data, legend=legend[0], **style_line)\n            else:\n                raise RuntimeError(\"The shape of time and data inputs does not match.\")\n        else:\n            raise RuntimeError(\"At least one of the list elements, specified in data or time, is \"\n                               \"not numeric.\")\n\n    else:\n        raise RuntimeError(\"The input 'data' or/and 'time' does not have a valid format. It should \"\n                           \"be a list of numbers or a list of lists.\")\n\n    # Application of the OpenSignals Style.\n    if apply_opensignals_style is True:\n        opensignals_style(fig_list)\n\n    # Show of plots.\n    if grid_plot is True:\n        nbr_of_spaces = grid_lines * grid_columns\n        nbr_of_figures = len(fig_list)\n\n        if nbr_of_spaces >= nbr_of_figures > (grid_lines - 1) * grid_columns:\n            # Organization of data accordingly to the number of rows and columns specified as input\n            # arguments.\n            grid_layout = []\n            fig_nbr = 0\n            for row in range(0, grid_lines):\n                grid_layout.append([])\n                for column in range(0, grid_columns):\n                    if fig_nbr <= nbr_of_figures - 1:\n                        grid_layout[-1].append(fig_list[fig_nbr])\n                    else:\n                        grid_layout[-1].append(None)\n\n                    # Update of auxiliary variable.\n                    fig_nbr += 1\n\n            # Generation of the gridplot.\n            grid = gridplot(grid_layout, **style_gridplot)\n\n            if show_plot is True:\n                show(grid)\n            #else:\n            #    save(grid)\n                #return HTML('<iframe width=100% height=350 src=\"generated_plots/' + file_name + '\"></iframe>')\n        else:\n            raise RuntimeError(\"The specified number of lines and columns for the grid plot is not \"\n                               \"compatible.\")\n\n    else:\n        if show_plot is True:\n            show(fig_list[-1])\n        #else:\n        #    save(fig_list[-1])\n            #return HTML('<iframe width=100% height=\"' + str(fig_list[-1].plot_height) + '\" src=\"generated_plots/' + file_name + '\"></iframe>')\n\n    if get_fig_list is True:\n        return fig_list", "response": "A basic function that can be used to plot data acquired with OpenSignals."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef opensignals_style(figure_list, grid_plot=None, toolbar=\"right\"):\n\n    for fig in figure_list:\n        fig.background_fill_color = (242, 242, 242)\n\n        fig.toolbar.active_scroll = fig.select_one(WheelZoomTool)\n\n        # Removal of unnecessary tools.\n        figure_tools = fig.tools\n        for tool in range(len(figure_tools) - 1, -1, -1):\n            if not isinstance(figure_tools[tool], (type(PanTool()), type(BoxZoomTool()),\n                                                   type(WheelZoomTool()), type(ResetTool()))):\n                del figure_tools[tool]\n\n        fig.sizing_mode = 'scale_width'\n        fig.height = 200\n        fig.toolbar.logo = None\n        fig.toolbar_location = toolbar\n\n        fig.xgrid.grid_line_color = (150, 150, 150)\n        fig.ygrid.grid_line_color = (150, 150, 150)\n\n        fig.xgrid.grid_line_dash = [2, 2]\n\n        fig.xaxis.major_tick_line_color = \"white\"\n        fig.xaxis.minor_tick_line_color = \"white\"\n        fig.xaxis.axis_line_color = \"white\"\n        fig.yaxis.major_tick_in = 0\n        fig.yaxis.major_tick_out = 0\n\n        fig.yaxis.major_tick_line_color = \"white\"\n        fig.yaxis.minor_tick_line_color = \"white\"\n        fig.yaxis.minor_tick_in = 0\n        fig.yaxis.minor_tick_out = 0\n        fig.yaxis.axis_line_color = (150, 150, 150)\n        fig.yaxis.axis_line_dash = [2, 2]\n\n        fig.yaxis.major_label_text_color = (88, 88, 88)\n        fig.xaxis.major_label_text_color = (88, 88, 88)\n\n        fig.ygrid.grid_line_dash = [2, 2]\n\n    if isinstance(grid_plot, list):\n        if grid_plot:\n            for g_plot in grid_plot:\n                g_plot.sizing_mode = 'scale_width'\n                g_plot.height = 600", "response": "This function applies the Opensignals graphical style to a set of figures and grid plots."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef opensignals_kwargs(obj):\n\n    out = None\n    if obj == \"figure\":\n        out = {}\n    elif obj == \"gridplot\":\n        out = {\"toolbar_options\": {\"logo\": None}, \"sizing_mode\": 'scale_width'}\n    elif obj == \"line\":\n        out = {\"line_width\": 2, \"line_color\": opensignals_color_pallet()}\n\n    return out", "response": "Returns a dictionary with keyword arguments for OpenSignals graphical style."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning that verifies that an input of function plot has a valid structure.", "response": "def _check_validity_of_inputs(data, input_arg, input_name, grid_plot, dimension):\n    \"\"\"\n    Function that verifies when an input ('input_arg') of function 'plot' has a valid structure.\n\n    ----------\n    Parameters\n    ----------\n    data : list or list of lists\n        Structure with the data that will be plotted.\n\n    input_arg : list or list of lists\n        The input data to be verified.\n\n    input_name : str\n        Name of the input_arg variable.\n\n    grid_plot : bool\n        A flag that identifies when the input_arg is a matrix or not.\n\n    dimension : int\n        Level of verification in the matrix format structure.\n\n    Returns\n    -------\n    out : list or list of lists\n        Returns the same value as input_arg or a modified version.\n    \"\"\"\n    if input_arg is not None:\n        if grid_plot is True:\n            if isinstance(input_arg, list):\n                if numpy.shape(input_arg)[:dimension] != numpy.shape(data)[:dimension]:\n                    raise RuntimeError(\"The shape of \" + input_name + \" does not match with data \"\n                                       \"input.\")\n\n            else:\n                raise RuntimeError(\"The specified data type of \" + input_name +\n                                   \" field is not valid. Input must be a list.\")\n        else:\n            if not isinstance(input_arg, str):\n                raise RuntimeError(\"Taking into account that only one time-series had been \"\n                                   \"specified at 'data', the \" + input_name + \" field must be a \"\n                                   \"string\")\n    elif grid_plot is True:\n        input_arg = numpy.ndarray(shape=numpy.shape(data)[:dimension], dtype=numpy.object)\n\n    return input_arg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sendFragmentStart(self, data):\r\n      opcode = BINARY\r\n      if isinstance(data, unicode):\r\n         opcode = TEXT\r\n      self._sendMessage(True, opcode, data)", "response": "Send the start of a data fragment stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sendMessage(self, data):\r\n      opcode = BINARY\r\n      if isinstance(data, unicode):\r\n         opcode = TEXT\r\n      self._sendMessage(False, opcode, data)", "response": "Send a message to the client."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_sync_h5_file(in_paths, channels=('channel_1', 'channel_1'), new_path='sync_file.h5'):\n\n    if type(in_paths) == list or type(in_paths) == str:\n        new_file = _create_h5_file(in_paths, new_path)\n    else:\n        raise TypeError('The path should be a list of str or a str.')\n\n    data, devices = [[], []]\n    for j, i in enumerate(list(new_file.keys())):\n        devices.append(i)\n        data.append(np.concatenate(new_file[i].get('raw').get(channels[j]))[:])\n    phase, s1, s2 = synchronise_signals(data[0], data[1])\n\n    if np.array_equal(s1, data[0]):\n        # Change the second device\n        new_device = new_file[devices[1]]\n    elif np.array_equal(s2, data[1]):\n        # Change the first device\n        new_device = new_file[devices[0]]\n    raw_digital = ['raw', 'digital', 'support']\n    for r_d in raw_digital:\n        raw_di = new_device[r_d]\n        for key in list(raw_di.keys()):\n            if r_d != 'support':\n                signal = list(raw_di[key])[phase:]\n                attribute = list(raw_di[key].attrs.items())\n                del raw_di[key]\n                raw_di.create_dataset(name=key, data=np.array(signal, dtype=np.int32))\n                for a in attribute:\n                    raw_di[key].attrs.__setitem__(name=a[0], value=a[1])\n            else:\n                for other_key in list(raw_di[key].keys()):\n                    channel = raw_di[key][other_key]\n                    level = int(key.split('_')[-1])\n                    for yak in list(channel.keys()):\n                        if yak is not 't':\n                            data_aux = list(channel[yak])[int(np.ceil(phase/level)):]\n                        else:\n                            data_aux = list(channel[yak])[:len(channel[yak][:])-int(np.ceil(phase / level))]\n                        del channel[yak]\n                        channel.create_dataset(name=yak, data=np.array(data_aux))\n\n    length = abs(len(new_file[devices[0]]['raw']['channel_1']) - len(new_file[devices[1]]['raw']['channel_1']))\n\n    if len(new_file[devices[0]]['raw']['channel_1']) > len(new_file[devices[1]]['raw']['channel_1']):\n        index = 1\n    else:\n        index = 0\n\n    for r_d in raw_digital:\n        raw_di = new_file[devices[index]][r_d]\n        for key in list(raw_di.keys()):\n            if r_d != 'support':\n                if 'channel' in key or 'digital' in key:\n                    signal = np.concatenate([list(raw_di[key]), np.zeros([length, 1])])\n                elif 'nSeq' in key:\n                    signal = np.arange(len(raw_di[key]) + length)\n                attribute = list(raw_di[key].attrs.items())\n                del raw_di[key]\n                raw_di.create_dataset(name=key, data=np.array(signal, dtype=np.int32))\n                for a in attribute:\n                    raw_di[key].attrs.__setitem__(name=a[0], value=a[1])\n            else:\n                for other_key in list(raw_di[key].keys()):\n                    channel = raw_di[key][other_key]\n                    level = int(key.split('_')[-1])\n                    for yak in list(channel.keys()):\n                        if yak is not 't':\n                            data_aux = np.vstack([list(channel[yak]),\n                                                  np.zeros(int(length / level)).reshape(-1, 1)])\n                        else:\n                            data_aux = np.arange(0, len(channel[yak][:])+length//level).reshape(-1, 1)*level\n\n                        del channel[yak]\n                        channel.create_dataset(name=yak, data=np.array(data_aux, dtype=int))\n\n    final_length = len(np.ravel(signal))\n    for i in list(new_file.keys()):\n        new_file[i].attrs['duration'] = str(int(final_length // new_file[i].attrs['sampling rate'])) + ' s'\n        new_file[i].attrs.modify(name='nsamples', value=int(final_length))\n    new_file.close()", "response": "This function generates a new H5 file with synchronised signals from the input files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _shape_array(array1, array2):\n    if len(array1) > len(array2):\n        new_array = array2\n        old_array = array1\n    else:\n        new_array = array1\n        old_array = array2\n\n    length = len(old_array) - len(new_array)\n\n    for i in range(length):\n        n = new_array[-1].copy()\n        n[0::3] += 1\n        n[2::3] = 0\n        new_array = np.vstack([new_array, [n]])\n\n    arrays = np.hstack([old_array, new_array])\n    return arrays", "response": "Function that equalises the input arrays by zero - padding the shortest one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_h5_file_old(in_paths, new_path):\n    if type(in_paths) == str:\n        in_paths = [in_paths]\n    new_file = File(new_path, 'w')\n    for in_path in in_paths:\n        with File(in_path) as file:\n            for attribute in (list(file.attrs.items())):\n                new_file.attrs.__setitem__(name=attribute[0], value=attribute[1])\n            for key in list(file.keys()):\n                new_file.create_group(key)\n                for attribute in list(file[key].attrs.items()):\n                    new_file[key].attrs.__setitem__(name=attribute[0], value=attribute[1])\n                for other_key in list(file[key].keys()):\n                    new_file.create_group(key + '/' + other_key)\n                    for attribute in list(file[key + '/' + other_key].attrs.items()):\n                        new_file[key + '/' + other_key].attrs.__setitem__(name=attribute[0], value=attribute[1])\n                    for another_key in list(file[key][other_key]):\n                        try:\n                            for yet_another_key in list(file[key][other_key][another_key].keys()):\n                                try:\n                                    for y in list(file[key][other_key][another_key][yet_another_key].keys()):\n                                        new_file.create_dataset(name=key + '/' + other_key + '/' + another_key + '/'\n                                                                     + yet_another_key + '/' + y,\n                                                                data=file[key][other_key][another_key][yet_another_key][\n                                                                    y])\n                                        for attribute in list(\n                                                file[key][other_key][another_key][yet_another_key].attrs.items()):\n                                            new_file[key + '/' + other_key + '/' + another_key + '/'\n                                                     + yet_another_key + '/' + y].attrs.__setitem__(\n                                                name=attribute[0],\n                                                value=attribute[1])\n                                except:\n                                    new_file.create_dataset(\n                                        name=key + '/' + other_key + '/' + another_key + '/' + yet_another_key,\n                                        data=file[key][other_key][another_key][yet_another_key])\n                                    for attribute in list(\n                                            file[\n                                                key + '/' + other_key + '/' + another_key + '/' + yet_another_key].attrs.items()):\n                                        new_file[\n                                            key + '/' + other_key + '/' + another_key + '/' + yet_another_key].attrs.__setitem__(\n                                            name=attribute[0],\n                                            value=attribute[1])\n                        except:\n                            new_file.create_dataset(name=key + '/' + other_key + '/' + another_key,\n                                                    data=list(file[key][other_key][another_key]))\n                            for attribute in list(file[key + '/' + other_key + '/' + another_key].attrs.items()):\n                                new_file[key + '/' + other_key + '/' + another_key].attrs.__setitem__(name=attribute[0],\n                                                                                                      value=attribute[\n                                                                                                          1])\n\n    return new_file", "response": "Function to create a new. h5 file that contains the copy of the contents of the input files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to create a new. h5 file containing the copy of the contents of the input files.", "response": "def _create_h5_file(in_paths, new_path):\n    \"\"\"\n    Function to create a new .h5 file that contains the copy of the contents of the input file(s).\n\n    in_paths : str or list\n        If the input is a string, it is assumed that the two signals are in the same file, else, if the input is a list,\n        it is assumed that the two signals are in different file (the list should contain the paths to the two files).\n    new_path : str\n        The path to create the new file. (default: 'sync_file.h5')\n\n    Returns\n    -------\n    new_file : h5py Object\n        Object of the h5py package containing the new file containing the copy of the contents of the input file(s).\n    \"\"\"\n    if type(in_paths) == str:\n        in_paths = [in_paths]\n    new_file = File(new_path, 'w')\n    for i, in_path in enumerate(in_paths):\n        with File(in_path, 'r') as file:\n            for key in list(file.keys()):\n                file.copy(source=file[key], dest=new_file, name=key)\n\n    return new_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_txt_from_str(in_path, channels, new_path):\n    header = [\"# OpenSignals Text File Format\"]\n    files = [bsnb.load(in_path)]\n    with open(in_path, encoding=\"latin-1\") as opened_p:\n        header.append(opened_p.readlines()[1])\n    header.append(\"# EndOfHeader\")\n\n    data = []\n    nr_channels = []\n    for file in files:\n        for i, device in enumerate(file.keys()):\n            nr_channels.append(len(list(file[device])))\n            data.append(file[device][channels[i]])\n\n    dephase, s1, s2 = synchronise_signals(data[0], data[1])\n\n    new_header = [h.replace(\"\\n\", \"\") for h in header]\n    sync_file = open(new_path, 'w')\n    sync_file.write(' \\n'.join(new_header) + '\\n')\n\n    old_columns = np.loadtxt(in_path)\n    if np.array_equal(s1, data[0]):\n        # Change the second device\n        aux = 3 * nr_channels[0]\n        columns = old_columns[dephase:, aux:]\n        new_file = _shape_array(old_columns[:, :aux], columns)\n    elif np.array_equal(s2, data[1]):\n        # Change the first device\n        aux = 3 * nr_channels[1]\n        columns = old_columns[dephase:, :aux]\n        new_file = _shape_array(columns, old_columns[:, aux:])\n    else:\n        print(\"The devices are synchronised.\")\n        return\n    for line in new_file:\n        sync_file.write('\\t'.join(str(int(i)) for i in line) + '\\t\\n')\n    sync_file.close()", "response": "This function allows to create a text file with synchronised signals from the input file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender a string from a path template using the provided bindings.", "response": "def render(self, bindings):\n        \"\"\"Renders a string from a path template using the provided bindings.\n\n        Args:\n            bindings (dict): A dictionary of var names to binding strings.\n\n        Returns:\n            str: The rendered instantiation of this path template.\n\n        Raises:\n            ValidationError: If a key isn't provided or if a sub-template can't\n                be parsed.\n        \"\"\"\n        out = []\n        binding = False\n        for segment in self.segments:\n            if segment.kind == _BINDING:\n                if segment.literal not in bindings:\n                    raise ValidationException(\n                        ('rendering error: value for key \\'{}\\' '\n                         'not provided').format(segment.literal))\n                out.extend(PathTemplate(bindings[segment.literal]).segments)\n                binding = True\n            elif segment.kind == _END_BINDING:\n                binding = False\n            else:\n                if binding:\n                    continue\n                out.append(segment)\n        path = _format(out)\n        self.match(path)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmatching a fully qualified path template string.", "response": "def match(self, path):\n        \"\"\"Matches a fully qualified path template string.\n\n        Args:\n            path (str): A fully qualified path template string.\n\n        Returns:\n            dict: Var names to matched binding values.\n\n        Raises:\n            ValidationException: If path can't be matched to the template.\n        \"\"\"\n        this = self.segments\n        that = path.split('/')\n        current_var = None\n        bindings = {}\n        segment_count = self.segment_count\n        j = 0\n        for i in range(0, len(this)):\n            if j >= len(that):\n                break\n            if this[i].kind == _TERMINAL:\n                if this[i].literal == '*':\n                    bindings[current_var] = that[j]\n                    j += 1\n                elif this[i].literal == '**':\n                    until = j + len(that) - segment_count + 1\n                    segment_count += len(that) - segment_count\n                    bindings[current_var] = '/'.join(that[j:until])\n                    j = until\n                elif this[i].literal != that[j]:\n                    raise ValidationException(\n                        'mismatched literal: \\'%s\\' != \\'%s\\'' % (\n                            this[i].literal, that[j]))\n                else:\n                    j += 1\n            elif this[i].kind == _BINDING:\n                current_var = this[i].literal\n        if j != len(that) or j != segment_count:\n            raise ValidationException(\n                'match error: could not render from the path template: {}'\n                .format(path))\n        return bindings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(self, data):\n        self.binding_var_count = 0\n        self.segment_count = 0\n\n        segments = self.parser.parse(data)\n        # Validation step: checks that there are no nested bindings.\n        path_wildcard = False\n        for segment in segments:\n            if segment.kind == _TERMINAL and segment.literal == '**':\n                if path_wildcard:\n                    raise ValidationException(\n                        'validation error: path template cannot contain more '\n                        'than one path wildcard')\n                path_wildcard = True\n        return segments", "response": "Parses a path template string and returns a list of _Segment objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_unbound_segments(self, p):\n        p[0] = p[1]\n        if len(p) > 2:\n            p[0].extend(p[3])", "response": "unbound_segments : unbound_terminal FORWARD_SLASH unbound_segments\n                            | unbound_terminal"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef populate(self, priority, address, rtr, data):\n        self.needs_low_priority(priority)\n        self.needs_rtr(rtr)\n        self.needs_no_data(data)\n        self.set_attributes(priority, address, rtr)", "response": "Populates the object with the data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(window, root):\n        notifications = {}\n        _id = root.get_property(\"id\")\n        from foxpuppet.windows.browser.notifications import addons\n\n        notifications.update(addons.NOTIFICATIONS)\n        return notifications.get(_id, BaseNotification)(window, root)", "response": "Create a new notification object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides access to the notification label.", "response": "def label(self):\n        \"\"\"Provide access to the notification label.\n\n        Returns:\n            str: The notification label\n\n        \"\"\"\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            return self.root.get_attribute(\"label\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef origin(self):\n        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            return self.root.get_attribute(\"origin\")", "response": "Provide access to the notification origin."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the primary button.", "response": "def find_primary_button(self):\n        \"\"\"Retrieve the primary button.\"\"\"\n        if self.window.firefox_version >= 67:\n            return self.root.find_element(\n                By.CLASS_NAME, \"popup-notification-primary-button\")\n        return self.root.find_anonymous_element_by_attribute(\n            \"anonid\", \"button\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport a python file at the given path and executes it with the given arguments.", "response": "def inject_path(path):\n    \"\"\"\n    Imports :func: from a python file at :path: and executes it with *args, **kwargs arguments. Everytime this function\n    is called the module is reloaded so that you can alter your debug code while the application is running.\n\n    The result of the function is returned, otherwise the exception is returned (if one is raised)\n    \"\"\"\n    try:\n        dirname = os.path.dirname(path)\n        if dirname not in sys.path:\n            exists_in_sys = False\n            sys.path.append(dirname)\n        else:\n            exists_in_sys = True\n        module_name = os.path.splitext(os.path.split(path)[1])[0]\n        if module_name in sys.modules:\n            reload(sys.modules[module_name])\n        else:\n            __import__(module_name)\n        if not exists_in_sys:\n            sys.path.remove(dirname)\n    except Exception as e:\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inject_module(module, *args, **kwargs):\n\n    try:\n        parsed = module.split('.')\n        if len(parsed) == 1:\n            module_name, func_name = parsed[0], 'debug'\n        elif len(parsed) == 2:\n            module_name, func_name = parsed\n\n        if module_name in sys.modules:\n            mod = sys.modules[module_name]\n            reload(mod)\n        else:\n            mod = __import__(module_name)\n        f = getattr(mod, func_name, None)\n        if f:\n            return f(*args, **kwargs)\n    except Exception as e:\n        print e\n        return e", "response": "Imports a function from a python module and executes it with the specified arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_config(log_level: Union[str, int]) -> dict:\n    if isinstance(log_level, int):\n        # to match django\n        log_level = {3: 'DEBUG', 2: 'INFO'}.get(log_level, 'WARNING')\n    assert log_level in {'DEBUG', 'INFO', 'WARNING', 'ERROR'}, 'wrong log level %s' % log_level\n    return {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'default': {'format': '%(message)s'},\n            'indent': {'format': '    %(message)s'},\n        },\n        'handlers': {\n            'default': {\n                'level': log_level,\n                'class': 'grablib.common.ClickHandler',\n                'formatter': 'default'\n            },\n            'progress': {\n                'level': log_level,\n                'class': 'grablib.common.ProgressHandler',\n                'formatter': 'indent'\n            },\n        },\n        'loggers': {\n            main_logger.name: {\n                'handlers': ['default'],\n                'level': log_level,\n                'propagate': False,\n            },\n            progress_logger.name: {\n                'handlers': ['progress'],\n                'level': log_level,\n                'propagate': False,\n            },\n        },\n    }", "response": "Setup default config. for dictConfig."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the user authentication.", "response": "def authenticate(self):\n        \"\"\"Check the user authentication.\"\"\"\n        endpoint = os.path.join(self.config.get('napps', 'api'), 'auth', '')\n        username = self.config.get('auth', 'user')\n        password = getpass(\"Enter the password for {}: \".format(username))\n        response = requests.get(endpoint, auth=(username, password))\n        if response.status_code != 201:\n            LOG.error(response.content)\n            LOG.error('ERROR: %s: %s', response.status_code, response.reason)\n            sys.exit(1)\n        else:\n            data = response.json()\n            KytosConfig().save_token(username, data.get('hash'))\n            return data.get('hash')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npopulates the object with the data from the specified byte array.", "response": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        :return: None\n        \"\"\"\n        assert isinstance(data, bytes)\n        self.needs_low_priority(priority)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 7)\n        self.set_attributes(priority, address, rtr)\n        self.channel = self.byte_to_channel(data[0])\n        self.needs_valid_channel(self.channel, 5)\n        self.disable_inhibit_forced = data[1]\n        self.status = data[2]\n        self.led_status = data[3]\n        (self.delay_time,) = struct.unpack('>L', bytes([0]) + data[4:])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_json(self):\n        json_dict = self.to_json_basic()\n        json_dict['channel'] = self.channel\n        json_dict['disable_inhibit_forced'] = self.disable_inhibit_forced\n        json_dict['status'] = self.status\n        json_dict['led_status'] = self.led_status\n        json_dict['delay_time'] = self.delay_time\n        return json.dumps(json_dict)", "response": "Returns a JSON string representation of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_to_binary(self):\n        return bytes([\n            COMMAND_CODE,\n            self.channels_to_byte([self.channel]),\n            self.disable_inhibit_forced,\n            self.status,\n            self.led_status\n        ]) + struct.pack('>L', self.delay_time)[-3:]", "response": "Converts the object to binary format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads a report to the server.", "response": "def upload_report(server, payload, timeout=HQ_DEFAULT_TIMEOUT):\n    \"\"\"\n    Upload a report to the server.\n    :param payload: Dictionary (JSON serializable) of crash data.\n    :return: server response\n    \"\"\"\n    try:\n        data = json.dumps(payload)\n        r = requests.post(server + '/reports/upload', data=data, timeout=timeout)\n    except Exception as e:\n        logging.error(e)\n        return False\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_report(server, report_number, timeout=HQ_DEFAULT_TIMEOUT):\n    try:\n        r = requests.post(server + \"/reports/delete/%d\" % report_number, timeout=timeout)\n    except Exception as e:\n        logging.error(e)\n        return False\n\n    return r", "response": "Delete a specific crash report from the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef windows(self):\n        from foxpuppet.windows import BrowserWindow\n\n        return [\n            BrowserWindow(self.selenium, handle)\n            for handle in self.selenium.window_handles\n        ]", "response": "Return a list of all open windows."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_effects(self, effects_json=None):\n        # type: (dict) -> Any\n        \"\"\"Parse multiple effects from an effects(list) json.\"\"\"\n        if isinstance(effects_json, list):\n            return [ValidatorEffect.parse(effect) for effect in effects_json]\n        elif isinstance(effects_json, dict):\n            return ValidatorEffect.parse(effects_json)\n        else:\n            raise Exception(\"The provided json, should be a list of valid effects, \"\n                            \"or a single effect. Got '{}'\".format(effects_json))", "response": "Parse multiple effects from an effects ( list ) json."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(cls, json):\n        # type: (Dict) -> Any\n        \"\"\"Parse a json dict and return the correct subclass of :class:`PropertyValidator`.\n\n        It uses the 'effect' key to determine which :class:`PropertyValidator` to instantiate.\n        Please refer to :class:`pykechain.enums.PropertyVTypes` for the supported effects.\n\n        :param json: dictionary containing the specific keys to parse into a :class:`PropertyValidator`\n        :type json: dict\n        :returns: the instantiated subclass of :class:`PropertyValidator`\n        :rtype: :class:`PropertyValidator` or subclass thereof\n        \"\"\"\n        if 'vtype' in json:\n            vtype = json.get('vtype')\n            if vtype not in PropertyVTypes.values():\n                raise Exception(\"Validator unknown, incorrect json: '{}'\".format(json))\n\n            from pykechain.models.validators import validators\n            vtype_implementation_classname = \"{}{}\".format(vtype[0].upper(), vtype[1:])  # type: ignore\n            if hasattr(validators, vtype_implementation_classname):\n                return getattr(validators, vtype_implementation_classname)(json=json)\n            else:\n                raise Exception('unknown vtype in json')\n        raise Exception(\"Validator unknown, incorrect json: '{}'\".format(json))", "response": "Parse a json dict and return the correct subclass of PropertyValidator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_json(self):\n        # type: () -> dict\n        \"\"\"JSON representation of the effect.\n\n        :returns: a python dictionary, serializable as json of the effect\n        :rtype: dict\n        \"\"\"\n        new_json = dict(\n            vtype=self.vtype,\n            config=self._config\n        )\n\n        if self.on_valid:\n            new_json['config']['on_valid'] = [effect.as_json() for effect in self.on_valid]\n        if self.on_invalid:\n            new_json['config']['on_invalid'] = [effect.as_json() for effect in self.on_invalid]\n\n        self._json = new_json\n        return self._json", "response": "Returns a python dictionary serializable as json of the effect."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _logic(self, value=None):\n        # type: (Any) -> Tuple[Union[bool, None], str]\n        \"\"\"Process the inner logic of the validator.\n\n        The validation results are returned as tuple (boolean (true/false), reasontext)\n        \"\"\"\n        self._validation_result, self._validation_reason = None, 'No reason'\n        return self._validation_result, self._validation_reason", "response": "Process the inner logic of the validator."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(cls, json):\n        # type: (dict) -> Any\n        \"\"\"Parse a json dict and return the correct subclass of :class:`ValidatorEffect`.\n\n        It uses the 'effect' key to determine which :class:`ValidatorEffect` to instantiate.\n        Please refer to :class:`enums.ValidatorEffectTypes` for the supported effects.\n\n        :param json: dictionary containing the specific keys to parse into a :class:`ValidatorEffect`\n        :type json: dict\n        :returns: the instantiated subclass of :class:`ValidatorEffect`\n        :rtype: :class:`ValidatorEffect` or subclass\n        \"\"\"\n        effect = json.get('effect')\n        if effect:\n            from pykechain.models.validators import effects\n            effect_implementation_classname = effect[0].upper() + effect[1:]\n            if hasattr(effects, effect_implementation_classname):\n                return getattr(effects, effect_implementation_classname)(json=json)\n            else:\n                raise Exception('unknown effect in json')\n        raise Exception(\"Effect unknown, incorrect json: '{}'\".format(json))", "response": "Parse a json dict and return the correct subclass of a ValidatorEffect."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_json(self):\n        # type: () -> dict\n        \"\"\"JSON representation of the effect.\n\n        :returns: a python dictionary, serializable as json of the effect\n        :rtype: dict\n        \"\"\"\n        self._json = dict(\n            effect=self.effect,\n            config=self._config\n        )\n        return self._json", "response": "Returns a python dictionary of the effect."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the data to binary format.", "response": "def data_to_binary(self):\n        \"\"\"\n        :return: bytes\n        \"\"\"\n        return bytes([\n            COMMAND_CODE,\n            int(self.temp_type),\n            int(self.temp)\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef populate(self, priority, address, rtr, data):\n        assert isinstance(data, bytes)\n        self.needs_firmware_priority(priority)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 6)\n        self.set_attributes(priority, address, rtr)\n        self.module_type = data[0]\n        prefix = bytes([0, 0])\n        (self.current_serial,) = struct.unpack(\n            '>L', prefix + data[1] + data[2])\n        self.module_address = data[3]\n        (self.new_serial,) = struct.unpack('>L', prefix + data[4] + data[5])", "response": "Populates the object with the contents of the data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the data to binary format.", "response": "def data_to_binary(self):\n        \"\"\"\n        :return: bytes\n        \"\"\"\n        return chr(COMMAND_CODE) + chr(self.module_type) + \\\n            struct.pack('>L', self.current_serial)[2:] + \\\n            chr(self.module_address) + \\\n            struct.pack('>L', self.new_serial)[2:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_station_board(\n        self,\n        crs,\n        rows=17,\n        include_departures=True,\n        include_arrivals=False,\n        destination_crs=None,\n        origin_crs=None\n    ):\n        \"\"\"\n        Query the darwin webservice to obtain a board for a particular station\n        and return a StationBoard instance\n\n        Positional arguments:\n        crs -- the three letter CRS code of a UK station\n\n        Keyword arguments:\n        rows -- the number of rows to retrieve (default 10)\n        include_departures -- include departing services in the departure board\n        (default True)\n        include_arrivals -- include arriving services in the departure board\n        (default False)\n        destination_crs -- filter results so they only include services\n        calling at a particular destination (default None)\n        origin_crs -- filter results so they only include services\n        originating from a particular station (default None)\n        \"\"\"\n        # Determine the darwn query we want to make\n        if include_departures and include_arrivals:\n            query_type = 'GetArrivalDepartureBoard'\n        elif include_departures:\n            query_type = 'GetDepartureBoard'\n        elif include_arrivals:\n            query_type = 'GetArrivalBoard'\n        else:\n            raise ValueError(\n                \"get_station_board must have either include_departures or \\\ninclude_arrivals set to True\"\n            )\n        # build a query function\n        q = partial(self._base_query()[query_type], crs=crs, numRows=rows)\n        if destination_crs:\n            if origin_crs:\n                log.warn(\n                    \"Station board query can only filter on one of \\\ndestination_crs and origin_crs, using only destination_crs\"\n                )\n            q = partial(q, filterCrs=destination_crs, filterType='to')\n        elif origin_crs:\n            q = partial(q, filterCrs=origin_crs, filterType='from')\n        try:\n            soap_response = q()\n        except WebFault:\n            raise WebServiceError\n        return StationBoard(soap_response)", "response": "This method returns a StationBoard instance for a particular station."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of an individual service and return a ServiceDetails instance.", "response": "def get_service_details(self, service_id):\n        \"\"\"\n        Get the details of an individual service and return a ServiceDetails\n        instance.\n\n        Positional arguments:\n        service_id: A Darwin LDB service id\n        \"\"\"\n        service_query = \\\n            self._soap_client.service['LDBServiceSoap']['GetServiceDetails']\n        try:\n            soap_response = service_query(serviceID=service_id)\n        except WebFault:\n            raise WebServiceError\n        return ServiceDetails(soap_response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_state(self, channel):\n        val = None\n        if channel not in self._unit:\n            return val\n        if self._unit[channel] == 'l/h':\n            val = ((1000 * 3600) / (self._delay[channel] * self._pulses[channel]))\n        elif self._unit[channel] == 'm3/h':\n            val = ((1000 * 3600) / (self._delay[channel] * self._pulses[channel]))\n        elif self._unit[channel] == 'W':\n            val = ((1000 * 1000 * 3600) / (self._delay[channel] * self._pulses[channel]))\n            if val < 55:\n                val = 0\n        return round(val, 2)", "response": "Get the state of a specific channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders and save API doc in openapi. yml.", "response": "def render_template(self):\n        \"\"\"Render and save API doc in openapi.yml.\"\"\"\n        self._parse_paths()\n        context = dict(napp=self._napp.__dict__, paths=self._paths)\n        self._save(context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the decorated functions in the code.", "response": "def _parse_decorated_functions(self, code):\n        \"\"\"Return URL rule, HTTP methods and docstring.\"\"\"\n        matches = re.finditer(r\"\"\"\n                # @rest decorators\n                (?P<decorators>\n                    (?:@rest\\(.+?\\)\\n)+  # one or more @rest decorators inside\n                )\n                # docstring delimited by 3 double quotes\n                .+?\"{3}(?P<docstring>.+?)\"{3}\n                \"\"\", code, re.VERBOSE | re.DOTALL)\n\n        for function_match in matches:\n            m_dict = function_match.groupdict()\n            self._parse_docstring(m_dict['docstring'])\n            self._add_function_paths(m_dict['decorators'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_docstring(self, docstring):\n        match = re.match(r\"\"\"\n            # Following PEP 257\n            \\s* (?P<summary>[^\\n]+?) \\s*   # First line\n\n            (                              # Description and YAML are optional\n              (\\n \\s*){2}                  # Blank line\n\n              # Description (optional)\n              (\n                (?!-{3,})                     # Don't use YAML as description\n                \\s* (?P<description>.+?) \\s*  # Third line and maybe others\n                (?=-{3,})?                    # Stop if \"---\" is found\n              )?\n\n              # YAML spec (optional) **currently not used**\n              (\n                -{3,}\\n                       # \"---\" begins yaml spec\n                (?P<open_api>.+)\n              )?\n            )?\n            $\"\"\", docstring, re.VERBOSE | re.DOTALL)\n\n        summary = 'TODO write the summary.'\n        description = 'TODO write/remove the description'\n        if match:\n            m_dict = match.groupdict()\n            summary = m_dict['summary']\n            if m_dict['description']:\n                description = re.sub(r'(\\s|\\n){2,}', ' ',\n                                     m_dict['description'])\n        self._summary = summary\n        self._description = description", "response": "Parse the method docstring."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns HTTP method list. Use json for security reasons.", "response": "def _parse_methods(cls, list_string):\n        \"\"\"Return HTTP method list. Use json for security reasons.\"\"\"\n        if list_string is None:\n            return APIServer.DEFAULT_METHODS\n        # json requires double quotes\n        json_list = list_string.replace(\"'\", '\"')\n        return json.loads(json_list)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert relative Flask rule to absolute OpenAPI path.", "response": "def _rule2path(cls, rule):\n        \"\"\"Convert relative Flask rule to absolute OpenAPI path.\"\"\"\n        typeless = re.sub(r'<\\w+?:', '<', rule)  # remove Flask types\n        return typeless.replace('<', '{').replace('>', '}')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the current environment is on App Engine.", "response": "def on_app_engine():\n    \"\"\"\n    :return: bool\n    \"\"\"\n    if 'SERVER_SOFTWARE' in os.environ:\n        server_software = os.environ['SERVER_SOFTWARE']\n        if server_software.startswith('Google App Engine') or \\\n                server_software.startswith('Development'):\n            return True\n        return False\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a module with the registry.", "response": "def register_module(module_name, module_class):\n    \"\"\"\n    :return: None\n    \"\"\"\n    assert isinstance(module_name, str)\n    assert isinstance(module_class, type)\n    if module_name not in ModuleRegistry:\n        ModuleRegistry[module_name] = module_class\n    else:\n        raise Exception(\"double registration in module registry\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checksum(data):\n    assert isinstance(data, bytes)\n    assert len(data) >= MINIMUM_MESSAGE_SIZE - 2\n    assert len(data) <= MAXIMUM_MESSAGE_SIZE - 2\n    __checksum = 0\n    for data_byte in data:\n        __checksum += data_byte\n    __checksum = -(__checksum % 256) + 256\n    try:\n        __checksum = bytes([__checksum])\n    except ValueError:\n        __checksum = bytes([0])\n    return __checksum", "response": "Calculate checksum of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the property belonging to this part based on its name or uuid.", "response": "def property(self, name):\n        \"\"\"Retrieve the property belonging to this part based on its name or uuid.\n\n        :param name: property name or property UUID to search for\n        :type name: basestring\n        :return: a single :class:`Property`\n        :raises NotFoundError: if the `Property` is not part of the `Part`\n\n        Example\n        -------\n\n        >>> part = project.part('Bike')\n        >>> part.properties\n        [<pyke Property ...>, ...]\n        # this returns a list of all properties of this part\n\n        >>> gears = part.property('Gears')\n        >>> gears.value\n        6\n\n        >>> gears = part.property('123e4567-e89b-12d3-a456-426655440000')\n        >>> gears.value\n        6\n\n        \"\"\"\n        found = None\n        if is_uuid(name):\n            found = find(self.properties, lambda p: name == p.id)\n        else:\n            found = find(self.properties, lambda p: name == p.name)\n\n        if not found:\n            raise NotFoundError(\"Could not find property with name or id {}\".format(name))\n\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the parent of this Part.", "response": "def parent(self):\n        # type: () -> Any\n        \"\"\"Retrieve the parent of this `Part`.\n\n        :return: the parent :class:`Part` of this part\n        :raises APIError: if an Error occurs\n\n        Example\n        -------\n\n        >>> part = project.part('Frame')\n        >>> bike = part.parent()\n\n        \"\"\"\n        if self.parent_id:\n            return self._client.part(pk=self.parent_id, category=self.category)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef children(self, **kwargs):\n        if not kwargs:\n            # no kwargs provided is the default, we aim to cache it.\n            if not self._cached_children:\n                self._cached_children = list(self._client.parts(parent=self.id, category=self.category))\n            return self._cached_children\n        else:\n            # if kwargs are provided, we assume no use of cache as specific filtering on the children is performed.\n            return self._client.parts(parent=self.id, category=self.category, **kwargs)", "response": "Retrieve the children of this Part as Partset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the siblings of this Part as Partset.", "response": "def siblings(self, **kwargs):\n        # type: (Any) -> Any\n        \"\"\"Retrieve the siblings of this `Part` as `Partset`.\n\n        Siblings are other Parts sharing the same parent of this `Part`, including the part itself.\n\n        :param kwargs: Additional search arguments to search for, check :class:`pykechain.Client.parts`\n                       for additional info\n        :type kwargs: dict\n        :return: a set of `Parts` as a :class:`PartSet`. Will be empty if no siblings.\n        :raises APIError: When an error occurs.\n        \"\"\"\n        if self.parent_id:\n            return self._client.parts(parent=self.parent_id, category=self.category, **kwargs)\n        else:\n            from pykechain.models.partset import PartSet\n            return PartSet(parts=[])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the model of this Part as Part.", "response": "def model(self):\n        \"\"\"\n        Retrieve the model of this `Part` as `Part`.\n\n        For instance, you can get the part model of a part instance. But trying to get the model of a part that\n        has no model, like a part model, will raise a :exc:`NotFoundError`.\n\n        .. versionadded:: 1.8\n\n        :return: the model of this part instance as :class:`Part` with category `MODEL`\n        :raises NotFoundError: if no model found\n\n        Example\n        -------\n        >>> front_fork = project.part('Front Fork')\n        >>> front_fork_model = front_fork.model()\n\n        \"\"\"\n        if self.category == Category.INSTANCE:\n            model_id = self._json_data['model'].get('id')\n            return self._client.model(pk=model_id)\n        else:\n            raise NotFoundError(\"Part {} has no model\".format(self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the instances of this Part as a PartSet.", "response": "def instances(self, **kwargs):\n        \"\"\"\n        Retrieve the instances of this `Part` as a `PartSet`.\n\n        For instance, if you have a model part, you can get the list of instances that are created based on this\n        moodel. If there are no instances (only possible if the multiplicity is :attr:`enums.Multiplicity.ZERO_MANY`)\n        than a :exc:`NotFoundError` is returned\n\n        .. versionadded:: 1.8\n\n        :return: the instances of this part model :class:`PartSet` with category `INSTANCE`\n        :raises NotFoundError: if no instances found\n\n        Example\n        -------\n        >>> wheel_model = project.model('Wheel')\n        >>> wheel_instance_set = wheel_model.instances()\n\n        An example with retrieving the front wheels only using the 'name__contains' search argument.\n\n        >>> wheel_model = project.model('Wheel')\n        >>> front_wheel_instances = wheel_model.instances(name__contains='Front')\n\n        \"\"\"\n        if self.category == Category.MODEL:\n            return self._client.parts(model=self, category=Category.INSTANCE, **kwargs)\n        else:\n            raise NotFoundError(\"Part {} is not a model\".format(self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef instance(self):\n        instances_list = list(self.instances())\n        if len(instances_list) == 1:\n            return instances_list[0]\n        elif len(instances_list) > 1:\n            raise MultipleFoundError(\"Part {} has more than a single instance. \"\n                                     \"Use the `Part.instances()` method\".format(self.name))\n        else:\n            raise NotFoundError(\"Part {} has no instance\".format(self.name))", "response": "Retrieves the single instance of this Part as a Part."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef proxy_model(self):\n        if self.category != Category.MODEL:\n            raise IllegalArgumentError(\"Part {} is not a model, therefore it cannot have a proxy model\".format(self))\n        if 'proxy' in self._json_data and self._json_data.get('proxy'):\n            catalog_model_id = self._json_data['proxy'].get('id')\n            return self._client.model(pk=catalog_model_id)\n        else:\n            raise NotFoundError(\"Part {} is not a proxy\".format(self.name))", "response": "Retrieve the proxy model of this proxied Part as a Part."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, model, **kwargs):\n        # type: (Part, **Any) -> Part\n        \"\"\"Add a new child instance, based on a model, to this part.\n\n        This can only act on instances. It needs a model from which to create the child instance.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :type kwargs: dict or None\n        :type model: :class:`Part`\n        :param kwargs: (optional) additional keyword=value arguments\n        :type kwargs: dict\n        :return: :class:`Part` with category `INSTANCE`.\n        :raises APIError: if unable to add the new child instance\n\n        Example\n        -------\n\n        >>> bike = project.part('Bike')\n        >>> wheel_model = project.model('Wheel')\n        >>> bike.add(wheel_model)\n        \"\"\"\n        if self.category != Category.INSTANCE:\n            raise APIError(\"Part should be of category INSTANCE\")\n\n        return self._client.create_part(self, model, **kwargs)", "response": "Add a new child instance based on a model to this part."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_to(self, parent, **kwargs):\n        # type: (Part, **Any) -> Part\n        \"\"\"Add a new instance of this model to a part.\n\n        This works if the current part is a model and an instance of this model is to be added\n        to a part instances in the tree.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :param parent: part to add the new instance to\n        :param kwargs: (optional) additional kwargs that will be passed in the during the edit/update request\n        :type kwargs: dict or None\n        :type parent: :class:`Part`\n        :param kwargs: (optional) additional keyword=value arguments\n        :type kwargs: dict\n        :return: :class:`Part` with category `INSTANCE`\n        :raises APIError: if unable to add the new child instance\n\n        Example\n        -------\n\n        >>> wheel_model = project.model('wheel')\n        >>> bike = project.part('Bike')\n        >>> wheel_model.add_to(bike)\n        \"\"\"\n        if self.category != Category.MODEL:\n            raise APIError(\"Part should be of category MODEL\")\n\n        return self._client.create_part(parent, self, **kwargs)", "response": "Add a new instance of this model to a part."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_model(self, *args, **kwargs):\n        # type: (*Any, **Any) -> Part\n        \"\"\"Add a new child model to this model.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :return: a :class:`Part` of category `MODEL`\n        \"\"\"\n        if self.category != Category.MODEL:\n            raise APIError(\"Part should be of category MODEL\")\n\n        return self._client.create_model(self, *args, **kwargs)", "response": "Add a new child model to this model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_proxy_to(self, parent, name, multiplicity=Multiplicity.ONE_MANY, **kwargs):\n        # type: (Any, AnyStr, Any, **Any) -> Part\n        \"\"\"Add this model as a proxy to another parent model.\n\n        This will add the current model as a proxy model to another parent model. It ensure that it will copy the\n        whole subassembly to the 'parent' model.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :param name: Name of the new proxy model\n        :type name: basestring\n        :param parent: parent of the to be proxied model\n        :type parent: :class:`Part`\n        :param multiplicity: the multiplicity of the new proxy model (default ONE_MANY)\n        :type multiplicity: basestring or None\n        :param kwargs: (optional) additional kwargs that will be passed in the during the edit/update request\n        :type kwargs: dict or None\n        :return: the new proxied :class:`Part`.\n        :raises APIError: in case an Error occurs\n\n        Examples\n        --------\n        >>> from pykechain.enums import Multiplicity\n        >>> bike_model = project.model('Bike')\n        # find the catalog model container, the highest parent to create catalog models under\n        >>> catalog_model_container = project.model('Catalog container')\n        >>> new_wheel_model = project.create_model(catalog_model_container, 'Wheel Catalog',\n        ...                                        multiplicity=Multiplicity.ZERO_MANY)\n        >>> new_wheel_model.add_proxy_to(bike_model, \"Wheel\", multiplicity=Multiplicity.ONE_MANY)\n\n        \"\"\"\n        return self._client.create_proxy_model(self, parent, name, multiplicity, **kwargs)", "response": "Add this model as a proxy to another parent model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_property(self, *args, **kwargs):\n        # type: (*Any, **Any) -> Property\n        \"\"\"Add a new property to this model.\n\n        See :class:`pykechain.Client.create_property` for available parameters.\n\n        :return: :class:`Property`\n        :raises APIError: in case an Error occurs\n        \"\"\"\n        if self.category != Category.MODEL:\n            raise APIError(\"Part should be of category MODEL\")\n\n        return self._client.create_property(self, *args, **kwargs)", "response": "Add a new property to this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nedits the details of a Part instance.", "response": "def edit(self, name=None, description=None, **kwargs):\n        # type: (AnyStr, AnyStr, **Any) -> None\n        \"\"\"\n        Edit the details of a part (model or instance).\n\n        For an instance you can edit the Part instance name and the part instance description. To alter the values\n        of properties use :func:`Part.update()`.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :param name: optional name of the part to edit\n        :param description: (optional) description of the part\n        :type description: basestring or None\n        :param kwargs: (optional) additional kwargs that will be passed in the during the edit/update request\n        :type kwargs: dict or None\n        :return: the updated object if successful\n        :raises IllegalArgumentError: when the type or value of an argument provided is incorrect\n        :raises APIError: in case an Error occurs\n\n        Example\n        -------\n\n        For changing a part:\n\n        >>> front_fork = project.part('Front Fork')\n        >>> front_fork.edit(name='Front Fork - updated')\n        >>> front_fork.edit(name='Front Fork cruizer', description='With my ragtop down so my hair can blow' )\n\n        for changing a model:\n\n        >>> front_fork = project.model('Front Fork')\n        >>> front_fork.edit(name='Front Fork basemodel', description='Some description here')\n\n        \"\"\"\n        update_dict = {'id': self.id}\n        if name:\n            if not isinstance(name, str):\n                raise IllegalArgumentError(\"name should be provided as a string\")\n            update_dict.update({'name': name})\n        if description:\n            if not isinstance(description, str):\n                raise IllegalArgumentError(\"description should be provided as a string\")\n            update_dict.update({'description': description})\n\n        if kwargs:  # pragma: no cover\n            update_dict.update(**kwargs)\n        r = self._client._request('PUT', self._client._build_url('part', part_id=self.id), json=update_dict)\n\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not update Part ({})\".format(r))\n\n        if name:\n            self.name = name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, name=None, update_dict=None, bulk=True, **kwargs):\n        # dict(name=name, properties=json.dumps(update_dict))) with property ids:value\n        action = 'bulk_update_properties'\n\n        request_body = dict()\n        for prop_name_or_id, property_value in update_dict.items():\n            if is_uuid(prop_name_or_id):\n                request_body[prop_name_or_id] = property_value\n            else:\n                request_body[self.property(prop_name_or_id).id] = property_value\n\n        if bulk and len(update_dict.keys()) > 1:\n            if name:\n                if not isinstance(name, str):\n                    raise IllegalArgumentError(\"Name of the part should be provided as a string\")\n            r = self._client._request('PUT', self._client._build_url('part', part_id=self.id),\n                                      data=dict(name=name, properties=json.dumps(request_body), **kwargs),\n                                      params=dict(select_action=action))\n            if r.status_code != requests.codes.ok:  # pragma: no cover\n                raise APIError('{}: {}'.format(str(r), r.content))\n        else:\n            for property_name, property_value in update_dict.items():\n                self.property(property_name).value = property_value", "response": "Update the properties of a KE - chain version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new part and update its properties in one go.", "response": "def add_with_properties(self, model, name=None, update_dict=None, bulk=True, **kwargs):\n        \"\"\"\n        Add a part and update its properties in one go.\n\n        In order to prevent the backend from updating the frontend you may add `suppress_kevents=True` as\n        additional keyword=value argument to this method. This will improve performance of the backend\n        against a trade-off that someone looking at the frontend won't notice any changes unless the page\n        is refreshed.\n\n        :param model: model of the part which to add a new instance, should follow the model tree in KE-chain\n        :type model: :class:`Part`\n        :param name: (optional) name provided for the new instance as string otherwise use the name of the model\n        :type name: basestring or None\n        :param update_dict: dictionary with keys being property names (str) or property_id (from the property models)\n                            and values being property values\n        :type update_dict: dict or None\n        :param bulk: True to use the bulk_update_properties API endpoint for KE-chain versions later then 2.1.0b\n        :type bulk: boolean or None\n        :param kwargs: (optional) additional keyword arguments that will be passed inside the update request\n        :type kwargs: dict or None\n        :return: the newly created :class:`Part`\n        :raises NotFoundError: when the property name is not a valid property of this part\n        :raises APIError: in case an Error occurs\n\n        Examples\n        --------\n        >>> bike = client.scope('Bike Project').part('Bike')\n        >>> wheel_model = client.scope('Bike Project').model('Wheel')\n        >>> bike.add_with_properties(wheel_model, 'Wooden Wheel', {'Spokes': 11, 'Material': 'Wood'})\n\n        \"\"\"\n        if self.category != Category.INSTANCE:\n            raise APIError(\"Part should be of category INSTANCE\")\n        name = name or model.name\n        action = 'new_instance_with_properties'\n\n        properties_update_dict = dict()\n        for prop_name_or_id, property_value in update_dict.items():\n            if is_uuid(prop_name_or_id):\n                properties_update_dict[prop_name_or_id] = property_value\n            else:\n                properties_update_dict[model.property(prop_name_or_id).id] = property_value\n\n        if bulk:\n            r = self._client._request('POST', self._client._build_url('parts'),\n                                      data=dict(\n                                          name=name,\n                                          model=model.id,\n                                          parent=self.id,\n                                          properties=json.dumps(properties_update_dict),\n                                          **kwargs\n                                      ),\n                                      params=dict(select_action=action))\n\n            if r.status_code != requests.codes.created:  # pragma: no cover\n                raise APIError('{}: {}'.format(str(r), r.content))\n            return Part(r.json()['results'][0], client=self._client)\n        else:  # do the old way\n            new_part = self.add(model, name=name)  # type: Part\n            new_part.update(update_dict=update_dict, bulk=bulk)\n            return new_part"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the properties of a part inside a dict in this structure.", "response": "def as_dict(self):\n        \"\"\"\n        Retrieve the properties of a part inside a dict in this structure: {property_name: property_value}.\n\n        .. versionadded:: 1.9\n\n        :returns: the values of the properties as a `dict`\n        :rtype: dict\n\n        Example\n        -------\n        >>> front_wheel = client.scope('Bike Project').part('Front Wheel')\n        >>> front_wheel_properties = front_wheel.as_dict()\n        {'Diameter': 60.8,\n         'Spokes': 24,\n         'Rim Material': 'Aluminium',\n         'Tire Thickness': 4.2}\n\n        \"\"\"\n        properties_dict = dict()\n        for prop in self.properties:\n            properties_dict[prop.name] = prop.value\n        return properties_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef order_properties(self, property_list=None):\n        if self.category != Category.MODEL:\n            raise APIError(\"Part should be of category MODEL\")\n        if not isinstance(property_list, list):\n            raise IllegalArgumentError('Expected a list of strings or Property() objects, got a {} object'.\n                                       format(type(property_list)))\n\n        order_dict = dict()\n\n        for prop in property_list:\n            if isinstance(prop, (str, text_type)):\n                order_dict[self.property(name=prop).id] = property_list.index(prop)\n            else:\n                order_dict[prop.id] = property_list.index(prop)\n\n        r = self._client._request('PUT', self._client._build_url('part', part_id=self.id),\n                                  data=dict(\n                                      property_order=json.dumps(order_dict)\n                                  ))\n        if r.status_code != requests.codes.ok:  # pragma: no cover\n            raise APIError(\"Could not reorder properties\")", "response": "Order the properties of a part model using a list of property objects or property id s."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate_descendants(self, batch=200):\n        descendants_flat_list = list(self._client.parts(pk=self.id, category=self.category, descendants='children',\n                                                        batch=batch))\n\n        for parent in descendants_flat_list:\n            parent._cached_children = list()\n            for child in descendants_flat_list:\n                if child.parent_id == parent.id:\n                    parent._cached_children.append(child)\n\n        self._cached_children = descendants_flat_list[0]._cached_children", "response": "Retrieve the descendants of a specific Part in a list of dicts and populate the Part. children method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clone(self, **kwargs):\n        parent = self.parent()\n        return self._client._create_clone(parent, self, **kwargs)", "response": "Clone a part.\n\n        .. versionadded:: 2.3\n\n        :param kwargs: (optional) additional keyword=value arguments\n        :type kwargs: dict\n        :return: cloned :class:`models.Part`\n        :raises APIError: if the `Part` could not be cloned\n\n        Example\n        -------\n        >>> bike = client.model('Bike')\n        >>> bike2 = bike.clone()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy the Part to target parent.", "response": "def copy(self, target_parent, name=None, include_children=True, include_instances=True):\n        \"\"\"\n        Copy the `Part` to target parent, both of them having the same category.\n\n        .. versionadded:: 2.3\n\n        :param target_parent: `Part` object under which the desired `Part` is copied\n        :type target_parent: :class:`Part`\n        :param name: how the copied top-level `Part` should be called\n        :type name: basestring\n        :param include_children: True to copy also the descendants of `Part`.\n        :type include_children: bool\n        :param include_instances: True to copy also the instances of `Part` to ALL the instances of target_parent.\n        :type include_instances: bool\n        :returns: copied :class:`Part` model.\n        :raises IllegalArgumentError: if part and target_parent have different `Category`\n        :raises IllegalArgumentError: if part and target_parent are identical\n\n        Example\n        -------\n        >>> model_to_copy = client.model(name='Model to be copied')\n        >>> bike = client.model('Bike')\n        >>> model_to_copy.copy(target_parent=bike, name='Copied model',\n        >>>                    include_children=True,\n        >>>                    include_instances=True)\n\n        \"\"\"\n        if self.category == Category.MODEL and target_parent.category == Category.MODEL:\n            # Cannot add a model under an instance or vice versa\n            copied_model = relocate_model(part=self, target_parent=target_parent, name=name,\n                                          include_children=include_children)\n            if include_instances:\n                instances_to_be_copied = list(self.instances())\n                parent_instances = list(target_parent.instances())\n                for parent_instance in parent_instances:\n                    for instance in instances_to_be_copied:\n                        instance.populate_descendants()\n                        move_part_instance(part_instance=instance, target_parent=parent_instance,\n                                           part_model=self, name=instance.name, include_children=include_children)\n            return copied_model\n\n        elif self.category == Category.INSTANCE and target_parent.category == Category.INSTANCE:\n            copied_instance = relocate_instance(part=self, target_parent=target_parent, name=name,\n                                                include_children=include_children)\n            return copied_instance\n        else:\n            raise IllegalArgumentError('part \"{}\" and target parent \"{}\" must have the same category')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef move(self, target_parent, name=None, include_children=True, include_instances=True):\n        if not name:\n            name = self.name\n        if self.category == Category.MODEL and target_parent.category == Category.MODEL:\n            moved_model = relocate_model(part=self, target_parent=target_parent, name=name,\n                                         include_children=include_children)\n            if include_instances:\n                retrieve_instances_to_copied = list(self.instances())\n                retrieve_parent_instances = list(target_parent.instances())\n                for parent_instance in retrieve_parent_instances:\n                    for instance in retrieve_instances_to_copied:\n                        instance.populate_descendants()\n                        move_part_instance(part_instance=instance, target_parent=parent_instance,\n                                           part_model=self, name=instance.name, include_children=include_children)\n            self.delete()\n            return moved_model\n        elif self.category == Category.INSTANCE and target_parent.category == Category.INSTANCE:\n            moved_instance = relocate_instance(part=self, target_parent=target_parent, name=name,\n                                               include_children=include_children)\n            try:\n                self.delete()\n            except APIError:\n                model_of_instance = self.model()\n                model_of_instance.delete()\n            return moved_instance\n        else:\n            raise IllegalArgumentError('part \"{}\" and target parent \"{}\" must have the same category')", "response": "Move the Part to target parent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the data to binary format.", "response": "def data_to_binary(self):\n        \"\"\"\n        :return: bytes\n        \"\"\"\n        return bytes([\n            COMMAND_CODE,\n            self.channels_to_byte([self.channel])\n            ]) + bytes(self.name, 'utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a JSON string representation of the current object.", "response": "def to_json(self):\n        \"\"\"\n        :return: str\n        \"\"\"\n        json_dict = self.to_json_basic()\n        json_dict['channel'] = self.channel\n        return json.dumps(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef populate(self, priority, address, rtr, data):\n        assert isinstance(data, bytes)\n        self.needs_low_priority(priority)\n        self.needs_no_rtr(rtr)\n        self.needs_data(data, 5)\n        self.set_attributes(priority, address, rtr)\n        self.channel = data[0]\n        self.name = \"\".join([chr(x) for x in data[1:]])", "response": "Populates the object with the data from the given bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_notebook_by_difficulty_body(notebook_object, dict_by_difficulty):\n\n    difficulty_keys = list(dict_by_difficulty.keys())\n    difficulty_keys.sort()\n    for difficulty in difficulty_keys:\n        markdown_cell = STAR_TABLE_HEADER\n        markdown_cell = _set_star_value(markdown_cell, int(difficulty))\n        for notebook_file in dict_by_difficulty[str(difficulty)]:\n            split_path = notebook_file.split(\"/\")\n            notebook_type = split_path[-2]\n            notebook_name = split_path[-1].split(\"&\")[0]\n            notebook_title = split_path[-1].split(\"&\")[1]\n            markdown_cell += \"\\n\\t<tr>\\n\\t\\t<td width='20%' class='header_image_color_\" + \\\n                             str(NOTEBOOK_KEYS[notebook_type]) + \"'><img \" \\\n                             \"src='../../images/icons/\" + notebook_type.title() +\\\n                             \".png' width='15%'>\\n\\t\\t</td>\"\n            markdown_cell += \"\\n\\t\\t<td width='60%' class='center_cell open_cell_light'>\" + \\\n                             notebook_title + \"\\n\\t\\t</td>\"\n            markdown_cell += \"\\n\\t\\t<td width='20%' class='center_cell'>\\n\\t\\t\\t<a href='\" \\\n                             \"../\" + notebook_type.title() + \"/\" + notebook_name + \\\n                             \"'><div class='file_icon'></div></a>\\n\\t\\t</td>\\n\\t</tr>\"\n\n        markdown_cell += \"</table>\"\n\n        # ==================== Insertion of HTML table in a new Notebook cell ======================\n        notebook_object[\"cells\"].append(nb.v4.new_markdown_cell(markdown_cell))", "response": "Internal function that is used for generating the page where notebooks are organized by difficulty level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_to_file(self, path, filename):\n\n        # =============================== Storage of Filename ======================================\n        self.filename = filename\n\n        categories_path = _generate_dir_structure(path)\n\n        full_path =(categories_path + \"\\\\\" + self.notebook_type + \"\\\\\" + filename\n                    + \".ipynb\").replace(\"\\\\\", \"/\")\n        nb.write(self.notebook, full_path)\n\n        # ========================== Run Notebook Code Instructions ================================\n        os.system(\"jupyter nbconvert --execute --inplace --ExecutePreprocessor.timeout=-1 \" +\n                  full_path)\n        os.system(\"jupyter trust \" + full_path)\n\n        return \"Notebook stored at \" + full_path", "response": "This method writes the notebook object to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the provided XYPoint can be recreated by a Hue lamp.", "response": "def in_lamp_reach(p):\n    ''' Check if the provided XYPoint can be recreated by a Hue lamp. '''\n    v1 = XYPoint(Lime.x - Red.x, Lime.y - Red.y)\n    v2 = XYPoint(Blue.x - Red.x, Blue.y - Red.y)\n\n    q = XYPoint(p.x - Red.x, p.y - Red.y)\n    s = cross_product(q, v2) / cross_product(v1, v2)\n    t = cross_product(v1, q) / cross_product(v1, v2)\n\n    return (s >= 0.0) and (t >= 0.0) and (s + t <= 1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the closest point to a line.", "response": "def get_closest_point_to_line(A, B, P):\n    '''\n    Find the closest point on a line. This point will be reproducible by a Hue\n    lamp.\n    '''\n    AP = XYPoint(P.x - A.x, P.y - A.y)\n    AB = XYPoint(B.x - A.x, B.y - A.y)\n    ab2 = AB.x * AB.x + AB.y * AB.y\n    ap_ab = AP.x * AB.x + AP.y * AB.y\n    t = ap_ab / ab2\n\n    if t < 0.0:\n        t = 0.0\n    elif t > 1.0:\n        t = 1.0\n\n    return XYPoint(A.x + AB.x * t, A.y + AB.y * t)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the closest point to a point in the unreproducible Color.", "response": "def get_closest_point_to_point(xy_point):\n    '''\n    Used to find the closest point to an unreproducible Color is unreproducible\n    on each line in the CIE 1931 'triangle'.\n    '''\n    pAB = get_closest_point_to_line(Red, Lime, xy_point)\n    pAC = get_closest_point_to_line(Blue, Red, xy_point)\n    pBC = get_closest_point_to_line(Lime, Blue, xy_point)\n\n    # Get the distances per point and see which point is closer to our Point.\n    dAB = get_distance_between_two_points(xy_point, pAB)\n    dAC = get_distance_between_two_points(xy_point, pAC)\n    dBC = get_distance_between_two_points(xy_point, pBC)\n\n    lowest = dAB\n    closest_point = pAB\n\n    if (dAC < lowest):\n        lowest = dAC\n        closest_point = pAC\n\n    if (dBC < lowest):\n        lowest = dBC\n        closest_point = pBC\n\n    # Change the xy value to a value which is within the reach of the lamp.\n    cx = closest_point.x\n    cy = closest_point.y\n\n    return XYPoint(cx, cy)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_xy_from_hex(hex_value):\n    '''\n    Returns X, Y coordinates containing the closest avilable CIE 1931\n    based on the hex_value provided.\n    '''\n    red, green, blue = struct.unpack('BBB', codecs.decode(hex_value, 'hex'))\n    r = ((red + 0.055) / (1.0 + 0.055)) ** 2.4 if (red > 0.04045) else (red / 12.92)  # pragma: noqa\n    g = ((green + 0.055) / (1.0 + 0.055)) ** 2.4 if (green > 0.04045) else (green / 12.92)  # pragma: noqa\n    b = ((blue + 0.055) / (1.0 + 0.055)) ** 2.4 if (blue > 0.04045) else (blue / 12.92)  # pragma: noqa\n\n    X = r * 0.4360747 + g * 0.3850649 + b * 0.0930804\n    Y = r * 0.2225045 + g * 0.7168786 + b * 0.0406169\n    Z = r * 0.0139322 + g * 0.0971045 + b * 0.7141733\n\n    if X + Y + Z == 0:\n        cx = cy = 0\n    else:\n        cx = X / (X + Y + Z)\n        cy = Y / (X + Y + Z)\n\n    # Check if the given XY value is within the colourreach of our lamps.\n    xy_point = XYPoint(cx, cy)\n    is_in_reach = in_lamp_reach(xy_point)\n\n    if not is_in_reach:\n        xy_point = get_closest_point_to_point(xy_point)\n\n    return xy_point", "response": "Returns the closest avilable CIE 1931 XY point based on the hex_value provided."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_other_keys(self, key, including_current=False):\r\n        other_keys = []\r\n        if key in self:\r\n            other_keys.extend(self.__dict__[str(type(key))][key])\r\n            if not including_current:\r\n                other_keys.remove(key)\r\n        return other_keys", "response": "Returns a list of other keys that are mapped to the same value as specified key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an iterator over the items of the object.", "response": "def iteritems(self, key_type=None, return_all_keys=False):\r\n        \"\"\" Returns an iterator over the dictionary's (key, value) pairs.\r\n            @param key_type if specified, iterator will be returning only (key,value) pairs for this type of key.\r\n                   Otherwise (if not specified) ((keys,...), value) \r\n                   i.e. (tuple of keys, values) pairs for all items in this dictionary will be generated.\r\n            @param return_all_keys if set to True - tuple of keys is retuned instead of a key of this type.\"\"\"\r\n\r\n        if key_type is None:\r\n            for item in self.items_dict.items():\r\n                yield item\r\n            return\r\n        used_keys = set()\r\n        key = str(key_type)\r\n        if key in self.__dict__:\r\n            for key, keys in self.__dict__[key].items():\r\n                if keys in used_keys:\r\n                    continue\r\n                used_keys.add(keys)\r\n                value = self.items_dict[keys]\r\n                if not return_all_keys:\r\n                    keys = tuple(k for k in keys if isinstance(k, key_type))\r\n                yield keys, value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterator over the keys of the object.", "response": "def iterkeys(self, key_type=None, return_all_keys=False):\r\n        \"\"\" Returns an iterator over the dictionary's keys.\r\n            @param key_type if specified, iterator for a dictionary of this type will be used. \r\n                   Otherwise (if not specified) tuples containing all (multiple) keys\r\n                   for this dictionary will be generated.\r\n            @param return_all_keys if set to True - tuple of keys is retuned instead of a key of this type.\"\"\"\r\n        if(key_type is not None):\r\n            the_key = str(key_type)\r\n            if the_key in self.__dict__:\r\n                for key in self.__dict__[the_key].keys():\r\n                    if return_all_keys:\r\n                        yield self.__dict__[the_key][key]\r\n                    else:\r\n                        yield key            \r\n        else:\r\n            for keys in self.items_dict.keys():\r\n                yield keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterator over the dictionary s values.", "response": "def itervalues(self, key_type=None):\r\n        \"\"\" Returns an iterator over the dictionary's values.\r\n            @param key_type if specified, iterator will be returning only values pointed by keys of this type.\r\n                   Otherwise (if not specified) all values in this dictinary will be generated.\"\"\"\r\n        if(key_type is not None):\r\n            intermediate_key = str(key_type)\r\n            if intermediate_key in self.__dict__:\r\n                for direct_key in self.__dict__[intermediate_key].values():\r\n                    yield self.items_dict[direct_key]\r\n        else:\r\n            for value in self.items_dict.values():\r\n                yield value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a copy of the dictionary s keys.", "response": "def keys(self, key_type=None):\r\n        \"\"\" Returns a copy of the dictionary's keys.\r\n            @param key_type if specified, only keys for this type will be returned.\r\n                 Otherwise list of tuples containing all (multiple) keys will be returned.\"\"\"\r\n        if key_type is not None:\r\n            intermediate_key = str(key_type)\r\n            if intermediate_key in self.__dict__:\r\n                return self.__dict__[intermediate_key].keys()\r\n        else:\r\n            all_keys = {} # in order to preserve keys() type (dict_keys for python3) \r\n            for keys in self.items_dict.keys():\r\n                all_keys[keys] = None\r\n            return all_keys.keys()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef values(self, key_type=None):\r\n        if(key_type is not None):\r\n            all_items = {} # in order to preserve keys() type (dict_values for python3) \r\n            keys_used = set()\r\n            direct_key = str(key_type)\r\n            if direct_key in self.__dict__:\r\n                for intermediate_key in self.__dict__[direct_key].values():\r\n                    if not intermediate_key in keys_used:\r\n                        all_items[intermediate_key] = self.items_dict[intermediate_key]\r\n                        keys_used.add(intermediate_key)\r\n            return all_items.values()\r\n        else:\r\n            return self.items_dict.values()", "response": "Returns a copy of the dictionary s values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an item to the multi - key dictionary", "response": "def __add_item(self, item, keys=None):\r\n        \"\"\" Internal method to add an item to the multi-key dictionary\"\"\"\r\n        if(not keys or not len(keys)):\r\n            raise Exception('Error in %s.__add_item(%s, keys=tuple/list of items): need to specify a tuple/list containing at least one key!'\r\n                            % (self.__class__.__name__, str(item)))\r\n        direct_key = tuple(keys) # put all keys in a tuple, and use it as a key\r\n        for key in keys:\r\n            key_type = str(type(key))\r\n\r\n            # store direct key as a value in an intermediate dictionary\r\n            if(not key_type in self.__dict__):\r\n                self.__setattr__(key_type, dict())\r\n            self.__dict__[key_type][key] = direct_key\r\n\r\n            # store the value in the actual dictionary\r\n            if(not 'items_dict' in self.__dict__):\r\n                self.items_dict = dict()\r\n            self.items_dict[direct_key] = item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, key, default=None):\r\n        if key in self:\r\n            return self.items_dict[self.__dict__[str(type(key))][key]]\r\n        else:\r\n            return default", "response": "Return the value at the specified index as key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract messages from Django template string.", "response": "def extract_translations(self, string):\n        \"\"\"Extract messages from Django template string.\"\"\"\n\n        trans = []\n        for t in Lexer(string.decode(\"utf-8\"), None).tokenize():\n            if t.token_type == TOKEN_BLOCK:\n                if not t.contents.startswith(\n                        (self.tranz_tag, self.tranzchoice_tag)):\n                    continue\n\n                is_tranzchoice = t.contents.startswith(\n                    self.tranzchoice_tag +\n                    \" \")\n                kwargs = {\n                    \"id\": self._match_to_transvar(id_re, t.contents),\n                    \"number\": self._match_to_transvar(number_re, t.contents),\n                    \"domain\": self._match_to_transvar(domain_re, t.contents),\n                    \"locale\": self._match_to_transvar(locale_re, t.contents),\n                    \"is_transchoice\": is_tranzchoice, \"parameters\": TransVar(\n                        [x.split(\"=\")[0].strip() for x in properties_re.findall(t.contents) if x],\n                        TransVar.LITERAL\n                    ),\n                    \"lineno\": t.lineno,\n                }\n\n                trans.append(Translation(**kwargs))\n        return trans"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the next marker name as a string and its genotypes as numpy. ndarray.", "response": "def next(self):\n        \"\"\"Returns the next marker.\n\n        Returns:\n            tuple: The marker name as a string and its genotypes as a\n            :py:class:`numpy.ndarray`.\n\n        \"\"\"\n        if self._mode != \"r\":\n            raise UnsupportedOperation(\"not available in 'w' mode\")\n\n        self._n += 1\n        if self._n > self._nb_markers:\n            raise StopIteration()\n\n        return self._bim.index[self._n - 1], self._read_current_marker()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the current marker and returns its genotypes.", "response": "def _read_current_marker(self):\n        \"\"\"Reads the current marker and returns its genotypes.\"\"\"\n        return self._geno_values[\n            np.frombuffer(self._bed.read(self._nb_bytes), dtype=np.uint8)\n        ].flatten(order=\"C\")[:self._nb_samples]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget to a certain marker in the BED file.", "response": "def seek(self, n):\n        \"\"\"Gets to a certain marker position in the BED file.\n\n        Args:\n            n (int): The index of the marker to seek to.\n\n        \"\"\"\n        if self._mode != \"r\":\n            raise UnsupportedOperation(\"not available in 'w' mode\")\n\n        if 0 <= n < self._nb_markers:\n            self._n = n\n            self._bed.seek(self._get_seek_position(n))\n\n        else:\n            # Invalid seek value\n            raise ValueError(\"invalid position in BED: {}\".format(n))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_bim(self):\n        # Reading the BIM file and setting the values\n        bim = pd.read_csv(self.bim_filename, delim_whitespace=True,\n                          names=[\"chrom\", \"snp\", \"cm\", \"pos\", \"a1\", \"a2\"],\n                          dtype=dict(snp=str, a1=str, a2=str))\n\n        # Saving the index as integer\n        bim[\"i\"] = bim.index\n\n        # Checking for duplicated markers\n        try:\n            bim = bim.set_index(\"snp\", verify_integrity=True)\n            self._has_duplicated = False\n\n        except ValueError as e:\n            # Setting this flag to true\n            self._has_duplicated = True\n\n            # Finding the duplicated markers\n            duplicated = bim.snp.duplicated(keep=False)\n            duplicated_markers = bim.loc[duplicated, \"snp\"]\n            duplicated_marker_counts = duplicated_markers.value_counts()\n\n            # The dictionary that will contain information about the duplicated\n            # markers\n            self._dup_markers = {\n                m: [] for m in duplicated_marker_counts.index\n            }\n\n            # Logging a warning\n            logger.warning(\"Duplicated markers found\")\n            for marker, count in duplicated_marker_counts.iteritems():\n                logger.warning(\"  - {}: {:,d} times\".format(marker, count))\n            logger.warning(\"Appending ':dupX' to the duplicated markers \"\n                           \"according to their location in the BIM file\")\n\n            # Renaming the markers\n            counter = Counter()\n            for i, marker in duplicated_markers.iteritems():\n                counter[marker] += 1\n                new_name = \"{}:dup{}\".format(marker, counter[marker])\n                bim.loc[i, \"snp\"] = new_name\n\n                # Updating the dictionary containing the duplicated markers\n                self._dup_markers[marker].append(new_name)\n\n            # Resetting the index\n            bim = bim.set_index(\"snp\", verify_integrity=True)\n\n        # Encoding the allele\n        #   - The original 0 is the actual 2 (a1/a1)\n        #   - The original 2 is the actual 1 (a1/a2)\n        #   - The original 3 is the actual 0 (a2/a2)\n        #   - The original 1 is the actual -1 (no call)\n        allele_encoding = np.array(\n            [bim.a2 * 2, bim.a1 + bim.a2, bim.a1 * 2,\n             list(repeat(\"00\", bim.shape[0]))],\n            dtype=\"U2\",\n        )\n        self._allele_encoding = allele_encoding.T\n\n        # Saving the data in the object\n        self._bim = bim[[\"chrom\", \"pos\", \"cm\", \"a1\", \"a2\", \"i\"]]\n        self._nb_markers = self._bim.shape[0]", "response": "Reads the BIM file and sets the internal _has_duplicated flag to True."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_fam(self):\n        # Reading the FAM file and setting the values\n        fam = pd.read_csv(self.fam_filename, delim_whitespace=True,\n                          names=[\"fid\", \"iid\", \"father\", \"mother\", \"gender\",\n                                 \"status\"],\n                          dtype=dict(fid=str, iid=str, father=str, mother=str))\n\n        # Getting the byte and bit location of each samples\n        fam[\"byte\"] = [\n            int(np.ceil((1 + 1) / 4.0)) - 1 for i in range(len(fam))\n        ]\n        fam[\"bit\"] = [(i % 4) * 2 for i in range(len(fam))]\n\n        # Saving the data in the object\n        self._fam = fam\n        self._nb_samples = self._fam.shape[0]", "response": "Reads the FAM file and sets the object self. _fam and self. _nb_samples attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the BED file and stores the metadata in self. _bed.", "response": "def _read_bed(self):\n        \"\"\"Reads the BED file.\"\"\"\n        # Checking if BIM and BAM files were both read\n        if (self._bim is None) or (self._fam is None):\n            raise RuntimeError(\"no BIM or FAM file were read\")\n\n        # The number of bytes per marker\n        self._nb_bytes = int(np.ceil(self._nb_samples / 4.0))\n\n        # Checking the file is valid by looking at the first 3 bytes and the\n        # last entry (correct size)\n        with open(self.bed_filename, \"rb\") as bed_file:\n            # Checking that the first two bytes are OK\n            if (ord(bed_file.read(1)) != 108) or (ord(bed_file.read(1)) != 27):\n                raise ValueError(\"not a valid BED file: \"\n                                 \"{}\".format(self.bed_filename))\n\n            # Checking that the format is SNP-major\n            if ord(bed_file.read(1)) != 1:\n                raise ValueError(\"not in SNP-major format (please recode): \"\n                                 \"{}\".format(self.bed_filename))\n\n            # Checking the last entry (for BED corruption)\n            seek_index = self._get_seek_position(self._bim.iloc[-1, :].i)\n            bed_file.seek(seek_index)\n            geno = self._geno_values[\n                np.frombuffer(bed_file.read(self._nb_bytes), dtype=np.uint8)\n            ].flatten(order=\"C\")[:self._nb_samples]\n            if geno.shape[0] != self._nb_samples:\n                raise ValueError(\"invalid number of entries: corrupted BED?\")\n\n        # Opening the file for the rest of the operations (reading 3 bytes)\n        self._bed = open(self.bed_filename, \"rb\")\n        self._bed.read(3)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the BED header.", "response": "def _write_bed_header(self):\n        \"\"\"Writes the BED first 3 bytes.\"\"\"\n        # Writing the first three bytes\n        final_byte = 1 if self._bed_format == \"SNP-major\" else 0\n        self._bed.write(bytearray((108, 27, final_byte)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over the genotypes of the a .", "response": "def iter_acgt_geno(self):\n        \"\"\"Iterates over genotypes (ACGT format).\n\n        Returns:\n            tuple: The name of the marker as a string, and its genotypes as a\n            :py:class:`numpy.ndarray` (ACGT format).\n\n        \"\"\"\n        # Need to iterate over itself, and modify the actual genotypes\n        for i, (marker, geno) in enumerate(self.iter_geno()):\n            yield marker, self._allele_encoding[i][geno]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_geno_marker(self, markers, return_index=False):\n        if self._mode != \"r\":\n            raise UnsupportedOperation(\"not available in 'w' mode\")\n\n        # If string, we change to list\n        if isinstance(markers, str):\n            markers = [markers]\n\n        # Iterating over all markers\n        if return_index:\n            for marker in markers:\n                geno, seek = self.get_geno_marker(marker, return_index=True)\n                yield marker, geno, seek\n        else:\n            for marker in markers:\n                yield marker, self.get_geno_marker(marker)", "response": "Iterates over genotypes for a list of markers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over the genotypes of the specified markers and return the corresponding tuple.", "response": "def iter_acgt_geno_marker(self, markers):\n        \"\"\"Iterates over genotypes for a list of markers (ACGT format).\n\n        Args:\n            markers (list): The list of markers to iterate onto.\n\n        Returns:\n            tuple: The name of the marker as a string, and its genotypes as a\n            :py:class:`numpy.ndarray` (ACGT format).\n\n        \"\"\"\n        # We iterate over the markers\n        for snp, geno, s in self.iter_geno_marker(markers, return_index=True):\n            # Getting the SNP position and converting to ACGT\n            yield snp, self._allele_encoding[s][geno]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_geno_marker(self, marker, return_index=False):\n        if self._mode != \"r\":\n            raise UnsupportedOperation(\"not available in 'w' mode\")\n\n        # Check if the marker exists\n        if marker not in self._bim.index:\n            raise ValueError(\"{}: marker not in BIM\".format(marker))\n\n        # Seeking to the correct position\n        seek_index = self._bim.loc[marker, \"i\"]\n        self.seek(seek_index)\n\n        if return_index:\n            return self._read_current_marker(), seek_index\n        return self._read_current_marker()", "response": "Returns the genotypes of a given marker."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the genotypes of an ACGT marker.", "response": "def get_acgt_geno_marker(self, marker):\n        \"\"\"Gets the genotypes for a given marker (ACGT format).\n\n        Args:\n            marker (str): The name of the marker.\n\n        Returns:\n            numpy.ndarray: The genotypes of the marker (ACGT format).\n\n        \"\"\"\n        # Getting the marker's genotypes\n        geno, snp_position = self.get_geno_marker(marker, return_index=True)\n\n        # Returning the ACGT's format of the genotypes\n        return self._allele_encoding[snp_position][geno]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the genotypes to the BED file.", "response": "def write_genotypes(self, genotypes):\n        \"\"\"Write genotypes to binary file.\n\n        Args:\n            genotypes (numpy.ndarray): The genotypes to write in the BED file.\n\n        \"\"\"\n        if self._mode != \"w\":\n            raise UnsupportedOperation(\"not available in 'r' mode\")\n\n        # Initializing the number of samples if required\n        if self._nb_values is None:\n            self._nb_values = len(genotypes)\n\n        # Checking the expected number of samples\n        if self._nb_values != len(genotypes):\n            raise ValueError(\"{:,d} samples expected, got {:,d}\".format(\n                self._nb_values,\n                len(genotypes),\n            ))\n\n        # Writing to file\n        byte_array = [\n            g[0] | (g[1] << 2) | (g[2] << 4) | (g[3] << 6) for g in\n            self._grouper((_byte_recode[geno] for geno in genotypes), 4)\n        ]\n        self._bed.write(bytearray(byte_array))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngroup data into fixed - length chunks or blocks.", "response": "def _grouper(iterable, n, fillvalue=0):\n        \"\"\"Collect data into fixed-length chunks or blocks.\n\n        Args:\n            n (int): The size of the chunk.\n            fillvalue (int): The fill value.\n\n        Returns:\n            iterator: An iterator over the chunks.\n\n        \"\"\"\n        args = [iter(iterable)] * n\n        return zip_longest(fillvalue=fillvalue, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning time series as pandas dataframe", "response": "def as_dataframe(self):\n        \"\"\"\n        Return time series as pandas dataframe\n        \"\"\"\n        time_series = {}\n        for ts_index, ts in enumerate(self.timeSeries):\n            index = []\n            data = []\n            for value in ts.values:\n                index.append(value.simTime)\n                data.append(value.value)\n            time_series[ts_index] = pd.Series(data, index=index)\n        return pd.DataFrame(time_series)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new Blueprint. cls and calls each deferred operation.", "response": "def register(self, obj=None, memo=None):\n        \"\"\"\n        Creates a :class:`Blueprint.cls` and calls each deferred operation.\n\n        :param obj:\n            The initialized object with which to call all deferred operations.\n        :type obj: object\n\n        :param memo:\n            A dictionary to cache registered Blueprints.\n        :type memo: dict[Blueprint,T]\n\n        :return:\n            The initialized object.\n        :rtype: Blueprint.cls | Blueprint\n\n        **--------------------------------------------------------------------**\n\n        Example::\n\n            >>> import schedula as sh\n            >>> blue = sh.BlueDispatcher().add_func(len, ['lenght'])\n            >>> blue.register()\n            <schedula.dispatcher.Dispatcher object at ...>\n        \"\"\"\n        if memo and self in memo:\n            obj = memo[self]\n            if obj is not None:\n                return obj\n        if obj is None:\n            obj = _safe_call(self.cls, *self.args, memo=memo, **self.kwargs)\n\n        for method, kwargs in self.deferred:\n            _safe_call(getattr(obj, method), memo=memo, **kwargs)\n\n        if memo is not None:\n            memo[self] = obj\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextends this object with the given Blueprints or Dispatchers.", "response": "def extend(self, *blues, memo=None):\n        \"\"\"\n        Extends deferred operations calling each operation of given Blueprints.\n\n        :param blues:\n            Blueprints or Dispatchers to extend deferred operations.\n        :type blues: Blueprint | schedula.dispatcher.Dispatcher\n\n        :param memo:\n            A dictionary to cache Blueprints.\n        :type memo: dict[T,Blueprint]\n\n        :return:\n            Self.\n        :rtype: Blueprint\n\n        **--------------------------------------------------------------------**\n\n        Example::\n\n            >>> import schedula as sh\n            >>> blue = sh.BlueDispatcher()\n            >>> blue.extend(\n            ...     BlueDispatcher().add_func(len, ['length']),\n            ...     BlueDispatcher().add_func(callable, ['is_callable'])\n            ... )\n            <schedula.utils.blue.BlueDispatcher object at ...>\n        \"\"\"\n        memo = {} if memo is None else memo\n        for blue in blues:\n            if isinstance(blue, Dispatcher):\n                blue = blue.blue(memo=memo)\n            for method, kwargs in blue.deferred:\n                getattr(self, method)(**kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_data(self, data_id=None, default_value=EMPTY, initial_dist=0.0,\n                 wait_inputs=False, wildcard=None, function=None, callback=None,\n                 description=None, filters=None, await_result=None, **kwargs):\n        \"\"\"\n        Add a single data node to the dispatcher.\n\n        :param data_id:\n            Data node id. If None will be assigned automatically ('unknown<%d>')\n            not in dmap.\n        :type data_id: str, optional\n\n        :param default_value:\n            Data node default value. This will be used as input if it is not\n            specified as inputs in the ArciDispatch algorithm.\n        :type default_value: T, optional\n\n        :param initial_dist:\n            Initial distance in the ArciDispatch algorithm when the data node\n            default value is used.\n        :type initial_dist: float, int, optional\n\n        :param wait_inputs:\n            If True ArciDispatch algorithm stops on the node until it gets all\n            input estimations.\n        :type wait_inputs: bool, optional\n\n        :param wildcard:\n            If True, when the data node is used as input and target in the\n            ArciDispatch algorithm, the input value will be used as input for\n            the connected functions, but not as output.\n        :type wildcard: bool, optional\n\n        :param function:\n            Data node estimation function.\n            This can be any function that takes only one dictionary\n            (key=function node id, value=estimation of data node) as input and\n            return one value that is the estimation of the data node.\n        :type function: callable, optional\n\n        :param callback:\n            Callback function to be called after node estimation.\n            This can be any function that takes only one argument that is the\n            data node estimation output. It does not return anything.\n        :type callback: callable, optional\n\n        :param description:\n            Data node's description.\n        :type description: str, optional\n\n        :param filters:\n            A list of functions that are invoked after the invocation of the\n            main function.\n        :type filters: list[function], optional\n\n        :param await_result:\n            If True the Dispatcher waits data results before assigning them to\n            the solution. If a number is defined this is used as `timeout` for\n            `Future.result` method [default: False]. Note this is used when\n            asynchronous or parallel execution is enable.\n        :type await_result: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n\n        :return:\n            Self.\n        :rtype: BlueDispatcher\n        \"\"\"\n        kwargs.update(_call_kw(locals()))\n        self.deferred.append(('add_data', kwargs))\n        return self", "response": "This method adds a single data node to the dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a single function node to dispatcher. :param function_id: Function node id. If None will be assigned as <fun.__name__>. :type function_id: str, optional :param function: Data node estimation function. :type function: callable, optional :param inputs: Ordered arguments (i.e., data node ids) needed by the function. :type inputs: list, optional :param outputs: Ordered results (i.e., data node ids) returned by the function. :type outputs: list, optional :param input_domain: A function that checks if input values satisfy the function domain. This can be any function that takes the same inputs of the function and returns True if input values satisfy the domain, otherwise False. In this case the dispatch algorithm doesn't pass on the node. :type input_domain: callable, optional :param weight: Node weight. It is a weight coefficient that is used by the dispatch algorithm to estimate the minimum workflow. :type weight: float, int, optional :param inp_weight: Edge weights from data nodes to the function node. It is a dictionary (key=data node id) with the weight coefficients used by the dispatch algorithm to estimate the minimum workflow. :type inp_weight: dict[str, float | int], optional :param out_weight: Edge weights from the function node to data nodes. It is a dictionary (key=data node id) with the weight coefficients used by the dispatch algorithm to estimate the minimum workflow. :type out_weight: dict[str, float | int], optional :param description: Function node's description. :type description: str, optional :param filters: A list of functions that are invoked after the invocation of the main function. :type filters: list[function], optional :param await_domain: If True the Dispatcher waits all input results before executing the `input_domain` function. If a number is defined this is used as `timeout` for `Future.result` method [default: True]. Note this is used when asynchronous or parallel execution is enable. :type await_domain: bool|int|float, optional :param await_result: If True the Dispatcher waits output results before assigning them to the workflow. If a number is defined this is used as `timeout` for `Future.result` method [default: False]. Note this is used when asynchronous or parallel execution is enable. :type await_result: bool|int|float, optional :param kwargs: Set additional node attributes using key=value. :type kwargs: keyword arguments, optional", "response": "def add_function(self, function_id=None, function=None, inputs=None,\n                     outputs=None, input_domain=None, weight=None,\n                     inp_weight=None, out_weight=None, description=None,\n                     filters=None, await_domain=None, await_result=None,\n                     **kwargs):\n        \"\"\"\n        Add a single function node to dispatcher.\n\n        :param function_id:\n            Function node id.\n            If None will be assigned as <fun.__name__>.\n        :type function_id: str, optional\n\n        :param function:\n            Data node estimation function.\n        :type function: callable, optional\n\n        :param inputs:\n            Ordered arguments (i.e., data node ids) needed by the function.\n        :type inputs: list, optional\n\n        :param outputs:\n            Ordered results (i.e., data node ids) returned by the function.\n        :type outputs: list, optional\n\n        :param input_domain:\n            A function that checks if input values satisfy the function domain.\n            This can be any function that takes the same inputs of the function\n            and returns True if input values satisfy the domain, otherwise\n            False. In this case the dispatch algorithm doesn't pass on the node.\n        :type input_domain: callable, optional\n\n        :param weight:\n            Node weight. It is a weight coefficient that is used by the dispatch\n            algorithm to estimate the minimum workflow.\n        :type weight: float, int, optional\n\n        :param inp_weight:\n            Edge weights from data nodes to the function node.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type inp_weight: dict[str, float | int], optional\n\n        :param out_weight:\n            Edge weights from the function node to data nodes.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type out_weight: dict[str, float | int], optional\n\n        :param description:\n            Function node's description.\n        :type description: str, optional\n\n        :param filters:\n            A list of functions that are invoked after the invocation of the\n            main function.\n        :type filters: list[function], optional\n\n        :param await_domain:\n            If True the Dispatcher waits all input results before executing the\n            `input_domain` function. If a number is defined this is used as\n            `timeout` for `Future.result` method [default: True]. Note this is\n            used when asynchronous or parallel execution is enable.\n        :type await_domain: bool|int|float, optional\n\n        :param await_result:\n            If True the Dispatcher waits output results before assigning them to\n            the workflow. If a number is defined this is used as `timeout` for\n            `Future.result` method [default: False]. Note this is used when\n            asynchronous or parallel execution is enable.\n        :type await_result: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n        \"\"\"\n        kwargs.update(_call_kw(locals()))\n        self.deferred.append(('add_function', kwargs))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a function to the dispatcher.", "response": "def add_func(self, function, outputs=None, weight=None, inputs_kwargs=False,\n                 inputs_defaults=False, filters=None, input_domain=None,\n                 await_domain=None, await_result=None, inp_weight=None,\n                 out_weight=None, description=None, inputs=None,\n                 function_id=None, **kwargs):\n        \"\"\"\n        Add a single function node to dispatcher.\n\n        :param inputs_kwargs:\n            Do you want to include kwargs as inputs?\n        :type inputs_kwargs: bool\n\n        :param inputs_defaults:\n            Do you want to set default values?\n        :type inputs_defaults: bool\n\n        :param function_id:\n            Function node id.\n            If None will be assigned as <fun.__name__>.\n        :type function_id: str, optional\n\n        :param function:\n            Data node estimation function.\n        :type function: callable, optional\n\n        :param inputs:\n            Ordered arguments (i.e., data node ids) needed by the function.\n            If None it will take parameters names from function signature.\n        :type inputs: list, optional\n\n        :param outputs:\n            Ordered results (i.e., data node ids) returned by the function.\n        :type outputs: list, optional\n\n        :param input_domain:\n            A function that checks if input values satisfy the function domain.\n            This can be any function that takes the same inputs of the function\n            and returns True if input values satisfy the domain, otherwise\n            False. In this case the dispatch algorithm doesn't pass on the node.\n        :type input_domain: callable, optional\n\n        :param weight:\n            Node weight. It is a weight coefficient that is used by the dispatch\n            algorithm to estimate the minimum workflow.\n        :type weight: float, int, optional\n\n        :param inp_weight:\n            Edge weights from data nodes to the function node.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type inp_weight: dict[str, float | int], optional\n\n        :param out_weight:\n            Edge weights from the function node to data nodes.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type out_weight: dict[str, float | int], optional\n\n        :param description:\n            Function node's description.\n        :type description: str, optional\n\n        :param filters:\n            A list of functions that are invoked after the invocation of the\n            main function.\n        :type filters: list[function], optional\n\n        :param await_domain:\n            If True the Dispatcher waits all input results before executing the\n            `input_domain` function. If a number is defined this is used as\n            `timeout` for `Future.result` method [default: True]. Note this is\n            used when asynchronous or parallel execution is enable.\n        :type await_domain: bool|int|float, optional\n\n        :param await_result:\n            If True the Dispatcher waits output results before assigning them to\n            the workflow. If a number is defined this is used as `timeout` for\n            `Future.result` method [default: False]. Note this is used when\n            asynchronous or parallel execution is enable.\n        :type await_result: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n\n        :return:\n            Self.\n        :rtype: BlueDispatcher\n        \"\"\"\n        kwargs.update(_call_kw(locals()))\n        self.deferred.append(('add_func', kwargs))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a single sub-dispatcher node to dispatcher. :param dsp: Child dispatcher that is added as sub-dispatcher node to the parent dispatcher. :type dsp: Dispatcher | dict[str, list] :param inputs: Inputs mapping. Data node ids from parent dispatcher to child sub-dispatcher. :type inputs: dict[str, str | list[str]] | tuple[str] | (str, ..., dict[str, str | list[str]]) :param outputs: Outputs mapping. Data node ids from child sub-dispatcher to parent dispatcher. :type outputs: dict[str, str | list[str]] | tuple[str] | (str, ..., dict[str, str | list[str]]) :param dsp_id: Sub-dispatcher node id. If None will be assigned as <dsp.name>. :type dsp_id: str, optional :param input_domain: A function that checks if input values satisfy the function domain. This can be any function that takes the a dictionary with the inputs of the sub-dispatcher node and returns True if input values satisfy the domain, otherwise False. .. note:: This function is invoked every time that a data node reach the sub-dispatcher node. :type input_domain: (dict) -> bool, optional :param weight: Node weight. It is a weight coefficient that is used by the dispatch algorithm to estimate the minimum workflow. :type weight: float, int, optional :param inp_weight: Edge weights from data nodes to the sub-dispatcher node. It is a dictionary (key=data node id) with the weight coefficients used by the dispatch algorithm to estimate the minimum workflow. :type inp_weight: dict[str, int | float], optional :param description: Sub-dispatcher node's description. :type description: str, optional :param include_defaults: If True the default values of the sub-dispatcher are added to the current dispatcher. :type include_defaults: bool, optional :param await_domain: If True the Dispatcher waits all input results before executing the `input_domain` function. If a number is defined this is used as `timeout` for `Future.result` method [default: True]. Note this is used when asynchronous or parallel execution is enable. :type await_domain: bool|int|float, optional :param kwargs: Set additional node attributes using key=value. :type kwargs: keyword arguments, optional :return: Self. :rtype: BlueDispatcher", "response": "def add_dispatcher(self, dsp, inputs, outputs, dsp_id=None,\n                       input_domain=None, weight=None, inp_weight=None,\n                       description=None, include_defaults=False,\n                       await_domain=None, **kwargs):\n        \"\"\"\n        Add a single sub-dispatcher node to dispatcher.\n\n        :param dsp:\n            Child dispatcher that is added as sub-dispatcher node to the parent\n            dispatcher.\n        :type dsp: Dispatcher | dict[str, list]\n\n        :param inputs:\n            Inputs mapping. Data node ids from parent dispatcher to child\n            sub-dispatcher.\n        :type inputs: dict[str, str | list[str]] | tuple[str] |\n                      (str, ..., dict[str, str | list[str]])\n\n        :param outputs:\n            Outputs mapping. Data node ids from child sub-dispatcher to parent\n            dispatcher.\n        :type outputs: dict[str, str | list[str]] | tuple[str] |\n                       (str, ..., dict[str, str | list[str]])\n\n        :param dsp_id:\n            Sub-dispatcher node id.\n            If None will be assigned as <dsp.name>.\n        :type dsp_id: str, optional\n\n        :param input_domain:\n            A function that checks if input values satisfy the function domain.\n            This can be any function that takes the a dictionary with the inputs\n            of the sub-dispatcher node and returns True if input values satisfy\n            the domain, otherwise False.\n\n            .. note:: This function is invoked every time that a data node reach\n               the sub-dispatcher node.\n        :type input_domain: (dict) -> bool, optional\n\n        :param weight:\n            Node weight. It is a weight coefficient that is used by the dispatch\n            algorithm to estimate the minimum workflow.\n        :type weight: float, int, optional\n\n        :param inp_weight:\n            Edge weights from data nodes to the sub-dispatcher node.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type inp_weight: dict[str, int | float], optional\n\n        :param description:\n            Sub-dispatcher node's description.\n        :type description: str, optional\n\n        :param include_defaults:\n            If True the default values of the sub-dispatcher are added to the\n            current dispatcher.\n        :type include_defaults: bool, optional\n\n        :param await_domain:\n            If True the Dispatcher waits all input results before executing the\n            `input_domain` function. If a number is defined this is used as\n            `timeout` for `Future.result` method [default: True]. Note this is\n            used when asynchronous or parallel execution is enable.\n        :type await_domain: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n\n        :return:\n            Self.\n        :rtype: BlueDispatcher\n        \"\"\"\n        kwargs.update(_call_kw(locals()))\n        self.deferred.append(('add_dispatcher', kwargs))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_from_lists(self, data_list=None, fun_list=None, dsp_list=None):\n        self.deferred.append(('add_from_lists', _call_kw(locals())))\n        return self", "response": "Add multiple function and data nodes to dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the default value of a data node in the dispatcher.", "response": "def set_default_value(self, data_id, value=EMPTY, initial_dist=0.0):\n        \"\"\"\n        Set the default value of a data node in the dispatcher.\n\n        :param data_id:\n            Data node id.\n        :type data_id: str\n\n        :param value:\n            Data node default value.\n\n            .. note:: If `EMPTY` the previous default value is removed.\n        :type value: T, optional\n\n        :param initial_dist:\n            Initial distance in the ArciDispatch algorithm when the data node\n            default value is used.\n        :type initial_dist: float, int, optional\n\n        :return:\n            Self.\n        :rtype: BlueDispatcher\n        \"\"\"\n        self.deferred.append(('set_default_value', _call_kw(locals())))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Assign file extension attribute to file object\n        self.fileExtension = extension\n\n        # Open file and parse into a data structure\n        with open(path, 'r') as f:\n            for line in f:\n                sline = line.strip().split()\n\n                if len(sline) == 1:\n                    self.numLocations = sline[0]\n                else:\n                    # Create GSSHAPY OutputLocation object\n                    location = OutputLocation(linkOrCellI=sline[0],\n                                              nodeOrCellJ=sline[1])\n\n                    # Associate OutputLocation with OutputLocationFile\n                    location.outputLocationFile = self", "response": "Generic Output Location Read from File Method"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef web(self, depth=-1, node_data=NONE, node_function=NONE, directory=None,\n            sites=None, run=True):\n        \"\"\"\n        Creates a dispatcher Flask app.\n\n        :param depth:\n            Depth of sub-dispatch plots. If negative all levels are plotted.\n        :type depth: int, optional\n\n        :param node_data:\n            Data node attributes to view.\n        :type node_data: tuple[str], optional\n\n        :param node_function:\n            Function node attributes to view.\n        :type node_function: tuple[str], optional\n\n        :param directory:\n            Where is the generated Flask app root located?\n        :type directory: str, optional\n\n        :param sites:\n            A set of :class:`~schedula.utils.drw.Site` to maintain alive the\n            backend server.\n        :type sites: set[~schedula.utils.drw.Site], optional\n\n        :param run:\n            Run the backend server?\n        :type run: bool, optional\n\n        :return:\n            A WebMap.\n        :rtype: ~schedula.utils.web.WebMap\n\n        Example:\n\n        From a dispatcher like this:\n\n        .. dispatcher:: dsp\n           :opt: graph_attr={'ratio': '1'}\n           :code:\n\n            >>> from schedula import Dispatcher\n            >>> dsp = Dispatcher(name='Dispatcher')\n            >>> def fun(a):\n            ...     return a + 1, a - 1\n            >>> dsp.add_function('fun', fun, ['a'], ['b', 'c'])\n            'fun'\n\n        You can create a web server with the following steps::\n\n            >>> webmap = dsp.web()\n            >>> print(\"Starting...\\\\n\"); site = webmap.site().run(); site\n            Starting...\n            Site(WebMap([(Dispatcher, WebMap())]), host='localhost', ...)\n            >>> import requests\n            >>> url = '%s/%s/%s' % (site.url, dsp.name, fun.__name__)\n            >>> requests.post(url, json={'args': (0,)}).json()['return']\n            [1, -1]\n            >>> site.shutdown()  # Remember to shutdown the server.\n            True\n\n        .. note::\n           When :class:`~schedula.utils.drw.Site` is garbage collected the\n           server is shutdown automatically.\n        \"\"\"\n\n        options = {'node_data': node_data, 'node_function': node_function}\n        options = {k: v for k, v in options.items() if v is not NONE}\n        from .web import WebMap\n        from .sol import Solution\n\n        obj = self.dsp if isinstance(self, Solution) else self\n\n        webmap = WebMap()\n        webmap.add_items(obj, workflow=False, depth=depth, **options)\n\n        if sites is not None:\n            import tempfile\n            directory = directory or tempfile.mkdtemp()\n            sites.add(webmap.site(directory, view=run))\n\n        return webmap", "response": "Creates a web server that will serve the node data and function nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the dispatcher with a graphviz graph.", "response": "def plot(self, workflow=None, view=True, depth=-1, name=NONE, comment=NONE,\n             format=NONE, engine=NONE, encoding=NONE, graph_attr=NONE,\n             node_attr=NONE, edge_attr=NONE, body=NONE, node_styles=NONE,\n             node_data=NONE, node_function=NONE, edge_data=NONE, max_lines=NONE,\n             max_width=NONE, directory=None, sites=None, index=False):\n        \"\"\"\n        Plots the Dispatcher with a graph in the DOT language with Graphviz.\n\n        :param workflow:\n           If True the latest solution will be plotted, otherwise the dmap.\n        :type workflow: bool, optional\n\n        :param view:\n            Open the rendered directed graph in the DOT language with the sys\n            default opener.\n        :type view: bool, optional\n\n        :param edge_data:\n            Edge attributes to view.\n        :type edge_data: tuple[str], optional\n\n        :param node_data:\n            Data node attributes to view.\n        :type node_data: tuple[str], optional\n\n        :param node_function:\n            Function node attributes to view.\n        :type node_function: tuple[str], optional\n\n        :param node_styles:\n            Default node styles according to graphviz node attributes.\n        :type node_styles: dict[str|Token, dict[str, str]]\n\n        :param depth:\n            Depth of sub-dispatch plots. If negative all levels are plotted.\n        :type depth: int, optional\n\n        :param name:\n            Graph name used in the source code.\n        :type name: str\n\n        :param comment:\n            Comment added to the first line of the source.\n        :type comment: str\n\n        :param directory:\n            (Sub)directory for source saving and rendering.\n        :type directory: str, optional\n\n        :param format:\n            Rendering output format ('pdf', 'png', ...).\n        :type format: str, optional\n\n        :param engine:\n            Layout command used ('dot', 'neato', ...).\n        :type engine: str, optional\n\n        :param encoding:\n            Encoding for saving the source.\n        :type encoding: str, optional\n\n        :param graph_attr:\n            Dict of (attribute, value) pairs for the graph.\n        :type graph_attr: dict, optional\n\n        :param node_attr:\n            Dict of (attribute, value) pairs set for all nodes.\n        :type node_attr: dict, optional\n\n        :param edge_attr:\n            Dict of (attribute, value) pairs set for all edges.\n        :type edge_attr: dict, optional\n\n        :param body:\n            Dict of (attribute, value) pairs to add to the graph body.\n        :type body: dict, optional\n\n        :param directory:\n            Where is the generated Flask app root located?\n        :type directory: str, optional\n\n        :param sites:\n            A set of :class:`~schedula.utils.drw.Site` to maintain alive the\n            backend server.\n        :type sites: set[~schedula.utils.drw.Site], optional\n\n        :param index:\n            Add the site index as first page?\n        :type index: bool, optional\n\n        :param max_lines:\n            Maximum number of lines for rendering node attributes.\n        :type max_lines: int, optional\n\n        :param max_width:\n            Maximum number of characters in a line to render node attributes.\n        :type max_width: int, optional\n\n        :param view:\n            Open the main page of the site?\n        :type view: bool, optional\n\n        :return:\n            A SiteMap.\n        :rtype: schedula.utils.drw.SiteMap\n\n        Example:\n\n        .. dispatcher:: dsp\n           :opt: graph_attr={'ratio': '1'}\n           :code:\n\n            >>> from schedula import Dispatcher\n            >>> dsp = Dispatcher(name='Dispatcher')\n            >>> def fun(a):\n            ...     return a + 1, a - 1\n            >>> dsp.add_function('fun', fun, ['a'], ['b', 'c'])\n            'fun'\n            >>> dsp.plot(view=False, graph_attr={'ratio': '1'})\n            SiteMap([(Dispatcher, SiteMap())])\n        \"\"\"\n\n        d = {\n            'name': name, 'comment': comment, 'format': format,\n            'engine': engine, 'encoding': encoding, 'graph_attr': graph_attr,\n            'node_attr': node_attr, 'edge_attr': edge_attr, 'body': body,\n        }\n        options = {\n            'digraph': {k: v for k, v in d.items() if v is not NONE} or NONE,\n            'node_styles': node_styles,\n            'node_data': node_data,\n            'node_function': node_function,\n            'edge_data': edge_data,\n            'max_lines': max_lines,  # 5\n            'max_width': max_width,  # 200\n        }\n        options = {k: v for k, v in options.items() if v is not NONE}\n        from .drw import SiteMap\n        from .sol import Solution\n\n        if workflow is None and isinstance(self, Solution):\n            workflow = True\n        else:\n            workflow = workflow or False\n\n        sitemap = SiteMap()\n        sitemap.add_items(self, workflow=workflow, depth=depth, **options)\n        if view:\n            import tempfile\n            directory = directory or tempfile.mkdtemp()\n            if sites is None:\n                sitemap.render(directory=directory, view=True, index=index)\n            else:\n                sites.add(sitemap.site(directory, view=True, index=index))\n        return sitemap"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a sub node of a dispatcher. :param node_ids: A sequence of node ids or a single node id. The id order identifies a dispatcher sub-level. :type node_ids: str :param node_attr: Output node attr. If the searched node does not have this attribute, all its attributes are returned. When 'auto', returns the \"default\" attributes of the searched node, which are: - for data node: its output, and if not exists, all its attributes. - for function and sub-dispatcher nodes: the 'function' attribute. When 'description', returns the \"description\" of the searched node, searching also in function or sub-dispatcher input/output description. When 'output', returns the data node output. When 'default_value', returns the data node default value. When 'value_type', returns the data node value's type. When `None`, returns the node attributes. :type node_attr: str, None, optional :return: Node attributes and its real path. :rtype: (T, (str, ...)) **Example**: .. dispatcher:: o :opt: graph_attr={'ratio': '1'}, depth=-1 >>> import schedula as sh >>> sub_dsp = sh.Dispatcher(name='Sub-dispatcher') >>> def fun(a, b): ... return a + b ... >>> sub_dsp.add_function('a + b', fun, ['a', 'b'], ['c']) 'a + b' >>> dispatch = sh.SubDispatch(sub_dsp, ['c'], output_type='dict') >>> dsp = sh.Dispatcher(name='Dispatcher') >>> dsp.add_function('Sub-dispatcher', dispatch, ['a'], ['b']) 'Sub-dispatcher' >>> o = dsp.dispatch(inputs={'a': {'a': 3, 'b': 1}}) ... Get the sub node output:: >>> dsp.get_node('Sub-dispatcher', 'c') (4, ('Sub-dispatcher', 'c')) >>> dsp.get_node('Sub-dispatcher', 'c', node_attr='type') ('data', ('Sub-dispatcher', 'c')) .. dispatcher:: sub_dsp :opt: workflow=True, graph_attr={'ratio': '1'} :code: >>> sub_dsp, sub_dsp_id = dsp.get_node('Sub-dispatcher')", "response": "def get_node(self, *node_ids, node_attr=NONE):\n        \"\"\"\n        Returns a sub node of a dispatcher.\n\n        :param node_ids:\n            A sequence of node ids or a single node id. The id order identifies\n            a dispatcher sub-level.\n        :type node_ids: str\n\n        :param node_attr:\n            Output node attr.\n\n            If the searched node does not have this attribute, all its\n            attributes are returned.\n\n            When 'auto', returns the \"default\" attributes of the searched node,\n            which are:\n\n              - for data node: its output, and if not exists, all its\n                attributes.\n              - for function and sub-dispatcher nodes: the 'function' attribute.\n\n            When 'description', returns the \"description\" of the searched node,\n            searching also in function or sub-dispatcher input/output\n            description.\n\n            When 'output', returns the data node output.\n\n            When 'default_value', returns the data node default value.\n\n            When 'value_type', returns the data node value's type.\n\n            When `None`, returns the node attributes.\n        :type node_attr: str, None, optional\n\n        :return:\n            Node attributes and its real path.\n        :rtype: (T, (str, ...))\n\n        **Example**:\n\n        .. dispatcher:: o\n           :opt: graph_attr={'ratio': '1'}, depth=-1\n\n            >>> import schedula as sh\n            >>> sub_dsp = sh.Dispatcher(name='Sub-dispatcher')\n            >>> def fun(a, b):\n            ...     return a + b\n            ...\n            >>> sub_dsp.add_function('a + b', fun, ['a', 'b'], ['c'])\n            'a + b'\n            >>> dispatch = sh.SubDispatch(sub_dsp, ['c'], output_type='dict')\n            >>> dsp = sh.Dispatcher(name='Dispatcher')\n            >>> dsp.add_function('Sub-dispatcher', dispatch, ['a'], ['b'])\n            'Sub-dispatcher'\n\n            >>> o = dsp.dispatch(inputs={'a': {'a': 3, 'b': 1}})\n            ...\n\n        Get the sub node output::\n\n            >>> dsp.get_node('Sub-dispatcher', 'c')\n            (4, ('Sub-dispatcher', 'c'))\n            >>> dsp.get_node('Sub-dispatcher', 'c', node_attr='type')\n            ('data', ('Sub-dispatcher', 'c'))\n\n        .. dispatcher:: sub_dsp\n           :opt: workflow=True, graph_attr={'ratio': '1'}\n           :code:\n\n            >>> sub_dsp, sub_dsp_id = dsp.get_node('Sub-dispatcher')\n        \"\"\"\n        kw = {}\n\n        from .sol import Solution\n        if node_attr is NONE:\n            node_attr = 'output' if isinstance(self, Solution) else 'auto'\n\n        if isinstance(self, Solution):\n            kw['solution'] = self\n\n        from .alg import get_sub_node\n        dsp = getattr(self, 'dsp', self)\n\n        # Returns the node.\n        return get_sub_node(dsp, node_ids, node_attr=node_attr, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _api_get(self, url, **kwargs):\n        kwargs['url'] = self.url + url\n        kwargs['auth'] = self.auth\n\n        headers = deepcopy(self.headers)\n        headers.update(kwargs.get('headers', {}))\n        kwargs['headers'] = headers\n        return self._get(**kwargs)", "response": "A convenience wrapper for _get. Adds headers auth and base url by\n        default\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _put(self, *args, **kwargs):\n        if 'data' in kwargs:\n            kwargs['data'] = json.dumps(kwargs['data'])\n        response = requests.put(*args, **kwargs)\n        response.raise_for_status()", "response": "A wrapper for putting things. It will also json encode your 'data' parameter\n\n        :returns: The response of your put\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _api_post(self, url, **kwargs):\n        kwargs['url'] = self.url + url\n        kwargs['auth'] = self.auth\n\n        headers = deepcopy(self.headers)\n        headers.update(kwargs.get('headers', {}))\n        kwargs['headers'] = headers\n        self._post(**kwargs)", "response": "A convenience wrapper for _post. Adds headers auth and base url by\n        default\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap for requests. post", "response": "def _post(self, *args, **kwargs):\n        \"\"\"\n        A wrapper for posting things. It will also json encode your 'data' parameter\n\n        :returns: The response of your post\n        :rtype: dict\n        \"\"\"\n        if 'data' in kwargs:\n            kwargs['data'] = json.dumps(kwargs['data'])\n        response = requests.post(*args, **kwargs)\n        response.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch for translations files matching the locale paths and the apps paths.", "response": "def discover_resources():\n    \"\"\"\n    Searches for translations files matching [catalog].[lang].[format]\n\n    Traverses TRANZ_LOCALE_PATHS:\n    -\n    | TRANZ_LOCALE_PATHS\n     +- messages.fr.yml\n      | messages.en.yml\n\n    And apps paths if TRANZ_SEARCH_LOCALE_IN_APPS is set to True):\n    -\n    | app_path\n    +- TRANZ_DIR_NAME\n     +- messages.fr.yml\n      | messages.en.yml\n\n    @rtype: list\n    @return: A list of all found translation files\n    \"\"\"\n    locale_discovery_paths = list(settings.TRANZ_LOCALE_PATHS)\n    if settings.TRANZ_SEARCH_LOCALE_IN_APPS:\n        locale_discovery_paths += [os.path.join(app.path, settings.TRANZ_DIR_NAME) for app in list(apps.app_configs.values())]\n        \n    APP_LANGUAGES = [l[0] for l in settings.TRANZ_LANGUAGES]\n\n    resources = []\n    for path in locale_discovery_paths:\n        if not os.path.isdir(path):\n            continue\n\n        # Try to match direct children or discovery paths\n        for file in os.listdir(path):\n            if os.path.isfile(os.path.join(path, file)):\n                try:\n                    domain, lang, format = file.split('.')\n                except ValueError as e:\n                    continue\n                resources.append((format, os.path.join(path, file), lang, domain))\n        \n        \n        # Try to match django's LC_MESSAGES directories\n        if settings.TRANZ_REPLACE_DJANGO_TRANSLATIONS:\n            for lang in APP_LANGUAGES:\n                if os.path.isdir(os.path.join(path, lang)):\n                    LC_MESSAGES_PATH = os.path.join(path, lang, 'LC_MESSAGES')\n                    if os.path.isdir(LC_MESSAGES_PATH):\n                        for file in os.listdir(LC_MESSAGES_PATH):\n                            try:\n                                domain, format = file.split('.')\n                            except ValueError as e:\n                                continue\n                            resources.append((format, os.path.join(LC_MESSAGES_PATH, file), lang, domain))\n    return resources"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getAsKmlGrid(self, session, path=None, documentName=None, colorRamp=ColorRampEnum.COLOR_RAMP_HUE, alpha=1.0,\n                     noDataValue=None):\n        \"\"\"\n        Retrieve the raster as a KML document with each cell of the raster represented as a vector polygon. The result\n        is a vector grid of raster cells. Cells with the no data value are excluded.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n            path (str, optional): Path to file where KML file will be written. Defaults to None.\n            documentName (str, optional): Name of the KML document. This will be the name that appears in the legend.\n                Defaults to 'Stream Network'.\n            colorRamp (:mod:`mapkit.ColorRampGenerator.ColorRampEnum` or dict, optional): Use ColorRampEnum to select a\n                default color ramp or a dictionary with keys 'colors' and 'interpolatedPoints' to specify a custom color\n                ramp. The 'colors' key must be a list of RGB integer tuples (e.g.: (255, 0, 0)) and the\n                'interpolatedPoints' must be an integer representing the number of points to interpolate between each\n                color given in the colors list.\n            alpha (float, optional): Set transparency of visualization. Value between 0.0 and 1.0 where 1.0 is 100%\n                opaque and 0.0 is 100% transparent. Defaults to 1.0.\n            noDataValue (float, optional): The value to treat as no data when generating visualizations of rasters.\n                Defaults to 0.0.\n\n        Returns:\n            str: KML string\n        \"\"\"\n        if type(self.raster) != type(None):\n            # Set Document Name\n            if documentName is None:\n                try:\n                    documentName = self.filename\n                except AttributeError:\n                    documentName = 'default'\n\n            # Set no data value to default\n            if noDataValue is None:\n                noDataValue = self.defaultNoDataValue\n\n            # Make sure the raster field is valid\n            converter = RasterConverter(sqlAlchemyEngineOrSession=session)\n\n            # Configure color ramp\n            if isinstance(colorRamp, dict):\n                converter.setCustomColorRamp(colorRamp['colors'], colorRamp['interpolatedPoints'])\n            else:\n                converter.setDefaultColorRamp(colorRamp)\n\n            kmlString = converter.getAsKmlGrid(tableName=self.tableName,\n                                               rasterId=self.id,\n                                               rasterIdFieldName='id',\n                                               rasterFieldName=self.rasterColumnName,\n                                               documentName=documentName,\n                                               alpha=alpha,\n                                               noDataValue=noDataValue,\n                                               discreet=self.discreet)\n\n            if path:\n                with open(path, 'w') as f:\n                    f.write(kmlString)\n\n            return kmlString", "response": "Returns the raster as a KML grid of the raster cells."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the raster as a PNG image ground overlay KML file.", "response": "def getAsKmlPng(self, session, path=None, documentName=None, colorRamp=ColorRampEnum.COLOR_RAMP_HUE, alpha=1.0,\n                    noDataValue=None, drawOrder=0, cellSize=None, resampleMethod='NearestNeighbour'):\n        \"\"\"\n        Retrieve the raster as a PNG image ground overlay KML format. Coarse grid resolutions must be resampled to\n        smaller cell/pixel sizes to avoid a \"fuzzy\" look. Cells with the no data value are excluded.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n            path (str, optional): Path to file where KML file will be written. Defaults to None.\n            documentName (str, optional): Name of the KML document. This will be the name that appears in the legend.\n                Defaults to 'Stream Network'.\n            colorRamp (:mod:`mapkit.ColorRampGenerator.ColorRampEnum` or dict, optional): Use ColorRampEnum to select a\n                default color ramp or a dictionary with keys 'colors' and 'interpolatedPoints' to specify a custom color\n                ramp. The 'colors' key must be a list of RGB integer tuples (e.g.: (255, 0, 0)) and the\n                'interpolatedPoints' must be an integer representing the number of points to interpolate between each\n                color given in the colors list.\n            alpha (float, optional): Set transparency of visualization. Value between 0.0 and 1.0 where 1.0 is 100%\n                opaque and 0.0 is 100% transparent. Defaults to 1.0.\n            noDataValue (float, optional): The value to treat as no data when generating visualizations of rasters.\n                Defaults to 0.0.\n            drawOrder (int, optional): Set the draw order of the images. Defaults to 0.\n            cellSize (float, optional): Define the cell size in the units of the project projection at which to resample\n                the raster to generate the PNG. Defaults to None which will cause the PNG to be generated with the\n                original raster cell size. It is generally better to set this to a size smaller than the original cell\n                size to obtain a higher resolution image. However, computation time increases exponentially as the cell\n                size is decreased.\n            resampleMethod (str, optional): If cellSize is set, this method will be used to resample the raster. Valid\n                values include: NearestNeighbour, Bilinear, Cubic, CubicSpline, and Lanczos. Defaults to\n                NearestNeighbour.\n\n        Returns:\n            (str, list): Returns a KML string and a list of binary strings that are the PNG images.\n        \"\"\"\n        if type(self.raster) != type(None):\n            # Set Document Name\n            if documentName is None:\n                try:\n                    documentName = self.filename\n                except AttributeError:\n                    documentName = 'default'\n\n            # Set no data value to default\n            if noDataValue is None:\n                noDataValue = self.defaultNoDataValue\n\n            # Make sure the raster field is valid\n            converter = RasterConverter(sqlAlchemyEngineOrSession=session)\n\n            # Configure color ramp\n            if isinstance(colorRamp, dict):\n                converter.setCustomColorRamp(colorRamp['colors'], colorRamp['interpolatedPoints'])\n            else:\n                converter.setDefaultColorRamp(colorRamp)\n\n            kmlString, binaryPngString = converter.getAsKmlPng(tableName=self.tableName,\n                                                               rasterId=self.id,\n                                                               rasterIdFieldName='id',\n                                                               rasterFieldName=self.rasterColumnName,\n                                                               documentName=documentName,\n                                                               alpha=alpha,\n                                                               drawOrder=drawOrder,\n                                                               noDataValue=noDataValue,\n                                                               cellSize=cellSize,\n                                                               resampleMethod=resampleMethod,\n                                                               discreet=self.discreet)\n            if path:\n                directory = os.path.dirname(path)\n                archiveName = (os.path.split(path)[1]).split('.')[0]\n                kmzPath = os.path.join(directory, (archiveName + '.kmz'))\n\n                with ZipFile(kmzPath, 'w') as kmz:\n                    kmz.writestr(archiveName + '.kml', kmlString)\n                    kmz.writestr('raster.png', binaryPngString)\n\n            return kmlString, binaryPngString"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the raster in the GRASS ASCII Grid format.", "response": "def getAsGrassAsciiGrid(self, session):\n        \"\"\"\n        Retrieve the raster in the GRASS ASCII Grid format.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n\n        Returns:\n            str: GRASS ASCII string.\n        \"\"\"\n        if type(self.raster) != type(None):\n            # Make sure the raster field is valid\n            converter = RasterConverter(sqlAlchemyEngineOrSession=session)\n\n            return converter.getAsGrassAsciiRaster(tableName=self.tableName,\n                                                   rasterIdFieldName='id',\n                                                   rasterId=self.id,\n                                                   rasterFieldName=self.rasterColumnName)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_context(module_name='thread'):\n    '''\n    This function initializes the context of execution deciding which\n    type of threads are being used: classic python threads or green\n    threads, provided by Gevent.\n\n    This should be called first of all in every execution, otherwise,\n    the library would not work.\n\n    The default module is 'thread'.\n\n    :param str. module_name: Name of the module you want to use\n        ('thread' or 'green_thread').\n    '''\n    global core_type\n    if core_type is None and module_name in available_types:\n        core_type = module_name\n        util.core_type = core_type\n        global actor\n        actor = __import__('pyactor.' + module_name + '.actor', globals(),\n                           locals(), ['Actor', 'ActorRef'], -1)\n        global intervals\n        intervals = __import__('pyactor.' + module_name + '.intervals',\n                               globals(), locals(),\n                               ['interval_host', 'sleep', 'later'], -1)\n        global parallels\n        parallels = __import__('pyactor.' + module_name + '.parallels',\n                               globals(), locals(),\n                               ['ActorParallel'], -1)\n        global future\n        future = __import__('pyactor.' + module_name + '.future',\n                            globals(), locals(), ['FutureManager'], -1)\n        set_actor(module_name)\n        global rpcactor\n        rpcactor = __import__('pyactor.' + module_name + '.rpcactor',\n                              globals(), locals(), ['RPCDispatcher'], -1)\n        global signal\n        if module_name == 'green_thread':\n            signal = __import__('gevent', ['signal'])\n        else:\n            signal = __import__('signal', ['signal'])\n    else:\n        raise Exception('Bad core type.')", "response": "This function initializes the context of the given module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_host(url=\"local://local:6666/host\"):\n    '''\n    This is the main function to create a new Host to which you can\n    spawn actors. It will be set by default at local address if no\n    parameter *url* is given, which would result in remote\n    incomunication between hosts. This function shuould be called once\n    for execution or after callig :meth:`~.shutdown` to the previous\n    host.\n\n    Nevertheless, it is possible to create locally more than one host\n    and simulate a remote communication between them if they are of some\n    remote type (`http` or `amqp`), but the first one created will\n    be the main host, which is the one that will host the queries from\n    the main function.\n    Of course, every host must be initialized with a diferent URL(port).\n    Although that, more than one host should not be requiered for any real\n    project.\n\n    :param str. url: URL where to start and bind the host.\n    :return: :class:`~.Proxy` to the new host created.\n    :raises: Exception if there is a host already created with that URL.\n    '''\n    if url in util.hosts.keys():\n        raise HostError('Host already created. Only one host can' +\n                        ' be ran with the same url.')\n    else:\n        if not util.hosts:\n            util.main_host = Host(url)\n            util.hosts[url] = util.main_host\n        else:\n            util.hosts[url] = Host(url)\n        return util.hosts[url].proxy", "response": "This function creates a new host that can be used to spawn actors from a remote host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop the Host passed by parameter or all of them if url is None.", "response": "def shutdown(url=None):\n    '''\n    Stops the Host passed by parameter or all of them if none is\n    specified, stopping at the same time all its actors.\n    Should be called at the end of its usage, to finish correctly\n    all the connections and threads.\n    '''\n    if url is None:\n        for host in util.hosts.values():\n            host.shutdown()\n        global core_type\n        core_type = None\n    else:\n        host = util.hosts[url]\n        host.shutdown()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serve_forever():\n    '''\n    This allows the host (main host) to keep alive indefinitely so its actors\n    can receive queries at any time.\n    The main thread stays blocked forever.\n    To kill the execution, press Ctrl+C.\n\n    See usage example in :ref:`sample6`.\n    '''\n    if not util.main_host.alive:\n        raise Exception(\"This host is already shutted down.\")\n    util.main_host.serving = True\n    signal.signal(SIGINT, signal_handler)\n    print 'Press Ctrl+C to kill the execution'\n    while util.main_host is not None and util.main_host.serving:\n        try:\n            sleep(1)\n        except Exception:\n            pass\n    print 'BYE!'", "response": "This function is used to serve a single node in a thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interval(host, time, actor, method, *args, **kwargs):\n    '''Creates an Event attached to the host for management that will\n    execute the *method* of the *actor* every *time* seconds.\n\n    See example in :ref:`sample_inter`\n\n    :param Proxy host: host that will manage the interval, commonly the\n        host of the actor.\n    :param int time: seconds for the intervals.\n    :param Proxy actor: actor to which make the call every *time* seconds.\n    :param Str. method: method of the *actor* to be called.\n    :param list args: arguments for *method*.\n    :return: :class:`Event` instance of the interval.\n    '''\n    call = getattr(actor, method, None)\n    if not callable(call):\n        raise IntervalError(\"The actor %s does not have the method %s.\"\n                            % (actor.get_id(), method))\n    if call.__class__.__name__ in [\"TellWrapper\", \"TellRefWrapper\"]:\n        # If the method is a normal tell, the interval thread can send\n        # the calls normally.\n        # It it is a Ref Tell, the proxies in the args would be parsed\n        # during the call to this very method. So the call can be made\n        # as a normall Tell. The actor will do the loads normally on the\n        # receive as it has its methods marked as ref.\n        if call.__class__.__name__ is \"TellRefWrapper\":\n            call.__call__ = TellWrapper.__call__\n\n        return intervals.interval_host(host, time, call, *args, **kwargs)\n    else:\n        raise IntervalError(\"The callable for the interval must be a tell \\\n            method of the actor.\")", "response": "Creates an Event attached to the host for management that will manage the interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets a timer that will call the method of the actor after the given time.", "response": "def later(timeout, actor, method, *args, **kwargs):\n    '''\n    Sets a timer that will call the *method* of the *actor* past *timeout*\n    seconds.\n\n    See example in :ref:`sample_inter`\n\n    :param int timeout: seconds until the method is called.\n    :param Proxy actor: actor to which make the call after *time* seconds.\n    :param Str. method: method of the *actor* to be called.\n    :param list args: arguments for *method*.\n    :return: manager of the later (Timer in thread,\n        Greenlet in green_thread)\n    '''\n    call = getattr(actor, method, None)\n    if not callable(call):\n        raise IntervalError(\"later: The actor %s does not have the method \\\n                             %s.\" % (actor.get_id(), method))\n    if call.__class__.__name__ in [\"TellWrapper\", \"TellRefWrapper\"]:\n        # As with the interval, args have already been dumped.\n        if call.__class__.__name__ is \"TellRefWrapper\":\n            call.__call__ = TellWrapper.__call__\n        return intervals.later(timeout, call, *args, **kwargs)\n    else:\n        raise IntervalError(\"The callable for the later must be a tell \\\n                             method of the actor.\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_transport(self, url):\n        '''\n        For remote communication. Sets the communication dispatcher of the host\n        at the address and port specified.\n\n        The scheme must be http if using a XMLRPC dispatcher.\n        amqp for RabbitMQ communications.\n\n        This methos is internal. Automatically called when creating the host.\n\n        :param str. url: URL where to bind the host. Must be provided in\n            the tipical form: 'scheme://address:port/hierarchical_path'\n        '''\n        aurl = urlparse(url)\n        addrl = aurl.netloc.split(':')\n        self.addr = addrl[0], addrl[1]\n        self.transport = aurl.scheme\n        self.host_url = aurl\n\n        if aurl.scheme == 'http':\n            self.launch_actor('http', rpcactor.RPCDispatcher(url, self, 'rpc'))\n\n        elif aurl.scheme == 'amqp':\n            self.launch_actor('amqp', rpcactor.RPCDispatcher(url, self,\n                                                             'rabbit'))", "response": "Load the transport and the host_url for remote communication."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the given id is used within the host by some actor.", "response": "def has_actor(self, aid):\n        '''\n        Checks if the given id is used in the host by some actor.\n\n        :param str. aid: identifier of the actor to check.\n        :return: True if the id is used within the host.\n        '''\n        url = '%s://%s/%s' % (self.transport, self.host_url.netloc, aid)\n        return url in self.actors.keys()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a new proxy that references to the actor identified by the given ID.", "response": "def lookup(self, aid):\n        '''\n        Gets a new proxy that references to the actor of this host\n        (only actors in this host) identified by the given ID.\n\n        This method can be called remotely synchronously.\n\n        :param str. aid: identifier of the actor you want.\n        :return: :class:`~.Proxy` of the actor requiered.\n        :raises: :class:`NotFoundError`  if the actor does not exist.\n        :raises: :class:`HostDownError`  if the host is down.\n        '''\n        if not self.alive:\n            raise HostDownError()\n        url = '%s://%s/%s' % (self.transport, self.host_url.netloc, aid)\n        if url in self.actors.keys():\n            return Proxy(self.actors[url])\n        else:\n            raise NotFoundError(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a proxy reference to the actor identified by the given URL.", "response": "def lookup_url(self, url, klass, module=None):\n        '''\n        Gets a proxy reference to the actor indicated by the URL in the\n        parameters. It can be a local reference or a remote direction to\n        another host.\n\n        This method can be called remotely synchronously.\n\n        :param srt. url: address that identifies an actor.\n        :param class klass: the class of the actor.\n        :param srt. module: if the actor class is not in the calling module,\n            you need to specify the module where it is here. Also, the *klass*\n            parameter change to be a string.\n        :return: :class:`~.Proxy` of the actor requested.\n        :raises: :class:`NotFoundError`, if the URL specified do not\n            correspond to any actor in the host.\n        :raises: :class:`HostDownError`  if the host is down.\n        :raises: :class:`HostError`  if there is an error looking for\n            the actor in another server.\n        '''\n        if not self.alive:\n            raise HostDownError()\n        aurl = urlparse(url)\n        if self.is_local(aurl):\n            if url not in self.actors.keys():\n                raise NotFoundError(url)\n            else:\n                return Proxy(self.actors[url])\n        else:\n            try:\n                dispatcher = self.actors[aurl.scheme]\n                if module is not None:\n                    try:\n                        module_ = __import__(module, globals(), locals(),\n                                             [klass], -1)\n                        klass_ = getattr(module_, klass)\n                    except Exception, e:\n                        raise HostError(\"At lookup_url: \" +\n                                        \"Import failed for module \" + module +\n                                        \", class \" + klass +\n                                        \". Check this values for the lookup.\" +\n                                        \" ERROR: \" + str(e))\n                elif isinstance(klass, (types.TypeType, types.ClassType)):\n                    klass_ = klass\n                else:\n                    raise HostError(\"The class specified to look up is\" +\n                                    \" not a class.\")\n                remote_actor = actor.ActorRef(url, klass_, dispatcher.channel)\n                return Proxy(remote_actor)\n            except HostError:\n                raise\n            except Exception, e:\n                raise HostError(\"ERROR looking for the actor on another \" +\n                                \"server. Hosts must \" +\n                                \"be in http to work properly. \" + str(e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dumps(self, param):\n        '''\n        Checks the parameters generating new proxy instances to avoid\n        query concurrences from shared proxies and creating proxies for\n        actors from another host.\n        '''\n        if isinstance(param, Proxy):\n            module_name = param.actor.klass.__module__\n            filename = sys.modules[module_name].__file__\n            return ProxyRef(param.actor.url, param.actor.klass.__name__,\n                            module_name)\n        elif isinstance(param, list):\n            return [self.dumps(elem) for elem in param]\n        elif isinstance(param, dict):\n            new_dict = param\n            for key in new_dict.keys():\n                new_dict[key] = self.dumps(new_dict[key])\n            return new_dict\n        elif isinstance(param, tuple):\n            return tuple([self.dumps(elem) for elem in param])\n        else:\n            return param", "response": "Dumps the parameters to avoid new proxies for the same object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the return parameters generating new proxy instances to avoid query concurrences from shared proxies and creating new proxy instances for actors from another host.", "response": "def loads(self, param):\n        '''\n        Checks the return parameters generating new proxy instances to\n        avoid query concurrences from shared proxies and creating\n        proxies for actors from another host.\n        '''\n        if isinstance(param, ProxyRef):\n            try:\n                return self.lookup_url(param.url, param.klass, param.module)\n            except HostError:\n                print \"Can't lookup for the actor received with the call. \\\n                    It does not exist or the url is unreachable.\", param\n                raise HostError(param)\n        elif isinstance(param, list):\n            return [self.loads(elem) for elem in param]\n        elif isinstance(param, tuple):\n            return tuple([self.loads(elem) for elem in param])\n        elif isinstance(param, dict):\n            new_dict = param\n            for key in new_dict.keys():\n                new_dict[key] = self.loads(new_dict[key])\n            return new_dict\n        else:\n            return param"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_parallel(self, function, *params):\n        '''\n        Register a new thread executing a parallel method.\n        '''\n        # Create a pool if not created (processes or Gevent...)\n        if self.ppool is None:\n            if core_type == 'thread':\n                from multiprocessing.pool import ThreadPool\n                self.ppool = ThreadPool(500)\n            else:\n                from gevent.pool import Pool\n                self.ppool = Pool(500)\n        # Add the new task to the pool\n        self.ppool.apply_async(function, *params)", "response": "Register a new thread executing a parallel method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_uri(config_uri):\n    if isinstance(config_uri, PlasterURL):\n        return config_uri\n\n    # force absolute paths to look like a uri for more accurate parsing\n    # we throw away the dummy scheme later and parse it from the resolved\n    # path extension\n    isabs = os.path.isabs(config_uri)\n    if isabs:\n        config_uri = 'dummy://' + config_uri\n\n    # check if the uri is actually a url\n    parts = urlparse.urlparse(config_uri)\n\n    # reconstruct the path without the scheme and fragment\n    path = urlparse.ParseResult(\n        scheme='',\n        netloc=parts.netloc,\n        path=parts.path,\n        params='',\n        query='',\n        fragment='',\n    ).geturl()\n    # strip off leading //\n    if path.startswith('//'):\n        path = path[2:]\n\n    if parts.scheme and not isabs:\n        scheme = parts.scheme\n\n    else:\n        scheme = os.path.splitext(path)[1]\n        if scheme.startswith('.'):\n            scheme = scheme[1:]\n\n        # tag uris coming from file extension as file+scheme\n        if scheme:\n            scheme = 'file+' + scheme\n\n    query = parts.query if parts.query else None\n    options = OrderedDict()\n    if query:\n        options.update(urlparse.parse_qsl(query))\n    fragment = parts.fragment if parts.fragment else None\n\n    if not scheme:\n        raise InvalidURI(config_uri, (\n            'Could not determine the loader scheme for the supplied '\n            'config_uri \"{0}\"'.format(config_uri)))\n\n    return PlasterURL(\n        scheme=scheme,\n        path=path,\n        options=options,\n        fragment=fragment,\n    )", "response": "Parse the supplied config_uri into a PlasterURL object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recurse(self, root_path, dir_cb, listing_cb, max_listing_size=0, \n                max_depth=MAX_REMOTE_RECURSION_DEPTH):\n        \"\"\"Recursively iterate a directory. Invoke callbacks for directories \n        and entries (both are optional, but it doesn't make sense unless one is \n        provided). \"max_listing_size\" will allow for the file-listing to be \n        chunked into manageable pieces. \"max_depth\" limited how deep recursion \n        goes. This can be used to make it easy to simply read a single \n        directory in chunks.\n        \"\"\"\n                \n        q = deque([(root_path, 0)])\n        collected = []\n\n        def push_file(path, file_path, entry):\n            collected.append((file_path, entry))\n            if max_listing_size > 0 and \\\n               len(collected) >= max_listing_size:\n                listing_cb(path, collected)\n\n                # Clear contents on the list. We delete it this way so that \n                # we're only -modifying- the list rather than replacing it (a \n                # requirement of a closure).\n                del collected[:]\n\n        while q:\n            (path, current_depth) = q.popleft()\n\n            entries = self.listdir(path)\n            for entry in entries:\n                filename = stringify(entry.name)\n                file_path = ('%s/%s' % (path, filename))\n\n                if entry.is_symlink:\n                    push_file(path, file_path, entry)\n                elif entry.is_directory:\n                    if filename == '.' or filename == '..':\n                        continue\n\n                    if dir_cb is not None:\n                        dir_cb(path, file_path, entry)\n\n                    new_depth = current_depth + 1\n                    \n                    if max_depth is None or new_depth <= max_depth:\n                        q.append((file_path, new_depth))\n                elif entry.is_regular:\n                    if listing_cb is not None:\n                        push_file(path, file_path, entry)\n\n        if listing_cb is not None and (max_listing_size == 0 or \n                                       len(collected) > 0):\n            listing_cb(path, collected)", "response": "Recursively iterate a directory and call a callback for directories \n        and entries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_to_local(self, filepath_from, filepath_to, mtime_dt=None):\n\n        self.__log.debug(\"Writing R[%s] -> L[%s].\" % (filepath_from, \n                                                      filepath_to))\n\n        with SftpFile(self, filepath_from, 'r') as sf_from:\n            with open(filepath_to, 'wb') as file_to:\n                while 1:\n                    part = sf_from.read(MAX_MIRROR_WRITE_CHUNK_SIZE)\n                    file_to.write(part)\n\n                    if len(part) < MAX_MIRROR_WRITE_CHUNK_SIZE:\n                        break\n\n        if mtime_dt is None:\n            mtime_dt = datetime.now()\n\n        mtime_epoch = mktime(mtime_dt.timetuple())\n        utime(filepath_to, (mtime_epoch, mtime_epoch))", "response": "Open a remote file and write it locally."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_to_remote(self, filepath_from, filepath_to, mtime_dt=None):\n\n        self.__log.debug(\"Writing L[%s] -> R[%s].\" % (filepath_from, \n                                                      filepath_to))\n\n        with open(filepath_from, 'rb') as file_from:\n            with SftpFile(self, filepath_to, 'w') as sf_to:\n                while 1:\n                    part = file_from.read(MAX_MIRROR_WRITE_CHUNK_SIZE)\n                    sf_to.write(part)\n\n                    if len(part) < MAX_MIRROR_WRITE_CHUNK_SIZE:\n                        break\n\n        if mtime_dt is None:\n            mtime_dt = datetime.now()\n\n        self.utimes_dt(filepath_to, mtime_dt, mtime_dt)", "response": "Open a local file and write it remotely."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an outer access mode to an inner access mode.", "response": "def __at_om_to_im(self, om):\n        \"\"\"Convert an \"outer\" access mode to an \"inner\" access mode.\n        Returns a tuple of:\n\n            (<system access mode>, <is append>, <is universal newlines>).\n        \"\"\"\n\n        original_om = om\n        \n        if om[0] == 'U':\n            om = om[1:]\n            is_um = True\n        else:\n            is_um = False\n\n        if om == 'r':\n            return (original_om, O_RDONLY, False, is_um)\n        elif om == 'w':\n            return (original_om, O_WRONLY | O_CREAT | O_TRUNC, False, is_um)\n        elif om == 'a':\n            return (original_om, O_WRONLY | O_CREAT, False, is_um)\n        elif om == 'r+':\n            return (original_om, O_RDWR | O_CREAT, False, is_um)\n        elif om == 'w+':\n            return (original_om, O_RDWR | O_CREAT | O_TRUNC, False, is_um)\n        elif om == 'a+':\n            return (original_om, O_RDWR | O_CREAT, True, is_um)\n        else:\n            raise Exception(\"Outer access mode [%s] is invalid.\" % \n                            (original_om))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, size=None):\n\n        if size is not None:\n            return self.__sf.read(size)\n\n        block_size = self.__class__.__block_size\n\n        b = bytearray()\n        received_bytes = 0\n        while 1:\n            partial = self.__sf.read(block_size)\n#            self.__log.debug(\"Reading (%d) bytes. (%d) bytes returned.\" % \n#                             (block_size, len(partial)))\n\n            b.extend(partial)\n            received_bytes += len(partial)\n\n            if len(partial) < block_size:\n                self.__log.debug(\"End of file.\")\n                break\n\n        self.__log.debug(\"Read (%d) bytes for total-file.\" % (received_bytes))\n\n        return b", "response": "Read a length of bytes. Return empty on EOF."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readline(self, size=None):\n\n# TODO: Add support for Unicode.\n        (line, nl) = self.__buffer.read_until_nl(self.__retrieve_data)\n\n        if self.__sf.access_type_has_universal_nl and nl is not None:\n            self.__newlines[nl] = True\n\n        return line", "response": "Read a single line of text with EOF."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __retrieve_data(self):\n\n        if self.__eof is True:\n            return b''\n\n        logging.debug(\"Reading another block.\")        \n        block = self.read(self.__block_size)\n        if block == b'':\n            self.__log.debug(\"We've encountered the EOF.\")\n            self.__eof = True\n\n        return block", "response": "Read more data from the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean_boundary_shapefile(shapefile_path):\n        wfg =  gpd.read_file(shapefile_path)\n        first_shape = wfg.iloc[0].geometry\n        if hasattr(first_shape, 'geoms'):\n            log.warning(\"MultiPolygon found in boundary. \"\n                        \"Picking largest area ...\")\n            # pick largest shape to be the watershed boundary\n            # and assume the other ones are islands to be removed\n            max_area = -9999.0\n            main_geom = None\n            for geom in first_shape.geoms:\n                if geom.area > max_area:\n                    main_geom = geom\n                    max_area = geom.area\n\n            # remove self intersections\n            if not main_geom.is_valid:\n                log.warning(\"Invalid geometry found in boundary. \"\n                            \"Attempting to self clean ...\")\n                main_geom = main_geom.buffer(0)\n            wfg.loc[0, 'geometry'] = main_geom\n            out_cleaned_boundary_shapefile = \\\n                os.path.splitext(shapefile_path)[0] +\\\n                str(uuid.uuid4()) +\\\n                '.shp'\n            wfg.to_file(out_cleaned_boundary_shapefile)\n            log.info(\"Cleaned boundary shapefile written to:\"\n                     \"{}\".format(out_cleaned_boundary_shapefile))\n            return out_cleaned_boundary_shapefile\n        return shapefile_path", "response": "Cleans the boundary shapefile to that there is only one main polygon."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_mask_from_shapefile(self, shapefile_path, cell_size):\n        # make sure paths are absolute as the working directory changes\n        shapefile_path = os.path.abspath(shapefile_path)\n        # ADD MASK\n        with tmp_chdir(self.project_directory):\n            mask_name = '{0}.msk'.format(self.project_manager.name)\n            msk_file = WatershedMaskFile(project_file=self.project_manager,\n                                         session=self.db_session)\n\n            msk_file.generateFromWatershedShapefile(shapefile_path,\n                                                    cell_size=cell_size,\n                                                    out_raster_path=mask_name,\n                                                    load_raster_to_db=self.load_rasters_to_db)", "response": "Adds a mask from a shapefile"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds elevation file to project", "response": "def set_elevation(self, elevation_grid_path, mask_shapefile):\n        \"\"\"\n        Adds elevation file to project\n        \"\"\"\n        # ADD ELEVATION FILE\n        ele_file = ElevationGridFile(project_file=self.project_manager,\n                                     session=self.db_session)\n        ele_file.generateFromRaster(elevation_grid_path,\n                                    mask_shapefile,\n                                    load_raster_to_db=self.load_rasters_to_db)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd outlet point to project", "response": "def set_outlet(self, latitude, longitude, outslope):\n        \"\"\"\n        Adds outlet point to project\n        \"\"\"\n        self.project_manager.setOutlet(latitude=latitude, longitude=longitude,\n                                       outslope=outslope)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_roughness(self,\n                      roughness=None,\n                      land_use_grid=None,\n                      land_use_grid_id=None,\n                      land_use_to_roughness_table=None):\n        \"\"\"\n        ADD ROUGHNESS FROM LAND COVER\n        See: http://www.gsshawiki.com/Project_File:Overland_Flow_%E2%80%93_Required\n        \"\"\"\n        if roughness is not None:\n            self.project_manager.setCard('MANNING_N', str(roughness))\n        elif land_use_grid is not None and (land_use_grid_id is not None \\\n                or land_use_to_roughness_table is not None):\n            # make sure paths are absolute as the working directory changes\n            land_use_grid = os.path.abspath(land_use_grid)\n            if land_use_to_roughness_table is not None:\n                land_use_to_roughness_table = os.path.abspath(land_use_to_roughness_table)\n\n            mapTableFile = MapTableFile(project_file=self.project_manager)\n            mapTableFile.addRoughnessMapFromLandUse(\"roughness\",\n                                                    self.db_session,\n                                                    land_use_grid,\n                                                    land_use_to_roughness_table=land_use_to_roughness_table,\n                                                    land_use_grid_id=land_use_grid_id)\n        else:\n            raise ValueError(\"Need to either set 'roughness', or need \"\n                             \"to set values from land use grid ...\")", "response": "Set roughness of a map from a land use grid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the event for the current model.", "response": "def set_event(self,\n                  simulation_start=None,\n                  simulation_duration=None,\n                  simulation_end=None,\n                  rain_intensity=2,\n                  rain_duration=timedelta(seconds=30*60),\n                  event_type='EVENT',\n                 ):\n        \"\"\"\n        Initializes event for GSSHA model\n        \"\"\"\n        # ADD TEMPORTAL EVENT INFORMAITON\n        if event_type == 'LONG_TERM':\n            self.event = LongTermMode(self.project_manager,\n                                      self.db_session,\n                                      self.project_directory,\n                                      simulation_start=simulation_start,\n                                      simulation_end=simulation_end,\n                                      simulation_duration=simulation_duration,\n                                     )\n        else: # 'EVENT'\n            self.event = EventMode(self.project_manager,\n                                   self.db_session,\n                                   self.project_directory,\n                                   simulation_start=simulation_start,\n                                   simulation_duration=simulation_duration,\n                                   )\n            self.event.add_uniform_precip_event(intensity=rain_intensity,\n                                                duration=rain_duration)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite project to directory", "response": "def write(self):\n        \"\"\"\n        Write project to directory\n        \"\"\"\n        # write data\n        self.project_manager.writeInput(session=self.db_session,\n                                        directory=self.project_directory,\n                                        name=self.project_manager.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mirror_to_local_no_recursion(self, path_from, path_to, \n                                     log_files=False):\n        \"\"\"Mirror a directory without descending into directories. Return a \n        list of subdirectory names (do not include full path). We will unlink \n        existing files without determining if they're just going to be \n        rewritten and then truncating them because it is our belief, based on \n        what little we could find, that unlinking is, usually, quicker than \n        truncating.\n        \"\"\"\n\n        self.__log.debug(\"Ensuring local target directory exists: %s\" % \n                         (path_to))\n\n        try:\n            mkdir(path_to)\n        except OSError:\n            already_exists = True\n            self.__log.debug(\"Local target already exists.\")\n        else:\n            already_exists = False\n            self.__log.debug(\"Local target created.\")\n\n        from_tuple = self.__get_remote_files(path_from)\n        to_tuple = self.__get_local_files(path_to)\n        delta_tuple = self.__get_deltas(from_tuple, to_tuple, log_files)\n\n        context = (from_tuple, path_from, path_to, delta_tuple)\n        ops = self.__get_local_ops()\n\n        return self.__fix_deltas_at_target(context, ops)", "response": "Mirror a directory without descending into directories. Return a list of subdirectory names that are not included in the local target."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlinks the link node dataset and the node dataset with the channel input file.", "response": "def linkToChannelInputFile(self, session, channelInputFile, force=False):\n        \"\"\"\n        Create database relationships between the link node dataset and the channel input file.\n\n        The link node dataset only stores references to the links and nodes--not the geometry. The link and node\n        geometries are stored in the channel input file. The two files must be linked with database relationships to\n        allow the creation of link node dataset visualizations.\n\n        This process is not performed automatically during reading, because it can be very costly in terms of read time.\n        This operation can only be performed after both files have been read into the database.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database\n            channelInputFile (:class:`gsshapy.orm.ChannelInputFile`): Channel input file object to be associated with\n                this link node dataset file.\n            force (bool, optional): Force channel input file reassignment. When false (default), channel input file\n                assignment is skipped if it has already been performed.\n        \"\"\"\n\n        # Only perform operation if the channel input file has not been assigned or the force parameter is true\n        if self.channelInputFile is not None and not force:\n            return\n\n        # Set the channel input file relationship\n        self.channelInputFile = channelInputFile\n\n        # Retrieve the fluvial stream links\n        orderedLinks = channelInputFile.getOrderedLinks(session)\n\n        # Retrieve the LinkNodeTimeStep objects\n        timeSteps = self.timeSteps\n\n        # Link each link dataset in each time step\n        for timeStep in timeSteps:\n            # Retrieve link datasets\n            linkDatasets = timeStep.linkDatasets\n\n            # Link each node dataset\n            for l, linkDataset in enumerate(linkDatasets):\n                # Get the fluvial link and nodes\n                streamLink = orderedLinks[l]\n                streamNodes = streamLink.nodes\n\n                # Link link datasets to fluvial links\n                linkDataset.link = streamLink\n\n                # Retrieve node datasets\n                nodeDatasets = linkDataset.nodeDatasets\n\n                # Link the node dataset with the channel input file nodes\n                if len(nodeDatasets) > 0 and len(streamNodes) > 0:\n                    for n, nodeDataset in enumerate(nodeDatasets):\n                        nodeDataset.node = streamNodes[n]\n\n        session.add(self)\n        session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a KML animation for the link node dataset file.", "response": "def getAsKmlAnimation(self, session, channelInputFile, path=None, documentName=None, styles={}):\n        \"\"\"\n        Generate a KML visualization of the the link node dataset file.\n\n        Link node dataset files are time stamped link node value datasets. This will yield a value for each stream node\n        at each time step that output is written. The resulting KML visualization will be an animation.\n\n        The stream nodes are represented by cylinders where the z dimension/elevation represents the values. A color\n        ramp is applied to make different values stand out even more. The method attempts to identify an appropriate\n        scale factor for the z dimension, but it can be set manually using the styles dictionary.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database\n            channelInputFile (:class:`gsshapy.orm.ChannelInputFile`): Channel input file object to be associated with\n                this link node dataset file.\n            path (str, optional): Path to file where KML will be written. Defaults to None.\n            documentName (str, optional): Name of the KML document. This will be the name that appears in the legend.\n                Defaults to the name of the link node dataset file.\n            styles (dict, optional): Custom styles to apply to KML geometry. Defaults to empty dictionary.\n\n                Valid keys (styles) include:\n                   * zScale (float): multiplier to apply to the values (z dimension)\n                   * radius (float): radius in meters of the node cylinder\n                   * colorRampEnum (:mod:`mapkit.ColorRampGenerator.ColorRampEnum` or dict): Use ColorRampEnum to select a default color ramp or a dictionary with keys 'colors' and 'interpolatedPoints' to specify a custom color ramp. The 'colors' key must be a list of RGB integer tuples (e.g.: (255, 0, 0)) and the 'interpolatedPoints' must be an integer representing the number of points to interpolate between each color given in the colors list.\n\n        Returns:\n            str: KML string\n        \"\"\"\n        # Constants\n        DECMIAL_DEGREE_METER = 0.00001\n        OPTIMAL_Z_MAX = 300  # meters\n\n        # Default styles\n        radiusMeters = 2 * DECMIAL_DEGREE_METER  # 2 meters\n        zScale = 1\n        colorRamp = ColorRampGenerator.generateDefaultColorRamp(ColorRampEnum.COLOR_RAMP_HUE)\n\n        # Validate\n        if not documentName:\n            documentName = self.name\n\n        if 'zScale' in styles:\n            try:\n                float(styles['zScale'])\n                zScale = styles['zScale']\n\n            except ValueError:\n                log.warning('zScale must be a valid number representing z dimension multiplier.')\n\n        if 'radius' in styles:\n            try:\n                float(styles['radius'])\n                radiusMeters = styles['radius'] * DECMIAL_DEGREE_METER\n\n            except ValueError:\n                log.warning('radius must be a number representing the radius of the value cylinders in meters.')\n\n        if 'colorRampEnum' in styles:\n            colorRampEnum = styles['colorRampEnum']\n\n            if isinstance(colorRampEnum, dict):\n                colorRamp = ColorRampGenerator.generateCustomColorRamp(colorRampEnum['colors'], colorRampEnum['interpolatedPoints'])\n            elif isinstance(colorRampEnum, int):\n                colorRamp = ColorRampGenerator.generateDefaultColorRamp(colorRampEnum)\n\n        # Link to channel input file\n        self.linkToChannelInputFile(session, channelInputFile)\n\n        # Create instance of GeometryConverter\n        converter = GeometryConverter(session)\n\n        # Get LinkNodeTimeSteps\n        linkNodeTimeSteps = self.timeSteps\n\n        # Get date time parameters\n        timeStepDelta = timedelta(minutes=self.timeStepInterval)\n        startDateTime = datetime(1970, 1, 1)\n        startTimeParts = self.startTime.split()\n\n        # Calculate min and max values for the color ramp\n        minValue = 0.0\n        maxValue = session.query(func.max(NodeDataset.value)).\\\n                           filter(NodeDataset.linkNodeDatasetFile == self).\\\n                           filter(NodeDataset.status == 1).\\\n                           scalar()\n\n        avgValue = session.query(func.avg(NodeDataset.value)).\\\n                           filter(NodeDataset.linkNodeDatasetFile == self).\\\n                           filter(NodeDataset.status == 1).\\\n                           scalar()\n\n        # Calculate automatic zScale if not assigned\n        if 'zScale' not in styles:\n            zScale = OPTIMAL_Z_MAX / ((maxValue + avgValue) / 2)\n\n        # Map color ramp to values\n        mappedColorRamp = ColorRampGenerator.mapColorRampToValues(colorRamp, minValue, maxValue)\n\n        if len(startTimeParts) > 5:\n            # Default start date time to epoch\n            startDateTime = datetime(year=int(startTimeParts[2]) or 1970,\n                                     month=int(startTimeParts[1]) or 1,\n                                     day=int(startTimeParts[0]) or 1,\n                                     hour=int(startTimeParts[3]) or 0,\n                                     minute=int(startTimeParts[4]) or 0)\n\n        # Start the Kml Document\n        kml = ET.Element('kml', xmlns='http://www.opengis.net/kml/2.2')\n        document = ET.SubElement(kml, 'Document')\n        docName = ET.SubElement(document, 'name')\n        docName.text = documentName\n\n        # Apply special style to hide legend items\n        style = ET.SubElement(document, 'Style', id='check-hide-children')\n        listStyle = ET.SubElement(style, 'ListStyle')\n        listItemType = ET.SubElement(listStyle, 'listItemType')\n        listItemType.text = 'checkHideChildren'\n        styleUrl = ET.SubElement(document, 'styleUrl')\n        styleUrl.text = '#check-hide-children'\n\n        for linkNodeTimeStep in linkNodeTimeSteps:\n            # Create current datetime objects\n            timeSpanBegin = startDateTime + (linkNodeTimeStep.timeStep * timeStepDelta)\n            timeSpanEnd = timeSpanBegin + timeStepDelta\n\n            # Get Link Datasets\n            linkDatasets = linkNodeTimeStep.linkDatasets\n\n            for linkDataset in linkDatasets:\n                # Don't process special link datasets (with node counts of -1 or 0)\n                if linkDataset.numNodeDatasets <= 0:\n                    break\n\n                # Get Node Datasets\n                nodeDatasets = linkDataset.nodeDatasets\n\n                for nodeDataset in nodeDatasets:\n                    # Get node\n                    node = nodeDataset.node\n                    link = node.streamLink\n                    extrude = nodeDataset.value\n\n                    # Don't extrude below 0\n                    if nodeDataset.value < 0.0:\n                        extrude = 0.0\n\n                    # Convert to circle\n                    circleString = converter.getPointAsKmlCircle(tableName=node.tableName,\n                                                                 radius=radiusMeters,\n                                                                 extrude=extrude,\n                                                                 zScaleFactor=zScale,\n                                                                 geometryId=node.id)\n\n                    # Convert alpha from 0.0-1.0 decimal to 00-FF string\n                    integerAlpha = mappedColorRamp.getAlphaAsInteger()\n\n                    # Get RGB color from color ramp and convert to KML hex ABGR string with alpha\n                    integerRGB = mappedColorRamp.getColorForValue(nodeDataset.value)\n\n                    # Make color ABGR string\n                    colorString = '%02X%02X%02X%02X' % (integerAlpha,\n                                                        integerRGB[mappedColorRamp.B],\n                                                        integerRGB[mappedColorRamp.G],\n                                                        integerRGB[mappedColorRamp.R])\n\n                    # Create placemark\n                    placemark = ET.SubElement(document, 'Placemark')\n\n                    # Create style tag and setup styles\n                    style = ET.SubElement(placemark, 'Style')\n\n                    # Set polygon line style\n                    lineStyle = ET.SubElement(style, 'LineStyle')\n\n                    # Disable lines by setting line width to 0\n                    lineWidth = ET.SubElement(lineStyle, 'width')\n                    lineWidth.text = str(0)\n\n                    # Set polygon fill color\n                    polyStyle = ET.SubElement(style, 'PolyStyle')\n                    polyColor = ET.SubElement(polyStyle, 'color')\n                    polyColor.text = colorString\n\n                    if len(linkNodeTimeSteps) > 1:\n                        # Create TimeSpan tag\n                        timeSpan = ET.SubElement(placemark, 'TimeSpan')\n\n                        # Create begin and end tags\n                        begin = ET.SubElement(timeSpan, 'begin')\n                        begin.text = timeSpanBegin.strftime('%Y-%m-%dT%H:%M:%S')\n                        end = ET.SubElement(timeSpan, 'end')\n                        end.text = timeSpanEnd.strftime('%Y-%m-%dT%H:%M:%S')\n\n                    # Append geometry\n                    polygonCircle = ET.fromstring(circleString)\n                    placemark.append(polygonCircle)\n\n                    # Embed node data\n                    nodeExtendedData = ET.SubElement(placemark, 'ExtendedData')\n\n                    nodeNumberData = ET.SubElement(nodeExtendedData, 'Data', name='node_number')\n                    nodeNumberValue = ET.SubElement(nodeNumberData, 'value')\n                    nodeNumberValue.text = str(node.nodeNumber)\n\n                    nodeLinkNumberData = ET.SubElement(nodeExtendedData, 'Data', name='link_number')\n                    nodeLinkNumberValue = ET.SubElement(nodeLinkNumberData, 'value')\n                    nodeLinkNumberValue.text = str(link.linkNumber)\n\n                    nodeElevationData = ET.SubElement(nodeExtendedData, 'Data', name='value')\n                    nodeElevationValue = ET.SubElement(nodeElevationData, 'value')\n                    nodeElevationValue.text = str(nodeDataset.value)\n\n\n        kmlString = ET.tostring(kml)\n\n        if path:\n            with open(path, 'w') as f:\n                f.write(kmlString)\n\n        return kmlString"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Dictionary of keywords/cards and parse function names\n        KEYWORDS = ('NUM_LINKS',\n                    'TIME_STEP',\n                    'NUM_TS',\n                    'START_TIME',\n                    'TS')\n\n        # Parse file into chunks associated with keywords/cards\n        with open(path, 'r') as f:\n            self.name = f.readline().strip()\n            chunks = pt.chunk(KEYWORDS, f)\n\n        # Parse chunks associated with each key\n        for card, chunkList in iteritems(chunks):\n            # Parse each chunk in the chunk list\n            for chunk in chunkList:\n                schunk = chunk[0].strip().split()\n\n                # Cases\n                if card == 'NUM_LINKS':\n                    # NUM_LINKS handler\n                    self.numLinks = schunk[1]\n\n                elif card == 'TIME_STEP':\n                    # TIME_STEP handler\n                    self.timeStepInterval = schunk[1]\n\n                elif card == 'NUM_TS':\n                    # NUM_TS handler\n                    self.numTimeSteps = schunk[1]\n\n                elif card == 'START_TIME':\n                    # START_TIME handler\n                    self.startTime = '%s  %s    %s  %s  %s  %s' % (\n                        schunk[1],\n                        schunk[2],\n                        schunk[3],\n                        schunk[4],\n                        schunk[5],\n                        schunk[6])\n\n                elif card == 'TS':\n                    # TS handler\n                    for line in chunk:\n                        sline = line.strip().split()\n                        token = sline[0]\n\n                        # Cases\n                        if token == 'TS':\n                            # Time Step line handler\n                            timeStep = LinkNodeTimeStep(timeStep=sline[1])\n                            timeStep.linkNodeDataset = self\n\n                        else:\n                            # Split the line\n                            spLinkLine = line.strip().split()\n\n                            # Create LinkDataset GSSHAPY object\n                            linkDataset = LinkDataset()\n                            linkDataset.numNodeDatasets = int(spLinkLine[0])\n                            linkDataset.timeStep = timeStep\n                            linkDataset.linkNodeDatasetFile = self\n\n                            # Parse line into NodeDatasets\n                            NODE_VALUE_INCREMENT = 2\n                            statusIndex = 1\n                            valueIndex = statusIndex + 1\n\n                            # Parse line into node datasets\n                            if linkDataset.numNodeDatasets > 0:\n                                for i in range(0, linkDataset.numNodeDatasets):\n                                    # Create NodeDataset GSSHAPY object\n                                    nodeDataset = NodeDataset()\n                                    nodeDataset.status = int(spLinkLine[statusIndex])\n                                    nodeDataset.value = float(spLinkLine[valueIndex])\n                                    nodeDataset.linkDataset = linkDataset\n                                    nodeDataset.linkNodeDatasetFile = self\n\n                                    # Increment to next status/value pair\n                                    statusIndex += NODE_VALUE_INCREMENT\n                                    valueIndex += NODE_VALUE_INCREMENT\n                            else:\n                                nodeDataset = NodeDataset()\n                                nodeDataset.value = float(spLinkLine[1])\n                                nodeDataset.linkDataset = linkDataset\n                                nodeDataset.linkNodeDatasetFile = self", "response": "Method to read Link Node Dataset File Method to set properties of Link Node Dataset object"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlinks Node Dataset File Write to File Method", "response": "def _write(self, session, openFile, replaceParamFile):\n        \"\"\"\n        Link Node Dataset File Write to File Method\n        \"\"\"\n        # Retrieve TimeStep objects\n        timeSteps = self.timeSteps\n\n        # Write Lines\n        openFile.write('%s\\n' % self.name)\n        openFile.write('NUM_LINKS     %s\\n' % self.numLinks)\n        openFile.write('TIME_STEP     %s\\n' % self.timeStepInterval)\n        openFile.write('NUM_TS        %s\\n' % self.numTimeSteps)\n        openFile.write('START_TIME    %s\\n' % self.startTime)\n\n        for timeStep in timeSteps:\n            openFile.write('TS    %s\\n' % timeStep.timeStep)\n\n            # Retrieve LinkDataset objects\n            linkDatasets = timeStep.linkDatasets\n\n            for linkDataset in linkDatasets:\n                # Write number of node datasets values\n                openFile.write('{0}   '.format(linkDataset.numNodeDatasets))\n\n                # Retrieve NodeDatasets\n                nodeDatasets = linkDataset.nodeDatasets\n\n                if linkDataset.numNodeDatasets > 0:\n                    for nodeDataset in nodeDatasets:\n                        # Write status and value\n                        openFile.write('{0}  {1:.5f}   '.format(nodeDataset.status, nodeDataset.value))\n                else:\n                    for nodeDataset in nodeDatasets:\n                        # Write status and value\n\n                        if linkDataset.numNodeDatasets < 0:\n                            openFile.write('{0:.5f}'.format(nodeDataset.value))\n                        else:\n                            openFile.write('{0:.3f}'.format(nodeDataset.value))\n\n                # Write new line character after each link dataset\n                openFile.write('\\n')\n\n            # Insert empty line between time steps\n            openFile.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_card(self, card_name, new_value, add_quotes=False):\n        self.project_manager.setCard(card_name, new_value, add_quotes)", "response": "Updates the value of a card in the project file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_simulation_start_cards(self):\n        if self.simulation_start is not None:\n            self._update_card(\"START_DATE\", self.simulation_start.strftime(\"%Y %m %d\"))\n            self._update_card(\"START_TIME\", self.simulation_start.strftime(\"%H %M\"))", "response": "Update GSSHA cards for simulation start"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate simulation end time from LSM record.", "response": "def _update_simulation_end_from_lsm(self):\n        \"\"\"\n        Update simulation end time from LSM\n        \"\"\"\n        te = self.l2g.xd.lsm.datetime[-1]\n        simulation_end = te.replace(tzinfo=utc) \\\n                           .astimezone(tz=self.tz) \\\n                           .replace(tzinfo=None)\n\n        if self.simulation_end is None:\n            self.simulation_end = simulation_end\n        elif self.simulation_end > simulation_end:\n            self.simulation_end = simulation_end\n        self._update_card(\"END_TIME\",\n                          self.simulation_end\n                              .strftime(\"%Y %m %d %H %M\"))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_precip_file(self, precip_file_path, interpolation_type=None):\n        # precip file read in\n        self._update_card('PRECIP_FILE', precip_file_path, True)\n\n        if interpolation_type is None:\n            # check if precip type exists already in card\n            if not self.project_manager.getCard('RAIN_INV_DISTANCE') \\\n                    and not self.project_manager.getCard('RAIN_THIESSEN'):\n                # if no type exists, then make it theissen\n                self._update_card('RAIN_THIESSEN', '')\n        else:\n            if interpolation_type.upper() not in self.PRECIP_INTERP_TYPES:\n                raise IndexError(\"Invalid interpolation_type {0}\".format(interpolation_type))\n            interpolation_type = interpolation_type.upper()\n\n            if interpolation_type == \"INV_DISTANCE\":\n                self._update_card('RAIN_INV_DISTANCE', '')\n                self.project_manager.deleteCard('RAIN_THIESSEN', self.db_session)\n            else:\n                self._update_card('RAIN_THIESSEN', '')\n                self.project_manager.deleteCard('RAIN_INV_DISTANCE', self.db_session)", "response": "Adds a precip file to the project with interpolation_type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing GSSHA simulation for the given LSM precipitation variable.", "response": "def prepare_gag_lsm(self, lsm_precip_data_var, lsm_precip_type, interpolation_type=None):\n        \"\"\"\n        Prepares Gage output for GSSHA simulation\n\n        Parameters:\n            lsm_precip_data_var(list or str): String of name for precipitation variable name or list of precip variable names.  See: :func:`~gsshapy.grid.GRIDtoGSSHA.lsm_precip_to_gssha_precip_gage`.\n            lsm_precip_type(str): Type of precipitation. See: :func:`~gsshapy.grid.GRIDtoGSSHA.lsm_precip_to_gssha_precip_gage`.\n            interpolation_type(str): Type of interpolation for LSM precipitation. Can be \"INV_DISTANCE\" or \"THIESSEN\". Default is \"THIESSEN\".\n        \"\"\"\n        if self.l2g is None:\n            raise ValueError(\"LSM converter not loaded ...\")\n\n        # remove uniform precip cards\n        for unif_precip_card in self.UNIFORM_PRECIP_CARDS:\n            self.project_manager.deleteCard(unif_precip_card, self.db_session)\n\n        with tmp_chdir(self.project_manager.project_directory):\n            # PRECIPITATION CARD\n            out_gage_file = '{0}.gag'.format(self.project_manager.name)\n            self.l2g.lsm_precip_to_gssha_precip_gage(out_gage_file,\n                                                     lsm_data_var=lsm_precip_data_var,\n                                                     precip_type=lsm_precip_type)\n\n            # SIMULATION TIME CARDS\n            self._update_simulation_end_from_lsm()\n\n            self.set_simulation_duration(self.simulation_end-self.simulation_start)\n            # precip file read in\n            self.add_precip_file(out_gage_file, interpolation_type)\n\n            # make sure xarray dataset closed\n            self.l2g.xd.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_rapid_streamflow(self, path_to_rapid_qout, connection_list_file):\n        ihg_filename = '{0}.ihg'.format(self.project_manager.name)\n        with tmp_chdir(self.project_manager.project_directory):\n            # write out IHG file\n            time_index_range = []\n            with RAPIDDataset(path_to_rapid_qout, out_tzinfo=self.tz) as qout_nc:\n\n                time_index_range = qout_nc.get_time_index_range(date_search_start=self.simulation_start,\n                                                                date_search_end=self.simulation_end)\n\n                if len(time_index_range) > 0:\n                    time_array = qout_nc.get_time_array(return_datetime=True,\n                                                        time_index_array=time_index_range)\n\n                    # GSSHA STARTS INGESTING STREAMFLOW AT SECOND TIME STEP\n                    if self.simulation_start is not None:\n                        if self.simulation_start == time_array[0]:\n                            log.warning(\"First timestep of streamflow skipped \"\n                                     \"in order for GSSHA to capture the streamflow.\")\n                            time_index_range = time_index_range[1:]\n                            time_array = time_array[1:]\n\n                if len(time_index_range) > 0:\n                    start_datetime = time_array[0]\n\n                    if self.simulation_start is None:\n                       self._update_simulation_start(start_datetime)\n\n                    if self.simulation_end is None:\n                        self.simulation_end = time_array[-1]\n\n                    qout_nc.write_flows_to_gssha_time_series_ihg(ihg_filename,\n                                                                 connection_list_file,\n                                                                 date_search_start=start_datetime,\n                                                                 date_search_end=self.simulation_end,\n                                                                 )\n                else:\n                    log.warning(\"No streamflow values found in time range ...\")\n\n            if len(time_index_range) > 0:\n                # update cards\n                self._update_simulation_start_cards()\n\n                self._update_card(\"END_TIME\", self.simulation_end.strftime(\"%Y %m %d %H %M\"))\n                self._update_card(\"CHAN_POINT_INPUT\", ihg_filename, True)\n\n                # update duration\n                self.set_simulation_duration(self.simulation_end-self.simulation_start)\n\n                # UPDATE GMT CARD\n                self._update_gmt()\n            else:\n                # cleanup\n                os.remove(ihg_filename)\n                self.project_manager.deleteCard('CHAN_POINT_INPUT', self.db_session)", "response": "Prepares RAPID streamflow for GSSHA simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_simulation_duration(self, simulation_duration):\n        self.project_manager.setCard('TOT_TIME', str(simulation_duration.total_seconds()/60.0))\n        super(EventMode, self).set_simulation_duration(simulation_duration)\n        self.simulation_duration = simulation_duration", "response": "set the simulation duration of the event mode"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_uniform_precip_event(self, intensity, duration):\n        self.project_manager.setCard('PRECIP_UNIF', '')\n        self.project_manager.setCard('RAIN_INTENSITY', str(intensity))\n        self.project_manager.setCard('RAIN_DURATION', str(duration.total_seconds()/60.0))", "response": "Add a uniform precip event to the project manager."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the GMT card based on timezone and start date", "response": "def _update_gmt(self):\n        \"\"\"\n        Based on timezone and start date, the GMT card is updated\n        \"\"\"\n        if self.simulation_start is not None:\n            # NOTE: Because of daylight savings time,\n            # offset result depends on time of the year\n            offset_string = str(self.simulation_start.replace(tzinfo=self.tz)\n                                .utcoffset().total_seconds()/3600.)\n            self._update_card('GMT', offset_string)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_hmet_lsm(self, lsm_data_var_map_array,\n                         hmet_ascii_output_folder=None,\n                         netcdf_file_path=None):\n        \"\"\"\n        Prepares HMET data for GSSHA simulation from land surface model data.\n\n        Parameters:\n            lsm_data_var_map_array(str): Array with connections for LSM output and GSSHA input. See: :func:`~gsshapy.grid.GRIDtoGSSHA.`\n            hmet_ascii_output_folder(Optional[str]): Path to diretory to output HMET ASCII files. Mutually exclusice with netcdf_file_path. Default is None.\n            netcdf_file_path(Optional[str]): If you want the HMET data output as a NetCDF4 file for input to GSSHA. Mutually exclusice with hmet_ascii_output_folder. Default is None.\n        \"\"\"\n        if self.l2g is None:\n            raise ValueError(\"LSM converter not loaded ...\")\n\n        with tmp_chdir(self.project_manager.project_directory):\n            # GSSHA simulation does not work after HMET data is finished\n            self._update_simulation_end_from_lsm()\n\n            # HMET CARDS\n            if netcdf_file_path is not None:\n                self.l2g.lsm_data_to_subset_netcdf(netcdf_file_path, lsm_data_var_map_array)\n                self._update_card(\"HMET_NETCDF\", netcdf_file_path, True)\n                self.project_manager.deleteCard('HMET_ASCII', self.db_session)\n            else:\n                if \"{0}\" in hmet_ascii_output_folder and \"{1}\" in hmet_ascii_output_folder:\n                    hmet_ascii_output_folder = hmet_ascii_output_folder.format(self.simulation_start.strftime(\"%Y%m%d%H%M\"),\n                                                                               self.simulation_end.strftime(\"%Y%m%d%H%M\"))\n                self.l2g.lsm_data_to_arc_ascii(lsm_data_var_map_array,\n                                               main_output_folder=os.path.join(self.gssha_directory,\n                                                                          hmet_ascii_output_folder))\n                self._update_card(\"HMET_ASCII\", os.path.join(hmet_ascii_output_folder, 'hmet_file_list.txt'), True)\n                self.project_manager.deleteCard('HMET_NETCDF', self.db_session)\n\n            # UPDATE GMT CARD\n            self._update_gmt()", "response": "Prepares HMET data for GSSHA simulation from LSM input and GSSHA output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef terminal_width():\n    try:\n        if os.name == 'nt':\n            _WindowsCSBI.initialize()\n            return _WindowsCSBI.get_info(_WindowsCSBI.HANDLE_STDOUT)['terminal_width']\n        return struct.unpack('hhhh', fcntl.ioctl(0, termios.TIOCGWINSZ, '\\000' * 8))[1]\n    except IOError:\n        return 80", "response": "Returns the terminal s width."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of characters available in the terminal if the sample string was to be printed in the terminal.", "response": "def get_remaining_width(sample_string, max_terminal_width=None):\n    \"\"\"Returns the number of characters available if sample string were to be printed in the terminal.\n\n    Positional arguments:\n    sample_string -- gets the length of this string.\n\n    Keyword arguments:\n    max_terminal_width -- limit the overall width of everything to these many characters.\n\n    Returns:\n    Integer.\n    \"\"\"\n    if max_terminal_width is not None:\n        available_width = min(terminal_width(), max_terminal_width)\n    else:\n        available_width = terminal_width()\n    return available_width - len(sample_string)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefining structs and populates _WindowsCSBI. CSBI.", "response": "def _define_csbi():\n        \"\"\"Defines structs and populates _WindowsCSBI.CSBI.\"\"\"\n        if _WindowsCSBI.CSBI is not None:\n            return\n\n        class COORD(ctypes.Structure):\n            \"\"\"Windows COORD structure. http://msdn.microsoft.com/en-us/library/windows/desktop/ms682119\"\"\"\n            _fields_ = [('X', ctypes.c_short), ('Y', ctypes.c_short)]\n\n        class SmallRECT(ctypes.Structure):\n            \"\"\"Windows SMALL_RECT structure. http://msdn.microsoft.com/en-us/library/windows/desktop/ms686311\"\"\"\n            _fields_ = [('Left', ctypes.c_short), ('Top', ctypes.c_short), ('Right', ctypes.c_short),\n                        ('Bottom', ctypes.c_short)]\n\n        class ConsoleScreenBufferInfo(ctypes.Structure):\n            \"\"\"Windows CONSOLE_SCREEN_BUFFER_INFO structure.\n            http://msdn.microsoft.com/en-us/library/windows/desktop/ms682093\n            \"\"\"\n            _fields_ = [\n                ('dwSize', COORD),\n                ('dwCursorPosition', COORD),\n                ('wAttributes', ctypes.wintypes.WORD),\n                ('srWindow', SmallRECT),\n                ('dwMaximumWindowSize', COORD)\n            ]\n\n        _WindowsCSBI.CSBI = ConsoleScreenBufferInfo"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize():\n        _WindowsCSBI._define_csbi()\n        _WindowsCSBI.HANDLE_STDERR = _WindowsCSBI.HANDLE_STDERR or _WindowsCSBI.WINDLL.kernel32.GetStdHandle(-12)\n        _WindowsCSBI.HANDLE_STDOUT = _WindowsCSBI.HANDLE_STDOUT or _WindowsCSBI.WINDLL.kernel32.GetStdHandle(-11)\n        if _WindowsCSBI.WINDLL.kernel32.GetConsoleScreenBufferInfo.argtypes:\n            return\n\n        _WindowsCSBI.WINDLL.kernel32.GetStdHandle.argtypes = [ctypes.wintypes.DWORD]\n        _WindowsCSBI.WINDLL.kernel32.GetStdHandle.restype = ctypes.wintypes.HANDLE\n        _WindowsCSBI.WINDLL.kernel32.GetConsoleScreenBufferInfo.restype = ctypes.wintypes.BOOL\n        _WindowsCSBI.WINDLL.kernel32.GetConsoleScreenBufferInfo.argtypes = [\n            ctypes.wintypes.HANDLE, ctypes.POINTER(_WindowsCSBI.CSBI)\n        ]", "response": "Initializes the WINDLL resource and populated the CSBI class variable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_info(handle):\n        # Query Win32 API.\n        csbi = _WindowsCSBI.CSBI()\n        try:\n            if not _WindowsCSBI.WINDLL.kernel32.GetConsoleScreenBufferInfo(handle, ctypes.byref(csbi)):\n                raise IOError('Unable to get console screen buffer info from win32 API.')\n        except ctypes.ArgumentError:\n            raise IOError('Unable to get console screen buffer info from win32 API.')\n\n        # Parse data.\n        result = dict(\n            buffer_width=int(csbi.dwSize.X - 1),\n            buffer_height=int(csbi.dwSize.Y),\n            terminal_width=int(csbi.srWindow.Right - csbi.srWindow.Left),\n            terminal_height=int(csbi.srWindow.Bottom - csbi.srWindow.Top),\n            bg_color=int(csbi.wAttributes & 240),\n            fg_color=int(csbi.wAttributes % 16),\n        )\n        return result", "response": "Get information about this current console window."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stencil(**kwargs):\n    alnfile = kwargs.get('alnfile')\n    gtypefile = kwargs.get('gtypefile')\n    grpfile = kwargs.get('grpfile')\n    if grpfile is None:\n        grpfile2chk = os.path.join(DATA_DIR, 'ref.gene2transcripts.tsv')\n        if os.path.exists(grpfile2chk):\n            grpfile = grpfile2chk\n        else:\n            print >> sys.stderr, '[gbrs::stencil] A group file is *not* given. Genotype will be stenciled as is.'\n\n    # Load alignment incidence matrix ('alnfile' is assumed to be in multiway transcriptome)\n    alnmat = emase.AlignmentPropertyMatrix(h5file=alnfile, grpfile=grpfile)\n\n    # Load genotype calls\n    hid = dict(zip(alnmat.hname, np.arange(alnmat.num_haplotypes)))\n    gid = dict(zip(alnmat.gname, np.arange(len(alnmat.gname))))\n    gtmask = np.zeros((alnmat.num_haplotypes, alnmat.num_loci))\n    gtcall_g = dict.fromkeys(alnmat.gname)\n    with open(gtypefile) as fh:\n        if grpfile is not None:\n            gtcall_t = dict.fromkeys(alnmat.lname)\n            for curline in dropwhile(is_comment, fh):\n                item = curline.rstrip().split(\"\\t\")\n                g, gt = item[:2]\n                gtcall_g[g] = gt\n                hid2set = np.array([hid[c] for c in gt])\n                tid2set = np.array(alnmat.groups[gid[g]])\n                gtmask[np.meshgrid(hid2set, tid2set)] = 1.0\n                for t in tid2set:\n                    gtcall_t[alnmat.lname[t]] = gt\n        else:\n            for curline in dropwhile(is_comment, fh):\n                item = curline.rstrip().split(\"\\t\")\n                g, gt = item[:2]\n                gtcall_g[g] = gt\n                hid2set = np.array([hid[c] for c in gt])\n                gtmask[np.meshgrid(hid2set, gid[g])] = 1.0\n\n    alnmat.multiply(gtmask, axis=2)\n    for h in xrange(alnmat.num_haplotypes):\n        alnmat.data[h].eliminate_zeros()\n    outfile = kwargs.get('outfile')\n    if outfile is None:\n        outfile = 'gbrs.stenciled.' + os.path.basename(alnfile)\n    alnmat.save(h5file=outfile)", "response": "This function is used to stencil the genotyped version of alignment incidence file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef quantify(**kwargs):\n    alnfile = kwargs.get('alnfile')\n    grpfile = kwargs.get('grpfile')\n    if grpfile is None:\n        grpfile2chk = os.path.join(DATA_DIR, 'ref.gene2transcripts.tsv')\n        if os.path.exists(grpfile2chk):\n            grpfile = grpfile2chk\n        else:\n            print >> sys.stderr, '[gbrs::quantify] A group file is not given. Group-level results will not be reported.'\n\n    outbase = kwargs.get('outbase')\n    gtypefile = kwargs.get('gtypefile')\n    pseudocount = kwargs.get('pseudocount')\n    lenfile = kwargs.get('lenfile')\n    if lenfile is None:\n        lenfile2chk = os.path.join(DATA_DIR, 'gbrs.hybridized.targets.info')\n        if os.path.exists(lenfile2chk):\n            lenfile = lenfile2chk\n        else:\n            print >> sys.stderr, '[gbrs::quantify] A length file is not given. Transcript length adjustment will *not* be performed.'\n\n    read_length = kwargs.get('read_length')\n    multiread_model = kwargs.get('multiread_model')\n    tolerance = kwargs.get('tolerance')\n    max_iters = kwargs.get('max_iters')\n    report_group_counts = grpfile is not None  # If grpfile exist, always report groupwise results too\n    report_alignment_counts = kwargs.get('report_alignment_counts')\n    report_posterior = kwargs.get('report_posterior')\n\n    # Load alignment incidence matrix ('alnfile' is assumed to be in multiway transcriptome)\n    alnmat = emase.AlignmentPropertyMatrix(h5file=alnfile, grpfile=grpfile)\n\n    # Load genotype calls\n    if gtypefile is not None:  # Genotype calls are at the gene level\n        outbase = outbase + '.diploid'\n        hid = dict(zip(alnmat.hname, np.arange(alnmat.num_haplotypes)))\n        gid = dict(zip(alnmat.gname, np.arange(len(alnmat.gname))))\n        gtmask = np.zeros((alnmat.num_haplotypes, alnmat.num_loci))\n        gtcall_g = dict.fromkeys(alnmat.gname)\n        gtcall_t = dict.fromkeys(alnmat.lname)\n        with open(gtypefile) as fh:\n            for curline in dropwhile(is_comment, fh):\n                item = curline.rstrip().split(\"\\t\")\n                g, gt = item[:2]\n                gtcall_g[g] = gt\n                hid2set = np.array([hid[c] for c in gt])\n                tid2set = np.array(alnmat.groups[gid[g]])\n                gtmask[np.meshgrid(hid2set, tid2set)] = 1.0\n                for t in tid2set:\n                    gtcall_t[alnmat.lname[t]] = gt\n        alnmat.multiply(gtmask, axis=2)\n        for h in xrange(alnmat.num_haplotypes):\n            alnmat.data[h].eliminate_zeros()\n    else:\n        outbase = outbase + \".multiway\"\n        gtcall_g = None\n        gtcall_t = None\n\n    # Run emase\n    em_factory = emase.EMfactory(alnmat)\n    em_factory.prepare(pseudocount=pseudocount, lenfile=lenfile, read_length=read_length)\n    em_factory.run(model=multiread_model, tol=tolerance, max_iters=max_iters, verbose=True)\n    em_factory.report_depths(filename=\"%s.isoforms.tpm\" % outbase, tpm=True, notes=gtcall_t)\n    em_factory.report_read_counts(filename=\"%s.isoforms.expected_read_counts\" % outbase, notes=gtcall_t)\n    if report_posterior:\n        em_factory.export_posterior_probability(filename=\"%s.posterior.h5\" % outbase)\n    if report_group_counts:\n        em_factory.report_depths(filename=\"%s.genes.tpm\" % outbase, tpm=True, grp_wise=True, notes=gtcall_g)\n        em_factory.report_read_counts(filename=\"%s.genes.expected_read_counts\" % outbase, grp_wise=True, notes=gtcall_g)\n    if report_alignment_counts:\n        alnmat = emase.AlignmentPropertyMatrix(h5file=alnfile, grpfile=grpfile)\n        alnmat.report_alignment_counts(filename=\"%s.isoforms.alignment_counts\" % outbase)\n        if report_group_counts:\n            alnmat._bundle_inline(reset=True)\n            alnmat.report_alignment_counts(filename=\"%s.genes.alignment_counts\" % outbase)", "response": "Quantify expected read counts for a single sequence of sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_items(self, items):\n        for item in items:\n            item.set_parent(self)\n        self.items.extend(items)", "response": "Bulk ``register_item``.\n\n        Args:\n            items (iterable[Tree]):\n                Sequence of nodes to be registered as children."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef endpoints(self):\n        children = [item.endpoints() for item in self.items]\n        return self.name, self.endpoint, children", "response": "Get all the endpoints under this node in a tree like structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef absolute_name(self):\n        if self.is_root() or self.parent.is_root():\n            return utils.slugify(self.name)\n        return ':'.join([self.parent.absolute_name, utils.slugify(self.name)])", "response": "Get the absolute name of self."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the absolute url of the current instance.", "response": "def absolute_url(self):\n        \"\"\"Get the absolute url of ``self``.\n\n        Returns:\n            str: the absolute url.\n        \"\"\"\n        if self.is_root():\n            return utils.concat_urls(self.url)\n        return utils.concat_urls(self.parent.absolute_url, self.url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the fraction with additional whitespace.", "response": "def str_fraction(self):\n        \"\"\"Returns the fraction with additional whitespace.\"\"\"\n        if self.undefined:\n            return None\n        denominator = locale.format('%d', self.denominator, grouping=True)\n        numerator = self.str_numerator.rjust(len(denominator))\n        return '{0}/{1}'.format(numerator, denominator)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the fraction with additional whitespace.", "response": "def str_fraction(self):\n        \"\"\"Returns the fraction with additional whitespace.\"\"\"\n        if self._eta.undefined:\n            return None\n\n        # Determine denominator and its unit.\n        unit_denominator, unit = self._unit_class(self.denominator).auto\n        formatter = '%d' if unit_denominator == self.denominator else '%0.2f'\n        denominator = locale.format(formatter, unit_denominator, grouping=True)\n\n        # Determine numerator.\n        unit_numerator = getattr(self._unit_class(self.numerator), unit)\n        if self.done:\n            rounded_numerator = unit_numerator\n        else:\n            rounded_numerator = float(Decimal(str(unit_numerator)).quantize(Decimal('.01'), rounding=ROUND_DOWN))\n        numerator = locale.format(formatter, rounded_numerator, grouping=True).rjust(len(denominator))\n\n        return '{0}/{1} {2}'.format(numerator, denominator, unit)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the numerator with formatting.", "response": "def str_numerator(self):\n        \"\"\"Returns the numerator with formatting.\"\"\"\n        if not self.undefined:\n            return None\n        unit_numerator, unit = self._unit_class(self.numerator).auto\n        formatter = '%d' if unit_numerator == self.numerator else '%0.2f'\n        numerator = locale.format(formatter, unit_numerator, grouping=True)\n        return '{0} {1}'.format(numerator, unit)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a formatted ETA value for the progress bar.", "response": "def str_eta(self):\n        \"\"\"Returns a formatted ETA value for the progress bar.\"\"\"\n        eta = eta_letters(self._eta.elapsed) if self.done else self._eta_string\n        if not eta:\n            return ''\n        if eta.count(' ') > 1:\n            eta = ' '.join(eta.split(' ')[:2])  # Only show up to two units (h and m, no s for example).\n        return (' in {0}' if self.done else 'eta {0}').format(eta)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef str_rate(self):\n        # Handle special cases.\n        if not self._eta.started or self._eta.stalled or not self.rate:\n            return '--.-KiB/s'\n\n        unit_rate, unit = UnitByte(self._eta.rate_overall if self.done else self.rate).auto\n        if unit_rate >= 100:\n            formatter = '%d'\n        elif unit_rate >= 10:\n            formatter = '%.1f'\n        else:\n            formatter = '%.2f'\n        return '{0}{1}/s'.format(locale.format(formatter, unit_rate, grouping=False), unit)", "response": "Returns the rate with formatting. If done returns the overall rate instead."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a formatted ETA value for the progress bar.", "response": "def str_eta(self):\n        \"\"\"Returns a formatted ETA value for the progress bar.\"\"\"\n        if self.done:\n            return eta_hms(self._eta.elapsed, always_show_hours=True, hours_leading_zero=True)\n        if not self._eta_string:\n            return ''\n        return '{0} ETA'.format(self._eta_string)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the numerator with formatting.", "response": "def str_numerator(self):\n        \"\"\"Returns the numerator with formatting.\"\"\"\n        unit_numerator, unit = UnitByte(self.numerator).auto_no_thousands\n        if unit_numerator >= 10:\n            formatter = '%d'\n        else:\n            formatter = '%0.1f'\n        return '{0} {1}'.format(locale.format(formatter, unit_numerator, grouping=False), unit)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef str_rate(self):\n        # Handle special cases.\n        if not self._eta.started or self._eta.stalled or not self.rate:\n            return '--- KiB/s'\n\n        unit_rate, unit = UnitByte(self.rate).auto_no_thousands\n        if unit_rate >= 10:\n            formatter = '%d'\n        else:\n            formatter = '%0.1f'\n        return '{0} {1}/s'.format(locale.format(formatter, unit_rate, grouping=False), unit)", "response": "Returns the rate with formatting."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize database with gsshapy tables", "response": "def init_db(sqlalchemy_url):\n    \"\"\"\n    Initialize database with gsshapy tables\n    \"\"\"\n    engine = create_engine(sqlalchemy_url)\n    start = time.time()\n    metadata.create_all(engine)\n    return time.time() - start"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_sqlite_memory(initTime=False):\n    sqlalchemy_url = 'sqlite://'\n    engine = create_engine(sqlalchemy_url,\n                           poolclass=SingletonThreadPool)\n    start = time.time()\n    metadata.create_all(engine)\n    \n    if initTime:\n        print('TIME: {0} seconds'.format(time.time() - start))\n        \n    return sqlalchemy_url, engine", "response": "Initialize SQLite in Memory Only Database\n    \n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes SQLite Database Args: path(str): Path to database (Ex. '/home/username/my_sqlite.db'). initTime(Optional[bool]): If True, it will print the amount of time to generate database. Example:: from gsshapy.lib.db_tools import init_sqlite_db, create_session sqlite_db_path = '/home/username/my_sqlite.db' init_postgresql_db(path=sqlite_db_path) sqlalchemy_url = init_sqlite_db(path=sqlite_db_path) db_work_sessionmaker = get_sessionmaker(sqlalchemy_url) db_work_session = db_work_sessionmaker() ##DO WORK db_work_session.close()", "response": "def init_sqlite_db(path, initTime=False):\n    \"\"\"\n    Initialize SQLite Database\n    \n    Args:\n        path(str): Path to database (Ex. '/home/username/my_sqlite.db').\n        initTime(Optional[bool]): If True, it will print the amount of time to generate database.\n\n    Example::\n    \n        from gsshapy.lib.db_tools import init_sqlite_db, create_session\n        \n        sqlite_db_path = '/home/username/my_sqlite.db'   \n        \n        init_postgresql_db(path=sqlite_db_path)\n        \n        sqlalchemy_url = init_sqlite_db(path=sqlite_db_path)\n        \n        db_work_sessionmaker = get_sessionmaker(sqlalchemy_url)\n\n        db_work_session = db_work_sessionmaker()\n        \n        ##DO WORK\n        \n        db_work_session.close()\n    \"\"\"\n    sqlite_base_url = 'sqlite:///'\n    \n    sqlalchemy_url = sqlite_base_url + path\n\n    init_time = init_db(sqlalchemy_url)\n    \n    if initTime:\n        print('TIME: {0} seconds'.format(init_time))\n        \n    return sqlalchemy_url"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_postgresql_db(username, host, database, port='', password='', initTime=False):\n    postgresql_base_url = 'postgresql://'\n    \n    if password != '':\n        password = ':%s' % password\n        \n    if port != '':\n        port = ':%s' % port\n        \n    sqlalchemy_url = '%s%s%s@%s%s/%s' % (\n                      postgresql_base_url,\n                      username,\n                      password,\n                      host,\n                      port,\n                      database\n                      )\n    \n    init_time = init_db(sqlalchemy_url)\n    \n    if initTime:\n        print('TIME: {0} seconds'.format(init_time))\n    \n    return sqlalchemy_url", "response": "Initialize PostgreSQL Database\n    \n    .. note:: psycopg2 or similar driver required\n    \n    Args:\n        username(str): Database username.\n        host(str): Database host URL.\n        database(str): Database name.\n        port(Optional[int,str]): Database port.\n        password(Optional[str]): Database password.\n        initTime(Optional[bool]): If True, it will print the amount of time to generate database.\n\n    Example::\n    \n        from gsshapy.lib.db_tools import init_postgresql_db, create_session\n        \n        sqlalchemy_url = init_postgresql_db(username='gsshapy', \n                                            host='localhost', \n                                            database='gsshapy_mysql_tutorial', \n                                            port='5432', \n                                            password='pass')\n\n        db_work_sessionmaker = get_sessionmaker(sqlalchemy_url)\n\n        db_work_session = db_work_sessionmaker()\n        \n        ##DO WORK\n        \n        db_work_session.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_mysql_db(username, host, database, port='', password='', initTime=False):\n    \n    mysql_base_url = 'mysql://'\n    \n    if password != '':\n        password = ':%s' % password\n        \n    if port != '':\n        port = ':%s' % port\n        \n    sqlalchemy_url = '%s%s%s@%s%s/%s' % (\n                      mysql_base_url,\n                      username,\n                      password,\n                      host,\n                      port,\n                      database\n                      )\n    \n    init_time = init_db(sqlalchemy_url)\n    \n    if initTime:\n        print('TIME: {0} seconds'.format(init_time))\n    \n    return sqlalchemy_url", "response": "Initialize MySQL Database\n    \n    .. note:: mysql-python or similar driver required\n    \n    Args:\n        username(str): Database username.\n        host(str): Database host URL.\n        database(str): Database name.\n        port(Optional[int,str]): Database port.\n        password(Optional[str]): Database password.\n        initTime(Optional[bool]): If True, it will print the amount of time to generate database.\n\n    Example::\n    \n        from gsshapy.lib.db_tools import init_mysql_db, create_session\n        \n        sqlalchemy_url = init_mysql_db(username='gsshapy', \n                                       host='localhost', \n                                       database='gsshapy_mysql_tutorial', \n                                       port='5432', \n                                       password='pass')\n                                       \n        db_work_sessionmaker = get_sessionmaker(sqlalchemy_url)\n\n        db_work_session = db_work_sessionmaker()\n        ##DO WORK\n        \n        db_work_session.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sessionmaker(sqlalchemy_url, engine=None):\n    if engine is None:\n        engine = create_engine(sqlalchemy_url)\n    return sessionmaker(bind=engine)", "response": "Create session with database to work in\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_project_session(project_name, project_directory, map_type=None):\n    sqlalchemy_url, sql_engine = init_sqlite_memory()\n    gdb_sessionmaker = get_sessionmaker(sqlalchemy_url, sql_engine)\n    project_manager = ProjectFile(name=project_name,\n                                  project_directory=project_directory,\n                                  map_type=map_type)\n    return project_manager, gdb_sessionmaker", "response": "Load project manager and in memory sqlite db sessionmaker for GSSHA project"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_settings(config_uri, section=None, defaults=None):\n    loader = get_loader(config_uri)\n    return loader.get_settings(section, defaults)", "response": "Load the settings from a named section."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_loader(config_uri, protocols=None):\n    config_uri = parse_uri(config_uri)\n    requested_scheme = config_uri.scheme\n\n    matched_loaders = find_loaders(requested_scheme, protocols=protocols)\n\n    if len(matched_loaders) < 1:\n        raise LoaderNotFound(requested_scheme, protocols=protocols)\n\n    if len(matched_loaders) > 1:\n        raise MultipleLoadersFound(\n            requested_scheme, matched_loaders, protocols=protocols)\n\n    loader_info = matched_loaders[0]\n    loader = loader_info.load(config_uri)\n    return loader", "response": "Find a loader object capable of handling config_uri."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding all loaders that match the requested scheme and protocols.", "response": "def find_loaders(scheme, protocols=None):\n    \"\"\"\n    Find all loaders that match the requested scheme and protocols.\n\n    :param scheme: Any valid scheme. Examples would be something like ``ini``\n        or ``ini+pastedeploy``.\n\n    :param protocols: Zero or more :term:`loader protocol` identifiers that\n        the loader must implement. If ``None`` then only generic loaders will\n        be returned.\n\n    :returns: A list containing zero or more :class:`plaster.ILoaderInfo`\n        objects.\n\n    \"\"\"\n    # build a list of all required entry points\n    matching_groups = ['plaster.loader_factory']\n    if protocols:\n        matching_groups += [\n            'plaster.{0}_loader_factory'.format(proto)\n            for proto in protocols\n        ]\n    scheme = scheme.lower()\n\n    # if a distribution is specified then it overrides the default search\n    parts = scheme.split('+', 1)\n    if len(parts) == 2:\n        try:\n            distro = pkg_resources.get_distribution(parts[0])\n        except pkg_resources.DistributionNotFound:\n            pass\n        else:\n            ep = _find_ep_in_dist(distro, parts[1], matching_groups)\n\n            # if we got one or more loaders from a specific distribution\n            # then they override everything else so we'll just return them\n            if ep:\n                return [EntryPointLoaderInfo(ep, protocols)]\n\n    # find any distributions supporting the default loader protocol\n    possible_entry_points = [\n        ep\n        for ep in pkg_resources.iter_entry_points('plaster.loader_factory')\n        if scheme is None or scheme == ep.name.lower()\n    ]\n    distros = {ep.dist for ep in possible_entry_points}\n    matched_entry_points = list(filter(None, [\n        _find_ep_in_dist(distro, scheme, matching_groups)\n        for distro in distros\n    ]))\n    return [\n        EntryPointLoaderInfo(ep, protocols=protocols)\n        for ep in matched_entry_points\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noverwrite Actor. receive adds the checks and the functionalities requiered by parallel methods.", "response": "def receive(self, msg):\n        '''\n        Overwriting :meth:`Actor.receive`, adds the checks and\n        functionalities requiered by parallel methods.\n\n        :param msg: The message is a dictionary using the constants\n            defined in util.py (:mod:`pyactor.util`).\n        '''\n        if msg[TYPE] == TELL and msg[METHOD] == 'stop':\n            self.running = False\n        else:\n            result = None\n            try:\n                invoke = getattr(self._obj, msg[METHOD])\n                params = msg[PARAMS]\n\n                if msg[METHOD] in self.ask_parallel:\n                    rpc_id = str(uuid.uuid4())\n                    # add rpc message to pendent AskResponse s\n                    self.pending[rpc_id] = msg\n                    # insert an rpc id to args\n                    para = list(params[0])\n                    para.insert(0, rpc_id)\n                    invoke(*para, **params[1])\n                    return\n                else:\n                    with self.__lock:\n                        sleep(0.01)\n                        result = invoke(*params[0], **params[1])\n            except Exception, e:\n                result = e\n                print result\n\n            self.send_response(result, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncombine multiple dicts in one.", "response": "def combine_dicts(*dicts, copy=False, base=None):\n    \"\"\"\n    Combines multiple dicts in one.\n\n    :param dicts:\n        A sequence of dicts.\n    :type dicts: dict\n\n    :param copy:\n        If True, it returns a deepcopy of input values.\n    :type copy: bool, optional\n\n    :param base:\n        Base dict where combine multiple dicts in one.\n    :type base: dict, optional\n\n    :return:\n        A unique dict.\n    :rtype: dict\n\n    Example::\n\n        >>> sorted(combine_dicts({'a': 3, 'c': 3}, {'a': 1, 'b': 2}).items())\n        [('a', 1), ('b', 2), ('c', 3)]\n    \"\"\"\n\n    if len(dicts) == 1 and base is None:  # Only one input dict.\n        cd = dicts[0].copy()\n    else:\n        cd = {} if base is None else base  # Initialize empty dict.\n\n        for d in dicts:  # Combine dicts.\n            if d:\n                # noinspection PyTypeChecker\n                cd.update(d)\n\n    # Return combined dict.\n    return {k: _copy.deepcopy(v) for k, v in cd.items()} if copy else cd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary that contains all the keys and values identical to keys.", "response": "def kk_dict(*kk, **adict):\n    \"\"\"\n    Merges and defines dictionaries with values identical to keys.\n\n    :param kk:\n        A sequence of keys and/or dictionaries.\n    :type kk: object | dict, optional\n\n    :param adict:\n        A dictionary.\n    :type adict: dict, optional\n\n    :return:\n        Merged dictionary.\n    :rtype: dict\n\n    Example::\n\n        >>> sorted(kk_dict('a', 'b', 'c').items())\n        [('a', 'a'), ('b', 'b'), ('c', 'c')]\n        \n        >>> sorted(kk_dict('a', 'b', **{'a-c': 'c'}).items())\n        [('a', 'a'), ('a-c', 'c'), ('b', 'b')]\n        \n        >>> sorted(kk_dict('a', {'b': 'c'}, 'c').items())\n        [('a', 'a'), ('b', 'c'), ('c', 'c')]\n        \n        >>> sorted(kk_dict('a', 'b', **{'b': 'c'}).items())\n        Traceback (most recent call last):\n         ...\n        ValueError: keyword argument repeated\n        >>> sorted(kk_dict('a', {'b': 'd'}, **{'b': 'c'}).items())\n        Traceback (most recent call last):\n         ...\n        ValueError: keyword argument repeated\n    \"\"\"\n\n    for k in kk:\n        if isinstance(k, dict):\n            if not set(k).isdisjoint(adict):\n                raise ValueError('keyword argument repeated')\n            adict.update(k)\n        elif k in adict:\n            raise ValueError('keyword argument repeated')\n        else:\n            adict[k] = k\n\n    return adict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bypass(*inputs, copy=False):\n\n    if len(inputs) == 1:\n        inputs = inputs[0]  # Same inputs.\n\n    return _copy.deepcopy(inputs) if copy else inputs", "response": "Returns the same arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict that maps the keys of the given dicts.", "response": "def map_dict(key_map, *dicts, copy=False, base=None):\n    \"\"\"\n    Returns a dict with new key values.\n\n    :param key_map:\n        A dictionary that maps the dict keys ({old key: new key}\n    :type key_map: dict\n\n    :param dicts:\n        A sequence of dicts.\n    :type dicts: dict\n\n    :param copy:\n        If True, it returns a deepcopy of input values.\n    :type copy: bool, optional\n\n    :param base:\n        Base dict where combine multiple dicts in one.\n    :type base: dict, optional\n\n    :return:\n        A unique dict with new key values.\n    :rtype: dict\n\n    Example::\n\n        >>> d = map_dict({'a': 'c', 'b': 'd'}, {'a': 1, 'b': 1}, {'b': 2})\n        >>> sorted(d.items())\n        [('c', 1), ('d', 2)]\n    \"\"\"\n\n    it = combine_dicts(*dicts).items()  # Combine dicts.\n\n    get = key_map.get  # Namespace shortcut.\n\n    # Return mapped dict.\n    return combine_dicts({get(k, k): v for k, v in it}, copy=copy, base=base)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_list(key_map, *inputs, copy=False, base=None):\n\n    d = {} if base is None else base  # Initialize empty dict.\n\n    for m, v in zip(key_map, inputs):\n        if isinstance(m, dict):\n            map_dict(m, v, base=d)  # Apply a map dict.\n        elif isinstance(m, list):\n            map_list(m, *v, base=d)  # Apply a map list.\n        else:\n            d[m] = v  # Apply map.\n\n    return combine_dicts(copy=copy, base=d)", "response": "Returns a new dict with the keys that map the input list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting the chosen dictionary keys from the given dictionary.", "response": "def selector(keys, dictionary, copy=False, output_type='dict',\n             allow_miss=False):\n    \"\"\"\n    Selects the chosen dictionary keys from the given dictionary.\n\n    :param keys:\n        Keys to select.\n    :type keys: list, tuple, set\n\n    :param dictionary:\n        A dictionary.\n    :type dictionary: dict\n\n    :param copy:\n        If True the output contains deep-copies of the values.\n    :type copy: bool\n\n    :param output_type:\n        Type of function output:\n\n            + 'list': a list with all values listed in `keys`.\n            + 'dict': a dictionary with any outputs listed in `keys`.\n            + 'values': if output length == 1 return a single value otherwise a\n                        tuple with all values listed in `keys`.\n\n        :type output_type: str, optional\n\n    :param allow_miss:\n        If True it does not raise when some key is missing in the dictionary.\n    :type allow_miss: bool\n\n    :return:\n        A dictionary with chosen dictionary keys if present in the sequence of\n        dictionaries. These are combined with :func:`combine_dicts`.\n    :rtype: dict\n\n    Example::\n\n        >>> from functools import partial\n        >>> fun = partial(selector, ['a', 'b'])\n        >>> sorted(fun({'a': 1, 'b': 2, 'c': 3}).items())\n        [('a', 1), ('b', 2)]\n    \"\"\"\n\n    if not allow_miss:\n        # noinspection PyUnusedLocal\n        def check(key):\n            return True\n    else:\n        def check(key):\n            return key in dictionary\n\n    if output_type == 'list':  # Select as list.\n        res = [dictionary[k] for k in keys if check(k)]\n        return _copy.deepcopy(res) if copy else res\n    elif output_type == 'values':\n        return bypass(*[dictionary[k] for k in keys if check(k)], copy=copy)\n\n    # Select as dict.\n    return bypass({k: dictionary[k] for k in keys if check(k)}, copy=copy)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplicate the input value n times the input value.", "response": "def replicate_value(value, n=2, copy=True):\n    \"\"\"\n    Replicates `n` times the input value.\n\n    :param n:\n        Number of replications.\n    :type n: int\n\n    :param value:\n        Value to be replicated.\n    :type value: T\n\n    :param copy:\n        If True the list contains deep-copies of the value.\n    :type copy: bool\n\n    :return:\n        A list with the value replicated `n` times.\n    :rtype: list\n\n    Example::\n\n        >>> from functools import partial\n        >>> fun = partial(replicate_value, n=5)\n        >>> fun({'a': 3})\n        ({'a': 3}, {'a': 3}, {'a': 3}, {'a': 3}, {'a': 3})\n    \"\"\"\n\n    return bypass(*[value] * n, copy=copy)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parent_func(func, input_id=None):\n    if isinstance(func, functools.partial):\n        if input_id is not None:\n            # noinspection PyTypeChecker\n            input_id += len(func.args)\n        return parent_func(func.func, input_id=input_id)\n\n    elif isinstance(func, add_args):\n        if input_id is not None:\n            input_id -= func.n\n        return parent_func(func.func, input_id=input_id)\n\n    if input_id is None:\n        return func\n    else:\n        return func, input_id", "response": "Return the parent function of a wrapped function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stack_nested_keys(nested_dict, key=(), depth=-1):\n\n    if depth != 0 and hasattr(nested_dict, 'items'):\n        for k, v in nested_dict.items():\n            yield from stack_nested_keys(v, key=key + (k,), depth=depth - 1)\n    else:\n        yield key, nested_dict", "response": "Stacks the keys of nested - dictionaries into tuples and yields a list of k - v pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nested_dicts(nested_dict, *keys, default=None, init_nesting=dict):\n\n    if keys:\n        default = default or init_nesting\n        if keys[0] in nested_dict:\n            nd = nested_dict[keys[0]]\n        else:\n            d = default() if len(keys) == 1 else init_nesting()\n            nd = nested_dict[keys[0]] = d\n        return get_nested_dicts(nd, *keys[1:], default=default,\n                                init_nesting=init_nesting)\n    return nested_dict", "response": "Get the value of nested - dicts."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if nested keys are inside of nested - dictionaries otherwise False.", "response": "def are_in_nested_dicts(nested_dict, *keys):\n    \"\"\"\n    Nested keys are inside of nested-dictionaries.\n\n    :param nested_dict:\n        Nested dictionary.\n    :type nested_dict: dict\n\n    :param keys:\n        Nested keys.\n    :type keys: object\n\n    :return:\n        True if nested keys are inside of nested-dictionaries, otherwise False.\n    :rtype: bool\n    \"\"\"\n\n    if keys:\n        # noinspection PyBroadException\n        try:\n            return are_in_nested_dicts(nested_dict[keys[0]], *keys[1:])\n        except Exception:  # Key error or not a dict.\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine nested - dictionaries.", "response": "def combine_nested_dicts(*nested_dicts, depth=-1, base=None):\n    \"\"\"\n    Merge nested-dictionaries.\n\n    :param nested_dicts:\n        Nested dictionaries.\n    :type nested_dicts: dict\n\n    :param depth:\n        Maximum keys depth.\n    :type depth: int, optional\n\n    :param base:\n        Base dict where combine multiple dicts in one.\n    :type base: dict, optional\n\n    :return:\n        Combined nested-dictionary.\n    :rtype: dict\n    \"\"\"\n\n    if base is None:\n        base = {}\n\n    for nested_dict in nested_dicts:\n        for k, v in stack_nested_keys(nested_dict, depth=depth):\n            while k:\n                # noinspection PyBroadException\n                try:\n                    get_nested_dicts(base, *k[:-1])[k[-1]] = v\n                    break\n                except Exception:\n                    # A branch of the nested_dict is longer than the base.\n                    k = k[:-1]\n                    v = get_nested_dicts(nested_dict, *k)\n\n    return base"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_function(dsp, inputs_kwargs=False, inputs_defaults=False, **kw):\n\n    def decorator(f):\n        dsp.add_func(\n            f, inputs_kwargs=inputs_kwargs, inputs_defaults=inputs_defaults,\n            **kw\n        )\n        return f\n\n    return decorator", "response": "Decorator to add a function to a dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Blueprint instance for the current object.", "response": "def blue(self, memo=None):\n        \"\"\"\n        Constructs a Blueprint out of the current object.\n\n        :param memo:\n            A dictionary to cache Blueprints.\n        :type memo: dict[T,schedula.utils.blue.Blueprint]\n\n        :return:\n            A Blueprint of the current object.\n        :rtype: schedula.utils.blue.Blueprint\n        \"\"\"\n        memo = {} if memo is None else memo\n        if self not in memo:\n            import inspect\n            from .blue import Blueprint, _parent_blue\n            keys = tuple(inspect.signature(self.__init__).parameters)\n            memo[self] = Blueprint(**{\n                k: _parent_blue(v, memo)\n                for k, v in self.__dict__.items() if k in keys\n            })._set_cls(self.__class__)\n        return memo[self]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef value_from_datadict(self, data, files, name):\n        value = super(FileSizeWidget, self).value_from_datadict(data, files, name)\n        if value not in EMPTY_VALUES:\n            try:\n                return parse_size(value)\n            except ValueError:\n                pass\n        return value", "response": "Returns the value of this widget from a dictionary of data and this widget s name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef control_force(self,c):\n        '''represents physical limitation of the control'''\n        \n        # bring instance variables into local scope\n        g = self.g\n        s = self.s\n        \n        return g * (2/\u03c0) * arctan((s/g) * c)", "response": "represents physical limitation of the control"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncontrols force plus the external forces", "response": "def force(self,c):\n        '''control force plus the external forces (drag, etc)'''\n        \n        # bring instance variables into local scope\n        x = self.x\n        v = self.v\n        b = self.b\n        k = self.k\n        x0 = self.x0\n        \n        F = self.control_force(c)\n        \n        return F - b*v - k*(x - x0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transform(self):\n        '''P\u20d7,Q\u20d7,W\u20d7'''\n        i = self.inclination\n        \u03a9 = self.longitude_of_ascending_node\n        \u03c9 = self.argument_of_periapsis\n\n        si = sin(i)\n        ci = cos(i)\n        s\u03a9 = sin(\u03a9)\n        c\u03a9 = cos(\u03a9)\n        s\u03c9 = sin(\u03c9)\n        c\u03c9 = cos(\u03c9)\n\n        Q = np.array([\n             [-s\u03a9*ci*s\u03c9 + c\u03a9*c\u03c9,\n               c\u03a9*ci*s\u03c9 + s\u03a9*c\u03c9,\n               si*s\u03c9],\n             [-s\u03a9*ci*c\u03c9 - c\u03a9*s\u03c9,\n               c\u03a9*ci*c\u03c9 - s\u03a9*s\u03c9,\n               si*c\u03c9],\n             [ s\u03a9*si,\n              -c\u03a9*si,\n               ci]])\n\n        return Q", "response": "Transform the internal state of the object into a vector of P Q and W."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_ssh_with_cb(ssh_cb, user, host, auth_cb, allow_new=True, \n                        verbosity=0):\n    \"\"\"A \"managed\" SSH session. When the session is ready, we'll invoke the \n    \"ssh_cb\" callback.\n    \"\"\"\n\n    with connect_ssh(user, host, auth_cb, allow_new=True, verbosity=0) as ssh:\n        ssh_cb(ssh)", "response": "Connect to a managed SSH session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect_sftp_with_cb(sftp_cb, *args, **kwargs):\n\n    with _connect_sftp(*args, **kwargs) as (ssh, sftp):\n        sftp_cb(ssh, sftp)", "response": "A managed SFTP session."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a function that adds an edge to the graph checking only the out node.", "response": "def add_edge_fun(graph):\n    \"\"\"\n    Returns a function that adds an edge to the `graph` checking only the out\n    node.\n\n    :param graph:\n        A directed graph.\n    :type graph: networkx.classes.digraph.DiGraph\n\n    :return:\n        A function that adds an edge to the `graph`.\n    :rtype: callable\n    \"\"\"\n\n    # Namespace shortcut for speed.\n    succ, pred, node = graph._succ, graph._pred, graph._node\n\n    def add_edge(u, v, **attr):\n        if v not in succ:  # Add nodes.\n            succ[v], pred[v], node[v] = {}, {}, {}\n\n        succ[u][v] = pred[v][u] = attr  # Add the edge.\n\n    return add_edge"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_edge_fun(graph):\n\n    # Namespace shortcut for speed.\n    rm_edge, rm_node = graph.remove_edge, graph.remove_node\n    from networkx import is_isolate\n\n    def remove_edge(u, v):\n        rm_edge(u, v)  # Remove the edge.\n        if is_isolate(graph, v):  # Check if v is isolate.\n            rm_node(v)  # Remove the isolate out node.\n\n    return remove_edge", "response": "Returns a function that removes an edge from the node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an unused node id in a directed graph.", "response": "def get_unused_node_id(graph, initial_guess='unknown', _format='{}<%d>'):\n    \"\"\"\n    Finds an unused node id in `graph`.\n\n    :param graph:\n        A directed graph.\n    :type graph: networkx.classes.digraph.DiGraph\n\n    :param initial_guess:\n        Initial node id guess.\n    :type initial_guess: str, optional\n\n    :param _format:\n        Format to generate the new node id if the given is already used.\n    :type _format: str, optional\n\n    :return:\n        An unused node id.\n    :rtype: str\n    \"\"\"\n\n    has_node = graph.has_node  # Namespace shortcut for speed.\n\n    n = counter()  # Counter.\n    node_id_format = _format.format(initial_guess)  # Node id format.\n\n    node_id = initial_guess  # Initial guess.\n    while has_node(node_id):  # Check if node id is used.\n        node_id = node_id_format % n()  # Guess.\n\n    return node_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd function node edges to the graph.", "response": "def add_func_edges(dsp, fun_id, nodes_bunch, edge_weights=None, input=True,\n                   data_nodes=None):\n    \"\"\"\n    Adds function node edges.\n\n    :param dsp:\n        A dispatcher that identifies the model adopted.\n    :type dsp: schedula.Dispatcher\n\n    :param fun_id:\n        Function node id.\n    :type fun_id: str\n\n    :param nodes_bunch:\n        A container of nodes which will be iterated through once.\n    :type nodes_bunch: iterable\n\n    :param edge_weights:\n        Edge weights.\n    :type edge_weights: dict, optional\n\n    :param input:\n        If True the nodes_bunch are input nodes, otherwise are output nodes.\n    :type input: bool, optional\n\n    :param data_nodes:\n        Data nodes to be deleted if something fail.\n    :type data_nodes: list\n\n    :return:\n        List of new data nodes.\n    :rtype: list\n    \"\"\"\n\n    # Namespace shortcut for speed.\n    add_edge = _add_edge_dmap_fun(dsp.dmap, edge_weights)\n    node, add_data = dsp.dmap.nodes, dsp.add_data\n    remove_nodes = dsp.dmap.remove_nodes_from\n\n    # Define an error message.\n    msg = 'Invalid %sput id: {} is not a data node' % ['out', 'in'][input]\n    i, j = ('i', 'o') if input else ('o', 'i')\n\n    data_nodes = data_nodes or []  # Update data nodes.\n\n    for u in nodes_bunch:  # Iterate nodes.\n        try:\n            if node[u]['type'] != 'data':  # The node is not a data node.\n                data_nodes.append(fun_id)  # Add function id to be removed.\n\n                remove_nodes(data_nodes)  # Remove function and new data nodes.\n\n                raise ValueError(msg.format(u))  # Raise error.\n        except KeyError:\n            data_nodes.append(add_data(data_id=u))  # Add new data node.\n\n        add_edge(**{i: u, j: fun_id, 'w': u})  # Add edge.\n\n    return data_nodes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd edge to the dispatcher map. :param graph: A directed graph. :type graph: networkx.classes.digraph.DiGraph :param edges_weights: Edge weights. :type edges_weights: dict, optional :return: A function that adds an edge to the `graph`. :rtype: callable", "response": "def _add_edge_dmap_fun(graph, edges_weights=None):\n    \"\"\"\n    Adds edge to the dispatcher map.\n\n    :param graph:\n        A directed graph.\n    :type graph: networkx.classes.digraph.DiGraph\n\n    :param edges_weights:\n        Edge weights.\n    :type edges_weights: dict, optional\n\n    :return:\n        A function that adds an edge to the `graph`.\n    :rtype: callable\n    \"\"\"\n\n    add = graph.add_edge  # Namespace shortcut for speed.\n\n    if edges_weights is not None:\n        def add_edge(i, o, w):\n            if w in edges_weights:\n                add(i, o, weight=edges_weights[w])  # Weighted edge.\n            else:\n                add(i, o)  # Normal edge.\n    else:\n        # noinspection PyUnusedLocal\n        def add_edge(i, o, w):\n            add(i, o)  # Normal edge.\n\n    return add_edge"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dispatcher node that matches the given node id.", "response": "def _get_node(nodes, node_id, fuzzy=True):\n    \"\"\"\n    Returns a dispatcher node that match the given node id.\n\n    :param nodes:\n        Dispatcher nodes.\n    :type nodes: dict\n\n    :param node_id:\n        Node id.\n    :type node_id: str\n\n    :return:\n         The dispatcher node and its id.\n    :rtype: (str, dict)\n    \"\"\"\n\n    try:\n        return node_id, nodes[node_id]  # Return dispatcher node and its id.\n    except KeyError as ex:\n        if fuzzy:\n            it = sorted(nodes.items())\n            n = next(((k, v) for k, v in it if node_id in k), EMPTY)\n            if n is not EMPTY:\n                return n\n        raise ex"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sub_node(dsp, path, node_attr='auto', solution=NONE, _level=0,\n                 _dsp_name=NONE):\n    \"\"\"\n    Returns a sub node of a dispatcher.\n\n    :param dsp:\n         A dispatcher object or a sub dispatch function.\n    :type dsp: schedula.Dispatcher | SubDispatch\n\n    :param path:\n        A sequence of node ids or a single node id. Each id identifies a\n        sub-level node.\n    :type path: tuple, str\n\n    :param node_attr:\n        Output node attr.\n\n        If the searched node does not have this attribute, all its attributes\n        are returned.\n\n        When 'auto', returns the \"default\" attributes of the searched node,\n        which are:\n\n          - for data node: its output, and if not exists, all its attributes.\n          - for function and sub-dispatcher nodes: the 'function' attribute.\n    :type node_attr: str | None\n\n    :param solution:\n        Parent Solution.\n    :type solution: schedula.utils.Solution\n\n    :param _level:\n        Path level.\n    :type _level: int\n\n    :param _dsp_name:\n        dsp name to show when the function raise a value error.\n    :type _dsp_name: str\n\n    :return:\n        A sub node of a dispatcher and its path.\n    :rtype: dict | object, tuple[str]\n\n    **Example**:\n\n    .. dispatcher:: o\n       :opt: graph_attr={'ratio': '1'}, depth=-1\n       :code:\n\n        >>> from schedula import Dispatcher\n        >>> s_dsp = Dispatcher(name='Sub-dispatcher')\n        >>> def fun(a, b):\n        ...     return a + b\n        ...\n        >>> s_dsp.add_function('a + b', fun, ['a', 'b'], ['c'])\n        'a + b'\n        >>> dispatch = SubDispatch(s_dsp, ['c'], output_type='dict')\n        >>> dsp = Dispatcher(name='Dispatcher')\n        >>> dsp.add_function('Sub-dispatcher', dispatch, ['a'], ['b'])\n        'Sub-dispatcher'\n\n        >>> o = dsp.dispatch(inputs={'a': {'a': 3, 'b': 1}})\n        ...\n\n    Get the sub node 'c' output or type::\n\n        >>> get_sub_node(dsp, ('Sub-dispatcher', 'c'))\n        (4, ('Sub-dispatcher', 'c'))\n        >>> get_sub_node(dsp, ('Sub-dispatcher', 'c'), node_attr='type')\n        ('data', ('Sub-dispatcher', 'c'))\n\n    Get the sub-dispatcher output:\n\n    .. dispatcher:: sol\n       :opt: graph_attr={'ratio': '1'}, depth=-1\n       :code:\n\n        >>> sol, p = get_sub_node(dsp, ('Sub-dispatcher',), node_attr='output')\n        >>> sol, p\n        (Solution([('a', 3), ('b', 1), ('c', 4)]), ('Sub-dispatcher',))\n    \"\"\"\n\n    path = list(path)\n\n    if isinstance(dsp, SubDispatch):  # Take the dispatcher obj.\n        dsp = dsp.dsp\n\n    if _dsp_name is NONE:  # Set origin dispatcher name for warning purpose.\n        _dsp_name = dsp.name\n\n    if solution is NONE:  # Set origin dispatcher name for warning purpose.\n        solution = dsp.solution\n\n    node_id = path[_level]  # Node id at given level.\n\n    try:\n        node_id, node = _get_node(dsp.nodes, node_id)  # Get dispatcher node.\n        path[_level] = node_id\n    except KeyError:\n        if _level == len(path) - 1 and node_attr in ('auto', 'output') \\\n                and solution is not EMPTY:\n            try:\n                # Get dispatcher node.\n                node_id, node = _get_node(solution, node_id, False)\n                path[_level] = node_id\n                return node, tuple(path)\n            except KeyError:\n                pass\n        msg = 'Path %s does not exist in %s dispatcher.' % (path, _dsp_name)\n        raise ValueError(msg)\n\n    _level += 1  # Next level.\n\n    if _level < len(path):  # Is not path leaf?.\n\n        try:\n            if node['type'] in ('function', 'dispatcher'):\n                try:\n                    solution = solution.workflow.node[node_id]['solution']\n                except (KeyError, AttributeError):\n                    solution = EMPTY\n                dsp = parent_func(node['function'])  # Get parent function.\n            else:\n                raise KeyError\n\n        except KeyError:\n            msg = 'Node of path %s at level %i is not a function or ' \\\n                  'sub-dispatcher node of %s ' \\\n                  'dispatcher.' % (path, _level, _dsp_name)\n            raise ValueError(msg)\n\n        # Continue the node search.\n        return get_sub_node(dsp, path, node_attr, solution, _level, _dsp_name)\n    else:\n        data, sol = EMPTY, solution\n        # Return the sub node.\n        if node_attr == 'auto' and node['type'] != 'data':  # Auto: function.\n            node_attr = 'function'\n        elif node_attr == 'auto' and sol is not EMPTY and node_id in sol:\n            data = sol[node_id]  # Auto: data output.\n        elif node_attr == 'output' and node['type'] != 'data':\n            data = sol.workflow.nodes[node_id]['solution']\n        elif node_attr == 'output' and node['type'] == 'data':\n            data = sol[node_id]\n        elif node_attr == 'description':  # Search and return node description.\n            data = dsp.search_node_description(node_id)[0]\n        elif node_attr == 'value_type' and node['type'] == 'data':\n            # Search and return data node value's type.\n            data = dsp.search_node_description(node_id, node_attr)[0]\n        elif node_attr == 'default_value':\n            data = dsp.default_values[node_id]\n        elif node_attr == 'dsp':\n            data = dsp\n        elif node_attr == 'sol':\n            data = sol\n\n        if data is EMPTY:\n            data = node.get(node_attr, node)\n\n        return data, tuple(path)", "response": "This function returns a sub - node of a dispatcher and a path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_full_pipe(sol, base=()):\n\n    pipe, i = DspPipe(), len(base)\n\n    for p in sol._pipe:\n        n, s = p[-1]\n        d = s.dsp\n        p = {'task': p}\n\n        if n in s._errors:\n            p['error'] = s._errors[n]\n\n        node_id = s.full_name + (n,)\n\n        assert base == node_id[:i], '%s != %s' % (node_id[:i], base)\n\n        n_id = node_id[i:]\n\n        n, path = d.get_node(n, node_attr=None)\n        if n['type'] == 'function' and 'function' in n:\n            try:\n                sub_sol = s.workflow.node[path[-1]]['solution']\n                sp = get_full_pipe(sub_sol, base=node_id)\n                if sp:\n                    p['sub_pipe'] = sp\n            except KeyError:\n                pass\n\n        pipe[bypass(*n_id)] = p\n\n    return pipe", "response": "Returns the full pipe of a solution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connectChunk(key, chunk):\n    schunk = chunk[0].strip().split()\n\n    result = {'slinkNumber': schunk[1],\n              'upSjunc': schunk[2],\n              'downSjunc': schunk[3]}\n\n    return result", "response": "Parse a Storm Pipe CONNECT Chunk Method"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses Super Junction Chunk Method", "response": "def sjuncChunk(key, chunk):\n    \"\"\"\n    Parse Super Junction (SJUNC) Chunk Method\n    \"\"\"\n    schunk = chunk[0].strip().split()\n\n    result = {'sjuncNumber': schunk[1],\n              'groundSurfaceElev': schunk[2],\n              'invertElev': schunk[3],\n              'manholeSA': schunk[4],\n              'inletCode': schunk[5],\n              'linkOrCellI': schunk[6],\n              'nodeOrCellJ': schunk[7],\n              'weirSideLength': schunk[8],\n              'orificeDiameter': schunk[9]}\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse Super Link Chunk Method", "response": "def slinkChunk(key, lines):\n    \"\"\"\n    Parse Super Link (SLINK) Chunk Method\n    \"\"\"\n    KEYWORDS = ('SLINK',\n                'NODE',\n                'PIPE')\n\n    result = {'slinkNumber':None,\n              'numPipes':None,\n              'nodes':[],\n              'pipes':[]}\n\n    chunks = pt.chunk(KEYWORDS, lines)\n\n    # Parse chunks associated with each key\n    for card, chunkList in iteritems(chunks):\n        # Parse each chunk in the chunk list\n        for chunk in chunkList:\n            schunk = chunk[0].strip().split()\n\n            # Cases\n            if card == 'SLINK':\n                # SLINK handler\n                result['slinkNumber'] = schunk[1]\n                result['numPipes'] = schunk[2]\n\n            elif card == 'NODE':\n                # NODE handler\n                node = {'nodeNumber': schunk[1],\n                        'groundSurfaceElev': schunk[2],\n                        'invertElev': schunk[3],\n                        'manholeSA': schunk[4],\n                        'inletCode': schunk[5],\n                        'cellI': schunk[6],\n                        'cellJ': schunk[7],\n                        'weirSideLength': schunk[8],\n                        'orificeDiameter': schunk[9]}\n\n                result['nodes'].append(node)\n\n            elif card == 'PIPE':\n                # PIPE handler\n                pipe = {'pipeNumber': schunk[1],\n                        'xSecType': schunk[2],\n                        'diameterOrHeight': schunk[3],\n                        'width': schunk[4],\n                        'slope': schunk[5],\n                        'roughness': schunk[6],\n                        'length': schunk[7],\n                        'conductance': schunk[8],\n                        'drainSpacing': schunk[9]}\n\n                result['pipes'].append(pipe)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_items(self, page=1, order_by=None, filters=None):\n        start = (page-1)*self.per_page\n        query = self.get_query()\n        if order_by is not None:\n            query = query.order_by(self._get_field(order_by))\n        if filters is not None:\n            query = self._filter(query, filters)\n        return query.offset(start).limit(self.per_page), self.count(query)", "response": "Fetch database for items matching."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting a colored line with coordinates x y z", "response": "def colorline(ax, x, y, z, **kwargs):\n    \"\"\"\n    http://nbviewer.ipython.org/github/dpsanders/matplotlib-examples/blob/master/colorline.ipynb\n    http://matplotlib.org/examples/pylab_examples/multicolored_line.html\n    Plot a colored line with coordinates x and y\n    Optionally specify colors in the array z\n    Optionally specify a colormap, a norm function and a line width\n    \"\"\"\n\n    # Special case if a single number:\n    if not hasattr(z, \"__iter__\"):  # to check for numerical input -- this is a hack\n        z = np.array([z])\n\n    z = np.asarray(z)\n\n    segments = make_segments(x, y)\n    lc = mcoll.LineCollection(segments, array=z, **kwargs)\n\n    ax.add_collection(lc)\n    if ax.get_autoscale_on():\n        ax.autoscale_view()\n\n    return lc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating list of line segments from x and y coordinates in the correct format", "response": "def make_segments(x, y):\n    \"\"\"\n    Create list of line segments from x and y coordinates, in the correct format\n    for LineCollection: an array of the form numlines x (points per line) x 2 (x\n    and y) array\n    \"\"\"\n\n    points = np.array([x, y]).T.reshape(-1, 1, 2)\n    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n    return segments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the set has no elements in common with other.", "response": "def isdisjoint(self, other):\n        r\"\"\"Return True if the set has no elements in common with other.\n\n        Sets are disjoint iff their intersection is the empty set.\n\n        >>> ms = Multiset('aab')\n        >>> ms.isdisjoint('bc')\n        False\n        >>> ms.isdisjoint(Multiset('ccd'))\n        True\n\n        Args:\n            other: The other set to check disjointedness. Can also be an :class:`~typing.Iterable`\\[~T]\n                or :class:`~typing.Mapping`\\[~T, :class:`int`] which are then converted to :class:`Multiset`\\[~T].\n        \"\"\"\n        if isinstance(other, _sequence_types + (BaseMultiset, )):\n            pass\n        elif not isinstance(other, Container):\n            other = self._as_multiset(other)\n        return all(element not in other for element in self._elements.keys())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef union(self, *others):\n        result = self.__copy__()\n        _elements = result._elements\n        _total = result._total\n        for other in map(self._as_mapping, others):\n            for element, multiplicity in other.items():\n                old_multiplicity = _elements.get(element, 0)\n                if multiplicity > old_multiplicity:\n                    _elements[element] = multiplicity\n                    _total += multiplicity - old_multiplicity\n        result._total = _total\n        return result", "response": "r Return a new multiset with all elements from the multiset and the others with maximal multiplicities."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intersection(self, *others):\n        result = self.__copy__()\n        _elements = result._elements\n        _total = result._total\n        for other in map(self._as_mapping, others):\n            for element, multiplicity in list(_elements.items()):\n                new_multiplicity = other.get(element, 0)\n                if new_multiplicity < multiplicity:\n                    if new_multiplicity > 0:\n                        _elements[element] = new_multiplicity\n                        _total -= multiplicity - new_multiplicity\n                    else:\n                        del _elements[element]\n                        _total -= multiplicity\n        result._total = _total\n        return result", "response": "r Return a new multiset with elements common to the multiset and all others."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef symmetric_difference(self, other):\n        other = self._as_multiset(other)\n        result = self.__class__()\n        _total = 0\n        _elements = result._elements\n        self_elements = self._elements\n        other_elements = other._elements\n        dist_elements = set(self_elements.keys()) | set(other_elements.keys())\n        for element in dist_elements:\n            multiplicity = self_elements.get(element, 0)\n            other_multiplicity = other_elements.get(element, 0)\n            new_multiplicity = (multiplicity - other_multiplicity\n                                if multiplicity > other_multiplicity else other_multiplicity - multiplicity)\n            _total += new_multiplicity\n            if new_multiplicity > 0:\n                _elements[element] = new_multiplicity\n        result._total = _total\n        return result", "response": "Return a new set with elements in either the set or other but not both."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef times(self, factor):\n        if factor == 0:\n            return self.__class__()\n        if factor < 0:\n            raise ValueError('The factor must no be negative.')\n        result = self.__copy__()\n        _elements = result._elements\n        for element in _elements:\n            _elements[element] *= factor\n        result._total *= factor\n        return result", "response": "Returns a new set with each element s multiplicity multiplied with the given scalar factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, *others):\n        _elements = self._elements\n        for other in map(self._as_mapping, others):\n            for element, multiplicity in other.items():\n                self[element] += multiplicity", "response": "Like dict. update but adds multiplicities instead of replacing them."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef union_update(self, *others):\n        _elements = self._elements\n        _total = self._total\n        for other in map(self._as_mapping, others):\n            for element, multiplicity in other.items():\n                old_multiplicity = _elements.get(element, 0)\n                if multiplicity > old_multiplicity:\n                    _elements[element] = multiplicity\n                    _total += multiplicity - old_multiplicity\n        self._total = _total", "response": "Update the multiset with the union of the elements in this set and the others."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef difference_update(self, *others):\n        for other in map(self._as_multiset, others):\n            for element, multiplicity in other.items():\n                self.discard(element, multiplicity)", "response": "r Returns a new multiset with the elements contained in others removed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the multiset to contain only elements in either this multiset or the other but not both.", "response": "def symmetric_difference_update(self, other):\n        r\"\"\"Update the multiset to contain only elements in either this multiset or the other but not both.\n\n        >>> ms = Multiset('aab')\n        >>> ms.symmetric_difference_update('abc')\n        >>> sorted(ms)\n        ['a', 'c']\n\n        You can also use the ``^=`` operator for the same effect. However, the operator version\n        will only accept a set as other operator, not any iterable, to avoid errors.\n\n        >>> ms = Multiset('aabbbc')\n        >>> ms ^= Multiset('abd')\n        >>> sorted(ms)\n        ['a', 'b', 'b', 'c', 'd']\n\n        For a variant of the operation which does not modify the multiset, but returns a new\n        multiset instead see :meth:`symmetric_difference`.\n\n        Args:\n            other: The other set to take the symmetric difference with. Can also be any :class:`~typing.Iterable`\\[~T]\n                or :class:`~typing.Mapping`\\[~T, :class:`int`] which are then converted to :class:`Multiset`\\[~T].\n        \"\"\"\n        other = self._as_multiset(other)\n        elements = set(self.distinct_elements()) | set(other.distinct_elements())\n        for element in elements:\n            multiplicity = self[element]\n            other_count = other[element]\n            self[element] = (multiplicity - other_count if multiplicity > other_count else other_count - multiplicity)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates each element in the multiset by multiplying each element s multiplicity with the given scalar factor.", "response": "def times_update(self, factor):\n        \"\"\"Update each this multiset by multiplying each element's multiplicity with the given scalar factor.\n\n        >>> ms = Multiset('aab')\n        >>> ms.times_update(2)\n        >>> sorted(ms)\n        ['a', 'a', 'a', 'a', 'b', 'b']\n\n        You can also use the ``*=`` operator for the same effect:\n\n        >>> ms = Multiset('ac')\n        >>> ms *= 3\n        >>> sorted(ms)\n        ['a', 'a', 'a', 'c', 'c', 'c']\n\n        For a variant of the operation which does not modify the multiset, but returns a new\n        multiset instead see :meth:`times`.\n\n        Args:\n            factor: The factor to multiply each multiplicity with.\n        \"\"\"\n        if factor < 0:\n            raise ValueError(\"The factor must not be negative.\")\n        elif factor == 0:\n            self.clear()\n        else:\n            _elements = self._elements\n            for element in _elements:\n                _elements[element] *= factor\n            self._total *= factor"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an element to the multiset.", "response": "def add(self, element, multiplicity=1):\n        \"\"\"Adds an element to the multiset.\n\n        >>> ms = Multiset()\n        >>> ms.add('a')\n        >>> sorted(ms)\n        ['a']\n\n        An optional multiplicity can be specified to define how many of the element are added:\n\n        >>> ms.add('b', 2)\n        >>> sorted(ms)\n        ['a', 'b', 'b']\n\n        This extends the :meth:`MutableSet.add` signature to allow specifying the multiplicity.\n\n        Args:\n            element:\n                The element to add to the multiset.\n            multiplicity:\n                The multiplicity i.e. count of elements to add.\n        \"\"\"\n        if multiplicity < 1:\n            raise ValueError(\"Multiplicity must be positive\")\n        self._elements[element] += multiplicity\n        self._total += multiplicity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, element, multiplicity=None):\n        _elements = self._elements\n        if element not in _elements:\n            raise KeyError\n        old_multiplicity = _elements.get(element, 0)\n        if multiplicity is None or multiplicity >= old_multiplicity:\n            del _elements[element]\n            self._total -= old_multiplicity\n        elif multiplicity < 0:\n            raise ValueError(\"Multiplicity must be not be negative\")\n        elif multiplicity > 0:\n            _elements[element] -= multiplicity\n            self._total -= multiplicity\n        return old_multiplicity", "response": "Removes an element from the multiset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef discard(self, element, multiplicity=None):\n        _elements = self._elements\n        if element in _elements:\n            old_multiplicity = _elements[element]\n            if multiplicity is None or multiplicity >= old_multiplicity:\n                del _elements[element]\n                self._total -= old_multiplicity\n            elif multiplicity < 0:\n                raise ValueError(\"Multiplicity must not be negative\")\n            elif multiplicity > 0:\n                _elements[element] -= multiplicity\n                self._total -= multiplicity\n            return old_multiplicity\n        else:\n            return 0", "response": "Removes the element from the multiset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shutdown_executors(wait=True):\n    return {k: shutdown_executor(k, wait) for k in list(_EXECUTORS.keys())}", "response": "Clean - up all resources of all initialized executors."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes functions in an asynchronous parallel process.", "response": "def async_process(funcs, *args, executor=False, sol=None, callback=None, **kw):\n    \"\"\"\n    Execute `func(*args)` in an asynchronous parallel process.\n\n    :param funcs:\n        Functions to be executed.\n    :type funcs: list[callable]\n\n    :param args:\n        Arguments to be passed to first function call.\n    :type args: tuple\n\n    :param executor:\n        Pool executor to run the function.\n    :type executor: str | bool\n\n    :param sol:\n        Parent solution.\n    :type sol: schedula.utils.sol.Solution\n\n    :param callback:\n        Callback function to be called after all function execution.\n    :type callback: callable\n\n    :param kw:\n        Keywords to be passed to first function call.\n    :type kw: dict\n\n    :return:\n        Functions result.\n    :rtype: object\n    \"\"\"\n    name = _executor_name(executor, sol.dsp)\n    e = _get_executor(name)\n    res = (e and e.process_funcs or _process_funcs)(\n        name, funcs, executor, *args, **kw\n    )\n\n    for r in res:\n        callback and callback('sol' in r, r.get('sol', r.get('res')))\n        if 'err' in r:\n            raise r['err']\n\n    return res[-1]['res']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef async_thread(sol, args, node_attr, node_id, *a, **kw):\n    executor = _get_executor(_executor_name(kw.get('executor', False), sol.dsp))\n    if not executor:\n        return sol._evaluate_node(args, node_attr, node_id, *a, **kw)\n\n    futures = args\n    if node_attr['type'] == 'data' and (\n            node_attr['wait_inputs'] or 'function' in node_attr):\n        futures = args[0].values()\n    from concurrent.futures import Future\n    futures = {v for v in futures if isinstance(v, Future)}\n\n    def _submit():\n        return executor.thread(\n            _async_eval, sol, args, node_attr, node_id, *a, **kw\n        )\n\n    if futures:  # Chain results.\n        result = Future()\n\n        def _set_res(fut):\n            try:\n                result.set_result(fut.result())\n            except BaseException as ex:\n                result.set_exception(ex)\n\n        def _submit_task(fut=None):\n            futures.discard(fut)\n            not futures and _submit().add_done_callback(_set_res)\n\n        for f in list(futures):\n            f.add_done_callback(_submit_task)\n    else:\n        result = _submit()\n\n    timeout = node_attr.get('await_result', False)\n    if timeout is not False:\n        return _await_result(result, timeout, sol, node_id)\n\n    n = len(node_attr.get('outputs', []))\n    return AsyncList(future=result, n=n) if n > 1 else result", "response": "Execute sol. _evaluate_node in an asynchronous thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the result of a `Future` object. :param obj: Value object. :type obj: concurrent.futures.Future | object :param timeout: The number of seconds to wait for the result if the future isn't done. If None, then there is no limit on the wait time. :type timeout: int :return: Result. :rtype: object Example:: >>> from concurrent.futures import Future >>> fut = Future() >>> fut.set_result(3) >>> await_result(fut), await_result(4) (3, 4)", "response": "def await_result(obj, timeout=None):\n    \"\"\"\n    Return the result of a `Future` object.\n\n    :param obj:\n        Value object.\n    :type obj: concurrent.futures.Future | object\n\n    :param timeout:\n        The number of seconds to wait for the result if the future isn't done.\n        If None, then there is no limit on the wait time.\n    :type timeout: int\n\n    :return:\n        Result.\n    :rtype: object\n\n    Example::\n\n        >>> from concurrent.futures import Future\n        >>> fut = Future()\n        >>> fut.set_result(3)\n        >>> await_result(fut), await_result(4)\n        (3, 4)\n    \"\"\"\n    from concurrent.futures import Future\n    return obj.result(timeout) if isinstance(obj, Future) else obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary of values derived directly from the VCS. This is the first attempt to find the versions of the VCS.", "response": "def git_versions_from_vcs(tag_prefix, root, verbose=False):\n    \"\"\"Return a dictionary of values derived directly from the VCS. This is the\n    third attempt to find information by get_versions().\n    \"\"\"\n\n    # this runs 'git' from the root of the source tree. This only gets called\n    # if the git-archive 'subst' keywords were *not* expanded, and\n    # _version.py hasn't already been rewritten with a short version string,\n    # meaning we're inside a checked out source tree.\n\n    if not os.path.exists(os.path.join(root, \".git\")):\n        if verbose:\n            print(\"no .git in %s\" % root)\n        return {}\n\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    versions = {}\n\n    full_revisionid = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_revisionid is None:\n        return {}\n    versions[\"full_revisionid\"] = full_revisionid.strip()\n\n    d = run_command(GITS,\n                    [\"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\"],\n                    cwd=root)\n    if d is None:\n        return {}\n    d = d.strip()\n    # \"TAG-DIST-gHASH[-dirty]\" , where DIST might be \"0\"\n    # or just \"HASH[-dirty]\" if there are no ancestor tags\n\n    versions[\"long\"] = d\n\n    mo1 = re.search(r\"^(.*)-(\\d+)-g([0-9a-f]+)(-dirty)?$\", d)\n    mo2 = re.search(r\"^([0-9a-f]+)(-dirty)?$\", d)\n    if mo1:\n        rawtag = mo1.group(1)\n        if not rawtag.startswith(tag_prefix):\n            if verbose:\n                print(\"tag '%s' doesn't start with prefix '%s'\" % (rawtag, tag_prefix))\n            return {}\n        tag = rawtag[len(tag_prefix):]\n        versions[\"closest_tag\"] = tag\n        versions[\"distance\"] = int(mo1.group(2))\n        versions[\"short_revisionid\"] = mo1.group(3)\n        versions[\"dirty\"] = bool(mo1.group(4))\n        versions[\"pep440\"] = tag\n        if versions[\"distance\"]:\n            versions[\"describe\"] = d\n            versions[\"pep440\"] += \".post%d\" % versions[\"distance\"]\n        else:\n            versions[\"describe\"] = tag\n            if versions[\"dirty\"]:\n                versions[\"describe\"] += \"-dirty\"\n        if versions[\"dirty\"]:\n            # not strictly correct, as X.dev0 sorts \"earlier\" than X, but we\n            # need some way to distinguish the two. You shouldn't be shipping\n            # -dirty code anyways.\n            versions[\"pep440\"] += \".dev0\"\n        versions[\"default\"] = versions[\"describe\"]\n\n    elif mo2: # no ancestor tags\n        versions[\"closest_tag\"] = None\n        versions[\"short_revisionid\"] = mo2.group(1)\n        versions[\"dirty\"] = bool(mo2.group(2))\n        # count revisions to compute [\"distance\"]\n        commits = run_command(GITS, [\"rev-list\", \"--count\", \"HEAD\"], cwd=root)\n        if commits is None:\n            return {}\n        versions[\"distance\"] = int(commits.strip())\n        versions[\"pep440\"] = \"0\"\n        if versions[\"distance\"]:\n            versions[\"pep440\"] += \".post%d\" % versions[\"distance\"]\n        if versions[\"dirty\"]:\n            versions[\"pep440\"] += \".dev0\" # same concern as above\n        versions[\"describe\"] = d\n        versions[\"default\"] = \"0-%d-g%s\" % (versions[\"distance\"], d)\n    else:\n        return {}\n    versions[\"dash_dirty\"] = \"-dirty\" if versions[\"dirty\"] else \"\"\n    versions[\"closest_tag_or_zero\"] = versions[\"closest_tag\"] or \"0\"\n    if versions[\"distance\"] == 0:\n        versions[\"dash_distance\"] = \"\"\n    else:\n        versions[\"dash_distance\"] = \"-%d\" % versions[\"distance\"]\n\n    return versions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_versions(default={\"version\": \"unknown\", \"full\": \"\"}, verbose=False):\n\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    keywords = { \"refnames\": git_refnames,\n                 \"full_revisionid\": git_full_revisionid,\n                 \"short_revisionid\": git_short_revisionid }\n    ver = git_versions_from_keywords(keywords, tag_prefix, verbose)\n    if ver:\n        return ver\n\n    try:\n        root = os.path.abspath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n# TODO(dustin): Shouldn't this always loop until it fails?\n        for i in range(len(versionfile_source.split(os.sep))):\n            root = os.path.dirname(root)\n    except NameError:\n        return default\n\n    return (git_versions_from_vcs(tag_prefix, root, verbose)\n            or versions_from_parentdir(parentdir_prefix, root, verbose)\n            or default)", "response": "This function returns a list of version strings for the current version of the current versionfile."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pivot(table, left, top, value):\n    rs = {}\n    ysort = []\n    xsort = []\n    for row in table:\n        yaxis = tuple([row[c] for c in left])       # e.g. yaxis = ('Simon',)\n        if yaxis not in ysort: ysort.append(yaxis)\n        xaxis = tuple([row[c] for c in top])        # e.g. xaxis = ('2004',)\n        if xaxis not in xsort: xsort.append(xaxis)\n        try:\n            rs[yaxis]\n        except KeyError:\n            rs[yaxis] = {}\n        if xaxis not in rs[yaxis]:\n            rs[yaxis][xaxis] = 0\n        rs[yaxis][xaxis] += row[value]\n\n    \"\"\"\n    In the following loop we take care of missing data,\n    e.g 'Eric' has a value in 2004 but not in 2005\n    \"\"\"\n    for key in rs:\n        if len(rs[key]) - len(xsort):\n            for var in xsort:\n                if var not in rs[key].keys():\n                    rs[key][var] = ''\n\n    headings = list(left)\n    headings.extend(xsort)\n\n    t = []\n\n    \"\"\"\n    The lists 'sortedkeys' and 'sortedvalues' make sure that\n     even if the field 'top' is unordered, data will be transposed correctly.\n    E.g. in the example above the table rows are not ordered by the year\n    \"\"\"\n\n    for left in ysort:\n        row = list(left)\n        sortedkeys = sorted(rs[left].keys())\n        sortedvalues = map(rs[left].get, sortedkeys)\n        row.extend(sortedvalues)\n        t.append(dict(zip(headings,row)))\n\n    return t", "response": "Pivot a table from a normalised input table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to download HRRR data for GSSHA forecasts.", "response": "def download_hrrr_for_gssha(main_directory,\n                            forecast_start_date_string, #EX. '20160913'\n                            forecast_start_hour_string, #EX. '00' to '23'\n                            leftlon=-180, rightlon=180,\n                            toplat=90,bottomlat=-90):\n    \"\"\"\n    Function to download HRRR data for GSSHA\n\n    URL:\n        http://nomads.ncep.noaa.gov/cgi-bin/filter_hrrr_2d.pl\n\n    Args:\n        main_directory(str): Location of the output for the forecast data.\n        forecast_start_date_string(str): String for day of forecast. Ex. '20160913'\n        forecast_start_hour_string(str): String for hour of forecast start. Ex. '02'\n        leftlon(Optional[double,int]): Left bound for longitude. Default is -180.\n        rightlon(Optional[double,int]): Right bound for longitude. Default is 180.\n        toplat(Optional[double,int]): Top bound for latitude. Default is 90.\n        bottomlat(Optional[double,int]): Bottom bound for latitude. Default is -90.\n\n    Returns:\n        downloaded_file_list(list): List of paths to downloaded files.\n\n    Example::\n\n        from gsshapy.grid.hrrr_to_gssha import download_hrrr_for_gssha\n\n        hrrr_folder = '/HRRR'\n        leftlon = -95\n        rightlon = -75\n        toplat = 35\n        bottomlat = 30\n        downloaded_file_list = download_hrrr_for_gssha(hrrr_folder,'20160914','01',\n                                                       leftlon,rightlon,toplat,bottomlat)\n\n    \"\"\"\n    out_directory = path.join(main_directory, forecast_start_date_string)\n\n    try:\n        mkdir(out_directory)\n    except OSError:\n        pass\n\n    forecast_timestep_hour_string_array = ['00', '01', '02', '03', '04',\n                                           '05', '06', '07', '08', '09',\n                                           '10', '11', '12', '13', '14',\n                                           '15', '16', '17', '18']\n    downloaded_file_list = []\n    for forecast_timestep_hour_string in forecast_timestep_hour_string_array:\n        file_name = 'hrrr.t{0}z.wrfsfcf{1}.grib2'.format(forecast_start_hour_string, forecast_timestep_hour_string)\n        payload = {\n                   'file': file_name,\n                   'lev_10_m_above_ground': 'on',\n                   'lev_2_m_above_ground': 'on',\n                   'lev_entire_atmosphere': 'on',\n                   'lev_surface': 'on',\n                   'var_DSWRF': 'on',\n                   'var_PRATE': 'on',\n                   'var_PRES': 'on',\n                   'var_RH': 'on',\n                   'var_TMP': 'on',\n                   'var_UGRD': 'on',\n                   'var_VGRD': 'on',\n                   'var_TCDC': 'on',\n                   'subregion': '',\n                   'leftlon': str(leftlon),\n                   'rightlon': str(rightlon),\n                   'toplat': str(toplat),\n                   'bottomlat': str(bottomlat),\n                   'dir': '/hrrr.{0}'.format(forecast_start_date_string),\n                   }\n\n        r = requests.get('http://nomads.ncep.noaa.gov/cgi-bin/filter_hrrr_2d.pl', params=payload, stream=True)\n\n        if r.status_code == requests.codes.ok:\n            out_file = path.join(out_directory, file_name)\n            downloaded_file_list.append(out_file)\n            with open(out_file, 'wb') as fd:\n                for chunk in r.iter_content(chunk_size=1024):\n                    fd.write(chunk)\n        else:\n            log.error(\"Problem downloading {0}\".format(file_name))\n            for filename in downloaded_file_list:\n                try:\n                    remove(filename)\n                except OSError:\n                    pass\n            downloaded_file_list = []\n            break\n\n    return downloaded_file_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _patch_resource(self, method):\n        resource = self.client.get_resource(\"\", self.resource.path, method)\n        if not resource:\n            raise UnsupportedResourceMethodError(self.resource.path, method)\n\n        self.resource = resource", "response": "Patch the current RAML ResourceNode by the resource with the specified method if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the RAML file and store the result in self. raml", "response": "def parse_raml(self):\n        \"\"\"\n            Parse RAML file\n        \"\"\"\n        if utils.is_url(self.ramlfile):\n            raml = utils.download_file(self.ramlfile)\n        else:\n            with codecs.open(self.ramlfile, \"rb\", encoding=\"utf-8\") as raml_f:\n                raml = raml_f.read()\n\n        loader = ramlfications.loads(raml)\n        config = ramlfications.setup_config(self.ramlconfig)\n        self.raml = ramlfications.parse_raml(loader, config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_resource(self, base_resource_path, resource_path, method=None):\n        basic_path = base_resource_path + resource_path\n        dynamic_path = base_resource_path + \"{\" + resource_path + \"}\"\n        for resource in self.raml.resources:\n            method_matched = method is None or resource.method == method\n            if method_matched and (resource.path == basic_path\n                                   or resource.path == basic_path + '/'):\n                return resource\n\n            if resource.path == dynamic_path and method_matched:\n                return NodeParameter(resource=resource, parameter=resource_path)\n        return None", "response": "Gets a resource by it s path and optional by it s method"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the highest whole - number unit.", "response": "def auto(self):\n        \"\"\"Returns the highest whole-number unit.\"\"\"\n        if self._value >= 1000000000000:\n            return self.tb, 'tb'\n        if self._value >= 1000000000:\n            return self.gb, 'gb'\n        if self._value >= 1000000:\n            return self.mb, 'mb'\n        if self._value >= 1000:\n            return self.kb, 'kb'\n        else:\n            return self.b, 'b'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef auto(self):\n        if self._value >= 1099511627776:\n            return self.TiB, 'TiB'\n        if self._value >= 1073741824:\n            return self.GiB, 'GiB'\n        if self._value >= 1048576:\n            return self.MiB, 'MiB'\n        if self._value >= 1024:\n            return self.KiB, 'KiB'\n        else:\n            return self.B, 'B'", "response": "Returns the highest whole - number unit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auto_no_thousands(self):\n        if self._value >= 1000000000000:\n            return self.TiB, 'TiB'\n        if self._value >= 1000000000:\n            return self.GiB, 'GiB'\n        if self._value >= 1000000:\n            return self.MiB, 'MiB'\n        if self._value >= 1000:\n            return self.KiB, 'KiB'\n        else:\n            return self.B, 'B'", "response": "Like self. auto but calculates the next unit if > 999. 99."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef error(message, code=1):\n    if message:\n        print('ERROR: {0}'.format(message), file=sys.stderr)\n    else:\n        print(file=sys.stderr)\n    sys.exit(code)", "response": "Prints an error message to stderr and exits with a status of 1 by default."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_hmet_card_file(hmet_card_file_path, new_hmet_data_path):\n    hmet_card_file_path_temp = \"{0}_tmp\".format(hmet_card_file_path)\n    try:\n        remove(hmet_card_file_path_temp)\n    except OSError:\n        pass\n\n    copy(hmet_card_file_path, hmet_card_file_path_temp)\n\n    with io_open(hmet_card_file_path_temp, 'w', newline='\\r\\n') as out_hmet_list_file:\n        with open(hmet_card_file_path) as old_hmet_list_file:\n            for date_path in old_hmet_list_file:\n                out_hmet_list_file.write(u\"{0}\\n\".format(path.join(new_hmet_data_path,\n                                                         path.basename(date_path))))\n    try:\n        remove(hmet_card_file_path)\n    except OSError:\n        pass\n\n    rename(hmet_card_file_path_temp, hmet_card_file_path)", "response": "This function updates the paths in the HMET card file to the new location of the HMET ASCII file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets xarray dataset file handle to LSM files", "response": "def xd(self):\n        \"\"\"get xarray dataset file handle to LSM files\"\"\"\n        if self._xd is None:\n            path_to_lsm_files = path.join(self.lsm_input_folder_path,\n                                          self.lsm_search_card)\n\n            self._xd = pa.open_mfdataset(path_to_lsm_files,\n                                         lat_var=self.lsm_lat_var,\n                                         lon_var=self.lsm_lon_var,\n                                         time_var=self.lsm_time_var,\n                                         lat_dim=self.lsm_lat_dim,\n                                         lon_dim=self.lsm_lon_dim,\n                                         time_dim=self.lsm_time_dim,\n                                         loader=self.pangaea_loader)\n\n            self.lsm_time_dim = 'time'\n            self.lsm_time_var = 'time'\n        return self._xd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_subset_indices(self, y_min, y_max, x_min, x_max):\n        y_coords, x_coords = self.xd.lsm.coords\n        dx = self.xd.lsm.dx\n        dy = self.xd.lsm.dy\n\n        lsm_y_indices_from_y, lsm_x_indices_from_y = \\\n            np.where((y_coords >= (y_min - 2*dy)) &\n                     (y_coords <= (y_max + 2*dy)))\n        lsm_y_indices_from_x, lsm_x_indices_from_x = \\\n            np.where((x_coords >= (x_min - 2*dx)) &\n                     (x_coords <= (x_max + 2*dx)))\n\n        lsm_y_indices = np.intersect1d(lsm_y_indices_from_y,\n                                       lsm_y_indices_from_x)\n        lsm_x_indices = np.intersect1d(lsm_x_indices_from_y,\n                                       lsm_x_indices_from_x)\n\n        self.xslice = slice(np.amin(lsm_x_indices),\n                            np.amax(lsm_x_indices)+1)\n        self.yslice = slice(np.amin(lsm_y_indices),\n                            np.amax(lsm_y_indices)+1)", "response": "Set subset indices based on extent"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_modeling_extent(self):\n        ####\n        # STEP 1: Get extent from GSSHA Grid in LSM coordinates\n        ####\n        # reproject GSSHA grid and get bounds\n        min_x, max_x, min_y, max_y = self.gssha_grid.bounds(as_projection=self.xd.lsm.projection)\n\n        # set subset indices\n        self._set_subset_indices(min_y,\n                                 max_y,\n                                 min_x,\n                                 max_x)", "response": "Load modeling extent from GSSHA grid and set subset indices"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _time_to_string(self, dt, conversion_string=\"%Y %m %d %H %M\"):\n        if self.output_timezone is not None:\n            dt = dt.replace(tzinfo=utc) \\\n                   .astimezone(self.output_timezone)\n        return dt.strftime(conversion_string)", "response": "This converts a UTC time integer to a string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_lsm_input(self, data_var_map_array):\n        REQUIRED_HMET_VAR_LIST = ['Prcp', 'Pres', 'Temp', 'Clod',\n                                  'RlHm', 'Drad', 'Grad', 'WndS']\n\n        # make sure all required variables exist\n        given_hmet_var_list = []\n        for gssha_data_var, lsm_data_var in data_var_map_array:\n            gssha_data_hmet_name = self.netcdf_attributes[gssha_data_var]['hmet_name']\n\n            if gssha_data_hmet_name in given_hmet_var_list:\n                raise ValueError(\"Duplicate parameter for HMET variable {0}\"\n                                 .format(gssha_data_hmet_name))\n            else:\n                given_hmet_var_list.append(gssha_data_hmet_name)\n\n        for REQUIRED_HMET_VAR in REQUIRED_HMET_VAR_LIST:\n            if REQUIRED_HMET_VAR not in given_hmet_var_list:\n                raise ValueError(\"ERROR: HMET param is required to continue \"\n                                 \"{0} ...\".format(REQUIRED_HMET_VAR))", "response": "This function checks the input var map array to ensure all required input variables exist and that all required input variables exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _resample_data(self, gssha_var):\n        self.data = self.data.lsm.resample(gssha_var, self.gssha_grid)", "response": "Resample the data to match the GSSHA grid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lsm_var_to_grid(self, out_grid_file, lsm_data_var, gssha_convert_var, time_step=0, ascii_format='grass'):\n        self._load_converted_gssha_data_from_lsm(gssha_convert_var, lsm_data_var, 'grid', time_step)\n        gssha_data_var_name = self.netcdf_attributes[gssha_convert_var]['gssha_name']\n        self.data = self.data.lsm.to_projection(gssha_data_var_name,\n                                                projection=self.gssha_grid.projection)\n        self._resample_data(gssha_data_var_name)\n        arr_grid = ArrayGrid(in_array=self.data[gssha_data_var_name].values,\n                             wkt_projection=self.data.lsm.projection.ExportToWkt(),\n                             geotransform=self.data.lsm.geotransform)\n\n        if ascii_format.strip().lower() == 'grass':\n            arr_grid.to_grass_ascii(out_grid_file)\n        elif ascii_format.strip().lower() == 'arc':\n            arr_grid.to_arc_ascii(out_grid_file)\n        else:\n            raise ValueError(\"Invalid argument for 'ascii_format'. Only 'grass' or 'arc' allowed.\")", "response": "This function takes array data and writes out a GSSHA ascii grid."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lsm_precip_to_gssha_precip_gage(self, out_gage_file, lsm_data_var, precip_type=\"RADAR\"):\n        VALID_TYPES = [\"ACCUM\", \"RADAR\", \"GAGES\"] #NOTE: \"RATES\" currently not supported\n        if precip_type not in VALID_TYPES:\n            raise ValueError(\"ERROR: {0} is not a valid type. Valid types include: {1}\".format(type, VALID_TYPES))\n\n        gssha_precip_type = \"precipitation_inc\"\n        if precip_type == \"ACCUM\":\n            gssha_precip_type = \"precipitation_acc\"\n        elif precip_type == \"RADAR\":\n            gssha_precip_type = \"precipitation_rate\"\n\n        self._load_converted_gssha_data_from_lsm(gssha_precip_type, lsm_data_var, 'gage')\n        gssha_data_var_name = self.netcdf_attributes[gssha_precip_type]['gssha_name']\n        self.data = self.data.lsm.to_projection(gssha_data_var_name,\n                                                projection=self.gssha_grid.projection)\n\n        #LOOP THROUGH TIME\n        with io_open(out_gage_file, 'w') as gage_file:\n            if self.data.dims['time']>1:\n                gage_file.write(u\"EVENT \\\"Event of {0} to {1}\\\"\\n\".format(self._time_to_string(self.data.lsm.datetime[0]),\n                                                                          self._time_to_string(self.data.lsm.datetime[-1])))\n            else:\n                gage_file.write(u\"EVENT \\\"Event of {0}\\\"\\n\".format(self._time_to_string(self.data.lsm.datetime[0])))\n            gage_file.write(u\"NRPDS {0}\\n\".format(self.data.dims['time']))\n            gage_file.write(u\"NRGAG {0}\\n\".format(self.data.dims['x']*self.data.dims['y']))\n            y_coords, x_coords = self.data.lsm.coords\n            for y_idx in range(self.data.dims['y']):\n                for x_idx in range(self.data.dims['x']):\n                    coord_idx = y_idx*self.data.dims['x'] + x_idx\n                    gage_file.write(u\"COORD {0} {1} \\\"center of pixel #{2}\\\"\\n\".format(x_coords[y_idx, x_idx],\n                                                                                       y_coords[y_idx, x_idx],\n                                                                                       coord_idx))\n            for time_idx in range(self.data.dims['time']):\n                date_str = self._time_to_string(self.data.lsm.datetime[time_idx])\n                data_str = \" \".join(self.data[gssha_data_var_name][time_idx].values.ravel().astype(str))\n                gage_file.write(u\"{0} {1} {2}\\n\".format(precip_type, date_str, data_str))", "response": "This function takes array data and writes out a GSSHA precip file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_hmet_card_file(self, hmet_card_file_path, main_output_folder):\n        with io_open(hmet_card_file_path, 'w') as out_hmet_list_file:\n            for hour_time in self.data.lsm.datetime:\n                date_str = self._time_to_string(hour_time, \"%Y%m%d%H\")\n                out_hmet_list_file.write(u\"{0}\\n\".format(path.join(main_output_folder, date_str)))", "response": "This function writes the HMET_ASCII card file with ASCII file list for input to GSSHA"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lsm_data_to_arc_ascii(self, data_var_map_array,\n                                    main_output_folder=\"\"):\n        \"\"\"Writes extracted data to Arc ASCII file format into folder\n        to be read in by GSSHA. Also generates the HMET_ASCII card file\n        for GSSHA in the folder named 'hmet_file_list.txt'.\n\n        .. warning:: For GSSHA 6 Versions, for GSSHA 7 or greater, use lsm_data_to_subset_netcdf.\n\n        .. note::\n            GSSHA CARDS:\n                * HMET_ASCII pointing to the hmet_file_list.txt\n                * LONG_TERM (see: http://www.gsshawiki.com/Long-term_Simulations:Global_parameters)\n\n        Parameters:\n            data_var_map_array(list): Array to map the variables in the LSM file to the\n                                      matching required GSSHA data.\n            main_output_folder(Optional[str]): This is the path to place the generated ASCII files.\n                                        If not included, it defaults to\n                                        os.path.join(self.gssha_project_folder, \"hmet_ascii_data\").\n\n        GRIDtoGSSHA Example:\n\n        .. code:: python\n\n            from gsshapy.grid import GRIDtoGSSHA\n\n            #STEP 1: Initialize class\n            g2g = GRIDtoGSSHA(gssha_project_folder='/path/to/gssha_project',\n                              gssha_project_file_name='gssha_project.prj',\n                              lsm_input_folder_path='/path/to/wrf-data',\n                              lsm_search_card='*.nc',\n                              lsm_lat_var='XLAT',\n                              lsm_lon_var='XLONG',\n                              lsm_time_var='Times',\n                              lsm_lat_dim='south_north',\n                              lsm_lon_dim='west_east',\n                              lsm_time_dim='Time',\n                              )\n\n            #STEP 2: Generate ASCII DATA\n\n            #SEE: http://www.meteo.unican.es/wiki/cordexwrf/OutputVariables\n\n            #EXAMPLE DATA ARRAY 1: WRF GRID DATA BASED\n            data_var_map_array = [\n                                  ['precipitation_acc', ['RAINC', 'RAINNC']],\n                                  ['pressure', 'PSFC'],\n                                  ['relative_humidity', ['Q2', 'PSFC', 'T2']], #MUST BE IN ORDER: ['SPECIFIC HUMIDITY', 'PRESSURE', 'TEMPERATURE']\n                                  ['wind_speed', ['U10', 'V10']], #['U_VELOCITY', 'V_VELOCITY']\n                                  ['direct_radiation', ['SWDOWN', 'DIFFUSE_FRAC']], #MUST BE IN ORDER: ['GLOBAL RADIATION', 'DIFFUSIVE FRACTION']\n                                  ['diffusive_radiation', ['SWDOWN', 'DIFFUSE_FRAC']], #MUST BE IN ORDER: ['GLOBAL RADIATION', 'DIFFUSIVE FRACTION']\n                                  ['temperature', 'T2'],\n                                  ['cloud_cover' , 'CLDFRA'], #'CLOUD_FRACTION'\n                                 ]\n\n            g2g.lsm_data_to_arc_ascii(data_var_map_array)\n\n        HRRRtoGSSHA Example:\n\n        .. code:: python\n\n            from gsshapy.grid import HRRRtoGSSHA\n\n            #STEP 1: Initialize class\n            h2g = HRRRtoGSSHA(\n                              #YOUR INIT PARAMETERS HERE\n                             )\n\n            #STEP 2: Generate ASCII DATA\n\n            #EXAMPLE DATA ARRAY 1: HRRR GRID DATA BASED\n            data_var_map_array = [\n                                  ['precipitation_rate', 'prate'],\n                                  ['pressure', 'sp'],\n                                  ['relative_humidity', '2r'],\n                                  ['wind_speed', ['10u', '10v']],\n                                  ['direct_radiation_cc', ['dswrf', 'tcc']],\n                                  ['diffusive_radiation_cc', ['dswrf', 'tcc']],\n                                  ['temperature', 't'],\n                                  ['cloud_cover_pc' , 'tcc'],\n                                 ]\n\n            h2g.lsm_data_to_arc_ascii(data_var_map_array)\n\n        \"\"\"\n        self._check_lsm_input(data_var_map_array)\n\n        if not main_output_folder:\n            main_output_folder = path.join(self.gssha_project_folder, \"hmet_ascii_data\")\n\n        try:\n            mkdir(main_output_folder)\n        except OSError:\n            pass\n\n        log.info(\"Outputting HMET data to {0}\".format(main_output_folder))\n\n        #PART 2: DATA\n        for data_var_map in data_var_map_array:\n            gssha_data_var, lsm_data_var = data_var_map\n            gssha_data_hmet_name = self.netcdf_attributes[gssha_data_var]['hmet_name']\n            gssha_data_var_name = self.netcdf_attributes[gssha_data_var]['gssha_name']\n\n            self._load_converted_gssha_data_from_lsm(gssha_data_var, lsm_data_var, 'ascii')\n            self._convert_data_to_hourly(gssha_data_var_name)\n            self.data = self.data.lsm.to_projection(gssha_data_var_name,\n                                                    projection=self.gssha_grid.projection)\n\n            for time_idx in range(self.data.dims['time']):\n                arr_grid = ArrayGrid(in_array=self.data[gssha_data_var_name][time_idx].values,\n                                     wkt_projection=self.data.lsm.projection.ExportToWkt(),\n                                     geotransform=self.data.lsm.geotransform,\n                                     nodata_value=-9999)\n                date_str = self._time_to_string(self.data.lsm.datetime[time_idx], \"%Y%m%d%H\")\n                ascii_file_path = path.join(main_output_folder, \"{0}_{1}.asc\".format(date_str, gssha_data_hmet_name))\n                arr_grid.to_arc_ascii(ascii_file_path)\n\n        #PART 3: HMET_ASCII card input file with ASCII file list\n        hmet_card_file_path = path.join(main_output_folder, 'hmet_file_list.txt')\n        self._write_hmet_card_file(hmet_card_file_path, main_output_folder)", "response": "Writes extracted data to Arc ASCII file format into folder containing the HMET_ASCII card file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lsm_data_to_subset_netcdf(self, netcdf_file_path,\n                                        data_var_map_array,\n                                        resample_method=None):\n        \"\"\"Writes extracted data to the NetCDF file format\n\n        .. todo:: NetCDF output data time is always in UTC time. Need to convert to local timezone for GSSHA.\n\n        .. warning:: The NetCDF GSSHA file is only supported in GSSHA 7 or greater.\n\n        .. note::\n            GSSHA CARDS:\n                * HMET_NETCDF pointing to the netcdf_file_path\n                * LONG_TERM (see: http://www.gsshawiki.com/Long-term_Simulations:Global_parameters)\n\n        Parameters:\n            netcdf_file_path(string): Path to output the NetCDF file for GSSHA.\n            data_var_map_array(list): Array to map the variables in the LSM file to the\n                                      matching required GSSHA data.\n            resample_method(Optional[gdalconst]): Resample input method to match hmet data to GSSHA grid for NetCDF output. Default is None.\n\n\n        GRIDtoGSSHA Example:\n\n        .. code:: python\n\n            from gsshapy.grid import GRIDtoGSSHA\n\n            #STEP 1: Initialize class\n            g2g = GRIDtoGSSHA(gssha_project_folder='/path/to/gssha_project',\n                              gssha_project_file_name='gssha_project.prj',\n                              lsm_input_folder_path='/path/to/wrf-data',\n                              lsm_search_card='*.nc',\n                              lsm_lat_var='XLAT',\n                              lsm_lon_var='XLONG',\n                              lsm_time_var='Times',\n                              lsm_lat_dim='south_north',\n                              lsm_lon_dim='west_east',\n                              lsm_time_dim='Time',\n                              )\n\n            #STEP 2: Generate NetCDF DATA\n\n            #EXAMPLE DATA ARRAY 1: WRF GRID DATA BASED\n            #SEE: http://www.meteo.unican.es/wiki/cordexwrf/OutputVariables\n\n            data_var_map_array = [\n                                  ['precipitation_acc', ['RAINC', 'RAINNC']],\n                                  ['pressure', 'PSFC'],\n                                  ['relative_humidity', ['Q2', 'PSFC', 'T2']], #MUST BE IN ORDER: ['SPECIFIC HUMIDITY', 'PRESSURE', 'TEMPERATURE']\n                                  ['wind_speed', ['U10', 'V10']], #['U_VELOCITY', 'V_VELOCITY']\n                                  ['direct_radiation', ['SWDOWN', 'DIFFUSE_FRAC']], #MUST BE IN ORDER: ['GLOBAL RADIATION', 'DIFFUSIVE FRACTION']\n                                  ['diffusive_radiation', ['SWDOWN', 'DIFFUSE_FRAC']], #MUST BE IN ORDER: ['GLOBAL RADIATION', 'DIFFUSIVE FRACTION']\n                                  ['temperature', 'T2'],\n                                  ['cloud_cover' , 'CLDFRA'], #'CLOUD_FRACTION'\n                                 ]\n\n            g2g.lsm_data_to_subset_netcdf(\"E/GSSHA/gssha_wrf_data.nc\",\n                                          data_var_map_array)\n\n        HRRRtoGSSHA Example:\n\n        .. code:: python\n\n            from gsshapy.grid import HRRRtoGSSHA\n\n            #STEP 1: Initialize class\n            h2g = HRRRtoGSSHA(\n                              #YOUR INIT PARAMETERS HERE\n                             )\n\n            #STEP 2: Generate NetCDF DATA\n\n            #EXAMPLE DATA ARRAY 2: HRRR GRID DATA BASED\n            data_var_map_array = [\n                                  ['precipitation_rate', 'prate'],\n                                  ['pressure', 'sp'],\n                                  ['relative_humidity', '2r'],\n                                  ['wind_speed', ['10u', '10v']],\n                                  ['direct_radiation_cc', ['dswrf', 'tcc']],\n                                  ['diffusive_radiation_cc', ['dswrf', 'tcc']],\n                                  ['temperature', 't'],\n                                  ['cloud_cover_pc' , 'tcc'],\n                                 ]\n\n            h2g.lsm_data_to_subset_netcdf(\"E:/GSSHA/gssha_wrf_data.nc\",\n                                          data_var_map_array)\n        \"\"\"\n        self._check_lsm_input(data_var_map_array)\n\n        output_datasets = []\n        #DATA\n        for gssha_var, lsm_var in data_var_map_array:\n            if gssha_var in self.netcdf_attributes:\n                self._load_converted_gssha_data_from_lsm(gssha_var, lsm_var, 'netcdf')\n                #previously just added data, but needs to be hourly\n                gssha_data_var_name = self.netcdf_attributes[gssha_var]['gssha_name']\n                self._convert_data_to_hourly(gssha_data_var_name)\n                if resample_method:\n                    self._resample_data(gssha_data_var_name)\n                else:\n                    self.data = self.data.lsm.to_projection(gssha_data_var_name,\n                                                            projection=self.gssha_grid.projection)\n\n                output_datasets.append(self.data)\n            else:\n                raise ValueError(\"Invalid GSSHA variable name: {0} ...\".format(gssha_var))\n        output_dataset = xr.merge(output_datasets)\n        #add global attributes\n        output_dataset.attrs['Convention'] = 'CF-1.6'\n        output_dataset.attrs['title'] = 'GSSHA LSM Input'\n        output_dataset.attrs['history'] = 'date_created: {0}'.format(datetime.utcnow())\n        output_dataset.attrs['proj4'] = self.data.attrs['proj4']\n        output_dataset.attrs['geotransform'] = self.data.attrs['geotransform']\n\n        output_dataset.to_netcdf(netcdf_file_path)", "response": "Writes extracted data to the NetCDF file format"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(self, **kwargs):\n\n        query_params = {\"_actions\": \"false\", \"_links\": \"true\", \"_embedded\": \"true\"}\n        path_params = {}\n        headers = {}\n        body = None\n\n        if \"applicationId\" in kwargs:\n            path_params[\"applicationId\"] = kwargs[\"applicationId\"]\n        if \"query\" in kwargs:\n            body = kwargs[\"query\"]\n        if \"losantdomain\" in kwargs:\n            headers[\"losantdomain\"] = kwargs[\"losantdomain\"]\n        if \"_actions\" in kwargs:\n            query_params[\"_actions\"] = kwargs[\"_actions\"]\n        if \"_links\" in kwargs:\n            query_params[\"_links\"] = kwargs[\"_links\"]\n        if \"_embedded\" in kwargs:\n            query_params[\"_embedded\"] = kwargs[\"_embedded\"]\n\n        path = \"/applications/{applicationId}/data/export\".format(**path_params)\n\n        return self.client.request(\"POST\", path, params=query_params, headers=headers, body=body)", "response": "This function returns a CSV file from a query of devices and attributes over a time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAsKml(self, session):\n        statement = \"\"\"\n                    SELECT ST_AsKml({0}) AS kml\n                    FROM {1}\n                    WHERE id={2};\n                    \"\"\".format(self.geometryColumnName,\n                               self.tableName,\n                               self.id)\n\n        result = session.execute(statement)\n\n        for row in result:\n            return row.kml", "response": "Retrieve the geometry in KML format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the geometry in Well Known Text format.", "response": "def getAsWkt(self, session):\n        \"\"\"\n        Retrieve the geometry in Well Known Text format.\n\n        This method is a veneer for an SQL query that calls the ``ST_AsText()`` function on the geometry column.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n\n        Returns:\n            str: Well Known Text string representation of geometry.\n        \"\"\"\n        statement = \"\"\"\n                    SELECT ST_AsText({0}) AS wkt\n                    FROM {1}\n                    WHERE id={2};\n                    \"\"\".format(self.geometryColumnName,\n                               self.tableName,\n                               self.id)\n\n        result = session.execute(statement)\n\n        for row in result:\n            return row.wkt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the geometry in GeoJSON format.", "response": "def getAsGeoJson(self, session):\n        \"\"\"\n        Retrieve the geometry in GeoJSON format.\n\n        This method is a veneer for an SQL query that calls the ``ST_AsGeoJSON()`` function on the geometry column.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n\n        Returns:\n            str: GeoJSON string representation of geometry.\n        \"\"\"\n        statement = \"\"\"\n                    SELECT ST_AsGeoJSON({0}) AS json\n                    FROM {1}\n                    WHERE id={2};\n                    \"\"\".format(self.geometryColumnName,\n                               self.tableName,\n                               self.id)\n\n        result = session.execute(statement)\n\n        for row in result:\n            return row.json"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getSpatialReferenceId(self, session):\n        statement = \"\"\"\n                    SELECT ST_SRID({0}) AS srid\n                    FROM {1}\n                    WHERE id={2};\n                    \"\"\".format(self.geometryColumnName,\n                               self.tableName,\n                               self.id)\n\n        result = session.execute(statement)\n\n        for row in result:\n            return row.srid", "response": "Returns the spatial reference id by which the geometry column is registered."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a new object from a message in message and return a response in the appropriate format.", "response": "def _generate(self, message):\n        \"\"\"Given a message in message,\n           return a response in the appropriate\n           format.\"\"\"\n        raw_params = {\"INPUT_TEXT\"  : message.encode('UTF8'),\n                      \"INPUT_TYPE\"  : self.input_type,\n                      \"OUTPUT_TYPE\" : self.output_type,\n                      \"LOCALE\"      : self._locale,\n                      \"AUDIO\"       : self.audio,\n                      \"VOICE\"       : self._voice,\n                      }\n        params = urlencode(raw_params)\n        headers = {}\n\n        logging.debug('maryclient: generate, raw_params=%s' % repr(raw_params))\n\n        # Open connection to self._host, self._port.\n        conn = httplib.HTTPConnection(self._host, self._port)\n\n        #conn.set_debuglevel(5)\n        \n        conn.request(\"POST\", \"/process\", params, headers)\n        response = conn.getresponse()\n        if response.status != 200:\n            logging.error(response.getheaders())\n            raise Exception (\"{0}: {1}\".format(response.status, response.reason))\n        return response.read()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef receive(self, msg):\n        '''\n        The message received from the queue specify a method of the\n        class the actor represents. This invokes it. If the\n        communication is an ASK, sends the result back\n        to the channel included in the message as an\n        ASKRESPONSE.\n\n        If it is a FUTURE, generates a FUTURERESPONSE\n        to send the result to the manager.\n\n        :param msg: The message is a dictionary using the constants\n            defined in util.py (:mod:`pyactor.util`).\n        '''\n        if msg[TYPE] == TELL and msg[METHOD] == 'stop':\n            self.running = False\n            self.future_manager.stop()\n        else:\n            result = None\n            try:\n                invoke = getattr(self._obj, msg[METHOD])\n                params = msg[PARAMS]\n                result = invoke(*params[0], **params[1])\n            except Exception, e:\n                if msg[TYPE] == TELL:\n                    print e\n                    return\n                result = e\n            self.send_response(result, msg)", "response": "This method is called when a message is received from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the actor thread wich will process the channel queue while the actor is alive.", "response": "def run(self):\n        '''\n        Creates the actor thread wich will process the channel queue\n        while the actor :meth:`is_alive`, making it able to receive\n        queries.\n        '''\n        self.thread = Thread(target=self.__processQueue)\n        self.thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef right_ascension_at_time(self,t):\n        '''RA of prime meridian'''\n        \u03b10 = self.right_ascension_at_epoch\n        t0 = self.epoch\n        \u03c9 = self.rotational_speed\n        return (\u03b10 + \u03c9 * (t - t0)) % (2*\u03c0)", "response": "RA of prime meridian"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef blit_alpha(dest, source, pos, opacity):\n    # http://www.nerdparadise.com/tech/python/pygame/blitopacity/\n    (x, y) = pos\n    temp = pygame.Surface((source.get_width(),\n                           source.get_height())).convert()\n    temp.blit(dest, (-x, -y))\n    temp.blit(source, (0, 0))\n    temp.set_alpha(opacity)\n    dest.blit(temp, pos)", "response": "blit per - pixel alpha source onto dest with surface opacity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_file(url):\n    response = requests.get(url)\n    if response.status_code is not 200:\n        return None\n    return response.text", "response": "Downloads a file from the specified URL and returns the content of the downloaded file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a single data node to the dispatcher. :param data_id: Data node id. If None will be assigned automatically ('unknown<%d>') not in dmap. :type data_id: str, optional :param default_value: Data node default value. This will be used as input if it is not specified as inputs in the ArciDispatch algorithm. :type default_value: T, optional :param initial_dist: Initial distance in the ArciDispatch algorithm when the data node default value is used. :type initial_dist: float, int, optional :param wait_inputs: If True ArciDispatch algorithm stops on the node until it gets all input estimations. :type wait_inputs: bool, optional :param wildcard: If True, when the data node is used as input and target in the ArciDispatch algorithm, the input value will be used as input for the connected functions, but not as output. :type wildcard: bool, optional :param function: Data node estimation function. This can be any function that takes only one dictionary (key=function node id, value=estimation of data node) as input and return one value that is the estimation of the data node. :type function: callable, optional :param callback: Callback function to be called after node estimation. This can be any function that takes only one argument that is the data node estimation output. It does not return anything. :type callback: callable, optional :param description: Data node's description. :type description: str, optional :param filters: A list of functions that are invoked after the invocation of the main function. :type filters: list[function], optional :param await_result: If True the Dispatcher waits data results before assigning them to the solution. If a number is defined this is used as `timeout` for `Future.result` method [default: False]. Note this is used when asynchronous or parallel execution is enable. :type await_result: bool|int|float, optional :param kwargs: Set additional node attributes using key=value. :type kwargs: keyword arguments, optional :return: Data node id. :rtype: str .. seealso:: :func:`add_func`, :func:`add_function`, :func:`add_dispatcher`, :func:`add_from_lists` **--------------------------------------------------------------------** **Example**: .. testsetup:: >>> dsp = Dispatcher(name='Dispatcher') Add a data to be estimated or a possible input data node:: >>> dsp.add_data(data_id='a') 'a' Add a data with a default value (i.e., input data node):: >>> dsp.add_data(data_id='b', default_value=1) 'b' Create a data node with function estimation and a default value. - function estimation: estimate one unique output from multiple estimations. - default value: is a default estimation. >>> def min_fun(kwargs): ... ''' ... Returns the minimum value of node estimations. ... ... :param kwargs: ... Node estimations. ... :type kwargs: dict ... ... :return: ... The minimum value of node estimations. ... :rtype: float ... ''' ... ... return min(kwargs.values()) ... >>> dsp.add_data(data_id='c', default_value=2, wait_inputs=True, ... function=min_fun) 'c' Create a data with an unknown id and return the generated id:: >>> dsp.add_data() 'unknown'", "response": "def add_data(self, data_id=None, default_value=EMPTY, initial_dist=0.0,\n                 wait_inputs=False, wildcard=None, function=None, callback=None,\n                 description=None, filters=None, await_result=None, **kwargs):\n        \"\"\"\n        Add a single data node to the dispatcher.\n\n        :param data_id:\n            Data node id. If None will be assigned automatically ('unknown<%d>')\n            not in dmap.\n        :type data_id: str, optional\n\n        :param default_value:\n            Data node default value. This will be used as input if it is not\n            specified as inputs in the ArciDispatch algorithm.\n        :type default_value: T, optional\n\n        :param initial_dist:\n            Initial distance in the ArciDispatch algorithm when the data node\n            default value is used.\n        :type initial_dist: float, int, optional\n\n        :param wait_inputs:\n            If True ArciDispatch algorithm stops on the node until it gets all\n            input estimations.\n        :type wait_inputs: bool, optional\n\n        :param wildcard:\n            If True, when the data node is used as input and target in the\n            ArciDispatch algorithm, the input value will be used as input for\n            the connected functions, but not as output.\n        :type wildcard: bool, optional\n\n        :param function:\n            Data node estimation function.\n            This can be any function that takes only one dictionary\n            (key=function node id, value=estimation of data node) as input and\n            return one value that is the estimation of the data node.\n        :type function: callable, optional\n\n        :param callback:\n            Callback function to be called after node estimation.\n            This can be any function that takes only one argument that is the\n            data node estimation output. It does not return anything.\n        :type callback: callable, optional\n\n        :param description:\n            Data node's description.\n        :type description: str, optional\n\n        :param filters:\n            A list of functions that are invoked after the invocation of the\n            main function.\n        :type filters: list[function], optional\n\n        :param await_result:\n            If True the Dispatcher waits data results before assigning them to\n            the solution. If a number is defined this is used as `timeout` for\n            `Future.result` method [default: False]. Note this is used when\n            asynchronous or parallel execution is enable.\n        :type await_result: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n\n        :return:\n            Data node id.\n        :rtype: str\n\n        .. seealso:: :func:`add_func`, :func:`add_function`,\n           :func:`add_dispatcher`, :func:`add_from_lists`\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        .. testsetup::\n            >>> dsp = Dispatcher(name='Dispatcher')\n\n        Add a data to be estimated or a possible input data node::\n\n            >>> dsp.add_data(data_id='a')\n            'a'\n\n        Add a data with a default value (i.e., input data node)::\n\n            >>> dsp.add_data(data_id='b', default_value=1)\n            'b'\n\n        Create a data node with function estimation and a default value.\n\n            - function estimation: estimate one unique output from multiple\n              estimations.\n            - default value: is a default estimation.\n\n            >>> def min_fun(kwargs):\n            ...     '''\n            ...     Returns the minimum value of node estimations.\n            ...\n            ...     :param kwargs:\n            ...         Node estimations.\n            ...     :type kwargs: dict\n            ...\n            ...     :return:\n            ...         The minimum value of node estimations.\n            ...     :rtype: float\n            ...     '''\n            ...\n            ...     return min(kwargs.values())\n            ...\n            >>> dsp.add_data(data_id='c', default_value=2, wait_inputs=True,\n            ...              function=min_fun)\n            'c'\n\n        Create a data with an unknown id and return the generated id::\n\n            >>> dsp.add_data()\n            'unknown'\n        \"\"\"\n\n        # Set special data nodes.\n        if data_id is START:\n            default_value, description = NONE, START.__doc__\n        elif data_id is SINK:\n            wait_inputs, function, description = True, bypass, SINK.__doc__\n        elif data_id is SELF:\n            default_value, description = self, SELF.__doc__\n        elif data_id is PLOT:\n            from .utils.drw import autoplot_callback, autoplot_function\n            callback, description = callback or autoplot_callback, PLOT.__doc__\n            function = function or autoplot_function\n\n        # Base data node attributes.\n        attr_dict = {\n            'type': 'data',\n            'wait_inputs': wait_inputs,\n            'index': (self.counter(),)\n        }\n\n        if function is not None:  # Add function as node attribute.\n            attr_dict['function'] = function\n\n        if await_result is not None:  # Add await_result as node attribute.\n            attr_dict['await_result'] = await_result\n\n        if callback is not None:  # Add callback as node attribute.\n            attr_dict['callback'] = callback\n\n        if wildcard is not None:  # Add wildcard as node attribute.\n            attr_dict['wildcard'] = wildcard\n\n        if description is not None:  # Add description as node attribute.\n            attr_dict['description'] = description\n\n        if filters:  # Add filters as node attribute.\n            attr_dict['filters'] = filters\n\n        attr_dict.update(kwargs)  # Additional attributes.\n\n        has_node = self.dmap.has_node  # Namespace shortcut for speed.\n\n        if data_id is None:  # Search for an unused node id.\n            from .utils.alg import get_unused_node_id\n            data_id = get_unused_node_id(self.dmap)  # Get an unused node id.\n\n        # Check if the node id exists as function.\n        elif has_node(data_id) and self.dmap.nodes[data_id]['type'] != 'data':\n            raise ValueError('Invalid data id: '\n                             'override function {}'.format(data_id))\n\n        # Add node to the dispatcher map.\n        self.dmap.add_node(data_id, **attr_dict)\n\n        # Set default value.\n        self.set_default_value(data_id, default_value, initial_dist)\n\n        return data_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_function(self, function_id=None, function=None, inputs=None,\n                     outputs=None, input_domain=None, weight=None,\n                     inp_weight=None, out_weight=None, description=None,\n                     filters=None, await_domain=None, await_result=None,\n                     **kwargs):\n        \"\"\"\n        Add a single function node to dispatcher.\n\n        :param function_id:\n            Function node id.\n            If None will be assigned as <fun.__name__>.\n        :type function_id: str, optional\n\n        :param function:\n            Data node estimation function.\n        :type function: callable, optional\n\n        :param inputs:\n            Ordered arguments (i.e., data node ids) needed by the function.\n        :type inputs: list, optional\n\n        :param outputs:\n            Ordered results (i.e., data node ids) returned by the function.\n        :type outputs: list, optional\n\n        :param input_domain:\n            A function that checks if input values satisfy the function domain.\n            This can be any function that takes the same inputs of the function\n            and returns True if input values satisfy the domain, otherwise\n            False. In this case the dispatch algorithm doesn't pass on the node.\n        :type input_domain: callable, optional\n\n        :param weight:\n            Node weight. It is a weight coefficient that is used by the dispatch\n            algorithm to estimate the minimum workflow.\n        :type weight: float, int, optional\n\n        :param inp_weight:\n            Edge weights from data nodes to the function node.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type inp_weight: dict[str, float | int], optional\n\n        :param out_weight:\n            Edge weights from the function node to data nodes.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type out_weight: dict[str, float | int], optional\n\n        :param description:\n            Function node's description.\n        :type description: str, optional\n\n        :param filters:\n            A list of functions that are invoked after the invocation of the\n            main function.\n        :type filters: list[function], optional\n\n        :param await_domain:\n            If True the Dispatcher waits all input results before executing the\n            `input_domain` function. If a number is defined this is used as\n            `timeout` for `Future.result` method [default: True]. Note this is\n            used when asynchronous or parallel execution is enable.\n        :type await_domain: bool|int|float, optional\n\n        :param await_result:\n            If True the Dispatcher waits output results before assigning them to\n            the workflow. If a number is defined this is used as `timeout` for\n            `Future.result` method [default: False]. Note this is used when\n            asynchronous or parallel execution is enable.\n        :type await_result: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n\n        :return:\n            Function node id.\n        :rtype: str\n\n        .. seealso:: :func:`add_data`, :func:`add_func`, :func:`add_dispatcher`,\n           :func:`add_from_lists`\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        .. testsetup::\n            >>> dsp = Dispatcher(name='Dispatcher')\n\n        Add a function node::\n\n            >>> def my_function(a, b):\n            ...     c = a + b\n            ...     d = a - b\n            ...     return c, d\n            ...\n            >>> dsp.add_function(function=my_function, inputs=['a', 'b'],\n            ...                  outputs=['c', 'd'])\n            'my_function'\n\n        Add a function node with domain::\n\n            >>> from math import log\n            >>> def my_log(a, b):\n            ...     return log(b - a)\n            ...\n            >>> def my_domain(a, b):\n            ...     return a < b\n            ...\n            >>> dsp.add_function(function=my_log, inputs=['a', 'b'],\n            ...                  outputs=['e'], input_domain=my_domain)\n            'my_log'\n        \"\"\"\n        from .utils.blue import _init\n        function = _init(function)\n\n        if inputs is None:  # Set a dummy input.\n            if START not in self.nodes:\n                self.add_data(START)\n\n            inputs = [START]  # Update inputs.\n\n        if outputs is None:  # Set a dummy output.\n            if SINK not in self.nodes:\n                self.add_data(SINK)\n\n            outputs = [SINK]  # Update outputs.\n\n        # Get parent function.\n        func = parent_func(function)\n\n        # Base function node attributes.\n        attr_dict = {\n            'type': 'function',\n            'inputs': inputs,\n            'outputs': outputs,\n            'function': function,\n            'wait_inputs': True,\n            'index': (self.counter(),)\n        }\n\n        if input_domain:  # Add domain as node attribute.\n            attr_dict['input_domain'] = input_domain\n\n        if await_domain is not None:  # Add await_domain as node attribute.\n            attr_dict['await_domain'] = await_domain\n\n        if await_result is not None:  # Add await_result as node attribute.\n            attr_dict['await_result'] = await_result\n\n        if description is not None:  # Add description as node attribute.\n            attr_dict['description'] = description\n\n        if filters:  # Add filters as node attribute.\n            attr_dict['filters'] = filters\n\n        # Set function name.\n        if function_id is None:\n            try:  # Set function name.\n                function_name = func.__name__\n            except AttributeError as ex:\n                raise ValueError('Invalid function id due to:\\n{}'.format(ex))\n        else:\n            function_name = function_id\n\n        from .utils.alg import get_unused_node_id\n        # Get an unused node id.\n        fun_id = get_unused_node_id(self.dmap, initial_guess=function_name)\n\n        if weight is not None:  # Add weight as node attribute.\n            attr_dict['weight'] = weight\n\n        attr_dict.update(kwargs)  # Set additional attributes.\n\n        # Add node to the dispatcher map.\n        self.dmap.add_node(fun_id, **attr_dict)\n\n        from .utils.alg import add_func_edges  # Add input edges.\n        n_data = add_func_edges(self, fun_id, inputs, inp_weight, True)\n\n        # Add output edges.\n        add_func_edges(self, fun_id, outputs, out_weight, False, n_data)\n\n        return fun_id", "response": "This method adds a function node to the dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a function to the dispatcher.", "response": "def add_func(self, function, outputs=None, weight=None,\n                 inputs_defaults=False, inputs_kwargs=False, filters=None,\n                 input_domain=None, await_domain=None, await_result=None,\n                 inp_weight=None, out_weight=None, description=None,\n                 inputs=None, function_id=None, **kwargs):\n        \"\"\"\n        Add a single function node to dispatcher.\n\n        :param inputs_kwargs:\n            Do you want to include kwargs as inputs?\n        :type inputs_kwargs: bool\n\n        :param inputs_defaults:\n            Do you want to set default values?\n        :type inputs_defaults: bool\n\n        :param function_id:\n            Function node id.\n            If None will be assigned as <fun.__name__>.\n        :type function_id: str, optional\n\n        :param function:\n            Data node estimation function.\n        :type function: callable, optional\n\n        :param inputs:\n            Ordered arguments (i.e., data node ids) needed by the function.\n            If None it will take parameters names from function signature.\n        :type inputs: list, optional\n\n        :param outputs:\n            Ordered results (i.e., data node ids) returned by the function.\n        :type outputs: list, optional\n\n        :param input_domain:\n            A function that checks if input values satisfy the function domain.\n            This can be any function that takes the same inputs of the function\n            and returns True if input values satisfy the domain, otherwise\n            False. In this case the dispatch algorithm doesn't pass on the node.\n        :type input_domain: callable, optional\n\n        :param weight:\n            Node weight. It is a weight coefficient that is used by the dispatch\n            algorithm to estimate the minimum workflow.\n        :type weight: float, int, optional\n\n        :param inp_weight:\n            Edge weights from data nodes to the function node.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type inp_weight: dict[str, float | int], optional\n\n        :param out_weight:\n            Edge weights from the function node to data nodes.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type out_weight: dict[str, float | int], optional\n\n        :param description:\n            Function node's description.\n        :type description: str, optional\n\n        :param filters:\n            A list of functions that are invoked after the invocation of the\n            main function.\n        :type filters: list[function], optional\n\n        :param await_domain:\n            If True the Dispatcher waits all input results before executing the\n            `input_domain` function. If a number is defined this is used as\n            `timeout` for `Future.result` method [default: True]. Note this is\n            used when asynchronous or parallel execution is enable.\n        :type await_domain: bool|int|float, optional\n\n        :param await_result:\n            If True the Dispatcher waits output results before assigning them to\n            the workflow. If a number is defined this is used as `timeout` for\n            `Future.result` method [default: False]. Note this is used when\n            asynchronous or parallel execution is enable.\n        :type await_result: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n\n        :return:\n            Function node id.\n        :rtype: str\n\n        .. seealso:: :func:`add_func`, :func:`add_function`,\n           :func:`add_dispatcher`, :func:`add_from_lists`\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        .. dispatcher:: sol\n           :opt: graph_attr={'ratio': '1'}\n           :code:\n\n            >>> import schedula as sh\n            >>> dsp = sh.Dispatcher(name='Dispatcher')\n            >>> def f(a, b, c, d=3, m=5):\n            ...     return (a + b) - c + d - m\n            >>> dsp.add_func(f, outputs=['d'])\n            'f'\n            >>> dsp.add_func(f, ['m'], inputs_defaults=True, inputs='beal')\n            'f<0>'\n            >>> dsp.add_func(f, ['i'], inputs_kwargs=True)\n            'f<1>'\n            >>> def g(a, b, c, *args, d=0):\n            ...     return (a + b) * c + d\n            >>> dsp.add_func(g, ['e'], inputs_defaults=True)\n            'g'\n            >>> sol = dsp({'a': 1, 'b': 3, 'c': 0}); sol\n            Solution([('a', 1), ('b', 3), ('c', 0), ('l', 3), ('d', 2),\n                      ('e', 0), ('m', 0), ('i', 6)])\n        \"\"\"\n        from .utils.blue import _init\n        from .utils.dsp import _get_par_args\n        function = _init(function)\n\n        if inputs is None:\n            inputs = tuple(_get_par_args(function, not inputs_kwargs)) or None\n\n        function_id = self.add_function(\n            weight=weight, filters=filters, outputs=outputs, function=function,\n            input_domain=input_domain, await_domain=await_domain, inputs=inputs,\n            description=description, out_weight=out_weight,\n            inp_weight=inp_weight, await_result=await_result,\n            function_id=function_id, **kwargs\n        )\n\n        if inputs_defaults:\n            for k, v in zip(inputs, _get_par_args(function, False).values()):\n                if v.default is not v.empty:\n                    self.set_default_value(k, v._default)\n\n        return function_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_dispatcher(self, dsp, inputs, outputs, dsp_id=None,\n                       input_domain=None, weight=None, inp_weight=None,\n                       description=None, include_defaults=False,\n                       await_domain=None, **kwargs):\n        \"\"\"\n        Add a single sub-dispatcher node to dispatcher.\n\n        :param dsp:\n            Child dispatcher that is added as sub-dispatcher node to the parent\n            dispatcher.\n        :type dsp: Dispatcher | dict[str, list]\n\n        :param inputs:\n            Inputs mapping. Data node ids from parent dispatcher to child\n            sub-dispatcher.\n        :type inputs: dict[str, str | list[str]] | tuple[str] | \n                      (str, ..., dict[str, str | list[str]])\n\n        :param outputs:\n            Outputs mapping. Data node ids from child sub-dispatcher to parent\n            dispatcher.\n        :type outputs: dict[str, str | list[str]] | tuple[str] | \n                       (str, ..., dict[str, str | list[str]])\n\n        :param dsp_id:\n            Sub-dispatcher node id.\n            If None will be assigned as <dsp.name>.\n        :type dsp_id: str, optional\n\n        :param input_domain:\n            A function that checks if input values satisfy the function domain.\n            This can be any function that takes the a dictionary with the inputs\n            of the sub-dispatcher node and returns True if input values satisfy\n            the domain, otherwise False.\n\n            .. note:: This function is invoked every time that a data node reach\n               the sub-dispatcher node.\n        :type input_domain: (dict) -> bool, optional\n\n        :param weight:\n            Node weight. It is a weight coefficient that is used by the dispatch\n            algorithm to estimate the minimum workflow.\n        :type weight: float, int, optional\n\n        :param inp_weight:\n            Edge weights from data nodes to the sub-dispatcher node.\n            It is a dictionary (key=data node id) with the weight coefficients\n            used by the dispatch algorithm to estimate the minimum workflow.\n        :type inp_weight: dict[str, int | float], optional\n\n        :param description:\n            Sub-dispatcher node's description.\n        :type description: str, optional\n\n        :param include_defaults:\n            If True the default values of the sub-dispatcher are added to the\n            current dispatcher.\n        :type include_defaults: bool, optional\n\n        :param await_domain:\n            If True the Dispatcher waits all input results before executing the\n            `input_domain` function. If a number is defined this is used as\n            `timeout` for `Future.result` method [default: True]. Note this is\n            used when asynchronous or parallel execution is enable.\n        :type await_domain: bool|int|float, optional\n\n        :param kwargs:\n            Set additional node attributes using key=value.\n        :type kwargs: keyword arguments, optional\n\n        :return:\n            Sub-dispatcher node id.\n        :rtype: str\n\n        .. seealso:: :func:`add_data`, :func:`add_func`, :func:`add_function`,\n           :func:`add_from_lists`\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        .. testsetup::\n            >>> dsp = Dispatcher(name='Dispatcher')\n\n        Create a sub-dispatcher::\n\n            >>> sub_dsp = Dispatcher()\n            >>> sub_dsp.add_function('max', max, ['a', 'b'], ['c'])\n            'max'\n\n        Add the sub-dispatcher to the parent dispatcher::\n\n            >>> dsp.add_dispatcher(dsp_id='Sub-Dispatcher', dsp=sub_dsp,\n            ...                    inputs={'A': 'a', 'B': 'b'},\n            ...                    outputs={'c': 'C'})\n            'Sub-Dispatcher'\n\n        Add a sub-dispatcher node with domain::\n\n\n            >>> def my_domain(kwargs):\n            ...     return kwargs['C'] > 3\n            ...\n            >>> dsp.add_dispatcher(dsp_id='Sub-Dispatcher with domain',\n            ...                    dsp=sub_dsp, inputs={'C': 'a', 'D': 'b'},\n            ...                    outputs={('c', 'b'): ('E', 'E1')},\n            ...                    input_domain=my_domain)\n            'Sub-Dispatcher with domain'\n        \"\"\"\n        from .utils.blue import _init\n        dsp = _init(dsp)\n\n        if not isinstance(dsp, self.__class__):\n            kw = dsp\n            dsp = self.__class__(\n                name=dsp_id or 'unknown',\n                executor=self.executor\n            )\n            dsp.add_from_lists(**kw)\n\n        if not dsp_id:  # Get the dsp id.\n            dsp_id = dsp.name or 'unknown'\n\n        if description is None:  # Get description.\n            description = dsp.__doc__ or None\n\n        if not isinstance(inputs, dict):  # Create the inputs dict.\n            inputs = kk_dict(*inputs)\n\n        if not isinstance(outputs, dict):  # Create the outputs dict.\n            outputs = kk_dict(*outputs)\n\n        # Set zero as default input distances.\n        # noinspection PyTypeChecker\n        _weight_from = dict.fromkeys(inputs.keys(), 0.0)\n        _weight_from.update(inp_weight or {})\n\n        from .utils.alg import _nodes\n\n        # Return dispatcher node id.\n        dsp_id = self.add_function(\n            dsp_id, dsp, sorted(_nodes(inputs)),\n            sorted(_nodes(outputs.values())), input_domain, weight,\n            _weight_from, type='dispatcher', description=description,\n            wait_inputs=False, await_domain=await_domain, **kwargs\n        )\n\n        # Set proper inputs.\n        self.nodes[dsp_id]['inputs'] = inputs\n\n        # Set proper outputs.\n        self.nodes[dsp_id]['outputs'] = outputs\n\n        if SINK not in dsp.nodes and \\\n                SINK in _nodes(inputs.values()).union(_nodes(outputs)):\n            dsp.add_data(SINK)  # Add sink node.\n\n        # Import default values from sub-dispatcher.\n        if include_defaults:\n            dsp_dfl = dsp.default_values  # Namespace shortcut.\n\n            remove = set()  # Set of nodes to remove after the import.\n\n            # Set default values.\n            for k, v in inputs.items():\n                if isinstance(v, str):\n                    if v in dsp_dfl:\n                        self.set_default_value(k, **dsp_dfl.pop(v))\n                else:\n                    if v[0] in dsp_dfl:\n                        self.set_default_value(k, **dsp_dfl.pop(v[0]))\n                        remove.update(v[1:])\n\n            # Remove default values.\n            for k in remove:\n                dsp_dfl.pop(k, None)\n\n        return dsp_id", "response": "Adds a single sub - dispatcher node to the dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd multiple function and data nodes to dispatcher.", "response": "def add_from_lists(self, data_list=None, fun_list=None, dsp_list=None):\n        \"\"\"\n        Add multiple function and data nodes to dispatcher.\n\n        :param data_list:\n            It is a list of data node kwargs to be loaded.\n        :type data_list: list[dict], optional\n\n        :param fun_list:\n            It is a list of function node kwargs to be loaded.\n        :type fun_list: list[dict], optional\n\n        :param dsp_list:\n            It is a list of sub-dispatcher node kwargs to be loaded.\n        :type dsp_list: list[dict], optional\n\n        :returns:\n\n            - Data node ids.\n            - Function node ids.\n            - Sub-dispatcher node ids.\n        :rtype: (list[str], list[str], list[str])\n\n        .. seealso:: :func:`add_data`, :func:`add_func`, :func:`add_function`,\n           :func:`add_dispatcher`\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        .. testsetup::\n            >>> dsp = Dispatcher(name='Dispatcher')\n\n        Define a data list::\n\n            >>> data_list = [\n            ...     {'data_id': 'a'},\n            ...     {'data_id': 'b'},\n            ...     {'data_id': 'c'},\n            ... ]\n\n        Define a functions list::\n\n            >>> def func(a, b):\n            ...     return a + b\n            ...\n            >>> fun_list = [\n            ...     {'function': func, 'inputs': ['a', 'b'], 'outputs': ['c']}\n            ... ]\n\n        Define a sub-dispatchers list::\n\n            >>> sub_dsp = Dispatcher(name='Sub-dispatcher')\n            >>> sub_dsp.add_function(function=func, inputs=['e', 'f'],\n            ...                      outputs=['g'])\n            'func'\n            >>>\n            >>> dsp_list = [\n            ...     {'dsp_id': 'Sub', 'dsp': sub_dsp,\n            ...      'inputs': {'a': 'e', 'b': 'f'}, 'outputs': {'g': 'c'}},\n            ... ]\n\n        Add function and data nodes to dispatcher::\n\n            >>> dsp.add_from_lists(data_list, fun_list, dsp_list)\n            (['a', 'b', 'c'], ['func'], ['Sub'])\n        \"\"\"\n\n        if data_list:  # Add data nodes.\n            data_ids = [self.add_data(**v) for v in data_list]  # Data ids.\n        else:\n            data_ids = []\n\n        if fun_list:  # Add function nodes.\n            fun_ids = [self.add_function(**v) for v in fun_list]  # Func ids.\n        else:\n            fun_ids = []\n\n        if dsp_list:  # Add dispatcher nodes.\n            dsp_ids = [self.add_dispatcher(**v) for v in dsp_list]  # Dsp ids.\n        else:\n            dsp_ids = []\n\n        # Return data, function, and sub-dispatcher node ids.\n        return data_ids, fun_ids, dsp_ids"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_default_value(self, data_id, value=EMPTY, initial_dist=0.0):\n\n        try:\n            if self.dmap.nodes[data_id]['type'] == 'data':  # Is data node?\n                if value is EMPTY:\n                    self.default_values.pop(data_id, None)  # Remove default.\n                else:  # Add default.\n                    self.default_values[data_id] = {\n                        'value': value,\n                        'initial_dist': initial_dist\n                    }\n                return\n        except KeyError:\n            pass\n        raise ValueError('Input error: %s is not a data node' % data_id)", "response": "Set the default value of a data node in the dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sub_dsp(self, nodes_bunch, edges_bunch=None):\n\n        # Get real paths.\n        nodes_bunch = [self.get_node(u)[1][0] for u in nodes_bunch]\n\n        # Define an empty dispatcher.\n        sub_dsp = self.copy_structure(\n            dmap=self.dmap.subgraph(nodes_bunch).copy()\n        )\n\n        # Namespace shortcuts for speed.\n        nodes, dmap_out_degree = sub_dsp.nodes, sub_dsp.dmap.out_degree\n        dmap_dv, dmap_rm_edge = self.default_values, sub_dsp.dmap.remove_edge\n        dmap_rm_node = sub_dsp.dmap.remove_node\n\n        # Remove function nodes that has not whole inputs available.\n        for u in nodes_bunch:\n            n = nodes[u].get('inputs', None)  # Function inputs.\n            # No all inputs\n            if n is not None and not set(n).issubset(nodes_bunch):\n                dmap_rm_node(u)  # Remove function node.\n\n        # Remove edges that are not in edges_bunch.\n        if edges_bunch is not None:\n            for e in edges_bunch:  # Iterate sub-graph edges.\n                dmap_rm_edge(*e)  # Remove edge.\n\n        # Remove function node with no outputs.\n        for u in [u for u, n in sub_dsp.dmap.nodes.items()\n                  if n['type'] == 'function']:\n\n            # noinspection PyCallingNonCallable\n            if not dmap_out_degree(u):  # No outputs.\n                dmap_rm_node(u)  # Remove function node.\n\n        from networkx import isolates\n        # Remove isolate nodes from sub-graph.\n        sub_dsp.dmap.remove_nodes_from(list(isolates(sub_dsp.dmap)))\n\n        # Set default values.\n        sub_dsp.default_values = {k: dmap_dv[k] for k in dmap_dv if k in nodes}\n\n        return sub_dsp", "response": "Returns the sub - dispatcher for given node and edge bunches."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sub_dsp_from_workflow(\n            self, sources, graph=None, reverse=False, add_missing=False,\n            check_inputs=True, blockers=None, wildcard=False,\n            _update_links=True):\n        \"\"\"\n        Returns the sub-dispatcher induced by the workflow from sources.\n\n        The induced sub-dispatcher of the dsp contains the reachable nodes and\n        edges evaluated with breadth-first-search on the workflow graph from\n        source nodes.\n\n        :param sources:\n            Source nodes for the breadth-first-search.\n            A container of nodes which will be iterated through once.\n        :type sources: list[str], iterable\n\n        :param graph:\n            A directed graph where evaluate the breadth-first-search.\n        :type graph: networkx.DiGraph, optional\n\n        :param reverse:\n            If True the workflow graph is assumed as reversed.\n        :type reverse: bool, optional\n\n        :param add_missing:\n            If True, missing function' inputs are added to the sub-dispatcher.\n        :type add_missing: bool, optional\n\n        :param check_inputs:\n            If True the missing function' inputs are not checked.\n        :type check_inputs: bool, optional\n\n        :param blockers:\n            Nodes to not be added to the queue.\n        :type blockers: set[str], iterable, optional\n\n        :param wildcard:\n            If True, when the data node is used as input and target in the\n            ArciDispatch algorithm, the input value will be used as input for\n            the connected functions, but not as output.\n        :type wildcard: bool, optional\n\n        :param _update_links:\n            If True, it updates remote links of the extracted dispatcher.\n        :type _update_links: bool, optional\n\n        :return:\n            A sub-dispatcher.\n        :rtype: Dispatcher\n\n        .. seealso:: :func:`get_sub_dsp`\n\n        .. note::\n\n            The sub-dispatcher edge or node attributes just point to the\n            original dispatcher. So changes to the node or edge structure\n            will not be reflected in the original dispatcher map while changes\n            to the attributes will.\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        A dispatcher with a function `fun` and a node `a` with a default value:\n\n        .. dispatcher:: dsp\n           :opt: graph_attr={'ratio': '1'}\n\n            >>> dsp = Dispatcher(name='Dispatcher')\n            >>> dsp.add_data(data_id='a', default_value=1)\n            'a'\n            >>> dsp.add_function(function_id='fun1', inputs=['a', 'b'],\n            ...                  outputs=['c', 'd'])\n            'fun1'\n            >>> dsp.add_function(function_id='fun2', inputs=['e'],\n            ...                  outputs=['c'])\n            'fun2'\n\n        Dispatch with no calls in order to have a workflow::\n\n            >>> o = dsp.dispatch(inputs=['a', 'b'], no_call=True)\n\n        Get sub-dispatcher from workflow inputs `a` and `b`::\n\n            >>> sub_dsp = dsp.get_sub_dsp_from_workflow(['a', 'b'])\n\n        .. dispatcher:: sub_dsp\n           :opt: graph_attr={'ratio': '1'}\n\n            >>> sub_dsp.name = 'Sub-Dispatcher'\n\n        Get sub-dispatcher from a workflow output `c`::\n\n            >>> sub_dsp = dsp.get_sub_dsp_from_workflow(['c'], reverse=True)\n\n        .. dispatcher:: sub_dsp\n           :opt: graph_attr={'ratio': '1'}\n\n            >>> sub_dsp.name = 'Sub-Dispatcher (reverse workflow)'\n        \"\"\"\n\n        # Define an empty dispatcher map.\n        sub_dsp = self.copy_structure()\n\n        if not graph:  # Set default graph.\n            graph = self.solution.workflow\n\n        # Visited nodes used as queue.\n        family = {}\n\n        # Namespace shortcuts for speed.\n        nodes, dmap_nodes = sub_dsp.dmap._node, self.dmap.nodes\n        dlt_val, dsp_dlt_val = sub_dsp.default_values, self.default_values\n\n        if not reverse:\n            # Namespace shortcuts for speed.\n            neighbors, dmap_succ = graph.neighbors, self.dmap.succ\n            succ, pred = sub_dsp.dmap._succ, sub_dsp.dmap._pred\n\n            # noinspection PyUnusedLocal\n            def _check_node_inputs(c, p):\n                if c == START:\n                    return True\n\n                node_attr = dmap_nodes[c]\n\n                if node_attr['type'] == 'function':\n                    if set(node_attr['inputs']).issubset(family):\n                        _set_node_attr(c)\n\n                        # namespace shortcuts for speed\n                        s_pred = pred[c]\n\n                        for p in node_attr['inputs']:\n                            # add attributes to both representations of edge\n                            succ[p][c] = s_pred[p] = dmap_succ[p][c]\n                    elif not check_inputs or add_missing:\n                        _set_node_attr(c)\n\n                        # namespace shortcuts for speed\n                        s_pred = pred[c]\n\n                        if add_missing:\n                            for p in node_attr['inputs']:\n                                if p not in family:\n                                    _set_node_attr(p, add2family=False)\n                                    succ[p][c] = s_pred[p] = dmap_succ[p][c]\n\n                        for p in set(node_attr['inputs']).intersection(family):\n                            # add attributes to both representations of edge\n                            succ[p][c] = s_pred[p] = dmap_succ[p][c]\n                        return False\n\n                    return True\n\n                return False\n\n        else:\n            # Namespace shortcuts for speed.\n            neighbors, dmap_succ = graph.predecessors, self.dmap.pred\n            pred, succ = sub_dsp.dmap._succ, sub_dsp.dmap._pred\n\n            def _check_node_inputs(c, p):\n                if c == START:\n                    try:\n                        node_attr = dmap_nodes[p]\n                        return node_attr['type'] == 'data'\n                    except KeyError:\n                        return True\n                return False\n\n        from collections import deque\n        queue = deque([])\n\n        blockers = set(blockers or ())\n\n        # Function to set node attributes.\n        def _set_node_attr(n, add2family=True, block=False):\n            # Set node attributes.\n            nodes[n] = dmap_nodes[n]\n\n            # Add node in the adjacency matrix.\n            succ[n], pred[n] = ({}, {})\n\n            if n in dsp_dlt_val:\n                dlt_val[n] = dsp_dlt_val[n]  # Set the default value.\n\n            if add2family:\n                # Append a new parent to the family.\n                family[n] = () if block and n in blockers else neighbors(n)\n\n                queue.append(n)\n\n        # Set initial node attributes.\n        for s in sources:\n            if s in dmap_nodes and s in graph.nodes:\n                _set_node_attr(s, block=not (wildcard and s in blockers))\n\n        # Start breadth-first-search.\n        while queue:\n            parent = queue.popleft()\n\n            # Namespace shortcuts for speed.\n            nbrs, dmap_nbrs = succ[parent], dmap_succ[parent]\n\n            # Iterate parent's children.\n            for child in sorted(family[parent]):\n\n                if _check_node_inputs(child, parent):\n                    continue\n\n                if child not in family:\n                    _set_node_attr(child, block=True)  # Set node attributes.\n\n                # Add attributes to both representations of edge: u-v and v-u.\n                nbrs[child] = pred[child][parent] = dmap_nbrs[child]\n\n        if _update_links:\n            from .utils.alg import _update_io, _get_sub_out, _get_sub_inp\n            succ, pred = sub_dsp.dmap.succ, sub_dsp.dmap.pred\n            for k, a in sub_dsp.sub_dsp_nodes.items():\n                nodes[k] = a = a.copy()\n\n                inp, out = _get_sub_inp(a, pred[k]), _get_sub_out(a, succ[k])\n\n                a['function'] = a['function'].get_sub_dsp_from_workflow(\n                    sources=out.union(inp), graph=a['function'].dmap,\n                    reverse=True, blockers=inp, wildcard=True\n                )\n\n                i, o = _update_io(a, pred[k], succ[k])  # Unreachable nodes.\n                msg = 'Sub-dsp {} missing: inp {}, out {}'\n                assert not i and not o, msg.format(k, i, o)\n        return sub_dsp", "response": "This method returns the sub - dispatcher induced by the workflow graph from the given sources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all data nodes of the dispatcher.", "response": "def data_nodes(self):\n        \"\"\"\n        Returns all data nodes of the dispatcher.\n\n        :return:\n            All data nodes of the dispatcher.\n        :rtype: dict[str, dict]\n        \"\"\"\n\n        return {k: v for k, v in self.nodes.items() if v['type'] == 'data'}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef function_nodes(self):\n\n        return {k: v for k, v in self.nodes.items() if v['type'] == 'function'}", "response": "Returns all function nodes of the dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all sub - dispatcher nodes of the dispatcher.", "response": "def sub_dsp_nodes(self):\n        \"\"\"\n        Returns all sub-dispatcher nodes of the dispatcher.\n\n        :return:\n            All sub-dispatcher nodes of the dispatcher.\n        :rtype: dict[str, dict]\n        \"\"\"\n\n        return {k: v for k, v in self.nodes.items() if\n                v['type'] == 'dispatcher'}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a BlueDispatcher out of the current object.", "response": "def blue(self, memo=None):\n        \"\"\"\n        Constructs a BlueDispatcher out of the current object.\n\n        :param memo:\n            A dictionary to cache Blueprints.\n        :type memo: dict[T,schedula.utils.blue.Blueprint]\n\n        :return:\n            A BlueDispatcher of the current object.\n        :rtype: schedula.utils.blue.BlueDispatcher\n        \"\"\"\n        memo = {} if memo is None else memo\n        if self in memo:\n            return memo[self]\n        from .utils.dsp import map_list\n        from .utils.blue import BlueDispatcher, _parent_blue\n        memo[self] = blue = BlueDispatcher(\n            executor=self.executor, name=self.name, raises=self.raises,\n            description=self.__doc__\n        )\n        dfl = self.default_values\n        key_map_data = ['data_id', {'value': 'default_value'}]\n        pred, succ = self.dmap.pred, self.dmap.succ\n\n        def _set_weight(n, r, d):\n            d = {i: j['weight'] for i, j in d.items() if 'weight' in j}\n            if d:\n                r[n] = d\n\n        for k, v in sorted(self.nodes.items(), key=lambda x: x[1]['index']):\n            v = v.copy()\n            t = v.pop('type')\n            del v['index']\n            if t == 'data':\n                method = 'add_data'\n                combine_dicts(map_list(key_map_data, k, dfl.get(k, {})), base=v)\n            elif t in ('function', 'dispatcher'):\n                method = 'add_%s' % t\n                if t == 'dispatcher':\n                    t = 'dsp'\n                v['%s_id' % t] = k\n                del v['wait_inputs']\n                _set_weight('inp_weight', v, pred[k])\n                _set_weight('out_weight', v, succ[k])\n                if 'function' in v:\n                    v[t] = _parent_blue(v.pop('function'), memo)\n            blue.deferred.append((method, v))\n        return blue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextending Dispatcher calling each deferred operation of given Blueprints or Dispatchers.", "response": "def extend(self, *blues, memo=None):\n        \"\"\"\n        Extends Dispatcher calling each deferred operation of given Blueprints.\n\n        :param blues:\n            Blueprints or Dispatchers to extend deferred operations.\n        :type blues: Blueprint | schedula.dispatcher.Dispatcher\n\n        :param memo:\n            A dictionary to cache Blueprints and Dispatchers.\n        :type memo: dict[T,schedula.utils.blue.Blueprint|Dispatcher]\n\n        :return:\n            Self.\n        :rtype: Dispatcher\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        .. dispatcher:: dsp\n           :opt: graph_attr={'ratio': '1'}\n           :code:\n\n            >>> import schedula as sh\n            >>> dsp = sh.Dispatcher()\n            >>> dsp.add_func(callable, ['is_callable'])\n            'callable'\n            >>> blue = sh.BlueDispatcher().add_func(len, ['length'])\n            >>> dsp = sh.Dispatcher().extend(dsp, blue)\n        \"\"\"\n        from .utils.blue import BlueDispatcher as Blue\n        return Blue().extend(*blues, memo=memo).register(self, memo=memo)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dispatch(self, inputs=None, outputs=None, cutoff=None, inputs_dist=None,\n                 wildcard=False, no_call=False, shrink=False,\n                 rm_unused_nds=False, select_output_kw=None, _wait_in=None,\n                 stopper=None, executor=False, sol_name=()):\n        \"\"\"\n        Evaluates the minimum workflow and data outputs of the dispatcher\n        model from given inputs.\n\n        :param inputs:\n            Input data values.\n        :type inputs: dict[str, T], list[str], iterable, optional\n\n        :param outputs:\n            Ending data nodes.\n        :type outputs: list[str], iterable, optional\n\n        :param cutoff:\n            Depth to stop the search.\n        :type cutoff: float, int, optional\n\n        :param inputs_dist:\n            Initial distances of input data nodes.\n        :type inputs_dist: dict[str, int | float], optional\n\n        :param wildcard:\n            If True, when the data node is used as input and target in the\n            ArciDispatch algorithm, the input value will be used as input for\n            the connected functions, but not as output.\n        :type wildcard: bool, optional\n\n        :param no_call:\n            If True data node estimation function is not used and the input\n            values are not used.\n        :type no_call: bool, optional\n\n        :param shrink:\n            If True the dispatcher is shrink before the dispatch.\n\n            .. seealso:: :func:`shrink_dsp`\n        :type shrink: bool, optional\n\n        :param rm_unused_nds:\n            If True unused function and sub-dispatcher nodes are removed from\n            workflow.\n        :type rm_unused_nds: bool, optional\n\n        :param select_output_kw:\n            Kwargs of selector function to select specific outputs.\n        :type select_output_kw: dict, optional\n\n        :param _wait_in:\n            Override wait inputs.\n        :type _wait_in: dict, optional\n\n        :param stopper:\n            A semaphore to abort the dispatching.\n        :type stopper: multiprocess.Event, optional\n\n        :param executor:\n            A pool executor id to dispatch asynchronously or in parallel.\n        :type executor: str, optional\n\n        :param sol_name:\n            Solution name.\n        :type sol_name: tuple[str], optional\n\n        :return:\n            Dictionary of estimated data node outputs.\n        :rtype: schedula.utils.sol.Solution\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        A dispatcher with a function :math:`log(b - a)` and two data `a` and `b`\n        with default values:\n\n        .. dispatcher:: dsp\n           :opt: graph_attr={'ratio': '1'}\n\n            >>> dsp = Dispatcher(name='Dispatcher')\n            >>> dsp.add_data(data_id='a', default_value=0)\n            'a'\n            >>> dsp.add_data(data_id='b', default_value=5)\n            'b'\n            >>> dsp.add_data(data_id='d', default_value=1)\n            'd'\n            >>> from math import log\n            >>> def my_log(a, b):\n            ...     return log(b - a)\n            >>> def my_domain(a, b):\n            ...     return a < b\n            >>> dsp.add_function('log(b - a)', function=my_log,\n            ...                  inputs=['c', 'd'],\n            ...                  outputs=['e'], input_domain=my_domain)\n            'log(b - a)'\n            >>> dsp.add_function('min', function=min, inputs=['a', 'b'],\n            ...                  outputs=['c'])\n            'min'\n\n        Dispatch without inputs. The default values are used as inputs:\n\n        .. dispatcher:: outputs\n           :opt: graph_attr={'ratio': '1'}\n           :code:\n\n            >>> outputs = dsp.dispatch()\n            >>> outputs\n            Solution([('a', 0), ('b', 5), ('d', 1), ('c', 0), ('e', 0.0)])\n\n        Dispatch until data node `c` is estimated:\n\n        .. dispatcher:: outputs\n           :opt: graph_attr={'ratio': '1'}\n           :code:\n\n            >>> outputs = dsp.dispatch(outputs=['c'])\n            >>> outputs\n            Solution([('a', 0), ('b', 5), ('c', 0)])\n\n        Dispatch with one inputs. The default value of `a` is not used as\n        inputs:\n\n        .. dispatcher:: outputs\n           :opt: graph_attr={'ratio': '1'}\n           :code:\n\n            >>> outputs = dsp.dispatch(inputs={'a': 3})\n            >>> outputs\n            Solution([('a', 3), ('b', 5), ('d', 1), ('c', 3)])\n        \"\"\"\n\n        dsp = self\n\n        if not no_call:\n            if shrink:  # Pre shrink.\n                dsp = self.shrink_dsp(\n                    inputs, outputs, cutoff, inputs_dist, wildcard\n                )\n            elif outputs:\n                dsp = self.get_sub_dsp_from_workflow(\n                    outputs, self.dmap, reverse=True, blockers=inputs,\n                    wildcard=wildcard\n                )\n\n        # Initialize.\n        self.solution = sol = self.solution.__class__(\n            dsp, inputs, outputs, wildcard, cutoff, inputs_dist, no_call,\n            rm_unused_nds, _wait_in, full_name=sol_name\n        )\n\n        # Dispatch.\n        sol._run(stopper=stopper, executor=executor)\n\n        if select_output_kw:\n            return selector(dictionary=sol, **select_output_kw)\n\n        # Return the evaluated data outputs.\n        return sol", "response": "Evaluate the dispatcher for the minimum workflow and data outputs of the data node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shrink_dsp(self, inputs=None, outputs=None, cutoff=None,\n                   inputs_dist=None, wildcard=True):\n        \"\"\"\n        Returns a reduced dispatcher.\n\n        :param inputs:\n            Input data nodes.\n        :type inputs: list[str], iterable, optional\n\n        :param outputs:\n            Ending data nodes.\n        :type outputs: list[str], iterable, optional\n\n        :param cutoff:\n            Depth to stop the search.\n        :type cutoff: float, int, optional\n\n        :param inputs_dist:\n            Initial distances of input data nodes.\n        :type inputs_dist: dict[str, int | float], optional\n\n        :param wildcard:\n            If True, when the data node is used as input and target in the\n            ArciDispatch algorithm, the input value will be used as input for\n            the connected functions, but not as output.\n        :type wildcard: bool, optional\n\n        :return:\n            A sub-dispatcher.\n        :rtype: Dispatcher\n\n        .. seealso:: :func:`dispatch`\n\n        **--------------------------------------------------------------------**\n\n        **Example**:\n\n        A dispatcher like this:\n\n        .. dispatcher:: dsp\n           :opt: graph_attr={'ratio': '1'}\n\n            >>> dsp = Dispatcher(name='Dispatcher')\n            >>> functions = [\n            ...     {\n            ...         'function_id': 'fun1',\n            ...         'inputs': ['a', 'b'],\n            ...         'outputs': ['c']\n            ...     },\n            ...     {\n            ...         'function_id': 'fun2',\n            ...         'inputs': ['b', 'd'],\n            ...         'outputs': ['e']\n            ...     },\n            ...     {\n            ...         'function_id': 'fun3',\n            ...         'function': min,\n            ...         'inputs': ['d', 'f'],\n            ...         'outputs': ['g']\n            ...     },\n            ...     {\n            ...         'function_id': 'fun4',\n            ...         'function': max,\n            ...         'inputs': ['a', 'b'],\n            ...         'outputs': ['g']\n            ...     },\n            ...     {\n            ...         'function_id': 'fun5',\n            ...         'function': max,\n            ...         'inputs': ['d', 'e'],\n            ...         'outputs': ['c', 'f']\n            ...     },\n            ... ]\n            >>> dsp.add_from_lists(fun_list=functions)\n            ([], [...])\n\n        Get the sub-dispatcher induced by dispatching with no calls from inputs\n        `a`, `b`, and `c` to outputs `c`, `e`, and `f`::\n\n            >>> shrink_dsp = dsp.shrink_dsp(inputs=['a', 'b', 'd'],\n            ...                             outputs=['c', 'f'])\n\n        .. dispatcher:: shrink_dsp\n           :opt: graph_attr={'ratio': '1'}\n\n            >>> shrink_dsp.name = 'Sub-Dispatcher'\n        \"\"\"\n\n        bfs = None\n        if inputs:\n            # Get all data nodes no wait inputs.\n            wait_in = self._get_wait_in(flag=False)\n\n            # Evaluate the workflow graph without invoking functions.\n            o = self.dispatch(\n                inputs, outputs, cutoff, inputs_dist, wildcard, True, False,\n                True, _wait_in=wait_in\n            )\n\n            data_nodes = self.data_nodes  # Get data nodes.\n\n            from .utils.alg import _union_workflow, _convert_bfs\n            bfs = _union_workflow(o)  # bfg edges.\n\n            # Set minimum initial distances.\n            if inputs_dist:\n                inputs_dist = combine_dicts(o.dist, inputs_dist)\n            else:\n                inputs_dist = o.dist\n\n            # Set data nodes to wait inputs.\n            wait_in = self._get_wait_in(flag=True)\n\n            while True:  # Start shrinking loop.\n                # Evaluate the workflow graph without invoking functions.\n                o = self.dispatch(\n                    inputs, outputs, cutoff, inputs_dist, wildcard, True, False,\n                    False, _wait_in=wait_in\n                )\n\n                _union_workflow(o, bfs=bfs)  # Update bfs.\n\n                n_d, status = o._remove_wait_in()  # Remove wait input flags.\n\n                if not status:\n                    break  # Stop iteration.\n\n                # Update inputs.\n                inputs = n_d.intersection(data_nodes).union(inputs)\n\n            # Update outputs and convert bfs in DiGraphs.\n            outputs, bfs = outputs or o, _convert_bfs(bfs)\n\n        elif not outputs:\n            return self.copy_structure()  # Empty Dispatcher.\n\n        # Get sub dispatcher breadth-first-search graph.\n        dsp = self._get_dsp_from_bfs(outputs, bfs_graphs=bfs)\n\n        return dsp", "response": "Returns a reduced dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the sub - dispatcher induced by the workflow from outputs.", "response": "def _get_dsp_from_bfs(self, outputs, bfs_graphs=None):\n        \"\"\"\n        Returns the sub-dispatcher induced by the workflow from outputs.\n\n        :param outputs:\n            Ending data nodes.\n        :type outputs: list[str], iterable, optional\n\n        :param bfs_graphs:\n            A dictionary with directed graphs where evaluate the\n            breadth-first-search.\n        :type bfs_graphs: dict[str | Token, networkx.DiGraph | dict], optional\n\n        :return:\n            A sub-dispatcher\n        :rtype: Dispatcher\n        \"\"\"\n\n        bfs = bfs_graphs[NONE] if bfs_graphs is not None else self.dmap\n\n        # Get sub dispatcher breadth-first-search graph.\n        dsp = self.get_sub_dsp_from_workflow(\n            sources=outputs, graph=bfs, reverse=True, _update_links=False\n        )\n\n        # Namespace shortcuts.\n        succ, nodes, pred = dsp.dmap.succ, dsp.nodes, dsp.dmap.pred\n        rm_edges, nds = dsp.dmap.remove_edges_from, dsp.data_nodes\n        from .utils.alg import _nodes, _get_sub_out, _update_io\n\n        for n in dsp.sub_dsp_nodes:\n            a = nodes[n] = nodes[n].copy()\n            bfs = bfs_graphs[n] if bfs_graphs is not None else None\n\n            out = _get_sub_out(a, succ[n])\n            if 'input_domain' in a:\n                out.update(_nodes(a['inputs'].values()))\n\n            a['function'] = a['function']._get_dsp_from_bfs(out, bfs)\n\n            i, o = _update_io(a, pred[n], succ[n])  # Unreachable nodes.\n            rm_edges({(u, n) for u in i}.union(((n, u) for u in o)))\n\n        return dsp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_wait_in(self, flag=True, all_domain=True):\n\n        wait_in = {}\n\n        for n, a in self.data_nodes.items():\n            if n is not SINK and a['wait_inputs']:\n                wait_in[n] = flag\n\n        if all_domain:\n            for a in self.function_nodes.values():\n                if 'input_domain' in a:\n                    wait_in.update(dict.fromkeys(a['outputs'], flag))\n\n            for n, a in self.sub_dsp_nodes.items():\n                if 'function' in a:\n                    dsp = a['function']\n                    wait_in[dsp] = w = dsp._get_wait_in(flag=flag)\n                    if 'input_domain' not in a:\n                        o = a['outputs']\n                        w = [o[k] for k in set(o).intersection(w)]\n                        wait_in.update(dict.fromkeys(w, flag))\n\n                if 'input_domain' in a:\n                    wait_in[n] = flag\n                    wait_in.update(dict.fromkeys(a['outputs'].values(), flag))\n\n        return wait_in", "response": "Get the wait_in dictionary for all data nodes that are not SINK and have a domain function and that are waiting inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a callback to the current object.", "response": "def add_callback(self, method):\n        \"\"\"\n        Attaches a mehtod that will be called when the future finishes.\n\n        :param method: A callable from an actor that will be called\n            when the future completes. The only argument for that\n            method must be the future itself from wich you can get the\n            result though `future.:meth:`result()``. If the future has\n            already completed, then the callable will be called\n            immediately.\n\n        .. note:: This functionallity only works when called from an actor,\n            specifying a method from the same actor.\n        \"\"\"\n        from_actor = get_current()\n        if from_actor is not None:\n            callback = (method, from_actor.channel, from_actor.url)\n            with self.__condition:\n                if self.__state is not FINISHED:\n                    self.__callbacks.append(callback)\n                    return\n            # Invoke the callback directly\n            # msg = TellRequest(TELL, method, [self], from_actor.url)\n            msg = {TYPE: TELL, METHOD: method, PARAMS: ([self], {}),\n                   TO: from_actor.url}\n            from_actor.channel.send(msg)\n        else:\n            raise FutureError(\"add_callback only works when called \" +\n                              \"from inside an actor\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the result of the call that the future represents.", "response": "def result(self, timeout=None):\n        \"\"\"Returns the result of the call that the future represents.\n\n        :param timeout: The number of seconds to wait for the result\n            if the future has not been completed. None, the default,\n            sets no limit.\n        :returns: The result of the call that the future represents.\n        :raises: TimeoutError: If the timeout is reached before the\n            future ends execution.\n        :raises: Exception: If the call raises the Exception.\n        \"\"\"\n        with self.__condition:\n            if self.__state == FINISHED:\n                return self.__get__result()\n\n            lock = get_lock()\n            if lock is not None:\n                lock.release()\n            self.__condition.wait(timeout)\n            if lock is not None:\n                lock.acquire()\n\n            if self.__state == FINISHED:\n                return self.__get__result()\n            else:\n                raise TimeoutError('Future: %r' % self.__method)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exception(self, timeout=None):\n        with self.__condition:\n            if self.__state == FINISHED:\n                return self.__exception\n\n            lock = get_lock()\n            if lock is not None:\n                lock.release()\n            self.__condition.wait(timeout)\n            if lock is not None:\n                lock.acquire()\n\n            if self.__state == FINISHED:\n                return self.__exception\n            else:\n                raise TimeoutError('Future: %r' % self.__method)", "response": "Return a exception raised by the future that the future has not completed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_work(self):\n        '''Sends the query to the actor for it to start executing the\n        work.\n\n        It is possible to execute once again a future that has finished\n        if necessary (overwriting the results), but only one execution\n        at a time.\n        '''\n        if self.__set_running():\n            # msg = FutureRequest(FUTURE, self.__method, self.__params,\n            #                     self.__channel, self.__target, self.__id)\n            msg = {TYPE: FUTURE, METHOD: self.__method, PARAMS: self.__params,\n                   CHANNEL: self.__channel, TO: self.__target,\n                   RPC_ID: self.__id}\n            self.__actor_channel.send(msg)\n        else:\n            raise FutureError(\"Future already running.\")", "response": "Sends the work to the actor for it to start executing the\n        work."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the result of the work associated with the future.", "response": "def set_result(self, result):\n        \"\"\"Sets the return value of work associated with the future.\n        Only called internally.\n        \"\"\"\n        with self.__condition:\n            self.__result = result\n            self.__state = FINISHED\n            self.__condition.notify_all()\n        self._invoke_callbacks()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the result of the future as being the given exception.", "response": "def set_exception(self, exception):\n        \"\"\"Sets the result of the future as being the given exception.\n        Only called internally.\n        \"\"\"\n        with self.__condition:\n            self.__exception = exception\n            self.__state = FINISHED\n            self.__condition.notify_all()\n        self._invoke_callbacks()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef result(self, timeout=None):\n        result = super(FutureRef, self).result(timeout)\n        return get_host().loads(result)", "response": "Returns the result of the call that the future represents."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the angle between vectors x and y", "response": "def angle_between_vectors(x, y):\n    \"\"\" Compute the angle between vector x and y \"\"\"\n    dp = dot_product(x, y)\n    if dp == 0:\n        return 0\n    xm = magnitude(x)\n    ym = magnitude(y)\n    return math.acos(dp / (xm*ym)) * (180. / math.pi)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef later(timeout, f, *args, **kwargs):\n    '''\n    Sets a timer that will call the *f* function past *timeout* seconds.\n\n    See example in :ref:`sample_inter`\n\n    :return: :class:`Timer`\n    '''\n    t = Timer(timeout, f, args, kwargs)\n    t.start()\n    return t", "response": "Sets a timer that will call the f function past timeout seconds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef interval_host(host, time, f, *args, **kwargs):\n    '''\n    Creates an Event attached to the *host* that will execute the *f*\n    function every *time* seconds.\n\n    See example in :ref:`sample_inter`\n\n    :param Proxy host: proxy of the host. Can be obtained from inside a\n        class with ``self.host``.\n    :param int time: seconds for the intervals.\n    :param func f: function to be called every *time* seconds.\n    :param list args: arguments for *f*.\n    :return: :class:`Event` instance of the interval.\n    '''\n    def wrap(*args, **kwargs):\n        thread = currentThread()\n        args = list(args)\n        stop_event = args[0]\n        del args[0]\n        args = tuple(args)\n        while not stop_event.is_set():\n            f(*args, **kwargs)\n            stop_event.wait(time)\n        host.detach_interval(thread_id)\n    t2_stop = Event()\n    args = list(args)\n    args.insert(0, t2_stop)\n    args = tuple(args)\n    t = Thread(target=wrap, args=args, kwargs=kwargs)\n    t.start()\n    thread_id = t.getName()\n    host.attach_interval(thread_id, t2_stop)\n    return t2_stop", "response": "A function that will execute the function f every time seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ssh_forward_accept(ssh_session, timeout_ms):\n\n    ssh_channel = c_ssh_forward_accept(c_void_p(ssh_session), \n                                       c_int(timeout_ms))\n\n    if ssh_channel is None:\n        raise SshTimeoutException()\n\n    return ssh_channel", "response": "Wait for an incoming connection from a forward forwarded port."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, cmd, block_size=DEFAULT_EXECUTE_READ_BLOCK_SIZE):\n    \n        with SshChannel(self) as sc:\n            self.__log.debug(\"Executing command: %s\" % (cmd))\n\n            sc.open_session()\n            sc.request_exec(cmd)\n\n            buffer_ = bytearray()\n            while 1:\n                bytes = sc.read(block_size)\n                yield bytes\n                \n                if len(bytes) < block_size:\n                    break", "response": "Execute a remote command and yield the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read(self, directory, filename, session, path, name, extension,\n              spatial, spatialReferenceID, replaceParamFile,\n              force_relative=True):\n        \"\"\"\n        Project File Read from File Method\n        \"\"\"\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Headers to ignore\n            HEADERS = ('GSSHAPROJECT',)\n\n            # WMS Cards to include (don't discount as comments)\n            WMS_CARDS = ('#INDEXGRID_GUID', '#PROJECTION_FILE', '#LandSoil',\n                         '#CHANNEL_POINT_INPUT_WMS')\n\n            GSSHAPY_CARDS = ('#GSSHAPY_EVENT_YML', )\n\n            with open(path, 'r') as f:\n                for line in f:\n                    if not line.strip():\n                        # Skip empty lines\n                        continue\n\n                    elif '#' in line.split()[0] and line.split()[0] \\\n                            not in WMS_CARDS + GSSHAPY_CARDS:\n                        # Skip comments designated by the hash symbol\n                        # (with the exception of WMS_CARDS and GSSHAPY_CARDS)\n                        continue\n\n                    try:\n                        card = self._extractCard(line, force_relative)\n\n                    except:\n                        card = self._extractDirectoryCard(line, force_relative)\n\n                    # Now that the cardName and cardValue are separated\n                    # load them into the gsshapy objects\n                    if card['name'] not in HEADERS:\n                        # Create GSSHAPY Project Card object\n                        prjCard = ProjectCard(name=card['name'], value=card['value'])\n\n                        # Associate ProjectCard with ProjectFile\n                        prjCard.projectFile = self\n\n                        # Extract MAP_TYPE card value for convenience working\n                        # with output maps\n                        if card['name'] == 'MAP_TYPE':\n                            self.mapType = int(card['value'])\n\n            # Assign properties\n            self.srid = spatialReferenceID\n            self.name = name\n            self.fileExtension = extension", "response": "Method to read from file and create GSSHAPY Project File"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write(self, session, openFile, replaceParamFile):\n        # Enforce cards that must be written in certain order\n        PRIORITY_CARDS = ('WMS', 'MASK_WATERSHED', 'REPLACE_LINE',\n                          'REPLACE_PARAMS', 'REPLACE_VALS', 'REPLACE_FOLDER')\n\n        filename = os.path.split(openFile.name)[1]\n        name = filename.split('.')[0]\n\n        # Write lines\n        openFile.write('GSSHAPROJECT\\n')\n\n        # Write priority lines\n        for card_key in PRIORITY_CARDS:\n            card = self.getCard(card_key)\n\n            # Write the card\n            if card is not None:\n                openFile.write(card.write(originalPrefix=self.name, newPrefix=name))\n\n        # Initiate write on each ProjectCard that belongs to this ProjectFile\n        for card in self.projectCards:\n            if card.name not in PRIORITY_CARDS:\n                openFile.write(card.write(originalPrefix=self.name, newPrefix=name))", "response": "Method to write to File Method to File Method"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending a directory to all the paths in the project file.", "response": "def appendDirectory(self, directory, projectFilePath):\n        \"\"\"\n        Append directory to relative paths in project file. By default, the project file paths are read and written as\n        relative paths. Use this method to prepend a directory to all the paths in the project file.\n\n        Args:\n            directory (str): Directory path to prepend to file paths in project file.\n            projectFilePath (str): Path to project file that will be modified.\n        \"\"\"\n        lines = []\n        with open(projectFilePath, 'r') as original:\n            for l in original:\n                lines.append(l)\n\n        with open(projectFilePath, 'w') as new:\n            for line in lines:\n                card = {}\n                try:\n                    card = self._extractCard(line)\n\n                except:\n                    card = self._extractDirectoryCard(line)\n\n                # Determine number of spaces between card and value for nice alignment\n                numSpaces = max(2, 25 - len(card['name']))\n\n                if card['value'] is None:\n                    rewriteLine = '%s\\n' % (card['name'])\n                else:\n                    if card['name'] == 'WMS':\n                        rewriteLine = '%s %s\\n' % (card['name'], card['value'])\n\n                    elif card['name'] == 'PROJECT_PATH':\n                        filePath = '\"%s\"' % os.path.normpath(directory)\n                        rewriteLine = '%s%s%s\\n' % (card['name'], ' ' * numSpaces, filePath)\n\n                    elif '\"' in card['value']:\n                        filename = card['value'].strip('\"')\n                        filePath = '\"%s\"' % os.path.join(directory, filename)\n                        rewriteLine = '%s%s%s\\n' % (card['name'], ' ' * numSpaces, filePath)\n\n                    else:\n                        rewriteLine = '%s%s%s\\n' % (card['name'], ' ' * numSpaces, card['value'])\n\n                new.write(rewriteLine)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef readProject(self, directory, projectFileName, session, spatial=False, spatialReferenceID=None):\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Add project file to session\n            session.add(self)\n\n            # First read self\n            self.read(directory, projectFileName, session, spatial=spatial, spatialReferenceID=spatialReferenceID)\n\n            # Get the batch directory for output\n            batchDirectory = self._getBatchDirectory(directory)\n\n            # Automatically derive the spatial reference system, if possible\n            if spatialReferenceID is None:\n                spatialReferenceID = self._automaticallyDeriveSpatialReferenceId(directory)\n\n            # Read in replace param file\n            replaceParamFile = self._readReplacementFiles(directory, session, spatial, spatialReferenceID)\n\n            # Read Input Files\n            self._readXput(self.INPUT_FILES, directory, session, spatial=spatial, spatialReferenceID=spatialReferenceID, replaceParamFile=replaceParamFile)\n\n            # Read Output Files\n            self._readXput(self.OUTPUT_FILES, batchDirectory, session, spatial=spatial, spatialReferenceID=spatialReferenceID, replaceParamFile=replaceParamFile)\n\n            # Read Input Map Files\n            self._readXputMaps(self.INPUT_MAPS, directory, session, spatial=spatial, spatialReferenceID=spatialReferenceID, replaceParamFile=replaceParamFile)\n\n            # Read WMS Dataset Files\n            self._readWMSDatasets(self.WMS_DATASETS, batchDirectory, session, spatial=spatial, spatialReferenceID=spatialReferenceID)\n\n            # Commit to database\n            self._commit(session, self.COMMIT_ERROR_MESSAGE)", "response": "Reads all files for a GSSHA project into the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread all input files for a GSSHA project into the database.", "response": "def readInput(self, directory, projectFileName, session, spatial=False, spatialReferenceID=None):\n        \"\"\"\n        Read only input files for a GSSHA project into the database.\n\n        Use this method to read a project when only pre-processing tasks need to be performed.\n\n        Args:\n            directory (str): Directory containing all GSSHA model files. This method assumes that all files are located\n                in the same directory.\n            projectFileName (str): Name of the project file for the GSSHA model which will be read (e.g.: 'example.prj').\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database\n            spatial (bool, optional): If True, spatially enabled objects will be read in as PostGIS spatial objects.\n                Defaults to False.\n            spatialReferenceID (int, optional): Integer id of spatial reference system for the model. If no id is\n                provided GsshaPy will attempt to automatically lookup the spatial reference ID. If this process fails,\n                default srid will be used (4326 for WGS 84).\n        \"\"\"\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Add project file to session\n            session.add(self)\n\n            # Read Project File\n            self.read(directory, projectFileName, session, spatial, spatialReferenceID)\n\n            # Automatically derive the spatial reference system, if possible\n            if spatialReferenceID is None:\n                spatialReferenceID = self._automaticallyDeriveSpatialReferenceId(directory)\n\n            # Read in replace param file\n            replaceParamFile = self._readReplacementFiles(directory, session, spatial, spatialReferenceID)\n\n            # Read Input Files\n            self._readXput(self.INPUT_FILES, directory, session, spatial=spatial, spatialReferenceID=spatialReferenceID, replaceParamFile=replaceParamFile)\n\n            # Read Input Map Files\n            self._readXputMaps(self.INPUT_MAPS, directory, session, spatial=spatial, spatialReferenceID=spatialReferenceID, replaceParamFile=replaceParamFile)\n\n            # Commit to database\n            self._commit(session, self.COMMIT_ERROR_MESSAGE)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readOutput(self, directory, projectFileName, session, spatial=False, spatialReferenceID=None):\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Add project file to session\n            session.add(self)\n\n            # Read Project File\n            self.read(directory, projectFileName, session, spatial, spatialReferenceID)\n\n            # Get the batch directory for output\n            batchDirectory = self._getBatchDirectory(directory)\n\n            # Read Mask (dependency of some output files)\n            maskMap = WatershedMaskFile()\n            maskMapFilename = self.getCard('WATERSHED_MASK').value.strip('\"')\n            maskMap.read(session=session, directory=directory, filename=maskMapFilename, spatial=spatial)\n            maskMap.projectFile = self\n\n            # Automatically derive the spatial reference system, if possible\n            if spatialReferenceID is None:\n                spatialReferenceID = self._automaticallyDeriveSpatialReferenceId(directory)\n\n            # Read Output Files\n            self._readXput(self.OUTPUT_FILES, batchDirectory, session, spatial=spatial, spatialReferenceID=spatialReferenceID)\n\n            # Read WMS Dataset Files\n            self._readWMSDatasets(self.WMS_DATASETS, batchDirectory, session, spatial=spatial, spatialReferenceID=spatialReferenceID)\n\n            # Commit to database\n            self._commit(session, self.COMMIT_ERROR_MESSAGE)", "response": "Reads all output files for a GSSHA project file and writes them to the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a specific IO file for a GSSHA project to the database.", "response": "def _readXputFile(self, file_cards, card_name, directory, session,\n                      spatial=False, spatialReferenceID=None,\n                      replaceParamFile=None, **kwargs):\n        \"\"\"\n        Read specific IO file for a GSSHA project to the database.\n        \"\"\"\n        # Automatically derive the spatial reference system, if possible\n        if spatialReferenceID is None:\n            spatialReferenceID = self._automaticallyDeriveSpatialReferenceId(directory)\n\n        card = self.getCard(card_name)\n        if card:\n            fileIO = file_cards[card.name]\n            filename = card.value.strip('\"').strip(\"'\")\n\n            # Invoke read method on each file\n            return self._invokeRead(fileIO=fileIO,\n                                    directory=directory,\n                                    filename=filename,\n                                    session=session,\n                                    spatial=spatial,\n                                    spatialReferenceID=spatialReferenceID,\n                                    replaceParamFile=replaceParamFile,\n                                    **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a specific input file for a GSSHA project card.", "response": "def readInputFile(self, card_name, directory, session, spatial=False,\n                      spatialReferenceID=None, **kwargs):\n        \"\"\"\n        Read specific input file for a GSSHA project to the database.\n\n        Args:\n            card_name(str): Name of GSSHA project card.\n            directory (str): Directory containing all GSSHA model files. This method assumes that all files are located\n                in the same directory.\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database\n            spatial (bool, optional): If True, spatially enabled objects will be read in as PostGIS spatial objects.\n                Defaults to False.\n            spatialReferenceID (int, optional): Integer id of spatial reference system for the model. If no id is\n                provided GsshaPy will attempt to automatically lookup the spatial reference ID. If this process fails,\n                default srid will be used (4326 for WGS 84).\n\n        Returns:\n            file object\n        \"\"\"\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Read in replace param file\n            replaceParamFile = self._readReplacementFiles(directory, session, spatial, spatialReferenceID)\n            return self._readXputFile(self.INPUT_FILES, card_name, directory,\n                                      session, spatial, spatialReferenceID,\n                                      replaceParamFile, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef readOutputFile(self, card_name, directory, session, spatial=False,\n                      spatialReferenceID=None, **kwargs):\n        \"\"\"\n        Read specific input file for a GSSHA project to the database.\n\n        Args:\n            card_name(str): Name of GSSHA project card.\n            directory (str): Directory containing all GSSHA model files. This method assumes that all files are located\n                in the same directory.\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database\n            spatial (bool, optional): If True, spatially enabled objects will be read in as PostGIS spatial objects.\n                Defaults to False.\n            spatialReferenceID (int, optional): Integer id of spatial reference system for the model. If no id is\n                provided GsshaPy will attempt to automatically lookup the spatial reference ID. If this process fails,\n                default srid will be used (4326 for WGS 84).\n\n        Returns:\n            file object\n        \"\"\"\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            return self._readXputFile(self.OUTPUT_FILES, card_name, directory,\n                                      session, spatial, spatialReferenceID, **kwargs)", "response": "Reads a specific output file for a GSSHA project card."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef writeProject(self, session, directory, name):\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Get the batch directory for output\n            batchDirectory = self._getBatchDirectory(directory)\n\n            # Get param file for writing\n            replaceParamFile = self.replaceParamFile\n\n            # Write the replacement files\n            self._writeReplacementFiles(session=session, directory=directory, name=name)\n\n            # Write Project File\n            self.write(session=session, directory=directory, name=name)\n\n            # Write input files\n            self._writeXput(session=session, directory=directory, fileCards=self.INPUT_FILES, name=name, replaceParamFile=replaceParamFile)\n\n            # Write output files\n            self._writeXput(session=session, directory=batchDirectory, fileCards=self.OUTPUT_FILES, name=name)\n\n            # Write input map files\n            self._writeXputMaps(session=session, directory=directory, mapCards=self.INPUT_MAPS, name=name, replaceParamFile=replaceParamFile)\n\n            # Write WMS Dataset Files\n            self._writeWMSDatasets(session=session, directory=batchDirectory, wmsDatasetCards=self.WMS_DATASETS, name=name)", "response": "Writes all files for a specific project to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef writeInput(self, session, directory, name):\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Get param file for writing\n            replaceParamFile = self.replaceParamFile\n\n            # Write Project File\n            self.write(session=session, directory=directory, name=name)\n\n            # Write input files\n            self._writeXput(session=session, directory=directory, fileCards=self.INPUT_FILES, name=name, replaceParamFile=replaceParamFile)\n\n            # Write input map files\n            self._writeXputMaps(session=session, directory=directory, mapCards=self.INPUT_MAPS, name=name, replaceParamFile=replaceParamFile)", "response": "Writes the input files for a GSSHA project to file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef writeOutput(self, session, directory, name):\n        self.project_directory = directory\n        with tmp_chdir(directory):\n            # Get the batch directory for output\n            batchDirectory = self._getBatchDirectory(directory)\n\n            # Write the replacement files\n            self._writeReplacementFiles(session=session, directory=directory, name=name)\n\n            # Write Project File\n            self.write(session=session, directory=directory, name=name)\n\n            # Write output files\n            self._writeXput(session=session, directory=batchDirectory, fileCards=self.OUTPUT_FILES, name=name)\n\n            # Write WMS Dataset Files\n            self._writeWMSDatasets(session=session, directory=batchDirectory, wmsDatasetCards=self.WMS_DATASETS, name=name)", "response": "Writes the output files for a GSSHA project to file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getFileKeys(self):\n        files = self.getFileObjects()\n\n        files_list = []\n\n        for key, value in files.iteritems():\n            if value:\n                files_list.append(key)\n\n        return files_list", "response": "Retrieves a list of file keys that have been read into the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of file objects that can be accessed by the getFileKeys method of the GsshaPy object.", "response": "def getFileObjects(self):\n        \"\"\"\n        Retrieve a dictionary of file objects.\n\n        This is a utility method that can be used to programmatically access the GsshaPy file objects. Use this method\n        in conjunction with the getFileKeys method to access only files that have been read into the database.\n\n        Returns:\n            dict: Dictionary with human readable keys and values of GsshaPy file object instances. Files that have not\n            been read into the database will have a value of None.\n        \"\"\"\n\n        files = {'project-file': self,\n                 'mapping-table-file': self.mapTableFile,\n                 'channel-input-file': self.channelInputFile,\n                 'precipitation-file': self.precipFile,\n                 'storm-pipe-network-file': self.stormPipeNetworkFile,\n                 'hmet-file': self.hmetFile,\n                 'nwsrfs-file': self.nwsrfsFile,\n                 'orographic-gage-file': self.orographicGageFile,\n                 'grid-pipe-file': self.gridPipeFile,\n                 'grid-stream-file': self.gridStreamFile,\n                 'time-series-file': self.timeSeriesFiles,\n                 'projection-file': self.projectionFile,\n                 'replace-parameters-file': self.replaceParamFile,\n                 'replace-value-file': self.replaceValFile,\n                 'output-location-file': self.outputLocationFiles,\n                 'maps': self.maps,\n                 'link-node-datasets-file': self.linkNodeDatasets}\n\n        return files"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getCard(self, name):\n        cards = self.projectCards\n\n        for card in cards:\n            if card.name.upper() == name.upper():\n                return card\n\n        return None", "response": "Retrieves a project card object for given name. Returns None if the card is not available."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd or updates a GSSHA project file card with the given name and value.", "response": "def setCard(self, name, value, add_quotes=False):\n        \"\"\"\n        Adds/updates card for gssha project file\n\n        Args:\n            name (str): Name of card to be updated/added.\n            value (str): Value to attach to the card.\n            add_quotes (Optional[bool]): If True, will add quotes around string. Default is False.\n        \"\"\"\n        gssha_card = self.getCard(name)\n\n        if add_quotes:\n            value = '\"{0}\"'.format(value)\n\n        if gssha_card is None:\n            # add new card\n            new_card = ProjectCard(name=name, value=value)\n            new_card.projectFile = self\n        else:\n            gssha_card.value = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deleteCard(self, card_name, db_session):\n        card_name = card_name.upper()\n        gssha_card = self.getCard(card_name)\n        if gssha_card is not None:\n            db_session.delete(gssha_card)\n            db_session.commit()", "response": "Removes a card from gssha project file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getModelSummaryAsKml(self, session, path=None, documentName=None, withStreamNetwork=True, withNodes=False, styles={}):\n        # Get mask map\n        watershedMaskCard = self.getCard('WATERSHED_MASK')\n        maskFilename = watershedMaskCard.value\n        maskExtension = maskFilename.strip('\"').split('.')[1]\n\n        maskMap = session.query(RasterMapFile).\\\n                          filter(RasterMapFile.projectFile == self).\\\n                          filter(RasterMapFile.fileExtension == maskExtension).\\\n                          one()\n\n        # Get mask map as a KML polygon\n        statement = \"\"\"\n                    SELECT val, ST_AsKML(geom) As polygon\n                    FROM (\n                    SELECT (ST_DumpAsPolygons({0})).*\n                    FROM {1} WHERE id={2}\n                    ) As foo\n                    ORDER BY val;\n                    \"\"\".format('raster', maskMap.tableName, maskMap.id)\n\n        result = session.execute(statement)\n\n        maskMapKmlPolygon = ''\n        for row in result:\n            maskMapKmlPolygon = row.polygon\n\n        # Set Default Styles\n        streamLineColorValue = (255, 255, 0, 0)  # Blue\n        streamLineWidthValue = 2\n        nodeIconHrefValue = 'http://maps.google.com/mapfiles/kml/paddle/red-circle.png'\n        nodeIconScaleValue = 1\n        maskLineColorValue = (255, 0, 0, 255)\n        maskFillColorValue = (128, 64, 64, 64)\n        maskLineWidthValue = 2\n\n        # Validate\n        if 'streamLineColor' in styles:\n            if len(styles['streamLineColor']) < 4:\n                log.warning('streamLineColor style must be a list or a tuple of '\n                         'four elements representing integer RGBA values.')\n            else:\n                userLineColor = styles['streamLineColor']\n                streamLineColorValue = (userLineColor[3], userLineColor[2], userLineColor[1], userLineColor[0])\n\n        if 'streamLineWidth' in styles:\n            try:\n                float(styles['streamLineWidth'])\n                streamLineWidthValue = styles['streamLineWidth']\n\n            except ValueError:\n                log.warning('streamLineWidth must be a valid '\n                         'number representing the width of the line in pixels.')\n\n        if 'nodeIconHref' in styles:\n            nodeIconHrefValue = styles['nodeIconHref']\n\n        if 'nodeIconScale' in styles:\n            try:\n                float(styles['nodeIconScale'])\n                nodeIconScaleValue = styles['nodeIconScale']\n\n            except ValueError:\n                log.warning('nodeIconScaleValue must be a valid number representing'\n                         ' the width of the line in pixels.')\n\n        if 'maskLineColor' in styles:\n            if len(styles['maskLineColor']) < 4:\n                log.warning('maskLineColor style must be a list or a tuple of four '\n                         'elements representing integer RGBA values.')\n            else:\n                userLineColor = styles['maskLineColor']\n                maskLineColorValue = (userLineColor[3], userLineColor[2], userLineColor[1], userLineColor[0])\n\n        if 'maskFillColor' in styles:\n            if len(styles['maskFillColor']) < 4:\n                log.warning('maskFillColor style must be a list or a tuple of four '\n                         'elements representing integer RGBA values.')\n            else:\n                userLineColor = styles['maskFillColor']\n                maskFillColorValue = (userLineColor[3], userLineColor[2], userLineColor[1], userLineColor[0])\n\n        if 'maskLineWidth' in styles:\n            try:\n                float(styles['maskLineWidth'])\n                maskLineWidthValue = styles['maskLineWidth']\n\n            except ValueError:\n                log.warning('maskLineWidth must be a valid number representing '\n                         'the width of the line in pixels.')\n\n        if not documentName:\n            documentName = self.name\n\n        # Initialize KML Document\n        kml = ET.Element('kml', xmlns='http://www.opengis.net/kml/2.2')\n        document = ET.SubElement(kml, 'Document')\n        docName = ET.SubElement(document, 'name')\n        docName.text = documentName\n\n        # Mask Map\n        maskPlacemark = ET.SubElement(document, 'Placemark')\n        maskPlacemarkName = ET.SubElement(maskPlacemark, 'name')\n        maskPlacemarkName.text = 'Mask Map'\n\n        # Mask Styles\n        maskStyles = ET.SubElement(maskPlacemark, 'Style')\n\n        # Set polygon line style\n        maskLineStyle = ET.SubElement(maskStyles, 'LineStyle')\n\n        # Set polygon line color and width\n        maskLineColor = ET.SubElement(maskLineStyle, 'color')\n        maskLineColor.text = '%02X%02X%02X%02X' % maskLineColorValue\n        maskLineWidth = ET.SubElement(maskLineStyle, 'width')\n        maskLineWidth.text = str(maskLineWidthValue)\n\n        # Set polygon fill color\n        maskPolyStyle = ET.SubElement(maskStyles, 'PolyStyle')\n        maskPolyColor = ET.SubElement(maskPolyStyle, 'color')\n        maskPolyColor.text = '%02X%02X%02X%02X' % maskFillColorValue\n\n        # Mask Geometry\n        maskPolygon = ET.fromstring(maskMapKmlPolygon)\n        maskPlacemark.append(maskPolygon)\n\n        if withStreamNetwork:\n            # Get the channel input file for the stream network\n            channelInputFile = self.channelInputFile\n\n            # Retrieve Stream Links\n            links = channelInputFile.getFluvialLinks()\n\n            # Stream Network\n            for link in links:\n                placemark = ET.SubElement(document, 'Placemark')\n                placemarkName = ET.SubElement(placemark, 'name')\n                placemarkName.text = 'Stream Link {0}'.format(str(link.linkNumber))\n\n                # Create style tag and setup styles\n                styles = ET.SubElement(placemark, 'Style')\n\n                # Set line style\n                lineStyle = ET.SubElement(styles, 'LineStyle')\n                lineColor = ET.SubElement(lineStyle, 'color')\n                lineColor.text = '%02X%02X%02X%02X' % streamLineColorValue\n                lineWidth = ET.SubElement(lineStyle, 'width')\n                lineWidth.text = str(streamLineWidthValue)\n\n                # Add the geometry to placemark\n                linkKML = link.getAsKml(session)\n                if linkKML:\n                    lineString = ET.fromstring(linkKML)\n                    placemark.append(lineString)\n                else:\n                    log.warning(\"No geometry found for link with id {0}\".format(link.id))\n\n                if withNodes:\n                    # Create the node styles\n                    nodeStyles = ET.SubElement(document, 'Style', id='node_styles')\n\n                    # Hide labels\n                    nodeLabelStyle = ET.SubElement(nodeStyles, 'LabelStyle')\n                    nodeLabelScale = ET.SubElement(nodeLabelStyle, 'scale')\n                    nodeLabelScale.text = str(0)\n\n                    # Style icon\n                    nodeIconStyle = ET.SubElement(nodeStyles, 'IconStyle')\n\n                    # Set icon\n                    nodeIcon = ET.SubElement(nodeIconStyle, 'Icon')\n                    iconHref = ET.SubElement(nodeIcon, 'href')\n                    iconHref.text = nodeIconHrefValue\n\n                    # Set icon scale\n                    iconScale = ET.SubElement(nodeIconStyle, 'scale')\n                    iconScale.text = str(nodeIconScaleValue)\n\n                    for node in link.nodes:\n                        # New placemark for each node\n                        nodePlacemark = ET.SubElement(document, 'Placemark')\n                        nodePlacemarkName = ET.SubElement(nodePlacemark, 'name')\n                        nodePlacemarkName.text = str(node.nodeNumber)\n\n                        # Styles for the node\n                        nodeStyleUrl = ET.SubElement(nodePlacemark, 'styleUrl')\n                        nodeStyleUrl.text = '#node_styles'\n\n                        nodeString = ET.fromstring(node.getAsKml(session))\n                        nodePlacemark.append(nodeString)\n\n        kmlString = ET.tostring(kml)\n\n        if path:\n            with open(path, 'w') as f:\n                f.write(kmlString)\n\n        return kmlString", "response": "Retrieve a KML representation of the model. Includes polygonized mask map vector stream network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getModelSummaryAsWkt(self, session, withStreamNetwork=True, withNodes=False):\n        # Get mask map\n        watershedMaskCard = self.getCard('WATERSHED_MASK')\n        maskFilename = watershedMaskCard.value\n        maskExtension = maskFilename.strip('\"').split('.')[1]\n\n        maskMap = session.query(RasterMapFile).\\\n                          filter(RasterMapFile.projectFile == self).\\\n                          filter(RasterMapFile.fileExtension == maskExtension).\\\n                          one()\n\n        # Get mask map as a KML polygon\n        statement = \"\"\"\n                    SELECT val, ST_AsText(geom) As polygon\n                    FROM (\n                    SELECT (ST_DumpAsPolygons({0})).*\n                    FROM {1} WHERE id={2}\n                    ) As foo\n                    ORDER BY val;\n                    \"\"\".format('raster', maskMap.tableName, maskMap.id)\n\n        result = session.execute(statement)\n\n        maskMapTextPolygon = ''\n        for row in result:\n            maskMapTextPolygon = row.polygon\n\n        # Default WKT model representation string is a geometry collection with the mask map polygon\n        wktString = 'GEOMCOLLECTION ({0})'.format(maskMapTextPolygon)\n\n        if withStreamNetwork:\n            # Get the channel input file for the stream network\n            channelInputFile = self.channelInputFile\n\n            # Some models may not have streams enabled\n            if channelInputFile is not None:\n                # Use the existing method on the channel input file to generate the stream network WKT\n                wktStreamNetwork = channelInputFile.getStreamNetworkAsWkt(session=session, withNodes=withNodes)\n\n                # Strip off the \"GEOMCOLLECTION\" identifier\n                wktStreamNetwork = wktStreamNetwork.replace('GEOMCOLLECTION (', '')\n\n                # Replace the WKT model representation string with a geometry collection with mask map\n                # and all stream network components\n                wktString = 'GEOMCOLLECTION ({0}, {1}'.format(maskMapTextPolygon, wktStreamNetwork)\n\n        return wktString", "response": "Returns a Well Known Text representation of the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a GeoJSON representation of the model.", "response": "def getModelSummaryAsGeoJson(self, session, withStreamNetwork=True, withNodes=False):\n        \"\"\"\n        Retrieve a GeoJSON representation of the model. Includes vectorized mask map and stream network.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database\n            withStreamNetwork (bool, optional): Include stream network. Defaults to True.\n            withNodes (bool, optional): Include nodes. Defaults to False.\n\n        Returns:\n            str: GeoJSON string\n        \"\"\"\n        # Get mask map\n        watershedMaskCard = self.getCard('WATERSHED_MASK')\n        maskFilename = watershedMaskCard.value\n        maskExtension = maskFilename.strip('\"').split('.')[1]\n\n        maskMap = session.query(RasterMapFile).\\\n                          filter(RasterMapFile.projectFile == self).\\\n                          filter(RasterMapFile.fileExtension == maskExtension).\\\n                          one()\n\n        # Get mask map as a KML polygon\n        statement = \"\"\"\n                    SELECT val, ST_AsGeoJSON(geom) As polygon\n                    FROM (\n                    SELECT (ST_DumpAsPolygons({0})).*\n                    FROM {1} WHERE id={2}\n                    ) As foo\n                    ORDER BY val;\n                    \"\"\".format('raster', maskMap.tableName, maskMap.id)\n\n        result = session.execute(statement)\n\n        maskMapJsonPolygon = ''\n        for row in result:\n            maskMapJsonPolygon = row.polygon\n\n        jsonString = maskMapJsonPolygon\n\n        if withStreamNetwork:\n            # Get the channel input file for the stream network\n            channelInputFile = self.channelInputFile\n\n            if channelInputFile is not None:\n                # Use the existing method on the channel input file to generate the stream network GeoJson\n                jsonStreamNetwork = channelInputFile.getStreamNetworkAsGeoJson(session=session, withNodes=withNodes)\n\n                # Convert to json Python objects\n                featureCollection = json.loads(jsonStreamNetwork)\n                jsonMaskMapObjects = json.loads(maskMapJsonPolygon)\n\n                # Create a mask feature\n                maskFeature = {\"type\": \"Feature\",\n                               \"geometry\": jsonMaskMapObjects,\n                               \"properties\": {},\n                               \"id\": maskMap.id}\n\n                # Add mask map to feature collection\n                tempFeatures = featureCollection['features']\n                tempFeatures.append(maskFeature)\n                featureCollection['features'] = tempFeatures\n\n                # Dump to string\n                jsonString = json.dumps(featureCollection)\n\n        return jsonString"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns GDALGrid object of GSSHA project card.", "response": "def getGridByCard(self, gssha_card_name):\n        \"\"\"\n        Returns GDALGrid object of GSSHA grid\n\n        Paramters:\n            gssha_card_name(str): Name of GSSHA project card for grid.\n\n        Returns:\n            GDALGrid\n        \"\"\"\n        with tmp_chdir(self.project_directory):\n            if gssha_card_name not in (self.INPUT_MAPS+self.WMS_DATASETS):\n                raise ValueError(\"Card {0} not found in valid grid cards ...\"\n                                 .format(gssha_card_name))\n\n            gssha_grid_card = self.getCard(gssha_card_name)\n            if gssha_grid_card is None:\n                raise ValueError(\"{0} card not found ...\".format(gssha_card_name))\n\n            gssha_pro_card = self.getCard(\"#PROJECTION_FILE\")\n            if gssha_pro_card is None:\n                raise ValueError(\"#PROJECTION_FILE card not found ...\")\n\n            # return gssha grid\n            return GDALGrid(gssha_grid_card.value.strip('\"').strip(\"'\"),\n                            gssha_pro_card.value.strip('\"').strip(\"'\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns GDALGrid object of GSSHA model bounds", "response": "def getGrid(self, use_mask=True):\n        \"\"\"\n        Returns GDALGrid object of GSSHA model bounds\n\n        Paramters:\n            use_mask(bool): If True, uses watershed mask. Otherwise, it uses the elevaiton grid.\n\n        Returns:\n            GDALGrid\n\n        \"\"\"\n        grid_card_name = \"WATERSHED_MASK\"\n        if not use_mask:\n            grid_card_name = \"ELEVATION\"\n\n        return self.getGridByCard(grid_card_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns GDALGrid object of index map with given name", "response": "def getIndexGrid(self, name):\n        \"\"\"\n        Returns GDALGrid object of index map\n\n        Paramters:\n            name(str): Name of index map in 'cmt' file.\n\n        Returns:\n            GDALGrid\n        \"\"\"\n        index_map = self.mapTableFile.indexMaps.filter_by(name=name).one()\n\n        gssha_pro_card = self.getCard(\"#PROJECTION_FILE\")\n        if gssha_pro_card is None:\n            raise ValueError(\"#PROJECTION_FILE card not found ...\")\n\n        with tmp_chdir(self.project_directory):\n            # return gssha grid\n            return GDALGrid(index_map.filename,\n                            gssha_pro_card.value.strip('\"').strip(\"'\"))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getWkt(self):\n        gssha_pro_card = self.getCard(\"#PROJECTION_FILE\")\n        if gssha_pro_card is None:\n            raise ValueError(\"#PROJECTION_FILE card not found ...\")\n\n        with tmp_chdir(self.project_directory):\n            gssha_prj_file = gssha_pro_card.value.strip('\"').strip(\"'\")\n            with open(gssha_prj_file) as pro_file:\n                wkt_string = pro_file.read()\n            return wkt_string", "response": "Returns GSSHA projection WKT string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the outlet latitude and longitude of the grid cell center.", "response": "def getOutlet(self):\n        \"\"\"\n        Gets the outlet latitude and longitude.\n\n        Returns:\n            latitude(float): Latitude of grid cell center.\n            longitude(float): Longitude of grid cell center.\n        \"\"\"\n        # OUTROW, OUTCOL\n        outrow = int(self.getCard(name='OUTROW').value)-1\n        outcol = int(self.getCard(name='OUTCOL').value)-1\n        gssha_grid = self.getGrid()\n        return gssha_grid.pixel2lonlat(outcol, outrow)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the outlet grid cell information in the project file.", "response": "def setOutlet(self, col, row, outslope=None):\n        \"\"\"\n        Sets the outlet grid cell information in the project file.\n\n        Parameters:\n            col(float): 1-based column index.\n            row(float): 1-based row index.\n            outslope(Optional[float]): River slope at outlet.\n        \"\"\"\n        #OUTROW, OUTCOL, OUTSLOPE\n        gssha_grid = self.getGrid()\n        # col, row = gssha_grid.lonlat2pixel(longitude, latitude)\n        # add 1 to row & col becasue GSSHA is 1-based\n        self.setCard(name='OUTROW', value=str(row))\n        self.setCard(name='OUTCOL', value=str(col))\n        if outslope is None:\n            self.calculateOutletSlope()\n        else:\n            self.setCard(name='OUTSLOPE', value=str(outslope))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findOutlet(self, shapefile_path):\n        # determine outlet from shapefile\n        # by getting outlet from first point in polygon\n\n        # make sure the boundary geometry is valid\n        check_watershed_boundary_geometry(shapefile_path)\n\n        shapefile = ogr.Open(shapefile_path)\n        source_layer = shapefile.GetLayer(0)\n        source_lyr_proj = source_layer.GetSpatialRef()\n        osr_geographic_proj = osr.SpatialReference()\n        osr_geographic_proj.ImportFromEPSG(4326)\n        proj_transform = osr.CoordinateTransformation(source_lyr_proj,\n                                                      osr_geographic_proj)\n        boundary_feature = source_layer.GetFeature(0)\n        feat_geom = boundary_feature.GetGeometryRef()\n        feat_geom.Transform(proj_transform)\n        polygon = shapely_loads(feat_geom.ExportToWkb())\n\n        # make lowest point on boundary outlet\n        mask_grid = self.getGrid()\n        elevation_grid = self.getGrid(use_mask=False)\n        elevation_array = elevation_grid.np_array()\n        ma_elevation_array = np.ma.array(elevation_array,\n                                         mask=mask_grid.np_array()==0)\n        min_elevation = sys.maxsize\n        outlet_pt = None\n        for coord in list(polygon.exterior.coords):\n            try:\n                col, row = mask_grid.lonlat2pixel(*coord)\n            except IndexError:\n                # out of bounds\n                continue\n\n            elevation_value = ma_elevation_array[row, col]\n            if elevation_value is np.ma.masked:\n                # search for closest value in mask to this point\n                # elevation within 5 pixels in any direction\n                actual_value = elevation_array[row, col]\n                max_diff = sys.maxsize\n                nrow = None\n                ncol = None\n                nval = None\n                for row_ix in range(max(row-5, 0), min(row+5, mask_grid.y_size)):\n                    for col_ix in range(max(col-5, 0), min(col+5, mask_grid.x_size)):\n                        val = ma_elevation_array[row_ix, col_ix]\n                        if not val is np.ma.masked:\n                            val_diff = abs(val-actual_value)\n                            if val_diff < max_diff:\n                                max_diff = val_diff\n                                nval = val\n                                nrow = row_ix\n                                ncol = col_ix\n\n                if None not in (nrow, ncol, nval):\n                    row = nrow\n                    col = ncol\n                    elevation_value = nval\n\n            if elevation_value < min_elevation:\n                min_elevation = elevation_value\n                outlet_pt = (col, row)\n\n        if outlet_pt is None:\n            raise IndexError('No valid outlet points found on boundary ...')\n\n        outcol, outrow = outlet_pt\n        self.setOutlet(col=outcol+1, row=outrow+1)", "response": "Find outlet location for a given shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculateOutletSlope(self):\n        try:\n            mask_grid = self.getGrid()\n            elevation_grid = self.getGrid(use_mask=False)\n\n            outrow = int(self.getCard(\"OUTROW\").value)-1\n            outcol = int(self.getCard(\"OUTCOL\").value)-1\n            cell_size = float(self.getCard(\"GRIDSIZE\").value)\n\n            min_row = max(0, outrow-1)\n            max_row = min(mask_grid.x_size, outrow+2)\n            min_col = max(0, outcol-1)\n            max_col = min(mask_grid.y_size, outcol+2)\n\n            mask_array = mask_grid.np_array()\n            mask_array[outrow, outcol] = 0\n            mask_array = mask_array[min_row:max_row, min_col:max_col]\n            mask_array = (mask_array==0)\n\n            elevation_array = elevation_grid.np_array()\n            original_elevation = elevation_array[outrow, outcol]\n            elevation_array = elevation_array[min_row:max_row, min_col:max_col]\n\n            slope_calc_array = (elevation_array-original_elevation)/cell_size\n            #NOTE: Ignoring distance to cells at angles. Assuming to small to matter\n            mask_array[slope_calc_array<=0] = True\n\n            slope_mask_array = np.ma.array(slope_calc_array, mask=mask_array)\n            outslope = slope_mask_array.mean()\n            if outslope is np.ma.masked or outslope < 0.001:\n                outslope = 0.001\n\n        except ValueError:\n            outslope = 0.001\n\n        self.setCard(\"OUTSLOPE\", str(outslope))", "response": "Calculates the slope at the OUTLET ArcGIS entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timezone(self):\n        if self._tz is None:\n            # GET CENTROID FROM GSSHA GRID\n            cen_lat, cen_lon = self.centerLatLon()\n            # update time zone\n            tf = TimezoneFinder()\n            tz_name = tf.timezone_at(lng=cen_lon, lat=cen_lat)\n\n            self._tz = timezone(tz_name)\n        return self._tz", "response": "Returns the timezone of the GSSHA model object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the center lat lon of the model", "response": "def centerLatLon(self):\n        \"\"\"\n        Get the center lat/lon of model\n        \"\"\"\n        # GET CENTROID FROM GSSHA GRID\n        gssha_grid = self.getGrid()\n\n        min_x, max_x, min_y, max_y = gssha_grid.bounds()\n        x_ext, y_ext = transform(gssha_grid.proj,\n                                 Proj(init='epsg:4326'),\n                                 [min_x, max_x, min_x, max_x],\n                                 [min_y, max_y, max_y, min_y],\n                                 )\n        return np.mean(y_ext), np.mean(x_ext)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _getBatchDirectory(self, projectRootDirectory):\n        # Set output directory to main directory as default\n        batchDirectory = projectRootDirectory\n\n        # Get the replace folder card\n        replaceFolderCard = self.getCard('REPLACE_FOLDER')\n\n        if replaceFolderCard:\n            replaceDir = replaceFolderCard.value.strip('\"')\n            batchDirectory = os.path.join(batchDirectory, replaceDir)\n\n        # Create directory if it doesn't exist\n        if not os.path.isdir(batchDirectory):\n            os.mkdir(batchDirectory)\n            log.info('Creating directory for batch output: {0}'.format(batchDirectory))\n\n        return batchDirectory", "response": "Get the batch directory path for the current project file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading files from File Method", "response": "def _readXput(self, fileCards, directory, session, spatial=False, spatialReferenceID=4236, replaceParamFile=None):\n        \"\"\"\n        GSSHAPY Project Read Files from File Method\n        \"\"\"\n        ## NOTE: This function is dependent on the project file being read first\n        # Read Input/Output Files\n        for card in self.projectCards:\n            if (card.name in fileCards) and self._noneOrNumValue(card.value) and fileCards[card.name]:\n                fileIO = fileCards[card.name]\n                filename = card.value.strip('\"')\n\n                # Invoke read method on each file\n                self._invokeRead(fileIO=fileIO,\n                                 directory=directory,\n                                 filename=filename,\n                                 session=session,\n                                 spatial=spatial,\n                                 spatialReferenceID=spatialReferenceID,\n                                 replaceParamFile=replaceParamFile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _readWMSDatasets(self, datasetCards, directory, session, spatial=False, spatialReferenceID=4236):\n        if self.mapType in self.MAP_TYPES_SUPPORTED:\n            # Get Mask Map dependency\n            maskMap = session.query(RasterMapFile).\\\n                              filter(RasterMapFile.projectFile == self).\\\n                              filter(RasterMapFile.fileExtension == 'msk').\\\n                              one()\n\n            for card in self.projectCards:\n                if (card.name in datasetCards) and self._noneOrNumValue(card.value):\n                    # Get filename from project file\n                    filename = card.value.strip('\"')\n                    path = os.path.join(directory, filename)\n\n                    if os.path.isfile(path):\n                        wmsDatasetFile = WMSDatasetFile()\n                        wmsDatasetFile.projectFile = self\n                        wmsDatasetFile.read(directory=directory,\n                                            filename=filename,\n                                            session=session,\n                                            maskMap=maskMap,\n                                            spatial=spatial,\n                                            spatialReferenceID=spatialReferenceID)\n                    else:\n                        self._readBatchOutputForFile(directory, WMSDatasetFile, filename, session, spatial,\n                                                     spatialReferenceID, maskMap=maskMap)", "response": "Method to handle special case of WMS Dataset Files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the parameter replacement files into the database if they exist.", "response": "def _readReplacementFiles(self, directory, session, spatial, spatialReferenceID):\n        \"\"\"\n        Check for the parameter replacement file cards\n        (REPLACE_PARAMS and REPLACE_VALS) and read the files into\n        database if they exist.\n\n        Returns:\n            replaceParamFile or None if it doesn't exist\n        \"\"\"\n        # Set default\n        replaceParamFile = None\n\n        # Check for REPLACE_PARAMS card\n        replaceParamCard = self.getCard('REPLACE_PARAMS')\n\n        # Read the file if it exists\n        if replaceParamCard is not None:\n            filename = replaceParamCard.value.strip('\"')\n            replaceParamFile = ReplaceParamFile()\n            replaceParamFile.read(directory=directory,\n                                  filename=filename,\n                                  session=session,\n                                  spatial=spatial,\n                                  spatialReferenceID=spatialReferenceID)\n            replaceParamFile.projectFile = self\n\n        # Check for the REPLACE_VALS card\n        replaceValsCard = self.getCard('REPLACE_VALS')\n\n        # Read the file if it exists\n        if replaceValsCard is not None:\n            filename = replaceValsCard.value.strip('\"')\n            replaceValsCard = ReplaceValFile()\n            replaceValsCard.read(directory=directory,\n                                 filename=filename,\n                                 session=session,\n                                 spatial=spatial,\n                                 spatialReferenceID=spatialReferenceID)\n            replaceValsCard.projectFile = self\n\n        return replaceParamFile"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the output of a file in a batch mode.", "response": "def _readBatchOutputForFile(self, directory, fileIO, filename, session, spatial, spatialReferenceID,\n                                replaceParamFile=None, maskMap=None):\n        \"\"\"\n        When batch mode is run in GSSHA, the files of the same type are\n        prepended with an integer to avoid filename conflicts.\n        This will attempt to read files in this format and\n        throw warnings if the files aren't found.\n        \"\"\"\n        # Get contents of directory\n        directoryList = os.listdir(directory)\n\n        # Compile a list of files with that include the filename in them\n        batchFiles = []\n        for thing in directoryList:\n            if filename in thing:\n                batchFiles.append(thing)\n\n        numFilesRead = 0\n\n        for batchFile in batchFiles:\n            instance = fileIO()\n            instance.projectFile = self\n\n            if isinstance(instance, WMSDatasetFile):\n                instance.read(directory=directory, filename=batchFile, session=session, maskMap=maskMap, spatial=spatial,\n                              spatialReferenceID=spatialReferenceID)\n            else:\n                instance.read(directory, batchFile, session, spatial=spatial, spatialReferenceID=spatialReferenceID,\n                              replaceParamFile=replaceParamFile)\n            # Increment runCounter for next file\n            numFilesRead += 1\n\n        # Issue warnings\n        if '[' in filename or ']' in filename:\n            log.info('A file cannot be read, because the path to the '\n                     'file in the project file has been replaced with '\n                     'replacement variable {0}.'.format(filename))\n\n        elif numFilesRead == 0:\n            log.warning('{0} listed in project file, but no such '\n                     'file exists.'.format(filename))\n\n        else:\n            log.info('Batch mode output detected. {0} files read '\n                     'for file {1}'.format(numFilesRead, filename))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninvokes File Read Method on Other Files", "response": "def _invokeRead(self, fileIO, directory, filename, session, spatial=False,\n                    spatialReferenceID=4236, replaceParamFile=None, **kwargs):\n        \"\"\"\n        Invoke File Read Method on Other Files\n        \"\"\"\n        path = os.path.join(directory, filename)\n\n        if os.path.isfile(path):\n            instance = fileIO()\n            instance.projectFile = self\n            instance.read(directory, filename, session, spatial=spatial,\n                          spatialReferenceID=spatialReferenceID,\n                          replaceParamFile=replaceParamFile, **kwargs)\n            return instance\n        else:\n            self._readBatchOutputForFile(directory, fileIO, filename, session,\n                                         spatial, spatialReferenceID, replaceParamFile)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _writeXputMaps(self, session, directory, mapCards,\n                       name=None, replaceParamFile=None):\n        \"\"\"\n        GSSHAPY Project Write Map Files to File Method\n        \"\"\"\n        if self.mapType in self.MAP_TYPES_SUPPORTED:\n            for card in self.projectCards:\n                if (card.name in mapCards) and self._noneOrNumValue(card.value):\n                    filename = card.value.strip('\"')\n\n                    # Determine new filename\n                    filename = self._replaceNewFilename(filename, name)\n\n                    # Write map file\n                    self._invokeWrite(fileIO=RasterMapFile,\n                                      session=session,\n                                      directory=directory,\n                                      filename=filename,\n                                      replaceParamFile=replaceParamFile)\n        else:\n            for card in self.projectCards:\n                if (card.name in mapCards) and self._noneOrNumValue(card.value):\n                    filename = card.value.strip('\"')\n\n                    fileExtension = filename.split('.')[1]\n\n                    if fileExtension in self.ALWAYS_READ_AND_WRITE_MAPS:\n                        # Determine new filename\n                        filename = self._replaceNewFilename(filename, name)\n\n                        # Write map file\n                        self._invokeWrite(fileIO=RasterMapFile,\n                                          session=session,\n                                          directory=directory,\n                                          filename=filename,\n                                          replaceParamFile=replaceParamFile)\n\n            log.error('Could not write map files. MAP_TYPE {0} '\n                      'not supported.'.format(self.mapType))", "response": "Method to write the xput maps to file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _writeReplacementFiles(self, session, directory, name):\n        if self.replaceParamFile:\n            self.replaceParamFile.write(session=session, directory=directory,\n                                        name=name)\n\n        if self.replaceValFile:\n            self.replaceValFile.write(session=session, directory=directory,\n                                      name=name)", "response": "Write the replacement files in the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _invokeWrite(self, fileIO, session, directory, filename, replaceParamFile):\n        # Default value for instance\n        instance = None\n\n        try:\n            # Handle case where fileIO interfaces with single file\n            # Retrieve File using FileIO\n            instance = session.query(fileIO). \\\n                filter(fileIO.projectFile == self). \\\n                one()\n\n        except:\n            # Handle case where fileIO interfaces with multiple files\n            # Retrieve File using FileIO and file extension\n            extension = filename.split('.')[1]\n\n            try:\n\n                instance = session.query(fileIO). \\\n                    filter(fileIO.projectFile == self). \\\n                    filter(fileIO.fileExtension == extension). \\\n                    one()\n\n            except NoResultFound:\n                # Handle case when there is no file in database but the\n                # card is listed in the project file\n                log.warning('{0} listed as card in project file, but '\n                         'the file is not found in the database.'.format(filename))\n            except MultipleResultsFound:\n                self._invokeWriteForMultipleOfType(directory, extension, fileIO,\n                                                   filename, session,\n                                                   replaceParamFile=replaceParamFile)\n                return\n\n        # Initiate Write Method on File\n        if instance is not None:\n            instance.write(session=session, directory=directory, name=filename,\n                           replaceParamFile=replaceParamFile)", "response": "Invoke File Write Method on Other Files\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, originalPrefix, newPrefix=None):\n        # Determine number of spaces between card and value for nice alignment\n        numSpaces = max(2, 25 - len(self.name))\n\n        # Handle special case of booleans\n        if self.value is None:\n            line = '%s\\n' % self.name\n        else:\n            if self.name == 'WMS':\n                line = '%s %s\\n' % (self.name, self.value)\n            elif newPrefix is None:\n                line = '%s%s%s\\n' % (self.name, ' ' * numSpaces, self.value)\n            elif originalPrefix in self.value:\n                line = '%s%s%s\\n' % (self.name, ' ' * numSpaces, self.value.replace(originalPrefix, newPrefix))\n            else:\n                line = '%s%s%s\\n' % (self.name, ' ' * numSpaces, self.value)\n        return line", "response": "Writes the project card to string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the number of seconds that the entry has been since the start until the latest entry.", "response": "def elapsed(self):\n        \"\"\"Returns the number of seconds it has been since the start until the latest entry.\"\"\"\n        if not self.started or self._start_time is None:\n            return 0.0\n        return self._timing_data[-1][0] - self._start_time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an unstable rate based on the last two entries in the timing data. Less intensive to compute.", "response": "def rate_unstable(self):\n        \"\"\"Returns an unstable rate based on the last two entries in the timing data. Less intensive to compute.\"\"\"\n        if not self.started or self.stalled:\n            return 0.0\n        x1, y1 = self._timing_data[-2]\n        x2, y2 = self._timing_data[-1]\n        return (y2 - y1) / (x2 - x1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rate_overall(self):\n        elapsed = self.elapsed\n        return self.rate if not elapsed else self.numerator / self.elapsed", "response": "Returns the overall average rate based on the start time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_numerator(self, numerator, calculate=True):\n        # Validate\n        if self._timing_data and numerator < self._timing_data[-1][1]:\n            raise ValueError('numerator cannot decrement.')\n\n        # Update data.\n        now = _NOW()\n        if self._timing_data and now == self._timing_data[-1][0]:\n            self._timing_data[-1] = (now, numerator)  # Overwrite.\n        else:\n            self._timing_data.append((now, numerator))\n\n        # Calculate ETA and rate.\n        if not self.done and calculate and self.started:\n            self._calculate()", "response": "Sets the new numerator for the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms the ETA and rate calculation.", "response": "def _calculate(self):\n        \"\"\"Perform the ETA and rate calculation.\n\n        Two linear lines are used to calculate the ETA: the linear regression (line through a scatter-plot), and the\n        fitted line (a line that runs through the latest data point in _timing_data but parallel to the linear\n        regression line).\n\n        As the percentage moves closer to 100%, _calculate() gradually uses the ETA based on the fitted line more and\n        more. This is done to prevent an ETA that's in the past.\n\n        http://code.activestate.com/recipes/578914-simple-linear-regression-with-pure-python/\n        http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient\n        \"\"\"\n        # Calculate means and standard deviations.\n        mean_x = sum(i[0] for i in self._timing_data) / len(self._timing_data)\n        mean_y = sum(i[1] for i in self._timing_data) / len(self._timing_data)\n        std_x = sqrt(sum(pow(i[0] - mean_x, 2) for i in self._timing_data) / (len(self._timing_data) - 1))\n        std_y = sqrt(sum(pow(i[1] - mean_y, 2) for i in self._timing_data) / (len(self._timing_data) - 1))\n\n        # Calculate coefficient.\n        sum_xy, sum_sq_v_x, sum_sq_v_y = 0, 0, 0\n        for x, y in self._timing_data:\n            x -= mean_x\n            y -= mean_y\n            sum_xy += x * y\n            sum_sq_v_x += pow(x, 2)\n            sum_sq_v_y += pow(y, 2)\n        pearson_r = sum_xy / sqrt(sum_sq_v_x * sum_sq_v_y)\n\n        # Calculate regression line. y = mx + b where m is the slope and b is the y-intercept.\n        m = self.rate = pearson_r * (std_y / std_x)\n        if self.undefined:\n            return\n        y = self.denominator\n        b = mean_y - m * mean_x\n        x = (y - b) / m\n\n        # Calculate fitted line (transformed/shifted regression line horizontally).\n        fitted_b = self._timing_data[-1][1] - (m * self._timing_data[-1][0])\n        fitted_x = (y - fitted_b) / m\n        adjusted_x = ((fitted_x - x) * (self.numerator / self.denominator)) + x\n        self.eta_epoch = adjusted_x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tranz(parser, token, is_transchoice=False):\n    tokens = token.split_contents()\n    id = tokens[1]\n    number = domain = locale = None\n    parameters = {}\n    if len(tokens) > 2:\n        skip_idx = None\n        for idx, token in enumerate(tokens[2:], start=2):\n            if idx == skip_idx:\n                skip_idx = None\n                continue\n\n            if \"=\" in token:\n                k, v = token[0:token.index('=')], token[token.index('=') + 1:]\n                parameters[k] = v\n            elif token == \"number\":\n                number = tokens[idx + 1]\n                skip_idx = idx + 1\n            elif token == \"from\":\n                domain = tokens[idx + 1]\n                skip_idx = idx + 1\n            elif token == \"into\":\n                locale = tokens[idx + 1]\n                skip_idx = idx + 1\n            else:\n                raise TemplateSyntaxError(\n                    \"Unexpected token {0} in tag tranz\".format(token))\n    if is_transchoice and number is None:\n        raise TemplateSyntaxError(\n            \"number parameter expected in tag {tag_name}\")\n\n    return TranzNode(\n        id,\n        parameters,\n        domain,\n        locale,\n        number,\n        is_transchoice=is_transchoice)", "response": "Returns a TranzNode from a token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tranz_context(parser, token):\n    tokens = token.split_contents()\n\n    parameters = {}\n    for idx, token in enumerate(tokens[1:], start=1):\n        if \"=\" in token:\n            if token[0:token.index('=')] not in (\"domain\", \"prefix\", \"locale\"):\n                raise TemplateSyntaxError(\n                    \"Unexpected token {0} in tag {{tag_name}}\".format(token)\n                )\n\n            k, v = token[0:token.index('=')], token[token.index('=') + 1:]\n            parameters[k] = v\n        else:\n            raise TemplateSyntaxError(\n                \"Unexpected token {0} in tag {{tag_name}}\".format(token))\n\n    return TranzContextNode(\n        parameters.get('prefix', None),\n        parameters.get('domain', None),\n        parameters.get('locale', None)\n    )", "response": "Returns TranzContextNode for the given token."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites HMET WES to File Method", "response": "def _write(self, session, openFile, replaceParamFile):\n        \"\"\"\n        Write HMET WES to File Method\n        \"\"\"\n        ## TODO: Ensure Other HMET Formats are supported\n        hmetRecords = self.hmetRecords\n\n        for record in hmetRecords:\n            openFile.write('%s\\t%s\\t%s\\t%s\\t%.3f\\t%s\\t%s\\t%s\\t%s\\t%.2f\\t%.2f\\n' % (\n                record.hmetDateTime.year,\n                record.hmetDateTime.month,\n                record.hmetDateTime.day,\n                record.hmetDateTime.hour,\n                record.barometricPress,\n                record.relHumidity,\n                record.totalSkyCover,\n                record.windSpeed,\n                record.dryBulbTemp,\n                record.directRad,\n                record.globalRad))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite to File Method", "response": "def _write(self, session, openFile, replaceParamFile=None):\n        \"\"\"\n        ProjectFileEvent Write to File Method\n        \"\"\"\n        openFile.write(\n            text(\n                yaml.dump([evt.as_yml() for evt in\n                           self.events.order_by(ProjectFileEvent.name,\n                           ProjectFileEvent.subfolder)]\n                          )\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the next id for a subfolder.", "response": "def next_id(self, subfolder):\n        \"\"\"\n        ProjectFileEvent Write to File Method\n        \"\"\"\n        evt_sim_folders = self.events.filter(\n                        ProjectFileEvent.subfolder\n                            .like(\"{0}_%\".format(subfolder))\n                    )\n        max_id = 0\n        num_search = re.compile(r'{0}_(\\d+)'.format(subfolder), re.IGNORECASE)\n        for prj_event in evt_sim_folders:\n            found_num = num_search.findall(prj_event.subfolder)\n            if found_num is not None:\n                max_id = max(max_id, int(found_num[0]))\n        return max_id + 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_event(self, name, subfolder, session):\n        if self._similar_event_exists(subfolder):\n            subfolder += \"_{0}\".format(self.next_id(subfolder))\n        new_event = ProjectFileEvent(name=name, subfolder=subfolder)\n        session.add(new_event)\n        self.events.append(new_event)\n        session.commit()\n        return new_event", "response": "Add an event to the list of events."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates an event in the log.", "response": "def generate_event(self, session):\n        \"\"\"\n        Add an event\n        \"\"\"\n        event_name = \"event_{0}\".format(self.next_id(\"event\"))\n        return self.add_event(name=event_name, subfolder=event_name,\n                              session=session)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_yml(self):\n        return YmlFileEvent(name=str(self.name),\n                            subfolder=str(self.subfolder))", "response": "Return a yaml compatible version of this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing request to node s API route with the given keyword arguments", "response": "def prepare_request(node):\n    \"\"\"\n        Prepare request to node's API route\n\n        :param Node node: the RAML node object\n    \"\"\"\n    if node.resource.method not in AVAILABLE_METHODS:\n        raise UnsupportedHTTPMethodError(node.resource.method)\n\n    def request(data=None, json=None, **kwargs):\n        \"\"\"\n            Make request to node's API route with the given keyword arguments\n        \"\"\"\n        # validate given query parameters\n        for key, value in kwargs.items():\n            param = next((p for p in node.resource.query_params if p.name == key), None)\n            if not param:\n                raise UnsupportedQueryParameter(node.resource.path, key)\n\n            if not match_type(value, param.type):\n                raise TypeError(\n                    \"Resource Query Parameter has type '{0}' but expected type '{1}'\".format(\n                        value.__class__.__name__, param.type))\n\n        response = requests.request(node.resource.method, node.resource.absolute_uri, params=kwargs,\n                                    data=data, json=json)\n        return response\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dsp2dot_option(arg):\n\n    # noinspection PyUnusedLocal\n    def map_args(*args, **kwargs):\n        from schedula.utils.base import Base\n        a = inspect.signature(Base.plot).bind(None, *args, **kwargs).arguments\n        a.popitem(last=False)\n        return a\n\n    kw = eval('map_args(%s)' % arg)\n\n    return kw if kw else PLOT", "response": "Used to convert the dmap option to auto directives."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xd(self):\n        if self._xd is None:\n            path_to_lsm_files = path.join(self.lsm_input_folder_path,\n                                          self.lsm_search_card)\n            self._xd = super(NWMtoGSSHA, self).xd\n            self._xd.lsm.coords_projected = True\n        return self._xd", "response": "get xarray dataset file handle to LSM files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_converted_gssha_data_from_lsm(self, gssha_var, lsm_var, load_type):\n        super(NWMtoGSSHA, self).\\\n            _load_converted_gssha_data_from_lsm(gssha_var, lsm_var, load_type)\n        self.data.lsm.coords_projected = True", "response": "This function loads data from LSM and converts to GSSHA format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_transcoder():\n\n\ttranscoders = ['ffmpeg', 'avconv']\n\ttranscoder_details = {}\n\n\tfor transcoder in transcoders:\n\t\tcommand_path = shutil.which(transcoder)\n\t\tif command_path is None:\n\t\t\ttranscoder_details[transcoder] = 'Not installed.'\n\t\t\tcontinue\n\n\t\tstdout = subprocess.run([command_path, '-codecs'], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, universal_newlines=True).stdout\n\t\tmp3_encoding_support = ('libmp3lame' in stdout and 'disable-libmp3lame' not in stdout)\n\t\tif mp3_encoding_support:\n\t\t\ttranscoder_details[transcoder] = \"MP3 encoding support.\"\n\t\t\tbreak\n\t\telse:\n\t\t\ttranscoder_details[transcoder] = \"No MP3 encoding support.\"\n\telse:\n\t\traise ValueError(\n\t\t\tf\"ffmpeg or avconv must be in the path and support mp3 encoding.\"\n\t\t\t\"\\nDetails: {transcoder_details}\"\n\t\t)\n\n\treturn command_path", "response": "Return the path to a transcoder with MP3 support."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefining the data to be plotted.", "response": "def define_plot_data(data, x_name, *y_names):\n    \"\"\"\n    Defines the data to be plotted.\n\n    :param data:\n        All data.\n    :type data: dict\n\n    :param x_name:\n        x-axes name.\n    :type x_name: str\n\n    :param y_names:\n        y-axes names to be plotted.\n    :type y_names: str\n\n    :return:\n        Data to be plotted.\n    :rtype: list\n    \"\"\"\n    it = []\n    for k in y_names:\n        it.append({\n            'x': data[x_name],\n            'y': data[k],\n            'name': k\n        })\n    return it"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the series lines.", "response": "def plot_lines(it):\n    \"\"\"\n    Plotting lines.\n\n    :param it:\n        Data to plot where key value is the name of the series.\n    :type it: list[dict]\n\n    :return:\n        The plot.\n    :rtype: plotly.plotly.iplot\n    \"\"\"\n    data = [go.Scatter(mode='lines', **d) for d in it]\n    return py.iplot(data, filename='scatter-mode')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ssh_channel_read(ssh_channel_int, count, is_stderr):\n\n    buffer_ = create_string_buffer(count)\n    while 1:\n        received_bytes = c_ssh_channel_read(ssh_channel_int, \n                                            cast(buffer_, c_void_p), \n                                            c_uint32(count),\n                                            c_int(int(is_stderr)))\n\n        if received_bytes == SSH_ERROR:\n            ssh_session_int = _ssh_channel_get_session(ssh_channel_int)\n            error = ssh_get_error(ssh_session_int)\n\n            raise SshError(\"Channel read failed: %s\" % (error))\n\n        # BUG: We're not using the nonblocking variant, but this can still \n        # return SSH_AGAIN due to that call's broken dependencies.\n# TODO: This call might return SSH_AGAIN, even though we should always be \n#       blocking. Reported as bug #115.\n        elif received_bytes == SSH_AGAIN:\n            continue\n\n        else:\n            break\n\n# TODO: Where is the timeout configured for the read?\n    return buffer_.raw[0:received_bytes]", "response": "Do a read on a channel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dictionary(values):\n    result = dict()\n    for key in values:\n        fname = values[key][0]\n        if fname not in functions_map:\n            result[key] = values[key]\n        else:\n            params = values[key][1] if len(values[key]) == 2 else {}\n            result[key] = functions_map[fname](**params)\n    return result", "response": "This function generates dictionary from dictionary values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat the value like a human - readable file size.", "response": "def filesizeformat(bytes, decimals=1):\n    \"\"\"\n    Formats the value like a 'human-readable' file size (i.e. 13 KB, 4.1 MB,\n    102 bytes, etc).\n    Based on django.template.defaultfilters.filesizeformat\n    \"\"\"\n\n    try:\n        bytes = float(bytes)\n    except (TypeError, ValueError, UnicodeDecodeError):\n        raise ValueError\n\n    def filesize_number_format(value):\n        return formats.number_format(round(value, decimals), decimals)\n\n    units_list = sorted(six.iteritems(FILESIZE_UNITS), key=operator.itemgetter(1))\n\n    value = unit = None\n    len_unints_list = len(units_list)\n    for i in xrange(1, len_unints_list):\n        if bytes < units_list[i][1]:\n            prev_unit = units_list[i - 1]\n            value = filesize_number_format(bytes / prev_unit[1])\n            unit = prev_unit[0]\n            break\n\n    if value is None:\n        value = filesize_number_format(bytes / units_list[-1][1])\n        unit = units_list[-1][0]\n\n    return SIZEFIELD_FORMAT.format(value=value, unit=unit)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eventChunk(key, lines):\n    ## NOTE: RADAR file format not supported currently.\n    ## TODO: Add Support for RADAR file format type values\n\n    # Contants\n    KEYWORDS = ('EVENT',\n                'NRPDS',\n                'NRGAG',\n                'COORD',\n                'GAGES',\n                'ACCUM',\n                'RATES',\n                'RADAR')\n\n    NUM_CARDS = ('NRPDS',\n                 'NRGAG')\n\n    VALUE_CARDS = ('GAGES',\n                   'ACCUM',\n                   'RATES',\n                   'RADAR')\n\n    # Define result object\n    result = {'description': None,\n              'nrgag': None,\n              'nrpds': None,\n              'coords':[],\n              'valLines':[]}\n\n    chunks = pt.chunk(KEYWORDS, lines)\n\n    # Parse chunks associated with each key\n    for card, chunkList in iteritems(chunks):\n        # Parse each chunk in the chunk list\n        for chunk in chunkList:\n            schunk = chunk[0].strip().split()\n\n            # Cases\n            if card == 'EVENT':\n                # EVENT handler\n                schunk = pt.splitLine(chunk[0])\n                result['description'] = schunk[1]\n\n            elif card in NUM_CARDS:\n                # Num cards handler\n                result[card.lower()] = schunk[1]\n\n            elif card == 'COORD':\n                # COORD handler\n                schunk = pt.splitLine(chunk[0])\n\n                try:\n                    # Extract the event description\n                    desc = schunk[3]\n                except:\n                    # Handle case where the event description is blank\n                    desc = \"\"\n\n                coord = {'x': schunk[1],\n                         'y': schunk[2],\n                         'description': desc}\n\n                result['coords'].append(coord)\n\n            elif card in VALUE_CARDS:\n                # Value cards handler\n                # Extract DateTime\n                dateTime = datetime(year=int(schunk[1]),\n                                    month=int(schunk[2]),\n                                    day=int(schunk[3]),\n                                    hour=int(schunk[4]),\n                                    minute=int(schunk[5]))\n\n                # Compile values into a list\n                values = []\n                for index in range(6, len(schunk)):\n                    values.append(schunk[index])\n\n                valueLine = {'type': schunk[0],\n                             'dateTime': dateTime,\n                             'values': values}\n\n                result['valLines'].append(valueLine)\n\n    return result", "response": "Parse EVENT chunks and return a dictionary of event data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    # Prepare.\n    if os.name == 'nt':\n        locale.setlocale(locale.LC_ALL, 'english-us')\n    else:\n        locale.resetlocale()\n    response = requests.get(OPTIONS['<url>'], stream=True)\n    content_length = None if OPTIONS['--ignore-length'] else int(response.headers.get('Content-Length'))\n    progress_bar = ProgressBarWget(content_length, eta_every=4)\n    thread = DownloadThread(response)\n    print_every_seconds = 0.25\n\n    # Download.\n    thread.start()\n    while True:\n        progress_bar.numerator = thread.bytes_downloaded\n        print(progress_bar, end='\\r')\n        sys.stdout.flush()\n\n        # For undefined downloads (no content-length), check if thread has stopped. Loop only checks defined downloads.\n        if not thread.isAlive():\n            progress_bar.force_done = True\n            break\n        if progress_bar.done:\n            break\n\n        time.sleep(print_every_seconds)\n    print(progress_bar)", "response": "Main function for the main function of the download thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbases method for making a Losant API request", "response": "def request(self, method, path, params=None, headers=None, body=None):\n        \"\"\" Base method for making a Losant API request \"\"\"\n        if not headers:\n            headers = {}\n        if not params:\n            params = {}\n\n        headers[\"Accept\"] = \"application/json\"\n        headers[\"Accept-Version\"] = \"^1.15.0\"\n        if self.auth_token:\n            headers[\"Authorization\"] = \"Bearer {0}\".format(self.auth_token)\n\n        path = self.url + path\n        params = self.flatten_params(params)\n        response = requests.request(method, path, params=params, headers=headers, json=body)\n\n        result = response.text\n        try:\n            result = response.json()\n        except Exception:\n            pass\n\n        if response.status_code >= 400:\n            raise LosantError(response.status_code, result)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flatten_params(self, data, base_key=None):\n        result = {}\n\n        if data is None:\n            return result\n\n        map_data = None\n        if not isinstance(data, collections.Mapping):\n            map_data = []\n            for idx, val in enumerate(data):\n                map_data.append([str(idx), val])\n        else:\n            map_data = list(data.items())\n\n        for key, value in map_data:\n            if not base_key is None:\n                key = base_key + \"[\" + key + \"]\"\n\n            if isinstance(value, basestring) or not hasattr(value, \"__iter__\"):\n                result[key] = value\n            else:\n                result.update(self.flatten_params(value, key))\n\n        return result", "response": "Flatten out nested arrays and dicts in query params into correct format"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the excel file.", "response": "def read_excel(input_fpath):\n    \"\"\"\n    Reads the excel file.\n\n    :param input_fpath:\n        Input file path.\n    :type input_fpath: str\n\n    :return:\n        Raw Data.\n    :rtype: dict\n    \"\"\"\n    return {k: v.values for k, v in pd.read_excel(input_fpath).items()}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave model outputs in an Excel file.", "response": "def save_outputs(outputs, output_fpath):\n    \"\"\"\n    Save model outputs in an Excel file.\n\n    :param outputs:\n        Model outputs.\n    :type outputs: dict\n\n    :param output_fpath:\n        Output file path.\n    :type output_fpath: str\n    \"\"\"\n    df = pd.DataFrame(outputs)\n    with pd.ExcelWriter(output_fpath) as writer:\n        df.to_excel(writer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads from File Method", "response": "def _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        \"\"\"\n        Precipitation Read from File Method\n        \"\"\"\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Dictionary of keywords/cards and parse function names\n        KEYWORDS = ('EVENT',)\n\n        # Parse file into chunks associated with keywords/cards\n        with open(path, 'r') as f:\n            chunks = pt.chunk(KEYWORDS, f)\n\n        # Parse chunks associated with each key\n        for key, chunkList in iteritems(chunks):\n            # Parse each chunk in the chunk list\n            for chunk in chunkList:\n                result = gak.eventChunk(key, chunk)\n                self._createGsshaPyObjects(result)\n\n        # Add this PrecipFile to the database session\n        session.add(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _createGsshaPyObjects(self, eventChunk):\n        ## TODO: Add Support for RADAR file format type values\n\n        # Create GSSHAPY PrecipEvent\n        event = PrecipEvent(description=eventChunk['description'],\n                            nrGag=eventChunk['nrgag'],\n                            nrPds=eventChunk['nrpds'])\n\n        # Associate PrecipEvent with PrecipFile\n        event.precipFile = self\n\n        gages = []\n        for coord in eventChunk['coords']:\n            # Create GSSHAPY PrecipGage object\n            gage = PrecipGage(description=coord['description'],\n                              x=coord['x'],\n                              y=coord['y'])\n\n            # Associate PrecipGage with PrecipEvent\n            gage.event = event\n\n            # Append to gages list for association with PrecipValues\n            gages.append(gage)\n\n        for valLine in eventChunk['valLines']:\n            for index, value in enumerate(valLine['values']):\n                # Create GSSHAPY PrecipValue object\n                val = PrecipValue(valueType=valLine['type'],\n                                  dateTime=valLine['dateTime'],\n                                  value=value)\n\n                # Associate PrecipValue with PrecipEvent and PrecipGage\n                val.event = event\n                val.gage = gages[index]", "response": "Create GSSHAPY PrecipEvent PrecipValue and PrecipGage Objects Method\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlook up spatial reference ID using the projection file.", "response": "def lookupSpatialReferenceID(cls, directory, filename):\n        \"\"\"\n        Look up spatial reference system using the projection file.\n\n        Args:\n            directory (str):\n            filename (str):\n\n        Return:\n            int: Spatial Reference ID\n        \"\"\"\n\n        path = os.path.join(directory, filename)\n\n        with open(path, 'r') as f:\n            srid = lookupSpatialReferenceID(f.read())\n\n        return srid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log_to_console(status=True, level=None):\n\n    if status:\n        if level is not None:\n            logger.setLevel(level)\n\n        console_handler = logging.StreamHandler()\n        # create formatter\n        formatter = logging.Formatter('%(levelname)s-%(name)s: %(message)s')\n        # add formatter to handler\n        console_handler.setFormatter(formatter)\n        logger.addHandler(console_handler)\n\n        logger.info(\"GSSHApy {0}\".format(version()))\n\n    else:\n        for h in logger.handlers:\n            if type(h).__name__ == 'StreamHandler':\n                logger.removeHandler(h)", "response": "Log events to the console."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_to_file(status=True, filename=default_log_file, level=None):\n\n    if status:\n        if level is not None:\n            logger.setLevel(level)\n\n        try:\n            os.mkdir(os.path.dirname(filename))\n        except OSError:\n            pass\n\n        file_handler = logging.FileHandler(filename)\n        # create formatter\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s-%(name)s: %(message)s')\n        # add formatter to handler\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        logger.info(\"GSSHApy {0}\".format(version()))\n\n    else:\n        for h in logger.handlers:\n            if type(h).__name__ == 'FileHandler':\n                logger.removeHandler(h)", "response": "Log events to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef numerator(self, value):\n        # If ETA is every iteration, don't do anything fancy.\n        if self.eta_every <= 1:\n            self._eta.numerator = value\n            self._eta_string = self._generate_eta(self._eta.eta_seconds)\n            return\n\n        # If ETA is not every iteration, unstable rate is used. If this bar is undefined, no point in calculating ever.\n        if self._eta.undefined:\n            self._eta.set_numerator(value, calculate=False)\n            return\n\n        # Calculate if this iteration is the right one.\n        if self._eta_count >= self.eta_every:\n            self._eta_count = 1\n            self._eta.numerator = value\n            self._eta_string = self._generate_eta(self._eta.eta_seconds)\n            return\n\n        self._eta_count += 1\n        self._eta.set_numerator(value, calculate=False)", "response": "Sets a new numerator and generates the ETA. Must be greater than or equal to previous numerator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the rate of the progress as a float. Selects the unstable rate if eta_every > 1 for performance.", "response": "def rate(self):\n        \"\"\"Returns the rate of the progress as a float. Selects the unstable rate if eta_every > 1 for performance.\"\"\"\n        return float(self._eta.rate_unstable if self.eta_every > 1 else self._eta.rate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an elevation grid for the GSSHA simulation from an elevation raster.", "response": "def generateFromRaster(self,\n                           elevation_raster,\n                           shapefile_path=None,\n                           out_elevation_grid=None,\n                           resample_method=gdalconst.GRA_Average,\n                           load_raster_to_db=True):\n        \"\"\"\n        Generates an elevation grid for the GSSHA simulation\n        from an elevation raster\n\n        Example::\n\n            from gsshapy.orm import ProjectFile, ElevationGridFile\n            from gsshapy.lib import db_tools as dbt\n\n\n            gssha_directory = '/gsshapy/tests/grid_standard/gssha_project'\n            elevation_raster = 'elevation.tif'\n\n            project_manager, db_sessionmaker = \\\n                dbt.get_project_session('grid_standard',\n                                        gssha_directory)\n\n            db_session = db_sessionmaker()\n\n            # read project file\n            project_manager.readInput(directory=gssha_directory,\n                                      projectFileName='grid_standard.prj',\n                                      session=db_session)\n\n            # generate elevation grid\n            elevation_grid = ElevationGridFile(session=db_session,\n                                               project_file=project_manager)\n            elevation_grid.generateFromRaster(elevation_raster)\n\n            # write out updated parameters\n            project_manager.writeInput(session=db_session,\n                                       directory=gssha_directory,\n                                       name='grid_standard')\n        \"\"\"\n        if not self.projectFile:\n            raise ValueError(\"Must be connected to project file ...\")\n\n        # make sure paths are absolute as the working directory changes\n        elevation_raster = os.path.abspath(elevation_raster)\n        shapefile_path = os.path.abspath(shapefile_path)\n\n        # must match elevation mask grid\n        mask_grid = self.projectFile.getGrid()\n        if out_elevation_grid is None:\n            out_elevation_grid = '{0}.{1}'.format(self.projectFile.name,\n                                                  self.fileExtension)\n\n        elevation_grid = resample_grid(elevation_raster,\n                                       mask_grid,\n                                       resample_method=resample_method,\n                                       as_gdal_grid=True)\n\n        with tmp_chdir(self.projectFile.project_directory):\n            elevation_grid.to_grass_ascii(out_elevation_grid, print_nodata=False)\n\n            # read raster into object\n            if load_raster_to_db:\n                self._load_raster_text(out_elevation_grid)\n\n        self.filename = out_elevation_grid\n        self.projectFile.setCard(\"ELEVATION\", out_elevation_grid, add_quotes=True)\n\n        # find outlet and add slope\n        self.projectFile.findOutlet(shapefile_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Dictionary of keywords/cards and parse function names\n        KEYWORDS = {'CONNECT': spc.connectChunk,\n                    'SJUNC': spc.sjuncChunk,\n                    'SLINK': spc.slinkChunk}\n\n        sjuncs = []\n        slinks = []\n        connections = []\n\n        # Parse file into chunks associated with keywords/cards\n        with open(path, 'r') as f:\n            chunks = pt.chunk(KEYWORDS, f)\n\n        # Parse chunks associated with each key\n        for key, chunkList in iteritems(chunks):\n            # Parse each chunk in the chunk list\n            for chunk in chunkList:\n                # Call chunk specific parsers for each chunk\n                result = KEYWORDS[key](key, chunk)\n\n                # Cases\n                if key == 'CONNECT':\n                    connections.append(result)\n                elif key == 'SJUNC':\n                    sjuncs.append(result)\n                elif key == 'SLINK':\n                    slinks.append(result)\n\n        # Create GSSHAPY objects\n        self._createConnection(connections)\n        self._createSjunc(sjuncs)\n        self._createSlink(slinks)", "response": "This method is called by the file read method in Storm Pipe Network File"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstorm Pipe Network File Write to File Method", "response": "def _write(self, session, openFile, replaceParamFile):\n        \"\"\"\n        Storm Pipe Network File Write to File Method\n        \"\"\"\n        # Retrieve Connection objects and write to file\n        connections = self.connections\n        self._writeConnections(connections=connections,\n                               fileObject=openFile)\n\n        # Retrieve SuperJunction objects and write to file\n        sjuncs = self.superJunctions\n        self._writeSuperJunctions(superJunctions=sjuncs,\n                                  fileObject=openFile)\n\n        # Retrieve SuperLink objects and write to file\n        slinks = self.superLinks\n        self._writeSuperLinks(superLinks=slinks,\n                              fileObject=openFile)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating GSSHAPY Connection Objects Method", "response": "def _createConnection(self, connections):\n        \"\"\"\n        Create GSSHAPY Connection Objects Method\n        \"\"\"\n\n        for c in connections:\n            # Create GSSHAPY Connection object\n            connection = Connection(slinkNumber=c['slinkNumber'],\n                                    upSjuncNumber=c['upSjunc'],\n                                    downSjuncNumber=c['downSjunc'])\n\n            # Associate Connection with StormPipeNetworkFile\n            connection.stormPipeNetworkFile = self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _writeConnections(self, connections, fileObject):\n        for connection in connections:\n            fileObject.write('CONNECT  %s  %s  %s\\n' % (\n                connection.slinkNumber,\n                connection.upSjuncNumber,\n                connection.downSjuncNumber))", "response": "Write Connections to File Method"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites SuperJunctions to File Method Accesses", "response": "def _writeSuperJunctions(self, superJunctions, fileObject):\n        \"\"\"\n        Write SuperJunctions to File Method\n        \"\"\"\n        for sjunc in superJunctions:\n            fileObject.write('SJUNC  %s  %.2f  %.2f  %.6f  %s  %s  %s  %.6f  %.6f\\n' % (\n                sjunc.sjuncNumber,\n                sjunc.groundSurfaceElev,\n                sjunc.invertElev,\n                sjunc.manholeSA,\n                sjunc.inletCode,\n                sjunc.linkOrCellI,\n                sjunc.nodeOrCellJ,\n                sjunc.weirSideLength,\n                sjunc.orificeDiameter))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _writeSuperLinks(self, superLinks, fileObject):\n        for slink in superLinks:\n            fileObject.write('SLINK   %s      %s\\n' % (\n                slink.slinkNumber,\n                slink.numPipes))\n\n            for node in slink.superNodes:\n                fileObject.write('NODE  %s  %.2f  %.2f  %.6f  %s  %s  %s  %.6f  %.6f\\n' % (\n                    node.nodeNumber,\n                    node.groundSurfaceElev,\n                    node.invertElev,\n                    node.manholeSA,\n                    node.nodeInletCode,\n                    node.cellI,\n                    node.cellJ,\n                    node.weirSideLength,\n                    node.orificeDiameter))\n            for pipe in slink.pipes:\n                fileObject.write('PIPE  %s  %s  %.6f  %.6f  %.6f  %.6f  %.2f  %.6f  %.6f\\n' % (\n                    pipe.pipeNumber,\n                    pipe.xSecType,\n                    pipe.diameterOrHeight,\n                    pipe.width,\n                    pipe.slope,\n                    pipe.roughness,\n                    pipe.length,\n                    pipe.conductance,\n                    pipe.drainSpacing))", "response": "Method writes SuperLinks to File Method Method"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eta_hms(seconds, always_show_hours=False, always_show_minutes=False, hours_leading_zero=False):\n    # Convert seconds to other units.\n    final_hours, final_minutes, final_seconds = 0, 0, seconds\n    if final_seconds >= 3600:\n        final_hours = int(final_seconds / 3600.0)\n        final_seconds -= final_hours * 3600\n    if final_seconds >= 60:\n        final_minutes = int(final_seconds / 60.0)\n        final_seconds -= final_minutes * 60\n    final_seconds = int(ceil(final_seconds))\n\n    # Determine which string template to use.\n    if final_hours or always_show_hours:\n        if hours_leading_zero:\n            template = '{hour:02.0f}:{minute:02.0f}:{second:02.0f}'\n        else:\n            template = '{hour}:{minute:02.0f}:{second:02.0f}'\n    elif final_minutes or always_show_minutes:\n        template = '{minute:02.0f}:{second:02.0f}'\n    else:\n        template = '{second:02.0f}'\n\n    return template.format(hour=final_hours, minute=final_minutes, second=final_seconds)", "response": "Converts seconds remaining into a human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eta_letters(seconds, shortest=False, leading_zero=False):\n    if not seconds:\n        return '00s' if leading_zero else '0s'\n\n    # Convert seconds to other units.\n    final_weeks, final_days, final_hours, final_minutes, final_seconds = 0, 0, 0, 0, seconds\n    if final_seconds >= 604800:\n        final_weeks = int(final_seconds / 604800.0)\n        final_seconds -= final_weeks * 604800\n    if final_seconds >= 86400:\n        final_days = int(final_seconds / 86400.0)\n        final_seconds -= final_days * 86400\n    if final_seconds >= 3600:\n        final_hours = int(final_seconds / 3600.0)\n        final_seconds -= final_hours * 3600\n    if final_seconds >= 60:\n        final_minutes = int(final_seconds / 60.0)\n        final_seconds -= final_minutes * 60\n    final_seconds = int(ceil(final_seconds))\n\n    # Handle shortest:\n    if shortest:\n        if final_weeks:\n            formatted = str(final_weeks) + 'w'\n        elif final_days:\n            formatted = str(final_days) + 'd'\n        elif final_hours:\n            formatted = str(final_hours) + 'h'\n        elif final_minutes:\n            formatted = '{0:0{1}d}m'.format(final_minutes, 2 if leading_zero else 1)\n        else:\n            formatted = '{0:0{1}d}s'.format(final_seconds, 2 if leading_zero else 1)\n        return formatted\n\n    # Determine which string template to use.\n    if final_weeks:\n        template = '{0:d}w {1:d}d {2:d}h {3:02d}m {4:02d}s' if leading_zero else '{0}w {1}d {2}h {3}m {4}s'\n    elif final_days:\n        template = '{1:d}d {2:d}h {3:02d}m {4:02d}s' if leading_zero else '{1}d {2}h {3}m {4}s'\n    elif final_hours:\n        template = '{2:d}h {3:02d}m {4:02d}s' if leading_zero else '{2}h {3}m {4}s'\n    elif final_minutes:\n        template = '{3:02d}m {4:02d}s' if leading_zero else '{3}m {4}s'\n    else:\n        template = '{4:02d}s' if leading_zero else '{4}s'\n\n    return template.format(final_weeks, final_days, final_hours, final_minutes, final_seconds)", "response": "Converts seconds remaining into human readable strings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ziegler_nichols(self,ku,tu,control_type='pid'):\n        '''\n            ku = ultimate gain\n            tu = period of oscillation at ultimate gain\n        '''\n        converter = dict(\n            p = lambda ku,tu: (.5*ku, 0, 0),\n            pi = lambda ku,tu: (.45*ku, 1.2*(.45*ku)/tu, 0),\n            pd = lambda ku,tu: (.8*ku, 0, (.8*ku)*tu/8),\n            pid = lambda ku,tu: (.6*ku, 2*(.6*ku)/tu, (.6*ku)*tu/8),\n            pessen = lambda ku,tu: (.7*ku, 2.5*(.7*ku)/tu, 3*(.7*ku)*tu/20),\n            some_overshoot = lambda ku,tu: (.33*ku, 2*(.33*ku)/tu, (.33*ku)*tu/3),\n            no_overshoot = lambda ku,tu: (.2*ku, 2*(.2*ku)/tu, (.2*ku)*tu/3)\n        )\n        self.kp,self.ki,self.kd = converter[control_type.lower()](ku,tu)", "response": "This function is used to set the control_type of the ziegler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef datasetHeaderChunk(key, lines):\n    KEYWORDS = ('DATASET',\n                'OBJTYPE',\n                'VECTYPE',\n                'BEGSCL',\n                'BEGVEC',\n                'OBJID',\n                'ND',\n                'NC',\n                'NAME')\n\n    TYPE_KEYS = ('BEGSCL', 'BEGVEC')\n\n    result = {'type': None,\n              'numberData': None,\n              'numberCells': None,\n              'name': None,\n              'objectID': None,\n              'objectType': None,\n              'vectorType': None}\n\n    chunks = pt.chunk(KEYWORDS, lines)\n\n    for key, chunkList in iteritems(chunks):\n\n        for chunk in chunkList:\n            schunk = pt.splitLine(chunk[0])\n\n            if key == 'ND':\n                result['numberData'] = int(schunk[1])\n\n            elif key == 'NC':\n                result['numberCells'] = int(schunk[1])\n\n            elif key == 'NAME':\n                result['name'] = schunk[1]\n\n            elif key == 'OBJID':\n                result['objectID'] = int(schunk[1])\n\n            elif key == 'OBJTYPE':\n                result['objectType'] = schunk[1]\n\n            elif key == 'VECTYPE':\n                result['vectorType'] = schunk[1]\n\n            elif key in TYPE_KEYS:\n                result['type'] = schunk[0]\n\n    return result", "response": "Process the dataset header chunk"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef datasetScalarTimeStepChunk(lines, numberColumns, numberCells):\n    END_DATASET_TAG = 'ENDDS'\n\n    # Define the result object\n    result = {'iStatus': None,\n              'timestamp': None,\n              'cellArray': None,\n              'rasterText': None}\n\n    # Split the chunks\n    timeStep = pt.splitLine(lines.pop(0))\n\n    # Extract cells, ignoring the status indicators\n    startCellsIndex = numberCells\n\n    # Handle case when status cells are not included (istat = 0)\n    iStatus = int(timeStep[1])\n\n    if iStatus == 0:\n        startCellsIndex = 0\n\n    # Strip off ending dataset tag\n    if END_DATASET_TAG in lines[-1]:\n        lines.pop(-1)\n\n    # Assemble the array string\n    arrayString = '[['\n    columnCounter = 1\n    lenLines = len(lines) - 1\n\n    # Also assemble raster text field to preserve for spatial datasets\n    rasterText = ''\n\n    for index in range(startCellsIndex, len(lines)):\n        # Check columns condition\n        if columnCounter % numberColumns != 0 and index != lenLines:\n            arrayString += lines[index].strip() + ', '\n        elif columnCounter % numberColumns == 0 and index != lenLines:\n            arrayString += lines[index].strip() + '], ['\n        elif index == lenLines:\n            arrayString += lines[index].strip() + ']]'\n\n        # Advance counter\n        columnCounter += 1\n\n        rasterText += lines[index]\n\n    # Get Value Array\n    result['cellArray'] = arrayString\n    result['rasterText'] = rasterText\n\n    # Assign Result\n    result['iStatus'] = iStatus\n    result['timestamp'] = float(timeStep[2])\n\n    return result", "response": "Process the time step chunks for scalar datasets\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_dispatcher(dsp, path):\n    import dill\n    with open(path, 'wb') as f:\n        dill.dump(dsp, f)", "response": "Save a dispatcher object in Python pickle format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_default_values(dsp, path):\n    import dill\n    with open(path, 'wb') as f:\n        dill.dump(dsp.default_values, f)", "response": "Writes Dispatcher default values in Python pickle format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the default values of a node or edge from a file.", "response": "def load_default_values(dsp, path):\n    \"\"\"\n    Load Dispatcher default values in Python pickle format.\n\n    Pickles are a serialized byte stream of a Python object.\n    This format will preserve Python objects used as nodes or edges.\n\n    :param dsp:\n        A dispatcher that identifies the model adopted.\n    :type dsp: schedula.Dispatcher\n\n    :param path:\n        File or filename to write.\n        File names ending in .gz or .bz2 will be uncompressed.\n    :type path: str, file\n\n    .. testsetup::\n        >>> from tempfile import mkstemp\n        >>> file_name = mkstemp()[1]\n\n    Example::\n\n        >>> from schedula import Dispatcher\n        >>> dsp = Dispatcher()\n        >>> dsp.add_data('a', default_value=1)\n        'a'\n        >>> dsp.add_function(function=max, inputs=['a', 'b'], outputs=['c'])\n        'max'\n        >>> save_default_values(dsp, file_name)\n\n        >>> dsp = Dispatcher(dmap=dsp.dmap)\n        >>> load_default_values(dsp, file_name)\n        >>> dsp.dispatch(inputs={'b': 3})['c']\n        3\n    \"\"\"\n    import dill\n    # noinspection PyArgumentList\n    with open(path, 'rb') as f:\n        dsp.__init__(dmap=dsp.dmap, default_values=dill.load(f))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_map(dsp, path):\n    import dill\n    with open(path, 'wb') as f:\n        dill.dump(dsp.dmap, f)", "response": "Save a dispatcher graph in Python pickle format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndivides a file into chunks between the keywords in the list", "response": "def chunk(keywords, lines):\n    \"\"\"\n    Divide a file into chunks between\n    key words in the list\n    \"\"\"\n    chunks = dict()\n    chunk = []\n      \n    # Create an empty dictionary using all the keywords\n    for keyword in keywords:\n        chunks[keyword] = []\n    \n    # Populate dictionary with lists of chunks associated\n    # with the keywords in the list   \n    for line in lines:\n        if line.strip():\n            token = line.split()[0]\n            if token in keywords:\n                chunk = [line]   \n                chunks[token].append(chunk)   \n            else:\n                chunk.append(line)\n\n    return chunks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies global pre - processing to values during reading throughout the project.", "response": "def valueReadPreprocessor(valueString, replaceParamsFile=None):\n    \"\"\"\n    Apply global pre-processing to values during reading throughout the project.\n\n    Args:\n        valueString (str): String representing the value to be preprocessed.\n        replaceParamsFile (gsshapy.orm.ReplaceParamFile, optional): Instance of the replace param file. Required if\n            replacement variables are included in the project.\n\n    Returns:\n        str: Processed value as a string\n    \"\"\"\n    if type(valueString) is bool:\n        log.warning(\"Only numerical variable types can be handled by the valueReadPreprocessor function.\")\n        return valueString\n\n    # Default\n    processedValue = valueString\n\n    # Check for replacement variables\n    if replaceParamsFile is not None and valueString is not None:\n        if '[' in valueString or ']' in valueString:\n            # Set default value\n            processedValue = '{0}'.format(REPLACE_NO_VALUE)\n\n            # Find the matching parameter and return the negative of the id\n            for targetParam in replaceParamsFile.targetParameters:\n                if targetParam.targetVariable == valueString:\n                    processedValue = '{0}'.format(-1 * targetParam.id)\n                    break\n\n    return processedValue"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef valueWritePreprocessor(valueString, replaceParamsFile=None):\n    if type(valueString) is bool:\n        log.warning(\"Only numerical variable types can be handled by the valueReadPreprocessor function.\")\n        return valueString\n\n    # Default\n    variableString = valueString\n\n    # Check for replacement variables\n    if replaceParamsFile is not None:\n        # Set Default\n        if variableString == REPLACE_NO_VALUE:\n            variableString = '[NO_VARIABLE]'\n        else:\n            try:\n                number = int(valueString)\n                if number < 0:\n                    parameterID = number * -1\n\n                    # Find the matching parameter\n                    for targetParam in replaceParamsFile.targetParameters:\n                        if targetParam.id == parameterID:\n                            variableString = targetParam.targetVariable\n                            break\n            except:\n                pass\n\n    return variableString", "response": "This function is used to preprocess a value in the replace param file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, dataset_path):\n        features = self._generate_features(self._feature_extractors)\n        features.to_csv(dataset_path)", "response": "Run all FeatureExtractors and output results to CSV."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_features(self, feature_extractors):\n        results = [pd.DataFrame()]\n        n_ext = len(feature_extractors)\n\n        for i, extractor in enumerate(feature_extractors):\n            log.info(\"generating: '%s' (%d/%d)\", extractor.name, i + 1, n_ext)\n            cached_extractor = self._cache[extractor.name]\n            if extractor.same(cached_extractor):\n                log.info('pulling from cache')\n                extractor = cached_extractor\n            else:\n                log.info('running...')\n                extractor.extract()\n            results.append(extractor.result)\n            if self.cache_path:\n                self._cache[extractor.name] = extractor\n\n        if self.cache_path:\n            with open(self.cache_path, 'wb') as f:\n                pickle.dump(self._cache, f)\n\n        return pd.concat(results, axis=1)", "response": "Run all FeatureExtractors and record results in a key - value format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, directory, filename, session, spatial=False,\n             spatialReferenceID=4236, replaceParamFile=None, **kwargs):\n        \"\"\"\n        Generic read file into database method.\n\n        Args:\n            directory (str): Directory containing the file to be read.\n            filename (str): Name of the file which will be read (e.g.: 'example.prj').\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n            spatial (bool, optional): If True, spatially enabled objects will be read in as PostGIS spatial objects.\n                Defaults to False.\n            spatialReferenceID (int, optional): Integer id of spatial reference system for the model. Required if\n                spatial is True. Defaults to srid 4236.\n            replaceParamFile (:class:`gsshapy.orm.ReplaceParamFile`, optional): ReplaceParamFile instance. Use this if\n                the file you are reading contains replacement parameters.\n        \"\"\"\n\n        # Read parameter derivatives\n        path = os.path.join(directory, filename)\n        filename_split = filename.split('.')\n        name = filename_split[0]\n\n        # Default file extension\n        extension = ''\n\n        if len(filename_split) >= 2:\n            extension = filename_split[-1]\n\n        if os.path.isfile(path):\n            # Add self to session\n            session.add(self)\n\n            # Read\n            self._read(directory, filename, session, path, name, extension,\n                       spatial, spatialReferenceID, replaceParamFile, **kwargs)\n\n            # Commit to database\n            self._commit(session, self.COMMIT_ERROR_MESSAGE)\n        else:\n            # Rollback the session if the file doesn't exist\n            session.rollback()\n\n            # Print warning\n            log.warning('Could not find file named {0}. File not read.'.format(filename))", "response": "Generic read method for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, session, directory, name, replaceParamFile=None, **kwargs):\n\n        # Assemble Path to file\n        name_split = name.split('.')\n        name = name_split[0]\n\n        # Default extension\n        extension = ''\n\n        if len(name_split) >= 2:\n            extension = name_split[-1]\n\n        # Run name preprocessor method if present\n        try:\n            name = self._namePreprocessor(name)\n        except:\n            'DO NOTHING'\n\n        if extension == '':\n            filename = '{0}.{1}'.format(name, self.fileExtension)\n        else:\n            filename = '{0}.{1}'.format(name, extension)\n\n        filePath = os.path.join(directory, filename)\n\n        with io_open(filePath, 'w') as openFile:\n            # Write Lines\n            self._write(session=session,\n                        openFile=openFile,\n                        replaceParamFile=replaceParamFile,\n                        **kwargs)", "response": "Writes a new entry to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):", "response": "Private method that reads the file object specified by the user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef svn_versions_from_vcs(tag_prefix, root, verbose=False):\n\n    if not os.path.exists(os.path.join(root, '.svn')):\n        if verbose:\n            print(\"no .svn in %s.\" % root)\n        return {}\n\n    current_module = sys.modules[__name__]\n\n    # If we're running from _version.py .\n    tag_url = getattr(current_module, 'svn_tag_url', None)\n    \n    # If we're running from versioneer.py .\n    if tag_url is None:\n        vcs_settings = getattr(current_module, 'vcs_settings', None)\n        if vcs_settings is not None and \\\n           'svn' in vcs_settings and \\\n           'tag_url' in vcs_settings['svn']:\n            tag_url = vcs_settings['svn']['tag_url']\n\n    if tag_url is None:\n        raise ValueError(\"Please define VCS-specific 'tag_url' setting for \"\n                         \"'svn' within 'versioneer'.\")\n\n    svn_commands = ['svn']\n    info_xml = run_command(svn_commands, ['ls', '--xml', tag_url], cwd=root)\n# TODO(dustin): This should raise an EnvironmentError upon failure.\n    if info_xml is None:\n        print(\"Error accessing Subversion for latest version.\")\n        return {}\n\n    (releases, latest_revision) = svn_parse_tag_xml(info_xml)\n\n    release_info = releases[latest_revision]\n    release_name = release_info['name']\n    versions = { 'default': release_name, \n                 'version': release_name, \n                 'full': release_name }\n\n# Examples of strings returned by Git.\n#\n#    versions[\"closest_tag\"]\n#    versions[\"distance\"]\n#    versions[\"short_revisionid\"]\n#    versions[\"dirty\"]\n#    versions[\"pep440\"]\n#    versions[\"describe\"]\n#    versions[\"default\"]\n#    versions[\"dash_dirty\"]\n#    versions[\"closest_tag_or_zero\"]\n#    versions[\"dash_distance\"]\n\n    return versions", "response": "Return a dictionary of values derived directly from the VCS. This is the first attempt to find information by get_versions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_versions(default=DEFAULT, verbose=False):\n\n\n    # returns dict with two keys: 'version' and 'full'\n    assert versionfile_source is not None, \"please set versioneer.versionfile_source\"\n    assert tag_prefix is not None, \"please set versioneer.tag_prefix\"\n    assert parentdir_prefix is not None, \"please set versioneer.parentdir_prefix\"\n    assert VCS is not None, \"please set versioneer.VCS\"\n\n    # I am in versioneer.py, which must live at the top of the source tree,\n    # which we use to compute the root directory. py2exe/bbfreeze/non-CPython\n    # don't have __file__, in which case we fall back to sys.argv[0] (which\n    # ought to be the setup.py script). We prefer __file__ since that's more\n    # robust in cases where setup.py was invoked in some weird way (e.g. pip)\n    root = get_root()\n    versionfile_abs = os.path.join(root, versionfile_source)\n\n    # extract version from first of _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    # Try to get the version info from the VCS-specific replacement keywords.\n\n    get_keywords_f = vcs_function(VCS, \"get_keywords\")\n    versions_from_keywords_f = vcs_function(VCS, \"versions_from_keywords\")\n    if get_keywords_f and versions_from_keywords_f:\n        vcs_keywords = get_keywords_f(versionfile_abs)\n        ver = versions_from_keywords_f(vcs_keywords, tag_prefix)\n        if ver:\n            if verbose: print(\"got version from expanded keyword %s\" % ver)\n            return ver\n\n    # Try to get the version info from _version.py .\n\n    ver = versions_from_file(versionfile_abs)\n    if ver:\n        if verbose: print(\"got version from file %s %s\" % (versionfile_abs,ver))\n        return ver\n\n    # Try to get the version info from the VCS, directly.\n\n    versions_from_vcs_f = vcs_function(VCS, \"versions_from_vcs\")\n    if versions_from_vcs_f:\n        ver = versions_from_vcs_f(tag_prefix, root, verbose)\n        if ver:\n            if verbose: print(\"got version from VCS %s\" % ver)\n            return ver\n\n    # Try to get the version info from the directory's naming.\n\n    ver = versions_from_parentdir(parentdir_prefix, root, verbose)\n    if ver:\n        if verbose: print(\"got version from parentdir %s\" % ver)\n        return ver\n\n    if verbose: print(\"got version from default %s\" % default)\n    return default", "response": "This function returns a dict with two keys version and full version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n\n        print(\" creating %s\" % versionfile_source)\n        with open(versionfile_source, \"w\") as f:\n            f.write(get_vcs_code())\n\n        ipy = os.path.join(os.path.dirname(versionfile_source), \"__init__.py\")\n        try:\n            with open(ipy, \"r\") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = \"\"\n        if INIT_PY_SNIPPET not in old:\n            print(\" appending to %s\" % ipy)\n            with open(ipy, \"a\") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print(\" %s unmodified\" % ipy)\n\n        # Make sure both the top-level \"versioneer.py\" and versionfile_source\n        # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n        # they'll be copied into source distributions. Pip won't be able to\n        # install the package without this.\n        manifest_in = os.path.join(get_root(), \"MANIFEST.in\")\n        simple_includes = set()\n        try:\n            with open(manifest_in, \"r\") as f:\n                for line in f:\n                    if line.startswith(\"include \"):\n                        for include in line.split()[1:]:\n                            simple_includes.add(include)\n        except EnvironmentError:\n            pass\n        # That doesn't cover everything MANIFEST.in can do\n        # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n        # it might give some false negatives. Appending redundant 'include'\n        # lines is safe, though.\n        if \"versioneer.py\" not in simple_includes:\n            print(\" appending 'versioneer.py' to MANIFEST.in\")\n            with open(manifest_in, \"a\") as f:\n                f.write(\"include versioneer.py\\n\")\n        else:\n            print(\" 'versioneer.py' already in MANIFEST.in\")\n        if versionfile_source not in simple_includes:\n            print(\" appending versionfile_source ('%s') to MANIFEST.in\" %\n                  versionfile_source)\n            with open(manifest_in, \"a\") as f:\n                f.write(\"include %s\\n\" % versionfile_source)\n        else:\n            print(\" versionfile_source already in MANIFEST.in\")\n\n        # Make VCS-specific changes. For git, this means creating/changing\n        # .gitattributes to mark _version.py for export-time keyword\n        # substitution.\n        \n        do_vcs_install_f = getattr(sys.modules[__name__], VCS + '_do_vcs_install')\n        do_vcs_install_f(manifest_in, versionfile_source, ipy)", "response": "Create the versioneer. py file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cardChunk(key, chunk):\n    for line in chunk:\n        values = []\n        sline = line.strip().split()\n\n        for idx in range(1, len(sline)):\n            values.append(sline[idx])\n\n    return {'card': sline[0],\n            'values': values}", "response": "Parse a card chunk into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a Card Chunk Method", "response": "def connectChunk(key, chunk):\n    \"\"\"\n    Parse Card Chunk Method\n    \"\"\"\n    upLinks = []\n    schunk = chunk[0].strip().split()\n\n    for idx in range(4, len(schunk)):\n        upLinks.append(schunk[idx])\n\n    result = {'link': schunk[1],\n              'downLink': schunk[2],\n              'numUpLinks': schunk[3],\n              'upLinks': upLinks}\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the STRUCTURE LINK Method and returns the structureLink object.", "response": "def structureLink(lines):\n    \"\"\"\n    Parse STRUCTURE LINK Method\n    \"\"\"\n    # Constants\n    KEYWORDS = ('LINK',\n                'STRUCTURE',\n                'NUMSTRUCTS',\n                'STRUCTTYPE')\n\n    WEIR_KEYWORDS = ('STRUCTTYPE',\n                     'CREST_LENGTH',\n                     'CREST_LOW_ELEV',\n                     'DISCHARGE_COEFF_FORWARD',\n                     'DISCHARGE_COEFF_REVERSE',\n                     'CREST_LOW_LOC',\n                     'STEEP_SLOPE',\n                     'SHALLOW_SLOPE')\n\n    CULVERT_KEYWORDS = ('STRUCTTYPE',\n                        'UPINVERT',\n                        'DOWNINVERT',\n                        'INLET_DISCH_COEFF',\n                        'REV_FLOW_DISCH_COEFF',\n                        'SLOPE',\n                        'LENGTH',\n                        'ROUGH_COEFF',\n                        'DIAMETER',\n                        'WIDTH',\n                        'HEIGHT')\n\n    WEIRS = ('WEIR', 'SAG_WEIR')\n\n    CULVERTS = ('ROUND_CULVERT', 'RECT_CULVERT')\n\n    CURVES = ('RATING_CURVE', 'SCHEDULED_RELEASE', 'RULE_CURVE')\n\n    result = {'type': 'STRUCTURE',\n              'header': {'link': None,\n                         'numstructs': None},\n              'structures':[]}\n\n    chunks = pt.chunk(KEYWORDS, lines)\n\n    # Parse chunks associated with each key\n    for key, chunkList in iteritems(chunks):\n        # Parse each chunk in the chunk list\n        for chunk in chunkList:\n            # Cases\n            if key == 'STRUCTTYPE':\n                # Structure handler\n                structType = chunk[0].strip().split()[1]\n\n                # Cases\n                if structType in WEIRS:\n\n                    weirResult = {'structtype': None,\n                                  'crest_length': None,\n                                  'crest_low_elev': None,\n                                  'discharge_coeff_forward': None,\n                                  'discharge_coeff_reverse': None,\n                                  'crest_low_loc': None,\n                                  'steep_slope': None,\n                                  'shallow_slope': None}\n\n                    # Weir type structures handler\n                    result['structures'].append(structureChunk(WEIR_KEYWORDS, weirResult, chunk))\n\n                elif structType in CULVERTS:\n\n                    culvertResult = {'structtype': None,\n                                     'upinvert': None,\n                                     'downinvert': None,\n                                     'inlet_disch_coeff': None,\n                                     'rev_flow_disch_coeff': None,\n                                     'slope': None,\n                                     'length': None,\n                                     'rough_coeff': None,\n                                     'diameter': None,\n                                     'width': None,\n                                     'height': None}\n\n                    # Culvert type structures handler\n                    result['structures'].append(structureChunk(CULVERT_KEYWORDS, culvertResult, chunk))\n\n                elif structType in CURVES:\n                    # Curve type handler\n                    pass\n            elif key != 'STRUCTURE':\n                # All other variables header\n                result['header'][key.lower()] = chunk[0].strip().split()[1]\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing Cross Section Links Method", "response": "def xSectionLink(lines):\n    \"\"\"\n    Parse Cross Section Links Method\n    \"\"\"\n    # Constants\n    KEYWORDS = ('LINK',\n                'DX',\n                'TRAPEZOID',\n                'TRAPEZOID_ERODE',\n                'TRAPEZOID_SUBSURFACE',\n                'ERODE_TRAPEZOID',\n                'ERODE_SUBSURFACE',\n                'SUBSURFACE_TRAPEZOID',\n                'SUBSURFACE_ERODE',\n                'TRAPEZOID_ERODE_SUBSURFACE',\n                'TRAPEZOID_SUBSURFACE_ERODE',\n                'ERODE_TRAPEZOID_SUBSURFACE',\n                'ERODE_SUBSURFACE_TRAPEZOID',\n                'SUBSURFACE_TRAPEZOID_ERODE',\n                'SUBSURFACE_ERODE_TRAPEZOID',\n                'BREAKPOINT',\n                'BREAKPOINT_ERODE',\n                'BREAKPOINT_SUBSURFACE',\n                'ERODE_BREAKPOINT',\n                'ERODE_SUBSURFACE',\n                'SUBSURFACE_BREAKPOINT',\n                'SUBSURFACE_ERODE',\n                'BREAKPOINT_ERODE_SUBSURFACE',\n                'BREAKPOINT_SUBSURFACE_ERODE',\n                'ERODE_BREAKPOINT_SUBSURFACE',\n                'ERODE_SUBSURFACE_BREAKPOINT',\n                'SUBSURFACE_BREAKPOINT_ERODE',\n                'SUBSURFACE_ERODE_BREAKPOINT',\n                'TRAP',\n                'TRAP_ERODE',\n                'TRAP_SUBSURFACE',\n                'ERODE_TRAP',\n                'ERODE_SUBSURFACE',\n                'SUBSURFACE_TRAP',\n                'SUBSURFACE_ERODE',\n                'TRAP_ERODE_SUBSURFACE',\n                'TRAP_SUBSURFACE_ERODE',\n                'ERODE_TRAP_SUBSURFACE',\n                'ERODE_SUBSURFACE_TRAP',\n                'SUBSURFACE_TRAP_ERODE',\n                'SUBSURFACE_ERODE_TRAP',\n                'NODES',\n                'NODE',\n                'XSEC')\n\n\n\n    ERODE = ('TRAPEZOID_ERODE',\n             'TRAP_ERODE',\n             'TRAP_SUBSURFACE_ERODE',\n             'TRAP_ERODE_SUBSURFACE',\n             'BREAKPOINT_ERODE',\n             'TRAPEZOID_SUBSURFACE_ERODE',\n             'TRAPEZOID_ERODE_SUBSURFACE',\n             'BREAKPOINT_SUBSURFACE_ERODE',\n             'BREAKPOINT_ERODE_SUBSURFACE')\n\n    SUBSURFACE = ('TRAPEZOID_SUBSURFACE',\n                  'TRAP_SUBSURFACE',\n                  'TRAP_SUBSURFACE_ERODE',\n                  'TRAP_ERODE_SUBSURFACE',\n                  'BREAKPOINT_SUBSURFACE',\n                  'TRAPEZOID_SUBSURFACE_ERODE',\n                  'TRAPEZOID_ERODE_SUBSURFACE',\n                  'BREAKPOINT_SUBSURFACE_ERODE',\n                  'BREAKPOINT_ERODE_SUBSURFACE')\n\n    result  =  {'type': 'XSEC',\n                'header': {'link': None,\n                           'dx': None,\n                           'xSecType': None,\n                           'nodes': None,\n                           'erode': False,\n                           'subsurface': False},\n                'xSection': None,\n                'nodes': []}\n\n    chunks = pt.chunk(KEYWORDS, lines)\n\n    # Parse chunks associated with each key\n    for key, chunkList in iteritems(chunks):\n        # Parse each chunk in the chunk list\n        for chunk in chunkList:\n            # Cases\n            if key == 'NODE':\n                # Extract node x and y\n                result['nodes'].append(nodeChunk(chunk))\n\n            elif key == 'XSEC':\n                # Extract cross section information\n                result['xSection'] = xSectionChunk(chunk)\n\n            elif ('TRAPEZOID' in key) or ('BREAKPOINT' in key) or ('TRAP' in key):\n                # Cross section type handler\n                result['header']['xSecType'] = key\n\n            elif key in ERODE:\n                # Erode handler\n                result['header']['erode'] = True\n\n            elif key in SUBSURFACE:\n                # Subsurface handler\n                result['header']['subsurface'] = True\n\n            else:\n                # Extract all other variables into header\n                result['header'][key.lower()] = chunk[0].strip().split()[1]\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing RESERVOIR Link Method", "response": "def reservoirLink(lines):\n    \"\"\"\n    Parse RESERVOIR Link Method\n    \"\"\"\n    # Constants\n    KEYWORDS = ('LINK',\n                'RESERVOIR',\n                'RES_MINWSE',\n                'RES_INITWSE',\n                'RES_MAXWSE',\n                'RES_NUMPTS',\n                'LAKE',\n                'MINWSE',\n                'INITWSE',\n                'MAXWSE',\n                'NUMPTS')\n\n    result  =  {'header': {'link': None,\n                           'res_minwse': None,\n                           'res_initwse': None,\n                           'res_maxwse': None,\n                           'res_numpts': None,\n                           'minwse': None,\n                           'initwse': None,\n                           'maxwse': None,\n                           'numpts': None},\n                'type': None,\n                'points': []}\n\n    pair = {'i': None,\n            'j': None}\n\n    # Rechunk the chunk\n    chunks = pt.chunk(KEYWORDS, lines)\n\n    # Parse chunks associated with each key\n    for key, chunkList in iteritems(chunks):\n        # Parse each chunk in the chunk list\n        for chunk in chunkList:\n            schunk = chunk[0].strip().split()\n\n\n            # Cases\n            if key in ('NUMPTS', 'RES_NUMPTS'):\n                # Points handler\n                result['header'][key.lower()] = schunk[1]\n\n                # Parse points\n                for idx in range(1, len(chunk)):\n                    schunk = chunk[idx].strip().split()\n\n                    for count, ordinate in enumerate(schunk):\n                        # Divide ordinates into ij pairs\n                        if (count % 2) == 0:\n                            pair['i'] = ordinate\n                        else:\n                            pair['j'] = ordinate\n                            result['points'].append(pair)\n                            pair = {'i': None,\n                                    'j': None}\n\n            elif key in ('LAKE', 'RESERVOIR'):\n                # Type handler\n                result['type'] = schunk[0]\n            else:\n                # Header variables handler\n                result['header'][key.lower()] = schunk[1]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xSectionChunk(lines):\n    # Constants\n    KEYWORDS = ('MANNINGS_N',\n                'BOTTOM_WIDTH',\n                'BANKFULL_DEPTH',\n                'SIDE_SLOPE',\n                'NPAIRS',\n                'NUM_INTERP',\n                'X1',\n                'ERODE',\n                'MAX_EROSION',\n                'SUBSURFACE',\n                'M_RIVER',\n                'K_RIVER')\n\n    result = {'mannings_n': None,\n              'bottom_width': None,\n              'bankfull_depth': None,\n              'side_slope': None,\n              'npairs': None,\n              'num_interp': None,\n              'erode': False,\n              'subsurface': False,\n              'max_erosion': None,\n              'm_river': None,\n              'k_river': None,\n              'breakpoints': []}\n\n    chunks = pt.chunk(KEYWORDS, lines)\n\n    # Parse chunks associated with each key\n    for key, chunkList in iteritems(chunks):\n        # Parse each chunk in the chunk list\n        for chunk in chunkList:\n            # Strip and split the line (only one item in each list)\n            schunk = chunk[0].strip().split()\n\n            # Cases\n            if key == 'X1':\n                # Extract breakpoint XY pairs\n                x = schunk[1]\n                y = schunk[2]\n                result['breakpoints'].append({'x': x, 'y': y})\n\n            if key in ('SUBSURFACE', 'ERODE'):\n                # Set booleans\n                result[key.lower()] = True\n\n            else:\n                # Extract value\n                result[key.lower()] = schunk[1]\n    return result", "response": "Parse XSEC Method Chunk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the complete progress bar.", "response": "def bar(self, width, **_):\n        \"\"\"Returns the completed progress bar.\n\n        Positional arguments:\n        width -- the width of the entire bar (including borders).\n        \"\"\"\n        return self.CHAR_LEFT_BORDER + self.CHAR_EMPTY * (width - self._width_offset) + self.CHAR_RIGHT_BORDER"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bar(self, width, **_):\n        width -= self._width_offset\n        self._position += self._direction\n\n        # Change direction.\n        if self._position <= 0 and self._direction < 0:\n            self._position = 0\n            self._direction = 1\n        elif self._position > width:\n            self._position = width - 1\n            self._direction = -1\n\n        final_bar = (\n            self.CHAR_LEFT_BORDER +\n            self.CHAR_EMPTY * self._position +\n            self.CHAR_ANIMATED +\n            self.CHAR_EMPTY * (width - self._position) +\n            self.CHAR_RIGHT_BORDER\n        )\n        return final_bar", "response": "Returns the complete progress bar. Every time this is called the animation moves."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the complete progress bar.", "response": "def bar(self, width, percent=0):\n        \"\"\"Returns the completed progress bar.\n\n        Positional arguments:\n        width -- the width of the entire bar (including borders).\n\n        Keyword arguments:\n        percent -- the percentage to draw.\n        \"\"\"\n        width -= self._width_offset\n        units = int(percent * 0.01 * width)\n        if not units:\n            return self.CHAR_LEFT_BORDER + self.CHAR_EMPTY * width + self.CHAR_RIGHT_BORDER\n\n        final_bar = (\n            self.CHAR_LEFT_BORDER +\n            self.CHAR_FULL * (units - 1) +\n            self.CHAR_LEADING +\n            self.CHAR_EMPTY * (width - units) +\n            self.CHAR_RIGHT_BORDER\n        )\n        return final_bar"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the completed progress bar.", "response": "def bar(self, width, percent=0):\n        \"\"\"Returns the completed progress bar.\n\n        Positional arguments:\n        width -- the width of the entire bar (including borders).\n\n        Keyword arguments:\n        percent -- the percentage to draw.\n        \"\"\"\n        width -= self._width_offset\n        units_float = percent * 0.01 * width\n        if units_float < 0.5:\n            return self.CHAR_LEFT_BORDER + self.CHAR_EMPTY * width + self.CHAR_RIGHT_BORDER\n        units = int(units_float)\n        show_half = units_float - units >= 0.5\n\n        if show_half:\n            final_bar = (\n                self.CHAR_LEFT_BORDER +\n                self.CHAR_FULL * units +\n                self.CHAR_HALF +\n                self.CHAR_EMPTY * (width - units - 1) +\n                self.CHAR_RIGHT_BORDER\n            )\n        else:\n            final_bar = (\n                self.CHAR_LEFT_BORDER +\n                self.CHAR_FULL * units +\n                self.CHAR_EMPTY * (width - units) +\n                self.CHAR_RIGHT_BORDER\n            )\n\n        return final_bar"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read(self, directory, filename, session, path, name, extension,\n              spatial=False, spatialReferenceID=4236, replaceParamFile=None,\n              readIndexMaps=True):\n        \"\"\"\n        Mapping Table Read from File Method\n        \"\"\"\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Dictionary of keywords/cards and parse function names\n        KEYWORDS = {'INDEX_MAP': mtc.indexMapChunk,\n                    'ROUGHNESS': mtc.mapTableChunk,\n                    'INTERCEPTION': mtc.mapTableChunk,\n                    'RETENTION': mtc.mapTableChunk,\n                    'GREEN_AMPT_INFILTRATION': mtc.mapTableChunk,\n                    'GREEN_AMPT_INITIAL_SOIL_MOISTURE': mtc.mapTableChunk,\n                    'RICHARDS_EQN_INFILTRATION_BROOKS': mtc.mapTableChunk,\n                    'RICHARDS_EQN_INFILTRATION_HAVERCAMP': mtc.mapTableChunk,\n                    'EVAPOTRANSPIRATION': mtc.mapTableChunk,\n                    'WELL_TABLE': mtc.mapTableChunk,\n                    'OVERLAND_BOUNDARY': mtc.mapTableChunk,\n                    'TIME_SERIES_INDEX': mtc.mapTableChunk,\n                    'GROUNDWATER': mtc.mapTableChunk,\n                    'GROUNDWATER_BOUNDARY': mtc.mapTableChunk,\n                    'AREA_REDUCTION': mtc.mapTableChunk,\n                    'WETLAND_PROPERTIES': mtc.mapTableChunk,\n                    'MULTI_LAYER_SOIL': mtc.mapTableChunk,\n                    'SOIL_EROSION_PROPS': mtc.mapTableChunk,\n                    'CONTAMINANT_TRANSPORT': mtc.contamChunk,\n                    'SEDIMENTS': mtc.sedimentChunk}\n\n        indexMaps = dict()\n        mapTables = []\n\n        # Parse file into chunks associated with keywords/cards\n        with io_open(path, 'r') as f:\n            chunks = pt.chunk(KEYWORDS, f)\n\n        # Parse chunks associated with each key\n        for key, chunkList in iteritems(chunks):\n            # Parse each chunk in the chunk list\n            for chunk in chunkList:\n                # Call chunk specific parsers for each chunk\n                result = KEYWORDS[key](key, chunk)\n\n                # Index Map handler\n                if key == 'INDEX_MAP':\n\n                    # Create GSSHAPY IndexMap object from result object\n                    indexMap = IndexMap(name=result['idxName'])\n\n                    # Dictionary used to map index maps to mapping tables\n                    indexMaps[result['idxName']] = indexMap\n\n                    # Associate IndexMap with MapTableFile\n                    indexMap.mapTableFile = self\n\n                    if readIndexMaps:\n                        # Invoke IndexMap read method\n                        indexMap.read(directory=directory, filename=result['filename'], session=session,\n                                      spatial=spatial, spatialReferenceID=spatialReferenceID)\n                    else:\n                        # add path to file\n                        indexMap.filename = result['filename']\n\n                # Map Table handler\n                else:\n                    # Create a list of all the map tables in the file\n                    if result:\n                        mapTables.append(result)\n\n        # Create GSSHAPY ORM objects with the resulting objects that are\n        # returned from the parser functions\n        self._createGsshaPyObjects(mapTables, indexMaps, replaceParamFile, directory, session, spatial, spatialReferenceID)", "response": "Read from File Method"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmap Table Write to File Method", "response": "def _write(self, session, openFile, replaceParamFile=None, writeIndexMaps=True):\n        \"\"\"\n        Map Table Write to File Method\n        \"\"\"\n        # Extract directory\n        directory = os.path.split(openFile.name)[0]\n\n        # Derive a Unique Set of Contaminants\n        for mapTable in self.getOrderedMapTables(session):\n            if mapTable.name == 'CONTAMINANT_TRANSPORT':\n                contaminantList = []\n                for mtValue in mapTable.values:\n                    if mtValue.contaminant not in contaminantList:\n                        contaminantList.append(mtValue.contaminant)\n\n                contaminants = sorted(contaminantList, key=lambda x: (x.indexMap.name, x.name))\n\n        # Write first line to file\n        openFile.write('GSSHA_INDEX_MAP_TABLES\\n')\n\n        # Write list of index maps\n        for indexMap in self.indexMaps:\n            # Write to map table file\n            openFile.write('INDEX_MAP%s\"%s\" \"%s\"\\n' % (' ' * 16, indexMap.filename, indexMap.name))\n\n            if writeIndexMaps:\n                # Initiate index map write\n                indexMap.write(directory, session=session)\n\n        for mapTable in self.getOrderedMapTables(session):\n            if mapTable.name == 'SEDIMENTS':\n                self._writeSedimentTable(session=session,\n                                         fileObject=openFile,\n                                         mapTable=mapTable,\n                                         replaceParamFile=replaceParamFile)\n            elif mapTable.name == 'CONTAMINANT_TRANSPORT':\n                self._writeContaminantTable(session=session,\n                                            fileObject=openFile,\n                                            mapTable=mapTable,\n                                            contaminants=contaminants,\n                                            replaceParamFile=replaceParamFile)\n            else:\n                self._writeMapTable(session=session,\n                                    fileObject=openFile,\n                                    mapTable=mapTable,\n                                    replaceParamFile=replaceParamFile)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getOrderedMapTables(self, session):\n        return session.query(MapTable).filter(MapTable.mapTableFile == self).order_by(MapTable.name).all()", "response": "Retrieve the map tables ordered by name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting duplicate map table if it exists", "response": "def deleteMapTable(self, name, session):\n        \"\"\"\n        Remove duplicate map table if it exists\n        \"\"\"\n\n        duplicate_map_tables = session.query(MapTable).filter(MapTable.mapTableFile == self).filter(MapTable.name == name).all()\n        for duplicate_map_table in duplicate_map_tables:\n            if duplicate_map_table.indexMap:\n                session.delete(duplicate_map_table.indexMap)\n            session.delete(duplicate_map_table)\n            session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating GSSHAPY ORM Objects Method Method", "response": "def _createGsshaPyObjects(self, mapTables, indexMaps, replaceParamFile, directory, session, spatial, spatialReferenceID):\n        \"\"\"\n        Create GSSHAPY Mapping Table ORM Objects Method\n        \"\"\"\n        for mt in mapTables:\n            # Create GSSHAPY MapTable object\n            try:\n                # Make sure the index map name listed with the map table is in the list of\n                # index maps read from the top of the mapping table file (Note that the index maps for the sediment\n                # and contaminant tables will have names of None, so we skip these cases.\n                if mt['indexMapName'] is not None:\n                    indexMaps[mt['indexMapName']]\n\n                mapTable = MapTable(name=mt['name'],\n                                    numIDs=mt['numVars']['NUM_IDS'],\n                                    maxNumCells=mt['numVars']['MAX_NUMBER_CELLS'],\n                                    numSed=mt['numVars'].get('NUM_SED'),\n                                    numContam=mt['numVars'].get('NUM_CONTAM'),\n                                    maxSoilID=mt['numVars'].get('MAX_SOIL_ID'))\n\n                # Associate MapTable with this MapTableFile and IndexMaps\n                mapTable.mapTableFile = self\n\n                ## NOTE: Index maps are associated wth contaminants for CONTAMINANT_TRANSPORT map\n                ## tables. The SEDIMENTS map table are associated with index maps via the\n                ## SOIL_EROSION_PROPS map table.\n                if mt['indexMapName']:\n                    mapTable.indexMap = indexMaps[mt['indexMapName']]\n\n\n                # CONTAMINANT_TRANSPORT map table handler\n                if mt['name'] == 'CONTAMINANT_TRANSPORT':\n                    for contam in mt['contaminants']:\n                        # Preprocess the contaminant output paths to be relative\n                        outputBaseFilename = self._preprocessContaminantOutFilePath(contam['outPath'])\n\n                        # Initialize GSSHAPY MTContaminant object\n                        contaminant = MTContaminant(name=contam['name'],\n                                                    outputFilename=outputBaseFilename,\n                                                    precipConc=vrp(contam['contamVars']['PRECIP_CONC'], replaceParamFile),\n                                                    partition=vrp(contam['contamVars']['PARTITION'], replaceParamFile),\n                                                    numIDs=contam['contamVars']['NUM_IDS'])\n\n                        # Associate MTContaminant with appropriate IndexMap\n                        indexMap = indexMaps[contam['indexMapName']]\n                        contaminant.indexMap = indexMap\n\n                        self._createValueObjects(contam['valueList'], contam['varList'], mapTable, indexMap,\n                                                 contaminant, replaceParamFile)\n\n                        # Read any output files if they are present\n                        self._readContaminantOutputFiles(directory, outputBaseFilename, session, spatial, spatialReferenceID)\n\n                # SEDIMENTS map table handler\n                elif mt['name'] == 'SEDIMENTS':\n                    for line in mt['valueList']:\n                        # Create GSSHAPY MTSediment object\n                        sediment = MTSediment(description=line[0],\n                                              specificGravity=vrp(line[1], replaceParamFile),\n                                              particleDiameter=vrp(line[2], replaceParamFile),\n                                              outputFilename=line[3])\n\n                        # Associate the MTSediment with the MapTable\n                        sediment.mapTable = mapTable\n\n                # All other map table handler\n                else:\n                    indexMap = indexMaps[mt['indexMapName']]\n\n                    # Create MTValue and MTIndex objects\n                    self._createValueObjects(mt['valueList'], mt['varList'], mapTable, indexMap, None, replaceParamFile)\n\n            except KeyError:\n                log.info(('Index Map \"%s\" for Mapping Table \"%s\" not found in list of index maps in the mapping '\n                          'table file. The Mapping Table was not read into the database.') % (\n                          mt['indexMapName'], mt['name']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _createValueObjects(self, valueList, varList, mapTable, indexMap, contaminant, replaceParamFile):\n        def assign_values_to_table(value_list, layer_id):\n            for i, value in enumerate(value_list):\n                value = vrp(value, replaceParamFile)\n                # Create MTValue object and associate with MTIndex and MapTable\n                mtValue = MTValue(variable=varList[i], value=float(value))\n                mtValue.index = mtIndex\n                mtValue.mapTable = mapTable\n                mtValue.layer_id = layer_id\n\n                # MTContaminant handler (associate MTValue with MTContaminant)\n                if contaminant:\n                    mtValue.contaminant = contaminant\n\n        for row in valueList:\n            # Create GSSHAPY MTIndex object and associate with IndexMap\n            mtIndex = MTIndex(index=row['index'], description1=row['description1'], description2=row['description2'])\n            mtIndex.indexMap = indexMap\n            if len(np.shape(row['values'])) == 2:\n                # this is for ids with multiple layers\n                for layer_id, values in enumerate(row['values']):\n                    assign_values_to_table(values, layer_id)\n            else:\n                assign_values_to_table(row['values'], 0)", "response": "Populate GSSHAPY MTValue and MTIndex Objects Method\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _readContaminantOutputFiles(self, directory, baseFileName, session, spatial, spatialReferenceID):\n        if not os.path.isdir(directory):\n            return\n        if baseFileName == '':\n            return\n\n        # Look for channel output files denoted by the \".chan\" after the base filename\n        chanBaseFileName = '.'.join([baseFileName, 'chan'])\n\n        # Get contents of directory\n        directoryList = os.listdir(directory)\n\n        # Compile a list of files with \"basename.chan\" in them\n        chanFiles = []\n        for thing in directoryList:\n            if chanBaseFileName in thing:\n                chanFiles.append(thing)\n\n        # Assume all \"chan\" files are link node dataset files and try to read them\n        for chanFile in chanFiles:\n            linkNodeDatasetFile = LinkNodeDatasetFile()\n            linkNodeDatasetFile.projectFile = self.projectFile\n\n            try:\n                linkNodeDatasetFile.read(directory=directory,\n                                         filename=chanFile,\n                                         session=session,\n                                         spatial=spatial,\n                                         spatialReferenceID=spatialReferenceID)\n            except:\n                log.warning('Attempted to read Contaminant Transport Output file {0}, but failed.'.format(chanFile))", "response": "Read any contaminant output files in the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites Generic Map Table Method This method writes a mapping table in the generic format to file. The method will handle both empty and filled cases of generic formatted mapping tables. session = SQLAlchemy session object for retrieving data from the database fileObject = The file object to write to mapTable = The GSSHAPY MapTable object to write", "response": "def _writeMapTable(self, session, fileObject, mapTable, replaceParamFile):\n        \"\"\"\n        Write Generic Map Table Method\n\n        This method writes a mapping table in the generic format to file. The method will handle\n        both empty and filled cases of generic formatted mapping tables.\n\n        session = SQLAlchemy session object for retrieving data from the database\n        fileObject = The file object to write to\n        mapTable = The GSSHAPY MapTable object to write\n        \"\"\"\n\n        # Write mapping name\n        fileObject.write('%s \"%s\"\\n' % (mapTable.name, mapTable.indexMap.name))\n\n        # Write mapping table global variables\n        if mapTable.numIDs:\n            fileObject.write('NUM_IDS %s\\n' % (mapTable.numIDs))\n\n        if mapTable.maxNumCells:\n            fileObject.write('MAX_NUMBER_CELLS %s\\n' % (mapTable.maxNumCells))\n\n        if mapTable.numSed:\n            fileObject.write('NUM_SED %s\\n' % (mapTable.numSed))\n\n        if mapTable.maxSoilID:\n            fileObject.write('MAX_SOIL_ID %s\\n' % (mapTable.maxSoilID))\n\n        # Write value lines from the database\n        self._writeValues(session, fileObject, mapTable, None, replaceParamFile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _writeContaminantTable(self, session, fileObject, mapTable, contaminants, replaceParamFile):\n        # Write the contaminant mapping table header\n        fileObject.write('%s\\n' % (mapTable.name))\n        fileObject.write('NUM_CONTAM %s\\n' % (mapTable.numContam))\n\n        # Write out each contaminant and it's values\n        for contaminant in contaminants:\n            fileObject.write(\n                '\"%s\"  \"%s\"  %s\\n' % (contaminant.name, contaminant.indexMap.name, contaminant.outputFilename))\n\n            # Add trailing zeros to values / replacement parameter\n            precipConcString = vwp(contaminant.precipConc, replaceParamFile)\n            partitionString = vwp(contaminant.partition, replaceParamFile)\n            try:\n                precipConc = '%.2f' % precipConcString\n            except:\n                precipConc = '%s' % precipConcString\n\n            try:\n                partition = '%.2f' % partitionString\n            except:\n                partition = '%s' % partitionString\n\n            # Write global variables for the contaminant\n            fileObject.write('PRECIP_CONC%s%s\\n' % (' ' * 10, precipConc))\n            fileObject.write('PARTITION%s%s\\n' % (' ' * 12, partition))\n            fileObject.write('NUM_IDS %s\\n' % contaminant.numIDs)\n\n            # Write value lines\n            self._writeValues(session, fileObject, mapTable, contaminant, replaceParamFile)", "response": "This method writes the contaminants mapping table case."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _writeSedimentTable(self, session, fileObject, mapTable, replaceParamFile):\n\n        # Write the sediment mapping table header\n        fileObject.write('%s\\n' % (mapTable.name))\n        fileObject.write('NUM_SED %s\\n' % (mapTable.numSed))\n\n        # Write the value header line\n        fileObject.write(\n            'Sediment Description%sSpec. Grav%sPart. Dia%sOutput Filename\\n' % (' ' * 22, ' ' * 3, ' ' * 5))\n\n        # Retrive the sediment mapping table values\n        sediments = session.query(MTSediment). \\\n            filter(MTSediment.mapTable == mapTable). \\\n            order_by(MTSediment.id). \\\n            all()\n\n        # Write sediments out to file\n        for sediment in sediments:\n            # Determine spacing for aesthetics\n            space1 = 42 - len(sediment.description)\n\n            # Pad values with zeros / Get replacement variable\n            specGravString = vwp(sediment.specificGravity, replaceParamFile)\n            partDiamString = vwp(sediment.particleDiameter, replaceParamFile)\n\n            try:\n                specGrav = '%.6f' % specGravString\n            except:\n                specGrav = '%s' % specGravString\n\n            try:\n                partDiam = '%.6f' % partDiamString\n            except:\n                partDiam = '%s' % partDiamString\n\n\n            fileObject.write('%s%s%s%s%s%s%s\\n' % (\n                sediment.description, ' ' * space1, specGrav, ' ' * 5, partDiam, ' ' * 6, sediment.outputFilename))", "response": "Method writes the sediments special mapping table to fileObject"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _valuePivot(self, session, mapTable, contaminant, replaceParaFile):\n        # Retrieve the indices for the current mapping table and mapping table file\n        indexes = session.query(MTIndex). \\\n            join(MTValue.index). \\\n            filter(MTValue.mapTable == mapTable). \\\n            filter(MTValue.contaminant == contaminant). \\\n            order_by(MTIndex.index). \\\n            all()\n\n        # determine number of layers\n        layer_indices = [0]\n        if mapTable.name in ('MULTI_LAYER_SOIL', 'RICHARDS_EQN_INFILTRATION_BROOKS'):\n            layer_indices = range(3)\n\n        # ----------------------------------------\n        # Construct each line in the mapping table\n        #-----------------------------------------\n\n        # All lines will be compiled into this list\n        lines = []\n        values = {}\n        for idx in indexes:\n            for layer_index in layer_indices:\n                # Retrieve values for the current index\n                values = session.query(MTValue). \\\n                         filter(MTValue.mapTable == mapTable). \\\n                         filter(MTValue.contaminant == contaminant). \\\n                         filter(MTValue.index == idx). \\\n                         filter(MTValue.layer_id == layer_index). \\\n                         order_by(MTValue.id). \\\n                         all()\n\n                # NOTE: The second order_by modifier in the query above handles the special ordering of XSEDIMENT columns\n                # in soil erosion properties table (i.e. these columns must be in the same order as the sediments in the\n                # sediments table. Accomplished by using the sedimentID field). Similarly, the contaminant filter is only\n                # used in the case of the contaminant transport table. Values that don't belong to a contaminant will have\n                # a contaminant attribute equal to None. Compare usage of this function by _writeMapTable and\n                # _writeContaminant.\n\n                #Value string\n                valString = ''\n\n                # Define valString\n                for val in values:\n                    if val.value <= -9999:\n                        continue\n                    # Format value with trailing zeros up to 6 digits\n                    processedValue = vwp(val.value, replaceParaFile)\n                    try:\n                        numString = '%.6f' % processedValue\n                    except:\n                        numString = '%s' % processedValue\n\n                    valString = '%s%s%s' % (valString, numString, ' ' * 3)\n\n                # Determine spacing for aesthetics (so each column lines up)\n                spacing1 = max(1, 6 - len(str(idx.index)))\n                spacing2 = max(1, 40 - len(idx.description1))\n                spacing3 = max(1, 40 - len(idx.description2))\n\n                # Compile each mapping table line\n                if layer_index == 0:\n                    line = '%s%s%s%s%s%s%s\\n' % (\n                        idx.index, ' ' * spacing1, idx.description1, ' ' * spacing2, idx.description2, ' ' * spacing3, valString)\n                else:\n                    num_prepend_spaces = len(str(idx.index)) + spacing1 + len(idx.description1) \\\n                                         + spacing2 + len(idx.description2) + spacing3\n                    line = '{0}{1}\\n'.format(' ' * num_prepend_spaces, valString)\n\n                # Compile each lines into a list\n                lines.append(line)\n\n        #-----------------------------\n        # Define the value header line\n        #-----------------------------\n\n        # Define varString for the header line\n        varString = ''\n\n        # Compile list of variables (from MTValue object list) into a single string of variables\n        for idx, val in enumerate(values):\n            if val.variable == 'XSEDIMENT':  # Special case for XSEDIMENT variable\n                if idx >= len(values) - 1:\n                    varString = '%s%s%s%s' % (varString, mapTable.numSed, ' SEDIMENTS....', ' ' * 2)\n            else:\n                varString = '%s%s%s' % (varString, val.variable, ' ' * 2)\n\n        # Compile the mapping table header\n        header = 'ID%sDESCRIPTION1%sDESCRIPTION2%s%s\\n' % (' ' * 4, ' ' * 28, ' ' * 28, varString)\n\n        # Prepend the header line to the list of lines\n        lines.insert(0, header)\n\n        # Return the list of lines\n        return lines", "response": "This function retrieves the values of a mapping table from the database and pivots them into the format that is required by the mapping table file. This function returns a list of strings that can be printed to the file\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addRoughnessMapFromLandUse(self, name,\n                                         session,\n                                         land_use_grid,\n                                         land_use_to_roughness_table=None,\n                                         land_use_grid_id=None,\n                                         ):\n        \"\"\"\n        Adds a roughness map from land use file\n\n        Example::\n\n            from gsshapy.orm import ProjectFile\n            from gsshapy.lib import db_tools as dbt\n\n            from os import path, chdir\n\n            gssha_directory = '/gsshapy/tests/grid_standard/gssha_project'\n            land_use_grid = 'LC_5min_global_2012.tif'\n            land_use_to_roughness_table = ''/gsshapy/gridtogssha/land_cover/land_cover_glcf_modis.txt'\n\n            # Create Test DB\n            sqlalchemy_url, sql_engine = dbt.init_sqlite_memory()\n\n            # Create DB Sessions\n            db_session = dbt.create_session(sqlalchemy_url, sql_engine)\n\n            # Instantiate GSSHAPY object for reading to database\n            project_manager = ProjectFile()\n\n            # Call read method\n            project_manager.readInput(directory=gssha_directory,\n                                      projectFileName='grid_standard.prj',\n                                      session=db_session)\n\n            project_manager.mapTableFile.addRoughnessMapFromLandUse(\"roughness\",\n                                                                    db_session,\n                                                                    land_use_to_roughness_table,\n                                                                    land_use_grid,\n                                                                    )\n            # WRITE OUT UPDATED GSSHA PROJECT FILE\n            project_manager.writeInput(session=db_session,\n                                       directory=gssha_directory,\n                                       name='grid_standard')\n\n        \"\"\"\n        LAND_USE_GRID_TABLES = {\n                                 'nga'  : 'land_cover_nga.txt',\n                                 'glcf' : 'land_cover_glcf_modis.txt',\n                                 'nlcd' : 'land_cover_nlcd.txt',\n                                }\n\n        # read in table\n        if isinstance(land_use_to_roughness_table, pd.DataFrame):\n            df = land_use_to_roughness_table\n        else:\n            if land_use_to_roughness_table is None:\n                if land_use_grid_id is None:\n                    raise ValueError(\"Must have land_use_to_roughness_table or land_use_grid_id set ...\")\n\n                land_use_to_roughness_table = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                                           '..', 'grid',\n                                                           'land_cover',\n                                                           LAND_USE_GRID_TABLES[land_use_grid_id])\n\n            # make sure paths are absolute as the working directory changes\n            land_use_to_roughness_table = os.path.abspath(land_use_to_roughness_table)\n\n            df = pd.read_table(land_use_to_roughness_table, delim_whitespace=True,\n                               header=None, skiprows=1,\n                               names=('id', 'description', 'roughness'),\n                               dtype={'id':'int', 'description':'str', 'roughness':'float'},\n                               )\n\n        # make sure paths are absolute as the working directory changes\n        land_use_grid = os.path.abspath(land_use_grid)\n\n        # resample land use grid to gssha grid\n        land_use_resampled = resample_grid(land_use_grid,\n                                           self.projectFile.getGrid(),\n                                           resample_method=gdalconst.GRA_NearestNeighbour,\n                                           as_gdal_grid=True)\n\n        unique_land_use_ids = np.unique(land_use_resampled.np_array())\n\n        # only add ids in index map subset\n        df = df[df.id.isin(unique_land_use_ids)]\n\n        # make sure all needed land use IDs exist\n        for land_use_id in unique_land_use_ids:\n            if land_use_id not in df.id.values:\n                raise IndexError(\"Land use ID {0} not found in table.\".format(land_use_id))\n\n        # delete duplicate/old tables with same name if they exist\n        self.deleteMapTable(\"ROUGHNESS\", session)\n\n        # get num ids\n        mapTable = MapTable(name=\"ROUGHNESS\",\n                            numIDs=len(df.index),\n                            maxNumCells=0,\n                            numSed=0,\n                            numContam=0)\n\n        # Create GSSHAPY IndexMap object from result object\n        indexMap = IndexMap(name=name)\n        indexMap.mapTableFile = self\n        mapTable.indexMap = indexMap\n\n        # Associate MapTable with this MapTableFile and IndexMaps\n        mapTable.mapTableFile = self\n        # add values to table\n        for row in df.itertuples():\n            idx = MTIndex(str(row.id), row.description, '')\n            idx.indexMap = indexMap\n            val = MTValue('ROUGH', row.roughness)\n            val.index = idx\n            val.mapTable = mapTable\n\n        # remove MANNING_N card becasue it is mutually exclusive\n        manningn_card = self.projectFile.getCard('MANNING_N')\n        if manningn_card:\n            session.delete(manningn_card)\n            session.commit()\n\n        mapTable.indexMap.filename = '{0}.idx'.format(name)\n        # write file\n        with tmp_chdir(self.projectFile.project_directory):\n            land_use_resampled.to_grass_ascii(mapTable.indexMap.filename,\n                                              print_nodata=False)\n\n        # update project card\n        if not self.projectFile.getCard('MAPPING_TABLE'):\n            self.projectFile.setCard('MAPPING_TABLE',\n                                     '{0}.cmt'.format(self.projectFile.name),\n                                     add_quotes=True)", "response": "Add a roughness map from land use file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates wildcards set with the input data nodes that are also outputs.", "response": "def _set_wildcards(self, inputs=None, outputs=None):\n        \"\"\"\n        Update wildcards set with the input data nodes that are also outputs.\n\n        :param inputs:\n            Input data nodes.\n        :type inputs: list[str], iterable, optional\n\n        :param outputs:\n            Ending data nodes.\n        :type outputs: list[str], iterable, optional\n        \"\"\"\n\n        w = self._wildcards = set()  # Clean wildcards.\n\n        if outputs and inputs:\n            node, wi = self.nodes, self._wait_in.get  # Namespace shortcut.\n\n            # Input data nodes that are in output_targets.\n            w_crd = {u: node[u] for u in inputs if u in outputs or wi(u, False)}\n\n            # Data nodes without the wildcard.\n            w.update([k for k, v in w_crd.items() if v.get('wildcard', True)])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef result(self, timeout=None):\n        it, exceptions, future_lists = [], [], []\n        from concurrent.futures import Future, wait as wait_fut\n\n        def update(fut, data, key):\n            if isinstance(fut, Future):\n                it.append((fut, data, key))\n            elif isinstance(fut, AsyncList) and fut not in future_lists:\n                future_lists.append(fut)\n                it.extend([(j, fut, i)\n                           for i, j in enumerate(fut)\n                           if isinstance(j, Future)][::-1])\n\n        for s in self.sub_sol.values():\n            for k, v in list(s.items()):\n                update(v, s, k)\n\n            for d in s.workflow.nodes.values():\n                if 'results' in d:\n                    update(d['results'], d, 'results')\n\n            for d in s.workflow.edges.values():\n                if 'value' in d:\n                    update(d['value'], d, 'value')\n\n        wait_fut({v[0] for v in it}, timeout)\n\n        for f, d, k in it:\n            try:\n                d[k] = await_result(f, 0)\n            except SkipNode as e:\n                exceptions.append((f, d, k, e.ex))\n                del d[k]\n            except (Exception, ExecutorShutdown, DispatcherAbort) as ex:\n                exceptions.append((f, d, k, ex))\n                del d[k]\n\n        if exceptions:\n            raise exceptions[0][-1]\n        return self", "response": "Set all asynchronous results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sub_dsp_from_workflow(self, sources, reverse=False,\n                                  add_missing=False, check_inputs=True):\n        \"\"\"\n        Returns the sub-dispatcher induced by the workflow from sources.\n\n        The induced sub-dispatcher of the dsp contains the reachable nodes and\n        edges evaluated with breadth-first-search on the workflow graph from\n        source nodes.\n\n        :param sources:\n           Source nodes for the breadth-first-search.\n           A container of nodes which will be iterated through once.\n        :type sources: list[str], iterable\n\n        :param reverse:\n           If True the workflow graph is assumed as reversed.\n        :type reverse: bool, optional\n\n        :param add_missing:\n           If True, missing function' inputs are added to the sub-dispatcher.\n        :type add_missing: bool, optional\n\n        :param check_inputs:\n           If True the missing function' inputs are not checked.\n        :type check_inputs: bool, optional\n\n        :return:\n           A sub-dispatcher.\n        :rtype: schedula.dispatcher.Dispatcher\n        \"\"\"\n        sub_dsp = self.dsp.get_sub_dsp_from_workflow(\n            sources, self.workflow, reverse=reverse, add_missing=add_missing,\n            check_inputs=check_inputs\n        )\n\n        return sub_dsp", "response": "Returns the sub - dispatcher induced by the workflow from sources."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_targets(self):\n\n        if self.outputs:\n\n            targets = self.outputs.copy()  # Namespace shortcut for speed.\n\n            def check_targets(node_id):\n                \"\"\"\n                Terminates ArciDispatch algorithm when all targets have been\n                visited.\n\n                :param node_id:\n                    Data or function node id.\n                :type node_id: str\n\n                :return:\n                    True if all targets have been visited, otherwise False.\n                :rtype: bool\n                \"\"\"\n\n                try:\n                    targets.remove(node_id)  # Remove visited node.\n                    return not targets  # If no targets terminate the algorithm.\n                except KeyError:  # The node is not in the targets set.\n                    return False\n        else:\n            # noinspection PyUnusedLocal\n            def check_targets(node_id):\n                return False\n\n        return check_targets", "response": "Returns a function to terminate the ArciDispatch algorithm when all targets have been visited."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function to stop the search of the investigated node of the ArciDispatch algorithm.", "response": "def _check_cutoff(self):\n        \"\"\"\n        Returns a function to stop the search of the investigated node of the\n        ArciDispatch algorithm.\n\n        :return:\n            A function to stop the search.\n        :rtype: (int | float) -> bool\n        \"\"\"\n\n        if self.cutoff is not None:\n\n            cutoff = self.cutoff  # Namespace shortcut for speed.\n\n            def check_cutoff(distance):\n                \"\"\"\n                Stops the search of the investigated node of the ArciDispatch\n                algorithm.\n\n                :param distance:\n                    Distance from the starting node.\n                :type distance: float, int\n\n                :return:\n                    True if distance > cutoff, otherwise False.\n                :rtype: bool\n                \"\"\"\n\n                return distance > cutoff  # Check cutoff distance.\n\n        else:  # cutoff is None.\n            # noinspection PyUnusedLocal\n            def check_cutoff(distance):\n                return False\n\n        return check_cutoff"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a function to stop the search of the investigated node of the ArciDispatch algorithm.", "response": "def _check_wait_input_flag(self):\n        \"\"\"\n        Returns a function to stop the search of the investigated node of the\n        ArciDispatch algorithm.\n\n        :return:\n            A function to stop the search.\n        :rtype: (bool, str) -> bool\n        \"\"\"\n\n        wf_pred = self._wf_pred  # Namespace shortcuts.\n        pred = {k: set(v).issubset for k, v in self._pred.items()}\n\n        if self._wait_in:\n            we = self._wait_in.get  # Namespace shortcut.\n\n            def check_wait_input_flag(wait_in, n_id):\n                \"\"\"\n                Stops the search of the investigated node of the ArciDispatch\n                algorithm, until all inputs are satisfied.\n\n                :param wait_in:\n                    If True the node is waiting input estimations.\n                :type wait_in: bool\n\n                :param n_id:\n                    Data or function node id.\n                :type n_id: str\n\n                :return:\n                    True if all node inputs are satisfied, otherwise False.\n                :rtype: bool\n                \"\"\"\n\n                # Return true if the node inputs are satisfied.\n                if we(n_id, wait_in):\n                    return not pred[n_id](wf_pred[n_id])\n                return False\n\n        else:\n            def check_wait_input_flag(wait_in, n_id):\n                # Return true if the node inputs are satisfied.\n                return wait_in and not pred[n_id](wf_pred[n_id])\n\n        return check_wait_input_flag"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_node_estimations(self, node_attr, node_id):\n\n        # Get data node estimations.\n        estimations = self._wf_pred[node_id]\n\n        wait_in = node_attr['wait_inputs']  # Namespace shortcut.\n\n        # Check if node has multiple estimations and it is not waiting inputs.\n        if len(estimations) > 1 and not self._wait_in.get(node_id, wait_in):\n            # Namespace shortcuts.\n            dist, edg_length, adj = self.dist, self._edge_length, self.dmap.adj\n\n            est = []  # Estimations' heap.\n\n            for k, v in estimations.items():  # Calculate length.\n                if k is not START:\n                    d = dist[k] + edg_length(adj[k][node_id], node_attr)\n                    heapq.heappush(est, (d, k, v))\n\n            # The estimation with minimum distance from the starting node.\n            estimations = {est[0][1]: est[0][2]}\n\n            # Remove unused workflow edges.\n            self.workflow.remove_edges_from([(v[1], node_id) for v in est[1:]])\n\n        return estimations, wait_in", "response": "Get the data node estimations and wait_inputs flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the node outputs from node inputs.", "response": "def _set_node_output(self, node_id, no_call, next_nds=None, **kw):\n        \"\"\"\n        Set the node outputs from node inputs.\n\n        :param node_id:\n            Data or function node id.\n        :type node_id: str\n\n        :param no_call:\n            If True data node estimation function is not used.\n        :type no_call: bool\n\n        :return:\n            If the output have been evaluated correctly.\n        :rtype: bool\n        \"\"\"\n\n        # Namespace shortcuts.\n        node_attr = self.nodes[node_id]\n        node_type = node_attr['type']\n\n        if node_type == 'data':  # Set data node.\n            return self._set_data_node_output(node_id, node_attr, no_call,\n                                              next_nds, **kw)\n\n        elif node_type == 'function':  # Set function node.\n            return self._set_function_node_output(node_id, node_attr, no_call,\n                                                  next_nds, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the data node output from node estimations.", "response": "def _set_data_node_output(self, node_id, node_attr, no_call, next_nds=None,\n                              **kw):\n        \"\"\"\n        Set the data node output from node estimations.\n\n        :param node_id:\n            Data node id.\n        :type node_id: str\n\n        :param node_attr:\n            Dictionary of node attributes.\n        :type node_attr: dict[str, T]\n\n        :param no_call:\n            If True data node estimations are not used.\n        :type no_call: bool\n\n        :return:\n            If the output have been evaluated correctly.\n        :rtype: bool\n        \"\"\"\n\n        # Get data node estimations.\n        est, wait_in = self._get_node_estimations(node_attr, node_id)\n\n        if not no_call:\n            if node_id is PLOT:\n                est = est.copy()\n                est[PLOT] = {'value': {'obj': self}}\n\n            sf, args = False, ({k: v['value'] for k, v in est.items()},)\n            if not (wait_in or 'function' in node_attr):\n                # Data node that has just one estimation value.\n                sf, args = True, tuple(args[0].values())\n            try:\n                # Final estimation of the node and node status.\n                value = async_thread(self, args, node_attr, node_id, sf, **kw)\n            except SkipNode:\n                return False\n\n            if value is not NONE:  # Set data output.\n                self[node_id] = value\n\n            value = {'value': value}  # Output value.\n        else:\n            self[node_id] = NONE  # Set data output.\n\n            value = {}  # Output value.\n\n        if next_nds:\n            # namespace shortcuts for speed.\n            wf_add_edge = self._wf_add_edge\n\n            for u in next_nds:  # Set workflow.\n                wf_add_edge(node_id, u, **value)\n\n        else:\n            # namespace shortcuts for speed.\n            n, has, sub_sol = self.nodes, self.workflow.has_edge, self.sub_sol\n\n            def no_visited_in_sub_dsp(i):\n                node = n[i]\n                if node['type'] == 'dispatcher' and has(i, node_id):\n                    visited = sub_sol[self.index + node['index']]._visited\n                    return node['inputs'][node_id] not in visited\n                return True\n\n            # List of functions.\n            succ_fun = [u for u in self._succ[node_id]\n                        if no_visited_in_sub_dsp(u)]\n\n            # Check if it has functions as outputs and wildcard condition.\n            if succ_fun and succ_fun[0] not in self._visited:\n                # namespace shortcuts for speed.\n                wf_add_edge = self._wf_add_edge\n\n                for u in succ_fun:  # Set workflow.\n                    wf_add_edge(node_id, u, **value)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_function_node_output(self, node_id, node_attr, no_call,\n                                  next_nds=None, **kw):\n        \"\"\"\n        Set the function node output from node inputs.\n\n        :param node_id:\n            Function node id.\n        :type node_id: str\n\n        :param node_attr:\n            Dictionary of node attributes.\n        :type node_attr: dict[str, T]\n\n        :param no_call:\n            If True data node estimation function is not used.\n        :type no_call: bool\n\n        :return:\n            If the output have been evaluated correctly.\n        :rtype: bool\n        \"\"\"\n\n        # Namespace shortcuts for speed.\n        o_nds, dist = node_attr['outputs'], self.dist\n\n        # List of nodes that can still be estimated by the function node.\n        output_nodes = next_nds or set(self._succ[node_id]).difference(dist)\n\n        if not output_nodes:  # This function is not needed.\n            self.workflow.remove_node(node_id)  # Remove function node.\n            return False\n\n        wf_add_edge = self._wf_add_edge  # Namespace shortcuts for speed.\n\n        if no_call:\n            for u in output_nodes:  # Set workflow out.\n                wf_add_edge(node_id, u)\n            return True\n\n        args = self._wf_pred[node_id]  # List of the function's arguments.\n        args = [args[k]['value'] for k in node_attr['inputs']]\n\n        try:\n            self._check_function_domain(args, node_attr, node_id)\n            res = async_thread(self, args, node_attr, node_id, **kw)\n            # noinspection PyUnresolvedReferences\n            self.workflow.node[node_id]['results'] = res\n        except SkipNode:\n            return False\n\n        # Set workflow.\n        for k, v in zip(o_nds, res if len(o_nds) > 1 else [res]):\n            if k in output_nodes and v is not NONE:\n                wf_add_edge(node_id, k, value=v)\n\n        return True", "response": "Set the function node output from node inputs and outputs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd initial values updating workflow seen and fringe.", "response": "def _add_initial_value(self, data_id, value, initial_dist=0.0,\n                           fringe=None, check_cutoff=None, no_call=None):\n        \"\"\"\n        Add initial values updating workflow, seen, and fringe.\n\n        :param fringe:\n            Heapq of closest available nodes.\n        :type fringe: list[(float | int, bool, (str, Dispatcher)]\n\n        :param check_cutoff:\n            Check the cutoff limit.\n        :type check_cutoff: (int | float) -> bool\n\n        :param no_call:\n            If True data node estimation function is not used.\n        :type no_call: bool\n\n        :param data_id:\n            Data node id.\n        :type data_id: str\n\n        :param value:\n            Data node value e.g., {'value': val}.\n        :type value: dict[str, T]\n\n        :param initial_dist:\n            Data node initial distance in the ArciDispatch algorithm.\n        :type initial_dist: float, int, optional\n\n        :return:\n            True if the data has been visited, otherwise false.\n        :rtype: bool\n        \"\"\"\n\n        # Namespace shortcuts for speed.\n        nodes, seen, edge_weight = self.nodes, self.seen, self._edge_length\n        wf_remove_edge, check_wait_in = self._wf_remove_edge, self.check_wait_in\n        wf_add_edge, dsp_in = self._wf_add_edge, self._set_sub_dsp_node_input\n        update_view = self._update_meeting\n\n        if fringe is None:\n            fringe = self.fringe\n\n        if no_call is None:\n            no_call = self.no_call\n\n        check_cutoff = check_cutoff or self.check_cutoff\n\n        if data_id not in nodes:  # Data node is not in the dmap.\n            return False\n\n        wait_in = nodes[data_id]['wait_inputs']  # Store wait inputs flag.\n\n        index = nodes[data_id]['index']  # Store node index.\n\n        wf_add_edge(START, data_id, **value)  # Add edge.\n\n        if data_id in self._wildcards:  # Check if the data node has wildcard.\n\n            self._visited.add(data_id)  # Update visited nodes.\n\n            self.workflow.add_node(data_id)  # Add node to workflow.\n\n            for w, edge_data in self.dmap[data_id].items():  # See func node.\n                wf_add_edge(data_id, w, **value)  # Set workflow.\n\n                node = nodes[w]  # Node attributes.\n\n                # Evaluate distance.\n                vw_dist = initial_dist + edge_weight(edge_data, node)\n\n                update_view(w, vw_dist)  # Update view distance.\n\n                # Check the cutoff limit and if all inputs are satisfied.\n                if check_cutoff(vw_dist):\n                    wf_remove_edge(data_id, w)  # Remove workflow edge.\n                    continue  # Pass the node.\n                elif node['type'] == 'dispatcher':\n                    dsp_in(data_id, w, fringe, check_cutoff, no_call, vw_dist)\n                elif check_wait_in(True, w):\n                    continue  # Pass the node.\n\n                seen[w] = vw_dist  # Update distance.\n\n                vd = (True, w, self.index + node['index'])  # Virtual distance.\n\n                heapq.heappush(fringe, (vw_dist, vd, (w, self)))  # Add 2 heapq.\n\n            return True\n\n        update_view(data_id, initial_dist)  # Update view distance.\n\n        if check_cutoff(initial_dist):  # Check the cutoff limit.\n            wf_remove_edge(START, data_id)  # Remove workflow edge.\n        elif not check_wait_in(wait_in, data_id):  # Check inputs.\n            seen[data_id] = initial_dist  # Update distance.\n\n            vd = (wait_in, data_id, self.index + index)  # Virtual distance.\n\n            # Add node to heapq.\n            heapq.heappush(fringe, (initial_dist, vd, (data_id, self)))\n\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvisit a node and update the workflow seen and fringe.", "response": "def _visit_nodes(self, node_id, dist, fringe, check_cutoff, no_call=False,\n                     **kw):\n        \"\"\"\n        Visits a node, updating workflow, seen, and fringe..\n\n        :param node_id:\n            Node id to visit.\n        :type node_id: str\n\n        :param dist:\n            Distance from the starting node.\n        :type dist: float, int\n\n        :param fringe:\n            Heapq of closest available nodes.\n        :type fringe: list[(float | int, bool, (str, Dispatcher)]\n\n        :param check_cutoff:\n            Check the cutoff limit.\n        :type check_cutoff: (int | float) -> bool\n\n        :param no_call:\n            If True data node estimation function is not used.\n        :type no_call: bool, optional\n\n        :return:\n            False if all dispatcher targets have been reached, otherwise True.\n        :rtype: bool\n        \"\"\"\n\n        # Namespace shortcuts.\n        wf_rm_edge, wf_has_edge = self._wf_remove_edge, self.workflow.has_edge\n        edge_weight, nodes = self._edge_length, self.nodes\n\n        self.dist[node_id] = dist  # Set minimum dist.\n\n        self._visited.add(node_id)  # Update visited nodes.\n\n        if not self._set_node_output(node_id, no_call, **kw):  # Set output.\n            # Some error occurs or inputs are not in the function domain.\n            return True\n\n        if self.check_targets(node_id):  # Check if the targets are satisfied.\n            return False  # Stop loop.\n\n        for w, e_data in self.dmap[node_id].items():\n            if not wf_has_edge(node_id, w):  # Check wildcard option.\n                continue\n\n            node = nodes[w]  # Get node attributes.\n\n            vw_d = dist + edge_weight(e_data, node)  # Evaluate dist.\n\n            if check_cutoff(vw_d):  # Check the cutoff limit.\n                wf_rm_edge(node_id, w)  # Remove edge that cannot be see.\n                continue\n\n            if node['type'] == 'dispatcher':\n                self._set_sub_dsp_node_input(\n                    node_id, w, fringe, check_cutoff, no_call, vw_d)\n\n            else:  # See the node.\n                self._see_node(w, fringe, vw_d)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a node is visible in the fringe.", "response": "def _see_node(self, node_id, fringe, dist, w_wait_in=0):\n        \"\"\"\n        See a node, updating seen and fringe.\n\n        :param node_id:\n            Node id to see.\n        :type node_id: str\n\n        :param fringe:\n            Heapq of closest available nodes.\n        :type fringe: list[(float | int, bool, (str, Dispatcher)]\n\n        :param dist:\n            Distance from the starting node.\n        :type dist: float, int\n\n        :param w_wait_in:\n            Additional weight for sorting correctly the nodes in the fringe.\n        :type w_wait_in: int, float\n\n        :return:\n            True if the node is visible, otherwise False.\n        :rtype: bool\n        \"\"\"\n\n        # Namespace shortcuts.\n        seen, dists = self.seen, self.dist\n\n        wait_in = self.nodes[node_id]['wait_inputs']  # Wait inputs flag.\n\n        self._update_meeting(node_id, dist)  # Update view distance.\n\n        # Check if inputs are satisfied.\n        if self.check_wait_in(wait_in, node_id):\n            pass  # Pass the node\n\n        elif node_id in dists:  # The node w already estimated.\n            if dist < dists[node_id]:  # Error for negative paths.\n                raise DispatcherError('Contradictory paths found: '\n                                      'negative weights?', sol=self)\n        elif node_id not in seen or dist < seen[node_id]:  # Check min dist.\n            seen[node_id] = dist  # Update dist.\n\n            index = self.nodes[node_id]['index']  # Node index.\n\n            # Virtual distance.\n            vd = (w_wait_in + int(wait_in), node_id, self.index + index)\n\n            # Add to heapq.\n            heapq.heappush(fringe, (dist, vd, (node_id, self)))\n\n            return True  # The node is visible.\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _remove_unused_nodes(self):\n\n        # Namespace shortcuts.\n        nodes, wf_remove_node = self.nodes, self.workflow.remove_node\n        add_visited, succ = self._visited.add, self.workflow.succ\n\n        # Remove unused function and sub-dispatcher nodes.\n        for n in (set(self._wf_pred) - set(self._visited)):\n            node_type = nodes[n]['type']  # Node type.\n\n            if node_type == 'data':\n                continue  # Skip data node.\n\n            if node_type == 'dispatcher' and succ[n]:\n                add_visited(n)  # Add to visited nodes.\n                i = self.index + nodes[n]['index']\n                self.sub_sol[i]._remove_unused_nodes()\n                continue  # Skip sub-dispatcher node with outputs.\n\n            wf_remove_node(n)", "response": "Removes unused function and sub - dispatcher nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the dispatcher as sub - dispatcher and update the fringe.", "response": "def _init_sub_dsp(self, dsp, fringe, outputs, no_call, initial_dist, index,\n                      full_name):\n        \"\"\"\n        Initialize the dispatcher as sub-dispatcher and update the fringe.\n\n        :param fringe:\n            Heapq of closest available nodes.\n        :type fringe: list[(float | int, bool, (str, Dispatcher)]\n\n        :param outputs:\n            Ending data nodes.\n        :type outputs: list[str], iterable\n\n        :param no_call:\n            If True data node estimation function is not used.\n        :type no_call: bool\n        \"\"\"\n\n        # Initialize as sub-dispatcher.\n        sol = self.__class__(\n            dsp, {}, outputs, False, None, None, no_call, False,\n            wait_in=self._wait_in.get(dsp, None), index=self.index + index,\n            full_name=full_name\n        )\n\n        sol.sub_sol = self.sub_sol\n\n        for f in sol.fringe:  # Update the fringe.\n            item = (initial_dist + f[0], (2,) + f[1][1:], f[-1])\n            heapq.heappush(fringe, item)\n\n        return sol"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _see_remote_link_node(self, node_id, fringe=None, dist=None,\n                              check_dsp=lambda x: True):\n        \"\"\"\n        See data remote links of the node (set output to remote links).\n\n        :param node_id:\n            Node id.\n        :type node_id: str\n\n        :param fringe:\n            Heapq of closest available nodes.\n        :type fringe: list[(float | int, bool, (str, Dispatcher)]\n\n        :param dist:\n            Distance from the starting node.\n        :type dist: float, int\n\n        :param check_dsp:\n            A function to check if the remote dispatcher is ok.\n        :type check_dsp: (Dispatcher) -> bool\n        \"\"\"\n        # Namespace shortcut.\n        node, p_id, c_i = self.nodes[node_id], self.index[:-1], self.index[-1:]\n\n        if node['type'] == 'data' and p_id and check_dsp(p_id):\n            sol = self.sub_sol[self.index[:-1]]  # Get parent solution.\n            for dsp_id, n in sol.dsp.nodes.items():\n                if n['index'] == c_i and node_id in n.get('outputs', {}):\n                    value = self[node_id]  # Get data output.\n                    for n_id in stlp(n['outputs'][node_id]):\n                        # Node has been visited or inp do not coincide with out.\n                        if not (n_id in sol._visited or\n                                sol.workflow.has_edge(n_id, dsp_id)):\n                            # Donate the result to the child.\n                            sol._wf_add_edge(dsp_id, n_id, value=value)\n                            if fringe is not None:\n                                # See node.\n                                sol._see_node(n_id, fringe, dist, w_wait_in=2)\n                    break", "response": "See remote links of the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the input of a sub - dispatcher node.", "response": "def _set_sub_dsp_node_input(self, node_id, dsp_id, fringe, check_cutoff,\n                                no_call, initial_dist):\n        \"\"\"\n        Initializes the sub-dispatcher and set its inputs.\n\n        :param node_id:\n            Input node to set.\n        :type node_id: str\n\n        :param dsp_id:\n            Sub-dispatcher node id.\n        :type dsp_id: str\n\n        :param fringe:\n            Heapq of closest available nodes.\n        :type fringe: list[(float | int, bool, (str, Dispatcher)]\n\n        :param check_cutoff:\n            Check the cutoff limit.\n        :type check_cutoff: (int | float) -> bool\n\n        :param no_call:\n            If True data node estimation function is not used.\n        :type no_call: bool\n\n        :param initial_dist:\n            Distance to reach the sub-dispatcher node.\n        :type initial_dist: int, float\n\n        :return:\n            If the input have been set.\n        :rtype: bool\n        \"\"\"\n\n        # Namespace shortcuts.\n        node = self.nodes[dsp_id]\n        dsp, pred = node['function'], self._wf_pred[dsp_id]\n        distances, sub_sol = self.dist, self.sub_sol\n\n        iv_nodes = [node_id]  # Nodes do be added as initial values.\n\n        self._meet[dsp_id] = initial_dist  # Set view distance.\n\n        # Check if inputs are satisfied.\n        if self.check_wait_in(node['wait_inputs'], dsp_id):\n            return False  # Pass the node\n\n        if dsp_id not in distances:\n            kw = {}\n            dom = self._check_sub_dsp_domain(dsp_id, node, pred, kw)\n            if dom is True:\n                iv_nodes = pred  # Args respect the domain.\n            elif dom is False:\n                return False\n\n            # Initialize the sub-dispatcher.\n            sub_sol[self.index + node['index']] = sol = self._init_sub_dsp(\n                dsp, fringe, node['outputs'], no_call, initial_dist,\n                node['index'], self.full_name + (dsp_id,)\n            )\n            self.workflow.add_node(dsp_id, solution=sol, **kw)\n\n            distances[dsp_id] = initial_dist  # Update min distance.\n        else:\n            sol = sub_sol[self.index + node['index']]\n\n        for n_id in iv_nodes:\n            # Namespace shortcuts.\n            val = pred[n_id]\n\n            for n in stlp(node['inputs'][n_id]):\n                # Add initial value to the sub-dispatcher.\n                sol._add_initial_value(\n                    n, val, initial_dist, fringe, check_cutoff, no_call\n                )\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _warning(self, msg, node_id, ex, *args, **kwargs):\n\n        raises = self.raises(ex) if callable(self.raises) else self.raises\n\n        if raises and isinstance(ex, DispatcherError):\n            ex.update(self)\n            raise ex\n\n        self._errors[node_id] = msg % ((node_id, ex) + args)\n        node_id = '/'.join(self.full_name + (node_id,))\n\n        if raises:\n            raise DispatcherError(msg, node_id, ex, *args, sol=self, **kwargs)\n        else:\n            kwargs['exc_info'] = kwargs.get('exc_info', 1)\n            log.error(msg, node_id, ex, *args, **kwargs)", "response": "Handles the error messages."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Keywords\n        KEYWORDS = ('STREAMCELLS',\n                    'CELLIJ')\n\n        # Parse file into chunks associated with keywords/cards\n        with open(path, 'r') as f:\n            chunks = pt.chunk(KEYWORDS, f)\n\n        # Parse chunks associated with each key\n        for key, chunkList in iteritems(chunks):\n            # Parse each chunk in the chunk list\n            for chunk in chunkList:\n\n                # Cases\n                if key == 'STREAMCELLS':\n                    # PIPECELLS Handler\n                    schunk = chunk[0].strip().split()\n                    self.streamCells = schunk[1]\n\n                elif key == 'CELLIJ':\n                    # CELLIJ Handler\n                    # Parse CELLIJ Chunk\n                    result = self._cellChunk(chunk)\n\n                    # Create GSSHAPY object\n                    self._createGsshaPyObjects(result)", "response": "Method to read from a Grid Stream File"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nindexes Map Read from File Method", "response": "def _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        \"\"\"\n        Index Map Read from File Method\n        \"\"\"\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Open file and read plain text into text field\n        with open(path, 'r') as f:\n            self.rasterText = f.read()\n\n        # Retrieve metadata from header\n        lines = self.rasterText.split('\\n')\n        for line in lines[0:6]:\n            spline = line.split()\n\n            if 'north' in spline[0].lower():\n                self.north = float(spline[1])\n            elif 'south' in spline[0].lower():\n                self.south = float(spline[1])\n            elif 'east' in spline[0].lower():\n                self.east = float(spline[1])\n            elif 'west' in spline[0].lower():\n                self.west = float(spline[1])\n            elif 'rows' in spline[0].lower():\n                self.rows = int(spline[1])\n            elif 'cols' in spline[0].lower():\n                self.columns = int(spline[1])\n\n        if spatial:\n            # Get well known binary from the raster file using the MapKit RasterLoader\n            wkbRaster = RasterLoader.grassAsciiRasterToWKB(session=session,\n                                                           grassRasterPath=path,\n                                                           srid=str(spatialReferenceID),\n                                                           noData='-1')\n            self.raster = wkbRaster\n            self.srid = spatialReferenceID\n\n        # Assign other properties\n        self.filename = filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, directory, name=None, session=None, replaceParamFile=None):\n\n        # Initiate file\n        if name != None:\n            filename = '%s.%s' % (name, self.fileExtension)\n            filePath = os.path.join(directory, filename)\n        else:\n            filePath = os.path.join(directory, self.filename)\n\n        # If the raster field is not empty, write from this field\n        if type(self.raster) != type(None):\n            # Configure RasterConverter\n            converter = RasterConverter(session)\n\n            # Use MapKit RasterConverter to retrieve the raster as a GRASS ASCII Grid\n            grassAsciiGrid = converter.getAsGrassAsciiRaster(rasterFieldName='raster',\n                                                             tableName=self.__tablename__,\n                                                             rasterIdFieldName='id',\n                                                             rasterId=self.id)\n\n            # Write to file\n            with open(filePath, 'w') as mapFile:\n                mapFile.write(grassAsciiGrid)\n\n        else:\n            if self.rasterText is not None:\n                # Open file and write, raster_text only\n                with open(filePath, 'w') as mapFile:\n                    mapFile.write(self.rasterText)", "response": "Writes the raster to a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write(self, session, openFile, replaceParamFile):\n        # Write Lines\n        openFile.write('GRIDPIPEFILE\\n')\n        openFile.write('PIPECELLS %s\\n' % self.pipeCells)\n\n        for cell in self.gridPipeCells:\n            openFile.write('CELLIJ    %s  %s\\n' % (cell.cellI, cell.cellJ))\n            openFile.write('NUMPIPES  %s\\n' % cell.numPipes)\n\n            for node in cell.gridPipeNodes:\n                openFile.write('SPIPE     %s  %s  %.6f\\n' % (\n                    node.linkNumber,\n                    node.nodeNumber,\n                    node.fractPipeLength))", "response": "Method to write to File Method to Write"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a CELLIJ Chunk Method", "response": "def _cellChunk(self, lines):\n        \"\"\"\n        Parse CELLIJ Chunk Method\n        \"\"\"\n        KEYWORDS = ('CELLIJ',\n                    'NUMPIPES',\n                    'SPIPE')\n\n        result = {'i': None,\n                  'j': None,\n                  'numPipes': None,\n                  'spipes': []}\n\n        chunks = pt.chunk(KEYWORDS, lines)\n\n        # Parse chunks associated with each key\n        for card, chunkList in iteritems(chunks):\n            # Parse each chunk in the chunk list\n            for chunk in chunkList:\n                schunk = chunk[0].strip().split()\n\n                # Cases\n                if card == 'CELLIJ':\n                    # CELLIJ handler\n                    result['i'] = schunk[1]\n                    result['j'] = schunk[2]\n\n                elif card == 'NUMPIPES':\n                    # NUMPIPES handler\n                    result['numPipes'] = schunk[1]\n\n                elif card == 'SPIPE':\n                    # SPIPE handler\n                    pipe = {'linkNumber': schunk[1],\n                            'nodeNumber': schunk[2],\n                            'fraction': schunk[3]}\n\n                    result['spipes'].append(pipe)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nescape any value in the SPARQL shortened form .", "response": "def escape_any(value):\n    \"\"\"\n    Section 4.1.2 defines SPARQL shortened forms\n    https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#QSynLiterals\n\n    Examples of literal syntax in SPARQL include:\n\n        \"chat\"\n        'chat'@fr with language tag \"fr\"\n        \"xyz\"^^<http://example.org/ns/userDatatype>\n        \"abc\"^^appNS:appDataType\n        '''The librarian said, \"Perhaps you would enjoy 'War and Peace'.\"'''\n        1, which is the same as \"1\"^^xsd:integer\n        1.3, which is the same as \"1.3\"^^xsd:decimal\n        1.300, which is the same as \"1.300\"^^xsd:decimal\n        1.0e6, which is the same as \"1.0e6\"^^xsd:double\n        true, which is the same as \"true\"^^xsd:boolean\n        false, which is the same as \"false\"^^xsd:boolean\n    \"\"\"\n    if isinstance(value, type):\n        raise TypeError(\"object %r is not an instance\" % value)\n    for type_, escape_method in escapers:\n        if isinstance(value, type_):\n            return escape_method(value)\n    return escape_string(str(value))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write(self, session, openFile, replaceParamFile):\n        # Retrieve TargetParameter objects\n        targets = self.targetParameters\n\n        # Write lines\n        openFile.write('%s\\n' % self.numParameters)\n\n        for target in targets:\n            openFile.write('%s %s\\n' % (target.targetVariable, target.varFormat))", "response": "Private method to write to File Method\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Open file and parse into a data structure\n        with open(path, 'r') as f:\n            for line in f:\n                valLine = ReplaceValLine()\n                valLine.contents = line\n                valLine.replaceValFile = self", "response": "Method to read from file Method to read from file Method to parse into a data structure"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites to File Method", "response": "def _write(self, session, openFile, replaceParamFile):\n        \"\"\"\n        Replace Val File Write to File Method\n        \"\"\"\n        # Write lines\n        for line in self.lines:\n            openFile.write(line.contents)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing this function in emit data into the store.", "response": "def emit(self, data_frame):\n        \"\"\"Use this function in emit data into the store.\n\n        :param data_frame: DataFrame to be recorded.\n        \"\"\"\n        if self.result is not None:\n            raise MultipleEmitsError()\n        data_frame.columns = [self.prefix + '__' + c\n                              for c in data_frame.columns]\n        self.result = data_frame"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trigger_hats(self, command, arg=None, callback=None):\n        threads = []\n        for scriptable in [self.project.stage] + self.project.sprites:\n            threads += self.trigger_scriptable_hats(scriptable, command, arg,\n                                                    callback)\n        return threads", "response": "Returns a list with each script that is triggered."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the script and add it to the list of threads.", "response": "def push_script(self, scriptable, script, callback=None):\n        \"\"\"Run the script and add it to the list of threads.\"\"\"\n        if script in self.threads:\n            self.threads[script].finish()\n        thread = Thread(self.run_script(scriptable, script),\n                                      scriptable, callback)\n        self.new_threads[script] = thread\n        return thread"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tick(self, events):\n        self.add_new_threads()\n\n        if self.drag_sprite:\n            (mx, my) = self.screen.get_mouse_pos()\n            (ox, oy) = self.drag_offset\n            new_position = (mx + ox, my + oy)\n            if self.drag_sprite.position != new_position:\n                self.has_dragged = True\n                self.drag_sprite.position = new_position\n\n        for event in events:\n            if event.kind == \"key_pressed\":\n                assert event.value in kurt.Insert(None, \"key\").options()\n                self.trigger_hats(\"whenKeyPressed\", event.value)\n\n            elif event.kind == \"mouse_down\":\n                mouse_pos = self.screen.get_mouse_pos()\n                for sprite in reversed(self.project.sprites):\n                    rect = bounds(sprite)\n                    if rect.collide_point(mouse_pos):\n                        if self.screen.touching_mouse(sprite):\n                            scriptable = sprite\n                            break\n                else:\n                    scriptable = self.project.stage\n\n                if scriptable.is_draggable:\n                    (mx, my) = self.screen.get_mouse_pos()\n                    (x, y) = scriptable.position\n                    self.drag_offset = (x - mx, y - my)\n                    self.drag_sprite = scriptable\n                    self.has_dragged = False\n                    go_to_front(scriptable)\n                else:\n                    self.trigger_scriptable_hats(scriptable, \"whenClicked\")\n\n            elif event.kind == \"mouse_up\":\n                if self.drag_sprite:\n                    if not self.has_dragged:\n                        self.trigger_scriptable_hats(self.drag_sprite,\n                                                     \"whenClicked\")\n                    self.drag_sprite = None\n\n        remove_threads = []\n        while 1:\n            for (script, thread) in self.threads.items():\n                modified = False\n                for event in thread.tick():\n                    if event.kind == \"stop\":\n                        if event.value == \"all\":\n                            self.stop()\n                            return\n                        elif event.value == \"other scripts in sprite\":\n                            for (script, other) in self.threads.items():\n                                if other.scriptable == thread.scriptable:\n                                    other.finish()\n                                    del self.threads[script]\n                            modified = True\n                            break\n                        else:\n                            thread.finish()\n                            del self.threads[script]\n                            modified = True\n                            break\n                    else: # Pass to Screen\n                        yield event\n                if modified:\n                    break\n            else:\n                break\n\n        self.add_new_threads()", "response": "Execute one frame of the interpreter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate the expression and return the value.", "response": "def evaluate(self, s, value, insert=None):\n        \"\"\"Expression evaluator.\n\n        * For expressions, returns the value of the expression.\n\n        * For Blocks, returns a generator (or the empty list []).\n\n        \"\"\"\n        assert not isinstance(value, kurt.Script)\n\n        if insert and insert.unevaluated:\n            return value\n\n        if isinstance(value, kurt.Block):\n            if value.type.shape == \"hat\":\n                return []\n\n            if value.type not in self.COMMANDS:\n                if getattr(value.type, '_workaround', None):\n                    value = value.type._workaround(value)\n                    if not value:\n                        raise kurt.BlockNotSupported(value.type)\n                else:\n                    raise kurt.BlockNotSupported(value.type)\n\n            f = self.COMMANDS[value.type]\n\n            args = [self.evaluate(s, arg, arg_insert)\n                    for (arg, arg_insert)\n                    in zip(list(value.args), value.type.inserts)]\n            value = f(s, *args)\n\n            def flatten_generators(gen):\n                for item in gen:\n                    if inspect.isgenerator(item):\n                        for x in flatten_generators(item):\n                            yield x\n                    else:\n                        yield item\n            if inspect.isgenerator(value):\n                value = flatten_generators(value)\n\n            if value is None:\n                value = []\n\n        if insert:\n            if isinstance(value, basestring):\n                value = unicode(value)\n\n                if insert.shape in (\"number\", \"number-menu\", \"string\"):\n                    try:\n                        value = float(value)\n                    except (TypeError, ValueError):\n                        if insert.shape == \"number\":\n                            value = 0\n\n            if isinstance(value, float) and value == int(value):\n                value = int(value)\n\n            if insert.kind in (\"spriteOrStage\", \"spriteOrMouse\", \"stageOrThis\",\n                               \"spriteOnly\", \"touching\"):\n                if value not in (\"mouse-pointer\", \"edge\"):\n                    value = (self.project.stage if value == \"Stage\"\n                             else self.project.get_sprite(value))\n            elif insert.kind == \"var\":\n                if value in s.variables:\n                    value = s.variables[value]\n                else:\n                    value = s.project.variables[value]\n            elif insert.kind == \"list\":\n                if value in s.lists:\n                    value = s.lists[value]\n                else:\n                    value = s.project.lists[value]\n            elif insert.kind == \"sound\":\n                for sound in s.sounds:\n                    if sound.name == value:\n                        value = sound\n                        break\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cluster_name(self):\n        return self._get(\n            url=self.url + '/api/cluster-name',\n            headers=self.headers,\n            auth=self.auth\n        )", "response": "Get the name of the RabbitMQ cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_node(self, name, memory=False, binary=False):\n        return self._api_get(\n            url='/api/nodes/{0}'.format(name),\n            params=dict(\n                binary=binary,\n                memory=memory,\n            ),\n        )", "response": "Get a specific node in the RabbitMQ cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_connection(self, name):\n        return self._api_get('/api/connections/{0}'.format(\n            urllib.parse.quote_plus(name)\n        ))", "response": "Get a single connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclosing an individual connection. Give an optional reason", "response": "def delete_connection(self, name, reason=None):\n        \"\"\"\n        Closes an individual connection. Give an optional reason\n\n        :param name: The connection name\n        :type name: str\n\n        :param reason: An option reason why the connection was deleted\n        :type reason: str\n        \"\"\"\n        headers = {'X-Reason': reason} if reason else {}\n\n        self._api_delete(\n            '/api/connections/{0}'.format(\n                urllib.parse.quote_plus(name)\n            ),\n            headers=headers,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_connection_channels(self, name):\n        return self._api_get('/api/connections/{0}/channels'.format(\n            urllib.parse.quote_plus(name)\n        ))", "response": "List all channels for a given connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_channel(self, name):\n        return self._api_get('/api/channels/{0}'.format(\n            urllib.parse.quote_plus(name)\n        ))", "response": "Get the details about an individual channel."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_consumers_for_vhost(self, vhost):\n        return self._api_get('/api/consumers/{0}'.format(\n            urllib.parse.quote_plus(vhost)\n        ))", "response": "Returns a list of all consumers for a given vhost."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_exchanges_for_vhost(self, vhost):\n        return self._api_get('/api/exchanges/{0}'.format(\n            urllib.parse.quote_plus(vhost)\n        ))", "response": "Returns a list of all exchanges in a given virtual host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_exchange_for_vhost(self, exchange, vhost):\n        return self._api_get('/api/exchanges/{0}/{1}'.format(\n            urllib.parse.quote_plus(vhost),\n            urllib.parse.quote_plus(exchange)\n        ))", "response": "Get the name of the exchange for a vhost"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_exchange_for_vhost(self, exchange, vhost, body):\n        self._api_put(\n            '/api/exchanges/{0}/{1}'.format(\n                urllib.parse.quote_plus(vhost),\n                urllib.parse.quote_plus(exchange)),\n            data=body\n        )", "response": "Create an individual exchange for a vhost."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting an individual exchange for a vhost.", "response": "def delete_exchange_for_vhost(self, exchange, vhost, if_unused=False):\n        \"\"\"\n        Delete an individual exchange. You can add the parameter\n        ``if_unused=True``. This prevents the delete from succeeding if the\n        exchange is bound to a queue or as a source to another exchange.\n\n        :param exchange: The exchange name\n        :type exchange: str\n\n        :param vhost: The vhost name\n        :type vhost: str\n\n        :param if_unused: Set to ``True`` to only delete if it is unused\n        :type if_unused: bool\n        \"\"\"\n        self._api_delete(\n            '/api/exchanges/{0}/{1}'.format(\n                urllib.parse.quote_plus(vhost),\n                urllib.parse.quote_plus(exchange)),\n            params={\n                'if-unused': if_unused\n            },\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all bindings for a given vhost.", "response": "def list_bindings_for_vhost(self, vhost):\n        \"\"\"\n        A list of all bindings in a given virtual host.\n\n        :param vhost: The vhost name\n        :type vhost: str\n        \"\"\"\n        return self._api_get('/api/bindings/{}'.format(\n            urllib.parse.quote_plus(vhost)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets details about an individual vhost.", "response": "def get_vhost(self, name):\n        \"\"\"\n        Details about an individual vhost.\n\n        :param name: The vhost name\n        :type name: str\n        \"\"\"\n        return self._api_get('/api/vhosts/{0}'.format(\n            urllib.parse.quote_plus(name)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_vhost(self, name):\n        self._api_delete('/api/vhosts/{0}'.format(\n            urllib.parse.quote_plus(name)\n        ))", "response": "Delete a vhost.\n\n        :param name: The vhost name\n        :type name: str"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_vhost(self, name, tracing=False):\n        data = {'tracing': True} if tracing else {}\n        self._api_put(\n            '/api/vhosts/{0}'.format(urllib.parse.quote_plus(name)),\n            data=data,\n        )", "response": "Create an individual vhost."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user(self, name):\n        return self._api_get('/api/users/{0}'.format(\n            urllib.parse.quote_plus(name)\n        ))", "response": "Get details about an individual user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_user(self, name):\n        self._api_delete('/api/users/{0}'.format(\n            urllib.parse.quote_plus(name)\n        ))", "response": "Delete a user.\n\n        :param name: The user's name\n        :type name: str"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_user(self, name, password, password_hash=None, tags=None):\n        data = {\n            'tags': ', '.join(tags or [])\n        }\n        if password:\n            data['password'] = password\n        elif password_hash:\n            data['password_hash'] = password_hash\n        else:\n            data['password_hash'] = \"\"\n\n        self._api_put(\n            '/api/users/{0}'.format(urllib.parse.quote_plus(name)),\n            data=data,\n        )", "response": "Creates a user in the national system."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all permissions for a given user.", "response": "def list_user_permissions(self, name):\n        \"\"\"\n        A list of all permissions for a given user.\n\n        :param name: The user's name\n        :type name: str\n        \"\"\"\n        return self._api_get('/api/users/{0}/permissions'.format(\n            urllib.parse.quote_plus(name)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a user permission.", "response": "def create_user_permission(self,\n                               name,\n                               vhost,\n                               configure=None,\n                               write=None,\n                               read=None):\n        \"\"\"\n        Create a user permission\n        :param name: The user's name\n        :type name: str\n        :param vhost: The vhost to assign the permission to\n        :type vhost: str\n\n        :param configure: A regex for the user permission. Default is ``.*``\n        :type configure: str\n        :param write: A regex for the user permission. Default is ``.*``\n        :type write: str\n        :param read: A regex for the user permission. Default is ``.*``\n        :type read: str\n        \"\"\"\n        data = {\n            'configure': configure or '.*',\n            'write': write or '.*',\n            'read': read or '.*',\n        }\n        self._api_put(\n            '/api/permissions/{0}/{1}'.format(\n                urllib.parse.quote_plus(vhost),\n                urllib.parse.quote_plus(name)\n            ),\n            data=data\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all policies for a vhost.", "response": "def list_policies_for_vhost(self, vhost):\n        \"\"\"\n        A list of all policies for a vhost.\n        \"\"\"\n        return self._api_get('/api/policies/{0}'.format(\n            urllib.parse.quote_plus(vhost)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a specific policy for a vhost.", "response": "def get_policy_for_vhost(self, vhost, name):\n        \"\"\"\n        Get a specific policy for a vhost.\n\n        :param vhost: The virtual host the policy is for\n        :type vhost: str\n        :param name: The name of the policy\n        :type name: str\n        \"\"\"\n        return self._api_get('/api/policies/{0}/{1}'.format(\n            urllib.parse.quote_plus(vhost),\n            urllib.parse.quote_plus(name),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a policy for a vhost.", "response": "def create_policy_for_vhost(\n            self, vhost, name,\n            definition,\n            pattern=None,\n            priority=0,\n            apply_to='all'):\n        \"\"\"\n        Create a policy for a vhost.\n\n        :param vhost: The virtual host the policy is for\n        :type vhost: str\n        :param name: The name of the policy\n        :type name: str\n\n        :param definition: The definition of the policy. Required\n        :type definition: dict\n        :param priority: The priority of the policy. Defaults to 0\n        :param pattern: The pattern of resource names to apply the policy to\n        :type pattern: str\n        :type priority: int\n        :param apply_to: What resource type to apply the policy to.\n            Usually \"exchanges\", \"queues\", or \"all\". Defaults to \"all\"\n        :type apply_to: str\n\n        Example ::\n\n            # Makes all queues and exchanges on vhost \"/\" highly available\n            >>> api.create_policy_for_vhost(\n            ... vhost=\"/\",\n            ... name=\"ha-all\",\n            ... definition={\"ha-mode\": \"all\"},\n            ... pattern=\"\",\n            ... apply_to=\"all\")\n\n        \"\"\"\n        data = {\n            \"pattern\": pattern,\n            \"definition\": definition,\n            \"priority\": priority,\n            \"apply-to\": apply_to\n        }\n        self._api_put(\n            '/api/policies/{0}/{1}'.format(\n                urllib.parse.quote_plus(vhost),\n                urllib.parse.quote_plus(name),\n            ),\n            data=data,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a specific policy for a vhost.", "response": "def delete_policy_for_vhost(self, vhost, name):\n        \"\"\"\n        Delete a specific policy for a vhost.\n\n        :param vhost: The virtual host of the policy\n        :type vhost: str\n        :param name: The name of the policy\n        :type name: str\n        \"\"\"\n        self._api_delete('/api/policies/{0}/{1}/'.format(\n            urllib.parse.quote_plus(vhost),\n            urllib.parse.quote_plus(name),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_vhost_alive(self, vhost):\n        return self._api_get('/api/aliveness-test/{0}'.format(\n            urllib.parse.quote_plus(vhost)\n        ))", "response": "Check if a vhost is alive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, session, directory, name, maskMap):\n\n        # Assemble Path to file\n        name_split = name.split('.')\n        name = name_split[0]\n\n        # Default extension\n        extension = ''\n\n        if len(name_split) >= 2:\n            extension = name_split[-1]\n\n        # Run name preprocessor method if present\n        try:\n            name = self._namePreprocessor(name)\n        except:\n            'DO NOTHING'\n\n        if extension == '':\n            filename = '{0}.{1}'.format(name, self.fileExtension)\n        else:\n            filename = '{0}.{1}'.format(name, extension)\n\n        filePath = os.path.join(directory, filename)\n\n        with open(filePath, 'w') as openFile:\n            # Write Lines\n            self._write(session=session,\n                        openFile=openFile,\n                        maskMap=maskMap)", "response": "Write from database to file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the WMS dataset as a gridded time stamped KML string.", "response": "def getAsKmlGridAnimation(self, session, projectFile=None, path=None, documentName=None, colorRamp=None, alpha=1.0, noDataValue=0.0):\n        \"\"\"\n        Retrieve the WMS dataset as a gridded time stamped KML string.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n            projectFile(:class:`gsshapy.orm.ProjectFile`): Project file object for the GSSHA project to which the WMS dataset belongs.\n            path (str, optional): Path to file where KML file will be written. Defaults to None.\n            documentName (str, optional): Name of the KML document. This will be the name that appears in the legend.\n                Defaults to 'Stream Network'.\n            colorRamp (:mod:`mapkit.ColorRampGenerator.ColorRampEnum` or dict, optional): Use ColorRampEnum to select a\n                default color ramp or a dictionary with keys 'colors' and 'interpolatedPoints' to specify a custom color\n                ramp. The 'colors' key must be a list of RGB integer tuples (e.g.: (255, 0, 0)) and the\n                'interpolatedPoints' must be an integer representing the number of points to interpolate between each\n                color given in the colors list.\n            alpha (float, optional): Set transparency of visualization. Value between 0.0 and 1.0 where 1.0 is 100%\n                opaque and 0.0 is 100% transparent. Defaults to 1.0.\n            noDataValue (float, optional): The value to treat as no data when generating visualizations of rasters.\n                Defaults to 0.0.\n\n        Returns:\n            str: KML string\n        \"\"\"\n        # Prepare rasters\n        timeStampedRasters = self._assembleRasterParams(projectFile, self.rasters)\n\n        # Create a raster converter\n        converter = RasterConverter(sqlAlchemyEngineOrSession=session)\n\n        # Configure color ramp\n        if isinstance(colorRamp, dict):\n            converter.setCustomColorRamp(colorRamp['colors'], colorRamp['interpolatedPoints'])\n        else:\n            converter.setDefaultColorRamp(colorRamp)\n\n        if documentName is None:\n            documentName = self.fileExtension\n\n        kmlString = converter.getAsKmlGridAnimation(tableName=WMSDatasetRaster.tableName,\n                                                    timeStampedRasters=timeStampedRasters,\n                                                    rasterIdFieldName='id',\n                                                    rasterFieldName='raster',\n                                                    documentName=documentName,\n                                                    alpha=alpha,\n                                                    noDataValue=noDataValue)\n\n        if path:\n            with open(path, 'w') as f:\n                f.write(kmlString)\n\n        return kmlString"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the WMS dataset as a PNG time stamped KMZ ArcGIS image.", "response": "def getAsKmlPngAnimation(self, session, projectFile=None, path=None, documentName=None, colorRamp=None, alpha=1.0,\n                              noDataValue=0, drawOrder=0, cellSize=None, resampleMethod='NearestNeighbour'):\n        \"\"\"\n        Retrieve the WMS dataset as a PNG time stamped KMZ\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n            projectFile(:class:`gsshapy.orm.ProjectFile`): Project file object for the GSSHA project to which the WMS dataset belongs.\n            path (str, optional): Path to file where KML file will be written. Defaults to None.\n            documentName (str, optional): Name of the KML document. This will be the name that appears in the legend.\n                Defaults to 'Stream Network'.\n            colorRamp (:mod:`mapkit.ColorRampGenerator.ColorRampEnum` or dict, optional): Use ColorRampEnum to select a\n                default color ramp or a dictionary with keys 'colors' and 'interpolatedPoints' to specify a custom color\n                ramp. The 'colors' key must be a list of RGB integer tuples (e.g.: (255, 0, 0)) and the\n                'interpolatedPoints' must be an integer representing the number of points to interpolate between each\n                color given in the colors list.\n            alpha (float, optional): Set transparency of visualization. Value between 0.0 and 1.0 where 1.0 is 100%\n                opaque and 0.0 is 100% transparent. Defaults to 1.0.\n            noDataValue (float, optional): The value to treat as no data when generating visualizations of rasters.\n                Defaults to 0.0.\n            drawOrder (int, optional): Set the draw order of the images. Defaults to 0.\n            cellSize (float, optional): Define the cell size in the units of the project projection at which to resample\n                the raster to generate the PNG. Defaults to None which will cause the PNG to be generated with the\n                original raster cell size. It is generally better to set this to a size smaller than the original cell\n                size to obtain a higher resolution image. However, computation time increases exponentially as the cell\n                size is decreased.\n            resampleMethod (str, optional): If cellSize is set, this method will be used to resample the raster. Valid\n                values include: NearestNeighbour, Bilinear, Cubic, CubicSpline, and Lanczos. Defaults to\n                NearestNeighbour.\n\n        Returns:\n            (str, list): Returns a KML string and a list of binary strings that are the PNG images.\n        \"\"\"\n        # Prepare rasters\n        timeStampedRasters = self._assembleRasterParams(projectFile, self.rasters)\n\n        # Make sure the raster field is valid\n        converter = RasterConverter(sqlAlchemyEngineOrSession=session)\n\n        # Configure color ramp\n        if isinstance(colorRamp, dict):\n            converter.setCustomColorRamp(colorRamp['colors'], colorRamp['interpolatedPoints'])\n        else:\n            converter.setDefaultColorRamp(colorRamp)\n\n        if documentName is None:\n            documentName = self.fileExtension\n\n        kmlString, binaryPngStrings = converter.getAsKmlPngAnimation(tableName=WMSDatasetRaster.tableName,\n                                                                     timeStampedRasters=timeStampedRasters,\n                                                                     rasterIdFieldName='id',\n                                                                     rasterFieldName='raster',\n                                                                     documentName=documentName,\n                                                                     alpha=alpha,\n                                                                     drawOrder=drawOrder,\n                                                                     cellSize=cellSize,\n                                                                     noDataValue=noDataValue,\n                                                                     resampleMethod=resampleMethod)\n\n        if path:\n            directory = os.path.dirname(path)\n            archiveName = (os.path.split(path)[1]).split('.')[0]\n            kmzPath = os.path.join(directory, (archiveName + '.kmz'))\n\n            with ZipFile(kmzPath, 'w') as kmz:\n                kmz.writestr(archiveName + '.kml', kmlString)\n\n                for index, binaryPngString in enumerate(binaryPngStrings):\n                    kmz.writestr('raster{0}.png'.format(index), binaryPngString)\n\n        return kmlString, binaryPngStrings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, maskMap):\n        # Assign file extension attribute to file object\n        self.fileExtension = extension\n\n        if isinstance(maskMap, RasterMapFile) and maskMap.fileExtension == 'msk':\n            # Vars from mask map\n            columns = maskMap.columns\n            rows = maskMap.rows\n            upperLeftX = maskMap.west\n            upperLeftY = maskMap.north\n\n            # Derive the cell size (GSSHA cells are square, so it is the same in both directions)\n            cellSizeX = int(abs(maskMap.west - maskMap.east) / columns)\n            cellSizeY = -1 * cellSizeX\n\n            # Dictionary of keywords/cards and parse function names\n            KEYWORDS = {'DATASET': wdc.datasetHeaderChunk,\n                        'TS': wdc.datasetScalarTimeStepChunk}\n\n            # Open file and read plain text into text field\n            with open(path, 'r') as f:\n                chunks = pt.chunk(KEYWORDS, f)\n\n            # Parse header chunk first\n            header = wdc.datasetHeaderChunk('DATASET', chunks['DATASET'][0])\n\n            # Parse each time step chunk and aggregate\n            timeStepRasters = []\n\n            for chunk in chunks['TS']:\n                timeStepRasters.append(wdc.datasetScalarTimeStepChunk(chunk, columns, header['numberCells']))\n\n            # Set WMS dataset file properties\n            self.name = header['name']\n            self.numberCells = header['numberCells']\n            self.numberData = header['numberData']\n            self.objectID = header['objectID']\n\n            if header['type'] == 'BEGSCL':\n                self.objectType = header['objectType']\n                self.type = self.SCALAR_TYPE\n\n            elif header['type'] == 'BEGVEC':\n                self.vectorType = header['objectType']\n                self.type = self.VECTOR_TYPE\n\n            # Create WMS raster dataset files for each raster\n            for timeStep, timeStepRaster in enumerate(timeStepRasters):\n                # Create new WMS raster dataset file object\n                wmsRasterDatasetFile = WMSDatasetRaster()\n\n                # Set the wms dataset for this WMS raster dataset file\n                wmsRasterDatasetFile.wmsDataset = self\n\n                # Set the time step and timestamp and other properties\n                wmsRasterDatasetFile.iStatus = timeStepRaster['iStatus']\n                wmsRasterDatasetFile.timestamp = timeStepRaster['timestamp']\n                wmsRasterDatasetFile.timeStep = timeStep + 1\n\n                # If spatial is enabled create PostGIS rasters\n                if spatial:\n                    # Process the values/cell array\n                    wmsRasterDatasetFile.raster = RasterLoader.makeSingleBandWKBRaster(session,\n                                                                                       columns, rows,\n                                                                                       upperLeftX, upperLeftY,\n                                                                                       cellSizeX, cellSizeY,\n                                                                                       0, 0,\n                                                                                       spatialReferenceID,\n                                                                                       timeStepRaster['cellArray'])\n\n                # Otherwise, set the raster text properties\n                else:\n                    wmsRasterDatasetFile.rasterText = timeStepRaster['rasterText']\n\n            # Add current file object to the session\n            session.add(self)\n\n        else:\n            log.warning(\"Could not read {0}. Mask Map must be supplied \"\n                     \"to read WMS Datasets.\".format(filename))", "response": "Method to read from WMS Dataset File Method to set properties of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getAsWmsDatasetString(self, session):\n        # Magic numbers\n        FIRST_VALUE_INDEX = 12\n\n        # Write value raster\n        if type(self.raster) != type(None):\n            # Convert to GRASS ASCII Raster\n            valueGrassRasterString = self.getAsGrassAsciiGrid(session)\n\n            # Split by lines\n            values = valueGrassRasterString.split()\n\n            # Assemble into string\n            wmsDatasetString = ''\n            for i in range(FIRST_VALUE_INDEX, len(values)):\n                wmsDatasetString += '{0:.6f}\\r\\n'.format(float(values[i]))\n\n            return wmsDatasetString\n\n        else:\n            wmsDatasetString = self.rasterText", "response": "Retrieve the WMS Raster as a string in the WMS Dataset format"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes sure that there are no random artifacts in the file.", "response": "def check_watershed_boundary_geometry(shapefile_path):\n    \"\"\"Make sure that there are no random artifacts in the file.\"\"\"\n    wfg =  gpd.read_file(shapefile_path)\n    first_shape = wfg.iloc[0].geometry\n    if hasattr(first_shape, 'geoms'):\n        raise ValueError(\n            \"Invalid watershed boundary geometry. \"\n            \"To fix this, remove disconnected shapes or run \"\n            \"gsshapy.modeling.GSSHAModel.clean_boundary_shapefile\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_batch(sentences,\n                  token_dict,\n                  ignore_case=False,\n                  unk_index=1,\n                  eos_index=2):\n        \"\"\"Get a batch of inputs and outputs from given sentences.\n\n        :param sentences: A list of list of tokens.\n        :param token_dict: The dict that maps a token to an integer. `<UNK>` and `<EOS>` should be preserved.\n        :param ignore_case: Whether ignoring the case of the token.\n        :param unk_index: The index for unknown token.\n        :param eos_index: The index for ending of sentence.\n\n        :return inputs, outputs: The inputs and outputs of the batch.\n        \"\"\"\n        batch_size = len(sentences)\n        max_sentence_len = max(map(len, sentences))\n        inputs = [[0] * max_sentence_len for _ in range(batch_size)]\n        outputs_forward = [[0] * max_sentence_len for _ in range(batch_size)]\n        outputs_backward = [[0] * max_sentence_len for _ in range(batch_size)]\n        for i, sentence in enumerate(sentences):\n            outputs_forward[i][len(sentence) - 1] = eos_index\n            outputs_backward[i][0] = eos_index\n            for j, token in enumerate(sentence):\n                if ignore_case:\n                    index = token_dict.get(token.lower(), unk_index)\n                else:\n                    index = token_dict.get(token, unk_index)\n                inputs[i][j] = index\n                if j - 1 >= 0:\n                    outputs_forward[i][j - 1] = index\n                if j + 1 < len(sentence):\n                    outputs_backward[i][j + 1] = index\n        outputs_forward = np.expand_dims(np.asarray(outputs_forward), axis=-1)\n        outputs_backward = np.expand_dims(np.asarray(outputs_backward), axis=-1)\n        return np.asarray(inputs), [outputs_forward, outputs_backward]", "response": "Get a batch of inputs and outputs from a list of sentences."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_feature_layers(self, input_layer=None, trainable=False, use_weighted_sum=False):\n        model = keras.models.clone_model(self.model, input_layer)\n        if not trainable:\n            for layer in model.layers:\n                layer.trainable = False\n        if use_weighted_sum:\n            rnn_layers_forward = list(map(\n                lambda x: model.get_layer(x.name.split('/')[0].split(':')[0].split('_')[0]).output,\n                self.rnn_layers_forward,\n            ))\n            rnn_layers_backward = list(map(\n                lambda x: model.get_layer(x.name.split('/')[0].split(':')[0].split('_')[0]).output,\n                self.rnn_layers_backward,\n            ))\n            forward_layer = WeightedSum(name='Bi-LM-Forward-Sum')(rnn_layers_forward)\n            backward_layer_rev = WeightedSum(name='Bi-LM-Backward-Sum-Rev')(rnn_layers_backward)\n            backward_layer = keras.layers.Lambda(\n                function=self._reverse_x,\n                mask=lambda _, mask: self._reverse_x(mask),\n                name='Bi-LM-Backward-Sum'\n            )(backward_layer_rev)\n        else:\n            forward_layer = model.get_layer(name='Bi-LM-Forward').output\n            backward_layer = model.get_layer(name='Bi-LM-Backward').output\n        output_layer = keras.layers.Concatenate(name='Bi-LM-Feature')([forward_layer, backward_layer])\n        if input_layer is None:\n            input_layer = model.layers[0].input\n            return input_layer, output_layer\n        return output_layer", "response": "Get layers that output the Bi - LM feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Open file and parse into HmetRecords\n        with open(path, 'r') as orthoFile:\n            for line in orthoFile:\n                sline = line.strip().split()\n\n                # Cases\n                if sline[0].lower() == 'num_sites:':\n                    self.numSites = sline[1]\n                elif sline[0].lower() == 'elev_base':\n                    self.elevBase = sline[1]\n                elif sline[0].lower() == 'elev_2':\n                    self.elev2 = sline[1]\n                elif sline[0].lower() == 'year':\n                    \"\"\"DO NOTHING\"\"\"\n                else:\n                    # Create datetime object\n                    dateTime = datetime(year=int(sline[0]),\n                                        month=int(sline[1]),\n                                        day=int(sline[2]),\n                                        hour=int(sline[3]))\n\n                    # Create GSSHAPY OrthoMeasurement object\n                    measurement = OrographicMeasurement(dateTime=dateTime,\n                                                        temp2=sline[4])\n\n                    # Associate OrthoMeasurement with OrthographicGageFile\n                    self.orographicMeasurements.append(measurement)", "response": "Method to read from file and populate the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write(self, session, openFile, replaceParamFile):\n        # Write lines\n        openFile.write('Num_Sites:    %s\\n' % self.numSites)\n        openFile.write('Elev_Base     %s\\n' % self.elevBase)\n        openFile.write('Elev_2        %s\\n' % self.elev2)\n        openFile.write('Year    Month   Day     Hour    Temp_2\\n')\n\n        # Retrieve OrographicMeasurements\n        measurements = self.orographicMeasurements\n\n        for measurement in measurements:\n            dateTime = measurement.dateTime\n            openFile.write('%s%s%s%s%s%s%s%s%.3f\\n' % (\n                dateTime.year,\n                '    ',\n                dateTime.month,\n                ' ' * (8 - len(str(dateTime.month))),\n                dateTime.day,\n                ' ' * (8 - len(str(dateTime.day))),\n                dateTime.hour,\n                ' ' * (8 - len(str(dateTime.hour))),\n                measurement.temp2))", "response": "Method to write the Orographic Gage File"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getFluvialLinks(self):\n        # Define fluvial types\n        fluvialTypeKeywords = ('TRAPEZOID', 'TRAP', 'BREAKPOINT', 'ERODE', 'SUBSURFACE')\n\n        fluvialLinks = []\n\n        for link in self.streamLinks:\n            for fluvialTypeKeyword in fluvialTypeKeywords:\n                if fluvialTypeKeyword in link.type:\n                    fluvialLinks.append(link)\n                    break\n\n        return fluvialLinks", "response": "Retrieve only the links that represent fluvial portions of the stream. Returns a list of StreamLink instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the links in the order of the link number.", "response": "def getOrderedLinks(self, session):\n        \"\"\"\n        Retrieve the links in the order of the link number.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n\n        Returns:\n            list: A list of :class:`.StreamLink` objects.\n        \"\"\"\n        streamLinks = session.query(StreamLink).\\\n                            filter(StreamLink.channelInputFile == self).\\\n                            order_by(StreamLink.linkNumber).\\\n                            all()\n\n        return streamLinks"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getStreamNetworkAsKml(self, session, path=None, documentName='Stream Network', withNodes=False, styles={}):\n        # Retrieve Stream Links\n        links = self.getFluvialLinks()\n\n        # Set Default Styles\n        lineColorValue = (255, 255, 0, 0)  # Blue\n        lineWidthValue = 2\n        nodeIconHrefValue = 'http://maps.google.com/mapfiles/kml/paddle/red-circle.png'\n        nodeIconScaleValue = 1.0\n\n        if 'lineColor' in styles:\n            if len(styles['lineColor']) < 4:\n                log.warning('lineColor style must be a list or a tuple of four elements containing integer RGBA values.')\n            else:\n                userLineColor = styles['lineColor']\n                lineColorValue = (userLineColor[3], userLineColor[2], userLineColor[1], userLineColor[0])\n\n        if 'lineWidth' in styles:\n            try:\n                float(styles['lineWidth'])\n                lineWidthValue = styles['lineWidth']\n\n            except ValueError:\n                log.warning('lineWidth must be a valid number containing the width of the line in pixels.')\n\n        if 'nodeIconHref' in styles:\n            nodeIconHrefValue = styles['nodeIconHref']\n\n        if 'nodeIconScale' in styles:\n            try:\n                float(styles['nodeIconScale'])\n                nodeIconScaleValue = styles['nodeIconScale']\n\n            except ValueError:\n                log.warning('nodeIconScaleValue must be a valid number containing the width of the line in pixels.')\n\n\n        # Initialize KML Document\n        kml = ET.Element('kml', xmlns='http://www.opengis.net/kml/2.2')\n        document = ET.SubElement(kml, 'Document')\n        docName = ET.SubElement(document, 'name')\n        docName.text = documentName\n\n        for link in links:\n            placemark = ET.SubElement(document, 'Placemark')\n            placemarkName = ET.SubElement(placemark, 'name')\n            placemarkName.text = str(link.linkNumber)\n\n            # Create style tag and setup styles\n            styles = ET.SubElement(placemark, 'Style')\n\n            # Set line style\n            lineStyle = ET.SubElement(styles, 'LineStyle')\n            lineColor = ET.SubElement(lineStyle, 'color')\n            lineColor.text = '%02X%02X%02X%02X' % lineColorValue\n            lineWidth = ET.SubElement(lineStyle, 'width')\n            lineWidth.text = str(lineWidthValue)\n\n            # Add the geometry to placemark\n            linkKML = link.getAsKml(session)\n            if linkKML:\n                lineString = ET.fromstring(linkKML)\n                placemark.append(lineString)\n            else:\n                log.warning(\"No geometry found for link with id {0}\".format(link.id))\n\n            if withNodes:\n                # Create the node styles\n                nodeStyles = ET.SubElement(document, 'Style', id='node_styles')\n\n                # Hide labels\n                nodeLabelStyle = ET.SubElement(nodeStyles, 'LabelStyle')\n                nodeLabelScale = ET.SubElement(nodeLabelStyle, 'scale')\n                nodeLabelScale.text = str(0)\n\n                # Style icon\n                nodeIconStyle = ET.SubElement(nodeStyles, 'IconStyle')\n\n                # Set icon\n                nodeIcon = ET.SubElement(nodeIconStyle, 'Icon')\n                iconHref = ET.SubElement(nodeIcon, 'href')\n                iconHref.text = nodeIconHrefValue\n\n                # Set icon scale\n                iconScale = ET.SubElement(nodeIconStyle, 'scale')\n                iconScale.text = str(nodeIconScaleValue)\n\n                for node in link.nodes:\n                    # New placemark for each node\n                    nodePlacemark = ET.SubElement(document, 'Placemark')\n                    nodePlacemarkName = ET.SubElement(nodePlacemark, 'name')\n                    nodePlacemarkName.text = str(node.nodeNumber)\n\n                    # Styles for the node\n                    nodeStyleUrl = ET.SubElement(nodePlacemark, 'styleUrl')\n                    nodeStyleUrl.text = '#node_styles'\n\n                    nodeString = ET.fromstring(node.getAsKml(session))\n                    nodePlacemark.append(nodeString)\n\n                    # Embed node data\n                    nodeExtendedData = ET.SubElement(nodePlacemark, 'ExtendedData')\n\n                    nodeNumberData = ET.SubElement(nodeExtendedData, 'Data', name='node_number')\n                    nodeNumberValue = ET.SubElement(nodeNumberData, 'value')\n                    nodeNumberValue.text = str(node.nodeNumber)\n\n                    nodeLinkNumberData = ET.SubElement(nodeExtendedData, 'Data', name='link_number')\n                    nodeLinkNumberValue = ET.SubElement(nodeLinkNumberData, 'value')\n                    nodeLinkNumberValue.text = str(link.linkNumber)\n\n                    nodeElevationData = ET.SubElement(nodeExtendedData, 'Data', name='elevation')\n                    nodeElevationValue = ET.SubElement(nodeElevationData, 'value')\n                    nodeElevationValue.text = str(node.elevation)\n\n            # Create the data tag\n            extendedData = ET.SubElement(placemark, 'ExtendedData')\n\n            # Add value to data\n            linkNumberData = ET.SubElement(extendedData, 'Data', name='link_number')\n            linkNumberValue = ET.SubElement(linkNumberData, 'value')\n            linkNumberValue.text = str(link.linkNumber)\n\n            linkTypeData = ET.SubElement(extendedData, 'Data', name='link_type')\n            linkTypeValue = ET.SubElement(linkTypeData, 'value')\n            linkTypeValue.text = str(link.type)\n\n            numElementsData = ET.SubElement(extendedData, 'Data', name='number_elements')\n            numElementsValue = ET.SubElement(numElementsData, 'value')\n            numElementsValue.text = str(link.numElements)\n\n            dxData = ET.SubElement(extendedData, 'Data', name='dx')\n            dxValue = ET.SubElement(dxData, 'value')\n            dxValue.text = str(link.dx)\n\n            erodeData = ET.SubElement(extendedData, 'Data', name='erode')\n            erodeValue = ET.SubElement(erodeData, 'value')\n            erodeValue.text = str(link.type)\n\n            subsurfaceData = ET.SubElement(extendedData, 'Data', name='subsurface')\n            subsurfaceValue = ET.SubElement(subsurfaceData, 'value')\n            subsurfaceValue.text = str(link.type)\n\n        kmlString = ET.tostring(kml)\n\n        if path:\n            with open(path, 'w') as f:\n                f.write(kmlString)\n\n        return kmlString", "response": "Returns the stream network visualization in KML format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the stream network geometry in Well Known Text format.", "response": "def getStreamNetworkAsWkt(self, session, withNodes=True):\n        \"\"\"\n        Retrieve the stream network geometry in Well Known Text format.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database\n            withNodes (bool, optional): Include nodes. Defaults to False.\n\n        Returns:\n            str: Well Known Text string.\n        \"\"\"\n        wkt_list = []\n\n        for link in self.streamLinks:\n            wkt_link = link.getAsWkt(session)\n\n            if wkt_link:\n                wkt_list.append(wkt_link)\n\n            if withNodes:\n                for node in link.nodes:\n                    wkt_node = node.getAsWkt(session)\n\n                    if wkt_node:\n                        wkt_list.append(wkt_node)\n\n        return 'GEOMCOLLECTION ({0})'.format(', '.join(wkt_list))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getStreamNetworkAsGeoJson(self, session, withNodes=True):\n        features_list = []\n\n        # Assemble link features\n        for link in self.streamLinks:\n            link_geoJson = link.getAsGeoJson(session)\n\n            if link_geoJson:\n                link_geometry = json.loads(link.getAsGeoJson(session))\n\n                link_properties = {\"link_number\": link.linkNumber,\n                                   \"type\": link.type,\n                                   \"num_elements\": link.numElements,\n                                   \"dx\": link.dx,\n                                   \"erode\": link.erode,\n                                   \"subsurface\": link.subsurface}\n\n                link_feature = {\"type\": \"Feature\",\n                                \"geometry\": link_geometry,\n                                \"properties\": link_properties,\n                                \"id\": link.id}\n\n                features_list.append(link_feature)\n\n            # Assemble node features\n            if withNodes:\n                for node in link.nodes:\n                    node_geoJson = node.getAsGeoJson(session)\n\n                    if node_geoJson:\n                        node_geometry = json.loads(node_geoJson)\n\n                    node_properties = {\"link_number\": link.linkNumber,\n                                       \"node_number\": node.nodeNumber,\n                                       \"elevation\": node.elevation}\n\n                    node_feature = {\"type\": \"Feature\",\n                                    \"geometry\": node_geometry,\n                                    \"properties\": node_properties,\n                                    \"id\": node.id}\n\n                    features_list.append(node_feature)\n\n        feature_collection = {\"type\": \"FeatureCollection\",\n                              \"features\": features_list}\n\n        return json.dumps(feature_collection)", "response": "Retrieve the stream network geometry in GeoJSON format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Set file extension property\n        self.fileExtension = extension\n\n        # Dictionary of keywords/cards and parse function names\n        KEYWORDS = {'ALPHA': cic.cardChunk,\n                    'BETA': cic.cardChunk,\n                    'THETA': cic.cardChunk,\n                    'LINKS': cic.cardChunk,\n                    'MAXNODES': cic.cardChunk,\n                    'CONNECT': cic.connectChunk,\n                    'LINK': cic.linkChunk}\n\n        links = []\n        connectivity = []\n\n        # Parse file into chunks associated with keywords/cards\n        with open(path, 'r') as f:\n            chunks = pt.chunk(KEYWORDS, f)\n\n        # Parse chunks associated with each key\n        for key, chunkList in iteritems(chunks):\n            # Parse each chunk in the chunk list\n            for chunk in chunkList:\n                # Call chunk specific parsers for each chunk\n                result = KEYWORDS[key](key, chunk)\n\n                # Cases\n                if key == 'LINK':\n                    # Link handler\n                    links.append(self._createLink(result, replaceParamFile))\n\n                elif key == 'CONNECT':\n                    # Connectivity handler\n                    connectivity.append(result)\n\n                else:\n                    # Global variable handler\n                    card = result['card']\n                    value = result['values'][0]\n                    # Cases\n                    if card == 'LINKS':\n                        self.links = int(value)\n                    elif card == 'MAXNODES':\n                        self.maxNodes = int(value)\n                    elif card == 'ALPHA':\n                        self.alpha = float(vrp(value, replaceParamFile))\n                    elif card == 'BETA':\n                        self.beta = float(vrp(value, replaceParamFile))\n                    elif card == 'THETA':\n                        self.theta = float(vrp(value, replaceParamFile))\n\n        self._createConnectivity(linkList=links, connectList=connectivity)\n\n        if spatial:\n            self._createGeometry(session, spatialReferenceID)", "response": "Method to read from a Channel Input File"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchannel Input File Write to File Method", "response": "def _write(self, session, openFile, replaceParamFile):\n        \"\"\"\n        Channel Input File Write to File Method\n        \"\"\"\n        # Write lines\n        openFile.write('GSSHA_CHAN\\n')\n\n        alpha = vwp(self.alpha, replaceParamFile)\n        try:\n            openFile.write('ALPHA%s%.6f\\n' % (' ' * 7, alpha))\n        except:\n            openFile.write('ALPHA%s%s\\n' % (' ' * 7, alpha))\n\n        beta = vwp(self.beta, replaceParamFile)\n        try:\n            openFile.write('BETA%s%.6f\\n' % (' ' * 8, beta))\n        except:\n            openFile.write('BETA%s%s\\n' % (' ' * 8, beta))\n\n        theta = vwp(self.theta, replaceParamFile)\n        try:\n            openFile.write('THETA%s%.6f\\n' % (' ' * 7, theta))\n        except:\n            openFile.write('THETA%s%s\\n' % (' ' * 7, theta))\n\n        openFile.write('LINKS%s%s\\n' % (' ' * 7, self.links))\n        openFile.write('MAXNODES%s%s\\n' % (' ' * 4, self.maxNodes))\n\n        # Retrieve StreamLinks\n        links = self.getOrderedLinks(session)\n\n        self._writeConnectivity(links=links,\n                                fileObject=openFile)\n        self._writeLinks(links=links,\n                         fileObject=openFile,\n                         replaceParamFile=replaceParamFile)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating GSSHAPY Link Object Method Method", "response": "def _createLink(self, linkResult, replaceParamFile):\n        \"\"\"\n        Create GSSHAPY Link Object Method\n        \"\"\"\n        link = None\n\n        # Cases\n        if linkResult['type'] == 'XSEC':\n            # Cross section link handler\n            link = self._createCrossSection(linkResult, replaceParamFile)\n\n        elif linkResult['type'] == 'STRUCTURE':\n            # Structure link handler\n            link = self._createStructure(linkResult, replaceParamFile)\n\n        elif linkResult['type'] in ('RESERVOIR', 'LAKE'):\n            # Reservoir/lake handler\n            link = self._createReservoir(linkResult, replaceParamFile)\n\n        return link"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _createConnectivity(self, linkList, connectList):\n        # Create StreamLink-Connectivity Pairs\n\n        for idx, link in enumerate(linkList):\n\n            connectivity = connectList[idx]\n\n            # Initialize GSSHAPY UpstreamLink objects\n            for upLink in connectivity['upLinks']:\n                upstreamLink = UpstreamLink(upstreamLinkID=int(upLink))\n                upstreamLink.streamLink = link\n\n            link.downstreamLinkID = int(connectivity['downLink'])\n            link.numUpstreamLinks = int(connectivity['numUpLinks'])", "response": "Create GSSHAPY Connect Object Method Method"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates GSSHAPY Cross Section Objects Method", "response": "def _createCrossSection(self, linkResult, replaceParamFile):\n        \"\"\"\n        Create GSSHAPY Cross Section Objects Method\n        \"\"\"\n        # Extract header variables from link result object\n        header = linkResult['header']\n\n        # Initialize GSSHAPY StreamLink object\n        link = StreamLink(linkNumber=int(header['link']),\n                          type=header['xSecType'],\n                          numElements=header['nodes'],\n                          dx=vrp(header['dx'], replaceParamFile),\n                          erode=header['erode'],\n                          subsurface=header['subsurface'])\n\n        # Associate StreamLink with ChannelInputFile\n        link.channelInputFile = self\n\n        # Initialize GSSHAPY TrapezoidalCS or BreakpointCS objects\n        xSection = linkResult['xSection']\n\n        # Cases\n        if 'TRAPEZOID' in link.type or 'TRAP' in link.type:\n            # Trapezoid cross section handler\n            # Initialize GSSHPY TrapeziodalCS object\n            trapezoidCS = TrapezoidalCS(mannings_n=vrp(xSection['mannings_n'], replaceParamFile),\n                                        bottomWidth=vrp(xSection['bottom_width'], replaceParamFile),\n                                        bankfullDepth=vrp(xSection['bankfull_depth'], replaceParamFile),\n                                        sideSlope=vrp(xSection['side_slope'], replaceParamFile),\n                                        mRiver=vrp(xSection['m_river'], replaceParamFile),\n                                        kRiver=vrp(xSection['k_river'], replaceParamFile),\n                                        erode=xSection['erode'],\n                                        subsurface=xSection['subsurface'],\n                                        maxErosion=vrp(xSection['max_erosion'], replaceParamFile))\n\n            # Associate TrapezoidalCS with StreamLink\n            trapezoidCS.streamLink = link\n\n        elif 'BREAKPOINT' in link.type:\n            # Breakpoint cross section handler\n            # Initialize GSSHAPY BreakpointCS objects\n            breakpointCS = BreakpointCS(mannings_n=vrp(xSection['mannings_n'], replaceParamFile),\n                                        numPairs=xSection['npairs'],\n                                        numInterp=vrp(xSection['num_interp'], replaceParamFile),\n                                        mRiver=vrp(xSection['m_river'], replaceParamFile),\n                                        kRiver=vrp(xSection['k_river'], replaceParamFile),\n                                        erode=xSection['erode'],\n                                        subsurface=xSection['subsurface'],\n                                        maxErosion=vrp(xSection['max_erosion'], replaceParamFile))\n\n            # Associate BreakpointCS with StreamLink\n            breakpointCS.streamLink = link\n\n            # Create GSSHAPY Breakpoint objects\n            for b in xSection['breakpoints']:\n                breakpoint = Breakpoint(x=b['x'],\n                                        y=b['y'])\n\n                # Associate Breakpoint with BreakpointCS\n                breakpoint.crossSection = breakpointCS\n\n        # Initialize GSSHAPY StreamNode objects\n        for n in linkResult['nodes']:\n            # Initialize GSSHAPY StreamNode object\n            node = StreamNode(nodeNumber=int(n['node']),\n                              x=n['x'],\n                              y=n['y'],\n                              elevation=n['elev'])\n\n            # Associate StreamNode with StreamLink\n            node.streamLink = link\n\n        return link"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating GSSHAPY Structure Objects Method", "response": "def _createStructure(self, linkResult, replaceParamFile):\n        \"\"\"\n        Create GSSHAPY Structure Objects Method\n        \"\"\"\n        # Constants\n        WEIRS = ('WEIR', 'SAG_WEIR')\n\n        CULVERTS = ('ROUND_CULVERT', 'RECT_CULVERT')\n\n        CURVES = ('RATING_CURVE', 'SCHEDULED_RELEASE', 'RULE_CURVE')\n\n        header = linkResult['header']\n\n        # Initialize GSSHAPY StreamLink object\n        link = StreamLink(linkNumber=header['link'],\n                          type=linkResult['type'],\n                          numElements=header['numstructs'])\n\n        # Associate StreamLink with ChannelInputFile\n        link.channelInputFile = self\n\n        # Create Structure objects\n        for s in linkResult['structures']:\n            structType = s['structtype']\n\n            # Cases\n            if structType in WEIRS:\n                # Weir type handler\n                # Initialize GSSHAPY Weir object\n                weir = Weir(type=structType,\n                            crestLength=vrp(s['crest_length'], replaceParamFile),\n                            crestLowElevation=vrp(s['crest_low_elev'], replaceParamFile),\n                            dischargeCoeffForward=vrp(s['discharge_coeff_forward'], replaceParamFile),\n                            dischargeCoeffReverse=vrp(s['discharge_coeff_reverse'], replaceParamFile),\n                            crestLowLocation=vrp(s['crest_low_loc'], replaceParamFile),\n                            steepSlope=vrp(s['steep_slope'], replaceParamFile),\n                            shallowSlope=vrp(s['shallow_slope'], replaceParamFile))\n\n                # Associate Weir with StreamLink\n                weir.streamLink = link\n\n            elif structType in CULVERTS:\n                # Culvert type handler\n                # Initialize GSSHAPY Culvert object\n                culvert = Culvert(type=structType,\n                                  upstreamInvert=vrp(s['upinvert'], replaceParamFile),\n                                  downstreamInvert=vrp(s['downinvert'], replaceParamFile),\n                                  inletDischargeCoeff=vrp(s['inlet_disch_coeff'], replaceParamFile),\n                                  reverseFlowDischargeCoeff=vrp(s['rev_flow_disch_coeff'], replaceParamFile),\n                                  slope=vrp(s['slope'], replaceParamFile),\n                                  length=vrp(s['length'], replaceParamFile),\n                                  roughness=vrp(s['rough_coeff'], replaceParamFile),\n                                  diameter=vrp(s['diameter'], replaceParamFile),\n                                  width=vrp(s['width'], replaceParamFile),\n                                  height=vrp(s['height'], replaceParamFile))\n\n                # Associate Culvert with StreamLink\n                culvert.streamLink = link\n\n            elif structType in CURVES:\n                # Curve type handler\n                pass\n\n        return link"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _createGeometry(self, session, spatialReferenceID):\n        # Flush the current session\n        session.flush()\n\n        # Create geometry for each fluvial link\n        for link in self.getFluvialLinks():\n\n            # Retrieve the nodes for each link\n            nodes = link.nodes\n            nodeCoordinates = []\n\n            # Create geometry for each node\n            for node in nodes:\n                # Assemble coordinates in well known text format\n                coordinates = '{0} {1} {2}'.format(node.x, node.y, node.elevation)\n                nodeCoordinates.append(coordinates)\n\n                # Create well known text string for point with z coordinate\n                wktPoint = 'POINT Z ({0})'.format(coordinates)\n\n                # Write SQL statement\n                statement = self._getUpdateGeometrySqlString(geometryID=node.id,\n                                                             tableName=node.tableName,\n                                                             spatialReferenceID=spatialReferenceID,\n                                                             wktString=wktPoint)\n\n                session.execute(statement)\n\n            # Assemble line string in well known text format\n            wktLineString = 'LINESTRING Z ({0})'.format(', '.join(nodeCoordinates))\n\n            # Write SQL statement\n            statement = self._getUpdateGeometrySqlString(geometryID=link.id,\n                                                         tableName=link.tableName,\n                                                         spatialReferenceID=spatialReferenceID,\n                                                         wktString=wktLineString)\n\n            session.execute(statement)", "response": "Create postGIS geometric objects for fluvial objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites Connectivity Lines to File Method", "response": "def _writeConnectivity(self, links, fileObject):\n        \"\"\"\n        Write Connectivity Lines to File Method\n        \"\"\"\n        for link in links:\n            linkNum = link.linkNumber\n            downLink = link.downstreamLinkID\n            numUpLinks = link.numUpstreamLinks\n            upLinks = ''\n            for upLink in link.upstreamLinks:\n                upLinks = '{}{:>5}'.format(upLinks, str(upLink.upstreamLinkID))\n\n            line = 'CONNECT{:>5}{:>5}{:>5}{}\\n'.format(linkNum, downLink, numUpLinks, upLinks)\n            fileObject.write(line)\n        fileObject.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _writeLinks(self, links, fileObject, replaceParamFile):\n        for link in links:\n            linkType = link.type\n            fileObject.write('LINK           %s\\n' % link.linkNumber)\n\n            # Cases\n            if 'TRAP' in linkType or 'TRAPEZOID' in linkType or 'BREAKPOINT' in linkType:\n                self._writeCrossSectionLink(link, fileObject, replaceParamFile)\n\n            elif linkType == 'STRUCTURE':\n                self._writeStructureLink(link, fileObject, replaceParamFile)\n\n            elif linkType in ('RESERVOIR', 'LAKE'):\n                self._writeReservoirLink(link, fileObject, replaceParamFile)\n\n            else:\n                log.error('OOPS: CIF LINE 417')  # THIS SHOULDN'T HAPPEN\n\n            fileObject.write('\\n')", "response": "Write Link Lines to File Method\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _writeReservoirLink(self, link, fileObject, replaceParamFile):\n        fileObject.write('%s\\n' % link.type)\n\n        # Retrieve reservoir\n        reservoir = link.reservoir\n\n        # Reservoir parameters\n        initWSE = vwp(reservoir.initWSE, replaceParamFile)\n        minWSE = vwp(reservoir.minWSE, replaceParamFile)\n        maxWSE = vwp(reservoir.maxWSE, replaceParamFile)\n        numElements = link.numElements\n\n        # Cases\n        if link.type == 'LAKE':\n            # Lake handler\n            try:\n                fileObject.write('INITWSE      %.6f\\n' % initWSE)\n            except:\n                fileObject.write('INITWSE      %s\\n' % initWSE)\n\n            try:\n                fileObject.write('MINWSE       %.6f\\n' % minWSE)\n            except:\n                fileObject.write('MINWSE       %s\\n' % minWSE)\n\n            try:\n                fileObject.write('MAXWSE       %.6f\\n' % maxWSE)\n            except:\n                fileObject.write('MAXWSE       %s\\n' % maxWSE)\n\n            fileObject.write('NUMPTS       %s\\n' % numElements)\n\n        elif link.type == 'RESERVOIR':\n            # Reservoir handler\n            try:\n                fileObject.write('RES_INITWSE      %.6f\\n' % initWSE)\n            except:\n                fileObject.write('RES_INITWSE      %s\\n' % initWSE)\n\n            try:\n                fileObject.write('RES_MINWSE       %.6f\\n' % minWSE)\n            except:\n                fileObject.write('RES_MINWSE       %s\\n' % minWSE)\n\n            try:\n                fileObject.write('RES_MAXWSE       %.6f\\n' % maxWSE)\n            except:\n                fileObject.write('RES_MAXWSE       %s\\n' % maxWSE)\n\n            fileObject.write('RES_NUMPTS       %s\\n' % numElements)\n\n        # Retrieve reservoir points\n        points = reservoir.reservoirPoints\n\n        for idx, point in enumerate(points):\n            if ((idx + 1) % 10) != 0:\n                fileObject.write('%s  %s     ' % (point.i, point.j))\n            else:\n                fileObject.write('%s  %s\\n' % (point.i, point.j))\n\n        if (link.numElements % 10) != 0:\n            fileObject.write('\\n')", "response": "Write Reservoir Link to File Method"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite Structure Link to File Method", "response": "def _writeStructureLink(self, link, fileObject, replaceParamFile):\n        \"\"\"\n        Write Structure Link to File Method\n        \"\"\"\n        fileObject.write('%s\\n' % link.type)\n        fileObject.write('NUMSTRUCTS     %s\\n' % link.numElements)\n\n        # Retrieve lists of structures\n        weirs = link.weirs\n        culverts = link.culverts\n\n        # Write weirs to file\n        for weir in weirs:\n            fileObject.write('STRUCTTYPE     %s\\n' % weir.type)\n\n            # Check for replacement vars\n            crestLength = vwp(weir.crestLength, replaceParamFile)\n            crestLowElevation = vwp(weir.crestLowElevation, replaceParamFile)\n            dischargeCoeffForward = vwp(weir.dischargeCoeffForward, replaceParamFile)\n            dischargeCoeffReverse = vwp(weir.dischargeCoeffReverse, replaceParamFile)\n            crestLowLocation = vwp(weir.crestLowLocation, replaceParamFile)\n            steepSlope = vwp(weir.steepSlope, replaceParamFile)\n            shallowSlope = vwp(weir.shallowSlope, replaceParamFile)\n\n            if weir.crestLength != None:\n                try:\n                    fileObject.write('CREST_LENGTH             %.6f\\n' % crestLength)\n                except:\n                    fileObject.write('CREST_LENGTH             %s\\n' % crestLength)\n\n            if weir.crestLowElevation != None:\n                try:\n                    fileObject.write('CREST_LOW_ELEV           %.6f\\n' % crestLowElevation)\n                except:\n                    fileObject.write('CREST_LOW_ELEV           %s\\n' % crestLowElevation)\n\n            if weir.dischargeCoeffForward != None:\n                try:\n                    fileObject.write('DISCHARGE_COEFF_FORWARD  %.6f\\n' % dischargeCoeffForward)\n                except:\n                    fileObject.write('DISCHARGE_COEFF_FORWARD  %s\\n' % dischargeCoeffForward)\n\n            if weir.dischargeCoeffReverse != None:\n                try:\n                    fileObject.write('DISCHARGE_COEFF_REVERSE  %.6f\\n' % dischargeCoeffReverse)\n                except:\n                    fileObject.write('DISCHARGE_COEFF_REVERSE  %s\\n' % dischargeCoeffReverse)\n\n            if weir.crestLowLocation != None:\n                    fileObject.write('CREST_LOW_LOC            %s\\n' % crestLowLocation)\n\n            if weir.steepSlope != None:\n                try:\n                    fileObject.write('STEEP_SLOPE              %.6f\\n' % steepSlope)\n                except:\n                    fileObject.write('STEEP_SLOPE              %s\\n' % steepSlope)\n\n            if weir.shallowSlope != None:\n                try:\n                    fileObject.write('SHALLOW_SLOPE            %.6f\\n' % shallowSlope)\n                except:\n                    fileObject.write('SHALLOW_SLOPE            %s\\n' % shallowSlope)\n\n        # Write culverts to file\n        for culvert in culverts:\n            fileObject.write('STRUCTTYPE     %s\\n' % culvert.type)\n\n            # Check for replacement vars\n            upstreamInvert = vwp(culvert.upstreamInvert, replaceParamFile)\n            downstreamInvert = vwp(culvert.downstreamInvert, replaceParamFile)\n            inletDischargeCoeff = vwp(culvert.inletDischargeCoeff, replaceParamFile)\n            reverseFlowDischargeCoeff = vwp(culvert.reverseFlowDischargeCoeff, replaceParamFile)\n            slope = vwp(culvert.slope, replaceParamFile)\n            length = vwp(culvert.length, replaceParamFile)\n            roughness = vwp(culvert.roughness, replaceParamFile)\n            diameter = vwp(culvert.diameter, replaceParamFile)\n            width = vwp(culvert.width, replaceParamFile)\n            height = vwp(culvert.height, replaceParamFile)\n\n            if culvert.upstreamInvert != None:\n                try:\n                    fileObject.write('UPINVERT                 %.6f\\n' % upstreamInvert)\n                except:\n                    fileObject.write('UPINVERT                 %s\\n' % upstreamInvert)\n\n            if culvert.downstreamInvert != None:\n                try:\n                    fileObject.write('DOWNINVERT               %.6f\\n' % downstreamInvert)\n                except:\n                    fileObject.write('DOWNINVERT               %s\\n' % downstreamInvert)\n\n            if culvert.inletDischargeCoeff != None:\n                try:\n                    fileObject.write('INLET_DISCH_COEFF        %.6f\\n' % inletDischargeCoeff)\n                except:\n                    fileObject.write('INLET_DISCH_COEFF        %s\\n' % inletDischargeCoeff)\n\n            if culvert.reverseFlowDischargeCoeff != None:\n                try:\n                    fileObject.write('REV_FLOW_DISCH_COEFF     %.6f\\n' % reverseFlowDischargeCoeff)\n                except:\n                    fileObject.write('REV_FLOW_DISCH_COEFF     %s\\n' % reverseFlowDischargeCoeff)\n\n            if culvert.slope != None:\n                try:\n                    fileObject.write('SLOPE                    %.6f\\n' % slope)\n                except:\n                    fileObject.write('SLOPE                    %s\\n' % slope)\n\n            if culvert.length != None:\n                try:\n                    fileObject.write('LENGTH                   %.6f\\n' % length)\n                except:\n                    fileObject.write('LENGTH                   %s\\n' % length)\n\n            if culvert.roughness != None:\n                try:\n                    fileObject.write('ROUGH_COEFF              %.6f\\n' % roughness)\n                except:\n                    fileObject.write('ROUGH_COEFF              %s\\n' % roughness)\n\n            if culvert.diameter != None:\n                try:\n                    fileObject.write('DIAMETER                 %.6f\\n' % diameter)\n                except:\n                    fileObject.write('DIAMETER                 %s\\n' % diameter)\n\n\n            if culvert.width != None:\n                try:\n                    fileObject.write('WIDTH                    %.6f\\n' % width)\n                except:\n                    fileObject.write('WIDTH                    %s\\n' % width)\n\n            if culvert.height != None:\n                try:\n                    fileObject.write('HEIGHT                   %.6f\\n' % height)\n                except:\n                    fileObject.write('HEIGHT                   %s\\n' % height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _writeCrossSectionLink(self, link, fileObject, replaceParamFile):\n        linkType = link.type\n\n        # Write cross section link header\n        dx = vwp(link.dx, replaceParamFile)\n        try:\n            fileObject.write('DX             %.6f\\n' % dx)\n        except:\n            fileObject.write('DX             %s\\n' % dx)\n\n        fileObject.write('%s\\n' % linkType)\n        fileObject.write('NODES          %s\\n' % link.numElements)\n\n        for node in link.nodes:\n            # Write node information\n            fileObject.write('NODE %s\\n' % node.nodeNumber)\n            fileObject.write('X_Y  %.6f %.6f\\n' % (node.x, node.y))\n            fileObject.write('ELEV %.6f\\n' % node.elevation)\n\n            if node.nodeNumber == 1:\n                # Write cross section information after first node\n                fileObject.write('XSEC\\n')\n\n                # Cases\n                if 'TRAPEZOID' in linkType or 'TRAP' in linkType:\n                    # Retrieve cross section\n                    xSec = link.trapezoidalCS\n\n                    # Write cross section properties\n                    mannings_n = vwp(xSec.mannings_n, replaceParamFile)\n                    bottomWidth = vwp(xSec.bottomWidth, replaceParamFile)\n                    bankfullDepth = vwp(xSec.bankfullDepth, replaceParamFile)\n                    sideSlope = vwp(xSec.sideSlope, replaceParamFile)\n\n                    try:\n                        fileObject.write('MANNINGS_N     %.6f\\n' % mannings_n)\n                    except:\n                        fileObject.write('MANNINGS_N     %s\\n' % mannings_n)\n\n                    try:\n                        fileObject.write('BOTTOM_WIDTH   %.6f\\n' % bottomWidth)\n                    except:\n                        fileObject.write('BOTTOM_WIDTH   %s\\n' % bottomWidth)\n\n                    try:\n                        fileObject.write('BANKFULL_DEPTH %.6f\\n' % bankfullDepth)\n                    except:\n                        fileObject.write('BANKFULL_DEPTH %s\\n' % bankfullDepth)\n\n                    try:\n                        fileObject.write('SIDE_SLOPE     %.6f\\n' % sideSlope)\n                    except:\n                        fileObject.write('SIDE_SLOPE     %s\\n' % sideSlope)\n\n                    # Write optional cross section properties\n                    self._writeOptionalXsecCards(fileObject=fileObject, xSec=xSec, replaceParamFile=replaceParamFile)\n\n                elif 'BREAKPOINT' in linkType:\n                    # Retrieve cross section\n                    xSec = link.breakpointCS\n\n                    # Write cross section properties\n                    mannings_n = vwp(xSec.mannings_n, replaceParamFile)\n                    try:\n                        fileObject.write('MANNINGS_N     %.6f\\n' % mannings_n)\n                    except:\n                        fileObject.write('MANNINGS_N     %s\\n' % mannings_n)\n\n                    fileObject.write('NPAIRS         %s\\n' % xSec.numPairs)\n                    fileObject.write('NUM_INTERP     %s\\n' % vwp(xSec.numInterp, replaceParamFile))\n\n\n                    # Write optional cross section properties\n                    self._writeOptionalXsecCards(fileObject=fileObject, xSec=xSec, replaceParamFile=replaceParamFile)\n\n                    # Write breakpoint lines\n                    for bp in xSec.breakpoints:\n                        fileObject.write('X1   %.6f %.6f\\n' % (bp.x, bp.y))\n                else:\n                    log.error('OOPS: MISSED A CROSS SECTION TYPE. CIF LINE 580. {0}'.format(linkType))", "response": "Write Cross Section Link to File Method"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _writeOptionalXsecCards(self, fileObject, xSec, replaceParamFile):\n        if xSec.erode:\n            fileObject.write('ERODE\\n')\n\n        if xSec.maxErosion != None:\n            fileObject.write('MAX_EROSION    %.6f\\n' % xSec.maxErosion)\n\n        if xSec.subsurface:\n            fileObject.write('SUBSURFACE\\n')\n\n        if xSec.mRiver != None:\n            mRiver = vwp(xSec.mRiver, replaceParamFile)\n            try:\n                fileObject.write('M_RIVER        %.6f\\n' % mRiver)\n            except:\n                fileObject.write('M_RIVER        %s\\n' % mRiver)\n\n        if xSec.kRiver != None:\n            kRiver = vwp(xSec.kRiver, replaceParamFile)\n            try:\n                fileObject.write('K_RIVER        %.6f\\n' % kRiver)\n            except:\n                fileObject.write('K_RIVER        %s\\n' % kRiver)", "response": "Write Optional Cross Section Cards to File Method"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces from_file with to_file", "response": "def replace_file(from_file, to_file):\n    \"\"\"\n    Replaces to_file with from_file\n    \"\"\"\n    try:\n        os.remove(to_file)\n    except OSError:\n        pass\n    copy(from_file, to_file)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prepare_lsm_gag(self):\n        lsm_required_vars = (self.lsm_precip_data_var,\n                             self.lsm_precip_type)\n\n        return self.lsm_input_valid and (None not in lsm_required_vars)", "response": "Determines whether to prepare gage data from LSM."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the file location of the card to new directory.", "response": "def _update_card_file_location(self, card_name, new_directory):\n        \"\"\"\n        Moves card to new gssha working directory\n        \"\"\"\n        with tmp_chdir(self.gssha_directory):\n            file_card = self.project_manager.getCard(card_name)\n            if file_card:\n                if file_card.value:\n                    original_location = file_card.value.strip(\"'\").strip('\"')\n                    new_location = os.path.join(new_directory,\n                                                os.path.basename(original_location))\n                    file_card.value = '\"{0}\"'.format(os.path.basename(original_location))\n                    try:\n                        move(original_location, new_location)\n                    except OSError as ex:\n                        log.warning(ex)\n                        pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_spt_forecast(self, extract_directory):\n        needed_vars = (self.spt_watershed_name,\n                       self.spt_subbasin_name,\n                       self.spt_forecast_date_string,\n                       self.ckan_engine_url,\n                       self.ckan_api_key,\n                       self.ckan_owner_organization)\n\n        if None not in needed_vars:\n\n            er_manager = ECMWFRAPIDDatasetManager(self.ckan_engine_url,\n                                                  self.ckan_api_key,\n                                                  self.ckan_owner_organization)\n            # TODO: Modify to only download one of the forecasts in the ensemble\n            er_manager.download_prediction_dataset(watershed=self.spt_watershed_name,\n                                                   subbasin=self.spt_subbasin_name,\n                                                   date_string=self.spt_forecast_date_string,  # '20160711.1200'\n                                                   extract_directory=extract_directory)\n\n            return glob(os.path.join(extract_directory, self.spt_forecast_date_string, \"Qout*52.nc\"))[0]\n\n        elif needed_vars.count(None) == len(needed_vars):\n            log.info(\"Skipping streamflow forecast download ...\")\n            return None\n        else:\n            raise ValueError(\"To download the forecasts, you need to set: \\n\"\n                             \"spt_watershed_name, spt_subbasin_name, spt_forecast_date_string \\n\"\n                             \"ckan_engine_url, ckan_api_key, and ckan_owner_organization.\")", "response": "Downloads the streamflow prediction tool forecast data from the specified extract directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare_hmet(self):\n        if self._prepare_lsm_hmet:\n            netcdf_file_path = None\n            hmet_ascii_output_folder = None\n            if self.output_netcdf:\n                netcdf_file_path = '{0}_hmet.nc'.format(self.project_manager.name)\n                if self.hotstart_minimal_mode:\n                    netcdf_file_path = '{0}_hmet_hotstart.nc'.format(self.project_manager.name)\n            else:\n                hmet_ascii_output_folder = 'hmet_data_{0}to{1}'\n                if self.hotstart_minimal_mode:\n                    hmet_ascii_output_folder += \"_hotstart\"\n\n            self.event_manager.prepare_hmet_lsm(self.lsm_data_var_map_array,\n                                                hmet_ascii_output_folder,\n                                                netcdf_file_path)\n            self.simulation_modified_input_cards += [\"HMET_NETCDF\",\n                                                     \"HMET_ASCII\"]\n        else:\n            log.info(\"HMET preparation skipped due to missing parameters ...\")", "response": "Prepare HMET data for simulation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare gage data for simulation", "response": "def prepare_gag(self):\n        \"\"\"\n        Prepare gage data for simulation\n        \"\"\"\n        if self._prepare_lsm_gag:\n            self.event_manager.prepare_gag_lsm(self.lsm_precip_data_var,\n                                               self.lsm_precip_type,\n                                               self.precip_interpolation_type)\n            self.simulation_modified_input_cards.append(\"PRECIP_FILE\")\n        else:\n            log.info(\"Gage file preparation skipped due to missing parameters ...\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rapid_to_gssha(self):\n        # if no streamflow given, download forecast\n        if self.path_to_rapid_qout is None and self.connection_list_file:\n            rapid_qout_directory = os.path.join(self.gssha_directory, 'rapid_streamflow')\n            try:\n                os.mkdir(rapid_qout_directory)\n            except OSError:\n                pass\n            self.path_to_rapid_qout = self.download_spt_forecast(rapid_qout_directory)\n\n        # prepare input for GSSHA if user wants\n        if self.path_to_rapid_qout is not None and self.connection_list_file:\n            self.event_manager.prepare_rapid_streamflow(self.path_to_rapid_qout,\n                                                        self.connection_list_file)\n            self.simulation_modified_input_cards.append('CHAN_POINT_INPUT')", "response": "Prepare RAPID data for simulation and GSSHA."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare simulation hotstart info", "response": "def hotstart(self):\n        \"\"\"\n        Prepare simulation hotstart info\n        \"\"\"\n        if self.write_hotstart:\n            hotstart_time_str = self.event_manager.simulation_end.strftime(\"%Y%m%d_%H%M\")\n            try:\n                os.mkdir('hotstart')\n            except OSError:\n                pass\n\n            ov_hotstart_path = os.path.join('..', 'hotstart',\n                                            '{0}_ov_hotstart_{1}.ovh'.format(self.project_manager.name,\n                                                                             hotstart_time_str))\n            self._update_card(\"WRITE_OV_HOTSTART\", ov_hotstart_path, True)\n            chan_hotstart_path = os.path.join('..', 'hotstart',\n                                              '{0}_chan_hotstart_{1}'.format(self.project_manager.name,\n                                                                             hotstart_time_str))\n            self._update_card(\"WRITE_CHAN_HOTSTART\", chan_hotstart_path, True)\n            sm_hotstart_path = os.path.join('..', 'hotstart',\n                                           '{0}_sm_hotstart_{1}.smh'.format(self.project_manager.name,\n                                                                            hotstart_time_str))\n            self._update_card(\"WRITE_SM_HOTSTART\", sm_hotstart_path, True)\n        else:\n            self._delete_card(\"WRITE_OV_HOTSTART\")\n            self._delete_card(\"WRITE_CHAN_HOTSTART\")\n            self._delete_card(\"WRITE_SM_HOTSTART\")\n\n        if self.read_hotstart:\n            hotstart_time_str = self.event_manager.simulation_start.strftime(\"%Y%m%d_%H%M\")\n            # OVERLAND\n            expected_ov_hotstart = os.path.join('hotstart',\n                                                '{0}_ov_hotstart_{1}.ovh'.format(self.project_manager.name,\n                                                                                  hotstart_time_str))\n            if os.path.exists(expected_ov_hotstart):\n                self._update_card(\"READ_OV_HOTSTART\", os.path.join(\"..\", expected_ov_hotstart), True)\n            else:\n                self._delete_card(\"READ_OV_HOTSTART\")\n                log.warning(\"READ_OV_HOTSTART not included as \"\n                         \"{0} does not exist ...\".format(expected_ov_hotstart))\n\n            # CHANNEL\n            expected_chan_hotstart = os.path.join('hotstart',\n                                                  '{0}_chan_hotstart_{1}'.format(self.project_manager.name,\n                                                                                 hotstart_time_str))\n            if os.path.exists(\"{0}.qht\".format(expected_chan_hotstart)) \\\n                    and os.path.exists(\"{0}.dht\".format(expected_chan_hotstart)):\n                self._update_card(\"READ_CHAN_HOTSTART\", os.path.join(\"..\", expected_chan_hotstart), True)\n            else:\n                self._delete_card(\"READ_CHAN_HOTSTART\")\n                log.warning(\"READ_CHAN_HOTSTART not included as \"\n                         \"{0}.qht and/or {0}.dht does not exist ...\".format(expected_chan_hotstart))\n\n            # INFILTRATION\n            expected_sm_hotstart = os.path.join('hotstart',\n                                                '{0}_sm_hotstart_{1}.smh'.format(self.project_manager.name,\n                                                                                 hotstart_time_str))\n            if os.path.exists(expected_sm_hotstart):\n                self._update_card(\"READ_SM_HOTSTART\", os.path.join(\"..\", expected_sm_hotstart), True)\n            else:\n                self._delete_card(\"READ_SM_HOTSTART\")\n                log.warning(\"READ_SM_HOTSTART not included as\"\n                         \" {0} does not exist ...\".format(expected_sm_hotstart))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, subdirectory=None):\n        with tmp_chdir(self.gssha_directory):\n            if self.hotstart_minimal_mode:\n                # remove all optional output cards\n                for gssha_optional_output_card in self.GSSHA_OPTIONAL_OUTPUT_CARDS:\n                    self._delete_card(gssha_optional_output_card)\n                # make sure running in SUPER_QUIET mode\n                self._update_card('SUPER_QUIET', '')\n                if subdirectory is None:\n                    # give execute folder name\n                    subdirectory = \"minimal_hotstart_run_{0}to{1}\" \\\n                                   .format(self.event_manager.simulation_start.strftime(\"%Y%m%d%H%M\"),\n                                           self.event_manager.simulation_end.strftime(\"%Y%m%d%H%M\"))\n            else:\n                # give execute folder name\n                subdirectory = \"run_{0}to{1}\".format(self.event_manager.simulation_start.strftime(\"%Y%m%d%H%M\"),\n                                                     self.event_manager.simulation_end.strftime(\"%Y%m%d%H%M\"))\n\n            # ensure unique folder naming conventions and add to exisitng event manager\n            prj_evt_manager = self.project_manager.projectFileEventManager\n            prj_event = prj_evt_manager.add_event(name=subdirectory,\n                                                  subfolder=subdirectory,\n                                                  session=self.db_session)\n            eventyml_path = self.project_manager.getCard('#GSSHAPY_EVENT_YML') \\\n                                                .value.strip(\"'\").strip('\"')\n            prj_evt_manager.write(session=self.db_session,\n                                  directory=self.gssha_directory,\n                                  name=os.path.basename(eventyml_path))\n            # ensure event manager not propagated to child event\n            self.project_manager.deleteCard('#GSSHAPY_EVENT_YML',\n                                            db_session=self.db_session)\n            self.db_session.delete(self.project_manager.projectFileEventManager)\n            self.db_session.commit()\n\n            # make working directory\n            working_directory = os.path.join(self.gssha_directory, prj_event.subfolder)\n            try:\n                os.mkdir(working_directory)\n            except OSError:\n                pass\n\n            # move simulation generated files to working directory\n            # PRECIP_FILE, HMET_NETCDF, HMET_ASCII, CHAN_POINT_INPUT\n            # TODO: Move HMET_ASCII files\n            for sim_card in self.simulation_modified_input_cards:\n                if sim_card != 'MAPPING_TABLE':\n                    self._update_card_file_location(sim_card, working_directory)\n\n            mapping_table_card = self.project_manager.getCard('MAPPING_TABLE')\n            if mapping_table_card:\n                # read in mapping table\n                map_table_object = self.project_manager.readInputFile('MAPPING_TABLE',\n                                                                      self.gssha_directory,\n                                                                      self.db_session,\n                                                                      readIndexMaps=False)\n\n                # connect index maps to main gssha directory\n                for indexMap in map_table_object.indexMaps:\n                    indexMap.filename = os.path.join(\"..\", os.path.basename(indexMap.filename))\n\n                # write copy of mapping table to working directory\n                map_table_filename = os.path.basename(mapping_table_card.value.strip(\"'\").strip('\"'))\n                map_table_object.write(session=self.db_session,\n                                       directory=working_directory,\n                                       name=map_table_filename,\n                                       writeIndexMaps=False)\n\n            # connect to other output files in main gssha directory\n            for gssha_card in self.project_manager.projectCards:\n                if gssha_card.name not in self.GSSHA_REQUIRED_OUTPUT_PATH_CARDS + \\\n                                            self.GSSHA_OPTIONAL_OUTPUT_PATH_CARDS + \\\n                                            tuple(self.simulation_modified_input_cards):\n                    if gssha_card.value:\n                        updated_value = gssha_card.value.strip('\"').strip(\"'\")\n                        if updated_value:\n                            if gssha_card.name == \"READ_CHAN_HOTSTART\":\n                                # there are two required files\n                                # the .dht and .qht\n                                if os.path.exists(updated_value + '.dht') \\\n                                        and os.path.exists(updated_value + '.qht'):\n                                    updated_path = os.path.join(\"..\", os.path.basename(updated_value))\n                                    gssha_card.value = '\"{0}\"'.format(updated_path)\n                            elif os.path.exists(updated_value):\n                                updated_path = os.path.join(\"..\", os.path.basename(updated_value))\n                                gssha_card.value = '\"{0}\"'.format(updated_path)\n                            elif gssha_card.name == '#INDEXGRID_GUID':\n                                path_split = updated_value.split()\n                                updated_path = os.path.basename(path_split[0].strip('\"').strip(\"'\"))\n                                if os.path.exists(updated_path):\n                                    new_path = os.path.join(\"..\", os.path.basename(updated_path))\n                                    try:\n                                        # Get WMS ID for Index Map as part of value\n                                        gssha_card.value = '\"{0}\" \"{1}\"'.format(new_path, path_split[1])\n                                    except:\n                                        # Like normal if the ID isn't there\n                                        gssha_card.value = '\"{0}\"'.format(new_path)\n                                else:\n                                    log.warning(\"{0} {1} not found in project directory ...\".format(\"#INDEXGRID_GUID\", updated_path))\n\n            # make sure project path is \"\"\n            self._update_card(\"PROJECT_PATH\", \"\", True)\n\n            # WRITE OUT UPDATED GSSHA PROJECT FILE\n            self.project_manager.write(session=self.db_session,\n                                       directory=working_directory,\n                                       name=self.project_manager.name)\n\n            with tmp_chdir(working_directory):\n                # RUN SIMULATION\n                if self.gssha_executable and find_executable(self.gssha_executable) is not None:\n                    log.info(\"Running GSSHA simulation ...\")\n\n                    try:\n                        run_gssha_command = [self.gssha_executable,\n                                             os.path.join(working_directory, self.project_filename)]\n                        # run GSSHA\n                        out = subprocess.check_output(run_gssha_command)\n\n                        # write out GSSHA output\n                        log_file_path = os.path.join(working_directory, 'simulation.log')\n                        with open(log_file_path, mode='w') as logfile:\n                            logfile.write(out.decode('utf-8'))\n                            # log to other logger if debug mode on\n                            if log.isEnabledFor(logging.DEBUG):\n                                for line in out.split(b'\\n'):\n                                    log.debug(line.decode('utf-8'))\n\n                    except subprocess.CalledProcessError as ex:\n                        log.error(\"{0}: {1}\".format(ex.returncode, ex.output))\n\n                else:\n                    missing_exe_error = (\"GSSHA executable not found. \"\n                                         \"Skipping GSSHA simulation run ...\")\n                    log.error(missing_exe_error)\n                    raise ValueError(missing_exe_error)\n\n            return working_directory", "response": "Runs GSSHA simulation and returns a new instance of IProjectFile."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_forecast(self):\n\n        \"\"\"\n        Updates card & runs for RAPID to GSSHA & LSM to GSSHA\n        \"\"\"\n        # ----------------------------------------------------------------------\n        # LSM to GSSHA\n        # ----------------------------------------------------------------------\n        self.prepare_hmet()\n        self.prepare_gag()\n\n        # ----------------------------------------------------------------------\n        # RAPID to GSSHA\n        # ----------------------------------------------------------------------\n        self.rapid_to_gssha()\n\n        # ----------------------------------------------------------------------\n        # HOTSTART\n        # ----------------------------------------------------------------------\n        self.hotstart()\n\n        # ----------------------------------------------------------------------\n        # Run GSSHA\n        # ----------------------------------------------------------------------\n        return self.run()", "response": "Runs the forecast for the current set of dates."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the cache key for the current page and tag type", "response": "def get_cache_key(request, page, lang, site_id, title):\n    \"\"\"\n    Create the cache key for the current page and tag type\n    \"\"\"\n    from cms.cache import _get_cache_key\n    from cms.templatetags.cms_tags import _get_page_by_untyped_arg\n\n    from cms.models import Page\n    if not isinstance(page, Page):\n        page = _get_page_by_untyped_arg(page, request, site_id)\n    if not site_id:\n        try:\n            site_id = page.node.site_id\n        except AttributeError:  # CMS_3_4\n            site_id = page.site_id\n    if not title:\n        return _get_cache_key('page_tags', page, '', site_id) + '_type:tags_list'\n    else:\n        return _get_cache_key('title_tags', page, lang, site_id) + '_type:tags_list'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_page_tags(page):\n    from .models import PageTags\n    try:\n        return page.pagetags.tags.all()\n    except PageTags.DoesNotExist:\n        return []", "response": "Returns all the tags attached to a Page instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a Page object is associated with the given tag.", "response": "def page_has_tag(page, tag):\n    \"\"\"\n    Check if a Page object is associated with the given tag.\n\n    :param page: a Page instance\n    :param tag: a Tag instance or a slug string.\n\n    :return: whether the Page instance has the given tag attached (False if no Page or no\n             attached PageTags exists)\n    :type: Boolean\n    \"\"\"\n    from .models import PageTags\n    if hasattr(tag, 'slug'):\n        slug = tag.slug\n    else:\n        slug = tag\n    try:\n        return page.pagetags.tags.filter(slug=slug).exists()\n    except PageTags.DoesNotExist:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all the tags attached to a Title object.", "response": "def get_title_tags(page, lang):\n    \"\"\"\n    Retrieves all the tags for a Title (given as page and language).\n    This function does not use fallbacks to retrieve title object.\n\n    :param page: a Page instance\n    :param lang: a language code\n\n    :return: list or queryset of attached tags\n    :type: List\n    \"\"\"\n    from .models import TitleTags\n    try:\n        return page.get_title_obj(language=lang, fallback=False).titletags.tags.all()\n    except TitleTags.DoesNotExist:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef title_has_tag(page, lang, tag):\n    from .models import TitleTags\n    if hasattr(tag, 'slug'):\n        slug = tag.slug\n    else:\n        slug = tag\n    try:\n        return page.get_title_obj(\n            language=lang, fallback=False\n        ).titletags.tags.filter(slug=slug).exists()\n    except TitleTags.DoesNotExist:\n        return False", "response": "Check if a Title object is associated with a given tag."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_page_tags_from_request(request, page_lookup, lang, site, title=False):\n    from cms.templatetags.cms_tags import _get_page_by_untyped_arg\n    from cms.utils import get_language_from_request, get_site_id\n    from django.core.cache import cache\n\n    try:\n        from cms.utils import get_cms_setting\n    except ImportError:\n        from cms.utils.conf import get_cms_setting\n\n    site_id = get_site_id(site)\n    if lang is None:\n        lang = get_language_from_request(request)\n    cache_key = get_cache_key(request, page_lookup, lang, site, title)\n    tags_list = cache.get(cache_key)\n    if not tags_list:\n        page = _get_page_by_untyped_arg(page_lookup, request, site_id)\n        if page:\n            if title:\n                tags_list = get_title_tags(page, lang)\n            else:\n                tags_list = get_page_tags(page)\n            cache.set(cache_key, tags_list, timeout=get_cms_setting('CACHE_DURATIONS')['content'])\n    if not tags_list:\n        tags_list = ()\n    return tags_list", "response": "Get the list of tags attached to a Page or a Title from a request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_title_tags_from_request(request, page_lookup, lang, site):\n    return get_page_tags_from_request(request, page_lookup, lang, site, True)", "response": "Get the list of tags attached to a Title from a request from usual\n    page_lookup parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the result of the call that the future represents.", "response": "def result(self, timeout=None):\n        \"\"\"Returns the result of the call that the future represents.\n\n        :param timeout: The number of seconds to wait for the result\n            if the future has not been completed. None, the default,\n            sets no limit.\n        :returns: The result of the call that the future represents.\n        :raises: TimeoutError: If the timeout is reached before the\n            future ends execution.\n        :raises: Exception: If the call raises the Exception.\n        \"\"\"\n        # with self.__condition:\n        if self.__state == FINISHED:\n            return self.__get__result()\n\n        self.__condition.wait(timeout)\n\n        if self.__state == FINISHED:\n            return self.__get__result()\n        else:\n            raise TimeoutError('Future: %r' % self.__method)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a exception raised by the future that the future has not completed.", "response": "def exception(self, timeout=None):\n        \"\"\"Return a exception raised by the call that the future\n        represents.\n        :param timeout: The number of seconds to wait for the exception\n            if the future has not been completed. None, the default,\n            sets no limit.\n        :returns: The exception raised by the call that the future\n            represents or None if the call completed without raising.\n        :raises: TimeoutError: If the timeout is reached before the\n            future ends execution.\n        \"\"\"\n        # with self.__condition:\n        if self.__state == FINISHED:\n            return self.__exception\n\n        self.__condition.wait(timeout)\n\n        if self.__state == FINISHED:\n            return self.__exception\n        else:\n            raise TimeoutError('Future: %r' % self.__method)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a mask from a watershed shapefile.", "response": "def generateFromWatershedShapefile(self,\n                                       shapefile_path,\n                                       cell_size,\n                                       out_raster_path=None,\n                                       load_raster_to_db=True):\n        \"\"\"\n        Generates a mask from a watershed_shapefile\n\n        Example::\n\n            from gsshapy.orm import ProjectFile, WatershedMaskFile\n            from gsshapy.lib import db_tools as dbt\n\n\n            gssha_directory = '/gsshapy/tests/grid_standard/gssha_project'\n            shapefile_path = 'watershed_boundary.shp'\n\n            # Create Test DB\n            sqlalchemy_url, sql_engine = dbt.init_sqlite_memory()\n\n            # Create DB Sessions\n            db_session = dbt.create_session(sqlalchemy_url, sql_engine)\n\n            # Instantiate GSSHAPY object for reading to database\n            project_manager = ProjectFile()\n\n            # read project file\n            project_manager.readInput(directory=gssha_directory,\n                                      projectFileName='grid_standard.prj',\n                                      session=db_session)\n\n            # generate watershed mask\n            watershed_mask = WatershedMaskFile(session=db_session,\n                                               project_file=project_manager)\n            watershed_mask.generateFromWatershedShapefile(shapefile_path,\n                                                          x_num_cells=50,\n                                                          y_num_cells=50,\n                                                          )\n            # write out updated parameters\n            project_manager.writeInput(session=db_session,\n                                       directory=gssha_directory,\n                                       name='grid_standard')\n        \"\"\"\n        if not self.projectFile:\n            raise ValueError(\"Must be connected to project file ...\")\n\n        # match elevation grid if exists\n        match_grid = None\n        try:\n            match_grid = self.projectFile.getGrid(use_mask=False)\n        except ValueError:\n            pass\n\n        # match projection if exists\n        wkt_projection = None\n        try:\n            wkt_projection = self.projectFile.getWkt()\n        except ValueError:\n            pass\n\n        if out_raster_path is None:\n            out_raster_path = '{0}.{1}'.format(self.projectFile.name, self.extension)\n\n        # make sure paths are absolute as the working directory changes\n        shapefile_path = os.path.abspath(shapefile_path)\n\n        # make sure the polygon is valid\n        check_watershed_boundary_geometry(shapefile_path)\n\n        gr = rasterize_shapefile(shapefile_path,\n                                 x_cell_size=cell_size,\n                                 y_cell_size=cell_size,\n                                 match_grid=match_grid,\n                                 raster_nodata=0,\n                                 as_gdal_grid=True,\n                                 raster_wkt_proj=wkt_projection,\n                                 convert_to_utm=True)\n\n        with tmp_chdir(self.projectFile.project_directory):\n            gr.to_grass_ascii(out_raster_path, print_nodata=False)\n            self.filename = out_raster_path\n\n            # update project file cards\n            self.projectFile.setCard('WATERSHED_MASK', out_raster_path, add_quotes=True)\n            self.projectFile.setCard('GRIDSIZE', str((gr.geotransform[1] - gr.geotransform[-1])/2.0))\n            self.projectFile.setCard('ROWS', str(gr.y_size))\n            self.projectFile.setCard('COLS', str(gr.x_size))\n\n            # write projection file if does not exist\n            if wkt_projection is None:\n                proj_file = ProjectionFile()\n                proj_file.projection = gr.wkt\n                proj_file.projectFile = self.projectFile\n                proj_path = \"{0}_prj.pro\".format(os.path.splitext(out_raster_path)[0])\n                gr.write_prj(proj_path)\n                self.projectFile.setCard('#PROJECTION_FILE', proj_path, add_quotes=True)\n            # read raster into object\n            if load_raster_to_db:\n                self._load_raster_text(out_raster_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tmp_chdir(new_path):\n    prev_cwd = os.getcwd()\n    os.chdir(new_path)\n    try:\n        yield\n    finally:\n        os.chdir(prev_cwd)", "response": "Change directory temporarily and return when done."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_era5_for_gssha(main_directory,\n                            start_datetime,\n                            end_datetime,\n                            leftlon=-180,\n                            rightlon=180,\n                            toplat=90,\n                            bottomlat=-90,\n                            precip_only=False):\n    \"\"\"\n    Function to download ERA5 data for GSSHA\n\n    .. note:: https://software.ecmwf.int/wiki/display/WEBAPI/Access+ECMWF+Public+Datasets\n\n    Args:\n        main_directory(:obj:`str`): Location of the output for the forecast data.\n        start_datetime(:obj:`str`): Datetime for download start.\n        end_datetime(:obj:`str`): Datetime for download end.\n        leftlon(Optional[:obj:`float`]): Left bound for longitude. Default is -180.\n        rightlon(Optional[:obj:`float`]): Right bound for longitude. Default is 180.\n        toplat(Optional[:obj:`float`]): Top bound for latitude. Default is 90.\n        bottomlat(Optional[:obj:`float`]): Bottom bound for latitude. Default is -90.\n        precip_only(Optional[bool]): If True, will only download precipitation.\n\n    Example::\n\n        from gsshapy.grid.era_to_gssha import download_era5_for_gssha\n\n        era5_folder = '/era5'\n        leftlon = -95\n        rightlon = -75\n        toplat = 35\n        bottomlat = 30\n        download_era5_for_gssha(era5_folder, leftlon, rightlon, toplat, bottomlat)\n\n    \"\"\"\n    # parameters: https://software.ecmwf.int/wiki/display/CKB/ERA5_test+data+documentation#ERA5_testdatadocumentation-Parameterlistings\n\n    # import here to make sure it is not required to run\n    from ecmwfapi import ECMWFDataServer\n    server = ECMWFDataServer()\n\n    try:\n        mkdir(main_directory)\n    except OSError:\n        pass\n\n    download_area = \"{toplat}/{leftlon}/{bottomlat}/{rightlon}\".format(toplat=toplat,\n                                                               leftlon=leftlon,\n                                                               bottomlat=bottomlat,\n                                                               rightlon=rightlon)\n    download_datetime = start_datetime\n    while download_datetime <= end_datetime:\n        download_file = path.join(main_directory, \"era5_gssha_{0}.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n        download_date = download_datetime.strftime(\"%Y-%m-%d\")\n        if not path.exists(download_file) and not precip_only:\n            server.retrieve({\n                'dataset': \"era5_test\",\n                #  'oper' specifies the high resolution daily data, as opposed to monthly means, wave, eda edmm, etc.\n                'stream': \"oper\",\n                #  We want instantaneous parameters, which are archived as type Analysis ('an') as opposed to forecast (fc)\n                'type': \"an\",\n                #  Surface level, as opposed to pressure level (pl) or model level (ml)\n                'levtype': \"sfc\",\n                # For parameter codes see the ECMWF parameter database at http://apps.ecmwf.int/codes/grib/param-db\n                'param': \"2t/2d/sp/10u/10v/tcc\",\n                # The spatial resolution in ERA5 is 31 km globally on a Gaussian grid.\n                # Here we us lat/long with 0.25 degrees, which is approximately the equivalent of 31km.\n                'grid': \"0.25/0.25\",\n                # ERA5 provides hourly analysis\n                'time': \"00/to/23/by/1\",\n                # area:  N/W/S/E\n                'area': download_area,\n                'date': download_date,\n                'target': download_file,\n                'format': 'netcdf',\n            })\n\n        era5_request = {\n            'dataset': \"era5_test\",\n            'stream': \"oper\",\n            'type': \"fc\",\n            'levtype': \"sfc\",\n            'param': \"tp/ssrd\",\n            'grid': \"0.25/0.25\",\n            'area': download_area,\n            'format': 'netcdf',\n        }\n        prec_download_file = path.join(main_directory, \"era5_gssha_{0}_fc.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n        loc_download_file0 = path.join(main_directory, \"era5_gssha_{0}_0_fc.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n        loc_download_file1 = path.join(main_directory, \"era5_gssha_{0}_1_fc.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n        loc_download_file2 = path.join(main_directory, \"era5_gssha_{0}_2_fc.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n        if download_datetime <= start_datetime and not path.exists(loc_download_file0):\n            loc_download_date = (download_datetime-timedelta(1)).strftime(\"%Y-%m-%d\")\n            # precipitation 0000-0600\n            era5_request['step'] = \"6/to/12/by/1\"\n            era5_request['time'] = \"18\"\n            era5_request['target'] = loc_download_file0\n            era5_request['date'] = loc_download_date\n            server.retrieve(era5_request)\n\n        if download_datetime == end_datetime and not path.exists(loc_download_file1):\n            loc_download_date = download_datetime.strftime(\"%Y-%m-%d\")\n            # precipitation 0600-1800\n            era5_request['step'] = \"1/to/12/by/1\"\n            era5_request['time'] = \"06\"\n            era5_request['target'] = loc_download_file1\n            era5_request['date'] = loc_download_date\n            server.retrieve(era5_request)\n        if download_datetime == end_datetime and not path.exists(loc_download_file2):\n            loc_download_date = download_datetime.strftime(\"%Y-%m-%d\")\n            # precipitation 1800-2300\n            era5_request['step'] = \"1/to/5/by/1\"\n            era5_request['time'] = \"18\"\n            era5_request['target'] = loc_download_file2\n            era5_request['date'] = loc_download_date\n            server.retrieve(era5_request)\n        if download_datetime < end_datetime and not path.exists(prec_download_file):\n            # precipitation 0600-0600 (next day)\n            era5_request['step'] = \"1/to/12/by/1\"\n            era5_request['time'] = \"06/18\"\n            era5_request['target'] = prec_download_file\n            era5_request['date'] = download_date\n            server.retrieve(era5_request)\n\n        download_datetime += timedelta(1)", "response": "Function to download ERA5 data for GSSHA specific time range."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction to download ERA5 data for GSSHA and return a list of the objects in the main_directory.", "response": "def download_interim_for_gssha(main_directory,\n                               start_datetime,\n                               end_datetime,\n                               leftlon=-180,\n                               rightlon=180,\n                               toplat=90,\n                               bottomlat=-90,\n                               precip_only=False):\n    \"\"\"\n    Function to download ERA5 data for GSSHA\n\n    .. note:: https://software.ecmwf.int/wiki/display/WEBAPI/Access+ECMWF+Public+Datasets\n\n    Args:\n        main_directory(:obj:`str`): Location of the output for the forecast data.\n        start_datetime(:obj:`str`): Datetime for download start.\n        end_datetime(:obj:`str`): Datetime for download end.\n        leftlon(Optional[:obj:`float`]): Left bound for longitude. Default is -180.\n        rightlon(Optional[:obj:`float`]): Right bound for longitude. Default is 180.\n        toplat(Optional[:obj:`float`]): Top bound for latitude. Default is 90.\n        bottomlat(Optional[:obj:`float`]): Bottom bound for latitude. Default is -90.\n        precip_only(Optional[bool]): If True, will only download precipitation.\n\n    Example::\n\n        from gsshapy.grid.era_to_gssha import download_era_interim_for_gssha\n\n        era_interim_folder = '/era_interim'\n        leftlon = -95\n        rightlon = -75\n        toplat = 35\n        bottomlat = 30\n        download_era_interim_for_gssha(era5_folder, leftlon, rightlon, toplat, bottomlat)\n\n    \"\"\"\n    # parameters: https://software.ecmwf.int/wiki/display/CKB/Details+of+ERA-Interim+parameters\n\n    # import here to make sure it is not required to run\n    from ecmwfapi import ECMWFDataServer\n    server = ECMWFDataServer()\n\n    try:\n        mkdir(main_directory)\n    except OSError:\n        pass\n\n    download_area = \"{toplat}/{leftlon}/{bottomlat}/{rightlon}\".format(toplat=toplat,\n                                                               leftlon=leftlon,\n                                                               bottomlat=bottomlat,\n                                                               rightlon=rightlon)\n    download_datetime = start_datetime\n    interim_request = {\n        'dataset': \"interim\",\n        #  'oper' specifies the high resolution daily data, as opposed to monthly means, wave, eda edmm, etc.\n        'stream': \"oper\",\n        #  Surface level, as opposed to pressure level (pl) or model level (ml)\n        'levtype': \"sfc\",\n        # The spatial resolution in ERA interim is 80 km globally on a Gaussian grid.\n        # Here we us lat/long with 0.75 degrees, which is approximately the equivalent of 80km.\n        'grid': \"0.5/0.5\",\n        'area': download_area,\n        'format': 'netcdf',\n    }\n    while download_datetime <= end_datetime:\n        interim_request['date'] = download_datetime.strftime(\"%Y-%m-%d\")\n        if not precip_only:\n            download_file = path.join(main_directory, \"erai_gssha_{0}_an.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n            if not path.exists(download_file):\n                #  We want instantaneous parameters, which are archived as type Analysis ('an') as opposed to forecast (fc)\n                interim_request['type'] = \"an\"\n                # For parameter codes see the ECMWF parameter database at http://apps.ecmwf.int/codes/grib/param-db\n                interim_request['param'] = \"2t/2d/sp/10u/10v/tcc\"\n                # step 0 is analysis, 3-12 is forecast\n                interim_request['step'] = \"0\"\n                # ERA Interim provides 6-hourly analysis\n                interim_request['time'] = \"00/06/12/18\"\n                interim_request['target'] = download_file\n                server.retrieve(interim_request)\n\n            download_file = path.join(main_directory, \"erai_gssha_{0}_1_fc.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n            if not path.exists(download_file):\n                interim_request['type'] = \"fc\"\n                interim_request['param'] = \"2t/2d/sp/10u/10v/tcc\"\n                interim_request['step'] = \"3\"\n                interim_request['time'] = \"00/06/12/18\"\n                interim_request['target'] = download_file\n                server.retrieve(interim_request)\n\n        download_file = path.join(main_directory, \"erai_gssha_{0}_fc.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n        if not path.exists(download_file):\n            interim_request['type'] = \"fc\"\n            interim_request['param'] = \"tp/ssrd\"\n            interim_request['step'] = \"3/6/9/12\"\n            interim_request['time'] = \"00/12\"\n            interim_request['target'] = download_file\n            server.retrieve(interim_request)\n            # TODO: READ FILE AND MODIFY VALUES SO IT IS NOT INCREMENTAL\n            # https://software.ecmwf.int/wiki/pages/viewpage.action?pageId=56658233\n            # You need  total precipitation for every 6 hours.\n            # Daily total precipitation (tp) is only available with a forecast base time 00:00 and 12:00,\n            # so to get tp for every 6 hours you will need to extract (and for the second and fourth period calculate):\n            # tp(00-06) = (time 00, step 6)\n            # tp(06-12) = (time 00, step 12) minus (time 00, step 6)\n            # tp(12-18) = (time 12, step 6)\n            # tp(18-24) = (time 12, step 12) minus (time 12, step 6)\n            # (Note the units for total precipitation is meters.)\n            tmp_download_file = download_file + '_tmp'\n            with xr.open_dataset(download_file) as xd:\n                diff_xd = xd.diff('time')\n                xd.tp[1:4] = diff_xd.tp[:3]\n                xd.tp[5:] = diff_xd.tp[4:]\n                xd.ssrd[1:4] = diff_xd.ssrd[:3]\n                xd.ssrd[5:] = diff_xd.ssrd[4:]\n                xd.to_netcdf(tmp_download_file)\n            remove(download_file)\n            rename(tmp_download_file, download_file)\n\n        download_file = path.join(main_directory, \"erai_gssha_{0}_0_fc.nc\".format(download_datetime.strftime(\"%Y%m%d\")))\n        if download_datetime <= start_datetime and not path.exists(download_file):\n            loc_download_date = (download_datetime-timedelta(1)).strftime(\"%Y-%m-%d\")\n            interim_request['type'] = \"fc\"\n            interim_request['param'] = \"tp/ssrd\"\n            interim_request['step'] = \"9/12\"\n            interim_request['time'] = \"12\"\n            interim_request['target'] = download_file\n            interim_request['date'] = loc_download_date\n            server.retrieve(interim_request)\n            # convert to incremental (see above)\n            tmp_download_file = download_file + '_tmp'\n            with xr.open_dataset(download_file) as xd:\n                inc_xd = xd.diff('time')\n                inc_xd.to_netcdf(tmp_download_file)\n            remove(download_file)\n            rename(tmp_download_file, download_file)\n        download_datetime += timedelta(1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _download(self):\n        # reproject GSSHA grid and get bounds\n        min_x, max_x, min_y, max_y = self.gssha_grid.bounds(as_geographic=True)\n        if self.era_download_data == 'era5':\n            log.info(\"Downloading ERA5 data ...\")\n            download_era5_for_gssha(self.lsm_input_folder_path,\n                                    self.download_start_datetime,\n                                    self.download_end_datetime,\n                                    leftlon=min_x-0.5,\n                                    rightlon=max_x+0.5,\n                                    toplat=max_y+0.5,\n                                    bottomlat=min_y-0.5)\n        else:\n            log.info(\"Downloading ERA Interim data ...\")\n            download_interim_for_gssha(self.lsm_input_folder_path,\n                                       self.download_start_datetime,\n                                       self.download_end_datetime,\n                                       leftlon=min_x-1,\n                                       rightlon=max_x+1,\n                                       toplat=max_y+1,\n                                       bottomlat=min_y-1)", "response": "Download ERA5 data for GSSHA domain."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting xarray dataset file handle to LSM files", "response": "def xd(self):\n        \"\"\"get xarray dataset file handle to LSM files\"\"\"\n        if self._xd is None:\n            # download files if the user requests\n            if None not in (self.download_start_datetime, self.download_end_datetime):\n                self._download()\n\n            self._xd = super(ERAtoGSSHA, self).xd\n            self._xd.lsm.lon_to_180 = True\n        return self._xd"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef later(timeout, f, *args, **kwargs):\n    '''\n    Sets a timer that will call the *f* function past *timeout* seconds.\n\n    See example in :ref:`sample_inter`\n\n    :return: :class:`Greenlet` new 'thread' which will perform the call\n        when specified.\n    '''\n    def wrap(*args, **kwargs):\n        sleep(timeout)\n        return f(*args, **kwargs)\n\n    return spawn(wrap, *args, **kwargs)", "response": "Sets a timer that will call the f function past timeout seconds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interval_host(host, time, f, *args, **kwargs):\n    '''\n    Creates an Event attached to the *host* for management that will\n    execute the *f* function every *time* seconds.\n\n    See example in :ref:`sample_inter`\n\n    :param Proxy host: proxy of the host. Can be obtained from inside a\n        class with ``self.host``.\n    :param int time: seconds for the intervals.\n    :param func f: function to be called every *time* seconds.\n    :param list args: arguments for *f*.\n    :return: :class:`Event` instance of the interval.\n    '''\n    def wrap(*args, **kwargs):\n        thread = getcurrent()\n        args = list(args)\n        stop_event = args[0]\n        del args[0]\n        args = tuple(args)\n        while not stop_event.is_set():\n            f(*args, **kwargs)\n            stop_event.wait(time)\n        host.detach_interval(thread)\n    t2_stop = Event()\n    args = list(args)\n    args.insert(0, t2_stop)\n    args = tuple(args)\n    t = spawn(wrap, *args, **kwargs)\n    thread_id = t\n    host.attach_interval(thread_id, t2_stop)\n    return t2_stop", "response": "Create an Event attached to the host for management that will execute the function f every time seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dispatch_request(self, *args, **kwargs):\n        if request.method in ('POST', 'PUT'):\n            return_url, context = self.post(*args, **kwargs)\n            if return_url is not None:\n                return redirect(return_url)\n        elif request.method in ('GET', 'HEAD'):\n            context = self.get(*args, **kwargs)\n        return self.render_response(self.context(context))", "response": "Dispatch the request.\n        Its the actual ``view`` flask will use."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_cmd_get_output(cmd):\n    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    return out or err", "response": "Runs a shell command returns console output."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remote_github_url_to_string(remote_url):\n    # TODO: make this work with https URLs\n    match = re.search('git@github\\.com:(.*)\\.git', remote_url)\n    if not match:\n        raise EnvironmentError('Remote is not a valid github URL')\n    identifier = match.group(1)\n    return re.sub('\\W', ':', identifier)", "response": "Parse out the repository identifier from a github URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_args(args):\n    parser = argparse.ArgumentParser(\n        description='A tool to extract features into a simple format.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument('--no-cache', action='store_true')\n    parser.add_argument('--deploy', action='store_true')\n    parser.add_argument('--cache-path', type=str, default='fex-cache.pckl',\n                        help='Path for cache file')\n    parser.add_argument('--path', type=str, default='features.csv',\n                        help='Path to write the dataset to')\n    args = parser.parse_args(args)\n    if args.no_cache:\n        args.cache_path = None\n    return args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(*extractor_list, **kwargs):\n    args = _get_args(kwargs.get('args'))\n    n_extractors = len(extractor_list)\n    log.info('Going to run list of {} FeatureExtractors'.format(n_extractors))\n    collection = fex.Collection(cache_path=args.cache_path)\n    for extractor in extractor_list:\n        collection.add_feature_extractor(extractor)\n\n    out_path = args.path\n    if args.deploy:\n        out_path = _prefix_git_hash(out_path)\n    collection.run(out_path)", "response": "Parse arguments provided on the commandline and execute extractors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_raster_text(self, raster_path):\n        # Open file and read plain text into text field\n        with open(raster_path, 'r') as f:\n            self.rasterText = f.read()\n\n        # Retrieve metadata from header\n        lines = self.rasterText.split('\\n')\n        for line in lines[0:6]:\n            spline = line.split()\n\n            if 'north' in spline[0].lower():\n                self.north = float(spline[1])\n            elif 'south' in spline[0].lower():\n                self.south = float(spline[1])\n            elif 'east' in spline[0].lower():\n                self.east = float(spline[1])\n            elif 'west' in spline[0].lower():\n                self.west = float(spline[1])\n            elif 'rows' in spline[0].lower():\n                self.rows = int(spline[1])\n            elif 'cols' in spline[0].lower():\n                self.columns = int(spline[1])", "response": "Loads the raster text into the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):\n        # Assign file extension attribute to file object\n        self.fileExtension = extension\n        self.filename = filename\n\n        self._load_raster_text(path)\n        \n        if spatial:\n            # Get well known binary from the raster file using the MapKit RasterLoader\n            wkbRaster = RasterLoader.grassAsciiRasterToWKB(session=session,\n                                                           grassRasterPath=path,\n                                                           srid=str(spatialReferenceID),\n                                                           noData='0')\n            self.raster = wkbRaster", "response": "Read from File Method"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_package(path=None, name='manage'):\n    if path is None:\n        manager = sys.modules[name]\n        init_package(manager.__path__, name)\n        return\n\n    if isinstance(path, str):\n        init_package([path], name)\n        return\n\n    for module_info in pkgutil.walk_packages(path, f'{name}.'):\n        if not module_info.ispkg:\n            importlib.import_module(module_info.name)", "response": "Initialize the submodules of a manage package at path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecorating a callable to replace it with a manufactured command class. Extends the interface of ``CommandDecorator``, allowing the same ``cmd`` to be used as a decorator or as a decorator factory:: @cmd(root=True) def build(): ... @build.register @cmd def deploy(): ... Further enables composition of configuration, for example via partials, as helpers.", "response": "def cmd(*args, **kwargs):\n    \"\"\"Decorate a callable to replace it with a manufactured command\n    class.\n\n    Extends the interface of ``CommandDecorator``, allowing the same\n    ``cmd`` to be used as a decorator or as a decorator factory::\n\n        @cmd(root=True)\n        def build():\n            ...\n\n        @build.register\n        @cmd\n        def deploy():\n            ...\n\n    Further enables composition of configuration, for example via\n    partials, as helpers.\n\n    \"\"\"\n    try:\n        (first, *remainder) = args\n    except ValueError:\n        pass\n    else:\n        if callable(first):\n            return CommandDecorator(*remainder, **kwargs)(first)\n\n    return CommandDecorator(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef entrypoint(cls):\n    if not isinstance(cls, type) or not issubclass(cls, Command):\n        raise TypeError(f\"inappropriate entrypoint instance of type {cls.__class__}\")\n    cls._argcmdr_entrypoint_ = True\n    return cls", "response": "Mark the decorated command as the intended entrypoint of the\n    command module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct an argparse action which stores the value of a command - line option to override a corresponding value in the process environment variable.", "response": "def store_env_override(option_strings,\n                       dest,\n                       envvar,\n                       nargs=None,\n                       default=None,\n                       type=None,\n                       choices=None,\n                       description=None,\n                       help=None,\n                       metavar=None):\n    \"\"\"Construct an argparse action which stores the value of a command\n    line option to override a corresponding value in the process\n    environment.\n\n    If the environment variable is not empty, then no override is\n    required. If the environment variable is empty, and no default is\n    provided, then the \"option\" is required.\n\n    In the case of a default value which is a *transformation* of the\n    single environment variable, this default may be provided as a\n    callable, (*e.g.* as a lambda function).\n\n    Rather than have to fully explain the relationship of this\n    environment-backed option, help text may be generated from a\n    provided description.\n\n    \"\"\"\n    if envvar == '':\n        raise ValueError(\"unsupported environment variable name\", envvar)\n\n    envvalue = os.getenv(envvar)\n\n    if callable(default):\n        default_value = default(envvalue)\n    elif envvalue:\n        default_value = envvalue\n    else:\n        default_value = default\n\n    if description and help:\n        raise ValueError(\n            \"only specify help to override its optional generation from \"\n            \"description -- not both\"\n        )\n    elif description:\n        if default_value:\n            help = '{} (default {} envvar {}: {})'.format(\n                description,\n                'provided by' if default is None else 'derived from',\n                envvar,\n                default_value,\n            )\n        else:\n            help = (f'{description} (required because '\n                    f'envvar {envvar} is empty)')\n\n    return argparse._StoreAction(\n        option_strings=option_strings,\n        dest=dest,\n        nargs=nargs,\n        const=None,\n        default=default_value,\n        type=type,\n        choices=choices,\n        required=(not default_value),\n        help=help,\n        metavar=metavar,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dict with ind_id as key and Individual as values.", "response": "def individual_dict(self, ind_ids):\n        \"\"\"Return a dict with ind_id as key and Individual as values.\"\"\"\n        ind_dict = {ind.ind_id: ind for ind in self.individuals(ind_ids=ind_ids)}\n        return ind_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the genes from all transcripts and add them to the variant.", "response": "def _get_genes(self, variant):\n        \"\"\"Add the genes for a variant\n\n        Get the hgnc symbols from all transcripts and add them\n        to the variant.\n\n        Args:\n            variant (dict): A variant dictionary\n\n        Returns:\n            genes (list): A list of Genes\n        \"\"\"\n        transcripts = variant['transcripts']\n        ensembl_ids = [transcript['ensembl_id'] for transcript in\n                       transcripts if transcript['ensembl_id']]\n        hgnc_symbols = [transcript['hgnc_symbol'] for transcript in\n                        transcripts if transcript['hgnc_symbol']]\n        genes = get_gene_info(ensembl_ids, hgnc_symbols)\n        return genes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean():\n    run('rm -rf build/')\n    run('rm -rf dist/')\n    run('rm -rf puzzle.egg-info')\n    run('find . -name __pycache__ -delete')\n    run('find . -name *.pyc -delete')\n    run('find . -name *.pyo -delete')\n    run('find . -name *~ -delete')\n\n    log.info('cleaned up')", "response": "clean - remove build artifacts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef row(self):\n        row = OrderedDict()\n        row['retro_game_id'] = self.retro_game_id\n        row['game_type'] = self.game_type\n        row['game_type_des'] = self.game_type_des\n        row['st_fl'] = self.st_fl\n        row['regseason_fl'] = self.regseason_fl\n        row['playoff_fl'] = self.playoff_fl\n        row['local_game_time'] = self.local_game_time\n        row['game_id'] = self.game_id\n        row['home_team_id'] = self.home_team_id\n        row['home_team_lg'] = self.home_team_lg\n        row['away_team_id'] = self.away_team_id\n        row['away_team_lg'] = self.away_team_lg\n        row['home_team_name'] = self.home_team_name\n        row['away_team_name'] = self.away_team_name\n        row['home_team_name_full'] = self.home_team_name_full\n        row['away_team_name_full'] = self.away_team_name_full\n        row['interleague_fl'] = self.interleague_fl\n        row['park_id'] = self.park_id\n        row['park_name'] = self.park_name\n        row['park_loc'] = self.park_loc\n        return row", "response": "Return OrderedDict of all the keys that are not None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads xml object :param url: contents url :param features: markup provider :param timestamp: game day :param game_number: game number :return: pitchpx.game.game.Game object", "response": "def read_xml(cls, url, features, timestamp, game_number):\n        \"\"\"\n        read xml object\n        :param url: contents url\n        :param features: markup provider\n        :param timestamp: game day\n        :param game_number: game number\n        :return: pitchpx.game.game.Game object\n        \"\"\"\n        soup = MlbamUtil.find_xml(\"\".join([url, cls.FILENAME]), features)\n        return cls._generate_game_object(soup, timestamp, game_number)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate game object from soup", "response": "def _generate_game_object(cls, soup, timestamp, game_number):\n        \"\"\"\n        get game data\n        :param soup: Beautifulsoup object\n        :param timestamp: game day\n        :param game_number: game number\n        :return: pitchpx.game.game.Game object\n        \"\"\"\n        game = Game(timestamp)\n\n        # Base Game Data(Spring Training, Regular Season, Play Off, etc...)\n        game.game_type = MlbamUtil.get_attribute(soup.game, 'type', unknown=MlbamConst.UNKNOWN_SHORT)\n        game.game_type_des = cls._get_game_type_des(game.game_type)\n        game.st_fl = cls._get_st_fl(game.game_type)\n        game.regseason_fl = cls._get_regseason_fl(game.game_type)\n        game.playoff_fl = cls._get_playoff_fl(game.game_type)\n        game.local_game_time = MlbamUtil.get_attribute(soup.game, 'local_game_time', unknown=MlbamConst.UNKNOWN_FULL)\n        game.game_id = MlbamUtil.get_attribute(soup.game, 'game_pk', unknown=MlbamConst.UNKNOWN_FULL)\n\n        # Team Data\n        game.home_team_id = cls._get_team_attribute(soup, cls.TEAM_TYPE_HOME, 'code')\n        game.home_team_lg = cls._get_team_attribute(soup, cls.TEAM_TYPE_HOME, 'league')\n        game.away_team_id = cls._get_team_attribute(soup, cls.TEAM_TYPE_AWAY, 'code')\n        game.away_team_lg = cls._get_team_attribute(soup, cls.TEAM_TYPE_AWAY, 'league')\n        game.home_team_name = cls._get_team_attribute(soup, cls.TEAM_TYPE_HOME, 'name')\n        game.away_team_name = cls._get_team_attribute(soup, cls.TEAM_TYPE_AWAY, 'name')\n        game.home_team_name_full = cls._get_team_attribute(soup, cls.TEAM_TYPE_HOME, 'name_full')\n        game.away_team_name_full = cls._get_team_attribute(soup, cls.TEAM_TYPE_AWAY, 'name_full')\n        game.interleague_fl = cls._get_interleague_fl(game.home_team_lg, game.away_team_lg)\n\n        # Stadium Data\n        game.park_id = cls._get_stadium_attribute(soup, 'id')\n        game.park_name = cls._get_stadium_attribute(soup, 'name')\n        game.park_loc = cls._get_stadium_attribute(soup, 'location')\n\n        # Retro ID\n        game.retro_game_id = cls._get_retro_id(game.home_team_id, timestamp, game_number)\n\n        return game"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_game_type_des(cls, game_type):\n        if game_type == 'S':\n            return 'Spring Training'\n        elif game_type == 'R':\n            return 'Regular Season'\n        elif game_type == 'F':\n            return 'Wild-card Game'\n        elif game_type == 'D':\n            return 'Divisional Series'\n        elif game_type == 'L':\n            return 'LCS'\n        elif game_type == 'W':\n            return 'World Series'\n        return MlbamConst.UNKNOWN_FULL", "response": "get game type description"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets team attribute :param soup: Beautifulsoup object :param team_type: team type(home or away) :param name: attribute name :return: attribute value", "response": "def _get_team_attribute(cls, soup, team_type, name):\n        \"\"\"\n        get team attribute\n        :param soup: Beautifulsoup object\n        :param team_type: team type(home or away)\n        :param name: attribute name\n        :return: attribute value\n        \"\"\"\n        if soup.find('team'):\n            return soup.find(\"team\", type=team_type)[name]\n        return MlbamConst.UNKNOWN_FULL"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_stadium_attribute(cls, soup, name):\n        if soup.find('stadium'):\n            return soup.stadium[name]\n        return MlbamConst.UNKNOWN_FULL", "response": "get stadium attribute\n        :param soup: Beautifulsoup object\n        :param name: attribute name\n        :return: attribute value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_interleague_fl(cls, home_team_lg, away_team_lg):\n        if (home_team_lg == MlbamConst.UNKNOWN_SHORT) or (away_team_lg == MlbamConst.UNKNOWN_SHORT):\n            return MlbamConst.UNKNOWN_SHORT\n        elif home_team_lg != away_team_lg:\n            return MlbamConst.FLG_TRUE\n        return MlbamConst.FLG_FALSE", "response": "get inter league flg"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets retro id from home team id timestamp and game number", "response": "def _get_retro_id(cls, home_team_id, timestamp, game_number):\n        \"\"\"\n        get retro id\n        :param home_team_id: home team id\n        :param timestamp: game day\n        :param game_number: game number\n        :return: retro id\n        \"\"\"\n        return '{home_team_id}{year}{month}{day}{game_number}'.format(\n            **{\n                'home_team_id': home_team_id.upper(),\n                'year': timestamp.year,\n                'month': timestamp.strftime('%m'),\n                'day': timestamp.strftime('%d'),\n                'game_number': int(game_number)-1,\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a case with individuals and save it in the database.", "response": "def add_case(self, case_obj, vtype='snv', mode='vcf', ped_svg=None):\n        \"\"\"Load a case with individuals.\n\n        Args:\n            case_obj (puzzle.models.Case): initialized case model\n        \"\"\"\n        new_case = Case(case_id=case_obj.case_id,\n                        name=case_obj.name,\n                        variant_source=case_obj.variant_source,\n                        variant_type=vtype,\n                        variant_mode=mode,\n                        pedigree=ped_svg,\n                        compressed=case_obj.compressed,\n                        tabix_index=case_obj.tabix_index)\n\n        # build individuals\n        inds = [Individual(\n            ind_id=ind.ind_id,\n            name=ind.name,\n            mother=ind.mother,\n            father=ind.father,\n            sex=ind.sex,\n            phenotype=ind.phenotype,\n            ind_index=ind.ind_index,\n            variant_source=ind.variant_source,\n            bam_path=ind.bam_path,\n        ) for ind in case_obj.individuals]\n        new_case.individuals = inds\n        \n        if self.case(new_case.case_id):\n            logger.warning(\"Case already exists in database!\")\n        else:\n            self.session.add(new_case)\n            self.save()\n        return new_case"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_case(self, case_obj):\n        for ind_obj in case_obj.individuals:\n            self.delete_individual(ind_obj)\n        logger.info(\"Deleting case {0} from database\".format(case_obj.case_id))\n        self.session.delete(case_obj)\n        self.save()\n        return case_obj", "response": "Delete a case from the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_individual(self, ind_obj):\n        logger.info(\"Deleting individual {0} from database\"\n                    .format(ind_obj.ind_id))\n        self.session.delete(ind_obj)\n        self.save()\n        return ind_obj", "response": "Delete an individual from the database"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef case(self, case_id):\n        case_obj = self.query(Case).filter_by(case_id=case_id).first()\n        return case_obj", "response": "Fetch a case from the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef individual(self, ind_id):\n        ind_obj = self.query(Individual).filter_by(ind_id=ind_id).first()\n        if ind_obj is None:\n            ind_obj = BaseIndividual(ind_id='unknown')\n        return ind_obj", "response": "Fetch a case from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch all individuals from the database.", "response": "def individuals(self, ind_ids=None):\n        \"\"\"Fetch all individuals from the database.\"\"\"\n        query = self.query(Individual)\n        if ind_ids:\n            query = query.filter(Individual.ind_id.in_(ind_ids))\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn only comments made on the case.", "response": "def case_comments(self):\n        \"\"\"Return only comments made on the case.\"\"\"\n        comments = (comment for comment in self.comments if\n                    comment.variant_id is None)\n        return comments"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post(self, url, body=None, **kwargs):\n\n        return self.request('post', url, body=body, **kwargs)", "response": "Send a POST request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a PUT request.", "response": "def put(self, url, body=None, **kwargs):\n        \"\"\"\n        Send a PUT request.\n\n        :param str url: Sub URL for the request. You MUST not specify neither base url nor api version prefix.\n        :param dict body: (optional) Dictionary of body attributes that will be wrapped with envelope and json encoded.\n        :param dict **kwargs: (optional) Other parameters which are directly passed to :func:`requests.request`.\n        :return: Tuple of three elements: (http status code, headers, response - either parsed json or plain text)\n        :rtype: tuple\n        \"\"\"\n\n        return self.request('put', url, body=body, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, url, params=None, **kwargs):\n\n        return self.request('delete', url, params=params, **kwargs)", "response": "Send a DELETE request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request(self, method, url, params=None, body=None, **kwargs):\n\n        url = \"{base_url}{version}{resource}\".format(base_url=self.config.base_url,\n                                                     version=self.API_VERSION,\n                                                     resource=url)\n        headers = {\n            'Accept': 'application/json',\n            'Authorization': \"Bearer {0}\".format(self.config.access_token),\n            'User-Agent': self.config.user_agent,\n        }\n\n        user_headers = {}\n        if  'headers' in kwargs and isinstance(kwargs['headers'], dict):\n            user_headers = kwargs['headers']\n\n        headers.update(user_headers)\n\n        raw = bool(kwargs['raw']) if 'raw' in kwargs else False\n\n        if body is not None:\n            headers['Content-Type'] = 'application/json'\n            payload = body if raw else self.wrap_envelope(body)\n            body = json.dumps(self.wrap_envelope(body), cls=DecimalEncoder)\n\n        resp = requests.request(method, url,\n                                params=params,\n                                data=body,\n                                headers=headers,\n                                timeout=float(self.config.timeout),\n                                verify=self.config.verify_ssl)\n\n        if not (200 <= resp.status_code < 300):\n            self.handle_error_response(resp)\n\n        if 'Content-Type' in resp.headers and 'json' in resp.headers['Content-Type']:\n            resp_body = munchify(resp.json()) if raw else self.unwrap_envelope(resp.json())\n        else:\n            resp_body = resp.content\n\n        return (resp.status_code, resp.headers, resp_body)", "response": "Send an HTTP request to the CRM API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef event(self, *topics, **kwargs):\n        workers = kwargs.pop(\"workers\", 1)\n        multi = kwargs.pop(\"multi\", False)\n        queue_limit = kwargs.pop(\"queue_limit\", 10000)\n\n        def wrapper(func):\n            for topic in topics:\n                queues = [Queue() for _ in range(workers)]\n                hash_ring = ketama.Continuum()\n                for q in queues:\n                    hash_ring[str(hash(q))] = q\n                self.worker_queues[topic] = hash_ring\n                self.workers[topic] = WorkerPool(\n                    queues, topic, func, multi=multi, queue_limit=queue_limit,\n                    logger_name=\"%s.%s\" % (self.name, topic))\n                self.socket.setsockopt(zmq.SUBSCRIBE, asbytes(topic))\n            return func\n        return wrapper", "response": "This is a callback function that is used to register a callback function for a topic."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        for worker_pool in self.workers.values():\n            worker_pool.start()\n\n        if isinstance(self.listen, list):\n            for i in self.listen:\n                self.socket.connect(i)\n        else:\n            self.socket.connect(self.listen)\n\n        try:\n            while True:\n                msg = self.socket.recv_string()\n                lst = msg.split()\n                if len(lst) == 2:\n                    topic, pks = lst[0], [lst[1], ]\n                elif len(lst) > 2:\n                    topic, pks = lst[0], lst[1:]\n                else:\n                    self.logger.error(\"msg corrupt -> %s\" % msg)\n                    continue\n\n                self.logger.debug(\"replicator: {0} -> {1}\".format(topic, pks))\n                for pk in pks:\n                    self.worker_queues[topic][str(hash(pk))].put(pk)\n        except Exception as e:\n            self.logger.exception(e)\n        finally:\n            for worker_pool in self.workers.values():\n                worker_pool.terminate()", "response": "Main process receive messages and distribute them to worker queues."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a function that returns the elements of the iterable that match the given function.", "response": "def map_by(fn: Callable[[T1], T2]) -> Callable[[ActualIterable[T1]], Iterable[T2]]:\n    \"\"\"\n    when pycharm supports type hinting for any implementation of currying,\n    map_with and map_on would be deprecated.\n    >>> from Redy.Collections import Traversal, Flow\n    >>> def double(x: int) -> int: return x * 2\n    >>> lst: Iterable[int] = [1, 2, 3]\n    >>> x = Flow(lst)[Traversal.map_by(double)][Traversal.sum_from(0)].unbox\n    >>> assert x is 12\n    now you can get the hinting that `x` is of type `int`\n    \"\"\"\n    return lambda collection: builtins.map(fn, collection)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reduce_by(fn: Callable[[T1, T1], T1]) -> Callable[[ActualIterable[T1]], T1]:\n    return lambda collection: functools.reduce(fn, collection)", "response": "Reduce the elements of a collection by a function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fold_by(fn: Callable[[T1, T2], T1], start: T1) -> Callable[[ActualIterable[T2]], T1]:\n    return lambda collection: functools.reduce(fn, collection, start)", "response": "Folds a function into a sequence of elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_by(fn: Callable[[T], bool]) -> Callable[[ActualIterable[T]], Iterable[T]]:\n    return lambda collection: builtins.filter(fn, collection)", "response": "Filter the elements of a iterable by a function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sum_from(zero: T1 = None) -> Callable[[ActualIterable[T1]], T1]:\n\n    def _(collection: Iterable[T1]) -> T1:\n        if zero is None:\n            collection = iter(collection)\n            _zero = next(collection)\n            return builtins.sum(collection, _zero)\n\n        return builtins.sum(collection, zero)\n\n    return _", "response": "Returns a function that sums the elements of a sequence of items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flatten_to(atom: Union[Tuple[Type[T]], Type[T]]):\n\n    def inner(nested: ActualIterable[Union[T, ActualIterable[T]]]) -> ActualIterable[T]:\n        for each in nested:\n            if isinstance(each, atom):\n                yield each\n            else:\n                yield from inner(each)\n\n    return inner", "response": "Flattens a list of items into a single element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flatten_if(cond: Callable[[Union[T, ActualIterable[T]]], bool]):\n\n    def inner(nested: ActualIterable[Union[T, ActualIterable[T]]]) -> ActualIterable[T]:\n        for each in nested:\n            if cond(each):\n                yield from inner(each)\n            else:\n                yield each\n\n    return inner", "response": "Flattens the entire list of items into one nested list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterable that returns items from a single iteration of the main sequence in order of their status.", "response": "def chunk_by(fn: Callable[[T], object]):\n    \"\"\"\n    >>> from Redy.Collections import Traversal, Flow\n    >>> lst: Iterable[int] = [0, 1, 2, 3, 4, 5, 6]\n    >>> x = Flow(lst)[Traversal.chunk_by(lambda x: x // 3)]\n    >>> assert list(x.unbox) == [[0, 1, 2], [3, 4, 5], [6]]\n    >>> x = Flow([])[Traversal.chunk_by(lambda x: x)]\n    >>> assert list(x.unbox) == []\n    \"\"\"\n\n    def inner(seq: ActualIterable[T]) -> ActualIterable[ActualIterable[T]]:\n        seq = iter(seq)\n        try:\n            head = next(seq)\n        except StopIteration:\n            return iter(seq)\n\n        current_status = fn(head)\n        group = [head]\n        for each in seq:\n            status = fn(each)\n            if status != current_status:\n                yield group\n                group = [each]\n            else:\n                group.append(each)\n            current_status = status\n        if group:\n            yield group\n\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding the items from the given iterable in order.", "response": "def chunk(seq: ActualIterable[T]) -> ActualIterable[ActualIterable[T]]:\n    \"\"\"\n    >>> from Redy.Collections import Traversal, Flow\n    >>> x = [1, 1, 2]\n    >>> assert Flow(x)[Traversal.chunk][list].unbox == [[1, 1], [2]]\n    >>> assert Flow([])[Traversal.chunk][list].unbox == []\n    \"\"\"\n    seq = iter(seq)\n    try:\n        head = next(seq)\n    except StopIteration:\n        return iter(seq)\n\n    current_status = head\n    group = [head]\n    for each in seq:\n        status = each\n        if status != current_status:\n            yield group\n            group = [each]\n        else:\n            group.append(each)\n        current_status = status\n    if group:\n        yield group"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef group_by(fn: Callable[[T], TR]):\n\n    def inner(seq: ActualIterable[T]) -> Dict[TR, List[T]]:\n        ret = defaultdict(list)\n        for each in seq:\n            ret[fn(each)].append(each)\n\n        return ret\n\n    return inner", "response": "Group items by a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group(seq: ActualIterable[T]) -> Dict[TR, List[T]]:\n    ret = defaultdict(list)\n    for each in seq:\n        ret[each].append(each)\n    return ret", "response": "Group the items in a sequence into a list of lists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_salt_bridges(self,color=\"blue\"):\n\t\tself.draw_saltbridges=\"\"\n\t\tif self.saltbridges!=None:\n\t\t\tfor bond in self.saltbridges.saltbridges_for_drawing:\n\t\t\t\tself.draw_saltbridges =\"<g class='SaltBridges' transform='translate(\"+str((self.molecule.x_dim-self.molecule.molsize1)/2)+\",\"+str((self.molecule.y_dim-self.molecule.molsize2)/2)+\")'>'\"\n\t\t\t\tatom = self.topology_data.universe.atoms[bond[0]-1] #zero-based index vs one-based index\n\t\t\t\tresidue = (atom.resname, str(atom.resid), atom.segid)\n\t\t\t\tself.draw_saltbridges=self.draw_saltbridges+\"<line x1='\"+str(int(self.molecule.nearest_points_coords[residue][0]))+\"' y1='\"+str(int(self.molecule.nearest_points_coords[residue][1]))+\"' x2='\"+str(float(self.molecule.ligand_atom_coords_from_diagr[bond[1]][0]))+\"' y2='\"+str(float(self.molecule.ligand_atom_coords_from_diagr[bond[1]][1]))+\"' style='stroke:white;stroke-width:15' />\"\n\t\t\t\tself.draw_saltbridges=self.draw_saltbridges+\"<line x1='\"+str(int(self.molecule.nearest_points_coords[residue][0]))+\"' y1='\"+str(int(self.molecule.nearest_points_coords[residue][1]))+\"' x2='\"+str(float(self.molecule.ligand_atom_coords_from_diagr[bond[1]][0]))+\"' y2='\"+str(float(self.molecule.ligand_atom_coords_from_diagr[bond[1]][1]))+\"' style='stroke:\"+color+\";stroke-width:4' />\"\n\t\t\t\tself.draw_saltbridges= self.draw_saltbridges+\"</g>\"", "response": "This method draws the salt bridges in the topology."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_lines_in_svg(self,filename, string1,string2):\n\t\tfor i,line in enumerate(fileinput.input(filename, inplace=1)):\n\t\t\tsys.stdout.write(line.replace(str(string1),str(string2)))", "response": "Used to change lines in an SVG file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_signal_handler(self, signal_name, handler_function):\n        self.bus.add_signal_receiver(signal_wrapper(handler_function),\n                                     signal_name=signal_name,\n                                     dbus_interface=self.IFACE,\n                                     bus_name=self.name,\n                                     path=self.OBJ_PATH)", "response": "register handler_function to receive signal_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters handler_function to receive signal_name.", "response": "def register_properties_handler(self, handler_function):\n        \"\"\"register `handler_function` to receive `signal_name`.\n\n        Uses dbus interface IPROPERTIES and objects path self.OBJ_PATH\n        to match 'PropertiesChanged' signal.\n\n        :param function handler_function: The function to be called.\n        \"\"\"\n\n        handler = filter_properties_signals(\n            signal_wrapper(handler_function), self.IFACE)\n\n        self.bus.add_signal_receiver(handler,\n                                     signal_name='PropertiesChanged',\n                                     dbus_interface=IPROPERTIES,\n                                     bus_name=self.name,\n                                     path=self.OBJ_PATH)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the thousand genomes frequency to the variant object.", "response": "def _add_thousand_g(self, variant_obj, info_dict):\n        \"\"\"Add the thousand genomes frequency\n        \n        Args:\n            variant_obj (puzzle.models.Variant)\n            info_dict (dict): A info dictionary\n        \n        \"\"\"\n        thousand_g = info_dict.get('1000GAF')\n        if thousand_g:\n            logger.debug(\"Updating thousand_g to: {0}\".format(\n                thousand_g))\n            variant_obj.thousand_g = float(thousand_g)\n            variant_obj.add_frequency('1000GAF', variant_obj.get('thousand_g'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the gmaf frequency to the variant_obj if it doesn t already exist", "response": "def _add_gmaf(self, variant_obj, info_dict):\n        \"\"\"Add the gmaf frequency\n        \n        Args:\n            variant_obj (puzzle.models.Variant)\n            info_dict (dict): A info dictionary\n        \n        \"\"\"\n        ##TODO search for max freq in info dict\n        for transcript in variant_obj.transcripts:\n            gmaf_raw = transcript.GMAF\n            if gmaf_raw:\n                gmaf = float(gmaf_raw.split(':')[-1])\n                variant_obj.add_frequency('GMAF', gmaf)\n                \n                if not variant_obj.thousand_g:\n                    variant_obj.thousand_g = gmaf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_exac(self, variant_obj, info_dict):\n        exac = None\n        exac_keys = ['ExAC', 'EXAC', 'ExACAF', 'EXACAF']\n        for key in exac_keys:\n            if info_dict.get(key):\n                exac = float(info_dict[key])\n        #If not found in vcf search transcripts\n        if not exac:\n            for transcript in variant_obj.transcripts:\n                exac_raw = transcript.ExAC_MAF\n                if exac_raw:\n                    exac = float(exac_raw.split(':')[-1])\n        \n        if exac:\n            variant_obj.add_frequency('ExAC', exac)", "response": "Add the gmaf frequency\n            to the variant_obj."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pk(self, obj):\n        pk_values = tuple(getattr(obj, c.name)\n                          for c in obj.__mapper__.primary_key)\n        if len(pk_values) == 1:\n            return pk_values[0]\n        return pk_values", "response": "Get pk values from object\n\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrecording the sqlalchemy object states in the middle of session", "response": "def session_update(self, session, *_):\n        \"\"\"Record the sqlalchemy object states in the middle of session,\n        prepare the events for the final pub in session_commit.\n        \"\"\"\n        self._session_init(session)\n        session.pending_write |= set(session.new)\n        session.pending_update |= set(session.dirty)\n        session.pending_delete |= set(session.deleted)\n        self.logger.debug(\"%s - session_update\" % session.meepo_unique_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef session_commit(self, session):\n        # this may happen when there's nothing to commit\n        if not hasattr(session, 'meepo_unique_id'):\n            self.logger.debug(\"skipped - session_commit\")\n            return\n\n        self._session_pub(session)\n        self._session_del(session)", "response": "Pub the events after the session committed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_basic_auth(dolt, username, password):\n    return dolt.with_headers(\n        Authorization='Basic %s' % base64.b64encode('%s:%s' % (username, password)).strip()\n    )", "response": "Add basic auth to a Tomcat application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_genotypes(self, variant_obj, gemini_variant, case_id,\n                       individual_objs):\n        \"\"\"Add the genotypes for a variant for all individuals\n\n                Args:\n                    variant_obj (puzzle.models.Variant)\n                    gemini_variant (GeminiQueryRow): The gemini variant\n                    case_id (str): related case id\n                    individual_objs (list(dict)): A list of Individuals\n\n        \"\"\"\n        for ind in individual_objs:\n            index = ind.ind_index\n            variant_obj.add_individual(Genotype(\n                sample_id=ind.ind_id,\n                genotype=gemini_variant['gts'][index],\n                case_id=case_id,\n                phenotype=ind.phenotype,\n                ref_depth=gemini_variant['gt_ref_depths'][index],\n                alt_depth=gemini_variant['gt_alt_depths'][index],\n                depth=gemini_variant['gt_depths'][index],\n                genotype_quality=gemini_variant['gt_quals'][index]\n            ))", "response": "Add the genotypes for a variant for all individuals\nCOOKIE"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimate_global_norm_range(N_t, func, num_of_sample=10, func_args={},\n                               percen_lower=1, percen_upper=99, show_progress=True):\n    \"\"\"Return estimated global range for 2D (pcolormesh-like) plot.\n\n    ## Returns:\n    - vmin\n    - vmax\n    \"\"\"\n\n    ## Check input arguments\n    for arg in [N_t, num_of_sample]:\n        assert is_integer_valued_real(arg)\n    assert num_of_sample <= N_t\n    for arg in [percen_lower, percen_upper]:\n        assert is_real_number(arg)\n        assert (0 <= arg) and (arg <= 100)\n    assert percen_lower <= percen_upper\n\n    assert callable(func)\n    assert type(func_args) is dict\n\n    assert type(show_progress) is bool\n\n    sample_indices = np.random.randint(0, N_t-1, num_of_sample)\n\n    if show_progress:\n        progress_bar = Progress_Bar(num_of_sample)\n    uppers = []\n    lowers = []\n\n    for idx, sample_index in enumerate(sample_indices):\n        frame_data = func(sample_index, **func_args)\n        lowers.append(np.percentile(frame_data, percen_lower))\n        uppers.append(np.percentile(frame_data, percen_upper))\n        if show_progress:\n            progress_bar.print(idx)\n\n    vmin = np.percentile(lowers, percen_lower)\n    vmax = np.percentile(uppers, percen_upper)\n\n    return vmin, vmax", "response": "Estimate global normal range for 2D plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks and process the frames argument into a proper iterable for an animation object.", "response": "def process_frames_argument(frames):\n    \"\"\"\n    Check and process 'frames' argument\n    into a proper iterable for an animation object\n\n    ## Arguments\n    # frames\n    : a seed for an integer-type iterable that is used as a sequence of frame indices\n    - if integer or integer-valued float (e.g. 1.0):\n        The 'frames' is interpreted as the number of total frames\n        and the sequence frame indices becomes [ 0, 1, 2, ..., 'frames' - 1 ]\n        which is equivalent to range('frames').\n    - if array-like:\n        All elements in 'frames' should be integer or integer-valued float.\n        Then, the 'frames' itself is used as a sequence of frame indices.\n    \"\"\"\n\n    result = None\n    if np.iterable(frames):\n        try: frames_arr = np.array(frames)\n        except: raise TypeError(\"'frames' should be convertable to numpy.array\")\n        for idx in range(len(frames_arr)):\n            frame_idx = frames_arr[idx]\n            assert is_real_number(frame_idx)\n            assert int(frame_idx) == frame_idx\n            frames_arr[idx] = int(frame_idx)\n        #self.frames = frames_arr\n        result = frames_arr\n    elif is_real_number(frames):\n        assert int(frames) == frames\n        frames = int(frames)\n        #self.frames = range(frames)\n        result = range(frames)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving animation into a movie file.", "response": "def save(self, *args, **kwargs):\n        \"\"\"Save animation into a movie file.\n        \n        [NOTE] If 'writer' is not specified, default writer defined in this module \n        will be used to generate the movie file.\n\n        [TODO] Implement docstring inheritance.\n        \"\"\"\n        writer = None\n        if 'writer' in kwargs.keys():\n            writer = kwargs.pop('writer')\n        else: writer = default_writer\n        super().save(*args, **kwargs, writer=writer)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string containing the content of the current image.", "response": "def show(self, temp_file_name = 'ani.mp4', **kwargs):\n        \"\"\"\n        \n        ## Arguments:\n        - 'args' and 'kwargs' will be passed to 'self.save()'\n        \"\"\"\n        ## [NOTE] Make this method as a method of base class.\n\n        ## [NOTE] This should be modified to prevent erasing other existing file with the same name.\n       \tassert type(temp_file_name) is str\n \n        self.save(temp_file_name, **kwargs)\n\n        ## [NOTE] Consider removing the temp file.\n        \n        ## [NOTE] Implement automatic showing.\n        return HTML(\"\"\"\n        <video width=\"%d\" height=\"%d\" controls>\n          <source src=\"%s\" type=\"video/mp4\">\n          </video>\n          \"\"\" % (640, 300, temp_file_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init(ctx, reset, root, phenomizer):\n    configs = {}\n    if root is None:\n        root = ctx.obj.get('root') or os.path.expanduser(\"~/.puzzle\")\n    configs['root'] = root\n    \n    if os.path.isfile(root):\n        logger.error(\"'root' can't be a file\")\n        ctx.abort()\n\n    logger.info(\"Root directory is: {}\".format(root))\n\n    db_path = os.path.join(root, 'puzzle_db.sqlite3')\n    logger.info(\"db path is: {}\".format(db_path))\n    \n    resource_dir = os.path.join(root, 'resources')\n    logger.info(\"resource dir is: {}\".format(resource_dir))\n\n    if os.path.exists(resource_dir):\n        logger.debug(\"Found puzzle directory: {0}\".format(root))\n        if os.path.exists(resource_dir) and not reset:\n            logger.warning(\"Puzzle db already in place\")\n            ctx.abort()\n    else:\n        logger.info(\"Create directory: {0}\".format(resource_dir))\n        os.makedirs(resource_dir)\n        logger.debug('Directory created')\n\n    logger.debug('Connect to database and create tables')\n    store = SqlStore(db_path)\n    store.set_up(reset=reset)\n    \n    if phenomizer:\n        phenomizer = [str(term) for term in phenomizer]\n        configs['phenomizer_auth'] = phenomizer\n    \n    if not ctx.obj.get('config_path'):\n        logger.info(\"Creating puzzle config file in {0}\".format(PUZZLE_CONFIG_PATH))\n        with codecs.open(PUZZLE_CONFIG_PATH, 'w', encoding='utf-8') as f:\n            f.write(yaml.dump(configs))\n        logger.debug(\"Config created\")", "response": "Initialize a database that stores metadata for a specific language."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode(value, encoding='utf-8', encoding_errors='strict'):\n    if isinstance(value, bytes):\n        return value\n    if not isinstance(value, basestring):\n        value = str(value)\n    if isinstance(value, unicode):\n        value = value.encode(encoding, encoding_errors)\n    return value", "response": "Encodes the given value into a bytestring representation of the given encoding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating nshares of the secret. threshold specifies the number of shares needed for reconstructing the secret value. A 0-16 bytes identifier must be provided. Optionally the secret is hashed with the algorithm specified by hash_id, a class attribute of Hash. This function must return a list of formatted shares or raises a TSSError exception if anything went wrong.", "response": "def share_secret(threshold, nshares, secret, identifier, hash_id=Hash.SHA256):\n    \"\"\"\n    Create nshares of the secret. threshold specifies the number of shares\n    needed for reconstructing the secret value. A 0-16 bytes identifier must\n    be provided. Optionally the secret is hashed with the algorithm specified\n    by hash_id, a class attribute of Hash.\n    This function must return a list of formatted shares or raises a TSSError\n    exception if anything went wrong.\n    \"\"\"\n    if identifier is None:\n        raise TSSError('an identifier must be provided')\n    if not Hash.is_valid(hash_id):\n        raise TSSError('invalid hash algorithm %s' % hash_id)\n    secret = encode(secret)\n    identifier = encode(identifier)\n    if hash_id != Hash.NONE:\n        secret += Hash.to_func(hash_id)(secret).digest()\n    shares = generate_shares(threshold, nshares, secret)\n    header = format_header(identifier, hash_id, threshold, len(secret) + 1)\n    return [format_share(header, share) for share in shares]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreconstructing the secret value of a list of well - formatted shares.", "response": "def reconstruct_secret(shares, strict_mode=True):\n    \"\"\"\n    shares must be a container with a sufficient number of well-formatted\n    shares used to reconstruct the secret value. If any share format is\n    invalid a TSSError exception is raised.\n    If strict_mode is False all combinations of shares are tried in order\n    to reconstruct the secret. Otherwise this function raises an exception\n    TSSError on the first error encountered (either a duplicate share was\n    detected or the provided hash value didn't match the one computed from\n    the recovered secret).\n    This function must return the secret value or raise TSSError.\n    \"\"\"\n    ref_header = None\n    data_shares = []\n    for share in shares:\n        share = encode(share)\n        if len(share) < 20:\n            raise TSSError('share format invalid')\n        header = parse_header(share[:20])\n        if ref_header is None:\n            ref_header = header\n            if header[2] > len(shares):\n                raise TSSError('not enough shares for reconstructing secret')\n        if ref_header != header:\n            raise TSSError('invalid share headers %s' % header)\n        data_share = share[20:]\n        if len(data_share) != header[3]:\n            raise TSSError('invalid share data size %d (expected %d)' % \\\n                               (len(data_share), header[3]))\n        data_shares.append(data_share)\n\n    for combination in itertools.combinations(data_shares, ref_header[2]):\n        secret = bytearray()\n        u = [byte_to_ord(share[0]) for share in combination]\n        if len(dict().fromkeys(u)) != len(u):\n            if strict_mode:\n                raise TSSError('invalid share with duplicate index')\n            else:\n                continue\n        for i in range(1, ref_header[3]):\n            v = [byte_to_ord(share[i]) for share in combination]\n            secret.append(lagrange_interpolation(u, v))\n        secret = bytes(secret)\n        if ref_header[1] != Hash.NONE:\n            d = Hash.to_func(ref_header[1])()\n            digestsize = digest_size(d)\n            d.update(secret[:-digestsize])\n            if len(secret) < digestsize or d.digest() != secret[-digestsize:]:\n                if strict_mode:\n                    raise TSSError('hash values mismatch')\n                else:\n                    continue\n            return secret[:-digestsize]\n        return secret\n    raise TSSError('not enough valid shares for reconstructing the secret')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the gmaf frequency to the variant object.", "response": "def _add_gmaf(self, variant_obj, gemini_variant):\n        \"\"\"Add the gmaf frequency\n        \n        Args:\n            variant_obj (puzzle.models.Variant)\n            gemini_variant (GeminiQueryRow)\n        \n        \"\"\"\n        max_af = gemini_variant['max_aaf_all']\n        if max_af:\n            max_af = float(max_af)\n            if max_af != -1.0:\n                variant_obj.set_max_freq(max_af)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the gmaf frequency to the variant_obj.", "response": "def _add_exac(self, variant_obj, gemini_variant):\n        \"\"\"Add the gmaf frequency\n        \n        Args:\n            variant_obj (puzzle.models.Variant)\n            gemini_variant (GeminiQueryRow)        \n        \"\"\"\n        exac = gemini_variant['aaf_exac_all']\n        if exac:\n            exac = float(exac)\n            variant_obj.add_frequency('ExAC', exac)\n            logger.debug(\"Updating ExAC to: {0}\".format(\n                exac))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_gene_symbols(chrom, start, stop):\n    gene_symbols = query_gene_symbol(chrom, start, stop)\n    logger.debug(\"Found gene symbols: {0}\".format(', '.join(gene_symbols)))\n    return gene_symbols", "response": "Get the gene symbols that a interval overlaps"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_gene_info(ensembl_ids=None, hgnc_symbols=None):\n    uniq_ensembl_ids = set(ensembl_id for ensembl_id in (ensembl_ids or []))\n    uniq_hgnc_symbols = set(hgnc_symbol for hgnc_symbol in (hgnc_symbols or []))\n    genes = []\n    gene_data = []\n    \n    if uniq_ensembl_ids:\n        for ensembl_id in uniq_ensembl_ids:\n            for res in query_gene(ensembl_id=ensembl_id):\n                gene_data.append(res) \n\n    elif uniq_hgnc_symbols:\n        for hgnc_symbol in uniq_hgnc_symbols:\n            query_res = query_gene(hgnc_symbol=hgnc_symbol)\n            if query_res:\n                for res in query_res:\n                    gene_data.append(res)\n            else:\n                # If no result we add just the symbol\n                gene_data.append({\n                    'hgnc_symbol': hgnc_symbol,\n                    'hgnc_id': None,\n                    'ensembl_id': None,\n                    'description': None,\n                    'chrom': 'unknown',\n                    'start': 0,\n                    'stop': 0,\n                    'hi_score': None,\n                    'constraint_score': None,\n                })\n    for gene in gene_data:\n        genes.append(Gene(\n            symbol=gene ['hgnc_symbol'],\n            hgnc_id=gene['hgnc_id'],\n            ensembl_id=gene['ensembl_id'],\n            description=gene['description'],\n            chrom=gene['chrom'],\n            start=gene['start'],\n            stop=gene['stop'],\n            location=get_cytoband_coord(gene['chrom'], gene['start']),\n            hi_score=gene['hi_score'],\n            constraint_score=gene['constraint_score'],\n            omim_number=get_omim_number(gene['hgnc_symbol'])\n            ))\n\n    return genes", "response": "Return the genes info based on the transcripts found\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the most severe consequence from the transcripts and return the most severe consequence.", "response": "def get_most_severe_consequence(transcripts):\n    \"\"\"Get the most severe consequence\n\n        Go through all transcripts and get the most severe consequence\n\n        Args:\n            transcripts (list): A list of transcripts to evaluate\n\n        Returns:\n            most_severe_consequence (str): The most severe consequence\n    \"\"\"\n    most_severe_consequence = None\n    most_severe_score = None\n\n    for transcript in transcripts:\n        for consequence in transcript['consequence'].split('&'):\n            logger.debug(\"Checking severity score for consequence: {0}\".format(\n                consequence\n            ))\n            severity_score = SEVERITY_DICT.get(\n                consequence\n            )\n            logger.debug(\"Severity score found: {0}\".format(\n                severity_score\n            ))\n            if severity_score != None:\n                if most_severe_score:\n                    if severity_score < most_severe_score:\n                        most_severe_consequence = consequence\n                        most_severe_score = severity_score\n                else:\n                    most_severe_consequence = consequence\n                    most_severe_score = severity_score\n\n    return most_severe_consequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the cytoband coordinate for a position in a sequence.", "response": "def get_cytoband_coord(chrom, pos):\n    \"\"\"Get the cytoband coordinate for a position\n\n        Args:\n            chrom(str): A chromosome\n            pos(int): The position\n\n        Returns:\n            cytoband\n    \"\"\"\n    chrom = chrom.strip('chr')\n    pos = int(pos)\n    result = None\n    logger.debug(\"Finding Cytoband for chrom:{0} pos:{1}\".format(chrom, pos))\n    if chrom in CYTOBANDS:\n        for interval in CYTOBANDS[chrom][pos]:\n            result = \"{0}{1}\".format(chrom, interval.data)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the dotfile mapping file.", "response": "def parse_mapping(self, map_path, source=None, dotfiles=None):\n        \"\"\"Do a simple parse of the dotfile mapping, using semicolons to\n        separate source file name from the target file paths.\"\"\"\n        include_re = r\"\"\"^\\s*#include\\s+(\".+\"|'.+')\"\"\"\n        include_re = re.compile(include_re, re.I)\n        mapping_re = r\"\"\"^(\"[^\"]+\"|\\'[^\\']+\\'|[^\\'\":]+)\\s*(?::\\s*(.*)\\s*)?$\"\"\"\n        mapping_re = re.compile(mapping_re)\n\n        filename = None\n        map_path = path.realpath(path.expanduser(map_path))\n\n        if path.isfile(map_path):\n            filename = map_path\n\n        elif path.isdir(map_path):\n            # try finding a mapping in the target directory\n            for map_name in '.dotfiles', 'dotfiles':\n                candidate = path.join(map_path, map_name)\n                if path.isfile(candidate):\n                    filename = candidate\n                    break\n\n        if filename is None:\n            raise ValueError('No dotfile mapping found in %s' % map_path)\n\n        if source is None:\n            source = path.dirname(map_path)\n\n        if dotfiles is None:\n            dotfiles = OrderedDict()\n\n        lineno = 0\n\n        with open(filename) as fh:\n            for line in fh:\n                lineno += 1\n                content = line.strip()\n\n                match = include_re.match(content)\n                if match:\n                    include_path = match.group(1).strip('\\'\"')\n                    if (include_path.startswith('/') or\n                            include_path.startswith('~')):\n                        include_path = path.realpath(\n                            path.expanduser(include_path))\n                    else:\n                        include_path = path.join(path.dirname(filename),\n                                                 include_path)\n\n                    if path.exists(include_path):\n                        self.log.debug('Recursively parsing mapping in %s',\n                                       include_path)\n                        dotfiles = self.parse_mapping(include_path,\n                                                      dotfiles=dotfiles)\n                    else:\n                        self.log.warning('Include command points to file or '\n                                         'directory that does not exist, \"%s\",'\n                                         ' on line %d', include_path, lineno)\n\n                if not content or content.startswith('#'):\n                    # comment line or empty line\n                    continue\n\n                match = mapping_re.match(content)\n                if match:\n                    source_path, target_path = match.groups()\n                    source_path = path.join(source, source_path.strip('\\'\"'))\n\n                    if source_path in dotfiles:\n                        self.log.warning('Duplicate dotfile source \"%s\" '\n                                         'on line #%d', lineno)\n                        continue\n\n                    if target_path is None:\n                        target_path = source_path\n\n                    dotfiles[source_path] = target_path\n\n                else:\n                    self.log.warning('Dotfile mapping regex failed on line '\n                                     '#%d', lineno)\n\n        return dotfiles"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a shell command with the given arguments.", "response": "def sh(self, *command, **kwargs):\n        \"\"\"Run a shell command with the given arguments.\"\"\"\n        self.log.debug('shell: %s', ' '.join(command))\n        return subprocess.check_call(' '.join(command),\n                                     stdout=sys.stdout,\n                                     stderr=sys.stderr,\n                                     stdin=sys.stdin,\n                                     shell=True, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun an ssh command using the configured user and server values.", "response": "def ssh(self, *command):\n        \"\"\"Run an ssh command using the configured user/server values.\"\"\"\n        if self.args.user:\n            ssh_spec = '{0}@{1}'.format(self.args.user, self.args.server)\n        else:\n            ssh_spec = self.args.server\n\n        return self.sh('ssh', ssh_spec, *command)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scp(self, local_file, remote_path=''):\n        if self.args.user:\n            upload_spec = '{0}@{1}:{2}'.format(self.args.user,\n                                               self.args.server,\n                                               remote_path)\n        else:\n            upload_spec = '{0}:{1}'.format(self.args.server, remote_path)\n\n        return self.sh('scp', local_file, upload_spec)", "response": "Copy a local file to the given remote path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting the dotfile deployment process.", "response": "def run(self):\n        \"\"\"Start the dotfile deployment process.\"\"\"\n        script = path.realpath(__file__)\n        self.log.debug('Running from %s with arguments: %s', script, self.args)\n\n        if self.args.source:\n            self.source = self.args.source\n        else:\n            # hardcoding as the parent-parent of the script for now\n            self.source = path.dirname(path.dirname(script))\n        self.log.debug('Sourcing dotfiles from %s', self.source)\n\n        try:\n            if self.args.repo:\n                self.clone_repo()\n\n            self.deploy_dotfiles(self.load_dotfiles())\n\n        except:\n            self.log.exception('Profile deploy failed')\n\n        finally:\n            if self.args.repo:\n                self.cleanup_repo()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading in the dotfile mapping as a dictionary.", "response": "def load_dotfiles(self):\n        \"\"\"Read in the dotfile mapping as a dictionary.\"\"\"\n        if self.args.map and path.exists(self.args.map):\n            dotfiles_path = self.args.map\n        else:\n            dotfiles_path = self.source\n\n        self.log.debug('Loading dotfile mapping from %s', dotfiles_path)\n\n        return self.parse_mapping(dotfiles_path, source=self.source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone_repo(self):\n        tempdir_path = tempfile.mkdtemp()\n\n        if self.args.git:\n            self.log.debug('Cloning git source repository from %s to %s',\n                           self.source, tempdir_path)\n            self.sh('git clone', self.source, tempdir_path)\n\n        else:\n            raise NotImplementedError('Unknown repo type')\n\n        self.source = tempdir_path", "response": "Clone a git repository containing the dotfiles source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cleanup_repo(self):\n        if self.source and path.isdir(self.source):\n            self.log.debug('Cleaning up source repo from %s', self.source)\n            shutil.rmtree(self.source)", "response": "Cleanup the temporary directory containing the dotfiles repo."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deploy_dotfiles(self, dotfiles):\n        if self.args.server:\n            return self.deploy_remote(dotfiles)\n        else:\n            return self.deploy_local(dotfiles)", "response": "Deploy dotfiles using the appropriate method."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeploy dotfiles to a remote server.", "response": "def deploy_remote(self, dotfiles):\n        \"\"\"Deploy dotfiles to a remote server.\"\"\"\n        tempfile_path = None\n        tempdir_path = None\n\n        try:\n            tempdir_path = tempfile.mkdtemp()\n            self.log.debug('Deploying to temp dir %s', tempdir_path)\n            self.deploy_local(dotfiles, target_root=tempdir_path)\n\n            if self.args.rsync:\n                local_spec = tempdir_path.rstrip('/') + '/'\n                remote_spec = self.args.path.rstrip('/') + '/'\n\n                if self.args.user:\n                    remote_spec = \"{0}@{1}:{2}\".format(self.args.user,\n                                                       self.args.server,\n                                                       remote_spec)\n                else:\n                    remote_spec = \"{0}:{1}\".format(self.args.server,\n                                                   remote_spec)\n\n                self.log.debug('Using rsync to sync dotfiles to %s',\n                               remote_spec)\n                self.sh('rsync', '-az', local_spec, remote_spec)\n\n            else:\n                fh, tempfile_path = tempfile.mkstemp(suffix='.tar.gz')\n                os.close(fh)\n\n                self.log.debug('Creating tar file %s', tempfile_path)\n                shutil.make_archive(tempfile_path.replace('.tar.gz', ''),\n                                    'gztar', tempdir_path)\n\n                upload_path = '_profile_upload.tgz'\n                self.log.debug('Uploading tarball to %s', upload_path)\n                self.scp(tempfile_path, upload_path)\n\n                if self.args.path:\n                    ssh_command = \"'mkdir -p {0} && \"\\\n                                  \"tar xf _profile_upload.tgz -C {0}; \"\\\n                                  \"rm -f _profile_upload.tgz'\"\\\n                                  \"\".format(self.args.path)\n                else:\n                    ssh_command = \"tar xf _profile_upload.tgz; \"\\\n                                  \"rm -f _profile_upload.tgz\"\n                self.log.debug('Using ssh to unpack tarball and clean up')\n                self.ssh(ssh_command)\n\n        finally:\n            if tempdir_path and path.isdir(tempdir_path):\n                self.log.debug('Removing temp dir %s', tempdir_path)\n                shutil.rmtree(tempdir_path)\n            if tempfile_path and path.isfile(tempfile_path):\n                self.log.debug('Removing temp file %s', tempfile_path)\n                os.unlink(tempfile_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndeploy dotfiles to a local path.", "response": "def deploy_local(self, dotfiles, target_root=None):\n        \"\"\"Deploy dotfiles to a local path.\"\"\"\n        if target_root is None:\n            target_root = self.args.path\n\n        for source_path, target_path in dotfiles.items():\n            source_path = path.join(self.source, source_path)\n            target_path = path.join(target_root, target_path)\n\n            if path.isfile(target_path) or path.islink(target_path):\n                self.log.debug('Removing existing file at %s', target_path)\n                os.unlink(target_path)\n\n            elif path.isdir(target_path):\n                self.log.debug('Removing existing dir at %s', target_path)\n                shutil.rmtree(target_path)\n\n            parent_dir = path.dirname(target_path)\n            if not path.isdir(parent_dir):\n                self.log.debug('Creating parent dir %s', parent_dir)\n                os.makedirs(parent_dir)\n\n            if self.args.copy:\n                if path.isdir(source_path):\n                    self.log.debug('Copying file %s to %s',\n                                   source_path, target_path)\n                    shutil.copytree(source_path, target_path)\n                else:\n                    self.log.debug('Copying dir %s to %s',\n                                   source_path, target_path)\n                    shutil.copy(source_path, target_path)\n\n            else:\n                self.log.debug('Symlinking %s -> %s', target_path, source_path)\n                os.symlink(source_path, target_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dedupe_list(l):\n    result = []\n\n    for el in l:\n        if el not in result:\n            result.append(el)\n\n    return result", "response": "Remove duplicates from a list preserving the order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_amino_diagrams(self):\n\n        for res in self.topology_data.dict_of_plotted_res:\n            try:\n                color = [self.colors_amino_acids[self.amino_acids[res[0]]],'white']\n            except KeyError:\n                color = [\"pink\",'white']\n            plt.figure(figsize=(2.5,2.5))\n            ring1,_=plt.pie([1],  radius=1, startangle=90, colors=color, counterclock=False)\n            plt.axis('equal')\n            plt.setp(ring1, width=1, edgecolor=color[0])\n            if len(self.topology_data.universe.protein.segments)<=1:\n                #Parameters for amino diagrams without segids\n                plt.text(0,-0.45,res[0]+\"\\n\"+res[1],ha='center',size=36, fontweight=\"bold\")\n            else:\n                #Parameters for amino diagrams with segids\n                plt.text(0,-0.37,res[0]+\"\\n\"+res[1]+\" \"+res[2],ha='center',size=30, fontweight=\"bold\")\n            #play with the dpi\n            pylab.savefig(str(res[1])+res[2]+\".svg\", dpi=300, transparent=True)", "response": "Plots the amino diagrams for the amino acid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the domain diagrams for each amino acid.", "response": "def plot_domain_diagrams(self):\n        \"\"\"\n        Plotting domain diagrams - a ring around the residue name and id and chain id. The colors are\n        determined by the chain id and are extracted from matplotlib colormap \"terrain\" (ver. \"09.2016\")\n        The plot is saved as svg file with residue id and chain id as filename for more certain\n        identification.\n        \"\"\"\n        # width of the circle around plot\n        width=0.20\n\n\n        # define color library\n        cmap = plt.get_cmap('terrain')\n        colors = [cmap(i) for i in numpy.linspace(0, 0.75, len(self.topology_data.universe.protein.segments))]\n        domain_colors = {seg:colors[i] for i,seg in enumerate(self.topology_data.universe.protein.segments.segids.tolist())}\n        for res in self.topology_data.dict_of_plotted_res:\n            color = [domain_colors[res[2]],'white']\n            #color = [self.colors_amino_acids[self.amino_acids[res.resname]],'white']\n            plt.figure(figsize=(2.5,2.5))\n            ring1,_=plt.pie([1],  radius=1-width, startangle=90, colors=color, counterclock=False)\n            plt.axis('equal')\n            plt.setp(ring1, width=width, edgecolor='white')\n            if len(self.topology_data.universe.protein.segments)<=1:\n                #Parameters for amino diagrams without segids\n                plt.text(0,-0.4,res[0]+\"\\n\"+res[1],ha='center',size=36, fontweight=\"bold\")\n            else:\n                #Parameters for amino diagrams with segids\n                plt.text(0,-0.22,res[0]+\"\\n\"+res[1]+\"\\n\"+res[2],ha='center',size=28, fontweight=\"bold\")\n            #play with the dpi\n            pylab.savefig(res[1]+res[2]+\".svg\", dpi=300, transparent=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_clock_diagrams(self, colormap=\"summer\"):\n        cmap = plt.get_cmap(colormap)\n        for res in self.topology_data.dict_of_plotted_res:\n            colors = [cmap(i) for i in numpy.linspace(0, 1, len(self.topology_data.dict_of_plotted_res[res]))]\n            traj_colors_ = {traj:colors[i] for i,traj in enumerate(self.topology_data.dict_of_plotted_res[res])}\n            plt.figure(figsize=(2.25, 2.25))\n            ring_number=[sum(1 for x in v if x) for k,v in self.topology_data.dict_of_plotted_res.items()][0]\n            self.topology_data.ring_number = ring_number\n            rings=[]\n            # When only a few rings to plot they can be thicker\n            if ring_number<2:\n                width = 0.3\n            else:\n                width = 0.2\n            for ring in range(0,ring_number):\n                ring,_=plt.pie([self.topology_data.dict_of_plotted_res[res][ring],1-self.topology_data.dict_of_plotted_res[res][ring]],  radius=0.9+width*(ring+1), startangle=90, colors=[colors[ring],\"white\"], counterclock=False)\n                rings=rings+ring\n            plt.setp(rings, width=width)\n            if len(self.topology_data.universe.protein.segments)<=1:\n            #Settings with domain\n                plt.text(-0.0,-0.62,res[0]+\"\\n\"+res[1],ha='center',size=32, fontweight='bold')\n            else:\n                plt.text(-0.0,-0.72,res[0]+\"\\n\"+res[1]+\"\\n\"+res[2],ha='center',size=25, fontweight='bold')\n            pylab.savefig(res[1]+res[2]+\".svg\", dpi=300, transparent=True)", "response": "Plots the clock diagrams of the ligand rings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_cases(variant_source, case_lines=None, case_type='ped',\n              variant_type='snv', variant_mode='vcf'):\n        \"\"\"Create a cases and populate it with individuals\n\n            Args:\n                variant_source (str): Path to vcf files\n                case_lines (Iterable): Ped like lines\n                case_type (str): Format of case lines\n\n            Returns:\n                case_objs (list(puzzle.models.Case))\n        \"\"\"\n        individuals = get_individuals(\n            variant_source=variant_source,\n            case_lines=case_lines,\n            case_type=case_type,\n            variant_mode=variant_mode\n        )\n        case_objs = []\n        case_ids = set()\n\n        compressed = False\n        tabix_index = False\n        #If no individuals we still need to have a case id\n        if variant_source.endswith('.gz'):\n            logger.debug(\"Found compressed variant source\")\n            compressed = True\n            tabix_file = '.'.join([variant_source, 'tbi'])\n            if os.path.exists(tabix_file):\n                logger.debug(\"Found index file\")\n                tabix_index = True\n\n        if len(individuals) > 0:\n            for individual in individuals:\n                case_ids.add(individual.case_id)\n        else:\n            case_ids = [os.path.basename(variant_source)]\n\n        for case_id in case_ids:\n            logger.info(\"Found case {0}\".format(case_id))\n            case = Case(\n                case_id=case_id,\n                name=case_id,\n                variant_source=variant_source,\n                variant_type=variant_type,\n                variant_mode=variant_mode,\n                compressed=compressed,\n                tabix_index=tabix_index\n                )\n\n            # Add the individuals to the correct case\n            for individual in individuals:\n                if individual.case_id == case_id:\n                    logger.info(\"Adding ind {0} to case {1}\".format(\n                        individual.name, individual.case_id\n                    ))\n                    case.add_individual(individual)\n\n            case_objs.append(case)\n\n        return case_objs", "response": "Create a list of cases and populate it with individuals"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_individuals(variant_source, case_lines=None, case_type='ped', variant_mode='vcf'):\n        individuals = []\n        ind_dict ={}\n\n        if variant_mode == 'vcf':\n            head = get_header(variant_source)\n            #Dictionary with ind_id:index where index show where in vcf ind info is\n\n            for index, ind in enumerate(head.individuals):\n                ind_dict[ind] = index\n\n            if case_lines:\n                # read individuals from ped file\n                family_parser = FamilyParser(case_lines, family_type=case_type)\n                families = family_parser.families\n                logger.debug(\"Found families {0}\".format(\n                            ','.join(list(families.keys()))))\n                if len(families) != 1:\n                    logger.error(\"Only one family can be used with vcf adapter\")\n                    raise IOError\n\n                case_id = list(families.keys())[0]\n                logger.debug(\"Family used in analysis: {0}\".format(case_id))\n\n                for ind_id in family_parser.individuals:\n                    ind = family_parser.individuals[ind_id]\n                    logger.info(\"Found individual {0}\".format(ind.individual_id))\n                    try:\n                        individual = Individual(\n                            ind_id=ind_id,\n                            case_id=case_id,\n                            mother=ind.mother,\n                            father=ind.father,\n                            sex=str(ind.sex),\n                            phenotype=str(ind.phenotype),\n                            variant_source=variant_source,\n                            ind_index=ind_dict[ind_id],\n                            )\n                        individuals.append(individual)\n                    except KeyError as err:\n                        #This is the case when individuals in ped does not exist\n                        #in vcf\n                        raise PedigreeError(\n                            family_id=case_id,\n                            individual_id=ind_id,\n                            message=\"Individual {0} exists in ped file but not in vcf\".format(ind_id)\n                            )\n\n            else:\n                case_id = os.path.basename(variant_source)\n\n                for ind in ind_dict:\n                    individual = Individual(\n                        ind_id=ind,\n                        case_id=case_id,\n                        variant_source=variant_source,\n                        ind_index=ind_dict[ind]\n                        )\n                    individuals.append(individual)\n\n                    logger.debug(\"Found individual {0} in {1}\".format(\n                                 ind, variant_source))\n        elif variant_mode == 'gemini':\n            gq = GeminiQuery(variant_source)\n            #Dictionaru with sample to index in the gemini database\n            ind_dict = gq.sample_to_idx\n            query = \"SELECT * from samples\"\n            gq.run(query)\n            for individual in gq:\n                logger.debug(\"Found individual {0} with family id {1}\".format(\n                    individual['name'], individual['family_id']))\n                individuals.append(\n                    Individual(\n                        ind_id=individual['name'],\n                        case_id=individual['family_id'],\n                        mother=individual['maternal_id'],\n                        father=individual['paternal_id'],\n                        sex=individual['sex'],\n                        phenotype=individual['phenotype'],\n                        ind_index=ind_dict.get(individual['name']),\n                        variant_source=variant_source,\n                        bam_path=None)\n                        )\n\n        return individuals", "response": "Get the individuals from a vcf file gemini database and ped file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the status of the current object.", "response": "def update_status(self, new_status=None):\n        \"\"\"Updates the zone status.\"\"\"\n        _LOGGER.debug(\"update_status: Zone %s\", self.zone_id)\n\n        if self.status and new_status is None:\n            _LOGGER.debug(\"Zone: healthy.\")\n        else:\n            old_status = self.status or {}\n\n            if new_status:\n                # merge new_status with existing for comparison\n                _LOGGER.debug(\"Set status: provided\")\n\n                # make a copy of the old_status\n                status = old_status.copy()\n\n                # merge updated items into status\n                status.update(new_status)\n\n                # promote merged_status to new_status\n                new_status = status\n            else:\n                _LOGGER.debug(\"Set status: own\")\n                new_status = self.get_status()\n\n            _LOGGER.debug(\"old_status: %s\", old_status)\n            _LOGGER.debug(\"new_status: %s\", new_status)\n            _LOGGER.debug(\"is_equal: %s\", old_status == new_status)\n\n            if new_status != old_status:\n                self.handle_message(new_status)\n                self._status_sent = False\n                self.status = new_status\n\n        if not self._status_sent:\n            self._status_sent = self.update_hass()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting status from device", "response": "def get_status(self):\n        \"\"\"Get status from device\"\"\"\n        req_url = ENDPOINTS[\"getStatus\"].format(self.ip_address, self.zone_id)\n        return request(req_url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_compounds(self, variant_obj, info_dict):\n        compound_list = []\n        compound_entry = info_dict.get('Compounds')\n        if compound_entry:\n            for family_annotation in compound_entry.split(','):\n                compounds = family_annotation.split(':')[-1].split('|')\n                for compound in compounds:\n                    splitted_compound = compound.split('>')\n\n                    compound_score = None\n                    if len(splitted_compound) > 1:\n                        compound_id = splitted_compound[0]\n                        compound_score = int(splitted_compound[-1])\n                    \n                    compound_list.append(Compound(\n                        variant_id=compound_id,\n                        combined_score=compound_score\n                        )\n                    )\n        \n        #Sort the compounds based on rank score\n        compound_list.sort(key = operator.attrgetter('combined_score'), reverse=True)\n        \n        for compound in compound_list:\n            variant_obj.add_compound(compound)", "response": "Add compounds to the variant object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the cadd score of the cadd_score attribute of the variant object.", "response": "def _add_cadd_score(self, variant_obj, info_dict):\n        \"\"\"Add the cadd score to the variant\n        \n            Args:\n                variant_obj (puzzle.models.Variant)\n                info_dict (dict): A info dictionary\n        \"\"\"\n        cadd_score = info_dict.get('CADD')\n        if cadd_score:\n            logger.debug(\"Updating cadd_score to: {0}\".format(\n                cadd_score))\n            variant_obj.cadd_score = float(cadd_score)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the genetic models to the variant object.", "response": "def _add_genetic_models(self, variant_obj, info_dict):\n        \"\"\"Add the genetic models found\n        \n        Args:\n            variant_obj (puzzle.models.Variant)\n            info_dict (dict): A info dictionary\n        \n        \"\"\"\n        genetic_models_entry = info_dict.get('GeneticModels')\n        if genetic_models_entry:\n            genetic_models = []\n            for family_annotation in genetic_models_entry.split(','):\n                for genetic_model in family_annotation.split(':')[-1].split('|'):\n                    genetic_models.append(genetic_model)\n            logger.debug(\"Updating genetic models to: {0}\".format(\n                ', '.join(genetic_models)))\n                \n            variant_obj.genetic_models = genetic_models"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_rank_score(self, variant_obj, info_dict):\n        rank_score_entry = info_dict.get('RankScore')\n        if rank_score_entry:\n            for family_annotation in rank_score_entry.split(','):\n                rank_score = family_annotation.split(':')[-1]\n            logger.debug(\"Updating rank_score to: {0}\".format(\n                rank_score))\n            variant_obj.rank_score = float(rank_score)", "response": "Adds the rank score to the variant if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loads_xml(s, object_pairs_hook=dict):\n    elem = ET.fromstring(s)\n    return object_pairs_hook(_fromXML(elem))", "response": "r Parses the contents of the string s as an XML properties document and returns a dict of the key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump_xml(props, fp, comment=None, encoding='UTF-8', sort_keys=False):\n    fp = codecs.lookup(encoding).streamwriter(fp, errors='xmlcharrefreplace')\n    print('<?xml version=\"1.0\" encoding={0} standalone=\"no\"?>'\n          .format(quoteattr(encoding)), file=fp)\n    for s in _stream_xml(props, comment, sort_keys):\n        print(s, file=fp)", "response": "Dump a series of key - value pairs to a binary filehandle fp."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dumps_xml(props, comment=None, sort_keys=False):\n    return ''.join(s + '\\n' for s in _stream_xml(props, comment, sort_keys))", "response": "Serializes a series of key - value pairs into an XML properties document."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(self, db_uri, debug=False):\n        kwargs = {'echo': debug, 'convert_unicode': True}\n        # connect to the SQL database\n        if 'mysql' in db_uri:\n            kwargs['pool_recycle'] = 3600\n        elif '://' not in db_uri:\n            logger.debug(\"detected sqlite path URI: {}\".format(db_uri))\n            db_path = os.path.abspath(os.path.expanduser(db_uri))\n            db_uri = \"sqlite:///{}\".format(db_path)\n\n        self.engine = create_engine(db_uri, **kwargs)\n        logger.debug('connection established successfully')\n        # make sure the same engine is propagated to the BASE classes\n        BASE.metadata.bind = self.engine\n        # start a session\n        self.session = scoped_session(sessionmaker(bind=self.engine))\n        # shortcut to query method\n        self.query = self.session.query\n        return self", "response": "Configure connection to a SQL database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_up(self, reset=False):\n        if reset:\n            self.tear_down()\n\n        logger.info(\"Creating database\")\n        # create the tables\n        BASE.metadata.create_all(self.engine)\n        return self", "response": "Initialize a new database with the default tables and columns."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tear_down(self):\n        # drop/delete the tables\n        logger.info('resetting database...')\n        BASE.metadata.drop_all(self.engine)\n        return self", "response": "Tear down a database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select_plugin(self, case_obj):\n        if case_obj.variant_mode == 'vcf':\n            logger.debug(\"Using vcf plugin\")\n            plugin = VcfPlugin(case_obj.variant_type)\n        elif case_obj.variant_mode == 'gemini':\n            logger.debug(\"Using gemini plugin\")\n            plugin = GeminiPlugin(case_obj.variant_type)\n        \n        #Add case to plugin\n        plugin.add_case(case_obj)\n\n        self.variant_type = case_obj.variant_type\n\n        case_id = case_obj.case_id\n        return plugin, case_id", "response": "Select and initialize the correct plugin for the case."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing the landing page.", "response": "def index():\n    \"\"\"Show the landing page.\"\"\"\n    gene_lists = app.db.gene_lists() if app.config['STORE_ENABLED'] else []\n    queries = app.db.gemini_queries() if app.config['STORE_ENABLED'] else []\n\n    case_groups = {}\n    for case in app.db.cases():\n        key = (case.variant_source, case.variant_type, case.variant_mode)\n        if key not in case_groups:\n            case_groups[key] = []\n        case_groups[key].append(case)\n\n    return render_template('index.html', case_groups=case_groups,\n                           gene_lists=gene_lists, queries=queries)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow the overview for a case.", "response": "def case(case_id):\n    \"\"\"Show the overview for a case.\"\"\"\n    case_obj = app.db.case(case_id)\n    return render_template('case.html', case=case_obj, case_id=case_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef phenotypes():\n    ind_id = request.form['ind_id']\n    phenotype_id = request.form['phenotype_id']\n\n    if not phenotype_id:\n        return abort(500, 'no phenotype_id submitted')\n\n    ind_obj = app.db.individual(ind_id)\n    try:\n        added_terms = app.db.add_phenotype(ind_obj, phenotype_id)\n        if added_terms is None:\n            flash(\"Term with id {} was not found\".format(phenotype_id),\n                  'danger')\n        elif added_terms == []:\n            flash(\"Term with id {} was already added\".format(phenotype_id),\n                  'warning')\n    except RuntimeError as error:\n        return abort(500, error.message)\n\n    return redirect(request.referrer)", "response": "Add phenotype to the case model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_phenotype(phenotype_id):\n    ind_id = request.form['ind_id']\n    ind_obj = app.db.individual(ind_id)\n    try:\n        app.db.remove_phenotype(ind_obj, phenotype_id)\n    except RuntimeError as error:\n        return abort(500, error.message)\n    return redirect(request.referrer)", "response": "Delete a phenotype from an individual."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays or add a gene list.", "response": "def gene_list(list_id=None):\n    \"\"\"Display or add a gene list.\"\"\"\n    all_case_ids = [case.case_id for case in app.db.cases()]\n    if list_id:\n        genelist_obj = app.db.gene_list(list_id)\n        case_ids = [case.case_id for case in app.db.cases()\n                    if case not in genelist_obj.cases]\n        if genelist_obj is None:\n            return abort(404, \"gene list not found: {}\".format(list_id))\n\n    if 'download' in request.args:\n        response = make_response('\\n'.join(genelist_obj.gene_ids))\n        filename = secure_filename(\"{}.txt\".format(genelist_obj.list_id))\n        header = \"attachment; filename={}\".format(filename)\n        response.headers['Content-Disposition'] = header\n        return response\n\n    if request.method == 'POST':\n        if list_id:\n            # link a case to the gene list\n            case_ids = request.form.getlist('case_id')\n            for case_id in case_ids:\n                case_obj = app.db.case(case_id)\n                if case_obj not in genelist_obj.cases:\n                    genelist_obj.cases.append(case_obj)\n                    app.db.save()\n        else:\n            # upload a new gene list\n            req_file = request.files['file']\n            new_listid = (request.form['list_id'] or\n                          secure_filename(req_file.filename))\n\n            if app.db.gene_list(new_listid):\n                return abort(500, 'Please provide a unique list name')\n\n            if not req_file:\n                return abort(500, 'Please provide a file for upload')\n\n            gene_ids = [line for line in req_file.stream\n                        if not line.startswith('#')]\n            genelist_obj = app.db.add_genelist(new_listid, gene_ids)\n            case_ids = all_case_ids\n\n    return render_template('gene_list.html', gene_list=genelist_obj,\n                           case_ids=case_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_genelist(list_id, case_id=None):\n    if case_id:\n        # unlink a case from a gene list\n        case_obj = app.db.case(case_id)\n        app.db.remove_genelist(list_id, case_obj=case_obj)\n        return redirect(request.referrer)\n    else:\n        # remove the whole gene list\n        app.db.remove_genelist(list_id)\n        return redirect(url_for('.index'))", "response": "Delete a whole gene list with links to cases or a link."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads a new resource for an individual.", "response": "def resources():\n    \"\"\"Upload a new resource for an individual.\"\"\"\n    ind_id = request.form['ind_id']\n\n    upload_dir = os.path.abspath(app.config['UPLOAD_DIR'])\n    req_file = request.files['file']\n    filename = secure_filename(req_file.filename)\n    file_path = os.path.join(upload_dir, filename)\n    name = request.form['name'] or filename\n    req_file.save(file_path)\n\n    ind_obj = app.db.individual(ind_id)\n    app.db.add_resource(name, file_path, ind_obj)\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading a new comment.", "response": "def comments(case_id):\n    \"\"\"Upload a new comment.\"\"\"\n    text = request.form['text']\n    variant_id = request.form.get('variant_id')\n    username = request.form.get('username')\n    case_obj = app.db.case(case_id)\n    app.db.add_comment(case_obj, text, variant_id=variant_id, username=username)\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef individual(ind_id):\n    individual_obj = app.db.individual(ind_id)\n    return render_template('individual.html', individual=individual_obj)", "response": "Show details for a specific individual."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef synopsis(case_id):\n    text = request.form['text']\n    case_obj = app.db.case(case_id)\n    app.db.update_synopsis(case_obj, text)\n    return redirect(request.referrer)", "response": "Update the case synopsis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_case():\n    ind_ids = request.form.getlist('ind_id')\n    case_id = request.form['case_id']\n    source = request.form['source']\n    variant_type = request.form['type']\n\n    if len(ind_ids) == 0:\n        return abort(400, \"must add at least one member of case\")\n\n    # only GEMINI supported\n    new_case = Case(case_id=case_id, name=case_id, variant_source=source,\n                    variant_type=variant_type, variant_mode='gemini')\n\n    # Add individuals to the correct case\n    for ind_id in ind_ids:\n        ind_obj = app.db.individual(ind_id)\n        new_case.individuals.append(ind_obj)\n\n    app.db.session.add(new_case)\n    app.db.save()\n\n    return redirect(url_for('.case', case_id=new_case.name))", "response": "Make a new case out of a list of individuals."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef binary_to_term(data):\n    if not isinstance(data, bytes):\n        raise ParseException('not bytes input')\n    size = len(data)\n    if size <= 1:\n        raise ParseException('null input')\n    if b_ord(data[0]) != _TAG_VERSION:\n        raise ParseException('invalid version')\n    try:\n        i, term = _binary_to_term(1, data)\n        if i != size:\n            raise ParseException('unparsed data')\n        return term\n    except struct.error:\n        raise ParseException('missing data')\n    except IndexError:\n        raise ParseException('missing data')", "response": "Decode binary data into a sequence of Erlang terms."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef term_to_binary(term, compressed=False):\n    data_uncompressed = _term_to_binary(term)\n    if compressed is False:\n        return b_chr(_TAG_VERSION) + data_uncompressed\n    else:\n        if compressed is True:\n            compressed = 6\n        if compressed < 0 or compressed > 9:\n            raise InputException('compressed in [0..9]')\n        data_compressed = zlib.compress(data_uncompressed, compressed)\n        size_uncompressed = len(data_uncompressed)\n        if size_uncompressed > 4294967295:\n            raise OutputException('uint32 overflow')\n        return (\n            b_chr(_TAG_VERSION) + b_chr(_TAG_COMPRESSED_ZLIB) +\n            struct.pack(b'>I', size_uncompressed) + data_compressed\n        )", "response": "Encode Python types into Erlang terms in binary data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef consult(string_in):\n    # pylint: disable=eval-used\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n\n    # manually parse textual erlang data to avoid external dependencies\n    list_out = []\n    tuple_binary = False   # binaries become tuples of integers\n    quoted_string = False  # strings become python string\n    atom_string = False    # atoms become python string\n    number = False\n    whitespace = frozenset(('\\n', '\\t', ' '))\n    i = 0\n    while i < len(string_in):\n        character = string_in[i]\n        if character == ',':\n            if atom_string:\n                list_out.append('\"')\n                atom_string = False\n            list_out.append(',')\n            number = string_in[i + 1].isdigit()\n        elif character == '{':\n            list_out.append('(')\n            number = string_in[i + 1].isdigit()\n        elif character == '}':\n            if atom_string:\n                list_out.append('\"')\n                atom_string = False\n            list_out.append(')')\n            number = False\n        elif character == '[':\n            list_out.append('[')\n            number = string_in[i + 1].isdigit()\n        elif character == ']':\n            if atom_string:\n                list_out.append('\"')\n                atom_string = False\n            list_out.append(']')\n            number = False\n        elif character == '<' and string_in[i + 1] == '<':\n            list_out.append('(')\n            tuple_binary = True\n            i += 1\n        elif character == '>' and string_in[i + 1] == '>':\n            list_out.append(')')\n            tuple_binary = False\n            i += 1\n        elif not quoted_string and not atom_string and character in whitespace:\n            number = string_in[i + 1].isdigit()\n        elif tuple_binary or number:\n            list_out.append(character)\n        elif character == '\"':\n            if quoted_string:\n                quoted_string = False\n            else:\n                quoted_string = True\n            list_out.append('\"')\n        elif character == \"'\":\n            if atom_string:\n                atom_string = False\n            else:\n                atom_string = True\n            list_out.append('\"')\n        elif not quoted_string and not atom_string:\n            atom_string = True\n            list_out.append('\"')\n            list_out.append(character)\n        else:\n            list_out.append(character)\n        i += 1\n    return eval(''.join(list_out))", "response": "Consults the contents of a string into a list of the elements of the order they appear in."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns encoded representation of a tag as a byte string", "response": "def binary(self):\n        \"\"\"\n        return encoded representation\n        \"\"\"\n        if isinstance(self.value, int):\n            return b_chr(_TAG_ATOM_CACHE_REF) + b_chr(self.value)\n        elif isinstance(self.value, TypeUnicode):\n            value_encoded = self.value.encode('utf-8')\n            length = len(value_encoded)\n            if length <= 255:\n                return (\n                    b_chr(_TAG_SMALL_ATOM_UTF8_EXT) +\n                    b_chr(length) + value_encoded\n                )\n            elif length <= 65535:\n                return (\n                    b_chr(_TAG_ATOM_UTF8_EXT) +\n                    struct.pack(b'>H', length) + value_encoded\n                )\n            else:\n                raise OutputException('uint16 overflow')\n        elif isinstance(self.value, bytes):\n            length = len(self.value)\n            if length <= 255:\n                return b_chr(_TAG_SMALL_ATOM_EXT) + b_chr(length) + self.value\n            elif length <= 65535:\n                return (\n                    b_chr(_TAG_ATOM_EXT) +\n                    struct.pack(b'>H', length) + self.value\n                )\n            else:\n                raise OutputException('uint16 overflow')\n        else:\n            raise OutputException('unknown atom type')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning encoded representation of the tag as a byte string", "response": "def binary(self):\n        \"\"\"\n        return encoded representation\n        \"\"\"\n        if isinstance(self.value, bytes):\n            length = len(self.value)\n            if length > 4294967295:\n                raise OutputException('uint32 overflow')\n            elif self.bits != 8:\n                return (\n                    b_chr(_TAG_BIT_BINARY_EXT) +\n                    struct.pack(b'>I', length) +\n                    b_chr(self.bits) + self.value\n                )\n            else:\n                return (\n                    b_chr(_TAG_BINARY_EXT) +\n                    struct.pack(b'>I', length) +\n                    self.value\n                )\n        else:\n            raise OutputException('unknown binary type')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef binary(self):\n        if isinstance(self.value, list):\n            length = len(self.value)\n            if length == 0:\n                return b_chr(_TAG_NIL_EXT)\n            elif length > 4294967295:\n                raise OutputException('uint32 overflow')\n            elif self.improper:\n                return (\n                    b_chr(_TAG_LIST_EXT) +\n                    struct.pack(b'>I', length - 1) +\n                    b''.join([_term_to_binary(element)\n                              for element in self.value])\n                )\n            else:\n                return (\n                    b_chr(_TAG_LIST_EXT) +\n                    struct.pack(b'>I', length) +\n                    b''.join([_term_to_binary(element)\n                              for element in self.value]) +\n                    b_chr(_TAG_NIL_EXT)\n                )\n        else:\n            raise OutputException('unknown list type')", "response": "returns encoded representation of the list of class entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef binary(self):\n        creation_size = len(self.creation)\n        if creation_size == 1:\n            return (\n                b_chr(_TAG_PID_EXT) +\n                self.node.binary() + self.id + self.serial + self.creation\n            )\n        elif creation_size == 4:\n            return (\n                b_chr(_TAG_NEW_PID_EXT) +\n                self.node.binary() + self.id + self.serial + self.creation\n            )\n        else:\n            raise OutputException('unknown pid type')", "response": "returns the binary representation of the object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning encoded representation of the port", "response": "def binary(self):\n        \"\"\"\n        return encoded representation\n        \"\"\"\n        creation_size = len(self.creation)\n        if creation_size == 1:\n            return (\n                b_chr(_TAG_PORT_EXT) +\n                self.node.binary() + self.id + self.creation\n            )\n        elif creation_size == 4:\n            return (\n                b_chr(_TAG_NEW_PORT_EXT) +\n                self.node.binary() + self.id + self.creation\n            )\n        else:\n            raise OutputException('unknown port type')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning encoded representation of the object", "response": "def binary(self):\n        \"\"\"\n        return encoded representation\n        \"\"\"\n        length = len(self.id) / 4\n        if length == 0:\n            return (\n                b_chr(_TAG_REFERENCE_EXT) +\n                self.node.binary() + self.id + self.creation\n            )\n        elif length <= 65535:\n            creation_size = len(self.creation)\n            if creation_size == 1:\n                return (\n                    b_chr(_TAG_NEW_REFERENCE_EXT) +\n                    struct.pack(b'>H', length) +\n                    self.node.binary() + self.creation + self.id\n                )\n            elif creation_size == 4:\n                return (\n                    b_chr(_TAG_NEWER_REFERENCE_EXT) +\n                    struct.pack(b'>H', length) +\n                    self.node.binary() + self.creation + self.id\n                )\n            else:\n                raise OutputException('unknown reference type')\n        else:\n            raise OutputException('uint16 overflow')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_resource(self, name, file_path, ind_obj):\n        new_resource = Resource(name=name, individual=ind_obj, path=file_path)\n        self.session.add(new_resource)\n        self.save()\n        return new_resource", "response": "Link a resource to an individual."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlinks a resource to an individual.", "response": "def delete_resource(self, resource_id):\n        \"\"\"Link a resource to an individual.\"\"\"\n        resource_obj = self.resource(resource_id)\n        logger.debug(\"Deleting resource {0}\".format(resource_obj.name))\n        self.session.delete(resource_obj)\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a file or string and returns a list of AudioClipSpec objects.", "response": "def parse(cls, specsFileOrString):\n        \"\"\"Parsers a file or string and returns a list of AudioClipSpec\n        \n        Arguments:\n            specsFileOrString (str): specifications' file or string\n        \n        Examples:\n            >>> SpecsParser.parse('23.4 34.1\\n40.2 79.65 Hello World!')\n            [<AudioClipSpec start:23.40, end:34.10, text:''>, \n            <AudioClipSpec start:40.20, end:79.65, text:'Hello World!'>]\n\n        Returns: list(AudioClipSpec) or None\n        \"\"\"\n        stringToParse = None\n\n        # Read the contents of the file if specsFileOrString is not a string\n        if os.path.isfile(specsFileOrString):\n            with open(specsFileOrString, 'r') as f:\n                stringToParse = f.read()\n        else:\n            stringToParse = specsFileOrString\n\n        # Audacity uses \\r for newlines\n        lines = [x.strip() for x in re.split(r'[\\r\\n]+', stringToParse)]\n\n        clips = []\n        for line in lines:\n            if line != '':\n                clips.append(cls._parseLine(line))\n\n            # if spec != None:\n            #     clips.append(spec)\n\n        return clips"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parseLine(cls, line):\n        r = cls._PROG.match(line)\n\n        if not r:\n            raise ValueError(\"Error: parsing '%s'. Correct: \\\"<number> <number> [<text>]\\\"\" % line)\n\n        d = r.groupdict()\n\n        if len(d['begin']) == 0 or len(d['end']) == 0:\n            raise ValueError(\"Error: parsing '%s'. Correct: \\\"<number> <number> [<text>]\\\"\" % line)\n        \n        return AudioClipSpec(d['begin'], d['end'], d['text'].strip())", "response": "Parses a single line of text and returns an AudioClipSpec"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mode_name(self):\n\n        for name, id in self.MODES.iteritems():\n            if id == self.mode:\n                return name", "response": "Returns the tunnel mode s name for printing purpose."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the tunnel. If the tunnel is already opened, the function will raised an AlreadyOpened exception.", "response": "def open(self):\n        \"\"\" Create the tunnel.\n            If the tunnel is already opened, the function will\n            raised an AlreadyOpened exception.\n        \"\"\"\n\n        if self.fd is not None:\n            raise self.AlreadyOpened()\n\n        logger.debug(\"Opening %s...\" % (TUN_KO_PATH, ))\n        self.fd = os.open(TUN_KO_PATH, os.O_RDWR)\n        \n        logger.debug(\"Opening %s tunnel '%s'...\" % (self.mode_name.upper(), self.pattern, ))\n        try:\n            ret = fcntl.ioctl(self.fd, self.TUNSETIFF, struct.pack(\"16sH\", self.pattern, self.mode | self.no_pi))\n\n        except IOError, e:\n            if e.errno == 1:\n                logger.error(\"Cannot open a %s tunnel because the operation is not permitted.\" % (self.mode_name.upper(), ))\n                raise self.NotPermitted()\n\n            raise\n\n        self.name = ret[:16].strip(\"\\x00\")\n\n        logger.info(\"Tunnel '%s' opened.\" % (self.name, ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose the tunnel. If the tunnel is already closed or never opened, do nothing.", "response": "def close(self):\n        \"\"\" Close the tunnel.\n            If the tunnel is already closed or never opened,\n            do nothing.\n        \"\"\"\n\n        if self.fd is None:\n            return\n            \n        logger.debug(\"Closing tunnel '%s'...\" % (self.name or \"\", ))\n\n        # Close tun.ko file\n        os.close(self.fd)\n        self.fd = None\n\n        logger.info(\"Tunnel '%s' closed.\" % (self.name or \"\", ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreceive a buffer. The default size is 1500, the classical MTU.", "response": "def recv(self, size = None):\n        \"\"\" Receive a buffer. The default size is 1500, the\n            classical MTU.\n        \"\"\"\n\n        size = size if size is not None else 1500\n\n        return os.read(self.fd, size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_mac(self, mac):\n        mac = map(ord, mac)\n        ifreq = struct.pack('16sH6B8', self.name, socket.AF_UNIX, *mac)\n        fcntl.ioctl(self.fileno(), self.SIOCSIFHWADDR, ifreq)", "response": "Sets the MAC address of the device to mac."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_ipv4(self, ip):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        bin_ip = socket.inet_aton(ip)\n        ifreq = struct.pack('16sH2s4s8s', self.name, socket.AF_INET, '\\x00'*2, bin_ip, '\\x00'*8)\n        fcntl.ioctl(sock, self.SIOCSIFADDR, ifreq)\n        ifreq = struct.pack('16sH', self.name, self.IFF_UP|self.IFF_POINTOPOINT|self.IFF_RUNNING|self.IFF_MULTICAST)\n        fcntl.ioctl(sock, self.SIOCSIFFLAGS, ifreq)", "response": "Sets the IP address of the device\n            parameter ip should be string representation of IP address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download(self):\n        p = Pool()\n        p.map(self._download, self.days)", "response": "Download all the entries in the MLBAM dataset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading MLBAM Game Day and return a list of dictionaries.", "response": "def _download(self, timestamp):\n        \"\"\"\n        download MLBAM Game Day\n        :param timestamp: day\n        \"\"\"\n        games, atbats, pitches = [], [], []\n        rosters, coaches, umpires = [], [], []\n        boxscores, actions = [], []\n        timestamp_params = {\n            'year': str(timestamp.year),\n            'month': str(timestamp.month).zfill(2),\n            'day': str(timestamp.day).zfill(2)\n        }\n\n        logging.info('->- Game data download start({year}/{month}/{day})'.format(**timestamp_params))\n\n        base_url = self.DELIMITER.join([self.url, self.PAGE_URL_GAME_DAY.format(**timestamp_params)])\n        html = MlbamUtil.find_xml(base_url, self.parser)\n\n        href = self.PAGE_URL_GAME_PREFIX.format(**timestamp_params)\n        for gid in html.find_all('a', href=re.compile(href)):\n            gid_path = gid.get_text().strip()\n            gid_url = self.DELIMITER.join([base_url, gid_path])\n            # Read XML & create dataset\n            try:\n                game = Game.read_xml(gid_url, self.parser, timestamp, MlbAm._get_game_number(gid_path))\n                players = Players.read_xml(gid_url, self.parser, game)\n                innings = Inning.read_xml(gid_url, self.parser, game, players)\n                boxscore = BoxScore.read_xml(gid_url, self.parser, game, players)\n            except MlbAmHttpNotFound as e:\n                logging.warning(e.msg)\n                continue\n\n            # append a dataset\n            games.append(game.row())\n            rosters.extend([roseter.row() for roseter in players.rosters.values()])\n            coaches.extend([coach.row() for coach in players.coaches.values()])\n            umpires.extend([umpire.row() for umpire in players.umpires.values()])\n            atbats.extend(innings.atbats)\n            pitches.extend(innings.pitches)\n            actions.extend(innings.actions)\n            boxscores.append(boxscore.row())\n\n        # writing csv\n        day = \"\".join([timestamp_params['year'], timestamp_params['month'], timestamp_params['day']])\n        for params in (\n                {'datasets': games, 'filename': Game.DOWNLOAD_FILE_NAME},\n                {'datasets': rosters, 'filename': Players.Player.DOWNLOAD_FILE_NAME},\n                {'datasets': coaches, 'filename': Players.Coach.DOWNLOAD_FILE_NAME},\n                {'datasets': umpires, 'filename': Players.Umpire.DOWNLOAD_FILE_NAME},\n                {'datasets': atbats, 'filename': AtBat.DOWNLOAD_FILE_NAME},\n                {'datasets': pitches, 'filename': Pitch.DOWNLOAD_FILE_NAME},\n                {'datasets': boxscores, 'filename': BoxScore.DOWNLOAD_FILE_NAME},\n                {'datasets': actions, 'filename': InningAction.DOWNLOAD_FILE_NAME},\n        ):\n            self._write_csv(params['datasets'], params['filename'].format(day=day, extension=self.extension))\n        time.sleep(2)\n\n        logging.info('-<- Game data download end({year}/{month}/{day})'.format(**timestamp_params))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_game_number(cls, gid_path):\n        game_number = str(gid_path[len(gid_path)-2:len(gid_path)-1])\n        if game_number.isdigit():\n            return int(game_number)\n        else:\n            for char in reversed(gid_path):\n                if char.isdigit():\n                    return int(char)\n        raise MlbAmException('Illegal Game Number:(gid:{gid_path})'.format(gid_path))", "response": "Get game number from game logs directory path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write_csv(self, datasets, filename):\n        with open('/'.join([self.output, filename]), mode='w', encoding=self.encoding) as write_file:\n            writer = csv.writer(write_file, delimiter=',')\n            for i, row in enumerate(datasets):\n                if i == 0:\n                    # header\n                    writer.writerow(list(row.keys()))\n                writer.writerow(list(row.values()))", "response": "Write CSV\n        :param datasets: Datasets\n        :param filename: File Name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate datetime value :param value: datetime value :return: None or validators.Invalid or MlbAmException", "response": "def _validate_datetime(cls, value):\n        \"\"\"\n        validate datetime value\n        :param value: datetime value\n        :return: None or validators.Invalid or MlbAmException\n        \"\"\"\n        datetime_check = validators.Int()\n        datetime_check.to_python(value)\n        if len(value) != 8:\n            raise MlbAmBadParameter(\"Length Error:{value}({length})\".format(value=value, length=len(value)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_datetime_from_to(cls, start, end):\n        if not start <= end:\n            raise MlbAmBadParameter(\"not Start Day({start}) <= End Day({end})\".format(start=start, end=end))", "response": "Validate datetime from start to end."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _days(cls, start, end):\n        days = []\n        # datetime\n        start_day, end_day = dt.strptime(start, cls.DATE_FORMAT), dt.strptime(end, cls.DATE_FORMAT)\n        delta = end_day - start_day\n\n        for day in range(delta.days+1):\n            days.append(start_day + timedelta(days=day))\n        return days", "response": "Scrape a MLBAM Data\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscrape a MLBAM data file from start to end.", "response": "def scrape(cls, start, end, output):\n        \"\"\"\n        Scrape a MLBAM Data\n        :param start: Start Day(YYYYMMDD)\n        :param end: End Day(YYYYMMDD)\n        :param output: Output directory\n        \"\"\"\n        # Logger setting\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"time:%(asctime)s.%(msecs)03d\" + \"\\tmessage:%(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\"\n        )\n\n        # validate\n        for param_day in ({'name': 'Start Day', 'value': start}, {'name': 'End Day', 'value': end}):\n            try:\n                cls._validate_datetime(param_day['value'])\n            except (validators.Invalid, MlbAmException) as e:\n                raise MlbAmException('{msg} a {name}.'.format(name=param_day['name'], msg=e.msg))\n        cls._validate_datetime_from_to(start, end)\n\n        # Download\n        logging.info('->- MLBAM dataset download start')\n        mlb = MlbAm(os.path.dirname(os.path.abspath(__file__)), output, cls._days(start, end))\n        mlb.download()\n        logging.info('-<- MLBAM dataset download end')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_salt_bridges(self):\n        #Define charge centers for protein - atom names\n        charge_definitions = {\"ARG\":\"name C* and around 1.4 (name N* and not name N)\", #carbon atom surrounded by nitrogens that are not in the peptide bond\n                            \"LYS\":\"name N* and not name N\", # The nitrogen that is not in the peptide bond\n                            \"HIS\":\"name N* and not name N\", # Both nitrogens (charge distributed between them)\n                            \"ASP\":\"name C* and around 1.4 (name O* and not name O)\", # Carbon surrounded by oxygens not in peptide bond\n                            \"GLU\":\"name C* and around 1.4 (name O* and not name O)\" # Carbon surrounded by oxygens not in peptide bond\n        }\n        alternative_name_sel = {\"ASP\":\"CG\", \"ARG\":\"CZ\",\"GLU\":\"CD\"}\n        prot_charge_center = {}\n        # Find the atom names - these can different in different forcefields\n        for res in [\"ASP\",\"GLU\",\"HIS\",\"LYS\",\"ARG\"]:\n            for residue in self.topology_data.universe.residues:\n                if residue.resname == res:\n                    atomselection = residue.atoms\n                    try:\n                        name_selection = atomselection.select_atoms(charge_definitions[res])\n                        prot_charge_center[res]=name_selection.names[0]\n                    except Exception as e:\n                        #In case MDAnalysis misbehaves and cannot use around\n                        prot_charge_center[res]=alternative_name_sel[res]\n                    break\n\n        #Measure distances\n        data = namedtuple(\"saltbridge\",\"frame time ligandatomid ligandatomname distance resname resid segid\")\n        i=0\n        if self.trajectory==[]:\n            self.trajectory = [self.topology_data.universe.filename]\n            self.start_frame_num=[None]\n            self.end_frame_num = [None]\n            self.skip =[None]\n        for traj in self.trajectory:\n            self.timeseries=[]\n            self.timesteps=[frame.time for frame in self.topology_data.universe.trajectory[self.start_frame_num[i]:self.end_frame_num[i]:self.skip[i]]]\n            start = timer()\n            self.topology_data.load_trajectory(traj)\n            for atom in self.lig_descr.ligand_atoms:\n                if self.lig_descr.ligand_atoms[atom][\"Formal charges\"]<0:\n                    for residue in self.topology_data.dict_of_plotted_res:\n                        if residue[0] in [\"LYS\",\"ARG\",\"HIS\"]:\n                            pos_res_atom = self.topology_data.universe.select_atoms(\"resname \"+residue[0]+\" and resid \"+residue[1]+\" and segid \"+ residue[2]+\" and name \"+prot_charge_center[residue[0]])\n                            lig_atom = self.topology_data.universe.ligand.select_atoms(\"name \"+self.lig_descr.ligand_atoms[atom][\"name\"])\n                            for frame in self.topology_data.universe.trajectory[self.start_frame_num[i]:self.end_frame_num[i]:self.skip[i]]:\n                                if residue[0]==[\"HIS\"]:\n                                    #HIS has two atoms in the selection - need to find a middle point (i.e. centroid)\n                                    pos_res_pos = pos_res_atom.centroid()\n                                    dist = math.euclidean3d(pos_res_pos,lig_atom.positions[0])\n                                else:\n                                    dist = math.euclidean3d(pos_res_atom.positions[0],lig_atom.positions[0])\n                                if dist <= math.saltbridge_dist:\n                                    contacts = data(frame=frame.frame, time=frame.time, ligandatomid=lig_atom.atoms.ids, ligandatomname=lig_atom.atoms.names,\n                                                    distance=dist, resname=residue[0],resid=residue[1],segid=residue[2])\n                                    self.timeseries.append(contacts)\n                if self.lig_descr.ligand_atoms[atom][\"Formal charges\"]>0:\n                    for residue in self.topology_data.dict_of_plotted_res:\n                        if residue[0] in [\"ASP\",\"GLU\"]:\n                            neg_res_atom = self.topology_data.universe.select_atoms(\"resname \"+residue[0]+\" and resid \"+residue[1]+\" and segid \"+ residue[2]+\" and name \"+prot_charge_center[residue[0]])\n                            lig_atom = self.topology_data.universe.ligand.select_atoms(\"name \"+self.lig_descr.ligand_atoms[atom][\"name\"])\n                            for frame in self.topology_data.universe.trajectory[self.start_frame_num[i]:self.end_frame_num[i]:self.skip[i]]:\n                                dist = math.euclidean3d(neg_res_atom.positions[0],lig_atom.positions[0])\n                                if dist <= self.saltbridge_dist:\n                                    contacts = data(frame=frame.frame, time=frame.time, ligandatomid=lig_atom.atoms.ids[0], ligandatomname=lig_atom.atoms.names[0],\n                                                    distance=dist, resname=residue[0],resid=residue[1],segid=residue[2])\n                                    self.timeseries.append(contacts)\n            self.saltbridges[i] = self.make_table()\n\n            self.saltbridges_by_time[i] = self.count_by_time()\n            self.saltbridges_by_type[i] = self.count_by_type()\n            i+=1\n        end = timer()\n        print \"Salt Bridges:\"+str(end-start)", "response": "Find the salt bridges for the current ligand atom."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes numpy array from timeseries data.", "response": "def make_table(self):\n        \"\"\"Make numpy array from timeseries data.\"\"\"\n        num_records = int(np.sum([1 for frame in self.timeseries]))\n        dtype = [(\"frame\",float),(\"time\",float),(\"ligand atom id\",int),\n                (\"ligand atom name\",\"|U4\"),(\"distance\",float),\n                (\"resid\",int),(\"resname\",\"|U4\"),(\"segid\",\"|U8\") ]\n        out = np.empty((num_records,),dtype=dtype)\n        cursor=0\n        for contact in self.timeseries:\n            out[cursor] = (contact.frame, contact.time,contact.ligandatomid,contact.ligandatomname,contact.distance,\n                           contact.resid,contact.resname,contact.segid)\n            cursor+=1\n        return out.view(np.recarray)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncounts how many times each individual salt bridge occured throughout the simulation.", "response": "def count_by_type(self):\n        \"\"\"Count how many times each individual salt bridge occured throughout the simulation.\n        Returns numpy array.\"\"\"\n        saltbridges = defaultdict(int)\n        for contact in self.timeseries:\n            #count by residue name not by proteinring\n            pkey = (contact.ligandatomid,contact.ligandatomname, contact.resid,contact.resname,contact.segid)\n            saltbridges[pkey]+=1\n        dtype = [(\"ligand_atom_id\",int),(\"ligand_atom_name\",\"|U4\"),(\"resid\",int),(\"resname\",\"|U4\"),(\"segid\",\"|U8\"),(\"frequency\",float) ]\n        out = np.empty((len(saltbridges),),dtype=dtype)\n        tsteps = float(len(self.timesteps))\n        for cursor,(key,count) in enumerate(saltbridges.iteritems()):\n            out[cursor] = key + (count / tsteps,)\n        return out.view(np.recarray)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_by_time(self):\n        out = np.empty((len(self.timesteps),), dtype=[('time', float), ('count', int)])\n        for cursor,timestep in enumerate(self.timesteps):\n            out[cursor] = (timestep,len([x for x in self.timeseries if x.time==timestep]))\n        return out.view(np.recarray)", "response": "Count how many salt bridges occured in each frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the frequency of salt bridges throughout simulations.", "response": "def get_saltbridge_frequency(self,analysis_cutoff):\n        \"\"\"Calculates the frequency of salt bridges throughout simulations. If the frequency exceeds the\n        analysis cutoff, this interaction will be taken for further consideration.\n        Takes:\n            * analysis_cutoff * - fraction of simulation time a feature has to be present for to be plotted\n        Output:\n            * self.saltbridge_frequency * - frequency of each salt bridge \"\"\"\n        self.frequency = defaultdict(int)\n        for traj in self.saltbridges_by_type:\n            for contact in self.saltbridges_by_type[traj]:\n                self.frequency[contact[\"ligand_atom_id\"],contact[\"ligand_atom_name\"],contact[\"resid\"],contact[\"resname\"],contact[\"segid\"]]+=contact[\"frequency\"]\n        draw_frequency = {i:self.frequency[i] for i in self.frequency if self.frequency[i]>(int(len(self.trajectory))*analysis_cutoff)}\n\n        self.saltbridges_for_drawing = {}\n        for contact in draw_frequency:\n            self.saltbridges_for_drawing[contact]=draw_frequency[contact]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keep_longest(head, update, down_path):\n        if update is None:\n            return 'f'\n        if head is None:\n            return 's'\n\n        return 'f' if len(head) >= len(update) else 's'", "response": "Keep longest field among head and update."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef comments(self, case_id=None, variant_id=None, username=None):\n        logger.debug(\"Looking for comments\")\n        comment_objs = self.query(Comment)\n\n        if case_id:\n            comment_objs = comment_objs.filter_by(case_id=case_id)\n\n        if variant_id:\n            comment_objs = comment_objs.filter_by(variant_id=variant_id)\n        elif case_id:\n            comment_objs = comment_objs.filter_by(variant_id=None)\n\n        return comment_objs", "response": "Return comments for a case or variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a comment to a variant or a case", "response": "def add_comment(self, case_obj, text, variant_id=None, username=None):\n        \"\"\"Add a comment to a variant or a case\"\"\"\n\n        comment = Comment(\n            text=text,\n            username=username or 'Anonymous',\n            case=case_obj,\n            # md5 sum of chrom, pos, ref, alt\n            variant_id=variant_id\n        )\n        self.session.add(comment)\n        self.save()\n        return comment"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_consequences(self, variant_obj, raw_variant_line):\n        consequences = []\n        for consequence in SO_TERMS:\n            if consequence in raw_variant_line:\n                consequences.append(consequence)\n        \n        variant_obj.consequences = consequences", "response": "Add the consequences found for a variant in a raw vcf variant."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_most_severe_consequence(self, variant_obj):\n        most_severe_consequence = None\n        most_severe_score = None\n\n        for consequence in variant_obj.consequences:\n            logger.debug(\"Checking severity score for consequence: {0}\".format(\n                consequence))\n            \n            severity_score = SEVERITY_DICT.get(consequence)\n            \n            if severity_score != None:\n                if most_severe_score:\n                    if severity_score < most_severe_score:\n                        most_severe_consequence = consequence\n                        most_severe_score = severity_score\n                else:\n                    most_severe_consequence = consequence\n                    most_severe_score = severity_score\n\n        variant_obj.most_severe_consequence = most_severe_consequence", "response": "Adds the most severe consequence to the variant_obj."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the impact severity for the most severe consequence sequence in the variant_obj.", "response": "def _add_impact_severity(self, variant_obj):\n        \"\"\"Add the impact severity for the most severe consequence\n        \n            Args:\n                variant_obj (puzzle.models.Variant)\n        \n        \"\"\"\n        if variant_obj.most_severe_consequence:\n            variant_obj.impact_severity = IMPACT_SEVERITIES.get(\n                variant_obj.most_severe_consequence\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_class():\n    # get frames\n    frames = inspect.stack()\n    cls = None\n    # should be the third frame\n    # 0: this function\n    # 1: function/decorator\n    # 2: class that contains the function\n    if len(frames) > 2:\n        frame = frames[2][0]\n        if '__module__' in frame.f_code.co_names:\n            cls = SillyClass(**frame.f_locals)\n            cls.__cls_name__ = frame.f_code.co_name\n    return cls", "response": "Return the class name for the current frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collect_appendvars(ap_, cls):\n    for key, value in cls.__dict__.items():\n        if key.startswith('appendvars_'):\n            varname = key[11:]\n            if varname not in ap_.appendvars:\n                ap_.appendvars[varname] = []\n            if value not in ap_.appendvars[varname]:\n                if not isinstance(value, list):\n                    value = [value]\n                ap_.appendvars[varname] += value", "response": "collects all appendvars in the list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isglob(value):\n    if os.name == 'nt':\n        if isinstance(value, basestring):\n            value = glob.glob(value)\n    return value", "response": "Windows non traduce automaticamente i wildchars cos\u00ec lo facciamo\n    a mano tramite glob."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef string_or_bool(value):\n    if value.lower() in ['t', 'true']:\n        value = True\n    elif value.lower() in ['f', 'false']:\n        value = False\n    elif str.isdigit(str(value)):\n        value = int(value) != 0\n    else:\n        value = str(value) # pylint: disable=redefined-variable-type\n    return value", "response": "A string o bool"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the index of the first element in the list of shared items.", "response": "def has_shared(arg, shared):\n    \"\"\"\n    Verifica se ci sono shared.\n    \"\"\"\n    try:\n        if isinstance(shared, list):\n            shared_arguments = shared\n        else:\n            shared_arguments = shared.__shared_arguments__\n        for idx, (args, kwargs) in enumerate(shared_arguments):\n            arg_name = kwargs.get(\n                'dest', args[-1].lstrip('-').replace('-', '_'))\n            if arg_name == arg:\n                return idx\n        idx = False\n    except (ValueError, AttributeError):\n        idx = False\n    return idx"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_functarguments(func):\n    argspec = inspect.getargspec(func)\n    if argspec.defaults is not None:\n        args = argspec.args[:-len(argspec.defaults)]\n        kwargs = dict(\n            zip(argspec.args[-len(argspec.defaults):], argspec.defaults))\n    else:\n        args = argspec.args\n        kwargs = {}\n    if args and args[0] == 'self':\n        args.pop(0)\n    func.__named__ = []\n    arguments = []\n    shared = get_shared(func)\n    for arg in args:\n        if has_shared(arg, shared) is not False:\n            continue\n        if has_argument(arg, func.__cls__) is not False:\n            continue\n        arguments.append(([arg], {}, ))\n        func.__named__.append(arg)\n    for key, val in kwargs.items():\n        if has_shared(key, shared) is not False:\n            continue\n        if has_argument(key, func.__cls__) is not False:\n            continue\n        if isinstance(val, dict):\n            flags = [val.pop('lflag', '--%s' % key)]\n            short = val.pop('flag', None)\n            dest = val.get('dest', key).replace('-', '_')\n            if short:\n                flags.insert(0, short)\n        else:\n            flags = ['--%s' % key]\n            val = dict(default=val)\n            dest = key.replace('-', '_')\n        func.__named__.append(dest)\n        arguments.append((flags, val, ))\n    return arguments", "response": "Return a list of functions that are passed to the function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_parser(func, parent):\n    parser = parent.add_parser(func.__cmd_name__, help=func.__doc__)\n    for args, kwargs in func.__arguments__:\n        parser.add_argument(*args, **kwargs)\n    return parser", "response": "Get Imposta il parser."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_shared(func):\n    shared = []\n    if not hasattr(func, '__cls__'):\n        return shared\n    if not hasattr(func.__cls__, '__shared_arguments__'):\n        return shared\n    if hasattr(func, '__no_share__'):\n        if func.__no_share__ is True:\n            return shared\n        else:\n            shared += [\n                s for s in func.__cls__.__shared_arguments__\n                if (s[0][-1].replace('--', '').replace('-', '_'))\n                not in func.__no_share__]\n    else:\n        shared = func.__cls__.__shared_arguments__\n    return shared", "response": "get_shared - Returns a list of all the shared items of the given function"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if help is available in argv.", "response": "def check_help():\n    \"\"\"\n    check know args in argv.\n    \"\"\"\n    # know arguments\n    know = set(('-h', '--help', '-v', '--version'))\n    # arguments\n    args = set(sys.argv[1:])\n    # returns True if there is at least one known argument in arguments\n    return len(know.intersection(args)) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_input_and_res_time_analysis(self):\n        self.topol_data = Data()\n        self.topol_data.load_data(self.topology,self.mol_file,self.ligand,self.offset)\n        if len(self.trajectory) == 0:\n            self.topol_data.analyse_topology(self.topology,self.cutoff)\n        else:\n            self.res_time = Residence_time(self.topol_data,self.trajectory, self.start, self.end, self.skip,self.topology, self.ligand,self.offset)\n            self.res_time.measure_residence_time(self.cutoff)\n            self.res_time.define_residues_for_plotting_traj(self.analysis_cutoff)\n            self.topol_data.find_the_closest_atoms(self.topology)", "response": "Load the data into Data and the Residence_Time object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_figure(self,data_for_color=None, data_for_size=None, data_for_clouds=None, rot_bonds=None, color_for_clouds=\"Blues\", color_type_color=\"viridis\"):\n        self.molecule = Molecule(self.topol_data)\n\n        self.draw = Draw(self.topol_data,self.molecule,self.hbonds,self.pistacking,self.salt_bridges,self.lig_descr)\n        self.draw.draw_molecule(data_for_color, data_for_size, data_for_clouds, rot_bonds, color_for_clouds, color_type_color)\n\n        self.figure = Figure(self.molecule,self.topol_data,self.draw)\n        self.figure.add_bigger_box()\n        self.figure.manage_the_plots()\n        self.figure.draw_white_circles()\n        self.figure.put_everything_together()\n        self.figure.write_final_draw_file(self.output_name)", "response": "Draw the molecule and then puts the final figure together with the figure."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_files(self):\n        while True:\n            try:\n                os.mkdir(self.output_name)\n            except Exception as e:\n                self.output_name = raw_input(\"This directory already exists - please enter a new name:\")\n            else:\n                break\n        self.workdir = os.getcwd()\n        os.chdir(self.workdir+\"/\"+self.output_name)", "response": "Saves all output from LINTools run in a single directory named after the output name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the current readings as a dictionary with the keys duration cpm uSvh and uSvhError.", "response": "def status(self):\n        \"\"\"Return current readings, as a dictionary with:\n            duration -- the duration of the measurements, in seconds;\n            cpm -- the radiation count by minute;\n            uSvh -- the radiation dose, exprimed in Sievert per house (uSv/h);\n            uSvhError -- the incertitude for the radiation dose.\"\"\"\n        minutes = min(self.duration, MAX_CPM_TIME) / 1000 / 60.0\n        cpm = self.count / minutes if minutes > 0 else 0\n        return dict(\n            duration=round(self.duration / 1000.0, 2),\n            cpm=round(cpm, 2),\n            uSvh=round(cpm / K_ALPHA, 3),\n            uSvhError=round(math.sqrt(self.count) / minutes / K_ALPHA, 3)\n            if minutes > 0\n            else 0,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the driver by setting up GPIO interrupts and periodic statistics processing.", "response": "def setup(self):\n        \"\"\"Initialize the driver by setting up GPIO interrupts\n        and periodic statistics processing. \"\"\"\n        # Initialize the statistics variables.\n        self.radiation_count = 0\n        self.noise_count = 0\n        self.count = 0\n        # Initialize count_history[].\n        self.count_history = [0] * HISTORY_LENGTH\n        self.history_index = 0\n        # Init measurement time.\n        self.previous_time = millis()\n        self.previous_history_time = millis()\n        self.duration = 0\n        # Init the GPIO context.\n        GPIO.setup(self.radiation_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n        GPIO.setup(self.noise_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n        # Register local callbacks.\n        GPIO.add_event_detect(\n            self.radiation_pin, GPIO.FALLING, callback=self._on_radiation\n        )\n        GPIO.add_event_detect(self.noise_pin, GPIO.FALLING, callback=self._on_noise)\n        # Enable the timer for processing the statistics periodically.\n        self._enable_timer()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscrape a MLBAM Data structure", "response": "def main(start, end, out):\n    \"\"\"\n    Scrape a MLBAM Data\n    :param start: Start Day(YYYYMMDD)\n    :param end: End Day(YYYYMMDD)\n    :param out: Output directory(default:\"../output/mlb\")\n    \"\"\"\n    try:\n        logging.basicConfig(level=logging.WARNING)\n        MlbAm.scrape(start, end, out)\n    except MlbAmBadParameter as e:\n        raise click.BadParameter(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(ctx, variant_source, family_file, family_type, root):\n    root = root or ctx.obj.get('root') or os.path.expanduser(\"~/.puzzle\")\n\n    if os.path.isfile(root):\n        logger.error(\"'root' can't be a file\")\n        ctx.abort()\n\n    logger.info(\"Root directory is: {}\".format(root))\n\n    db_path = os.path.join(root, 'puzzle_db.sqlite3')\n    logger.info(\"db path is: {}\".format(db_path))\n    \n    if not os.path.exists(db_path):\n        logger.warn(\"database not initialized, run 'puzzle init'\")\n        ctx.abort()\n\n\n    if not os.path.isfile(variant_source):\n        logger.error(\"Variant source has to be a file\")\n        ctx.abort()\n    \n    mode = get_file_type(variant_source)\n    if mode == 'unknown':\n        logger.error(\"Unknown file type\")\n        ctx.abort()\n    #Test if gemini is installed\n    elif mode == 'gemini':\n        logger.debug(\"Initialzing GEMINI plugin\")\n        if not GEMINI:\n            logger.error(\"Need to have gemini installed to use gemini plugin\")\n            ctx.abort()\n\n    logger.debug('Set puzzle backend to {0}'.format(mode))\n    \n    variant_type = get_variant_type(variant_source)\n    logger.debug('Set variant type to {0}'.format(variant_type))\n    \n    cases = get_cases(\n        variant_source=variant_source,\n        case_lines=family_file, \n        case_type=family_type, \n        variant_type=variant_type, \n        variant_mode=mode\n    )\n    \n    if len(cases) == 0:\n        logger.warning(\"No cases found\")\n        ctx.abort()\n\n    logger.info(\"Initializing sqlite plugin\")\n    store = SqlStore(db_path)\n\n    for case_obj in cases:\n        if store.case(case_obj.case_id) is not None:\n            logger.warn(\"{} already exists in the database\"\n                        .format(case_obj.case_id))\n            continue\n        # extract case information\n        logger.debug(\"adding case: {} to puzzle db\".format(case_obj.case_id))\n        store.add_case(case_obj, vtype=variant_type, mode=mode)", "response": "Load a single variant source into the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hash_from_stream(n, hash_stream):\n    x = to_int64(0x345678)\n    multiplied = to_int64(1000003)\n    for i in range(n - 1, -1, -1):\n        h = next(hash_stream)\n        if h is -1:\n            return -1\n        x = (x ^ h) * multiplied\n        multiplied += to_int64(82520 + 2 * n)\n    x += 97531\n    if x == -1:\n        return -2\n\n    return x", "response": "Return the hash of the n - th element of the stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve deal s associated contacts", "response": "def list(self, deal_id, **params):\n        \"\"\"\n        Retrieve deal's associated contacts\n\n        Returns all deal associated contacts\n\n        :calls: ``get /deals/{deal_id}/associated_contacts``\n        :param int deal_id: Unique identifier of a Deal.\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of AssociatedContacts.\n        :rtype: list\n        \"\"\"\n\n        _, _, associated_contacts = self.http_client.get(\"/deals/{deal_id}/associated_contacts\".format(deal_id=deal_id), params=params)\n        return associated_contacts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self, deal_id, *args, **kwargs):\n\n        if not args and not kwargs:\n            raise Exception('attributes for AssociatedContact are missing')\n\n        attributes = args[0] if args else kwargs\n        attributes = dict((k, v) for k, v in attributes.iteritems() if k in self.OPTS_KEYS_TO_PERSIST)\n\n        _, _, associated_contact = self.http_client.post(\"/deals/{deal_id}/associated_contacts\".format(deal_id=deal_id), body=attributes)\n        return associated_contact", "response": "Creates an associated contact and role associated contact."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef destroy(self, deal_id, contact_id) :\n\n        status_code, _, _ = self.http_client.delete(\"/deals/{deal_id}/associated_contacts/{contact_id}\".format(deal_id=deal_id, contact_id=contact_id))\n        return status_code == 204", "response": "Removes an associated contact_id from the associated_contacts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(self, **params):\n\n        _, _, contacts = self.http_client.get(\"/contacts\", params=params)\n        return contacts", "response": "Retrieve all contacts available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retrieve(self, id) :\n\n        _, _, contact = self.http_client.get(\"/contacts/{id}\".format(id=id))\n        return contact", "response": "Retrieve a single contact from the API"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves all deals available to the user according to the parameters provided.", "response": "def list(self, **params):\n        \"\"\"\n        Retrieve all deals\n\n        Returns all deals available to the user according to the parameters provided\n\n        :calls: ``get /deals``\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of Deals.\n        :rtype: list\n        \"\"\"\n\n        _, _, deals = self.http_client.get(\"/deals\", params=params)\n        for deal in deals:\n            deal['value'] = Coercion.to_decimal(deal['value'])\n\n        return deals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new deal with the given attributes.", "response": "def create(self, *args, **kwargs):\n        \"\"\"\n        Create a deal\n\n        Create a new deal\n\n        :calls: ``post /deals``\n        :param tuple *args: (optional) Single object representing Deal resource.\n        :param dict **kwargs: (optional) Deal attributes.\n        :return: Dictionary that support attriubte-style access and represents newely created Deal resource.\n        :rtype: dict\n        \"\"\"\n\n        if not args and not kwargs:\n            raise Exception('attributes for Deal are missing')\n\n        attributes = args[0] if args else kwargs\n        attributes = dict((k, v) for k, v in attributes.iteritems() if k in self.OPTS_KEYS_TO_PERSIST)\n\n        if \"value\" in attributes:\n            attributes[\"value\"] = Coercion.to_string(attributes[\"value\"])\n        _, _, deal = self.http_client.post(\"/deals\", body=attributes)\n        deal[\"value\"] = Coercion.to_decimal(deal[\"value\"])\n        return deal"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a single deal from the API.", "response": "def retrieve(self, id) :\n        \"\"\"\n        Retrieve a single deal\n\n        Returns a single deal available to the user, according to the unique deal ID provided\n        If the specified deal does not exist, the request will return an error\n\n        :calls: ``get /deals/{id}``\n        :param int id: Unique identifier of a Deal.\n        :return: Dictionary that support attriubte-style access and represent Deal resource.\n        :rtype: dict\n        \"\"\"\n\n        _, _, deal = self.http_client.get(\"/deals/{id}\".format(id=id))\n        deal[\"value\"] = Coercion.to_decimal(deal[\"value\"])\n        return deal"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating a deal s information with the attributes provided.", "response": "def update(self, id, *args, **kwargs):\n        \"\"\"\n        Update a deal\n\n        Updates deal information\n        If the specified deal does not exist, the request will return an error\n        <figure class=\"notice\">\n        In order to modify tags used on a record, you need to supply the entire set\n        `tags` are replaced every time they are used in a request\n        </figure>\n\n        :calls: ``put /deals/{id}``\n        :param int id: Unique identifier of a Deal.\n        :param tuple *args: (optional) Single object representing Deal resource which attributes should be updated.\n        :param dict **kwargs: (optional) Deal attributes to update.\n        :return: Dictionary that support attriubte-style access and represents updated Deal resource.\n        :rtype: dict\n        \"\"\"\n\n        if not args and not kwargs:\n            raise Exception('attributes for Deal are missing')\n\n        attributes = args[0] if args else kwargs\n        attributes = dict((k, v) for k, v in attributes.iteritems() if k in self.OPTS_KEYS_TO_PERSIST)\n        if \"value\" in attributes:\n            attributes[\"value\"] = Coercion.to_string(attributes[\"value\"])\n\n        _, _, deal = self.http_client.put(\"/deals/{id}\".format(id=id), body=attributes)\n        deal[\"value\"] = Coercion.to_decimal(deal[\"value\"])\n        return deal"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves all sources Returns all sources", "response": "def list(self, **params):\n        \"\"\"\n        Retrieve all sources\n\n        Returns all deal sources available to the user according to the parameters provided\n\n        :calls: ``get /deal_sources``\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of DealSources.\n        :rtype: list\n        \"\"\"\n\n        _, _, deal_sources = self.http_client.get(\"/deal_sources\", params=params)\n        return deal_sources"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a single source by the unique identifier", "response": "def retrieve(self, id) :\n        \"\"\"\n        Retrieve a single source\n\n        Returns a single source available to the user by the provided id\n        If a source with the supplied unique identifier does not exist it returns an error\n\n        :calls: ``get /deal_sources/{id}``\n        :param int id: Unique identifier of a DealSource.\n        :return: Dictionary that support attriubte-style access and represent DealSource resource.\n        :rtype: dict\n        \"\"\"\n\n        _, _, deal_source = self.http_client.get(\"/deal_sources/{id}\".format(id=id))\n        return deal_source"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, id, *args, **kwargs):\n\n        if not args and not kwargs:\n            raise Exception('attributes for DealSource are missing')\n\n        attributes = args[0] if args else kwargs\n        attributes = dict((k, v) for k, v in attributes.iteritems() if k in self.OPTS_KEYS_TO_PERSIST)\n\n        _, _, deal_source = self.http_client.put(\"/deal_sources/{id}\".format(id=id), body=attributes)\n        return deal_source", "response": "Updates a source s attributes and returns a dict containing the updated attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a source and all associated data sets.", "response": "def destroy(self, id) :\n        \"\"\"\n        Delete a source\n\n        Delete an existing source\n        If the specified source does not exist, the request will return an error\n        This operation cannot be undone\n\n        :calls: ``delete /deal_sources/{id}``\n        :param int id: Unique identifier of a DealSource.\n        :return: True if the operation succeeded.\n        :rtype: bool\n        \"\"\"\n\n        status_code, _, _ = self.http_client.delete(\"/deal_sources/{id}\".format(id=id))\n        return status_code == 204"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, **params):\n\n        _, _, deal_unqualified_reasons = self.http_client.get(\"/deal_unqualified_reasons\", params=params)\n        return deal_unqualified_reasons", "response": "Returns all deal unqualified reasons available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retrieve(self, id) :\n\n        _, _, deal_unqualified_reason = self.http_client.get(\"/deal_unqualified_reasons/{id}\".format(id=id))\n        return deal_unqualified_reason", "response": "Retrieve a single deal unqualified reason from the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(self, **params):\n\n        _, _, leads = self.http_client.get(\"/leads\", params=params)\n        return leads", "response": "Retrieve all leads available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a single lead by its unique identifier", "response": "def retrieve(self, id) :\n        \"\"\"\n        Retrieve a single lead\n\n        Returns a single lead available to the user, according to the unique lead ID provided\n        If the specified lead does not exist, this query returns an error\n\n        :calls: ``get /leads/{id}``\n        :param int id: Unique identifier of a Lead.\n        :return: Dictionary that support attriubte-style access and represent Lead resource.\n        :rtype: dict\n        \"\"\"\n\n        _, _, lead = self.http_client.get(\"/leads/{id}\".format(id=id))\n        return lead"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves all sources Returns all lead sources available to the user according to the parameters provided", "response": "def list(self, **params):\n        \"\"\"\n        Retrieve all sources\n\n        Returns all lead sources available to the user according to the parameters provided\n\n        :calls: ``get /lead_sources``\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of LeadSources.\n        :rtype: list\n        \"\"\"\n\n        _, _, lead_sources = self.http_client.get(\"/lead_sources\", params=params)\n        return lead_sources"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve(self, id) :\n\n        _, _, lead_source = self.http_client.get(\"/lead_sources/{id}\".format(id=id))\n        return lead_source", "response": "Retrieve a single source by the provided unique identifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, **params):\n\n        _, _, lead_unqualified_reasons = self.http_client.get(\"/lead_unqualified_reasons\", params=params)\n        return lead_unqualified_reasons", "response": "Returns all lead unqualified reasons available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(self, order_id, **params):\n\n        _, _, line_items = self.http_client.get(\"/orders/{order_id}/line_items\".format(order_id=order_id), params=params)\n        return line_items", "response": "Retrieve order s line items associated to order_id"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve(self, order_id, id) :\n\n        _, _, line_item = self.http_client.get(\"/orders/{order_id}/line_items/{id}\".format(order_id=order_id, id=id))\n        return line_item", "response": "Retrieve a single line item from an order according to the unique line item ID provided"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, **params):\n\n        _, _, loss_reasons = self.http_client.get(\"/loss_reasons\", params=params)\n        return loss_reasons", "response": "Retrieve all reasons\n        Returns all reasons\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve(self, id) :\n\n        _, _, loss_reason = self.http_client.get(\"/loss_reasons/{id}\".format(id=id))\n        return loss_reason", "response": "Retrieve a single loss reason by the provided unique identifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(self, **params):\n\n        _, _, notes = self.http_client.get(\"/notes\", params=params)\n        return notes", "response": "Retrieve all notes available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve(self, id) :\n\n        _, _, note = self.http_client.get(\"/notes/{id}\".format(id=id))\n        return note", "response": "Retrieve a single note by its unique identifier"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve all orders available to the user according to the parameters provided.", "response": "def list(self, **params):\n        \"\"\"\n        Retrieve all orders\n\n        Returns all orders available to the user according to the parameters provided\n\n        :calls: ``get /orders``\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of Orders.\n        :rtype: list\n        \"\"\"\n\n        _, _, orders = self.http_client.get(\"/orders\", params=params)\n        return orders"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an order for a deal", "response": "def create(self, *args, **kwargs):\n        \"\"\"\n        Create an order\n\n        Create a new order for a deal\n        User needs to have access to the deal to create an order\n        Each deal can have at most one order and error is returned when attempting to create more\n\n        :calls: ``post /orders``\n        :param tuple *args: (optional) Single object representing Order resource.\n        :param dict **kwargs: (optional) Order attributes.\n        :return: Dictionary that support attriubte-style access and represents newely created Order resource.\n        :rtype: dict\n        \"\"\"\n\n        if not args and not kwargs:\n            raise Exception('attributes for Order are missing')\n\n        attributes = args[0] if args else kwargs\n        attributes = dict((k, v) for k, v in attributes.iteritems() if k in self.OPTS_KEYS_TO_PERSIST)\n\n        _, _, order = self.http_client.post(\"/orders\", body=attributes)\n        return order"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retrieve(self, id) :\n\n        _, _, order = self.http_client.get(\"/orders/{id}\".format(id=id))\n        return order", "response": "Retrieve a single order by its unique identifier"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self, **params):\n\n        _, _, pipelines = self.http_client.get(\"/pipelines\", params=params)\n        return pipelines", "response": "Retrieve all pipelines available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve all products available to the user according to the parameters provided.", "response": "def list(self, **params):\n        \"\"\"\n        Retrieve all products\n\n        Returns all products available to the user according to the parameters provided\n\n        :calls: ``get /products``\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of Products.\n        :rtype: list\n        \"\"\"\n\n        _, _, products = self.http_client.get(\"/products\", params=params)\n        return products"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a single product by its unique identifier", "response": "def retrieve(self, id) :\n        \"\"\"\n        Retrieve a single product\n\n        Returns a single product, according to the unique product ID provided\n        If the specified product does not exist, the request will return an error\n\n        :calls: ``get /products/{id}``\n        :param int id: Unique identifier of a Product.\n        :return: Dictionary that support attriubte-style access and represent Product resource.\n        :rtype: dict\n        \"\"\"\n\n        _, _, product = self.http_client.get(\"/products/{id}\".format(id=id))\n        return product"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, **params):\n\n        _, _, sources = self.http_client.get(\"/sources\", params=params)\n        return sources", "response": "Retrieve all sources available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a single source by the provided unique identifier", "response": "def retrieve(self, id) :\n        \"\"\"\n        Retrieve a single source\n\n        Returns a single source available to the user by the provided id\n        If a source with the supplied unique identifier does not exist it returns an error\n\n        :calls: ``get /sources/{id}``\n        :param int id: Unique identifier of a Source.\n        :return: Dictionary that support attriubte-style access and represent Source resource.\n        :rtype: dict\n        \"\"\"\n\n        _, _, source = self.http_client.get(\"/sources/{id}\".format(id=id))\n        return source"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, **params):\n\n        _, _, stages = self.http_client.get(\"/stages\", params=params)\n        return stages", "response": "Retrieve all stages available to the user according to the parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves all tags available to the user according to the parameters provided.", "response": "def list(self, **params):\n        \"\"\"\n        Retrieve all tags\n\n        Returns all tags available to the user, according to the parameters provided\n\n        :calls: ``get /tags``\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of Tags.\n        :rtype: list\n        \"\"\"\n\n        _, _, tags = self.http_client.get(\"/tags\", params=params)\n        return tags"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve(self, id) :\n\n        _, _, tag = self.http_client.get(\"/tags/{id}\".format(id=id))\n        return tag", "response": "Retrieve a single tag from the API."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self, **params):\n\n        _, _, tasks = self.http_client.get(\"/tasks\", params=params)\n        return tasks", "response": "Retrieve all tasks available to the user according to the parameters provided\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef retrieve(self, id) :\n\n        _, _, task = self.http_client.get(\"/tasks/{id}\".format(id=id))\n        return task", "response": "Retrieve a single task from the user s account."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, **params):\n\n        _, _, text_messages = self.http_client.get(\"/text_messages\", params=params)\n        return text_messages", "response": "Retrieve text messages by the key provided by the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a single text message according to the unique ID provided", "response": "def retrieve(self, id) :\n        \"\"\"\n        Retrieve a single text message\n\n        Returns a single text message according to the unique  ID provided\n        If the specified user does not exist, this query returns an error\n\n        :calls: ``get /text_messages/{id}``\n        :param int id: Unique identifier of a TextMessage.\n        :return: Dictionary that support attriubte-style access and represent TextMessage resource.\n        :rtype: dict\n        \"\"\"\n\n        _, _, text_message = self.http_client.get(\"/text_messages/{id}\".format(id=id))\n        return text_message"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, **params):\n\n        _, _, users = self.http_client.get(\"/users\", params=params)\n        return users", "response": "Retrieve all users by the given parameters"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a single user according to the unique user ID provided", "response": "def retrieve(self, id) :\n        \"\"\"\n        Retrieve a single user\n\n        Returns a single user according to the unique user ID provided\n        If the specified user does not exist, this query returns an error\n\n        :calls: ``get /users/{id}``\n        :param int id: Unique identifier of a User.\n        :return: Dictionary that support attriubte-style access and represent User resource.\n        :rtype: dict\n        \"\"\"\n\n        _, _, user = self.http_client.get(\"/users/{id}\".format(id=id))\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self, **params):\n\n        _, _, visits = self.http_client.get(\"/visits\", params=params)\n        return visits", "response": "Retrieve visits by the given parameters"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving visit outcomes according to the parameters provided", "response": "def list(self, **params):\n        \"\"\"\n        Retrieve visit outcomes\n\n        Returns Visit Outcomes, according to the parameters provided\n\n        :calls: ``get /visit_outcomes``\n        :param dict params: (optional) Search options.\n        :return: List of dictionaries that support attriubte-style access, which represent collection of VisitOutcomes.\n        :rtype: list\n        \"\"\"\n\n        _, _, visit_outcomes = self.http_client.get(\"/visit_outcomes\", params=params)\n        return visit_outcomes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing the HTTP Request and return data", "response": "def request(url, *args, **kwargs):\n    \"\"\"Do the HTTP Request and return data\"\"\"\n    method = kwargs.get('method', 'GET')\n    timeout = kwargs.pop('timeout', 10)  # hass default timeout\n    req = requests.request(method, url, *args, timeout=timeout, **kwargs)\n    data = req.json()\n    _LOGGER.debug(json.dumps(data))\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef message_worker(device):\n    _LOGGER.debug(\"Starting Worker Thread.\")\n    msg_q = device.messages\n\n    while True:\n\n        if not msg_q.empty():\n            message = msg_q.get()\n\n            data = {}\n            try:\n                data = json.loads(message.decode(\"utf-8\"))\n            except ValueError:\n                _LOGGER.error(\"Received invalid message: %s\", message)\n\n            if 'device_id' in data:\n                device_id = data.get('device_id')\n                if device_id == device.device_id:\n                    device.handle_event(data)\n                else:\n                    _LOGGER.warning(\"Received message for unknown device.\")\n            msg_q.task_done()\n\n        time.sleep(0.2)", "response": "Loop through messages and pass them on to right device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef socket_worker(sock, msg_q):\n    _LOGGER.debug(\"Starting Socket Thread.\")\n    while True:\n        try:\n            data, addr = sock.recvfrom(1024)    # buffer size is 1024 bytes\n        except OSError as err:\n            _LOGGER.error(err)\n        else:\n            _LOGGER.debug(\"received message: %s from %s\", data, addr)\n            msg_q.put(data)\n        time.sleep(0.2)", "response": "Socket Loop that fills message queue"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toposort(graph, pick_first='head'):\n    in_deg = {}\n    for node, next_nodes in six.iteritems(graph):\n        for next_node in [next_nodes.head_node, next_nodes.update_node]:\n            if next_node is None:\n                continue\n            in_deg[next_node] = in_deg.get(next_node, 0) + 1\n\n    stk = [FIRST]\n    ordered = []\n    visited = set()\n    while stk:\n        node = stk.pop()\n        visited.add(node)\n        if node != FIRST:\n            ordered.append(node)\n        traversal = _get_traversal(graph.get(node, BeforeNodes()), pick_first)\n        for next_node in traversal:\n            if next_node is None:\n                continue\n            if next_node in visited:\n                raise ValueError('Graph has a cycle')\n\n            in_deg[next_node] -= 1\n            if in_deg[next_node] == 0:\n                stk.append(next_node)\n\n    # Nodes may not be walked because they don't reach in degree 0.\n    if len(ordered) != len(graph) - 1:\n        raise ValueError('Graph has a cycle')\n    return ordered", "response": "Toplogically sorts a list of nodes in a graph using a tiebreaker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort_cyclic_graph_best_effort(graph, pick_first='head'):\n    ordered = []\n    visited = set()\n    # Go first on the pick_first chain then go back again on the others\n    # that were not visited. Given the way the graph is built both chains\n    # will always contain all the elements.\n    if pick_first == 'head':\n        fst_attr, snd_attr = ('head_node', 'update_node')\n    else:\n        fst_attr, snd_attr = ('update_node', 'head_node')\n\n    current = FIRST\n    while current is not None:\n        visited.add(current)\n        current = getattr(graph[current], fst_attr)\n        if current not in visited and current is not None:\n            ordered.append(current)\n    current = FIRST\n    while current is not None:\n        visited.add(current)\n        current = getattr(graph[current], snd_attr)\n        if current not in visited and current is not None:\n            ordered.append(current)\n    return ordered", "response": "Fallback for cases in which the graph has cycles."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post(url, var):\n    data = {b[0]: b[1] for b in [a.split(\"=\") for a in var]}\n    writeln(\"Sending data to url\", url)\n    response = requests.post(url, data=data)\n    if response.status_code == 200:\n        writeln(response.text)\n    else:\n        writeln(str(response.status_code), response.reason)", "response": "Post data to an url."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cast_bytes(s, encoding='utf8', errors='strict'):\n    if isinstance(s, bytes):\n        return s\n    elif isinstance(s, str):\n        return s.encode(encoding, errors)\n    else:\n        raise TypeError(\"Expected unicode or bytes, got %r\" % s)", "response": "cast str or bytes to bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cast_str(s, encoding='utf8', errors='strict'):\n    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    elif isinstance(s, str):\n        return s\n    else:\n        raise TypeError(\"Expected unicode or bytes, got %r\" % s)", "response": "cast bytes or str to str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncast timestamp to datetime or date str", "response": "def cast_datetime(ts, fmt=None):\n    \"\"\"cast timestamp to datetime or date str\"\"\"\n    dt = datetime.datetime.fromtimestamp(ts)\n    if fmt:\n        return dt.strftime(fmt)\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread xml object containing all objects", "response": "def read_xml(cls, url, markup, game):\n        \"\"\"\n        read xml object\n        :param url: contents url\n        :param markup: markup provider\n        :param game: MLBAM Game object\n        :return: pitchpx.game.players.Players object\n        \"\"\"\n        return Players._read_objects(MlbamUtil.find_xml(\"\".join([url, cls.FILENAME]), markup) ,game)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading objects from the object soup and game", "response": "def _read_objects(cls, soup, game):\n        \"\"\"\n        read objects\n        :param soup: Beautifulsoup object\n        :param game: MLBAM Game object\n        :return: pitchpx.game.players.Players object\n        \"\"\"\n        players = Players()\n        players.game = Players.Game()\n        # game data\n        players.game.venue = soup.game['venue']\n        players.game.date = soup.game['date']\n        # players & team data\n        for team in soup.find_all('team'):\n            team_object = cls._get_team(team)\n            if team['type'] == Game.TEAM_TYPE_HOME:\n                # team data(home)\n                players.home_team = team_object\n            elif team['type'] == Game.TEAM_TYPE_AWAY:\n                # team data(away)\n                players.away_team = team_object\n            # player data\n            players.rosters.update(\n                    {player['id']: cls.Player(player, game.retro_game_id) for player in team.find_all('player')}\n            )\n            # coach data\n            players.coaches.update(\n                    {coach['id']: cls.Coach(coach, game.retro_game_id, team_object) for coach in team.find_all('coach')}\n            )\n        # umpire data\n        umpires = soup.find('umpires')\n        players.umpires.update(\n                {umpire['id']: cls.Umpire(umpire, game.retro_game_id) for umpire in umpires.find_all('umpire')}\n        )\n        return players"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_team(cls, soup):\n        team = cls.Team()\n        team.team_type = soup['type']\n        team.id = soup['id']\n        team.name = soup['name']\n        return team", "response": "get team data from soup"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks for stats ditit check for stats", "response": "def isdigit(cls, value):\n        \"\"\"\n        ditit check for stats\n        :param value: stats value\n        :return: True or False\n        \"\"\"\n        if str(value).replace('.','').replace('-','').isdigit():\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef singleton_init_by(init_fn=None):\n\n    if not init_fn:\n        def wrap_init(origin_init):\n            return origin_init\n    else:\n        def wrap_init(origin_init):\n            def __init__(self):\n                origin_init(self)\n                init_fn(self)\n\n            return __init__\n\n    def inner(cls_def: type):\n        if not hasattr(cls_def, '__instancecheck__') or isinstance(cls_def.__instancecheck__,\n                                                                   (types.BuiltinMethodType, _slot_wrapper)):\n            def __instancecheck__(self, instance):\n                return instance is self\n\n            cls_def.__instancecheck__ = __instancecheck__\n\n        _origin_init = cls_def.__init__\n        cls_def.__init__ = wrap_init(_origin_init)\n\n        return cls_def()\n\n    return inner", "response": "Decorator to create a singleton object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a function that returns a new object.", "response": "def execute(func: types.FunctionType):\n    \"\"\"\n    >>> from Redy.Magic.Classic import execute\n    >>> x = 1\n    >>> @execute\n    >>> def f(x = x) -> int:\n    >>>     return x + 1\n    >>> assert f is 2\n    \"\"\"\n    spec = getfullargspec(func)\n    default = spec.defaults\n    arg_cursor = 0\n\n    def get_item(name):\n        nonlocal arg_cursor\n        ctx = func.__globals__\n        value = ctx.get(name, _undef)\n        if value is _undef:\n            try:\n                value = default[arg_cursor]\n                arg_cursor += 1\n            except (TypeError, IndexError):\n                raise ValueError(f\"Current context has no variable `{name}`\")\n        return value\n\n    return func(*(get_item(arg_name) for arg_name in spec.args))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cast(cast_fn):\n\n    def inner(func):\n        def call(*args, **kwargs):\n            return cast_fn(func(*args, **kwargs))\n\n        functools.update_wrapper(call, func)\n        return call\n\n    return inner", "response": "A decorator that casts a list of objects to a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrecord the class definition.", "response": "def record(cls_def):\n    \"\"\"\n    Namedtuple which could inherit from other types.\n    >>> from Redy.Magic.Classic import record\n    >>> class Interface: pass\n    >>> @record\n    >>> class S(Interface):\n    >>>     name: str\n    >>>     addr: str\n    >>>     sex : int\n\n    >>> s = S(\"sam\", \"I/O\", 1)\n    \"\"\"\n    annotations = getattr(cls_def, '__annotations__', {})\n    typ: type = namedtuple(cls_def.__name__, list(annotations.keys()))\n    return cls_def.__class__(cls_def.__name__, (typ, *cls_def.__bases__), dict(cls_def.__dict__))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new action with specific priority", "response": "def insert(self, action: Action, where: 'Union[int, Delegate.Where]'):\n        \"\"\"\n        add a new action with specific priority\n\n        >>> delegate: Delegate\n        >>> delegate.insert(lambda task, product, ctx: print(product), where=Delegate.Where.after(lambda action: action.__name__ == 'myfunc'))\n        the codes above inserts an action after the specific action whose name is 'myfunc'.\n        \"\"\"\n        if isinstance(where, int):\n            self.actions.insert(where, action)\n            return\n\n        here = where(self.actions)\n        self.actions.insert(here, action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_to_conflict_set(patch):\n    patch_type, patched_key, value = patch\n    if isinstance(patched_key, list):\n        key_path = tuple(patched_key)\n    else:\n        key_path = tuple(k for k in patched_key.split('.') if k)\n\n    conflicts = set()\n    if patch_type == REMOVE:\n        conflict_type = ConflictType.REMOVE_FIELD\n        for key, obj in value:\n            conflicts.add(Conflict(conflict_type, key_path + (key, ), None))\n    elif patch_type == CHANGE:\n        conflict_type = ConflictType.SET_FIELD\n        first_val, second_val = value\n        conflicts.add(Conflict(conflict_type, key_path, second_val))\n    elif patch_type == ADD:\n        conflict_type = ConflictType.SET_FIELD\n        for key, obj in value:\n            conflicts.add(Conflict(conflict_type, key_path + (key, ), obj))\n\n    return conflicts", "response": "Translates a dictdiffer conflict into a json_merger one."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms merge of head and update starting from root.", "response": "def merge(self):\n        \"\"\"Perform merge of head and update starting from root.\"\"\"\n        if isinstance(self.head, dict) and isinstance(self.update, dict):\n            if not isinstance(self.root, dict):\n                self.root = {}\n            self._merge_dicts()\n        else:\n            self._merge_base_values()\n\n        if self.conflict_set:\n            raise MergeError('Dictdiffer Errors', self.conflicts)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_pa_terminal(cls, ball_tally, strike_tally, pitch_res, event_cd):\n        if RetroSheet.is_pa_terminal(ball_tally, strike_tally, pitch_res, event_cd):\n            return MlbamConst.FLG_TRUE\n        return MlbamConst.FLG_FALSE", "response": "Return whether or not the current event is a PA terminal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of all the information for a given pitch and pa.", "response": "def row(cls, pitch, pa, pitch_list, ball_tally, strike_tally):\n        \"\"\"\n        Pitching Result\n        Pitch f/x fields: https://fastballs.wordpress.com/category/pitchfx-glossary/\n        :param pitch: pitch object(type:Beautifulsoup)\n        :param pa: At bat data for pa(dict)\n        :param pitch_list: Pitching\n        :param ball_tally: Ball telly\n        :param strike_tally: Strike telly\n        :return: {\n            'retro_game_id': Retrosheet Game id\n            'game_type': Game Type(S/R/F/D/L/W)\n            'game_type_des': Game Type Description\n            (Spring Training or Regular Season or Wild-card Game or Divisional Series or LCS or World Series)\n            'st_fl': Spring Training FLAG(T or F)\n            'regseason_fl': Regular Season FLAG(T or F)\n            'playoff_fl': Play Off Flag(T or F)\n            'local_game_time': Game Time(UTC -5)\n            'game_id': Game Id\n            'home_team_id': Home Team Id\n            'home_team_lg': Home Team league(AL or NL)\n            'away_team_id': Away Team Id\n            'away_team_lg': Away Team league(AL or NL)\n            'home_team_name': Home Team Name\n            'away_team_name': Away Team Name\n            'home_team_name_full': Home Team Name(Full Name)\n            'away_team_name_full': Away Team Name(Full Name)\n            'interleague_fl': Inter League Flag(T or F)\n            'park_id': Park Id\n            'park_name': Park Name\n            'park_loc': Park Location\n            'inning_number': Inning Number\n            'bat_home_id': Batter Id\n            'outs_ct': Out count\n            'pit_mlbid': Pitcher Id\n            'pit_first_name': Pitcher First Name\n            'pit_last_name': Pitcher Last Name\n            'pit_box_name': Pitcher Box name\n            'pit_hand_cd': Pitcher Throw Hand(R or L)\n            'bat_first_name': Batter First Name\n            'bat_last_name': Batter Last Name\n            'bat_box_name': Batter Box name\n            'ab_number': At Bat Sequence Number in Game\n            'start_bases': Bases(Before At Bat)\n            (___, 1__, 12_, 123, etc...)\n            'end_bases': Bases(After At Bat)\n            (___, 1__, 12_, 123, etc...)\n            'event_outs_ct': Event Out Count\n            'pa_ball_ct': Plate appearance Ball count\n            'pa_strike_ct': Plate appearance Strike count\n            'pitch_seq': Pitch Sequence(Strike or Ball) ex: B, SSB, BBSBS etc...\n            'pa_terminal_fl': Plate appearance Terminate Flag(T or F)\n            'pa_event_cd': Event Code for Retrosheet http://www.retrosheet.org/datause.txt\n            'pitch_res': Pitch Response(S or B or X) X = In Play\n            'pitch_des': Pitch Description\n            'pitch_id': Pitch Id\n            'x': Point for X(inches)\n            'y': Point for Y(inches)\n            'start_speed': The pitch speed(MPH) at the initial point\n            'end_speed': The pitch speed(MPH) at the current batters\n            'sz_top': The distance in feet from the ground to the top of the current batter\u2019s\n            'sz_bot': The distance in feet from the ground to the bottom of the current batter\u2019s\n            'pfx_x': The horizontal movement, in inches, of the pitch between the release point and home plate\n            'pfx_z': The vertical movement, in inches, of the pitch between the release point and home plate\n            'px': The left/right distance, in feet, of the pitch from the middle of the plate as it crossed home plate\n            'pz': The height of the pitch in feet as it crossed the front of home plate\n            'x0': The left/right distance, in feet, of the pitch, measured at the initial point\n            'y0': The distance in feet from home plate where the PITCHf/x system is set to measure the initial parameters\n            'z0': The height, in feet, of the pitch, measured at the initial point\n            'vx0': The velocity of the pitch, in feet per second, in three dimensions, measured at the initial point\n            'vy0': The velocity of the pitch, in feet per second, in three dimensions, measured at the initial point\n            'vz0': The velocity of the pitch, in feet per second, in three dimensions, measured at the initial point\n            'ax': The acceleration of the pitch, in feet per second per second, in three dimensions, measured at the initial point\n            'ay': The acceleration of the pitch, in feet per second per second, in three dimensions, measured at the initial point\n            'az': The acceleration of the pitch, in feet per second per second, in three dimensions, measured at the initial point\n            'break_y': The distance in feet from the ground to the top of the current batter\u2019s\n            'break_angle': The angle, in degrees, from vertical to the straight line path from the release point to where the pitch crossed the front of home plate, as seen from the catcher\u2019s/umpire\u2019s perspective\n            'break_length': The measurement of the greatest distance, in inches, between the trajectory of the pitch at any point between the release point and the front of home plate\n            'pitch_type': Pitch Type\n            'pitch_type_seq': Pitch type Sequence, ex:FF|CU|FF\n            'type_confidence': Pitch type confidence\n            'zone': Pitch Zone\n            'spin_dir': Pitch Spin Dir\n            'spin_rate': Pitch Spin Rate\n            'sv_id': Pitch in the air(From Datetime_To Datetime)\n            'event_num': Event Sequence Number(atbat, pitch, action)\n        }\n        \"\"\"\n        pitch_res = MlbamUtil.get_attribute_stats(pitch, 'type', str, MlbamConst.UNKNOWN_FULL)\n        pitch_seq = [pitch['pitch_res'] for pitch in pitch_list]\n        pitch_seq.extend([pitch_res])\n        pitch_type = MlbamUtil.get_attribute_stats(pitch, 'pitch_type', str, MlbamConst.UNKNOWN_SHORT)\n        pitch_type_seq = [pitch['pitch_type'] for pitch in pitch_list]\n        pitch_type_seq.extend([pitch_type])\n        pitching = OrderedDict()\n        pitching['retro_game_id'] = pa['retro_game_id']\n        pitching['year'] = pa['year']\n        pitching['month'] = pa['month']\n        pitching['day'] = pa['day']\n        pitching['st_fl'] = pa['st_fl']\n        pitching['regseason_fl'] = pa['regseason_fl']\n        pitching['playoff_fl'] = pa['playoff_fl']\n        pitching['game_type'] = pa['game_type']\n        pitching['game_type_des'] = pa['game_type_des']\n        pitching['local_game_time'] = pa['local_game_time']\n        pitching['game_id'] = pa['game_id']\n        pitching['home_team_id'] = pa['home_team_id']\n        pitching['away_team_id'] = pa['away_team_id']\n        pitching['home_team_lg'] = pa['home_team_lg']\n        pitching['away_team_lg'] = pa['away_team_lg']\n        pitching['interleague_fl'] = pa['interleague_fl']\n        pitching['park_id'] = pa['park_id']\n        pitching['park_name'] = pa['park_name']\n        pitching['park_location'] = pa['park_location']\n        pitching['inning_number'] = pa['inning_number']\n        pitching['bat_home_id'] = pa['bat_home_id']\n        pitching['outs_ct'] = pa['outs_ct']\n        pitching['pit_mlbid'] = pa['pit_mlbid']\n        pitching['pit_first_name'] = pa['pit_first_name']\n        pitching['pit_last_name'] = pa['pit_last_name']\n        pitching['pit_box_name'] = pa['pit_box_name']\n        pitching['pit_hand_cd'] = pa['pit_hand_cd']\n        pitching['bat_mlbid'] = pa['bat_mlbid']\n        pitching['bat_first_name'] = pa['bat_first_name']\n        pitching['bat_last_name'] = pa['bat_last_name']\n        pitching['bat_box_name'] = pa['bat_box_name']\n        pitching['bat_hand_cd'] = pa['bat_hand_cd']\n        pitching['ab_number'] = pa['ab_number']\n        pitching['start_bases'] = pa['start_bases']\n        pitching['end_bases'] = pa['end_bases']\n        pitching['event_outs_ct'] = pa['event_outs_ct']\n        pitching['pa_ball_ct'] = ball_tally\n        pitching['pa_strike_ct'] = strike_tally\n        pitching['pitch_seq'] = ''.join(pitch_seq)\n        pitching['pa_terminal_fl'] = cls.is_pa_terminal(ball_tally, strike_tally, pitch_res, pa['event_cd'])\n        pitching['pa_event_cd'] = pa['event_cd']\n        pitching['pitch_res'] = pitch_res\n        pitching['pitch_des'] = MlbamUtil.get_attribute_stats(pitch, 'des', str, MlbamConst.UNKNOWN_FULL)\n        pitching['pitch_id'] = MlbamUtil.get_attribute_stats(pitch, 'id', int, None)\n        pitching['x'] = MlbamUtil.get_attribute_stats(pitch, 'x', float, None)\n        pitching['y'] = MlbamUtil.get_attribute_stats(pitch, 'y', float, None)\n        pitching['start_speed'] = MlbamUtil.get_attribute_stats(pitch, 'start_speed', float, None)\n        pitching['end_speed'] = MlbamUtil.get_attribute_stats(pitch, 'end_speed', float, None)\n        pitching['sz_top'] = MlbamUtil.get_attribute_stats(pitch, 'sz_top', float, None)\n        pitching['sz_bot'] = MlbamUtil.get_attribute_stats(pitch, 'sz_bot', float, None)\n        pitching['pfx_x'] = MlbamUtil.get_attribute_stats(pitch, 'pfx_x', float, None)\n        pitching['pfx_z'] = MlbamUtil.get_attribute_stats(pitch, 'pfx_z', float, None)\n        pitching['px'] = MlbamUtil.get_attribute_stats(pitch, 'px', float, None)\n        pitching['pz'] = MlbamUtil.get_attribute_stats(pitch, 'pz', float, None)\n        pitching['x0'] = MlbamUtil.get_attribute_stats(pitch, 'x0', float, None)\n        pitching['y0'] = MlbamUtil.get_attribute_stats(pitch, 'y0', float, None)\n        pitching['z0'] = MlbamUtil.get_attribute_stats(pitch, 'z0', float, None)\n        pitching['vx0'] = MlbamUtil.get_attribute_stats(pitch, 'vx0', float, None)\n        pitching['vy0'] = MlbamUtil.get_attribute_stats(pitch, 'vy0', float, None)\n        pitching['vz0'] = MlbamUtil.get_attribute_stats(pitch, 'vz0', float, None)\n        pitching['ax'] = MlbamUtil.get_attribute_stats(pitch, 'ax', float, None)\n        pitching['ay'] = MlbamUtil.get_attribute_stats(pitch, 'ay', float, None)\n        pitching['az'] = MlbamUtil.get_attribute_stats(pitch, 'az', float, None)\n        pitching['break_y'] = MlbamUtil.get_attribute_stats(pitch, 'break_y', float, None)\n        pitching['break_angle'] = MlbamUtil.get_attribute_stats(pitch, 'break_angle', float, None)\n        pitching['break_length'] = MlbamUtil.get_attribute_stats(pitch, 'break_length', float, None)\n        pitching['pitch_type'] = pitch_type\n        pitching['pitch_type_seq'] = '|'.join(pitch_type_seq)\n        pitching['type_confidence'] = MlbamUtil.get_attribute_stats(pitch, 'type_confidence', float, None)\n        pitching['zone'] = MlbamUtil.get_attribute_stats(pitch, 'zone', float, None)\n        pitching['spin_dir'] = MlbamUtil.get_attribute_stats(pitch, 'spin_dir', float, None)\n        pitching['spin_rate'] = MlbamUtil.get_attribute_stats(pitch, 'spin_rate', float, None)\n        pitching['sv_id'] = MlbamUtil.get_attribute_stats(pitch, 'sv_id', str, None)\n        pitching['event_num'] = MlbamUtil.get_attribute_stats(pitch, 'event_num', int, -1)\n        return pitching"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets start and end bases from start and end base names.", "response": "def _get_bases(cls, ab):\n        \"\"\"\n        Start Bases & End Bases\n        :param ab: at bat object(type:Beautifulsoup)\n        :param attribute_name: attribute name\n        :return: start base, end base\n        \"\"\"\n        start_bases, end_bases = [], []\n        for base in ('1B', '2B', '3B'):\n            if ab.find('runner', start=base):\n                start_bases.append(base[0:1])\n            else:\n                start_bases.append('_')\n            if ab.find('runner', end=base):\n                end_bases.append(base[0:1])\n            else:\n                end_bases.append('_')\n        return ''.join(start_bases), ''.join(end_bases)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns dictionary of all the properties of a specific plate appearance.", "response": "def pa(cls, ab, game, rosters, inning_number, inning_id, out_ct, hit_location):\n        \"\"\"\n        plate appearance data\n        :param ab: at bat object(type:Beautifulsoup)\n        :param game: MLBAM Game object\n        :param rosters: Game Rosters\n        :param inning_number: Inning Number\n        :param inning_id: Inning Id(0:home 1:away)\n        :param pitch_list: Pitching\n        :param out_ct: out count\n        :param hit_location: Hitlocation data(dict)\n        :return: {\n            'retro_game_id': Retrosheet Game id\n            'game_type': Game Type(S/R/F/D/L/W)\n            'game_type_des': Game Type Description\n            (Spring Training or Regular Season or Wild-card Game or Divisional Series or LCS or World Series)\n            'st_fl': Spring Training FLAG(T or F)\n            'regseason_fl': Regular Season FLAG(T or F)\n            'playoff_fl': Play Off Flag(T or F)\n            'local_game_time': Game Time(UTC -5)\n            'game_id': Game Id\n            'home_team_id': Home Team Id\n            'home_team_lg': Home Team league(AL or NL)\n            'away_team_id': Away Team Id\n            'away_team_lg': Away Team league(AL or NL)\n            'home_team_name': Home Team Name\n            'away_team_name': Away Team Name\n            'home_team_name_full': Home Team Name(Full Name)\n            'away_team_name_full': Away Team Name(Full Name)\n            'interleague_fl': Inter League Flag(T or F)\n            'park_id': Park Id\n            'park_name': Park Name\n            'park_loc': Park Location\n            'inning_number': Inning Number\n            'bat_home_id': Batter Id\n            'outs_ct': Out count\n            'pit_mlbid': Pitcher Id\n            'pit_first_name': Pitcher First Name\n            'pit_last_name': Pitcher Last Name\n            'pit_box_name': Pitcher Box name\n            'pit_hand_cd': Pitcher Throw Hand(R or L)\n            'bat_first_name': Batter First Name\n            'bat_last_name': Batter Last Name\n            'bat_box_name': Batter Box name\n            'ab_number': At Bat Sequence Number in Game\n            'start_bases': Bases(Before At Bat)\n            (___, 1__, 12_, 123, etc...)\n            'end_bases': Bases(After At Bat)\n            (___, 1__, 12_, 123, etc...)\n            'event_outs_ct': Event Out Count\n            'ab_des': At Bat Description\n            'event_tx': Event Text\n            'event_cd': Event Code for Retrosheet http://www.retrosheet.org/datause.txt\n            'hit_x': Hit Location(x)\n            'hit_y': Hit Location(y)\n            'event_num': Event Sequence Number(atbat, pitch, action)\n            'home_team_runs': Score(Home)\n            'away_team_runs': Score(Away)\n        }\n        \"\"\"\n        ab_des = MlbamUtil.get_attribute_stats(ab, 'des', str, MlbamConst.UNKNOWN_FULL)\n        event_tx = MlbamUtil.get_attribute_stats(ab, 'event', str, MlbamConst.UNKNOWN_FULL)\n        event_cd = RetroSheet.event_cd(event_tx, ab_des)\n        event_outs_ct = MlbamUtil.get_attribute_stats(ab, 'o', int, 0)\n        start_bases, end_bases = cls._get_bases(ab)\n        pit_mlbid = MlbamUtil.get_attribute_stats(ab, 'pitcher', str, MlbamConst.UNKNOWN_FULL)\n        bat_mlbid = MlbamUtil.get_attribute_stats(ab, 'batter', str, MlbamConst.UNKNOWN_FULL)\n        pit_player = rosters.get(pit_mlbid)\n        bat_player = rosters.get(bat_mlbid)\n        location_key = Inning.HITLOCATION_KEY_FORMAT.format(\n            inning=inning_number,\n            des=event_tx,\n            pitcher=pit_mlbid,\n            batter=bat_mlbid,\n        )\n        location = hit_location.get(location_key, {})\n        atbat = OrderedDict()\n        atbat['retro_game_id'] = game.retro_game_id\n        atbat['year'] = game.timestamp.year\n        atbat['month'] = game.timestamp.month\n        atbat['day'] = game.timestamp.day\n        atbat['st_fl'] = game.st_fl\n        atbat['regseason_fl'] = game.regseason_fl\n        atbat['playoff_fl'] = game.playoff_fl\n        atbat['game_type'] = game.game_type\n        atbat['game_type_des'] = game.game_type_des\n        atbat['local_game_time'] = game.local_game_time\n        atbat['game_id'] = game.game_id\n        atbat['home_team_id'] = game.home_team_id\n        atbat['away_team_id'] = game.away_team_id\n        atbat['home_team_lg'] = game.home_team_lg\n        atbat['away_team_lg'] = game.away_team_lg\n        atbat['interleague_fl'] = game.interleague_fl\n        atbat['park_id'] = game.park_id\n        atbat['park_name'] = game.park_name\n        atbat['park_location'] = game.park_loc\n        atbat['inning_number'] = inning_number\n        atbat['bat_home_id'] = inning_id\n        atbat['outs_ct'] = out_ct\n        atbat['pit_mlbid'] = pit_mlbid\n        atbat['pit_first_name'] = pit_player.first\n        atbat['pit_last_name'] = pit_player.last\n        atbat['pit_box_name'] = pit_player.box_name\n        atbat['pit_hand_cd'] = MlbamUtil.get_attribute_stats(ab, 'p_throws', str, MlbamConst.UNKNOWN_FULL)\n        atbat['bat_mlbid'] = bat_mlbid\n        atbat['bat_first_name'] = bat_player.first\n        atbat['bat_last_name'] = bat_player.last\n        atbat['bat_box_name'] = bat_player.box_name\n        atbat['bat_hand_cd'] = MlbamUtil.get_attribute_stats(ab, 'stand', str, MlbamConst.UNKNOWN_FULL)\n        atbat['ab_number'] = MlbamUtil.get_attribute_stats(ab, 'num', int, None)\n        atbat['start_bases'] = start_bases\n        atbat['end_bases'] = end_bases\n        atbat['event_outs_ct'] = event_outs_ct\n        atbat['ab_des'] = ab_des\n        atbat['event_tx'] = event_tx\n        atbat['event_cd'] = event_cd\n        atbat['hit_x'] = location.get('hit_x', None)\n        atbat['hit_y'] = location.get('hit_y', None)\n        atbat['event_num'] = MlbamUtil.get_attribute_stats(ab, 'event_num', int, -1)\n        atbat['home_team_runs'] = MlbamUtil.get_attribute_stats(ab, 'home_team_runs', int, 0)\n        atbat['away_team_runs'] = MlbamUtil.get_attribute_stats(ab, 'away_team_runs', int, 0)\n        return atbat"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the result of a plate appearance at a given time.", "response": "def result(cls, ab, pa, pitch_list):\n        \"\"\"\n        At Bat Result\n        :param ab: at bat object(type:Beautifulsoup)\n        :param pa: atbat data for plate appearance\n        :param pitch_list: Pitching data\n        :return: pa result value(dict)\n        \"\"\"\n        atbat = OrderedDict()\n        atbat['ball_ct'] = MlbamUtil.get_attribute_stats(ab, 'b', int, None)\n        atbat['strike_ct'] = MlbamUtil.get_attribute_stats(ab, 's', int, None)\n        atbat['pitch_seq'] = ''.join([pitch['pitch_res'] for pitch in pitch_list])\n        atbat['pitch_type_seq'] = '|'.join([pitch['pitch_type'] for pitch in pitch_list])\n        atbat['battedball_cd'] = RetroSheet.battedball_cd(pa['event_cd'], pa['event_tx'], pa['ab_des'])\n        return atbat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of the data for an action.", "response": "def action(cls, action, game, rosters, inning_number, inning_id):\n        \"\"\"\n        action data\n        :param action: action object(type:Beautifulsoup)\n        :param game: MLBAM Game object\n        :param rosters: Game Rosters\n        :param inning_number: Inning Number\n        :param inning_id: Inning Id(0:home 1:away)\n        :return: {\n            'retro_game_id': Retrosheet Game id\n            'game_type': Game Type(S/R/F/D/L/W)\n            'game_type_des': Game Type Description\n            (Spring Training or Regular Season or Wild-card Game or Divisional Series or LCS or World Series)\n            'st_fl': Spring Training FLAG(T or F)\n            'regseason_fl': Regular Season FLAG(T or F)\n            'playoff_fl': Play Off Flag(T or F)\n            'local_game_time': Game Time(UTC -5)\n            'game_id': Game Id\n            'home_team_id': Home Team Id\n            'home_team_lg': Home Team league(AL or NL)\n            'away_team_id': Away Team Id\n            'away_team_lg': Away Team league(AL or NL)\n            'home_team_name': Home Team Name\n            'away_team_name': Away Team Name\n            'home_team_name_full': Home Team Name(Full Name)\n            'away_team_name_full': Away Team Name(Full Name)\n            'interleague_fl': Inter League Flag(T or F)\n            'park_id': Park Id\n            'park_name': Park Name\n            'park_loc': Park Location\n            'b': Ball Count\n            's': Strike count\n            'o': Out count\n            'des': Description\n            'event': Event Action\n            'player_mlbid': Player Id\n            'player_first_name': Player First Name\n            'player_last_name': Player Last Name\n            'player_box_name': Player Box Name\n            'pitch': Pitch count\n            'event_num': Event Sequence Number(atbat, pitch, action)\n            'home_team_runs': Score(Home)\n            'away_team_runs': Score(Away)\n        }\n        \"\"\"\n        player_mlbid = MlbamUtil.get_attribute_stats(action, 'player', str, MlbamConst.UNKNOWN_FULL)\n        player = rosters.get(player_mlbid)\n        act = OrderedDict()\n        act['retro_game_id'] = game.retro_game_id\n        act['year'] = game.timestamp.year\n        act['month'] = game.timestamp.month\n        act['day'] = game.timestamp.day\n        act['st_fl'] = game.st_fl\n        act['regseason_fl'] = game.regseason_fl\n        act['playoff_fl'] = game.playoff_fl\n        act['game_type'] = game.game_type\n        act['game_type_des'] = game.game_type_des\n        act['local_game_time'] = game.local_game_time\n        act['game_id'] = game.game_id\n        act['home_team_id'] = game.home_team_id\n        act['away_team_id'] = game.away_team_id\n        act['home_team_lg'] = game.home_team_lg\n        act['away_team_lg'] = game.away_team_lg\n        act['interleague_fl'] = game.interleague_fl\n        act['park_id'] = game.park_id\n        act['park_name'] = game.park_name\n        act['inning_number'] = inning_number\n        act['home_id'] = inning_id\n        act['park_location'] = game.park_loc\n        act['b'] = MlbamUtil.get_attribute_stats(action, 'b', int, 0)\n        act['s'] = MlbamUtil.get_attribute_stats(action, 's', int, 0)\n        act['o'] = MlbamUtil.get_attribute_stats(action, 'o', int, 0)\n        act['des'] = MlbamUtil.get_attribute_stats(action, 'des', str, MlbamConst.UNKNOWN_FULL)\n        act['event'] = MlbamUtil.get_attribute_stats(action, 'event', str, MlbamConst.UNKNOWN_FULL)\n        act['player_mlbid'] = player_mlbid\n        try:\n            act['player_first_name'] = player.first\n            act['player_last_name'] = player.last\n            act['player_box_name'] = player.box_name\n        except AttributeError as e:\n            logging.error('Attribute Error(retro_game_id:{retro_game_id} player_mlbid:{player_mlbid})'\n                          .format(**{'retro_game_id': game.retro_game_id, 'player_mlbid': player_mlbid}))\n            act['player_first_name'] = MlbamConst.UNKNOWN_FULL\n            act['player_last_name'] = MlbamConst.UNKNOWN_FULL\n            act['player_box_name'] = MlbamConst.UNKNOWN_FULL\n        act['pitch'] = MlbamUtil.get_attribute_stats(action, 'pitch', int, 0)\n        act['event_num'] = MlbamUtil.get_attribute_stats(action, 'event_num', int, -1)\n        act['home_team_runs'] = MlbamUtil.get_attribute_stats(action, 'home_team_runs', int, 0)\n        act['away_team_runs'] = MlbamUtil.get_attribute_stats(action, 'away_team_runs', int, 0)\n        return act"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread xml object :param url: contents url :param markup: markup provider :param game: MLBAM Game object :param players: MLBAM Players object :return: pitchpx.game.game.Game object", "response": "def read_xml(cls, url, markup, game, players):\n        \"\"\"\n        read xml object\n        :param url: contents url\n        :param markup: markup provider\n        :param game: MLBAM Game object\n        :param players: MLBAM Players object\n        :return: pitchpx.game.game.Game object\n        \"\"\"\n        innings = Inning(game, players)\n        base_url = \"\".join([url, cls.DIRECTORY])\n        # hit location data\n        hit_location = cls._read_hit_chart_data(\n                MlbamUtil.find_xml('/'.join([base_url, cls.FILENAME_INNING_HIT]), markup)\n        )\n\n        # create for atbat & pitch data\n        for inning in MlbamUtil.find_xml_all(base_url, markup, cls.TAG, cls.FILENAME_PATTERN):\n            soup = MlbamUtil.find_xml(\"/\".join([base_url, inning.get_text().strip()]), markup)\n            inning_number = int(soup.inning['num'])\n            for inning_type in cls.INNINGS.keys():\n                inning_soup = soup.inning.find(inning_type)\n                if inning_soup is None:\n                    break\n                innings._inning_events(inning_soup, inning_number, cls.INNINGS[inning_type], hit_location)\n                innings._inning_actions(inning_soup, inning_number, cls.INNINGS[inning_type])\n        return innings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _inning_actions(self, soup, inning_number, inning_id):\n        # at bat(batter box data) & pitching data\n        for act in soup.find_all('action'):\n            self.actions.append(InningAction.action(act, self.game, self.players.rosters, inning_number, inning_id))", "response": "Parse the inning actions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninning Events. :param soup: Beautifulsoup object :param inning_number: Inning Number :param inning_id: Inning Id(0:home, 1:away) :param hit_location: Hitlocation data(dict)", "response": "def _inning_events(self, soup, inning_number, inning_id, hit_location):\n        \"\"\"\n        Inning Events.\n        :param soup: Beautifulsoup object\n        :param inning_number: Inning Number\n        :param inning_id: Inning Id(0:home, 1:away)\n        :param hit_location: Hitlocation data(dict)\n        \"\"\"\n        # at bat(batter box data) & pitching data\n        out_ct = 0\n        for ab in soup.find_all('atbat'):\n            # plate appearance data(pa)\n            at_bat = AtBat.pa(ab, self.game, self.players.rosters, inning_number, inning_id, out_ct, hit_location)\n            # pitching data\n            pitching_stats = self._get_pitch(ab, at_bat)\n            # at bat(pa result)\n            pa_result = AtBat.result(ab, at_bat, pitching_stats)\n            at_bat.update(pa_result)\n            self.atbats.append(at_bat)\n            self.pitches.extend(pitching_stats)\n            # out count\n            out_ct = at_bat['event_outs_ct']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting pitch data for a given plate appearance", "response": "def _get_pitch(self, soup, pa):\n        \"\"\"\n        get pitch data\n        :param soup: Beautifulsoup object\n        :param pa: atbat data for plate appearance\n        :return: pitches result(list)\n        \"\"\"\n        pitches = []\n        ball_tally, strike_tally = 0, 0\n        for pitch in soup.find_all('pitch'):\n            # pitching result\n            pitch = Pitch.row(pitch, pa, pitches, ball_tally, strike_tally)\n            pitches.append(pitch)\n            # ball count\n            ball_tally, strike_tally = RetroSheet.ball_count(ball_tally, strike_tally, pitch['pitch_res'])\n        return pitches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an individual to the adapter", "response": "def _add_individual(self, ind_obj):\n        \"\"\"Add a individual to the adapter\n        \n            Args:\n                ind_obj (puzzle.models.Individual)\n        \"\"\"\n        logger.debug(\"Adding individual {0} to plugin\".format(ind_obj.ind_id))\n        self.individual_objs.append(ind_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a case to the adapter", "response": "def add_case(self, case_obj):\n        \"\"\"Add a case obj with individuals to adapter\n        \n            Args:\n                case_obj (puzzle.models.Case)\n                \n        \"\"\"\n        for ind_obj in case_obj.individuals:\n            self._add_individual(ind_obj)\n        logger.debug(\"Adding case {0} to plugin\".format(case_obj.case_id))\n        self.case_objs.append(case_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef case(self, case_id=None):\n        cases = self.cases()\n        if case_id:\n            for case in cases:\n                if case.case_id == case_id:\n                    return case\n        else:\n            if cases:\n                return cases[0]\n\n        return None", "response": "Return a Case object for the given case_id. If no case_id is given return one case."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef individual(self, ind_id=None):\n        for ind_obj in self.individuals:\n            if ind_obj.ind_id == ind_id:\n                return ind_obj\n        return None", "response": "Return a individual object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chebyshev(point1, point2):\n\n    return max(abs(point1[0] - point2[0]), abs(point1[1] - point2[1]))", "response": "Computes distance between two points using chebyshev metric"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hilbertrot(n, x, y, rx, ry):\n    if ry == 0:\n        if rx == 1:\n            x = n - 1 - x\n            y = n - 1 - y\n        return y, x\n    return x, y", "response": "Rotates and flips a quadrant appropriately for the Hilbert scan\n    generator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscan pixels in a circle pattern around a center point x0 y0 and r1 and r2.", "response": "def circlescan(x0, y0, r1, r2):\n    \"\"\"Scan pixels in a circle pattern around a center point\n\n    :param x0: Center x-coordinate\n    :type x0: float\n    :param y0: Center y-coordinate\n    :type y0: float\n    :param r1: Initial radius\n    :type r1: float\n    :param r2: Final radius\n    :type r2: float\n    :returns: Coordinate generator\n    :rtype: function\n    \"\"\"\n\n    # Validate inputs\n    if r1 < 0: raise ValueError(\"Initial radius must be non-negative\")\n    if r2 < 0: raise ValueError(\"Final radius must be non-negative\")\n\n    # List of pixels visited in previous diameter\n    previous = []\n    \n    # Scan distances outward (1) or inward (-1)\n    rstep = 1 if r2 >= r1 else -1\n    for distance in range(r1, r2 + rstep, rstep):\n\n        if distance == 0:\n\n            yield x0, y0\n\n        else:\n\n            # Computes points for first octant and the rotate by multiples of\n            # 45 degrees to compute the other octants\n            a = 0.707107\n            rotations = {0: [[ 1, 0], [ 0, 1]],\n                         1: [[ a, a], [-a, a]],\n                         2: [[ 0, 1], [-1, 0]],\n                         3: [[-a, a], [-a,-a]],\n                         4: [[-1, 0], [ 0,-1]],\n                         5: [[-a,-a], [ a,-a]],\n                         6: [[ 0,-1], [ 1, 0]],\n                         7: [[ a,-a], [ a, a]]}\n            nangles = len(rotations)\n\n            # List of pixels visited in current diameter\n            current = []\n\n            for angle in range(nangles):\n                x = 0\n                y = distance\n                d = 1 - distance\n                while x < y:\n                    xr = rotations[angle][0][0]*x + rotations[angle][0][1]*y\n                    yr = rotations[angle][1][0]*x + rotations[angle][1][1]*y\n                    xr = x0 + xr\n                    yr = y0 + yr\n                    \n                    # First check  if point was in previous diameter\n                    # since our scan pattern can lead to duplicates in\n                    # neighboring diameters\n                    point = (int(round(xr)), int(round(yr)))\n                    if point not in previous:\n                        yield xr, yr\n                        current.append(point)\n\n                    # Move pixel according to circle constraint\n                    if (d < 0):\n                        d += 3 + 2 * x\n                    else:\n                        d += 5 - 2 * (y-x)\n                        y -= 1\n                    x += 1\n\n            previous = current"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscanning pixels in a grid pattern along the x - coordinate then y - coordinate", "response": "def gridscan(xi, yi, xf, yf, stepx=1, stepy=1):\n    \"\"\"Scan pixels in a grid pattern along the x-coordinate then y-coordinate\n\n    :param xi: Initial x-coordinate\n    :type xi: int\n    :param yi: Initial y-coordinate\n    :type yi: int\n    :param xf: Final x-coordinate\n    :type xf: int\n    :param yf: Final y-coordinate\n    :type yf: int\n    :param stepx: Step size in x-coordinate\n    :type stepx: int\n    :param stepy: Step size in y-coordinate\n    :type stepy: int\n    :returns: Coordinate generator\n    :rtype: function\n    \"\"\"\n\n    if stepx <= 0: raise ValueError(\"X-step must be positive\")\n    if stepy <= 0: raise ValueError(\"Y-step must be positive\")\n\n    # Determine direction to move\n    dx = stepx if xf >= xi else -stepx\n    dy = stepy if yf >= yi else -stepy\n\n    for y in range(yi, yf + dy, dy):\n        for x in range(xi, xf + dx, dx):\n            yield x, y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hilbertscan(size, distance):\n\n    size = 2*(1<<(size-1).bit_length());\n    if (distance > size**2 - 1): raise StopIteration(\"Invalid distance!\")\n\n    for d in range(distance):\n        t = d\n        x = 0\n        y = 0\n        s = 1\n        while (s < size):\n            rx = 1 & (t / 2)\n            ry = 1 & (t ^ rx)\n            x, y = hilbertrot(s, x, y, rx, ry)\n            x += s * rx\n            y += s * ry\n            t /= 4\n            s *= 2\n        yield x, y", "response": "Scan pixels in a Hilbert curve pattern in the first quadrant."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ringscan(x0, y0, r1, r2, metric=chebyshev):\n\n    # Validate inputs\n    if r1 < 0: raise ValueError(\"Initial radius must be non-negative\")\n    if r2 < 0: raise ValueError(\"Final radius must be non-negative\")\n    if not hasattr(metric, \"__call__\"): raise TypeError(\"Metric not callable\")\n\n    # Define clockwise step directions\n    direction = 0\n    steps = {0: [ 1, 0],\n             1: [ 1,-1],\n             2: [ 0,-1],\n             3: [-1,-1],\n             4: [-1, 0],\n             5: [-1, 1],\n             6: [ 0, 1],\n             7: [ 1, 1]}\n    nsteps = len(steps)\n\n    center = [x0, y0]\n    \n    # Scan distances outward (1) or inward (-1)\n    rstep = 1 if r2 >= r1 else -1\n    for distance in range(r1, r2 + rstep, rstep):\n\n        initial = [x0, y0 + distance]\n        current = initial\n\n        # Number of tries to find a valid neighrbor\n        ntrys = 0\n\n        while True:\n\n            # Short-circuit special case\n            if distance == 0:\n                yield current[0], current[1]\n                break\n\n            # Try and take a step and check if still within distance\n            nextpoint = [current[i] + steps[direction][i] for i in range(2)]\n            if metric(center, nextpoint) != distance:\n\n                # Check if we tried all step directions and failed\n                ntrys += 1\n                if ntrys == nsteps:\n                    break\n                \n                # Try the next direction\n                direction = (direction + 1) % nsteps\n                continue\n\n            ntrys = 0\n            yield current[0], current[1]\n\n            # Check if we have come all the way around\n            current = nextpoint\n            if current == initial:\n                break\n\n        # Check if we tried all step directions and failed\n        if ntrys == nsteps:\n            break", "response": "Scan pixels in a ring pattern around a center point clockwise and yield a set of coordinates."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscans the pixels in a snake pattern along the x - coordinate then y - coordinate.", "response": "def snakescan(xi, yi, xf, yf):\n    \"\"\"Scan pixels in a snake pattern along the x-coordinate then y-coordinate\n\n    :param xi: Initial x-coordinate\n    :type xi: int\n    :param yi: Initial y-coordinate\n    :type yi: int\n    :param xf: Final x-coordinate\n    :type xf: int\n    :param yf: Final y-coordinate\n    :type yf: int\n    :returns: Coordinate generator\n    :rtype: function\n    \"\"\"\n\n    # Determine direction to move\n    dx = 1 if xf >= xi else -1\n    dy = 1 if yf >= yi else -1\n\n    # Scan pixels first along x-coordinate then y-coordinate and flip\n    # x-direction when the end of the line is reached\n    x, xa, xb = xi, xi, xf\n    for y in range(yi, yf + dy, dy):\n        for x in range(xa, xb + dx, dx):\n            yield x, y\n\n        # Swap x-direction\n        if x == xa or x == xb:\n            dx *= -1\n            xa, xb = xb, xa"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef walkscan(x0, y0, xn=0.25, xp=0.25, yn=0.25, yp=0.25):\n\n    # Validate inputs\n    if xn < 0: raise ValueError(\"Negative x probabilty must be non-negative\")\n    if xp < 0: raise ValueError(\"Positive x probabilty must be non-negative\")\n    if yn < 0: raise ValueError(\"Negative y probabilty must be non-negative\")\n    if yp < 0: raise ValueError(\"Positive y probabilty must be non-negative\")\n\n    # Compute normalized probability\n    total = xp + xn + yp + yn\n    xn /= total\n    xp /= total\n    yn /= total\n    yp /= total\n\n    # Compute cumulative probability\n    cxn = xn\n    cxp = cxn + xp\n    cyn = cxp + yn\n\n    # Initialize position\n    x, y = x0, y0\n\n    while True:\n\n        yield x, y\n\n        # Take random step\n        probability = random.random()\n        if probability <= cxn:\n            x -= 1\n        elif probability <= cxp:\n            x += 1\n        elif probability <= cyn:\n            y -= 1\n        else:\n            y += 1", "response": "Scan pixels in a random walk pattern with given step probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef next(self):\n        while True:\n            x, y = next(self.scan)\n            if self.predicate is not None and not self.predicate(x,y):\n                if self.abort: raise StopIteration(\"Boundary crossed!\")\n            elif (x < self.minx or\n                x > self.maxx or\n                y < self.miny or\n                y > self.maxy):\n                if self.abort: raise StopIteration(\"Boundary crossed!\")\n            else:\n                return x, y", "response": "Return the next point in iteration\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next(self):\n        x, y = next(self.scan)\n        xr = -x if self.rx else x\n        yr = -y if self.ry else y\n        return xr, yr", "response": "Next point in iteration\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the next entry in the reservoir.", "response": "def next(self):\n        \"\"\"Next point in iteration\n        \"\"\"\n        if self.count < len(self.reservoir):\n            self.count += 1\n            return self.reservoir[self.count-1]\n\n        raise StopIteration(\"Reservoir exhausted\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn x y coordinates of the next iteration of the set of entries.", "response": "def next(self):\n        \"\"\"Next point in iteration\n        \"\"\"\n        x, y = next(self.scan)\n        xr = self.sx * x\n        yr = self.sy * y\n        return xr, yr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn next point in iteration", "response": "def next(self):\n        \"\"\"Next point in iteration\n        \"\"\"\n        while True:\n            x, y = next(self.scan)\n            self.index += 1\n            if (self.index < self.start): continue\n            if (self.index > self.stop): raise StopIteration(\"skip stopping\")\n            if ((self.index-self.start) % self.step != 0): continue\n            return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next(self):\n        x, y = next(self.scan)\n        xs = int(round(x))\n        ys = int(round(y))\n        return xs, ys", "response": "Next point in iteration\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next(self):\n        x, y = next(self.scan)\n        xr = x + self.tx\n        yr = y + self.ty\n        return xr, yr", "response": "Next point in iteration\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates whether a configuration is valid.", "response": "def validate(self):\n        \"\"\"Validates whether a configuration is valid.\n\n        :rtype: bool\n        :raises ConfigurationError: if no ``access_token`` provided.\n        :raises ConfigurationError: if provided ``access_token`` is invalid - contains disallowed characters.\n        :raises ConfigurationError: if provided ``access_token`` is invalid - has invalid length.\n        :raises ConfigurationError: if provided ``base_url`` is invalid.\n        \"\"\"\n        if self.access_token is None:\n            raise ConfigurationError('No access token provided. '\n                                     'Set your access token during client initialization using: '\n                                     '\"basecrm.Client(access_token= <YOUR_PERSONAL_ACCESS_TOKEN>)\"')\n\n        if re.search(r'\\s', self.access_token):\n            raise ConfigurationError('Provided access token is invalid '\n                                     'as it contains disallowed characters. '\n                                     'Please double-check you access token.')\n\n        if len(self.access_token) != 64:\n            raise ConfigurationError('Provided access token is invalid '\n                                     'as it has invalid length. '\n                                     'Please double-check your access token.')\n\n        if not self.base_url or not re.match(self.URL_REGEXP, self.base_url):\n            raise ConfigurationError('Provided base url is invalid '\n                                     'as it not a valid URI. '\n                                     'Please make sure it incldues the schema part, '\n                                     'both http and https are accepted, '\n                                     'and the hierarchical part')\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching fresh data from the named queue.", "response": "def fetch(self, device_uuid, session_id):\n        \"\"\"\n        Get data from queue\n\n        Fetch fresh data from the named queue.\n        Using session identifier you call continously the `#fetch` method to drain the named queue.\n\n        :calls: ``get /sync/{session_id}/queues/main``\n        :param string device_uuid: Device's UUID for which to perform synchronization.\n        :param string session_id: Unique identifier of a synchronization session.\n        :param string queue: (optional) Queue name.\n        :return: List of dictionaries that support attribute-style access,\n            which represent resources (data) and associated meta data (meta).\n            Empty list if there is no more data to synchronize.\n        :rtype: list\n        \"\"\"\n\n        status_code, _, root = self.http_client.get(\"/sync/{session_id}/queues/main\".format(session_id=session_id),\n                                                    params=None,\n                                                    headers=self.build_headers(device_uuid),\n                                                    raw=True)\n\n        return [] if status_code == 204 else root['items']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nacknowledges received data for a specific device_uuid", "response": "def ack(self, device_uuid, ack_keys):\n        \"\"\"\n        Acknowledge received data\n\n        Send acknowledgement keys to let know the Sync service which data you have.\n        As you fetch new data, you need to send acknowledgement keys.\n\n        :calls: ``post /sync/ack``\n        :param string device_uuid: Device's UUID for which to perform synchronization.\n        :param list ack_keys: List of acknowledgement keys.\n        :return: True if the operation succeeded.\n        :rtype: bool\n        \"\"\"\n\n        attributes = {'ack_keys': ack_keys}\n        status_code, _, _ = self.http_client.post('/sync/ack',\n                                                  body=attributes,\n                                                  headers=self.build_headers(device_uuid))\n\n        return status_code == 202"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming a full synchronization flow for a given device s UUID.", "response": "def fetch(self, callback):\n        \"\"\"\n        Perform a full synchronization flow.\n\n        .. code-block:: python\n            :linenos:\n\n            >>> client = basecrm.Client(access_token='<YOUR_PERSONAL_ACCESS_TOKEN>')\n            >>> sync = basecrm.Sync(client=client, device_uuid='<YOUR_DEVICES_UUID>')\n            >>> sync.fetch(lambda meta, data: basecrm.Sync.ACK)\n\n        :param callback: Callback that will be called for every item in a queue.\n            Takes two input arguments: synchronization meta data and assodicated data.\n            It must return either ack or nack.\n        \"\"\"\n\n        # Set up a new synchronization session for a given device's UUID\n        session = self.client.sync.start(self.device_uuid)\n\n        # Check if there is anything to synchronize\n        if session is None or 'id' not in session:\n            return\n\n        # Drain the main queue until there is no more data (empty array)\n        while True:\n            # Fetch the main queue\n            queue_items = self.client.sync.fetch(self.device_uuid, session['id'])\n\n            # nothing more to synchronize ?\n            if not queue_items:\n                break\n\n            # let client know about both data and meta\n            ack_keys = []\n\n            for item in queue_items:\n                if callback(item['meta'], item['data']):\n                    ack_keys.append(item['meta']['sync']['ack_key'])\n\n            # As we fetch new data, we need to send acknowledgement keys\n            # if any ..\n            if ack_keys:\n                self.client.sync.ack(self.device_uuid, ack_keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef weighted_mean(data, weights=None):\n    if weights is None:\n        return mean(data)\n    total_weight = float(sum(weights))\n    weights = [weight / total_weight for weight in weights]\n    w_mean = 0\n    for i, weight in enumerate(weights):\n        w_mean += weight * data[i]\n    return w_mean", "response": "Calculate the weighted mean of a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef numpy_weighted_mean(data, weights=None):\n    import numpy as np\n    weights = np.array(weights).flatten() / float(sum(weights))\n    return np.dot(np.array(data), weights)", "response": "Calculate the weighted mean of an array using numpy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef median(data):\n    data.sort()\n    num_values = len(data)\n    half = num_values // 2\n    if num_values % 2:\n        return data[half]\n    return 0.5 * (data[half-1] + data[half])", "response": "Calculate the median of a list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the weighted median of a list.", "response": "def weighted_median(data, weights=None):\n    \"\"\"Calculate the weighted median of a list.\"\"\"\n    if weights is None:\n        return median(data)\n    midpoint = 0.5 * sum(weights)\n    if any([j > midpoint for j in weights]):\n        return data[weights.index(max(weights))]\n    if any([j > 0 for j in weights]):\n        sorted_data, sorted_weights = zip(*sorted(zip(data, weights)))\n        cumulative_weight = 0\n        below_midpoint_index = 0\n        while cumulative_weight <= midpoint:\n            below_midpoint_index += 1\n            cumulative_weight += sorted_weights[below_midpoint_index-1]\n        cumulative_weight -= sorted_weights[below_midpoint_index-1]\n        if cumulative_weight - midpoint < sys.float_info.epsilon:\n            bounds = sorted_data[below_midpoint_index-2:below_midpoint_index]\n            return sum(bounds) / float(len(bounds))\n        return sorted_data[below_midpoint_index-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef numpy_weighted_median(data, weights=None):\n    import numpy as np\n    if weights is None:\n        return np.median(np.array(data).flatten())\n    data, weights = np.array(data).flatten(), np.array(weights).flatten()\n    if any(weights > 0):\n        sorted_data, sorted_weights = map(np.array, zip(*sorted(zip(data, weights))))\n        midpoint = 0.5 * sum(sorted_weights)\n        if any(weights > midpoint):\n            return (data[weights == np.max(weights)])[0]\n        cumulative_weight = np.cumsum(sorted_weights)\n        below_midpoint_index = np.where(cumulative_weight <= midpoint)[0][-1]\n        if cumulative_weight[below_midpoint_index] - midpoint < sys.float_info.epsilon:\n            return np.mean(sorted_data[below_midpoint_index:below_midpoint_index+2])\n        return sorted_data[below_midpoint_index+1]", "response": "Calculate the weighted median of an array or list using numpy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_url(url):\n    # Parse URL, make sure string is valid.\n    try:\n        split = urlsplit(url.rstrip('/'))\n    except (AttributeError, TypeError) as e:\n        raise ValueError('Malformed URL specified: {0}'.format(e))\n    if split.scheme not in ['redis+socket', 'redis', 'file']:\n        raise ValueError('Malformed URL specified.')\n    scheme = split.scheme\n    netloc = split.netloc\n    hostname = split.hostname\n    path = split.path\n    password = split.password\n    try:\n        port = split.port\n    except ValueError:\n        port = None  # Stupid urlsplit bug on Windows.\n\n    # urlsplit sucks on Windows, work around this.\n    if os.name == 'nt' and not path and '\\\\' in netloc:\n        if '@' in netloc:\n            position = netloc.find('@') + 1\n            path = netloc[position:]\n            netloc = netloc[:position]\n        else:\n            path = netloc\n            netloc = ''\n\n    # Handle non-socket URLs.\n    if scheme == 'redis' and netloc and not netloc.endswith('.') and not netloc.endswith('@'):\n        result = dict(host=hostname)\n        if password:\n            result['password'] = password\n        if port:\n            result['port'] = port\n        if path:\n            if not path[1:].isdigit():\n                raise ValueError('Network URL path has non-digit characters: {0}'.format(path[1:]))\n            result['db'] = int(path[1:])\n        return result\n\n    # Handle socket URLs.\n    if port:\n        raise ValueError('Socket URL looks like non-socket URL.')\n    if not password:\n        socket_path = '{0}{1}'.format(netloc, path)\n    elif netloc.endswith('.'):\n        socket_path = '{0}{1}'.format(netloc.split('@')[1], path)\n    elif not path:\n        socket_path = netloc.split('@')[1]\n    else:\n        socket_path = path\n\n    # Catch bad paths.\n    parent_dir = os.path.split(socket_path)[0]\n    if parent_dir and not os.path.isdir(parent_dir):\n        raise ValueError(\"Unix socket path's parent not a dir: {0}\".format(parent_dir))\n\n    # Finish up.\n    result = dict(unix_socket_path=socket_path)\n    if password:\n        result['password'] = password\n    return result", "response": "Parses a Redis URL and returns a dictionary with the parsed and split data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary compatible with StrictRedis.", "response": "def read_config(config, prefix):\n    \"\"\"Return a StrictRedis.__init__() compatible dictionary from data in the Flask config.\n\n    Generate a dictionary compatible with StrictRedis.__init__() keyword arguments from data in the Flask\n    application's configuration values relevant to Redis.\n\n    This is where REDIS_URL (or whatever prefix used) is parsed, by calling parse_url().\n\n    Positional arguments:\n    config -- Flask application config dict.\n    prefix -- Prefix used in config key names in the Flask app's configuration.\n\n    Returns:\n    Dictionary with parsed data, compatible with StrictRedis.__init__() keyword arguments.\n    \"\"\"\n    # Get all relevant config values from Flask application.\n    suffixes = ('URL', 'SOCKET', 'HOST', 'PORT', 'PASSWORD', 'DB')\n    config_url, config_socket, config_host, config_port, config_password, config_db = [\n        config.get('{0}_{1}'.format(prefix, suffix)) for suffix in suffixes\n    ]\n    result = dict()\n    # Get more values from URL if provided.\n    if config_url:\n        result.update(parse_url(config_url))\n    # Apply other config values.\n    if config_socket:\n        result['unix_socket_path'] = config_socket\n    else:\n        if config_host:\n            result['host'] = config_host\n        if config_port is not None:\n            result['port'] = int(config_port)\n    if config_password is not None:\n        result['password'] = config_password\n    if config_db is not None:\n        result['db'] = int(config_db)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the StrictRedis instance with the settings from the Flask application configuration and initializes the StrictRedis instance.", "response": "def init_app(self, app, config_prefix=None):\n        \"\"\"Actual method to read Redis settings from app configuration and initialize the StrictRedis instance.\n\n        Positional arguments:\n        app -- Flask application instance.\n        config_prefix -- Prefix used in config key names in the Flask app's configuration. Useful for applications which\n            interface with more than one Redis server. Default value is 'REDIS'. Will be converted to upper case (e.g.\n            'REDIS_CACHE').\n            Examples:\n              REDIS_URL = 'redis://localhost/0'\n              REDIS_CACHE_URL = 'redis://localhost/1'\n        \"\"\"\n        # Normalize the prefix and add this instance to app.extensions.\n        config_prefix = (config_prefix or 'REDIS').rstrip('_').upper()\n        if not hasattr(app, 'extensions'):\n            app.extensions = dict()\n        if config_prefix.lower() in app.extensions:\n            raise ValueError('Already registered config prefix {0!r}.'.format(config_prefix))\n        app.extensions[config_prefix.lower()] = _RedisState(self, app)\n\n        # Read config.\n        args = read_config(app.config, config_prefix)\n\n        # Instantiate StrictRedis.\n        super(Redis, self).__init__(**args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the real path of the path.", "response": "def _realpath(fs, path, seen=pset()):\n    \"\"\"\n    .. warning::\n\n        The ``os.path`` module's realpath does not error or warn about\n        loops, but we do, following the behavior of GNU ``realpath(1)``!\n    \"\"\"\n\n    real = Path.root()\n    for segment in path.segments:\n        current = real / segment\n        seen = seen.add(current)\n        while True:\n            try:\n                current = fs.readlink(current)\n            except (exceptions.FileNotFound, exceptions.NotASymlink):\n                break\n            else:\n                current = current.relative_to(real)\n                if current in seen:\n                    raise exceptions.SymbolicLoop(path)\n                current = fs.realpath(current, seen=seen)\n        real = current\n    return real"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _recursive_remove(fs, path):\n    if not fs.is_link(path=path) and fs.is_dir(path=path):\n        for child in fs.children(path=path):\n            _recursive_remove(fs=fs, path=child)\n        fs.remove_empty_directory(path=path)\n    else:\n        fs.remove_file(path=path)", "response": "A recursive directory removal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(\n    name,\n\n    create_file,\n    open_file,\n    remove_file,\n\n    create_directory,\n    list_directory,\n    remove_empty_directory,\n    temporary_directory,\n\n    stat,\n\n    lstat,\n    link,\n    readlink,\n\n    realpath=_realpath,\n    remove=_recursive_remove,\n):\n    \"\"\"\n    Create a new kind of filesystem.\n    \"\"\"\n\n    methods = dict(\n        create=create_file,\n        open=lambda fs, path, mode=\"r\": open_file(\n            fs=fs, path=path, mode=mode,\n        ),\n        remove_file=remove_file,\n\n        create_directory=create_directory,\n        list_directory=list_directory,\n        remove_empty_directory=remove_empty_directory,\n        temporary_directory=temporary_directory,\n\n        get_contents=_get_contents,\n        set_contents=_set_contents,\n        create_with_contents=_create_with_contents,\n\n        remove=remove,\n        removing=_removing,\n\n        stat=stat,\n\n        lstat=lstat,\n        link=link,\n        readlink=readlink,\n        realpath=realpath,\n\n        exists=_exists,\n        is_dir=_is_dir,\n        is_file=_is_file,\n        is_link=_is_link,\n\n        touch=_touch,\n\n        children=_children,\n        glob_children=_glob_children,\n    )\n    return attr.s(hash=True)(type(name, (object,), methods))", "response": "Create a new kind of filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _exists(fs, path):\n    try:\n        fs.stat(path)\n    except (exceptions.FileNotFound, exceptions.NotADirectory):\n        return False\n    return True", "response": "Check that the given path exists on the filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_dir(fs, path):\n\n    try:\n        return stat.S_ISDIR(fs.stat(path).st_mode)\n    except exceptions.FileNotFound:\n        return False", "response": "Check that the given path is a directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the given path is a file.", "response": "def _is_file(fs, path):\n    \"\"\"\n    Check that the given path is a file.\n\n    Note that unlike `os.path.isfile`, we *do* propagate file system errors\n    other than a non-existent path or non-existent directory component.\n\n    E.g., should EPERM or ELOOP be raised, an exception will bubble up.\n    \"\"\"\n    try:\n        return stat.S_ISREG(fs.stat(path).st_mode)\n    except exceptions.FileNotFound:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the given path is a symbolic link.", "response": "def _is_link(fs, path):\n    \"\"\"\n    Check that the given path is a symbolic link.\n\n    Note that unlike `os.path.islink`, we *do* propagate file system errors\n    other than a non-existent path or non-existent directory component.\n\n    E.g., should EPERM or ELOOP be raised, an exception will bubble up.\n    \"\"\"\n\n    try:\n        return stat.S_ISLNK(fs.lstat(path).st_mode)\n    except exceptions.FileNotFound:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a single page of items from the API.", "response": "def get_page(self, page_number):\n        \"\"\"\n        :param page_number: The page number to fetch (1-indexed)\n        :return: The requested page fetched from the API for the query\n        \"\"\"\n\n        if page_number:\n            kwargs = dict(self._list_kwargs)\n            kwargs['limit'] = self.limit\n            kwargs['page'] = page_number\n\n            return self._controller.list(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list(self, **kwargs):\n\n        return ModelList(\n            self.ghost.execute_get('%s/' % self._type_name, **kwargs),\n            self._type_name, self, kwargs, model_type=self._model_type\n        )", "response": "Fetch a list of resources from the API."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, id=None, slug=None, **kwargs):\n\n        if id:\n            items = self.ghost.execute_get('%s/%s/' % (self._type_name, id), **kwargs)\n\n        elif slug:\n            items = self.ghost.execute_get('%s/slug/%s/' % (self._type_name, slug), **kwargs)\n\n        else:\n            raise GhostException(\n                500, 'Either the ID or the Slug of the resource needs to be specified'\n            )\n\n        return self._model_type(items[self._type_name][0])", "response": "Fetch a resource from the API. Either the id or the slug has to be specified."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new resource.", "response": "def create(self, **kwargs):\n        \"\"\"\n        Creates a new resource.\n\n        :param kwargs: The properties of the resource\n        :return: The created item returned by the API\n            wrapped as a `Model` object\n        \"\"\"\n\n        response = self.ghost.execute_post('%s/' % self._type_name, json={\n            self._type_name: [\n                kwargs\n            ]\n        })\n\n        return self._model_type(response.get(self._type_name)[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate an existing resource.", "response": "def update(self, id, **kwargs):\n        \"\"\"\n        Updates an existing resource.\n\n        :param id: The ID of the resource\n        :param kwargs: The properties of the resource to change\n        :return: The updated item returned by the API\n            wrapped as a `Model` object\n        \"\"\"\n\n        response = self.ghost.execute_put('%s/%s/' % (self._type_name, id), json={\n            self._type_name: [\n                kwargs\n            ]\n        })\n\n        return self._model_type(response.get(self._type_name)[0])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self, **kwargs):\n\n        return super(PostController, self).create(**self._with_markdown(kwargs))", "response": "Creates a new post object with the specified properties."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, id, **kwargs):\n\n        return super(PostController, self).update(id, **self._with_markdown(kwargs))", "response": "Updates an existing post."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef define_residues_for_plotting_traj(self, analysis_cutoff):\n        self.residue_counts_fraction = {}\n        #Calculate the fraction of time a residue spends in each simulation\n        for traj in self.residue_counts:\n            self.residue_counts_fraction[traj] = {residue:float(values)/len(self.contacts_per_timeframe[traj]) for residue,values in self.residue_counts[traj].items()}\n\n        for traj in self.residue_counts_fraction:\n            for residue in self.residue_counts_fraction[traj]:\n                self.frequency[residue].append(self.residue_counts_fraction[traj][residue])\n\n        self.topology_data.dict_of_plotted_res = {i:self.frequency[i] for i in self.frequency if sum(self.frequency[i])>(int(len(self.trajectory))*analysis_cutoff)}\n\n        assert len(self.topology_data.dict_of_plotted_res)!=0,\"Nothing to draw for this ligand:(residue number: \"+ str(self.topology_data.universe.ligand.resids[0]) +\" on the chain \"+ str(self.topology_data.universe.ligand.segids[0]) +\") - try reducing the analysis cutoff.\"", "response": "This function defines the residues that have made contact with the ligand over a lenghty ligand and the simulation is not always feasible or desirable. This function calculates the fraction of time each residue spends in each simulation and plots the residues that have made contact with the ligand over a long amount of time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_table(self):\n        num_records = np.sum([1 for frame in self.timeseries])\n        dtype = [(\"frame\",float),(\"time\",float),(\"ligand atom id\",int),\n                (\"ligand atom name\",\"|U4\"),(\"cutoff\",float),\n                (\"protein atom names\",list),(\"protein atom ids\",list),\n                (\"resid\",int),(\"resname\",\"|U4\"),(\"segid\",\"|U8\") ]\n        out = np.empty((num_records,),dtype=dtype)\n        cursor=0\n        for contact in self.timeseries:\n            out[cursor] = (contact.frame, contact.time,contact.ligandatomindex,contact.ligandatomname,contact.cutoff,\n                           contact.proteinatomname,contact.proteinatomindex,contact.resid,contact.resname,contact.segid)\n            cursor+=1\n        return out.view(np.recarray)", "response": "Make numpy array from timeseries data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect aromatic rings in ligand - size 4 - 6 atoms and all atoms are part of the ring. Saves self. aromatic_rings self. ligand_ring_num.", "response": "def detect_aromatic_rings_in_ligand(self):\n        \"\"\"Using rdkit to detect aromatic rings in ligand - size 4-6 atoms and all atoms are part of the ring. Saves this data in self.ligrings.\"\"\"\n        self.ligrings = {}\n        try:\n            ring_info = self.topology_data.mol.GetRingInfo()\n            self.ligand_ring_num = ring_info.NumRings()\n        except Exception as e:\n            m = Chem.MolFromPDBFile(\"lig.pdb\")\n            ring_info = m.GetRingInfo()\n            self.ligand_ring_num = ring_info.NumRings()\n        i=0\n        for ring in range(self.ligand_ring_num):\n            if 4 < len(ring_info.AtomRings()[ring]) <= 6 and  False not in [self.topology_data.mol.GetAtomWithIdx(x).GetIsAromatic() for x in ring_info.AtomRings()[ring]]: #narrow ring definition\n                atom_ids_in_ring = []\n                for atom in ring_info.AtomRings()[ring]:\n                    atom_ids_in_ring.append(self.topology_data.universe.ligand.atoms[atom].name)\n                self.ligrings[i]=atom_ids_in_ring\n                i+=1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_aromatic_rings_in_protein(self):\n        self.rings = {}\n        for ar_resname in [\"PHE\",\"TRP\",\"TYR\",\"HIS\"]:\n            for res in self.topology_data.universe.residues:\n                if res.resname == ar_resname:\n                    aromatic_aa = self.topology_data.universe.select_atoms(\"resname \"+res.resname+\" and resid \"+str(res.resid)+\" and segid \"+ res.segid)\n                    aromatic_aa.write(ar_resname+\".pdb\")\n                    break\n        for ar_resname in [\"PHE\",\"TRP\",\"TYR\",\"HIS\"]:\n            try:\n                arom_aa_rdkit = Chem.MolFromPDBFile(ar_resname+\".pdb\")\n                arom_aa_mda = MDAnalysis.Universe(ar_resname+\".pdb\")\n                ring_info = arom_aa_rdkit.GetRingInfo()\n                number_of_rings = ring_info.NumRings()\n                for ring in range(number_of_rings):\n                    atom_names_in_ring = []\n                    for atom in ring_info.AtomRings()[ring]:\n                        atom_names_in_ring.append(arom_aa_mda.atoms[atom].name)\n                    self.rings[(ar_resname,ring)]=atom_names_in_ring\n            except IOError:\n                continue", "response": "Use rdkit to detect aromatic rings in protein."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_all_pistacking_pairs(self):\n        data = namedtuple(\"pistacking\",\"frame time proteinring ligandring distance angle offset type resname resid segid\")\n        i=0\n        if self.trajectory==[]:\n            self.trajectory = [self.topology_data.universe.filename]\n            self.start_frame_num=[None]\n            self.end_frame_num = [None]\n            self.skip =[None]\n        for traj in self.trajectory:\n            self.timeseries=[]\n            self.timesteps=[frame.time for frame in self.topology_data.universe.trajectory[self.start_frame_num[i]:self.end_frame_num[i]:self.skip[i]]]\n            start = timer()\n            self.topology_data.load_trajectory(traj)\n            for prot_ring in self.protein_rings:\n                    for ring in self.ligrings:\n                        l = self.get_ligand_ring_selection(ring)\n                        p = self.protein_rings[prot_ring]\n                        for frame in self.topology_data.universe.trajectory[self.start_frame_num[i]:self.end_frame_num[i]:self.skip[i]]:\n                            lig_norm_vec = math.prepare_normal_vectors(l)\n                            protein_norm_vec = math.prepare_normal_vectors(p)\n                            dist = math.euclidean3d(l.center_of_geometry(),p.center_of_geometry())\n\n                            a = math.vecangle(lig_norm_vec,protein_norm_vec)\n                            angle = min(a, 180 - a if not 180 - a < 0 else a)\n\n                            #Measure offset\n                            proj1 = math.projection(lig_norm_vec,l.center_of_geometry(),p.center_of_geometry())\n                            proj2 = math.projection(protein_norm_vec,p.center_of_geometry(),l.center_of_geometry())\n                            offset = min(math.euclidean3d(proj1,l.center_of_geometry()), math.euclidean3d(proj2,p.center_of_geometry()))\n\n\n                            if dist < self.max_distance:\n                                if 0 < angle < self.max_angle_dev and offset < self.max_offset:\n                                    contacts = data(frame=frame.frame, time=frame.time, proteinring=tuple([a.id for a in p]), ligandring=tuple([a.id for a in l]), distance=dist, angle=angle, offset=offset,\n                                                    type=\"P\",resname=self.protein_rings[prot_ring].residues.resnames[0],\n                                                    resid=self.protein_rings[prot_ring].residues.resids[0], segid=self.protein_rings[prot_ring].residues.segids[0])\n                                    self.timeseries.append(contacts)\n                                if 90 - self.max_angle_dev < angle < 90 + self.max_angle_dev and offset < self.max_offset:\n                                    contacts = data(frame=frame.frame, time=frame.time, proteinring=tuple([a.id for a in p]), ligandring=tuple([a.id for a in l]), distance=dist, angle=angle, offset=offset,\n                                                    type=\"T\",resname=self.protein_rings[prot_ring].residues.resnames[0],\n                                                    resid=self.protein_rings[prot_ring].residues.resids[0], segid=self.protein_rings[prot_ring].residues.segids[0])\n                                    self.timeseries.append(contacts)\n            self.pistacking[i] = self.make_table()\n\n            self.pistacking_by_time[i] = self.count_by_time()\n            self.pistacking_by_type[i] = self.count_by_type()\n            self.write_output_files(i)\n            i+=1\n        end = timer()\n        print \"Pi-Stacking:\"+str(end-start)", "response": "This function is called by the analysis function. It finds all pi - pi interactions between previously defined rings and ligand rings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_table(self):\n        num_records = int(np.sum([1 for frame in self.timeseries]))\n        dtype = [\n                (\"frame\",float),(\"time\",float),(\"proteinring\",list),\n                (\"ligand_ring_ids\",list),(\"distance\",float),(\"angle\",float),\n                (\"offset\",float),(\"type\",\"|U4\"),(\"resid\",int),(\"resname\",\"|U4\"),(\"segid\",\"|U8\") ]\n        out = np.empty((num_records,),dtype=dtype)\n        cursor=0\n        for contact in self.timeseries:\n            out[cursor] = (contact.frame, contact.time,contact.proteinring,contact.ligandring,contact.distance,contact.angle,contact.offset,contact.type,contact.resid,contact.resname,contact.segid)\n            cursor+=1\n        return out.view(np.recarray)", "response": "Make numpy array from timeseries data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef count_by_type(self):\n        pistack = defaultdict(int)\n        for contact in self.timeseries:\n            #count by residue name not by proteinring\n            pkey = (contact.ligandring,contact.type, contact.resid,contact.resname,contact.segid)\n            pistack[pkey]+=1\n        dtype = [(\"ligand_ring_ids\",list),(\"type\",\"|U4\"),(\"resid\",int),(\"resname\",\"|U4\"),(\"segid\",\"|U8\"),(\"frequency\",float) ]\n        out = np.empty((len(pistack),),dtype=dtype)\n        tsteps = float(len(self.timesteps))\n        for cursor,(key,count) in enumerate(pistack.iteritems()):\n            out[cursor] = key + (count / tsteps,)\n        return out.view(np.recarray)", "response": "Count how many times each individual pi - pi interaction occured throughout the simulation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ligand_ring_selection(self,ring):\n        ring_names = \"\"\n        for atom in self.ligrings[ring]:\n            ring_names = ring_names+\" \"+str(atom)\n        ring_selection = self.topology_data.universe.ligand.select_atoms(\"name \"+ring_names)\n        return ring_selection", "response": "Returns the atom selections of aromatic rings present in the ligand molecule."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_pistacking_frequency(self,analysis_cutoff):\n        self.frequency = defaultdict(int)\n        for traj in self.pistacking_by_type:\n            for contact in self.pistacking_by_type[traj]:\n                self.frequency[contact[\"ligand_ring_ids\"],contact[\"type\"],contact[\"resid\"],contact[\"resname\"],contact[\"segid\"]]+=contact[\"frequency\"]\n        draw_frequency = {i:self.frequency[i] for i in self.frequency if self.frequency[i]>(int(len(self.trajectory))*float(analysis_cutoff))}\n\n        self.pi_contacts_for_drawing = {}\n        for contact in draw_frequency:\n            self.pi_contacts_for_drawing[contact]=draw_frequency[contact]", "response": "Calculates the frequency of pi - pi interactions throughout simulations."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the CSV output files for the trajectory.", "response": "def write_output_files(self,traj):\n        \"\"\"\n        The total hydrogen bond count per frame is provided as CSV output file.\n        Each trajectory has a separate file.\n        \"\"\"\n        try:\n            os.chdir(\"analysis\")\n        except Exception as e:\n            os.mkdir(\"analysis\")\n            os.chdir(\"analysis\")\n        os.mkdir(\"pistacking\")\n        os.chdir(\"pistacking\")\n        with open('pistacking_data_total_'+str(traj)+'.csv', 'wb') as outfile:\n            hwriter = csv.writer(outfile, delimiter=' ')\n            for time in self.pistacking_by_time[traj]:\n                hwriter.writerow([time[0],time[1]])\n        for bond in self.pistacking_by_type[traj]:\n            if bond in self.pi_contacts_for_drawing.keys():\n                with open(\"pi_contact_\"+str(traj)+\".csv\",\"wb\") as outfile:\n                    hwriter = csv.writer(outfile, delimiter=' ')\n                    for time in self.timesteps:\n                        result = [1 if x[0]==time and x[\"acceptor_idx\"]==bond[\"acceptor_idx\"] else 0 for x in self.timeseries][0]\n                        hwriter.writerow([time,result])\n        os.chdir(\"../../\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a timestamp from a string.", "response": "def get_timestamp(cls, time_string: str, fmt: str):\n        \"\"\" \u83b7\u5f97\u7ed9\u5b9a\u7684\u65f6\u95f4\u5b57\u7b26\u4e32\uff0c\u83b7\u5f97\u76f8\u5e94\u7684\u65f6\u95f4\u6233\n        :param time_string: \u65f6\u95f4\u5b57\u7b26\u4e32\n        :param fmt:\n        :return:\n        \"\"\"\n        return int(time.mktime(time.strptime(time_string, fmt)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_formatted_datetime_string(cls, timestamp: int, fmt=\"%Y-%m-%d %H:%M:%S\"):\n        return time.strftime(fmt, time.localtime(timestamp))", "response": "Returns a formatted datetime string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npuzzling - base function.", "response": "def base(ctx, verbose, config):\n    \"\"\"Puzzle: manage DNA variant resources.\"\"\"\n    # configure root logger to print to STDERR\n    loglevel = LEVELS.get(min(verbose, 3))\n    configure_stream(level=loglevel)\n    ctx.obj = {}\n    if config and os.path.exists(config):\n        ctx.obj = yaml.load(open(config, 'r')) or {}\n        ctx.obj['config_path'] = config\n    # launch the command line interface\n    logger.debug('Booting up command line interface')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef repl_db_sub(master_dsn, slave_dsn, tables):\n    logger = logging.getLogger(\"meepo.sub.replicate_sub\")\n\n    # sqlalchemy reflection\n    logger.info(\"reflecting master database: {}\".format(master_dsn))\n    master_engine = sa.create_engine(master_dsn)\n    master_base = automap_base()\n    master_base.prepare(engine=master_engine, reflect=True)\n    MasterSession = scoped_session(sessionmaker(bind=master_engine))\n\n    logger.info(\"reflecting slave database: {}\".format(slave_dsn))\n    slave_engine = sa.create_engine(slave_dsn)\n    slave_base = automap_base()\n    slave_base.prepare(engine=slave_engine, reflect=True)\n    SlaveSession = scoped_session(sessionmaker(bind=slave_engine))\n\n    def _write_by_pk(name, pk):\n        \"\"\"Copy row from master to slave based on pk\n        \"\"\"\n        MasterModel = master_base.classes[name]\n        obj = MasterSession.query(MasterModel).get(pk)\n        if not obj:\n            logger.error(\"pk for {} not found in master: {}\".format(name, pk))\n            return\n\n        SlaveModel = slave_base.classes[name]\n        columns = [c.name for c in SlaveModel.__table__.columns]\n        s_obj = SlaveModel(**{k: v\n                              for k, v in obj.__dict__.items()\n                              if k in columns})\n        SlaveSession.add(s_obj)\n\n        try:\n            SlaveSession.commit()\n        except SQLAlchemyError as e:\n            SlaveSession.rollback()\n            logger.exception(e)\n\n        # cleanup\n        MasterSession.close()\n        SlaveSession.close()\n\n    def _update_by_pk(name, pk):\n        \"\"\"Update row from master to slave based on pk\n        \"\"\"\n        MasterModel = master_base.classes[name]\n        obj = MasterSession.query(MasterModel).get(pk)\n\n        SlaveModel = slave_base.classes[name]\n        s_obj = SlaveSession.query(SlaveModel).get(pk)\n        if not s_obj:\n            return _write_by_pk(name, pk)\n\n        columns = [c.name for c in SlaveModel.__table__.columns]\n        for col in columns:\n            try:\n                val = getattr(obj, col)\n            except AttributeError as e:\n                continue\n            setattr(s_obj, col, val)\n\n        try:\n            SlaveSession.commit()\n        except SQLAlchemyError as e:\n            SlaveSession.rollback()\n            logger.exception(e)\n\n        # cleanup\n        MasterSession.close()\n        SlaveSession.close()\n\n    def _delete_by_pk(name, pk):\n        \"\"\"Copy row from slave based on pk\n        \"\"\"\n        Model = slave_base.classes[name]\n        obj = SlaveSession.query(Model).get(pk)\n        if obj:\n            SlaveSession.delete(obj)\n        SlaveSession.commit()\n\n        # cleanup\n        SlaveSession.close()\n\n    def _sub(table):\n\n        def _sub_write(pk):\n            logger.info(\"repl_db {}_write: {}\".format(table, pk))\n            _write_by_pk(table, pk)\n        signal(\"%s_write\" % table).connect(_sub_write, weak=False)\n\n        def _sub_update(pk):\n            logger.info(\"repl_db {}_update: {}\".format(table, pk))\n            _update_by_pk(table, pk)\n        signal(\"%s_update\" % table).connect(_sub_update, weak=False)\n\n        def _sub_delete(pk):\n            logger.info(\"repl_db {}_delete: {}\".format(table, pk))\n            _delete_by_pk(table, pk)\n        signal(\"%s_delete\" % table).connect(_sub_delete, weak=False)\n\n    tables = (t for t in tables if t in slave_base.classes.keys())\n    for table in tables:\n        _sub(table)", "response": "This function is used to subscribe to the event sourcing pk stream and update the master table with the new values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn height / width ratio of the given Axes object.", "response": "def get_axes_ratio(ax):\n    \"\"\"Return height / width ratio of the given Axes object.\n    \n    The ratio is calculated in 'display coordinate',\n    defined in matplotlib document on transformation.\n    Thus, the calculated ratio is what one would feels when the Axes\n    is displayed to the her/him.\n    \"\"\"\n    ax_bbox_points_in_fig_coord = ax.get_position().get_points()\n    ax_bbox_points_in_display_coord = [\n            ax.figure.transFigure.transform(point) for point in ax_bbox_points_in_fig_coord ]\n    lower_left_coord, upper_right_coord = ax_bbox_points_in_display_coord\n    ax_bbox_dimension_in_display_coord = upper_right_coord - lower_left_coord\n    width, height = ax_bbox_dimension_in_display_coord\n    ratio = height / width\n    return ratio"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn text position corresponding to given pos in given axes.", "response": "def get_text_position_in_ax_coord(ax, pos, scale=default_text_relative_padding):\n    \"\"\"Return text position corresponding to given 'pos'.\n    The text alignment in the bounding box should be set accordingly \n    in order to have a good-looking layout.\n    This corresponding text alignment can be obtained by 'get_text_alignment'\n    or 'get_text_position_and_inner_alignment' function.\n    \"\"\"\n    ratio = get_axes_ratio(ax)\n    x, y = scale ,scale\n    if ratio > 1: # vertical is longer\n        y /= ratio\n    elif 0 < ratio: # 0 < ratio <= 1\n        x *= ratio\n    pos = pos.lower()\n    if pos == 'nw': y = 1 - y\n    elif pos == 'ne': x, y = 1 - x, 1 - y\n    elif pos == 'sw': pass\n    elif pos == 'se': x = 1 - x\n    else: raise ValueError(\"Unknown value for 'pos': %s\" % (str(pos)))\n        \n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn vertical alignment and horizontal alignment for given position", "response": "def get_text_alignment(pos):\n    \"\"\"Return 'verticalalignment'('va') and 'horizontalalignment'('ha') for given 'pos' (position)\"\"\"\n    pos = pos.lower()  # to make it case insensitive\n    va, ha = None, None\n    if pos == 'nw': va, ha = 'top', 'left'\n    elif pos == 'ne': va, ha = 'top', 'right'\n    elif pos == 'sw': va, ha = 'bottom', 'left'\n    elif pos == 'se': va, ha = 'bottom', 'right'\n    else: raise ValueError(\"Unknown value for 'pos': %s\" % (str(pos)))\n    return {'va':va, 'ha':ha}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_text_position_and_inner_alignment(ax, pos, scale=default_text_relative_padding, with_transAxes_kwargs=True):\n    xy = get_text_position_in_ax_coord(ax,pos,scale=scale)\n    alignment_fontdict = get_text_alignment(pos)\n    if with_transAxes_kwargs: alignment_fontdict = {**alignment_fontdict, **{'transform':ax.transAxes}}\n    return xy, alignment_fontdict", "response": "Return text position and alignment in its bounding box."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_text_position(fig, ax, ha='left', va='top', pad_scale=1.0):\n    ## Check and preprocess input arguments\n    try: pad_scale = float(pad_scale)\n    except: raise TypeError(\"'pad_scale should be of type 'float'\")\n        \n    for arg in [va, ha]: \n        assert type(arg) is str\n        arg = arg.lower() # Make it lowercase to prevent case problem.\n    \n    ## Get axis size in inches\n    ax_height, ax_width = get_ax_size_in_inch(fig, ax)\n    \n    ## Construct inversion factor from inch to plot coordinate\n    length_x = ax.get_xlim()[1] - ax.get_xlim()[0]\n    length_y = ax.get_ylim()[1] - ax.get_ylim()[0]\n    inch2coord_x = length_x / ax_width\n    inch2coord_y = length_y / ax_height\n    \n    ## Set padding size relative to the text size\n    #pad_inch = text_bbox_inch.height * pad_scale\n    #pad_inch = fontsize_points * point2inch * pad_scale\n    ax_length_geom_average = (ax_height * ax_width) ** 0.5\n    pad_inch = ax_length_geom_average * 0.03 * pad_scale\n    pad_inch_x, pad_inch_y = pad_inch, pad_inch\n    \n    pad_coord_x = pad_inch_x * inch2coord_x\n    pad_coord_y = pad_inch_y * inch2coord_y\n    \n    if ha == 'left': pos_x = ax.get_xlim()[0] + pad_coord_x\n    elif ha == 'right': pos_x = ax.get_xlim()[1] - pad_coord_x\n    else: raise Exception(\"Unsupported value for 'ha'\")\n    \n    if va in ['top','up','upper']: pos_y = ax.get_ylim()[1] - pad_coord_y\n    elif va in ['bottom','down','lower']: pos_y = ax.get_ylim()[0] + pad_coord_y\n    else: raise Exception(\"Unsupported value for 'va'\")\n    \n    return pos_x, pos_y", "response": "Return the position of the text inside of the given axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_app(config=None, config_obj=None):\n    app = Flask(__name__)\n    # configure application from external configs\n    configure_app(app, config=config, config_obj=config_obj)\n    # register different parts of the application\n    register_blueprints(app)\n    # setup extensions\n    bind_extensions(app)\n    return app", "response": "Create a new Flask application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure_app(app, config=None, config_obj=None):\n    app.config.from_object(config_obj or BaseConfig)\n    if config is not None:\n        app.config.from_pyfile(config)", "response": "Configure application instance.\n\n    Args:\n        app (Flask): initialized Flask app instance\n        config (Optional[path]): path to a Python module config file\n        config_obj (Optional[class]): Python config object"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures extensions. Args: app (Flask): initialized Flask app instance", "response": "def bind_extensions(app):\n    \"\"\"Configure extensions.\n\n    Args:\n        app (Flask): initialized Flask app instance\n    \"\"\"\n    # bind plugin to app object\n    app.db = app.config['PUZZLE_BACKEND']\n    app.db.init_app(app)\n\n    # bind bootstrap blueprints\n    bootstrap.init_app(app)\n    markdown(app)\n\n    @app.template_filter('islist')\n    def islist(object):\n        return isinstance(object, (tuple, list))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncounting how many times each individual hydrogen bonds occured throughout the simulation.", "response": "def count_by_type(self,table,timesteps):\n        \"\"\"Count how many times each individual hydrogen bonds occured throughout the simulation.\n        Returns numpy array.\"\"\"\n        hbonds = defaultdict(int)\n        for contact in table:\n            #count by residue name not by proteinring\n            pkey = (contact.donor_idx,contact.acceptor_idx,contact.donor_atom, contact.acceptor_atom,contact.donor_resnm,contact.donor_resid, contact.acceptor_resnm,contact.acceptor_resid)\n            hbonds[pkey]+=1\n        dtype = [(\"donor_idx\",int),(\"acceptor_idx\",int),(\"donor_atom\",\"|U4\"),(\"acceptor_atom\",\"|U4\"),(\"donor_resnm\",\"|U8\"),(\"donor_resid\",\"|U8\"),(\"acceptor_resnm\",\"|U8\"),(\"acceptor_resid\",\"|U8\"),(\"frequency\",float) ]\n        out = np.empty((len(hbonds),),dtype=dtype)\n        tsteps = float(len(timesteps))\n        for cursor,(key,count) in enumerate(hbonds.iteritems()):\n            out[cursor] = key + (count / tsteps,)\n        return out.view(np.recarray)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analyse_hydrogen_bonds_topology(self,distance=3):\n        h = MDAnalysis.analysis.hbonds.HydrogenBondAnalysis(self.topology_data.universe,'(segid '+str(self.topology_data.universe.ligand.segids[0])+' and resid '+str(self.topology_data.universe.ligand.resids[0])+')',\"protein\",distance=3,acceptors=self.acceptors,donors=self.donors)\n        h.run()\n        h.generate_table()\n        self.hbonds[0]=h.table\n        self.hbonds_by_time[0] = h.count_by_time()\n        self.hbonds_by_type[0] = h.count_by_type()", "response": "This function is used to analyse hydrogen bonds in the topology."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the output files for the hydrogen bond tables.", "response": "def write_output_files(self):\n        \"\"\"\n        The total hydrogen bond count per frame is provided as CSV output file.\n        Each trajectory has a separate file.\n        \"\"\"\n        os.mkdir(\"analysis\")\n        os.chdir(\"analysis\")\n        os.mkdir(\"hydrogen_bonds\")\n        os.chdir(\"hydrogen_bonds\")\n        for traj in range(len(self.trajectory)):\n            with open('hbonds_data_total_'+str(traj)+'.csv', 'wb') as outfile:\n                hwriter = csv.writer(outfile, delimiter=' ')\n                for time in self.hbonds_by_time[traj]:\n                    hwriter.writerow([time[0],time[1]])\n            for bond in self.hbonds_by_type[traj]:\n                if bond[\"donor_resnm\"]==\"LIG\" and (bond[\"acceptor_idx\"],bond[\"donor_idx\"]) in [(k[0],k[3]) for k,v in self.hbonds_for_drawing.items()]:\n                    with open('hbond_'+bond[\"acceptor_resnm\"]+\"_\"+str(bond[\"acceptor_resid\"])+\"_\"+bond[\"donor_atom\"]+\"_\"+str(traj)+\".csv\",\"wb\") as outfile:\n                        hwriter = csv.writer(outfile, delimiter=' ')\n                        for time in self.hbonds_timesteps[traj]:\n                            result = [1 if x[0]==time and x[\"acceptor_idx\"]==bond[\"acceptor_idx\"] else 0 for x in self.hbonds[traj]][0]\n                            hwriter.writerow([time,result])\n                if bond[\"donor_resnm\"]!=\"LIG\" and (bond[\"donor_idx\"],bond[\"acceptor_idx\"]) in [(k[0],k[3]) for k,v in self.hbonds_for_drawing.items()]:\n                    with open('hbond_'+bond[\"donor_resnm\"]+\"_\"+str(bond[\"donor_resid\"])+\"_\"+bond[\"acceptor_atom\"]+\"_\"+str(traj)+\".csv\",\"wb\") as outfile:\n                        hwriter = csv.writer(outfile, delimiter=' ')\n                        for time in self.hbonds_timesteps[traj]:\n                            result = [1 if x[0]==time and x[\"donor_idx\"]==bond[\"donor_idx\"] else 0 for x in self.hbonds[traj]][0]\n                            hwriter.writerow([time,result])\n        os.chdir(\"../../\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert2dbus(value, signature):\n    if len(signature) == 2 and signature.startswith('a'):\n        return dbus.Array(value, signature=signature[-1])\n    dbus_string_type = dbus.String if PY3 else dbus.UTF8String\n    type_map = {\n        'b': dbus.Boolean, 'y': dbus.Byte, 'n': dbus.Int16,\n        'i': dbus.Int32, 'x': dbus.Int64, 'q': dbus.UInt16, 'u': dbus.UInt32,\n        't': dbus.UInt64, 'd': dbus.Double, 'o': dbus.ObjectPath,\n        'g': dbus.Signature, 's': dbus_string_type}\n    return type_map[signature](value)", "response": "Converts value type from python to dbus according signature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting dbus_obj from dbus type to python type.", "response": "def convert(dbus_obj):\n    \"\"\"Converts dbus_obj from dbus type to python type.\n\n    :param dbus_obj: dbus object.\n    :returns: dbus_obj in python type.\n    \"\"\"\n    _isinstance = partial(isinstance, dbus_obj)\n    ConvertType = namedtuple('ConvertType', 'pytype dbustypes')\n\n    pyint = ConvertType(int, (dbus.Byte, dbus.Int16, dbus.Int32, dbus.Int64,\n                              dbus.UInt16, dbus.UInt32, dbus.UInt64))\n    pybool = ConvertType(bool, (dbus.Boolean, ))\n    pyfloat = ConvertType(float, (dbus.Double, ))\n    pylist = ConvertType(lambda _obj: list(map(convert, dbus_obj)),\n                         (dbus.Array, ))\n    pytuple = ConvertType(lambda _obj: tuple(map(convert, dbus_obj)),\n                          (dbus.Struct, ))\n    types_str = (dbus.ObjectPath, dbus.Signature, dbus.String)\n    if not PY3:\n        types_str += (dbus.UTF8String,)\n    pystr = ConvertType(str if PY3 else unicode, types_str)\n\n    pydict = ConvertType(\n        lambda _obj: dict(zip(map(convert, dbus_obj.keys()),\n                              map(convert, dbus_obj.values())\n                              )\n                          ),\n        (dbus.Dictionary, )\n    )\n\n    for conv in (pyint, pybool, pyfloat, pylist, pytuple, pystr, pydict):\n        if any(map(_isinstance, conv.dbustypes)):\n            return conv.pytype(dbus_obj)\n    else:\n        return dbus_obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef converter(f):\n    @wraps(f)\n    def wrapper(*args, **kwds):\n        return convert(f(*args, **kwds))\n    return wrapper", "response": "Decorator to convert value from dbus type to python type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch and returns set of unique names of objects which implements MPRIS2 interfaces.", "response": "def available_players():\n    \"\"\"Searchs and returns set of unique names of objects\n    which implements MPRIS2 interfaces.\n\n    :returns: set of unique names.\n    :type: set\n    \"\"\"\n    bus = dbus.SessionBus()\n    players = set()\n    for name in filter(lambda item: item.startswith(MPRIS_NAME_PREFIX),\n                       bus.list_names()):\n        owner_name = bus.get_name_owner(name)\n        players.add(convert(owner_name))\n    return players"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_properties_signals(f, signal_iface_name):\n    @wraps(f)\n    def wrapper(iface, changed_props, invalidated_props, *args, **kwargs):\n        if iface == signal_iface_name:\n            f(changed_props, invalidated_props)\n\n    return wrapper", "response": "Decorator to filter out properties of a given signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn pairs of matching indices from l1 and l2.", "response": "def distance_function_match(l1, l2, thresh, dist_fn, norm_funcs=[]):\n    \"\"\"Returns pairs of matching indices from l1 and l2.\"\"\"\n    common = []\n    # We will keep track of the global index in the source list as we\n    # will successively reduce their sizes.\n    l1 = list(enumerate(l1))\n    l2 = list(enumerate(l2))\n\n    # Use the distance function and threshold on hints given by normalization.\n    # See _match_by_norm_func for implementation details.\n    # Also wrap the list element function function to ignore the global list\n    # index computed above.\n    for norm_fn in norm_funcs:\n        new_common, l1, l2 = _match_by_norm_func(\n                l1, l2,\n                lambda a: norm_fn(a[1]),\n                lambda a1, a2: dist_fn(a1[1], a2[1]),\n                thresh)\n        # Keep only the global list index in the end result.\n        common.extend((c1[0], c2[0]) for c1, c2 in new_common)\n\n    # Take any remaining umatched entries and try to match them using the\n    # Munkres algorithm.\n    dist_matrix = [[dist_fn(e1, e2) for i2, e2 in l2] for i1, e1 in l1]\n\n    # Call Munkres on connected components on the remaining bipartite graph.\n    # An edge links an element from l1 with an element from l2 only if\n    # the distance between the elements is less (or equal) than the theshold.\n    components = BipartiteConnectedComponents()\n    for l1_i in range(len(l1)):\n        for l2_i in range(len(l2)):\n            if dist_matrix[l1_i][l2_i] > thresh:\n                continue\n            components.add_edge(l1_i, l2_i)\n\n    for l1_indices, l2_indices in components.get_connected_components():\n        # Build a partial distance matrix for each connected component.\n        part_l1 = [l1[i] for i in l1_indices]\n        part_l2 = [l2[i] for i in l2_indices]\n\n        part_dist_matrix = [[dist_matrix[l1_i][l2_i] for l2_i in l2_indices]\n                            for l1_i in l1_indices]\n        part_cmn = _match_munkres(part_l1, part_l2, part_dist_matrix, thresh)\n\n        common.extend((c1[0], c2[0]) for c1, c2 in part_cmn)\n\n    return common"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _match_by_norm_func(l1, l2, norm_fn, dist_fn, thresh):\n    common = []\n\n    l1_only_idx = set(range(len(l1)))\n    l2_only_idx = set(range(len(l2)))\n\n    buckets_l1 = _group_by_fn(enumerate(l1), lambda x: norm_fn(x[1]))\n    buckets_l2 = _group_by_fn(enumerate(l2), lambda x: norm_fn(x[1]))\n\n    for normed, l1_elements in buckets_l1.items():\n        l2_elements = buckets_l2.get(normed, [])\n        if not l1_elements or not l2_elements:\n            continue\n        _, (_, e1_first) = l1_elements[0]\n        _, (_, e2_first) = l2_elements[0]\n        match_is_ambiguous = not (\n            len(l1_elements) == len(l2_elements) and (\n                all(e2 == e2_first for (_, (_, e2)) in l2_elements) or\n                all(e1 == e1_first for (_, (_, e1)) in l1_elements)\n            )\n        )\n        if match_is_ambiguous:\n            continue\n        for (e1_idx, e1), (e2_idx, e2) in zip(l1_elements, l2_elements):\n            if dist_fn(e1, e2) > thresh:\n                continue\n            l1_only_idx.remove(e1_idx)\n            l2_only_idx.remove(e2_idx)\n            common.append((e1, e2))\n\n    l1_only = [l1[i] for i in l1_only_idx]\n    l2_only = [l2[i] for i in l2_only_idx]\n\n    return common, l1_only, l2_only", "response": "Matches elements in l1 and l2 using normalization functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmatches two lists using Munkres algorithm.", "response": "def _match_munkres(l1, l2, dist_matrix, thresh):\n    \"\"\"Matches two lists using the Munkres algorithm.\n\n    Returns pairs of matching indices from the two lists by minimizing the sum\n    of the distance between the linked elements and taking only the elements\n    which have the distance between them less (or equal) than the threshold.\n    \"\"\"\n    equal_dist_matches = set()\n    m = Munkres()\n    indices = m.compute(dist_matrix)\n\n    for l1_idx, l2_idx in indices:\n        dst = dist_matrix[l1_idx][l2_idx]\n        if dst > thresh:\n            continue\n        for eq_l2_idx, eq_val in enumerate(dist_matrix[l1_idx]):\n            if abs(dst - eq_val) < 1e-9:\n                equal_dist_matches.add((l1_idx, eq_l2_idx))\n        for eq_l1_idx, eq_row in enumerate(dist_matrix):\n            if abs(dst - eq_row[l2_idx]) < 1e-9:\n                equal_dist_matches.add((eq_l1_idx, l2_idx))\n\n    return [(l1[l1_idx], l2[l2_idx]) for l1_idx, l2_idx in equal_dist_matches]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_suspect(self, case_obj, variant_obj):\n        new_suspect = Suspect(case=case_obj, variant_id=variant_obj.variant_id,\n                              name=variant_obj.display_name)\n        self.session.add(new_suspect)\n        self.save()\n        return new_suspect", "response": "Link a suspect to a case."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_suspect(self, suspect_id):\n        suspect_obj = self.suspect(suspect_id)\n        logger.debug(\"Deleting suspect {0}\".format(suspect_obj.name))\n        self.session.delete(suspect_obj)\n        self.save()", "response": "De - link a suspect from a case."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvisualize DNA variant resources.", "response": "def view(ctx, host, port, debug, pattern, family_file, family_type,\n         variant_source, root, no_browser, phenomizer):\n    \"\"\"Visualize DNA variant resources.\n\n    1. Look for variant source(s) to visualize and inst. the right plugin\n    \"\"\"\n    main_loop = (not debug) or (os.environ.get('WERKZEUG_RUN_MAIN') == 'true')\n    root = root or ctx.obj.get('root') or os.path.expanduser(\"~/.puzzle\")\n    phenomizer_auth = phenomizer or ctx.obj.get('phenomizer_auth')\n    BaseConfig.PHENOMIZER_AUTH = True if ctx.obj.get('phenomizer_auth') else False\n    BaseConfig.STORE_ENABLED = True\n\n    if variant_source is None:\n        logger.info(\"Root directory is: {}\".format(root))\n\n        db_path = os.path.join(root, 'puzzle_db.sqlite3')\n        logger.info(\"db path is: {}\".format(db_path))\n        if not os.path.exists(db_path):\n            logger.warn(\"database not initialized, run 'puzzle init'\")\n            ctx.abort()\n\n        if os.path.isfile(root):\n            logger.error(\"'root' can't be a file\")\n            ctx.abort()\n\n        store = SqlStore(db_path, phenomizer_auth=phenomizer_auth)\n        for case_obj in store.cases():\n            if case_obj.variant_mode == 'gemini':\n                if not GEMINI:\n                    logger.error(\"Need to have gemini instaled to view gemini database\")\n                    ctx.abort()\n\n    else:\n        logger.info(\"Using in memory database\")\n        tmpdir = tempfile.mkdtemp()\n        tmpdb = os.path.join(tmpdir, 'puzzle.sqlite3')\n        logger.info(\"building database: {}\".format(tmpdb))\n        store = SqlStore(\"sqlite:///{}\".format(tmpdb),\n                         phenomizer_auth=phenomizer_auth)\n        if main_loop:\n            store.set_up()\n            cases = []\n            if os.path.isfile(variant_source):\n                file_type = get_file_type(variant_source)\n                #Test if gemini is installed\n                if file_type == 'unknown':\n                    logger.error(\"File has to be vcf or gemini db\")\n                    ctx.abort()\n                elif file_type == 'gemini':\n                    #Check if gemini is installed\n                    if not GEMINI:\n                        logger.error(\"Need to have gemini installed to use gemini plugin\")\n                        ctx.abort()\n                variant_type = get_variant_type(variant_source)\n                cases = get_cases(\n                    variant_source=variant_source,\n                    case_lines=family_file,\n                    case_type=family_type,\n                    variant_type=variant_type,\n                    variant_mode=file_type\n                )\n            else:\n                for file in path(variant_source).walkfiles(pattern):\n                    file_type = get_file_type(file)\n                    if file_type != 'unknown':\n                        variant_type = get_variant_type(file)\n                        #Test if gemini is installed\n                        if file_type == 'gemini':\n                            if not GEMINI:\n                                logger.error(\"Need to have gemini installed to use gemini plugin\")\n                                ctx.abort()\n\n                        for case in get_cases(\n                            variant_source=file,\n                            case_type=family_type,\n                            variant_type=variant_type,\n                            variant_mode=file_type):\n\n                            cases.append(case)\n\n            for case_obj in cases:\n                if store.case(case_obj.case_id) is not None:\n                    logger.warn(\"{} already exists in the database\"\n                                .format(case_obj.case_id))\n                    continue\n\n                # extract case information\n                logger.debug(\"adding case: {}\".format(case_obj.case_id))\n                store.add_case(case_obj, vtype=case_obj.variant_type, mode=case_obj.variant_mode)\n\n    logger.debug(\"Plugin setup was succesfull\")\n    BaseConfig.PUZZLE_BACKEND = store\n    BaseConfig.UPLOAD_DIR = os.path.join(root, 'resources')\n\n    app = create_app(config_obj=BaseConfig)\n\n    if no_browser is False:\n        webbrowser.open_new_tab(\"http://{}:{}\".format(host, port))\n\n    app.run(host=host, port=port, debug=debug)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconfigure root logger using a standard stream handler.", "response": "def configure_stream(level='WARNING'):\n    \"\"\"Configure root logger using a standard stream handler.\n\n    Args:\n        level (string, optional): lowest level to log to the console\n\n    Returns:\n        logging.RootLogger: root logger instance with attached handler\n    \"\"\"\n    # get the root logger\n    root_logger = logging.getLogger()\n    # set the logger level to the same as will be used by the handler\n    root_logger.setLevel(level)\n\n    # customize formatter, align each column\n    template = \"[%(asctime)s] %(name)-25s %(levelname)-8s %(message)s\"\n    formatter = logging.Formatter(template)\n\n    # add a basic STDERR handler to the logger\n    console = logging.StreamHandler()\n    console.setLevel(level)\n    console.setFormatter(formatter)\n\n    root_logger.addHandler(console)\n    return root_logger"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_by_fields(self, obj, fields, get_field_fn, exclude_list):\n        cleaned_list = []\n        obj_model = get_model_from_instance(obj)\n\n        for field in fields:\n            field_accessor = get_field_fn(field)\n            # This field is excluded if:\n            # 1/ it's parent model key is in exclude list keys\n            # AND\n            # 2/ the field has been defined as excluded for this parent model\n            is_excluded = obj_model in exclude_list and field_accessor in exclude_list[obj_model]\n\n            if not is_excluded:\n                cleaned_list.append(field)\n\n        return cleaned_list", "response": "Function used to exclude defined fields from object collect."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_same_type_as_root(self, obj):\n        if not self.ALLOWS_SAME_TYPE_AS_ROOT_COLLECT:\n            obj_model = get_model_from_instance(obj)\n            obj_key = get_key_from_instance(obj)\n            is_same_type_as_root = obj_model == self.root_obj_model and obj_key != self.root_obj_key\n\n            if is_same_type_as_root:\n                self.emit_event(type='same_type_as_root', obj=obj)\n\n            return is_same_type_as_root\n        else:\n            return False", "response": "Tests if an object of the same type as root."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_by_threshold(self, objects, current_instance, field_name):\n        objs_count = len(objects)\n        if objs_count == 0:\n            return []\n        object_example = objects[0]\n\n        related_model_name = get_model_from_instance(object_example)\n        max_count = self.get_maximum_allowed_instances_for_model(related_model_name)\n        if objs_count > max_count:\n            self.emit_event(type='too_many_related_objects', obj=current_instance, related_model=related_model_name)\n            self.add_excluded_field(get_key_from_instance(current_instance), field_name,\n                                    related_model_name, objs_count, max_count)\n            return []\n\n        return objects", "response": "Filter the list of objects by a threshold."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_collect(self, obj):\n        if not self.ALLOWS_SAME_TYPE_AS_ROOT_COLLECT:\n            for field in self.get_local_fields(obj):\n                if isinstance(field, ForeignKey) and not field.unique:\n                    # Relative field's API has been changed Django 2.0\n                    # See https://docs.djangoproject.com/en/2.0/releases/1.9/#field-rel-changes for details\n                    if django.VERSION[0] == 2:\n                        remote_model = field.remote_field.model\n                    else:\n                        remote_model = field.rel.to\n                    if isinstance(self.root_obj, remote_model):\n                        setattr(obj, field.name, self.root_obj)", "response": "This method is called when the object is created to collect the related items of the same type as the root model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the object from loaded data", "response": "def initialize(self, data):\n        \"\"\" initialize variable from loaded data \"\"\"\n        for item in data:\n            if hasattr(self, item):\n                setattr(self, item, data[item])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the contents of the README file.", "response": "def parse_readme():\n    \"\"\"Parse contents of the README.\"\"\"\n    # Get the long description from the relevant file\n    here = os.path.abspath(os.path.dirname(__file__))\n    readme_path = os.path.join(here, 'README.md')\n    with codecs.open(readme_path, encoding='utf-8') as handle:\n        long_description = handle.read()\n\n    return long_description"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_transcripts(self, variant_obj, info_dict):\n        vep_string = info_dict.get('CSQ')\n\n        #Check if snpeff annotation:\n        snpeff_string = info_dict.get('ANN')\n        \n        # We check one of these.\n        # VEP has presedence over snpeff\n        if vep_string:\n            #Get the vep annotations\n            vep_info = get_vep_info(\n                vep_string = vep_string,\n                vep_header = self.vep_header\n                )\n            for transcript_info in vep_info:\n                transcript = self._get_vep_transcript(transcript_info)\n                variant_obj.add_transcript(transcript)\n\n        elif snpeff_string:\n            #Get the vep annotations\n            snpeff_info = get_snpeff_info(\n                snpeff_string = snpeff_string,\n                snpeff_header = self.snpeff_header\n                )\n            for transcript_info in snpeff_info:\n                transcript = self._get_snpeff_transcript(transcript_info)\n                variant_obj.add_transcript(transcript)", "response": "Add transcripts from the vep file to the variant object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_vep_transcript(self, transcript_info):\n        transcript = Transcript(\n                hgnc_symbol = transcript_info.get('SYMBOL'),\n                transcript_id = transcript_info.get('Feature'),\n                ensembl_id = transcript_info.get('Gene'),\n                biotype = transcript_info.get('BIOTYPE'),\n                consequence = transcript_info.get('Consequence'),\n                strand = transcript_info.get('STRAND'),\n                sift = transcript_info.get('SIFT'),\n                polyphen = transcript_info.get('PolyPhen'),\n                exon = transcript_info.get('EXON'),\n                HGVSc = transcript_info.get('HGVSc'),\n                HGVSp = transcript_info.get('HGVSp'),\n                GMAF = transcript_info.get('GMAF'),\n                ExAC_MAF = transcript_info.get('ExAC_MAF')\n            )\n        return transcript", "response": "Create a Transcript object based on the vep annotation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_snpeff_transcript(self, transcript_info):\n        transcript = Transcript(\n                hgnc_symbol = transcript_info.get('Gene_Name'),\n                transcript_id = transcript_info.get('Feature'),\n                ensembl_id = transcript_info.get('Gene_ID'),\n                biotype = transcript_info.get('Transcript_BioType'),\n                consequence = transcript_info.get('Annotation'),\n                exon = transcript_info.get('Rank'),\n                HGVSc = transcript_info.get('HGVS.c'),\n                HGVSp = transcript_info.get('HGVS.p')\n            )\n        return transcript", "response": "Create a Transcript object based on the snpeff annotation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _makes_clone(_func, *args, **kw):\n    self = args[0]._clone()\n    _func(self, *args[1:], **kw)\n    return self", "response": "A decorator that returns a clone of the current object so that it can be re - used for similar requests."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_response(self, response, data):\n        # Content-Type headers can include additional parameters(RFC 1521), so\n        # we split on ; to match against only the type/subtype\n        if data and response.get('content-type', '').split(';')[0] in (\n            'application/json', \n            'application/x-javascript',\n            'text/javascript',\n            'text/x-javascript',\n            'text/x-json'\n        ):\n            return json.loads(data)\n        else:\n            return data", "response": "Handles the response from the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a body to the request.", "response": "def with_body(self, body=None, **params):\n        \"\"\"\n        Add a body to the request.\n\n        When `body` is a:\n            - string, it will be used as is. \n            - dict or list of (key, value) pairs, it will be form encoded\n            - None, remove request body\n            - anything else, a TypeError will be raised \n            \n        If `body` is a dict or None you can also pass in keyword\n        arguments to add to the body.\n\n        ::\n            >>> dolt.with_body(dict(key='val'), foo='bar')._body\n            'foo=bar&key=val'\n        \"\"\"\n\n        if isinstance(body, (tuple, list)):\n            body = dict(body)\n\n        if params:\n            # Body must be None or able to be a dict\n            if isinstance(body, dict):\n                body.update(params)\n            elif body is None:\n                body = params\n            else:\n                raise ValueError('Body must be None or a dict if used with params, got: %r' % body)\n\n        if isinstance(body, basestring):\n            self._body = body\n        elif isinstance(body, dict):\n            self._body = urllib.urlencode(body)\n        elif body is None:\n            self._body = None\n        else:\n            raise TypeError('Invalid body type %r' % body)        \n        \n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef with_json(self, data=None, **params):\n        if isinstance(data, (tuple, list)):\n            data = dict(data)\n\n        if params:\n            # data must be None or able to be a dict\n            if isinstance(data, dict):\n                data.update(params)\n            elif data is None:\n                data = params\n            else:\n                raise ValueError('Data must be None or a dict if used with params, got: %r' % data)\n\n        req = self.with_headers({'Content-Type': 'application/json', 'Accept': 'application/json'})\n        if isinstance(data, basestring):\n            # Looks like it's already been encoded\n            return req.with_body(data)\n        else:\n            return req.with_body(json.dumps(data))", "response": "Add a json body to the request. \n "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef with_headers(self, headers=None, **params):\n        if isinstance(headers, (tuple, list)):\n            headers = dict(headers)\n\n        if params:\n            if isinstance(headers, dict):\n                headers.update(params)\n            elif headers is None:\n                headers = params\n\n        self._headers.update(headers)\n        return self", "response": "Add headers to the request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_url(self, *paths, **params):\n        path_stack = self._attribute_stack[:]\n        if paths:\n            path_stack.extend(paths)\n\n        u = self._stack_collapser(path_stack)\n        url = self._url_template % {\n            \"domain\": self._api_url,\n            \"generated_url\" : u,\n        }\n\n        if self._params or params:\n            internal_params = self._params.copy()\n            internal_params.update(params)\n            url += self._generate_params(internal_params)\n\n        return url", "response": "Returns the URL for this request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _clone(self):\n        cls = self.__class__\n        q = cls.__new__(cls)\n        q.__dict__ = self.__dict__.copy()\n        q._params = self._params.copy()\n        q._headers = self._headers.copy()\n        q._attribute_stack = self._attribute_stack[:]\n\n        return q", "response": "Returns a copy of the current state of the current operation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a case or individual from the database.", "response": "def delete(ctx, family_id, individual_id, root):\n    \"\"\"\n    Delete a case or individual from the database.\n\n    If no database was found run puzzle init first.\n    \"\"\"\n    root = root or ctx.obj.get('root') or os.path.expanduser(\"~/.puzzle\")\n\n    if os.path.isfile(root):\n        logger.error(\"'root' can't be a file\")\n        ctx.abort()\n\n    logger.info(\"Root directory is: {}\".format(root))\n\n    db_path = os.path.join(root, 'puzzle_db.sqlite3')\n    logger.info(\"db path is: {}\".format(db_path))\n\n    if not os.path.exists(db_path):\n        logger.warn(\"database not initialized, run 'puzzle init'\")\n        ctx.abort()\n\n    store = SqlStore(db_path)\n\n    if family_id:\n        case_obj = store.case(case_id=family_id)\n        if case_obj is None:\n            logger.warning(\"Family {0} does not exist in database\"\n                           .format(family_id))\n            ctx.abort()\n        store.delete_case(case_obj)\n    elif individual_id:\n        ind_obj = store.individual(ind_id=individual_id)\n        if ind_obj.ind_id != individual_id:\n            logger.warning(\"Individual {0} does not exist in database\"\n                           .format(individual_id))\n            ctx.abort()\n        store.delete_individual(ind_obj)\n    else:\n        logger.warning(\"Please provide a family or individual id\")\n        ctx.abort()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef variants(case_id):\n    filters = parse_filters()\n    values = [value for key, value in iteritems(filters)\n              if not isinstance(value, dict) and key != 'skip']\n    is_active = any(values)\n    variants, nr_of_variants = app.db.variants(\n        case_id,\n        skip=filters['skip'],\n        filters={\n            'gene_ids': filters['gene_symbols'],\n            'frequency': filters.get('frequency'),\n            'cadd': filters.get('cadd'),\n            'sv_len': filters.get('sv_len'),\n            'consequence': filters['selected_consequences'],\n            'genetic_models': filters['selected_models'],\n            'sv_types': filters['selected_sv_types'],\n            'gene_lists': filters['gene_lists'],\n            'impact_severities': filters['impact_severities'],\n            'gemini_query': filters['gemini_query'],\n            'range': filters['range'],\n        }\n    )\n    gene_lists = ([gene_list.list_id for gene_list in app.db.gene_lists()]\n                  if app.config['STORE_ENABLED'] else [])\n    queries = ([(query.name or query.query, query.query) for query\n                in app.db.gemini_queries()]\n               if app.config['STORE_ENABLED'] else [])\n    kwargs = dict(variants=variants, case_id=case_id, db=app.db,\n                  filters=filters, consequences=SO_TERMS,\n                  inheritance_models=INHERITANCE_MODELS_SHORT,\n                  gene_lists=gene_lists, impact_severities=IMPACT_LEVELS,\n                  is_active=is_active, nr_of_variants=nr_of_variants,\n                  queries=queries)\n\n    if app.db.variant_type == 'sv':\n        return render_template('sv_variants.html', sv_types=SV_TYPES, **kwargs)\n    else:\n        return render_template('variants.html', **kwargs)", "response": "Show all variants for a given case."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef variant(case_id, variant_id):\n    case_obj = app.db.case(case_id)\n    variant = app.db.variant(case_id, variant_id)\n    if variant is None:\n        return abort(404, \"variant not found\")\n\n    comments = app.db.comments(variant_id=variant.md5)\n    template = 'sv_variant.html' if app.db.variant_type == 'sv' else 'variant.html'\n    return render_template(template, variant=variant, case_id=case_id,\n                           comments=comments, case=case_obj)", "response": "Show a single variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses variant filters from the request object.", "response": "def parse_filters():\n    \"\"\"Parse variant filters from the request object.\"\"\"\n    genes_str = request.args.get('gene_symbol')\n    filters = {}\n    for key in ('frequency', 'cadd', 'sv_len'):\n        try:\n            filters[key] = float(request.args.get(key))\n        except (ValueError, TypeError):\n            pass\n\n    filters['gene_symbols'] = genes_str.split(',') if genes_str else None\n    filters['selected_models'] = request.args.getlist('inheritance_models')\n    filters['selected_consequences'] = request.args.getlist('consequences')\n    filters['selected_sv_types'] = request.args.getlist('sv_types')\n    filters['skip'] = int(request.args.get('skip', 0))\n    filters['gene_lists'] = request.args.getlist('gene_lists')\n    filters['gemini_query'] = (request.args.get('gemini_query') or\n                               request.args.get('preset_gemini_query'))\n    filters['impact_severities'] = request.args.getlist('impact_severities')\n    filters['range'] = None\n\n    if request.args.get('range'):\n        chromosome, raw_pos = request.args.get('range').split(':')\n        start, end = map(int, raw_pos.split('-'))\n        filters['range'] = {'chromosome': chromosome, 'start': start,\n                            'end': end}\n\n    filters['query_dict'] = {key: request.args.getlist(key) for key\n                             in request.args.keys()}\n    filters['query_dict'].update({'skip': (filters['skip'] + 30)})\n\n    return filters"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef suspects(case_id, variant_id):\n    case_obj = app.db.case(case_id)\n    variant_obj = app.db.variant(case_id, variant_id)\n    app.db.add_suspect(case_obj, variant_obj)\n    return redirect(request.referrer)", "response": "Pin a variant as a suspect for a given case."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef queries():\n    query = request.form['query']\n    name = request.form.get('name')\n    app.db.add_gemini_query(name, query)\n    return redirect(request.referrer)", "response": "Store a new GEMINI query."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform a function to a sequence of functions.", "response": "def _constexpr_transform(fn):\n    \"\"\"\n    >>> from Redy.Opt.ConstExpr import constexpr, const, optimize, macro\n    >>> import dis\n    >>> a = 1; b = \"\"; c = object()\n    >>> x = 1\n    >>> @optimize\n    >>> def f(y):\n    >>>     val1: const[int] = a\n    >>>     val2: const = b\n    >>>     if constexpr[x is c]:\n    >>>         return val1, y\n    >>>     elif constexpr[x is 1]:\n    >>>         return None, y\n    >>>     else:\n    >>>         return val2, y\n    >>> assert f(1) == (None, 1)\n    >>> dis.dis(f)\n    >>> @optimize\n    >>> def f(x):\n    >>>     d: const = 1\n    >>>     return x + d + constexpr[2]\n    #\n    #\n    >>> dis.dis(f)\n    >>> print('result:', f(1))\n    #\n    #\n    >>> @optimize\n    >>> def f(z):\n    >>>     @macro\n    >>>     def g(a):\n    >>>         x = a + 1\n    #\n    >>>     g(z)\n    >>>     return x\n    #\n    #\n    >>> dis.dis(f)\n    >>> print('result:', f(1))\n    #\n    >>> c = 10\n    #\n    #\n    >>> @optimize\n    >>> def f(x):\n    >>>     if constexpr[1 + c < 10]:\n    >>>         return x + 1\n    >>>     else:\n    >>>         return x - 1\n    #\n    #\n    >>> print(dis.dis(f))\n    >>> print(f(5))\n    #\n    >>> @optimize\n    >>> def f(x):\n    >>>     return (x + constexpr[c * 20]) if constexpr[c > 10] else  constexpr[c - 2]\n    #\n    >>> dis.dis(f)\n    >>> print(f(20))\n\n    >>> def g(lst: list):\n    >>>    k = 1\n\n    >>>    @optimize\n    >>>    def _():\n    >>>        nonlocal k\n    >>>        f: const = lst.append\n    >>>        for i in range(1000):\n    >>>            f(i)\n    >>>        k += 1\n    >>>        f(k)\n\n    >>>    _()\n\n    >>>    return lst\n\n\n    >>> # dis.dis(g)\n    >>> print(g([]))\n\n    \"\"\"\n    code_string = inspect.getsource(fn)\n    while _s.match(code_string):\n        code_string = textwrap.dedent(code_string)\n\n    module = ast.parse(code_string)\n    fn_ast = module.body[0]\n    fn: types.FunctionType\n    fn_name = fn.__name__\n\n    closure = fn.__closure__\n    closure_dict = {v: c.cell_contents for v, c in zip(fn.__code__.co_freevars, closure if closure else ())}\n    ctx = CompilingTimeMapping(fn.__globals__, closure_dict)\n    ce = ConstExpr(ctx, [], OrderedDict(), {}, [], fn.__code__.co_filename)\n    body = fn_ast.body\n\n    macro_def = new_transformer(ce, MazcroDef)\n    macro_invoke = new_transformer(ce, MacroInvoke)\n    const_def = new_transformer(ce, ConstExprConstDef)\n    const_if = new_transformer(ce, ConstExprIf)\n    name_fold = new_transformer(ce, ConstExprNameFold)\n\n    body = _visit_suite(macro_def.visit, body)\n    body = _visit_suite(macro_invoke.visit, body)\n    body = _visit_suite(const_def.visit, body)\n    body = _visit_suite(const_if.visit, body)\n    body = _visit_suite(name_fold.visit, body)\n    fn_ast.body = body\n    module.body = [fn_ast]\n    code = compile(module, \"<const-optimize>\", \"exec\")\n    fn_code: types.CodeType = next(\n            each for each in code.co_consts if isinstance(each, types.CodeType) and each.co_name == fn_name)\n\n    fn_code = const_link(fn_code, ce.constant_symbols, ce.additional_consts, fn.__code__, ce.nonlocal_names)\n\n    new_fn = types.FunctionType(fn_code, fn.__globals__, fn.__name__, fn.__defaults__, fn.__closure__)\n\n    new_fn.__annotations__ = fn.__annotations__\n    new_fn.__doc__ = fn.__doc__\n    new_fn.__kwdefaults__ = fn.__kwdefaults__\n    new_fn.__module__ = fn.__module__\n    return new_fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(fp, object_pairs_hook=dict):\n    return object_pairs_hook((k,v) for k,v,_ in parse(fp) if k is not None)", "response": "Loads the contents of the. properties file - like objectfp into a dictionary of key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(s, object_pairs_hook=dict):\n    fp = BytesIO(s) if isinstance(s, binary_type) else StringIO(s)\n    return load(fp, object_pairs_hook=object_pairs_hook)", "response": "Parses the contents of the string s as a simple line - oriented ASCII - 1 file and returns a dict of the key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the contents of the ~io. IOBase. readLine - supportinging file - like objectfp and return a generator of tuples each containing the key value and original_lines of the line in the file.", "response": "def parse(fp):\n    \"\"\"\n    Parse the contents of the `~io.IOBase.readline`-supporting file-like object\n    ``fp`` as a simple line-oriented ``.properties`` file and return a\n    generator of ``(key, value, original_lines)`` triples for every entry in\n    ``fp`` (including duplicate keys) in order of occurrence.  The third\n    element of each triple is the concatenation of the unmodified lines in\n    ``fp`` (including trailing newlines) from which the key and value were\n    extracted.  The generator also includes comments and blank/all-whitespace\n    lines found in ``fp``, one triple per line, with the first two elements of\n    the triples set to `None`.  This is the only way to extract comments from a\n    ``.properties`` file with this library.\n\n    ``fp`` may be either a text or binary filehandle, with or without universal\n    newlines enabled.  If it is a binary filehandle, its contents are decoded\n    as Latin-1.\n\n    .. versionchanged:: 0.5.0\n        Invalid ``\\\\uXXXX`` escape sequences will now cause an\n        `InvalidUEscapeError` to be raised\n\n    :param fp: the file from which to read the ``.properties`` document\n    :type fp: file-like object\n    :rtype: generator of triples of text strings\n    :raises InvalidUEscapeError: if an invalid ``\\\\uXXXX`` escape sequence\n        occurs in the input\n    \"\"\"\n    def lineiter():\n        while True:\n            ln = fp.readline()\n            if isinstance(ln, binary_type):\n                ln = ln.decode('iso-8859-1')\n            if ln == '':\n                return\n            for l in ascii_splitlines(ln):\n                yield l\n    liter = lineiter()\n    for source in liter:\n        line = source\n        if re.match(r'^[ \\t\\f]*(?:[#!]|\\r?\\n?$)', line):\n            yield (None, None, source)\n            continue\n        line = line.lstrip(' \\t\\f').rstrip('\\r\\n')\n        while re.search(r'(?<!\\\\)(?:\\\\\\\\)*\\\\$', line):\n            line = line[:-1]\n            nextline = next(liter, '')\n            source += nextline\n            line += nextline.lstrip(' \\t\\f').rstrip('\\r\\n')\n        if line == '':  # series of otherwise-blank lines with continuations\n            yield (None, None, source)\n            continue\n        m = re.search(r'(?<!\\\\)(?:\\\\\\\\)*([ \\t\\f]*[=:]|[ \\t\\f])[ \\t\\f]*', line)\n        if m:\n            yield (unescape(line[:m.start(1)]),unescape(line[m.end():]),source)\n        else:\n            yield (unescape(line), '', source)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding escape sequences in a ``.properties`` key or value. The following escape sequences are recognized:: \\\\t \\\\n \\\\f \\\\r \\\\uXXXX \\\\\\\\ If a backslash is followed by any other character, the backslash is dropped. In addition, any valid UTF-16 surrogate pairs in the string after escape-decoding are further decoded into the non-BMP characters they represent. (Invalid & isolated surrogate code points are left as-is.) .. versionchanged:: 0.5.0 Invalid ``\\\\uXXXX`` escape sequences will now cause an `InvalidUEscapeError` to be raised :param field: the string to decode :type field: text string :rtype: text string :raises InvalidUEscapeError: if an invalid ``\\\\uXXXX`` escape sequence occurs in the input", "response": "def unescape(field):\n    \"\"\"\n    Decode escape sequences in a ``.properties`` key or value.  The following\n    escape sequences are recognized::\n\n        \\\\t \\\\n \\\\f \\\\r \\\\uXXXX \\\\\\\\\n\n    If a backslash is followed by any other character, the backslash is\n    dropped.\n\n    In addition, any valid UTF-16 surrogate pairs in the string after\n    escape-decoding are further decoded into the non-BMP characters they\n    represent.  (Invalid & isolated surrogate code points are left as-is.)\n\n    .. versionchanged:: 0.5.0\n        Invalid ``\\\\uXXXX`` escape sequences will now cause an\n        `InvalidUEscapeError` to be raised\n\n    :param field: the string to decode\n    :type field: text string\n    :rtype: text string\n    :raises InvalidUEscapeError: if an invalid ``\\\\uXXXX`` escape sequence\n        occurs in the input\n    \"\"\"\n    return re.sub(r'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]', _unsurrogate,\n                  re.sub(r'\\\\(u.{0,4}|.)', _unesc, field))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract clips according to the specification file or string.", "response": "def extractClips(self, specsFilePathOrStr, outputDir=None, zipOutput=False):\n        \"\"\"Extract clips according to the specification file or string.\n        \n        Arguments:\n            specsFilePathOrStr (str): Specification file path or string\n            outputDir (str): Location of the extracted clips\n            zipOutput (bool): Archive extracted clips' flag\n\n        Specifications format:\n            <begin:seconds> <end:seconds> [<text_metadata>]\n\n            20.5    59.75   Discussion about dogs\n            105.3   200.3   Cat story\n\n        Notes:\n            <text_metadata> is completely optional\n        \"\"\"\n        clips = SpecsParser.parse(specsFilePathOrStr)\n\n        # Output to current working directory if no outputDir was provided\n        if not outputDir:\n            outputDir = os.path.abspath('.')\n\n        zipFile = None\n        if zipOutput:\n            bname = os.path.splitext(os.path.basename(specsFilePathOrStr))[0]\n            zipPath = \"%s_clips.zip\" % bname\n            zipFile = zipfile.ZipFile(os.path.join(outputDir, zipPath), mode='w')\n\n        for i, clip in enumerate(clips):\n            # 13 clips => clip01.mp3, clip12.mp3...\n            filenameFormat = 'clip%%0%dd.mp3' % len(str(len(clips)))\n\n            filepath = os.path.join(outputDir, filenameFormat % (i+1))\n\n            clipData = self._extractClipData(clip)\n\n            with open(filepath, 'wb') as f_out:\n                f_out.write(clipData)\n\n            if zipFile:\n                zipFile.write(filepath, arcname=os.path.basename(filepath))\n                os.unlink(filepath)\n\n        if zipFile:\n            zipFile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extractClipData(self, audioClipSpec, showLogs=False):\n        command = [self._ffmpegPath]\n\n        if not showLogs:\n            command += ['-nostats', '-loglevel', '0']\n\n        command += [\n            '-i', self._audioFilePath,\n            '-ss', '%.3f' % audioClipSpec.start,\n            '-t', '%.3f' % audioClipSpec.duration(),\n            '-c', 'copy',\n            '-map', '0',\n            '-acodec', 'libmp3lame',\n            '-ab', '128k',\n            '-f', 'mp3'\n        ]\n\n        # Add clip TEXT as metadata and set a few more to default\n        metadata = { self._textMetadataName: audioClipSpec.text }\n\n        for k, v in metadata.items():\n            command.append('-metadata')\n            command.append(\"{}='{}'\".format(k, v))\n\n        command.append('pipe:1')\n\n        return subprocess.check_output(command)", "response": "Extracts a single clip according to audioClipSpec."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_phenotype(self, ind_obj, phenotype_id):\n        if phenotype_id.startswith('HP:') or len(phenotype_id) == 7:\n            logger.debug('querying on HPO term')\n            hpo_results = phizz.query_hpo([phenotype_id])\n        else:\n            logger.debug('querying on OMIM term')\n            hpo_results = phizz.query_disease([phenotype_id])\n\n        added_terms = [] if hpo_results else None\n        existing_ids = set(term.phenotype_id for term in ind_obj.phenotypes)\n        for result in hpo_results:\n            if result['hpo_term'] not in existing_ids:\n                term = PhenotypeTerm(phenotype_id=result['hpo_term'],\n                                     description=result['description'])\n                logger.info('adding new HPO term: %s', term.phenotype_id)\n                ind_obj.phenotypes.append(term)\n                added_terms.append(term)\n\n        logger.debug('storing new HPO terms')\n        self.save()\n\n        if added_terms is not None and len(added_terms) > 0:\n            for case_obj in ind_obj.cases:\n                self.update_hpolist(case_obj)\n\n        return added_terms", "response": "Add a phenotype term to the case."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the HPO gene list for a case based on current terms.", "response": "def update_hpolist(self, case_obj):\n        \"\"\"Update the HPO gene list for a case based on current terms.\"\"\"\n        hpo_list = self.case_genelist(case_obj)\n        hpo_results = hpo_genes(case_obj.phenotype_ids(),\n                                *self.phenomizer_auth)\n\n        if hpo_results is None:\n            pass\n            # Why raise here?\n            # raise RuntimeError(\"couldn't link to genes, try again\")\n        else:\n            gene_ids = [result['gene_id'] for result in hpo_results\n                        if result['gene_id']]\n            hpo_list.gene_ids = gene_ids\n            self.save()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_phenotype(self, ind_obj, phenotypes=None):\n        if phenotypes is None:\n            logger.info(\"delete all phenotypes related to %s\", ind_obj.ind_id)\n            self.query(PhenotypeTerm).filter_by(ind_id=ind_obj.id).delete()\n        else:\n            for term in ind_obj.phenotypes:\n                if term.phenotype_id in phenotypes:\n                    logger.info(\"delete phenotype: %s from %s\",\n                                term.phenotype_id, ind_obj.ind_id)\n                    self.session.delete(term)\n        logger.debug('persist removals')\n        self.save()\n        for case_obj in ind_obj.cases:\n            self.update_hpolist(case_obj)", "response": "Remove multiple phenotypes from an individual."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef data(cls_def: type):\n    __annotations__ = cls_def.__annotations__\n\n    def constructor_descriptor(f):\n        return discrete_cache(f) if issubclass(cls_def, Discrete) else f\n\n    __dict__ = {}\n    for each in reversed(cls_def.__mro__):\n        __dict__.update({k: v for k, v in each.__dict__.items() if v})\n\n    new_cls_def = type(cls_def.__name__, cls_def.__bases__, __dict__)\n\n    new_cls_def.__slots__ = ['__sig_str__', '__structure__']\n\n    if not hasattr(new_cls_def, '__repr__') or isinstance(new_cls_def.__repr__, BuiltinMethod):\n        def __repr__(self):\n            s = self.__structure__\n            if not isinstance(s, tuple):\n                return self.__sig_str__\n            return f'{self.__sig_str__}({\", \".join(tuple(map(repr, self.__structure__[1:])))})'\n\n        new_cls_def.__repr__ = __repr__\n\n    @property\n    def __inst_str__(self):\n        return self.__sig_str__\n\n    @__inst_str__.setter\n    def __inst_str__(self, value):\n        warnings.warn('`__inst_str__` of ADT will not removed after Redy 0.3!')\n        self.__sig_str__ = value\n\n    new_cls_def.__inst_str__ = __inst_str__\n\n    def __destruct__(self):\n        return self.__structure__\n\n    def __iter__(self):\n        yield from self.__structure__\n\n    new_cls_def.__destruct__ = __destruct__\n    new_cls_def.__iter__ = __iter__\n\n    def make_type(str_value: object, default: str):\n        entity = new_cls_def()\n        entity.__sig_str__ = default if str_value is ... else str_value\n        entity.__structure__ = None\n\n        return entity\n\n    # if Discrete, it means that for a definite input, the return of data constructor is definitely the same object.\n\n    for each, annotation in __annotations__.items():\n        if callable(annotation):\n            spec = getfullargspec(annotation)\n            if isinstance(annotation, DTTransDescriptor):\n                _args = \", \".join(getargs(annotation.func.__code__).args)\n                code: types.CodeType = annotation.func.__code__\n                impl = (f\"def call({_args}):\\n\"\n                        f\"    entity = new_cls_def()\\n\"\n                        f\"    __structure__, __sig_str__ = annotation.func({_args})\\n\"\n                        f\"    entity.__sig_str__ = {repr(each)} if __sig_str__  is ... else __sig_str__\\n\"\n                        f\"    entity.__structure__ = cons, *__structure__\\n\"\n                        f\"    return entity\\n\")\n            else:\n                _args = \", \".join(getargs(annotation.__code__).args)\n                code: types.CodeType = annotation.__code__\n                impl = (f\"def call({_args}):\\n\"\n                        f\"    entity = new_cls_def()\\n\"\n                        f\"    str_value = annotation({_args})\\n\"\n                        f\"    entity.__sig_str__ = {repr(each)} if str_value is ... else str_value\\n\"\n                        f\"    entity.__structure__ = cons, {_args}\\n\"\n                        f\"    return entity\")\n            if spec.defaults or spec.kwonlyargs or spec.varkw:\n                raise TypeError('An ADT constructor must be a lambda without default args or keyword args.')\n\n            scope = {'annotation': annotation, 'new_cls_def': new_cls_def}\n            exec(compile(f\"{impl}\", \"Redy.ADT.Core.py\", \"exec\", 0, False), scope)\n            _singleton_inst = scope['call']\n\n            maker = new_func_maker(_singleton_inst)\n            _singleton_inst = maker(filename=code.co_filename, firstlineno=code.co_firstlineno, lnotab=code.co_lnotab, name=each)\n\n            singleton_inst = constructor_descriptor(_singleton_inst)\n            scope['cons'] = singleton_inst\n\n        else:\n            singleton_inst = make_type(annotation, each)\n\n        setattr(new_cls_def, each, singleton_inst)\n\n    return new_cls_def", "response": "A simple data structure for a given class."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if mode_lst matches obj.", "response": "def match(mode_lst: list, obj: 'object that has __destruct__ method'):\n    \"\"\"\n    >>> from Redy.ADT.Core import match, data, P\n    >>> from Redy.ADT.traits import ConsInd, Discrete\n    >>> @data\n    >>> class List(ConsInd, Discrete):\n    >>>     # ConsInd(index following constructing)\n    >>>     #    |-> Ind;\n    >>>     # Discrete\n    >>>     #    |-> Im(Immutable), Eq\n    >>>     Nil : ...\n    >>>     Cons: lambda head, tail: ...\n    >>> lst = List.Cons(2, List.Cons(1, List.Nil))\n    >>> mode_lst = P[List.Cons, P, P[List.Cons, 1]]\n    >>> if match(mode_lst,  lst):\n    >>>     assert mode_lst == [List.Cons, 2, [List.Cons, 1]]\n    \"\"\"\n    # noinspection PyUnresolvedReferences\n    try:\n        # noinspection PyUnresolvedReferences\n        structure = obj.__destruct__()\n    except AttributeError:\n        return False\n    n = len(mode_lst)\n    if n > len(structure):\n        return False\n\n    for i in range(n):\n        mode = mode_lst[i]\n        # noinspection PyUnresolvedReferences\n        elem = obj[i]\n        if isinstance(mode, PatternList):\n            if not match(mode, elem):\n                return False\n        elif mode is P:\n            # noinspection PyUnresolvedReferences\n            mode_lst[i] = elem\n        elif mode is any:\n            pass\n        elif mode != elem:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gemini_query(self, query_id):\n        logger.debug(\"Looking for query with id {0}\".format(query_id))\n        return self.query(GeminiQuery).filter_by(id=query_id).first()", "response": "Returns a gemini query"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_gemini_query(self, name, query):\n        logger.info(\"Adding query {0} with text {1}\".format(name, query))\n        new_query = GeminiQuery(name=name, query=query)\n        self.session.add(new_query)\n        self.save()\n        return new_query", "response": "Add a user defined gemini query to the session"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_gemini_query(self, query_id):\n        query_obj = self.gemini_query(query_id)\n        logger.debug(\"Delete query: {0}\".format(query_obj.name_query))\n        self.session.delete(query_obj)\n        self.save()", "response": "Delete a gemini query"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef row(self):\n        row = OrderedDict()\n        row['retro_game_id'] = self.retro_game_id\n        row['home_team_id'] = self.home_team_id\n        row['away_team_id'] = self.away_team_id\n        for b in self.home_batting:\n            if not b['starting']:\n                continue\n            row['home_lineup_{bo}_id'.format(**b)] = b.get('id')\n            row['home_lineup_{bo}_name'.format(**b)] = b.get('box_name')\n            row['home_lineup_{bo}_pos'.format(**b)] = b.get('pos')\n        row['home_batter'] = json.dumps(self.home_batting)\n        row['home_pitcher'] = json.dumps(self.home_pitching)\n        for b in self.away_batting:\n            if not b['starting']:\n                continue\n            row['away_lineup_{bo}_id'.format(**b)] = b.get('id')\n            row['away_lineup_{bo}_name'.format(**b)] = b.get('box_name')\n            row['away_lineup_{bo}_pos'.format(**b)] = b.get('pos')\n        row['away_batter'] = json.dumps(self.away_batting)\n        row['away_pitcher'] = json.dumps(self.away_pitching)\n        return row", "response": "Return a dict containing the current row of the log."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading xml object :param url: contents url :param features: markup provider :param game: MLBAM Game object :param players: MLBAM Players object :return: pitchpx.box_score.box_score.BoxScore object", "response": "def read_xml(cls, url, features, game, players):\n        \"\"\"\n        read xml object\n        :param url: contents url\n        :param features: markup provider\n        :param game: MLBAM Game object\n        :param players: MLBAM Players object\n        :return: pitchpx.box_score.box_score.BoxScore object\n        \"\"\"\n        soup = MlbamUtil.find_xml(\"\".join([url, cls.FILENAME]), features)\n        return cls._generate_object(soup, game, players)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates object from soup game and players", "response": "def _generate_object(cls, soup, game, players):\n        \"\"\"\n        get box_score data\n        :param soup: Beautifulsoup object\n        :param game: MLBAM Game object\n        :param players: MLBAM Players object\n        :return: pitchpx.box_score.box_score.BoxScore object\n        \"\"\"\n\n        def get_batter(soup, team_flag):\n            batting = soup.find('batting', attrs={'team_flag': team_flag})\n            if batting:\n                return batting.find_all('batter')\n            return []\n\n        def get_pitcher(soup, team_flag):\n            pitching = soup.find('pitching', attrs={'team_flag': team_flag})\n            if pitching:\n                return pitching.find_all('pitcher')\n            return []\n\n        box_score = BoxScore(game, players)\n\n        box_score.retro_game_id = game.retro_game_id\n        box_score.home_team_id = game.home_team_id\n        box_score.away_team_id = game.away_team_id\n        box_score.home_batting = [box_score._get_batter(b) for b in get_batter(soup, 'home')]\n        box_score.away_batting = [box_score._get_batter(b) for b in get_batter(soup, 'away')]\n        box_score.home_pitching = [box_score._get_pitcher(p) for p in get_pitcher(soup, 'home')]\n        box_score.away_pitching = [box_score._get_pitcher(p) for p in get_pitcher(soup, 'away')]\n\n        return box_score"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_batter(self, batter):\n        values = OrderedDict()\n        player = self.players.rosters.get(batter.get('id'))\n        bo, starting = self._get_batting_order_starting_flg(batter)\n        values['bo'] = bo\n        values['pos'] = batter.get('pos', MlbamConst.UNKNOWN_SHORT)\n        values['id'] = batter.get('id', MlbamConst.UNKNOWN_SHORT)\n        values['first'] = player.first\n        values['last'] = player.last\n        values['box_name'] = player.box_name\n        values['rl'] = player.rl\n        values['bats'] = player.bats\n        values['starting'] = starting\n        return values", "response": "get batter object\n        :param batter: Beautifulsoup object(batter element)\n        :return: batter(dict)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_batting_order_starting_flg(cls, batter):\n        bo = batter.get('bo', None)\n        if not bo or len(bo) != 3:\n            return False, False\n        batting_order = bo[:1]\n        starting = True if bo[1:3] == '00' else False\n        return batting_order, starting", "response": "get batting order and starting member flg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_pitcher(self, pitcher):\n        values = OrderedDict()\n        player = self.players.rosters.get(pitcher.get('id'))\n        values['pos'] = pitcher.get('pos', MlbamConst.UNKNOWN_SHORT)\n        values['id'] = pitcher.get('id', MlbamConst.UNKNOWN_SHORT)\n        values['first'] = player.first\n        values['last'] = player.last\n        values['box_name'] = player.box_name\n        values['rl'] = player.rl\n        values['bats'] = player.bats\n        values['out'] = pitcher.get('out', MlbamConst.UNKNOWN_SHORT)\n        values['bf'] = pitcher.get('bf', MlbamConst.UNKNOWN_SHORT)\n        return values", "response": "get pitcher object\n        :param pitcher: Beautifulsoup object(pitcher element)\n        :return: pitcher(dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nincrements the version number with n at index i.", "response": "def increment(self, version_number_idx: int, increment: int):\n        \"\"\"\n        increment the version number with `n` at index `i`.\n        >>> from Redy.Tools.Version import Version\n        >>> a = Version('1.0.0.2')\n        >>> a.increment(version_number_idx=2, increment=1)\n        >>> assert a == ('1.0.1.0')\n        >>> print(a[0])\n        >>> a[0] = 2\n        \"\"\"\n        _numbers = list(self._numbers)\n        _numbers[version_number_idx] += increment\n        for each in range(version_number_idx + 1, len(_numbers)):\n            _numbers[each] = 0\n        self._numbers = tuple(_numbers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef distance_to(self, point, unit='km'):\n        assert isinstance(point, GeoPoint), (\n            'Other point should also be a Point instance.'\n        )\n        if self == point:\n            return 0.0\n        coefficient = 69.09\n        theta = self.longitude - point.longitude\n        unit = unit.lower() if unit else None\n\n        distance = math.degrees(math.acos(\n            math.sin(self.rad_latitude) * math.sin(point.rad_latitude) +\n            math.cos(self.rad_latitude) * math.cos(point.rad_latitude) *\n            math.cos(math.radians(theta))\n        )) * coefficient\n\n        if unit == 'km':\n            return utils.mi_to_km(distance)\n\n        return distance", "response": "Calculate distance between current and other point."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rad_longitude(self):\n        if self._rad_longitude is None:\n            self._rad_longitude = math.radians(self.longitude)\n        return self._rad_longitude", "response": "Lazy conversion degrees longitude to radians."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(term, stream):\n    payload = erlang.term_to_binary(term)\n    header = struct.pack('!I', len(payload))\n    stream.write(header)\n    stream.write(payload)\n    stream.flush()", "response": "Write an Erlang term to an output stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recv(stream):\n    header = stream.read(4)\n    if len(header) != 4:\n        return None # EOF\n    (length,) = struct.unpack('!I', header)\n    payload = stream.read(length)\n    if len(payload) != length:\n        return None\n    term = erlang.binary_to_term(payload)\n    return term", "response": "Read an Erlang term from an input stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields Erlang terms from an input stream.", "response": "def recv_loop(stream):\n    \"\"\"Yield Erlang terms from an input stream.\"\"\"\n    message = recv(stream)\n    while message:\n        yield message\n        message = recv(stream)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the genotype calls for the variant.", "response": "def _add_genotype_calls(self, variant_obj, variant_line, case_obj):\n        \"\"\"Add the genotype calls for the variant\n\n        Args:\n            variant_obj (puzzle.models.Variant)\n            variant_dict (dict): A variant dictionary\n            case_obj (puzzle.models.Case)\n\n        \"\"\"\n        variant_line = variant_line.split('\\t')\n        #if there is gt calls we have no individuals to add\n        if len(variant_line) > 8:\n            gt_format = variant_line[8].split(':')\n            for individual in case_obj.individuals:\n                sample_id = individual.ind_id\n                index = individual.ind_index\n\n                gt_call = variant_line[9+index].split(':')\n\n                raw_call = dict(zip(gt_format, gt_call))\n\n                genotype = Genotype(**raw_call)\n\n                variant_obj.add_individual(puzzle_genotype(\n                    sample_id = sample_id,\n                    genotype = genotype.genotype,\n                    case_id = case_obj.name,\n                    phenotype = individual.phenotype,\n                    ref_depth = genotype.ref_depth,\n                    alt_depth = genotype.alt_depth,\n                    genotype_quality = genotype.genotype_quality,\n                    depth = genotype.depth_of_coverage,\n                    supporting_evidence = genotype.supporting_evidence,\n                    pe_support = genotype.pe_support,\n                    sr_support = genotype.sr_support,\n                ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new conflict with a prepended prefix as a path.", "response": "def with_prefix(self, root_path):\n        \"\"\"Returns a new conflict with a prepended prefix as a path.\"\"\"\n        return Conflict(self.conflict_type, root_path + self.path, self.body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a series of key - value pairs to a file - like object.", "response": "def dump(props, fp, separator='=', comments=None, timestamp=True,\n         sort_keys=False):\n    \"\"\"\n    Write a series of key-value pairs to a file in simple line-oriented\n    ``.properties`` format.\n\n    :param props: A mapping or iterable of ``(key, value)`` pairs to write to\n        ``fp``.  All keys and values in ``props`` must be text strings.  If\n        ``sort_keys`` is `False`, the entries are output in iteration order.\n    :param fp: A file-like object to write the values of ``props`` to.  It must\n        have been opened as a text file with a Latin-1-compatible encoding.\n    :param separator: The string to use for separating keys & values.  Only\n        ``\" \"``, ``\"=\"``, and ``\":\"`` (possibly with added whitespace) should\n        ever be used as the separator.\n    :type separator: text string\n    :param comments: if non-`None`, ``comments`` will be written to ``fp`` as a\n        comment before any other content\n    :type comments: text string or `None`\n    :param timestamp: If neither `None` nor `False`, a timestamp in the form of\n        ``Mon Sep 02 14:00:54 EDT 2016`` is written as a comment to ``fp``\n        after ``comments`` (if any) and before the key-value pairs.  If\n        ``timestamp`` is `True`, the current date & time is used.  If it is a\n        number, it is converted from seconds since the epoch to local time.  If\n        it is a `datetime.datetime` object, its value is used directly, with\n        na\u00efve objects assumed to be in the local timezone.\n    :type timestamp: `None`, `bool`, number, or `datetime.datetime`\n    :param bool sort_keys: if true, the elements of ``props`` are sorted\n        lexicographically by key in the output\n    :return: `None`\n    \"\"\"\n    if comments is not None:\n        print(to_comment(comments), file=fp)\n    if timestamp is not None and timestamp is not False:\n        print(to_comment(java_timestamp(timestamp)), file=fp)\n    for k,v in itemize(props, sort_keys=sort_keys):\n        print(join_key_value(k, v, separator), file=fp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dumps(props, separator='=', comments=None, timestamp=True, sort_keys=False):\n    s = StringIO()\n    dump(props, s, separator=separator, comments=comments, timestamp=timestamp,\n         sort_keys=sort_keys)\n    return s.getvalue()", "response": "Serializes a series of key - value pairs into a simple simple\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a string to a. properties file comment.", "response": "def to_comment(comment):\n    \"\"\"\n    Convert a string to a ``.properties`` file comment.  All non-Latin-1\n    characters in the string are escaped using ``\\\\uXXXX`` escapes (after\n    converting non-BMP characters to surrogate pairs), a ``#`` is prepended to\n    the string, any CR LF or CR line breaks in the string are converted to LF,\n    and a ``#`` is inserted after any line break not already followed by a\n    ``#`` or ``!``.  No trailing newline is added.\n\n    >>> to_comment('They say foo=bar,\\\\r\\\\nbut does bar=foo?')\n    '#They say foo=bar,\\\\n#but does bar=foo?'\n\n    :param comment: the string to convert to a comment\n    :type comment: text string\n    :rtype: text string\n    \"\"\"\n    return '#' + re.sub(r'[^\\x00-\\xFF]', _esc,\n                        re.sub(r'\\n(?![#!])', '\\n#',\n                               re.sub(r'\\r\\n?', '\\n', comment)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef join_key_value(key, value, separator='='):\n    # Escapes `key` and `value` the same way as java.util.Properties.store()\n    return escape(key) \\\n        + separator \\\n        + re.sub(r'^ +', lambda m: r'\\ ' * m.end(), _base_escape(value))", "response": "r Join a key and value together into a single line suitable for adding to a. properties file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef java_timestamp(timestamp=True):\n    if timestamp is None or timestamp is False:\n        return ''\n    if isinstance(timestamp, datetime) and timestamp.tzinfo is not None:\n        timebits = timestamp.timetuple()\n        # Assumes `timestamp.tzinfo.tzname()` is meaningful/useful\n        tzname = timestamp.tzname()\n    else:\n        if timestamp is True:\n            timestamp = None\n        elif isinstance(timestamp, datetime):\n            try:\n                # Use `datetime.timestamp()` if it's available, as it (unlike\n                # `datetime.timetuple()`) takes `fold` into account for na\u00efve\n                # datetimes\n                timestamp = timestamp.timestamp()\n            except AttributeError:  # Pre-Python 3.3\n                # Mapping `timetuple` through `mktime` and `localtime` is\n                # necessary for determining whether DST is in effect (which, in\n                # turn, is necessary for determining which timezone name to\n                # use).  The only downside to using standard functions instead\n                # of `python-dateutil` is that `mktime`, apparently, handles\n                # times duplicated by DST non-deterministically (cf.\n                # <https://git.io/vixsE>), but there's no right way to deal\n                # with those anyway, so...\n                timestamp = time.mktime(timestamp.timetuple())\n        elif not isinstance(timestamp, numbers.Number):\n            raise TypeError('Timestamp must be number or datetime.datetime')\n        timebits = time.localtime(timestamp)\n        try:\n            tzname = timebits.tm_zone\n        except AttributeError:\n            # This assumes that `time.tzname` is meaningful/useful.\n            tzname = time.tzname[timebits.tm_isdst > 0]\n    assert 1 <= timebits.tm_mon <= 12, 'invalid month'\n    assert 0 <= timebits.tm_wday <= 6, 'invalid day of week'\n    return '{wday} {mon} {t.tm_mday:02d}' \\\n           ' {t.tm_hour:02d}:{t.tm_min:02d}:{t.tm_sec:02d}' \\\n           ' {tz} {t.tm_year:04d}'.format(\n                t=timebits,\n                tz=tzname,\n                mon=MONTHS[timebits.tm_mon-1],\n                wday=DAYS_OF_WEEK[timebits.tm_wday]\n            )", "response": "Return a timestamp in the format produced by Java 8."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a set of playlists from the DBus.", "response": "def GetPlaylists(self, start, max_count, order, reversed):\n        \"\"\"Gets a set of playlists.\n\n        :param int start: The index of the first playlist to be fetched\n                           (according to the ordering).\n        :param int max_count: The maximum number of playlists to fetch.\n        :param str order: The ordering that should be used.\n        :param bool reversed: Whether the order should be reversed.\n        \"\"\"\n        cv = convert2dbus\n        return self.iface.GetPlaylists(cv(start, 'u'),\n                                       cv(max_count, 'u'),\n                                       cv(order, 's'),\n                                       cv(reversed, 'b'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the Flask application with this extension. It grabs the necessary configuration values from app. config that are being used by this extension.", "response": "def init_app(self, app):\n        '''Initializes the Flask application with this extension. It grabs\n        the necessary configuration values from ``app.config``, those being\n        HASHING_METHOD and HASHING_ROUNDS. HASHING_METHOD defaults to ``sha256``\n        but can be any one of ``hashlib.algorithms``. HASHING_ROUNDS specifies\n        the number of times to hash the input with the specified algorithm.\n        This defaults to 1.\n\n        :param app: Flask application object\n        '''\n        self.algorithm = app.config.get('HASHING_METHOD', 'sha256')\n        if self.algorithm not in algs:\n            raise ValueError('{} not one of {}'.format(self.algorithm, algs))\n        self.rounds = app.config.get('HASHING_ROUNDS', 1)\n        if not isinstance(self.rounds, int):\n            raise TypeError('HASHING_ROUNDS must be type int')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhash the specified value combined with the specified salt.", "response": "def hash_value(self, value, salt=''):\n        '''Hashes the specified value combined with the specified salt.\n        The hash is done HASHING_ROUNDS times as specified by the application\n        configuration.\n\n        An example usage of :class:``hash_value`` would be::\n\n            val_hash = hashing.hash_value('mysecretdata', salt='abcd')\n            # save to a db or check against known hash\n\n        :param value: The value we want hashed\n        :param salt: The salt to use when generating the hash of ``value``. Default is ''.\n        :return: The resulting hash as a string\n        :rtype: str\n        '''\n        def hashit(value, salt):\n            h = hashlib.new(self.algorithm)\n            tgt = salt+value\n            h.update(tgt)\n            return h.hexdigest()\n\n        def fix_unicode(value):\n            if VER < 3 and isinstance(value, unicode):\n                value = str(value)\n            elif VER >= 3 and isinstance(value, str):\n                value = str.encode(value)\n            return value\n\n        salt = fix_unicode(salt)\n        for i in range(self.rounds):\n            value = fix_unicode(value)\n            value = hashit(value, salt)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_value(self, value_hash, value, salt=''):\n        '''Checks the specified hash value against the hash of the provided\n        salt and value.\n\n        An example usage of :class:`check_value` would be::\n\n            val_hash = hashing.hash_value('mysecretdata', salt='abcd')\n            if hashing.check_value(val_hash, 'mysecretdata', salt='abcd'):\n                # do something special\n\n        :param value_hash: The hash value to check against\n        :param value: The value we want hashed to compare\n        :param salt: The salt to use when generating the hash of ``value``. Default is ''.\n        :return: True if equal, False otherwise\n        :rtype: bool\n        '''\n        h = self.hash_value(value, salt=salt)\n        return h == value_hash", "response": "Checks the specified hash value against the hash of the provided salt and value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_dir(self, filter_fn=None) -> 'Tuple[Path, ...]':\n\n        path = str(self)\n        items = os.listdir(path)\n        if filter_fn is not None:\n            items = filter(filter_fn, items)\n\n        return tuple(Path(path_join((path, item))) for item in items)", "response": "List the directory contents of the current directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the relative path of the current path to the given path.", "response": "def relative(self,\n                 start: typing.Optional[typing.Union['Path', str]] = None\n                 ) -> str:\n        \"\"\"\n        :param start: an object of NoneType or Path or str.\n        :return: a string\n            If `start` is None:\n                returns the relative path of current Path object from its own directory.\n            Else:\n                returns the relative path of current Path object from the `start` path.\n        e.g\n        - Dir1\n            - Dir2\n                - File1\n                - File2\n            - Dir3\n                - File3\n            Path(<path of File1>).relative() => \"<filename of File1>\"\n            Path(<path of Dir2>).relative() => \"<directory name of Dir1>\"\n            Path(<path of File3>).relative(<path of File1>) => \"../Dir2/<filename of File1>\"\n        \"\"\"\n        if start is None:\n            return os.path.split(str(self))[1]\n        if isinstance(start, Path):\n            start = str(start)\n        return os.path.relpath(str(self), start)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nopen the file with the specified mode and encoding", "response": "def open(self,\n             mode='r',\n             encoding: typing.Union[list, tuple, set, str, typing.\n                                    Callable] = 'auto'):\n        \"\"\"\n        :param mode: the same as the argument `mode` of `builtins.open`\n        :param encoding: similar to the argument `encoding` of `builtins.open` which is compatible to io.open.\n                         - `encoding='auto'` to automatically detect the encoding.\n                         - `encoding is a <list|tuple|set>` to automatically detect the encoding from the limited collection.\n                         - `encoding=<function>`\n                            If you want to apply custom encoding detection method, you could pass\n                            an encoding detecting function `(filename: str) -> (encoding: str)` here\n                            which receives the filename and returns encoding of the file.\n        :return: the same as the return of ``builtins.open`.\n\n        \"\"\"\n\n        if 'b' in mode:\n            return io.open(str(self), mode)\n\n        if callable(encoding):\n            return self.open(mode, encoding=encoding(str(self)))\n\n        if isinstance(encoding, (tuple, list, set)):\n            return default_auto_open(encoding, self, mode)\n\n        if encoding == 'auto':\n            return default_auto_open(_encodings, self, mode)\n\n        return io.open(str(self), mode, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddTrack(self, uri, after_track, set_as_current):\n        self.iface.AddTrack(uri,\n                            convert2dbus(after_track, 'o'),\n                            convert2dbus(set_as_current, 'b'))", "response": "Adds a URI to the TrackList."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_gene(self, *gene_ids):\n        self.gene_ids = [gene_id for gene_id in self.gene_ids\n                         if gene_id not in gene_ids]", "response": "Delete one or more gene ids form the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a case to the adapter", "response": "def add_case(self, case_obj):\n        \"\"\"Add a case obj with individuals to adapter\n        \n            Args:\n                case_obj (puzzle.models.Case)\n                \n        \"\"\"\n        for ind_obj in case_obj.individuals:\n            self._add_individual(ind_obj)\n        logger.debug(\"Adding case {0} to plugin\".format(case_obj.case_id))\n        self.case_objs.append(case_obj)\n        if case_obj.tabix_index:\n            logger.debug(\"Setting filters.can_filter_range to True\")\n            self.filters.can_filter_range = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Case object for the given case_id. If no case_id is given return one.", "response": "def case(self, case_id=None):\n        \"\"\"Return a Case object\n\n            If no case_id is given return one case\n\n            Args:\n                case_id (str): A case id\n\n            Returns:\n                A Case object\n        \"\"\"\n        if case_id:\n            for case in self.case_objs:\n                if case.case_id == case_id:\n                    return case\n        else:\n            if self.cases:\n                return list(self.case_objs)[0]\n\n        return Case(case_id='unknown')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a individual object", "response": "def individual(self, ind_id=None):\n        \"\"\"Return a individual object\n        \n            Args:\n                ind_id (str): A individual id\n            \n            Returns:\n                individual (puzzle.models.individual)\n        \"\"\"\n        for ind_obj in self.individual_objs:\n            if ind_obj.ind_id == ind_id:\n                return ind_obj\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef individuals(self, ind_ids=None):\n        if ind_ids:\n            for ind_id in ind_ids:\n                for ind in self.individual_objs:\n                    if ind.ind_id == ind_id:\n                        yield ind\n        else:\n            for ind in self.individual_objs:\n                yield ind", "response": "Return information about individuals\n getItsId"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking state of update timer.", "response": "def healthy_update_timer(self):\n        \"\"\"Check state of update timer.\"\"\"\n        state = None\n\n        if self.update_status_timer and self.update_status_timer.is_alive():\n            _LOGGER.debug(\"Timer: healthy\")\n            state = True\n        else:\n            _LOGGER.debug(\"Timer: not healthy\")\n            state = False\n\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen and initialize the socket.", "response": "def initialize_socket(self):\n        \"\"\"initialize the socket\"\"\"\n        try:\n            _LOGGER.debug(\"Trying to open socket.\")\n            self._socket = socket.socket(\n                socket.AF_INET,     # IPv4\n                socket.SOCK_DGRAM   # UDP\n            )\n            self._socket.bind(('', self._udp_port))\n        except socket.error as err:\n            raise err\n        else:\n            _LOGGER.debug(\"Socket open.\")\n            socket_thread = threading.Thread(\n                name=\"SocketThread\", target=socket_worker,\n                args=(self._socket, self.messages,))\n            socket_thread.setDaemon(True)\n            socket_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the worker thread", "response": "def initialize_worker(self):\n        \"\"\"initialize the worker thread\"\"\"\n        worker_thread = threading.Thread(\n            name=\"WorkerThread\", target=message_worker, args=(self,))\n        worker_thread.setDaemon(True)\n        worker_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting status from device to register keep alive UDP", "response": "def get_status(self):\n        \"\"\"Get status from device to register/keep alive UDP\"\"\"\n        headers = {\n            \"X-AppName\": \"MusicCast/0.1(python)\",\n            \"X-AppPort\": str(self._udp_port)\n        }\n        req_url = ENDPOINTS[\"getStatus\"].format(self.ip_address, 'main')\n        return request(req_url, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling status from device", "response": "def handle_status(self):\n        \"\"\"Handle status from device\"\"\"\n        status = self.get_status()\n\n        if status:\n            # Update main-zone\n            self.zones['main'].update_status(status)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_netusb(self, message):\n        # _LOGGER.debug(\"message: {}\".format(message))\n        needs_update = 0\n\n        if self._yamaha:\n            if 'play_info_updated' in message:\n                play_info = self.get_play_info()\n                # _LOGGER.debug(play_info)\n                if play_info:\n                    new_media_status = MediaStatus(play_info, self._ip_address)\n\n                    if self._yamaha.media_status != new_media_status:\n                        # we need to send an update upwards\n                        self._yamaha.new_media_status(new_media_status)\n                        needs_update += 1\n\n                    playback = play_info.get('playback')\n                    # _LOGGER.debug(\"Playback: {}\".format(playback))\n                    if playback == \"play\":\n                        new_status = STATE_PLAYING\n                    elif playback == \"stop\":\n                        new_status = STATE_IDLE\n                    elif playback == \"pause\":\n                        new_status = STATE_PAUSED\n                    else:\n                        new_status = STATE_UNKNOWN\n\n                    if self._yamaha.status is not new_status:\n                        _LOGGER.debug(\"playback: %s\", new_status)\n                        self._yamaha.status = new_status\n                        needs_update += 1\n\n        return needs_update", "response": "Handle a netusb message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling features of the device.", "response": "def handle_features(self, device_features):\n        \"\"\"Handles features of the device\"\"\"\n\n        self.device_features = device_features\n\n        if device_features and 'zone' in device_features:\n            for zone in device_features['zone']:\n                zone_id = zone.get('id')\n                if zone_id in self.zones:\n                    _LOGGER.debug(\"handle_features: %s\", zone_id)\n                    input_list = zone.get('input_list', [])\n                    input_list.sort()\n                    self.zones[zone_id].source_list = input_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndispatching all event messages.", "response": "def handle_event(self, message):\n        \"\"\"Dispatch all event messages\"\"\"\n        # _LOGGER.debug(message)\n        needs_update = 0\n        for zone in self.zones:\n            if zone in message:\n                _LOGGER.debug(\"Received message for zone: %s\", zone)\n                self.zones[zone].update_status(message[zone])\n\n        if 'netusb' in message:\n            needs_update += self.handle_netusb(message['netusb'])\n\n        if needs_update > 0:\n            _LOGGER.debug(\"needs_update: %d\", needs_update)\n            self.update_hass()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates status of the device.", "response": "def update_status(self, reset=False):\n        \"\"\"Update device status.\"\"\"\n        if self.healthy_update_timer and not reset:\n            return\n\n        # get device features only once\n        if not self.device_features:\n            self.handle_features(self.get_features())\n\n        # Get status from device to register/keep alive UDP\n        self.handle_status()\n\n        # Schedule next execution\n        self.setup_update_timer()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nschedule a Timer Thread.", "response": "def setup_update_timer(self, reset=False):\n        \"\"\"Schedule a Timer Thread.\"\"\"\n        _LOGGER.debug(\"Timer: firing again in %d seconds\", self._interval)\n        self.update_status_timer = threading.Timer(\n            self._interval, self.update_status, [True])\n        self.update_status_timer.setDaemon(True)\n        self.update_status_timer.start()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nappend sql to a gemini query", "response": "def build_gemini_query(self, query, extra_info):\n        \"\"\"Append sql to a gemini query\n\n        Args:\n            query(str): The gemini query\n            extra_info(str): The text that should be added\n\n        Return:\n            extended_query(str)\n        \"\"\"\n        if 'WHERE' in query:\n            return \"{0} AND {1}\".format(query, extra_info)\n        else:\n            return \"{0} WHERE {1}\".format(query, extra_info)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of variants in a gemini db.", "response": "def variants(self, case_id, skip=0, count=1000, filters=None):\n        \"\"\"Return count variants for a case.\n\n        This function needs to have different behaviours based on what is asked\n        for. It should allways try to give minimal information back to improve\n        on speed. For example, if consequences are not asked for we will not\n        build all transcripts. If not sv variants we will not build sv\n        coordinates.\n        So the minimal case is to just show what is asked for in the variants\n        interface.\n\n            Args:\n                case_id (str): A gemini db\n                skip (int): Skip first variants\n                count (int): The number of variants to return\n                filters (dict): A dictionary with filters. Currently this will\n                look like: {\n                    gene_list: [] (list of hgnc ids),\n                    frequency: None (float),\n                    cadd: None (float),\n                    consequence: [] (list of consequences),\n                    impact_severities: [] (list of consequences),\n                    genetic_models [] (list of genetic models)\n                }\n            Returns:\n                puzzle.constants.Results : Named tuple with variants and\n                                           nr_of_variants\n\n        \"\"\"\n        filters = filters or {}\n        logger.debug(\"Looking for variants in {0}\".format(case_id))\n\n        limit = count + skip\n\n        gemini_query = filters.get('gemini_query') or \"SELECT * from variants v\"\n\n        any_filter = False\n\n        if filters.get('frequency'):\n            frequency = filters['frequency']\n\n            extra_info = \"(v.max_aaf_all < {0} or v.max_aaf_all is\"\\\n                         \" Null)\".format(frequency)\n            gemini_query = self.build_gemini_query(gemini_query, extra_info)\n\n        if filters.get('cadd'):\n            cadd_score = filters['cadd']\n\n            extra_info = \"(v.cadd_scaled > {0})\".format(cadd_score)\n            gemini_query = self.build_gemini_query(gemini_query, extra_info)\n\n        if filters.get('gene_ids'):\n            gene_list = [gene_id.strip() for gene_id in filters['gene_ids']]\n\n            gene_string = \"v.gene in (\"\n            for index, gene_id in enumerate(gene_list):\n                if index == 0:\n                    gene_string += \"'{0}'\".format(gene_id)\n                else:\n                    gene_string += \", '{0}'\".format(gene_id)\n            gene_string += \")\"\n\n            gemini_query = self.build_gemini_query(gemini_query, gene_string)\n\n        if filters.get('range'):\n            chrom = filters['range']['chromosome']\n            if not chrom.startswith('chr'):\n                chrom = \"chr{0}\".format(chrom)\n\n            range_string = \"v.chrom = '{0}' AND \"\\\n                           \"((v.start BETWEEN {1} AND {2}) OR \"\\\n                           \"(v.end BETWEEN {1} AND {2}))\".format(\n                               chrom,\n                               filters['range']['start'],\n                               filters['range']['end']\n                           )\n            gemini_query = self.build_gemini_query(gemini_query, range_string)\n\n        filtered_variants = self._variants(\n            case_id=case_id,\n            gemini_query=gemini_query,\n        )\n\n        if filters.get('consequence'):\n            consequences = set(filters['consequence'])\n\n            filtered_variants = (variant for variant in filtered_variants if\n                set(variant.consequences).intersection(consequences))\n\n        if filters.get('impact_severities'):\n            severities = set([severity.strip()\n                    for severity in filters['impact_severities']])\n            new_filtered_variants = []\n            filtered_variants = (variant for variant in filtered_variants if\n                set([variant.impact_severity]).intersection(severities))\n\n        if filters.get('sv_len'):\n            sv_len = int(filters['sv_len'])\n            filtered_variants = (variant for variant in filtered_variants if\n                variant.sv_len >= sv_len)\n\n        variants = []\n        for index, variant_obj in enumerate(filtered_variants):\n            if index >= skip:\n                if index < limit:\n                    variants.append(variant_obj)\n                else:\n                    break\n\n        return Results(variants, len(variants))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a specific variant.", "response": "def variant(self, case_id, variant_id):\n        \"\"\"Return a specific variant.\n\n            We solve this by building a gemini query and send it to _variants\n\n            Args:\n                case_id (str): Path to a gemini database\n                variant_id (int): A gemini variant id\n\n            Returns:\n                variant_obj (dict): A puzzle variant\n\n        \"\"\"\n        #Use the gemini id for fast lookup\n        variant_id = int(variant_id)\n        gemini_query = \"SELECT * from variants WHERE variant_id = {0}\".format(\n            variant_id\n        )\n\n        individuals = []\n        # Get the individuals for the case\n        case_obj = self.case(case_id)\n        for individual in case_obj.individuals:\n            individuals.append(individual)\n\n        self.db = case_obj.variant_source\n        self.variant_type = case_obj.variant_type\n\n        gq = GeminiQuery(self.db)\n        gq.run(gemini_query)\n\n        for gemini_variant in gq:\n            variant = self._format_variant(\n                case_id=case_id,\n                gemini_variant=gemini_variant,\n                individual_objs=individuals,\n                index=gemini_variant['variant_id'],\n                add_all_info = True\n            )\n            return variant\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator that yields all the variants in the gemini database.", "response": "def _variants(self, case_id, gemini_query):\n        \"\"\"Return variants found in the gemini database\n\n            Args:\n                case_id (str): The case for which we want to see information\n                gemini_query (str): What variants should be chosen\n                filters (dict): A dictionary with filters\n\n            Yields:\n                variant_obj (dict): A Variant formatted dictionary\n        \"\"\"\n        individuals = []\n        # Get the individuals for the case\n        case_obj = self.case(case_id)\n        for individual in case_obj.individuals:\n            individuals.append(individual)\n\n        self.db = case_obj.variant_source\n        self.variant_type = case_obj.variant_type\n\n        gq = GeminiQuery(self.db)\n\n        gq.run(gemini_query)\n\n        index = 0\n        for gemini_variant in gq:\n            variant = None\n\n            # Check if variant is non ref in the individuals\n            is_variant = self._is_variant(gemini_variant, individuals)\n\n            if self.variant_type == 'snv' and not is_variant:\n                variant = None\n\n            else:\n                index += 1\n                logger.debug(\"Updating index to: {0}\".format(index))\n                variant = self._format_variant(\n                        case_id=case_id,\n                        gemini_variant=gemini_variant,\n                        individual_objs=individuals,\n                        index=index\n                        )\n\n            if variant:\n\n                yield variant"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _format_variant(self, case_id, gemini_variant, individual_objs,\n                        index=0, add_all_info=False):\n        \"\"\"Make a puzzle variant from a gemini variant\n\n            Args:\n                case_id (str): related case id\n                gemini_variant (GeminiQueryRow): The gemini variant\n                individual_objs (list(dict)): A list of Individuals\n                index(int): The index of the variant\n\n            Returns:\n                variant (dict): A Variant object\n        \"\"\"\n        chrom = gemini_variant['chrom']\n        if chrom.startswith('chr') or chrom.startswith('CHR'):\n            chrom = chrom[3:]\n\n        variant_dict = {\n            'CHROM':chrom,\n            'POS':str(gemini_variant['start']),\n            'ID':gemini_variant['rs_ids'],\n            'REF':gemini_variant['ref'],\n            'ALT':gemini_variant['alt'],\n            'QUAL':gemini_variant['qual'],\n            'FILTER':gemini_variant['filter']\n        }\n\n        variant = Variant(**variant_dict)\n\n        # Use the gemini id for fast search\n        variant.update_variant_id(gemini_variant['variant_id'])\n        logger.debug(\"Creating a variant object of variant {0}\".format(\n            variant.variant_id))\n\n        variant['index'] = index\n\n        # Add the most severe consequence\n        self._add_most_severe_consequence(variant, gemini_variant)\n\n        #Add the impact severity\n        self._add_impact_severity(variant, gemini_variant)\n        ### POSITON ANNOATTIONS ###\n        variant.start = int(gemini_variant['start'])\n        variant.stop = int(gemini_variant['end'])\n\n        #Add the sv specific coordinates\n        if self.variant_type == 'sv':\n            variant.sv_type = gemini_variant['sub_type']\n            variant.stop = int(gemini_variant['end'])\n            self._add_sv_coordinates(variant)\n\n        else:\n            ### Consequence and region annotations\n            #Add the transcript information\n            self._add_transcripts(variant, gemini_variant)\n            self._add_thousand_g(variant, gemini_variant)\n            self._add_exac(variant, gemini_variant)\n            self._add_gmaf(variant, gemini_variant)\n            #### Check the impact annotations ####\n            if gemini_variant['cadd_scaled']:\n                variant.cadd_score = gemini_variant['cadd_scaled']\n\n            # We use the prediction in text\n            polyphen = gemini_variant['polyphen_pred']\n            if polyphen:\n                variant.add_severity('Polyphen', polyphen)\n\n            # We use the prediction in text\n            sift = gemini_variant['sift_pred']\n            if sift:\n                variant.add_severity('SIFT', sift)\n\n        #Add the genes based on the hgnc symbols\n        self._add_hgnc_symbols(variant)\n        if self.variant_type == 'snv':\n            self._add_genes(variant)\n\n        self._add_consequences(variant)\n\n        ### GENOTYPE ANNOATTIONS ###\n        #Get the genotype info\n        if add_all_info:\n            self._add_genotypes(variant, gemini_variant, case_id, individual_objs)\n            if self.variant_type == 'sv':\n                self._add_genes(variant)\n\n        return variant", "response": "Create a puzzle variant from a gemini variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_variant(self, gemini_variant, ind_objs):\n\n        indexes = (ind.ind_index for ind in ind_objs)\n        #Check if any individual have a heterozygous or homozygous variant call\n        for index in indexes:\n            gt_call = gemini_variant['gt_types'][index]\n            if (gt_call == 1 or gt_call == 3):\n                return True\n\n        return False", "response": "Check if the variant is a variation in any of the individuals\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a gene list from the database.", "response": "def gene_list(self, list_id):\n        \"\"\"Get a gene list from the database.\"\"\"\n        return self.query(GeneList).filter_by(list_id=list_id).first()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_genelist(self, list_id, gene_ids, case_obj=None):\n        new_genelist = GeneList(list_id=list_id)\n        new_genelist.gene_ids = gene_ids\n        if case_obj:\n            new_genelist.cases.append(case_obj)\n\n        self.session.add(new_genelist)\n        self.save()\n        return new_genelist", "response": "Create a new gene list and optionally link to cases."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a gene list and links to cases.", "response": "def remove_genelist(self, list_id, case_obj=None):\n        \"\"\"Remove a gene list and links to cases.\"\"\"\n        gene_list = self.gene_list(list_id)\n\n        if case_obj:\n            # remove a single link between case and gene list\n            case_ids = [case_obj.id]\n        else:\n            # remove all links and the list itself\n            case_ids = [case.id for case in gene_list.cases]\n            self.session.delete(gene_list)\n\n        case_links = self.query(CaseGenelistLink).filter(\n            CaseGenelistLink.case_id.in_(case_ids),\n            CaseGenelistLink.genelist_id == gene_list.id\n        )\n        for case_link in case_links:\n            self.session.delete(case_link)\n\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef case_genelist(self, case_obj):\n        list_id = \"{}-HPO\".format(case_obj.case_id)\n        gene_list = self.gene_list(list_id)\n\n        if gene_list is None:\n            gene_list = GeneList(list_id=list_id)\n            case_obj.gene_lists.append(gene_list)\n            self.session.add(gene_list)\n\n        return gene_list", "response": "Get or create a new case specific gene list record."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef manage_the_plots(self):\n        diagram = \"\"\n        for residue in sorted(self.molecule.nearest_points_coords.keys()):\n            for i, line in enumerate(fileinput.input(residue[1]+residue[2]+\".svg\", inplace=1)):\n                    if i <= 8:\n                        continue\n                    else:\n                        sys.stdout.write(line.replace (\"</svg>\",\"</g>\"))\n            input1 = \"</defs>\"\n            x = str(int(self.molecule.nearest_points_coords[residue][0]+(self.molecule.x_dim-self.molecule.molsize1)/2)-90)\n            y = str(int(self.molecule.nearest_points_coords[residue][1]+(self.molecule.y_dim-self.molecule.molsize2)/2)-90)\n            output1 = \"<g  id='\"+residue[0]+residue[1]+\"_\"+residue[2]+\"' class='residue' transform='translate(\"+x+\",\"+y+\" )' x='\"+x+\"' y='\"+y+\"'>\"\n            self.change_lines_in_svg(residue[1]+residue[2]+'.svg', input1, output1)\n            input2 = \"font-style:normal;\"\n            output2 = \"font-style:normal;font-weight:bold;\"\n            self.change_lines_in_svg(residue[1]+residue[2]+'.svg', input2, output2)\n            with open(residue[1]+residue[2]+\".svg\", \"r\") as f:\n                lines = f.readlines()\n                diagram = diagram +\"\".join(map(str,lines))\n                f.close()\n        self.draw_plots = diagram", "response": "This function handles the actual plotting of the 2D drug molecule."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_bigger_box(self):\n        start1 = \"width='\"+str(int(self.molecule.molsize1))+\"px' height='\"+str(int(self.molecule.molsize2))+\"px' >\"\n        start2 = \"<rect style='opacity:1.0;fill:#FFFFFF;stroke:none' width='\"+str(int(self.molecule.molsize1))+\"' height='\"+str(int(self.molecule.molsize2))+\"' x='0' y='0'> </rect>\"\n        bigger_box =\"width='100%' height='100%' viewbox='0 0 \"+str(int(self.molecule.x_dim))+\" \"+str(int(self.molecule.y_dim))+\"' > \"\n        big_box2= \"<rect style='opacity:1.0;fill:white;stroke:none' width='\"+str(int(self.molecule.x_dim))+\"px' height='\"+str(int(self.molecule.y_dim))+\"px' x='0' y='0'> </rect> <g id='molecularDrawing' transform='translate(\"+str((self.molecule.x_dim-self.molecule.molsize1)/2)+\",\"+str((self.molecule.y_dim-self.molecule.molsize2)/2)+\")'>'<rect style='opacity:1.0;fill:#ffffff;stroke:none' width='\"+str(self.molecule.molsize1)+\"' height='\"+str(self.molecule.molsize2)+\"' x='0' y='0' /> \"\n        self.end_symbol = \"</svg>\"\n        no_end_symbol = \"</g>\"\n        #Make the lines in molecule drawing thicker to look better with the large plots\n        linewidth1 = \"stroke-width:2px\"\n        linewidth2 = \"stroke-width:5px\"\n        self.change_lines_in_svg(\"molecule.svg\", linewidth1,linewidth2)\n\n        self.change_lines_in_svg(\"molecule.svg\", start1, bigger_box)\n        self.change_lines_in_svg(\"molecule.svg\", start2, big_box2)\n        self.change_lines_in_svg(\"molecule.svg\", self.end_symbol, no_end_symbol)\n        with open(\"molecule.svg\",\"r\") as f:\n            lines = f.readlines()\n            self.filestart = \" \".join(map(str,lines[0:8]))\n            self.draw_molecule =\"\".join(map(str,lines[8:]))\n            f.close()", "response": "Adds the bigger box to the figure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw_white_circles(self):\n        for atom in sorted(self.molecule.nearest_points_coords.keys()):\n                self.white_circles = self.white_circles+\"<circle cx='\"+str(int(self.molecule.nearest_points_coords[atom][0]))+\"' cy='\"+str(int(self.molecule.nearest_points_coords[atom][1]))+\"' r='55' fill='white' />\"", "response": "This function creates white circles that are used to cover the middle\n        part of the plots."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extend_with(func):\n    if not func.__name__ in ArgParseInator._plugins:\n        ArgParseInator._plugins[func.__name__] = func", "response": "Extends with class or function"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef arg(*args, **kwargs):\n    def decorate(func):\n        \"\"\"\n        Decorate\n        \"\"\"\n        # we'll set the command name with the passed cmd_name argument, if\n        # exist, else the command name will be the function name\n        func.__cmd_name__ = kwargs.pop(\n            'cmd_name', getattr(func, '__cmd_name__', func.__name__))\n        # retrieve the class (SillyClass)\n        func.__cls__ = utils.check_class()\n        if not hasattr(func, '__arguments__'):\n            # if the funcion hasn't the __arguments__ yet, we'll setup them\n            # using get_functarguments.\n            func.__arguments__ = utils.get_functarguments(func)\n        if len(args) or len(kwargs):\n            # if we have some argument or keyword argument\n            # we'll try to get the destination name from the kwargs ('dest')\n            # else we'll use the last arg name as destination\n            arg_name = kwargs.get(\n                'dest', args[-1].lstrip('-').replace('-', '_'))\n            try:\n                # we try to get the command index.\n                idx = func.__named__.index(arg_name)\n                # and delete it from the named list\n                del func.__named__[idx]\n                # and delete it from the arguments list\n                del func.__arguments__[idx]\n            except ValueError:\n                pass\n            # append the args and kwargs to the function arguments list\n            func.__arguments__.append((args, kwargs,))\n        if func.__cls__ is None and isinstance(func, types.FunctionType):\n            # if the function don't have a class and is a FunctionType\n            # we'll add it directly to he commands list.\n            ap_ = ArgParseInator(skip_init=True)\n            if func.__cmd_name__ not in ap_.commands:\n                # we'll add it if not exists\n                ap_.commands[func.__cmd_name__] = func\n        return func\n    return decorate", "response": "Decorator that adds an argument to the argument parser."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecorate a class to handle the arguments parser.", "response": "def class_args(cls):\n    \"\"\"\n    Decorates a class to handle the arguments parser.\n    \"\"\"\n    # get the Singleton\n    ap_ = ArgParseInator(skip_init=True)\n    # collect special vars (really need?)\n    utils.collect_appendvars(ap_, cls)\n    # set class reference\n    cls.__cls__ = cls\n    cmds = {}\n    # get eventual class arguments\n    cls.__arguments__ = getattr(cls, '__arguments__', [])\n    # cycle through class functions\n    for func in [f for f in cls.__dict__.values()\n                 if hasattr(f, '__cmd_name__') and not inspect.isclass(f)]:\n        # clear subcommands\n        func.__subcommands__ = None\n        # set the parent class\n        func.__cls__ = cls\n        # assign to commands dict\n        cmds[func.__cmd_name__] = func\n    if hasattr(cls, '__cmd_name__') and cls.__cmd_name__ not in ap_.commands:\n        # if che class has the __cmd_name__ attribute and is not already present\n        # in the ArgParseInator commands\n        # set the class subcommands\n        cls.__subcommands__ = cmds\n        # add the class as ArgParseInator command\n        ap_.commands[cls.__cmd_name__] = cls\n    else:\n        # else if we don't have a __cmd_name__\n        # we will add all the functions directly to the ArgParseInator commands\n        # if it don't already exists.\n        for name, func in cmds.items():\n            if name not in ap_.commands:\n                ap_.commands[name] = func\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting authorization for command or subcommand.", "response": "def cmd_auth(auth_phrase=None):\n    \"\"\"\n    set authorization for command or subcommand.\n    \"\"\"\n    def decorate(func):\n        \"\"\"\n        decorates the funcion\n        \"\"\"\n        # get the Singleton\n        ap_ = ArgParseInator(skip_init=True)\n        # set the authorization name\n        auth_name = id(func)\n        if auth_phrase is None:\n            # if we don't have a specific auth_phrase we set the\n            # **authorization needed** to True\n            ap_.auths[auth_name] = True\n        else:\n            # else if we have a specific auth_phrase we set it for the\n            # command authorization\n            ap_.auths[auth_name] = str(auth_phrase)\n        return func\n    return decorate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compile(self, module=None):\n        # get the main module\n        mod = module or sys.modules['__main__']\n        self.mod = mod\n        # setup the script version\n        if hasattr(mod, '__version__'):\n            version = \"%(prog)s \" + mod.__version__\n            self.ap_args.append(\n                ap_arg('-v', '--version', action='version', version=version))\n        # setup the script description\n        if module is None:\n            if 'description' not in self.argparse_args and hasattr(mod, '__doc__'):\n                self.argparse_args['description'] = mod.__doc__\n        # setup main parser\n        self.parser = argparse.ArgumentParser(\n            formatter_class=self.formatter_class, **self.argparse_args)\n        if self.error:\n            # setup the error method if we have one\n            setattr(\n                self.parser, 'error', types.MethodType(self.error, self.parser))\n        if self.add_output:\n            # add the output options if we have the add_output true\n            if isinstance(self.add_output, basestring):\n                odefault = self.add_output\n            else:\n                odefault = None\n            self.parser.add_argument(\n                '-o', '--output', metavar=\"FILE\", default=odefault,\n                help=\"Output to file\")\n            self.parser.add_argument(\n                '--encoding', default=\"utf-8\", help=\"Encoding for output file.\")\n            self.parser.add_argument(\n                '--write-mode', default=self.write_mode, help=\"Write mode\")\n        if self._cfg_factory:\n            # if we have a config factory add the config options\n            self.parser.add_argument(\n                '-c', '--config', metavar=\"FILE\", default=self.cfg_file,\n                help=\"Configuration file\")\n        if self.auths:\n            # if we have authorizations enabled add the auth options\n            self.parser.add_argument(\n                '--auth',\n                help=\"Authorization phrase for special commands.\")\n\n        # let's exexcute plugins\n        for plugin in ArgParseInator._plugins.values():\n            plugin(self)\n\n        if self.ap_args is not None:\n            # if we have argment parser args we will add them to the main parser\n            for aargs, akargs in self.ap_args:\n                self.parser.add_argument(*aargs, **akargs)\n\n        if len(self.commands) == 1 and self.never_single is False:\n            # if we have only one command ad never_single is False\n            # setup the command as the only command.\n            func = six.next(six.itervalues(self.commands))\n            # add the arguments to the main parser\n            for args, kwargs in func.__arguments__:\n                self.parser.add_argument(*args, **kwargs)\n            # shortcut for the single command\n            self._single = func\n            if not self.parser.description:\n                # replace the description if we dont' have one\n                self.parser.description = func.__doc__ or self.parser.description\n            else:\n                # or add to the main description\n                if func.__doc__:\n                    self.parser.description += linesep + linesep + func.__doc__\n            if not self.parser.epilog:\n                # replace the description if we dont' have one\n                self.parser.epilog = getattr(\n                    func, \"__epilog__\", self.parser.epilog)\n            else:\n                # or add to the main description\n                if hasattr(func, '__epilog__'):\n                    self.parser.epilog += linesep + linesep + func.__epilog__\n            utils.set_subcommands(func, self.parser)\n        else:\n            # if we have more than one command or we don't want a single command\n            # set the single to None\n            self._single = None\n            # setup the subparsers\n            self.subparsers = self.parser.add_subparsers(\n                title=utils.COMMANDS_LIST_TITLE, dest='command',\n                description=utils.COMMANDS_LIST_DESCRIPTION)\n            # add all the commands\n            for func in self.commands.values():\n                parser = utils.get_parser(func, self.subparsers)\n                utils.set_subcommands(func, parser)\n        return self.parser", "response": "Compile functions for argparsing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_auth(self, name):\n        if name in self.auths:\n            # if the command name is in the **need authorization list**\n            # get the authorization for the command\n            auth = self.auths[name]\n            if self.args.auth is None:\n                # if we didn't pass the authorization phrase raise the\n                # appropriate exception\n                raise exceptions.ArgParseInatorAuthorizationRequired\n            elif ((auth is True and self.args.auth != self.auth_phrase) or\n                  (auth is not True and self.args.auth != auth)):\n                # else if the authorization phrase is wrong\n                raise exceptions.ArgParseInatorNotValidAuthorization\n        return True", "response": "Check the authorization for the command name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_command(self, **new_attributes):\n        # let's parse arguments if we didn't before.\n        if not self._is_parsed:\n            self.parse_args()\n        if not self.commands:\n            # if we don't have commands raise an Exception\n            raise exceptions.ArgParseInatorNoCommandsFound\n        elif self._single:\n            # if we have a single function we get it directly\n            func = self._single\n        else:\n            if not self.args.command:\n                self.parser.error(\"too few arguments\")\n            # get the right command\n            func = self.commands[self.args.command]\n\n        if hasattr(func, '__subcommands__') and func.__subcommands__:\n            # if we have subcommands get the command from them\n            command = func.__subcommands__[self.args.subcommand]\n        else:\n            # else the command IS the function\n            command = func\n        # get the command name\n        self.cmd_name = command.__cmd_name__\n        # check authorization\n        if not self.check_auth(id(command)):\n            return 0\n        # let's execute the command.\n        return self._execute(func, command, **new_attributes)", "response": "Check if the command was passed and if so executes it by passing the parameters and returning the result."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _execute(self, func, command, **new_attributes):\n        if self._cfg_factory:\n            # if we have a cfg_factory\n            try:\n                # we try to load a config with the factory\n                if self.cfg_file:\n                    self.cfg = self._cfg_factory(self.cfg_file)\n            except Exception as error:\n                # raise se exception\n                self._cfg_error(error)\n        # let's get command(function) argspec\n        arg_specs = inspect.getargspec(command)\n        if arg_specs.defaults:\n            # if we have defaults\n            # count defaults arguments\n            count = len(arg_specs.defaults)\n            # get arguments names\n            args_names = arg_specs.args[:count]\n            # get keyword arguments names\n            kwargs_name = arg_specs.args[count:]\n        else:\n            # else all names are the args only\n            args_names = arg_specs.args\n            # and keyword arguments is empty\n            kwargs_name = []\n        pargs = []\n        kwargs = {}\n        # for every argument in argument names\n        for name in args_names:\n            if name == 'args':\n                # if argument name is *special name* **args**\n                # we append a reference to self.args\n                pargs.append(self.args)\n            elif name == 'self':\n                # else if argment name is *special name* **self**\n                if ArgParseInated in inspect.getmro(func.__cls__):\n                    # if the class that holds the function is subclass of\n                    # ArgParseInated we'll instantiate it, passing some\n                    # parameter\n                    pargs.append(func.__cls__(self, **new_attributes))\n                else:\n                    # else we'll instatiate the class without parameters\n                    pargs.append(func.__cls__())\n            else:\n                # else we'll append the argument getting it from the self.args\n                pargs.append(getattr(self.args, name))\n        # for every argument in keyword arguments\n        for name in kwargs_name:\n            if name == 'args':\n                # if argument name is *special name* **args**\n                # we set for the arg a reference to self.args\n                kwargs[name] = self.args\n            elif name in self.args:\n                # else if name is in self.args we'll set the relative value.\n                kwargs[name] = getattr(self.args, name)\n        # set the **global** write function\n        setattr(__builtin__, self._write_name, self.write)\n        # set the **global** write line function\n        setattr(__builtin__, self._write_line_name, self.writeln)\n        # let's setup something.\n        for setup_func in self.setup:\n            setup_func(self)\n        # call event before_execute\n        self._self_event('before_execute', command, *pargs, **kwargs)\n        # if events returns a non None value we use it as retrval.\n        retval, pargs, kwargs = self._call_event(\n            'before_execute', command, pargs, kwargs)\n        # if before_execute event returns None go on with command\n        if retval is None:\n            # let's execute the command and assign the returned value to retval\n            retval = command(*pargs, **kwargs)\n            # call event after_execute\n            self._call_event('after_execute', command, pargs, kwargs)\n            self._self_event('after_execute', command, *pargs, **kwargs)\n        if self.auto_exit:\n            # if we have auto_exit is True\n            if retval is None:\n                self._self_event(\n                    'before_exit_ok', command, retval=EXIT_OK, *pargs, **kwargs)\n                # if retval is None we'll assume it's EXIT_OK\n                self.exit(EXIT_OK)\n            elif isinstance(retval, basestring):\n                self._self_event('before_exit_ok', command, retval=retval, *pargs, **kwargs)\n                # else if retval is a string we will exit with the message and\n                # ERRORCODE is equal to 0\n                self.exit(EXIT_OK, retval)\n            elif isinstance(retval, int):\n                if retval == EXIT_OK:\n                    self._self_event('before_exit_ok', command, retval=retval, *pargs, **kwargs)\n                else:\n                    self._self_event('before_exit_error', command, retval=retval, *pargs, **kwargs)\n                # else if retval is an integer we'll exits with it as ERRORCODE\n                self.exit(retval)\n            elif isinstance(retval, (tuple, list,)):\n                self._self_event('before_exit_error', command, retval=retval, *pargs, **kwargs)\n                # if retval is a tuple or a list we'll exist with ERRORCODE and\n                # message\n                self.exit(retval[0], retval[1])\n            self._self_event('before_exit', command, retval=retval, *pargs, **kwargs)\n            self.exit()\n        else:\n            # else if auto_exit is not True\n            # we'll simply return  retval\n            return retval", "response": "Execute the command and return the result of the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to call events for the command object.", "response": "def _call_event(self, event_name, cmd, pargs, kwargs, **kws):\n        \"\"\"\n        Try to call events for cmd.\n        \"\"\"\n        def get_result_params(res):\n            \"\"\"return the right list of params\"\"\"\n            if not isinstance(res, (list, tuple)):\n                return res, pargs, kwargs\n            elif len(res) == 2:\n                return res, pargs, kwargs\n            return res[0], (pargs[0], ) + tuple(res[1]), kwargs\n        if hasattr(cmd, event_name):\n            return get_result_params(\n                getattr(cmd, event_name)(pargs[0], *pargs[1:], **kwargs))\n        elif hasattr(cmd.__cls__, event_name):\n            return get_result_params(\n                getattr(cmd.__cls__, event_name)(\n                    pargs[0], cmd.__cmd_name__ or cmd.__name__, *pargs[1:],\n                    **kwargs))\n        return None, pargs, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_event(cls, event, event_name=None):\n        # setattr(cls, event_name, event)\n        event_name = event_name or event.__name__\n        setattr(cls, event_name, types.MethodType(event, cls))", "response": "Add an event to the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, *string):\n        self._output.write(' '.join([six.text_type(s) for s in string]))\n        return self", "response": "Writes to the output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nanalyse all ligand SASA.", "response": "def analyse_ligand_sasa(self):\n\t\t\"\"\"Analysis of ligand SASA.\"\"\"\n\t\ti=0\n\t\tstart = timer()\n\t\tif self.trajectory == []:\n\t\t\tself.trajectory = [self.topology_data.universe.filename]\n\t\ttry:\n\t\t\tfor traj in self.trajectory:\n\t\t\t\tnew_traj = mdtraj.load(traj,top=self.topology_data.universe.filename)\n\t\t\t\t#Analyse only non-H ligand\n\t\t\t\tligand_slice = new_traj.atom_slice(atom_indices=self.topology_data.universe.ligand_noH.ids)\n\n\t\t\t\tself.sasa = mdtraj.shrake_rupley(ligand_slice)\n\t\t\t\tself.atom_sasa[i]=self.assign_per_atom_sasa()\n\t\t\t\ti+=1\n\t\t\tself.total_sasa = self.get_total_per_atom_sasa()\n\t\texcept KeyError as e:\n\t\t\tprint \"WARNING: SASA analysis cannot be performed due to incorrect atom names in\"\n\t\t\tprint \"the topology \", e\n\n\t\tprint \"SASA: \"+str(timer()-start)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assign_per_atom_sasa(self):\n\t\tatom_names= [atom.name for atom in self.topology_data.universe.ligand_noH.atoms]\n\t\tsasa_dict = {}\n\t\tfor atom in range(0,self.topology_data.universe.ligand_noH.n_atoms):\n\t\t\tsasa_dict[atom_names[atom]]=[self.sasa[i][atom] for i in range(len(self.sasa))]\n\t\treturn sasa_dict", "response": "Make a dictionary with SASA assigned to each ligand atom stored as list of SASA values over\n\tthe simulation time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_total_per_atom_sasa(self):\n\t\ttotal_sasa = defaultdict(int)\n\t\tfor traj in range(len(self.atom_sasa)):\n\t\t\tfor atom in self.atom_sasa[traj]:\n\t\t\t\ttotal_sasa[atom]+=float(sum((self.atom_sasa[traj][atom])))/len(self.atom_sasa[traj][atom])\n\t\tfor atom in total_sasa:\n\t\t\ttotal_sasa[atom]=float(total_sasa[atom])/len(self.atom_sasa)\n\t\treturn total_sasa", "response": "Return average SASA of the atoms."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, *args):\n        if self.running:\n            return self\n\n        self._mut_finished(False)  # in case of recovery from a disaster.\n        self._mut_running(True)\n\n        stream = self.target(*args)\n\n        # noinspection SpellCheckingInspection\n        def subr():\n            self._mut_running(True)\n            try:\n                for each in stream:\n                    self._product = each\n                    desc = self.descriptor_mapping(each)\n                    event = self.events.get(desc)\n                    if event:\n                        event(self, each, globals)\n                self._mut_finished(True)\n            except ThreadExit:\n                pass\n            finally:\n                self._mut_running(False)\n\n        self._thread = thread = threading.Thread(target=subr, args=())\n        thread.start()\n        return self", "response": "You can choose whether to use lock method when running threads."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the consequences found in all transcripts in the variant_obj.", "response": "def _add_consequences(self, variant_obj):\n        \"\"\"Add the consequences found in all transcripts\n\n        Args:\n            variant_obj (puzzle.models.Variant)\n        \"\"\"\n\n        consequences = set()\n        for transcript in variant_obj.transcripts:\n            for consequence in transcript.consequence.split('&'):\n                consequences.add(consequence)\n\n        variant_obj.consequences = list(consequences)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_impact_severity(self, variant_obj, gemini_variant):\n        gemini_impact = gemini_variant['impact_severity']\n        if gemini_impact == 'MED':\n            gemini_impact = 'MEDIUM'\n        variant_obj.impact_severity = gemini_impact", "response": "Add the impact severity for the most severe consequence\n        \n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds hgnc symbols to the variant object.", "response": "def _add_hgnc_symbols(self, variant_obj):\n        \"\"\"Add hgnc symbols to the variant\n        \n            If there are transcripts use the symbols found here,\n            otherwise use phizz to get the gene ids.\n        \"\"\"\n        hgnc_symbols = set()\n        if variant_obj.transcripts:\n            for transcript in variant_obj.transcripts:\n                if transcript.hgnc_symbol:\n                    hgnc_symbols.add(transcript.hgnc_symbol)\n        else:\n            chrom = variant_obj.CHROM\n            start = variant_obj.start\n            stop = variant_obj.stop\n        \n            hgnc_symbols = get_gene_symbols(chrom, start, stop)\n        #Make unique ids\n        variant_obj.gene_symbols = list(hgnc_symbols)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_genes(self, variant_obj):\n        genes = []\n        ensembl_ids = []\n        hgnc_symbols = []\n        \n        if variant_obj.transcripts:\n            for transcript in variant_obj.transcripts:\n                if transcript.ensembl_id:\n                    ensembl_ids.append(transcript.ensembl_id)\n                if transcript.hgnc_symbol:\n                    hgnc_symbols.append(transcript.hgnc_symbol)\n\n        else:\n            hgnc_symbols = variant_obj.gene_symbols\n        \n        genes = get_gene_info(\n                        ensembl_ids=ensembl_ids, \n                        hgnc_symbols=hgnc_symbols\n                        )\n            \n        for gene in genes:\n            variant_obj.add_gene(gene)", "response": "Add the Gene objects for a variant"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef phase(self, session):\n        sp_key, _ = self._keygen(session)\n        if self.r.sismember(sp_key, session.meepo_unique_id):\n            return \"prepare\"\n        else:\n            return \"commit\"", "response": "Determine the session phase in prepare or commit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing phase for session.", "response": "def prepare(self, session, event):\n        \"\"\"Prepare phase for session.\n\n        :param session: sqlalchemy session\n        \"\"\"\n        if not event:\n            self.logger.warn(\"event empty!\")\n            return\n\n        sp_key, sp_hkey = self._keygen(session)\n\n        def _pk(obj):\n            pk_values = tuple(getattr(obj, c.name)\n                              for c in obj.__mapper__.primary_key)\n            if len(pk_values) == 1:\n                return pk_values[0]\n            return pk_values\n\n        def _get_dump_value(value):\n            if hasattr(value, '__mapper__'):\n                return _pk(value)\n            return value\n        pickled_event = {\n            k: pickle.dumps({_get_dump_value(obj) for obj in objs})\n            for k, objs in event.items()}\n        with self.r.pipeline(transaction=False) as p:\n            p.sadd(sp_key, session.meepo_unique_id)\n            p.hmset(sp_hkey, pickled_event)\n            p.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commit(self, session):\n        sp_key, sp_hkey = self._keygen(session)\n        with self.r.pipeline(transaction=False) as p:\n            p.srem(sp_key, session.meepo_unique_id)\n            p.expire(sp_hkey, 60 * 60)\n            p.execute()", "response": "Commit phase for session.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef session_info(self, session):\n        _, sp_hkey = self._keygen(session)\n        picked_event = self.r.hgetall(sp_hkey)\n        event = {s(k): pickle.loads(v) for k, v in picked_event.items()}\n        return event", "response": "Return all session unique ids recorded in prepare phase."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_info(self, ts=None):\n        sp_key = \"%s:session_prepare\" % self.namespace(ts or int(time.time()))\n        return set(s(m) for m in self.r.smembers(sp_key))", "response": "Return all session unique ids recorded in prepare phase."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear(self, ts=None):\n        sp_key = \"%s:session_prepare\" % self.namespace(ts or int(time.time()))\n        return self.r.delete(sp_key)", "response": "Clear all session in prepare phase."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_file_type(variant_source):\n    file_type = 'unknown'\n    valid_vcf_suffixes = ('.vcf', '.vcf.gz')\n    if variant_source:\n        logger.debug(\"Check file type with file: {0}\".format(variant_source))\n        if variant_source.endswith('.db'):\n            file_type = 'gemini'\n            logger.debug(\"File {0} is a gemini database\".format(variant_source))\n        elif variant_source.endswith(valid_vcf_suffixes):\n            file_type = 'vcf'\n            logger.debug(\"File {0} is a vcf\".format(variant_source))\n        else:\n            logger.debug(\"File is in a unknown format\")\n    \n    return file_type", "response": "Check what kind of file is a variant source and return it"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_variant_type(variant_source):\n    file_type = get_file_type(variant_source)\n    variant_type = 'sv'\n    if file_type == 'vcf':\n        variants = VCF(variant_source)\n    elif file_type == 'gemini':\n        variants = GeminiQuery(variant_source)\n        gemini_query = \"SELECT * from variants\"\n        variants.run(gemini_query)\n    # Check 1000 first variants, if anyone is a snv we set the variant_type\n    # to 'snv'\n    for i,variant in enumerate(variants):\n        if file_type == 'vcf':\n            if variant.is_snp:\n                variant_type = 'snv'\n        elif file_type == 'gemini':\n            if variant['type'] == 'snp':\n                variant_type = 'snv'\n            \n        if i > 1000:\n            break\n    \n    return variant_type", "response": "Try to find out what type of variants that exists in a variant source\n            source_mode is vcf or gemini."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the event code for the event_cd.", "response": "def event_cd(cls, event_tx, ab_des):\n        \"\"\"\n        Event Code for Retrosheet\n        :param event_tx: Event text\n        :param ab_des: at bat description\n        :return: event_cd(int)\n        \"\"\"\n        _event_tx = event_tx.lower()\n        _ab_des = ab_des.lower()\n        # Generic out(event_cd:2)\n        if _event_tx in cls.EVENT_02_GENERIC_OUT_FLYBALL:\n            return 2\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_LINEDRIVE:\n            return 2\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_POPUP:\n            return 2\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_GROUNDBALL:\n            return 2\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_OTHER:\n            return 2\n        # Strike out(event_cd:3)\n        elif _event_tx in cls.EVENT_03_STRIKE_OUT:\n            return 3\n        # Walk(event_cd:14)\n        elif _event_tx in cls.EVENT_14_WALK:\n            return 14\n        # Intent Walk(event_cd:15)\n        elif _event_tx in cls.EVENT_15_INTENT_WALK:\n            return 15\n        # Hit By Pitch(event_cd:16)\n        elif _event_tx in cls.EVENT_16_HIT_BY_PITCH:\n            return 16\n        # Interference(event_cd:17)\n        elif _event_tx.lower().count('interference') > 0:\n            return 17\n        # Error(event_cd:18)\n        elif _event_tx[-5:] == 'error':\n            return 18\n        # Fielder's choice(event_cd:19)\n        elif _event_tx in cls.EVENT_19_FIELDERS_CHOICE:\n            return 19\n        # Single(event_cd:20)\n        elif _event_tx in cls.EVENT_20_SINGLE:\n            return 20\n        # 2B(event_cd:21)\n        elif _event_tx in cls.EVENT_21_DOUBLE:\n            return 21\n        # 3B(event_cd:22)\n        elif _event_tx in cls.EVENT_22_TRIPLE:\n            return 22\n        # HR(event_cd:22)\n        elif _event_tx in cls.EVENT_23_HOME_RUN:\n            return 23\n        # Runner Out\n        elif _event_tx == 'runner out':\n            # Caught stealing(event_cd:6)\n            if _ab_des.count(\"caught stealing\") > 0:\n                return 6\n            # Picks off(event_cd:6)\n            elif _ab_des.count(\"picks off\") > 0:\n                return 8\n            # Unknown event(event_cd:0)\n            else:\n                return 0\n        # Unknown event(event_cd:0)\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef battedball_cd(cls, event_cd, event_tx, ab_des):\n        _event_tx = event_tx.lower()\n        # Fly Out\n        if _event_tx in cls.EVENT_02_GENERIC_OUT_FLYBALL:\n            return 'F'\n        # Line Out\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_LINEDRIVE:\n            return 'L'\n        # Pop Out\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_POPUP:\n            return 'P'\n        # Grounder\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_GROUNDBALL:\n            return 'G'\n        # Force out, double play, triple play\n        elif _event_tx in cls.EVENT_02_GENERIC_OUT_OTHER:\n            return cls._battedball_cd(ab_des)\n        # Single, 2B, 3B, HR\n        elif event_cd in cls.EVENT_CD_HITS:\n            return cls._battedball_cd(ab_des)\n        # Unknown\n        else:\n            return ''", "response": "Return the battedball code for Retrosheet event_cd event_tx ab_des."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _battedball_cd(cls, ab_des):\n        _ab_des = ab_des.lower()\n        if ab_des.count(\"ground\")>0:\n            return 'G'\n        elif _ab_des.count(\"lines\")>0:\n            return 'L'\n        elif _ab_des.count(\"flies\")>0:\n            return 'F'\n        elif _ab_des.count(\"pops\")>0:\n            return 'P'\n        elif _ab_des.count(\"on a line drive\")>0:\n            return 'L'\n        elif _ab_des.count(\"fly ball\")>0:\n            return 'F'\n        elif _ab_des.count(\"ground ball\")>0:\n            return 'G'\n        elif _ab_des.count(\"pop up\")>0:\n            return 'P'\n        else:\n            return ''", "response": "Return the Batted ball Code for at bat description."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the ball count and strike count for a given pitching result.", "response": "def ball_count(cls, ball_tally, strike_tally, pitch_res):\n        \"\"\"\n        Ball/Strike counter\n        :param ball_tally: Ball telly\n        :param strike_tally: Strike telly\n        :param pitch_res: pitching result(Retrosheet format)\n        :return: ball count, strike count\n        \"\"\"\n        b, s = ball_tally, strike_tally\n        if pitch_res == \"B\":\n            if ball_tally < 4:\n                b += 1\n        elif pitch_res == \"S\" or pitch_res == \"C\" or pitch_res == \"X\":\n            if strike_tally < 3:\n                s += 1\n        elif pitch_res == \"F\":\n            if strike_tally < 2:\n                s += 1\n        return b, s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_pa_terminal(cls, ball_tally, strike_tally, pitch_res, event_cd):\n        # In Play\n        if pitch_res == 'X':\n            return True\n        # Strike Out(Strike or Call)\n        elif (pitch_res == 'S' or pitch_res == 'C') and event_cd == 3 and strike_tally == 2:\n            return True\n        # Walk(Ball or Intent Ball)\n        elif pitch_res == 'B' and (event_cd == 14 or event_cd == 15) and ball_tally == 3:\n            return True\n        return False", "response": "Check if a set of attributes is a PA terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshows all cases in the database.", "response": "def cases(ctx, root):\n    \"\"\"\n    Show all cases in the database.\n\n    If no database was found run puzzle init first.\n    \"\"\"\n    root = root or ctx.obj.get('root') or os.path.expanduser(\"~/.puzzle\")\n\n    if os.path.isfile(root):\n        logger.error(\"'root' can't be a file\")\n        ctx.abort()\n\n    logger.info(\"Root directory is: {}\".format(root))\n\n    db_path = os.path.join(root, 'puzzle_db.sqlite3')\n    logger.info(\"db path is: {}\".format(db_path))\n\n    if not os.path.exists(db_path):\n        logger.warn(\"database not initialized, run 'puzzle init'\")\n        ctx.abort()\n\n    store = SqlStore(db_path)\n    for case in store.cases():\n        click.echo(case)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_skeleton(\n        name, src, dest, renames=None, description=None, ignore=False,\n        exclude_dirs=None, exclude_files=None):\n    \"\"\"Copy skeleton\"\"\"\n    fmt = DevFormatter(name, dest, description=description)\n    if os.path.exists(dest):\n        if ignore is False:\n            print(\"project already exists.\")\n            return 1\n    else:\n        os.makedirs(dest)\n    renames = renames or []\n    exclude_dirs = exclude_dirs or []\n    exclude_files = exclude_files or []\n    for root, dirs, files in os.walk(src, topdown=True):\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n        files[:] = [\n            f for f in files if f not in exclude_files\n            and not f.endswith('pyo') and not f.endswith('pyc')]\n        for dname in [join(dest, d) for d in dirs if d]:\n            for rsrc, rdest in renames:\n                dname = dname.replace(rsrc, rdest)\n            try:\n                os.makedirs(dname)\n            except Exception:\n                pass\n        for fname in files:\n            sfile = join(root, fname)\n            dfile = join(dest, relpath(sfile, src))\n            for rsrc, rdest in renames:\n                dfile = dfile.replace(rsrc, rdest)\n            if os.path.exists(dfile):\n                continue\n            name = basename(splitext(dfile)[0])\n            wholetitle = \"{} :core:`{}.{}`\".format(\n                name.title(), basename(dirname(dfile)), name)\n            wholetitlemark = \"=\" * len(wholetitle)\n            fmt.info.update(dict(\n                title=name.title(),\n                titlemark=\"=\" * len(name.title()),\n                wholetitle=wholetitle,\n                wholetitlemark=wholetitlemark,\n                parentname=basename(dirname(dfile)),\n            ))\n            script = open(sfile, 'r').read()\n            try:\n                code = fmt.format(script, **fmt.info)\n                open(dfile, 'w').write(code)\n            except ValueError:\n                pass", "response": "Copy skeleton from src to dest."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a standalone subprojects or submodules script sctrucure", "response": "def init(name, subnames, dest, skeleton, description, project_type, skip_core):\n    \"\"\"Creates a standalone, subprojects or submodules script sctrucure\"\"\"\n    dest = dest or CUR_DIR\n    skeleton = join(skeleton or SKEL_PATH, project_type)\n    project = join(dest, name)\n    script = join(project, name + '.py')\n    core = join(project, name)\n    if project_type == 'standalone':\n        renames = [\n            (join(project, 'project.py'), script),\n            (join(project, 'project'), core)]\n        copy_skeleton(\n            name, skeleton, project, renames=renames, description=description,\n            ignore=False)\n    else:\n        renames = [\n            (join(project, 'project.py'), script),\n            (join(project, 'project'), core)]\n        exclude_dirs = ['submodule'] + (['project'] if skip_core else [])\n        copy_skeleton(\n            name, skeleton, project, renames=renames, description=description,\n            exclude_dirs=exclude_dirs, ignore=True)\n        for subname in subnames:\n            renames = [\n                (join(project, 'submodule'), join(project, subname))\n            ]\n            copy_skeleton(\n                subname, skeleton, project, renames=renames,\n                description=description, ignore=True,\n                exclude_dirs=['project'], exclude_files=['project.py'])\n    return 0, \"\\n{}\\n\".format(project)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the internal consistency of the instance s data structures.", "response": "def _check(self):\n        \"\"\"\n        Assert the internal consistency of the instance's data structures.\n        This method is for debugging only.\n        \"\"\"\n        for k,ix in six.iteritems(self._indices):\n            assert k is not None, 'null key'\n            assert ix, 'Key does not map to any indices'\n            assert ix == sorted(ix), \"Key's indices are not in order\"\n            for i in ix:\n                assert i in self._lines, 'Key index does not map to line'\n                assert self._lines[i].key is not None, 'Key maps to comment'\n                assert self._lines[i].key == k, 'Key does not map to itself'\n                assert self._lines[i].value is not None, 'Key has null value'\n        prev = None\n        for i, line in six.iteritems(self._lines):\n            assert prev is None or prev < i, 'Line indices out of order'\n            prev = i\n            if line.key is None:\n                assert line.value is None, 'Comment/blank has value'\n                assert line.source is not None, 'Comment source not stored'\n                assert loads(line.source) == {}, 'Comment source is not comment'\n            else:\n                assert line.value is not None, 'Key has null value'\n                if line.source is not None:\n                    assert loads(line.source) == {line.key: line.value}, \\\n                        'Key source does not deserialize to itself'\n                assert line.key in self._indices, 'Key is missing from map'\n                assert i in self._indices[line.key], \\\n                    'Key does not map to itself'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(cls, fp):\n        obj = cls()\n        for i, (k, v, src) in enumerate(parse(fp)):\n            if k is not None:\n                obj._indices.setdefault(k, []).append(i)\n            obj._lines[i] = PropertyLine(k, v, src)\n        return obj", "response": "Loads the contents of the ~io. IOBase. readLine - supporting file - like objectfp as a simple line - oriented file - like object and returns a PropertiesFile instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the contents of the string s as a simple line - oriented . properties file and return a PropertiesFile instance.", "response": "def loads(cls, s):\n        \"\"\"\n        Parse the contents of the string ``s`` as a simple line-oriented\n        ``.properties`` file and return a `PropertiesFile` instance.\n\n        ``s`` may be either a text string or bytes string.  If it is a bytes\n        string, its contents are decoded as Latin-1.\n\n        .. versionchanged:: 0.5.0\n            Invalid ``\\\\uXXXX`` escape sequences will now cause an\n            `InvalidUEscapeError` to be raised\n\n        :param string s: the string from which to read the ``.properties``\n            document\n        :rtype: PropertiesFile\n        :raises InvalidUEscapeError: if an invalid ``\\\\uXXXX`` escape sequence\n            occurs in the input\n        \"\"\"\n        if isinstance(s, six.binary_type):\n            fp = six.BytesIO(s)\n        else:\n            fp = six.StringIO(s)\n        return cls.load(fp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the mapping to a file - like object.", "response": "def dump(self, fp, separator='='):\n        \"\"\"\n        Write the mapping to a file in simple line-oriented ``.properties``\n        format.\n\n        If the instance was originally created from a file or string with\n        `PropertiesFile.load()` or `PropertiesFile.loads()`, then the output\n        will include the comments and whitespace from the original input, and\n        any keys that haven't been deleted or reassigned will retain their\n        original formatting and multiplicity.  Key-value pairs that have been\n        modified or added to the mapping will be reformatted with\n        `join_key_value()` using the given separator.  All key-value pairs are\n        output in the order they were defined, with new keys added to the end.\n\n        .. note::\n\n            Serializing a `PropertiesFile` instance with the :func:`dump()`\n            function instead will cause all formatting information to be\n            ignored, as :func:`dump()` will treat the instance like a normal\n            mapping.\n\n        :param fp: A file-like object to write the mapping to.  It must have\n            been opened as a text file with a Latin-1-compatible encoding.\n        :param separator: The string to use for separating new or modified keys\n            & values.  Only ``\" \"``, ``\"=\"``, and ``\":\"`` (possibly with added\n            whitespace) should ever be used as the separator.\n        :type separator: text string\n        :return: `None`\n        \"\"\"\n        ### TODO: Support setting the timestamp\n        for line in six.itervalues(self._lines):\n            if line.source is None:\n                print(join_key_value(line.key, line.value, separator), file=fp)\n            else:\n                fp.write(line.source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dumps(self, separator='='):\n        s = six.StringIO()\n        self.dump(s, separator=separator)\n        return s.getvalue()", "response": "Serializes the mapping to a text string in simple line - oriented format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a copy of the mapping including formatting information", "response": "def copy(self):\n        \"\"\" Create a copy of the mapping, including formatting information \"\"\"\n        dup = type(self)()\n        dup._indices = OrderedDict(\n            (k, list(v)) for k,v in six.iteritems(self._indices)\n        )\n        dup._lines = self._lines.copy()\n        return dup"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and normalize a vector across ring plane.", "response": "def prepare_normal_vectors(atomselection):\n    \"\"\"Create and normalize a vector across ring plane.\"\"\"\n    ring_atomselection = [atomselection.coordinates()[a] for a in [0,2,4]]\n    vect1 = self.vector(ring_atomselection[0],ring_atomselection[1])\n    vect2 = self.vector(ring_atomselection[2],ring_atomselection[0])\n    return self.normalize_vector(np.cross(vect1,vect2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvectors from p1 to p2.", "response": "def vector(p1, p2):\n    \"\"\"Vector from p1 to p2.\n    :param p1: coordinates of point p1\n    :param p2: coordinates of point p2\n    :returns : numpy array with vector coordinates\n    \"\"\"\n    return None if len(p1) != len(p2) else np.array([p2[i] - p1[i] for i in xrange(len(p1))])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the centroid from a 3D point cloud and returns the coordinates of point orthogonally projected on the plane", "response": "def projection(pnormal1, ppoint, tpoint):\n    \"\"\"Calculates the centroid from a 3D point cloud and returns the coordinates\n    :param pnormal1: normal of plane\n    :param ppoint: coordinates of point in the plane\n    :param tpoint: coordinates of point to be projected\n    :returns : coordinates of point orthogonally projected on the plane\n    \"\"\"\n    # Choose the plane normal pointing to the point to be projected\n    pnormal2 = [coo*(-1) for coo in pnormal1]\n    d1 = self.euclidean3d(tpoint, pnormal1 + ppoint)\n    d2 = self.euclidean3d(tpoint, pnormal2 + ppoint)\n    pnormal = pnormal1 if d1 < d2 else pnormal2\n    # Calculate the projection of tpoint to the plane\n    sn = -np.dot(pnormal, self.vector(ppoint, tpoint))\n    sd = np.dot(pnormal, pnormal)\n    sb = sn / sd\n    return [c1 + c2 for c1, c2 in zip(tpoint, [sb * pn for pn in pnormal])]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refresh_session_if_necessary(f):\n\n    @functools.wraps(f)\n    def wrapped(self, *args, **kwargs):\n        try:\n            result = f(self, *args, **kwargs)\n        except Exception as ex:\n            if hasattr(ex, 'code') and ex.code in (401, 403):\n                self.refresh_session()\n                # retry now\n                result = f(self, *args, **kwargs)\n            else:\n                raise ex\n\n        return result\n\n    return wrapped", "response": "Decorator to use on methods that are allowed\n    to retry the request after reauthenticating the client."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_db(db_path):\n    logger.info(\"Creating database\")\n    with closing(connect_database(db_path)) as db:\n        with open(SCHEMA, 'r') as f:\n            db.cursor().executescript(f.read())\n        db.commit()\n    return", "response": "Build the sqlite database"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge(self):\n        self.merged_root = self._recursive_merge(self.root, self.head,\n                                                 self.update)\n        if self.conflicts:\n            raise MergeError('Conflicts Occurred in Merge Process',\n                             self.conflicts)", "response": "Populates the result of the merge with the current configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning list of HGNC symbols matching HPO phenotype ids.", "response": "def hpo_genes(phenotype_ids, username, password):\n    \"\"\"Return list of HGNC symbols matching HPO phenotype ids.\n\n    Args:\n        phenotype_ids (list): list of phenotype ids\n        username (str): username to connect to phenomizer\n        password (str): password to connect to phenomizer\n\n    Returns:\n        query_result: a list of dictionaries on the form\n        {\n            'p_value': float,\n            'gene_id': str,\n            'omim_id': int,\n            'orphanet_id': int,\n            'decipher_id': int,\n            'any_id': int,\n            'mode_of_inheritance': str,\n            'description': str,\n            'raw_line': str\n        }\n    \"\"\"\n    if phenotype_ids:\n        try:\n            results = query_phenomizer.query(username, password, phenotype_ids)\n            return [result for result in results\n                    if result['p_value'] is not None]\n        except SystemExit, RuntimeError:\n            pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _keygen(self, event, ts=None):\n        return \"%s:%s\" % (self.namespace(ts or time.time()), event)", "response": "Generate redis key for event at timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, event, pk, ts=None, ttl=None):\n        key = self._keygen(event, ts)\n        try:\n            self._zadd(key, pk, ts, ttl)\n            return True\n        except redis.ConnectionError as e:\n            # connection error typically happens when redis server can't be\n            # reached or timed out, the error will be silent with an error\n            # log and return None.\n            self.logger.error(\n                \"redis event store failed with connection error %r\" % e)\n            return False", "response": "Add an event to the event store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replay(self, event, ts=0, end_ts=None, with_ts=False):\n        key = self._keygen(event, ts)\n        end_ts = end_ts if end_ts else \"+inf\"\n        elements = self.r.zrangebyscore(key, ts, end_ts, withscores=with_ts)\n\n        if not with_ts:\n            return [s(e) for e in elements]\n        else:\n            return [(s(e[0]), int(e[1])) for e in elements]", "response": "Return the list of events that have a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the last update timestamp of an event pk.", "response": "def query(self, event, pk, ts=None):\n        \"\"\"Query the last update timestamp of an event pk.\n\n        You can pass a timestamp to only look for events later than that\n        within the same namespace.\n\n        :param event: the event name.\n        :param pk: the pk value for query.\n        :param ts: query event pk after ts, default to None which will query\n         all span of current namespace.\n        \"\"\"\n        key = self._keygen(event, ts)\n        pk_ts = self.r.zscore(key, pk)\n        return int(pk_ts) if pk_ts else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclear all stored record of event.", "response": "def clear(self, event, ts=None):\n        \"\"\"Clear all stored record of event.\n\n        :param event: event name to be cleared.\n        :param ts: timestamp used locate the namespace\n        \"\"\"\n        return self.r.delete(self._keygen(event, ts))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_configuration_file(self, file_name):\n        '''Register a file path from which to read parameter values.\n\n        This method can be called multiple times to register multiple files for\n        querying.  Files are expected to be ``ini`` formatted.\n\n        No assumptions should be made about the order that the registered files\n        are read and values defined in multiple files may have unpredictable\n        results.\n\n        **Arguments**\n\n        :``file_name``: Name of the file to add to the parameter search.\n\n        '''\n\n        logger.info('adding %s to configuration files', file_name)\n\n        if file_name not in self.configuration_files and self._inotify:\n            self._watch_manager.add_watch(file_name, pyinotify.IN_MODIFY)\n\n        if os.access(file_name, os.R_OK):\n            self.configuration_files[file_name] = SafeConfigParser()\n            self.configuration_files[file_name].read(file_name)\n        else:\n            logger.warn('could not read %s', file_name)\n            warnings.warn('could not read {}'.format(file_name), ResourceWarning)", "response": "Add a configuration file to the dictionary of configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the parameter to the Parameters object.", "response": "def add_parameter(self, **kwargs):\n        '''Add the parameter to ``Parameters``.\n\n        **Arguments**\n\n        The arguments are lumped into two groups:``Parameters.add_parameter``\n        and ``argparse.ArgumentParser.add_argument``.  Parameters that are only\n        used by ``Parameters.add_parameter`` are removed before ``kwargs`` is\n        passed directly to argparse.ArgumentParser.add_argument``.\n\n        .. note::\n            Once ``parse`` has been called ``Parameters.parsed`` will be True\n            and it is inadvisable to add more parameters to the ``Parameters``.\n\n        *``Parameters.add_parameter`` Arguments*\n\n        :``environment_prefix``: Prefix to add when searching the environment\n                                 for this parameter.  Default:\n                                 os.path.basename(sys.argv[0]).\n        :``group``:              Group (namespace or prefix) for parameter\n                                 (corresponds to section name in configuration\n                                 files).  Default: 'default'.\n        :``options``:            REQUIRED.  The list of options to match for\n                                 this parameter in argv.\n        :``only``:               Iterable containing the components that this\n                                 parameter applies to (i.e. 'environment',\n                                 'configuration', 'argument').  Default:\n                                 ('environment', 'configuration', 'argument').\n\n        *``argparse.ArgumentParser.add_argument`` Arguments*\n\n        :``name or flags``: Positional argument filled in by options keyword\n                            argument.\n        :``action``:        The basic type of action to be taken when this\n                            argument is encountered at the command line.\n        :``nargs``:         The number of command-line arguments that should be\n                            consumed.\n        :``const``:         A constant value required by some action and nargs\n                            selections.\n        :``default``:       The value produced if the argument is absent from\n                            the command line.\n        :``type``:          The type to which the command-line argument should\n                            be converted.\n        :``choices``:       A container of the allowable values for the\n                            argument.\n        :``required``:      Whether or not the command-line option may be\n                            omitted (optionals only).\n        :``help``:          A brief description of what the argument does.\n        :``metavar``:       A name for the argument in usage messages.\n        :``dest``:          The name of the attribute to be added to the object\n                            returned by parse_args().\n\n        '''\n\n        parameter_name = max(kwargs['options'], key = len).lstrip('-')\n\n        if 'dest' in kwargs:\n            parameter_name = kwargs['dest']\n\n        group = kwargs.pop('group', 'default')\n        self.groups.add(group)\n\n        parameter_name = '.'.join([ group, parameter_name ]).lstrip('.').replace('-', '_')\n\n        logger.info('adding parameter %s', parameter_name)\n\n        if self.parsed:\n            logger.warn('adding parameter %s after parse', parameter_name)\n            warnings.warn('adding parameter {} after parse'.format(parameter_name), RuntimeWarning)\n\n        self.parameters[parameter_name] = copy.copy(kwargs)\n        self.parameters[parameter_name]['group'] = group\n        self.parameters[parameter_name]['type'] = kwargs.get('type', str)\n        self.parameters[parameter_name]['environment_prefix'] = kwargs.pop('environment_prefix', os.path.basename(sys.argv[0]))\n\n        if self.parameters[parameter_name]['environment_prefix'] is not None:\n            self.parameters[parameter_name]['environment_prefix'] = self.parameters[parameter_name]['environment_prefix'].upper().replace('-', '_')\n\n        logger.info('group: %s', group)\n\n        self.grouped_parameters.setdefault(group, {}).setdefault(parameter_name.replace(group + '.', ''), self.parameters[parameter_name])\n\n        action_defaults = {\n            'store': kwargs.get('default'),\n            'store_const': kwargs.get('const'),\n            'store_true': False,\n            'store_false': True,\n            'append': [],\n            'append_const': [],\n            'count': 0,\n        }\n\n        self.defaults[parameter_name] = action_defaults[kwargs.get('action', 'store')]\n\n        logger.info('default value: %s', kwargs.get('default'))\n\n        if 'argument' in kwargs.pop('only', [ 'argument' ]):\n            if group not in self._group_parsers:\n                self._group_parsers[group] = self._group_parsers['default'].add_argument_group(group)\n\n            if self._group_prefix and group != 'default':\n                long_option = max(kwargs['options'], key = len)\n\n                kwargs['options'].remove(long_option)\n                kwargs['options'].append(long_option.replace('--', '--' + group.replace('_', '-') + '-'))\n\n                logger.debug('options: %s', kwargs['options'])\n\n            self._group_parsers[group].add_argument(*kwargs.pop('options'), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse sys. argv and sets self. parsed to True if only_known is False.", "response": "def parse(self, only_known = False):\n        '''Ensure all sources are ready to be queried.\n\n        Parses ``sys.argv`` with the contained ``argparse.ArgumentParser`` and\n        sets ``parsed`` to True if ``only_known`` is False.  Once ``parsed`` is\n        set to True, it is inadvisable to add more parameters (cf.\n        ``add_parameter``).  Also, if ``parsed`` is not set to True, retrieving\n        items (cf. ``__getitem__``) will result in a warning that values are\n        being retrieved from an uparsed Parameters.\n\n        **Arguments**\n\n        :``only_known``: If True, do not error or fail when unknown parameters\n                         are encountered.\n\n                         .. note::\n                             If ``only_known`` is True, the ``--help`` and\n                             ``-h`` options on the command line (``sys.argv``)\n                             will be ignored during parsing as it is unexpected\n                             that these parameters' default behavior would be\n                             desired at this stage of execution.\n\n        '''\n\n        self.parsed = not only_known or self.parsed\n\n        logger.info('parsing parameters')\n\n        logger.debug('sys.argv: %s', sys.argv)\n\n        if only_known:\n            args = [ _ for _ in copy.copy(sys.argv) if not re.match('-h|--help', _) ]\n\n            self._group_parsers['default'].parse_known_args(args = args, namespace = self._argument_namespace)\n        else:\n            self._group_parsers['default'].parse_args(namespace = self._argument_namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_configuration_files(self):\n        '''Explicitly read the configuration files.\n\n        Reads all configuration files in this Parameters object.  Even if\n        inotify is watching or a read has already occurred.\n\n        .. note::\n\n           The order that the configuration files are read is not guaranteed.\n\n        '''\n\n        for file_name, configuration_parser in self.configuration_files.items():\n            if os.access(file_name, os.R_OK):\n                configuration_parser.read(file_name)\n            else:\n                logger.warn('could not read %s', file_name)\n                warnings.warn('could not read {}'.format(file_name), ResourceWarning)", "response": "Explicitly read the configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nr_genes(self):\n        if self['genes']:\n            nr_genes = len(self['genes'])\n        else:\n            nr_genes = len(self['gene_symbols'])\n        return nr_genes", "response": "Return the number of genes in the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a md5 key string based on position ref and alt", "response": "def md5(self):\n        \"\"\"Return a md5 key string based on position, ref and alt\"\"\"\n        return hashlib.md5('_'.join([self.CHROM, str(self.POS), self.REF,\n                                     self.ALT])).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a frequency to the frequency field of the specified resource", "response": "def add_frequency(self, name, value):\n        \"\"\"Add a frequency that will be displayed on the variant level\n\n            Args:\n                name (str): The name of the frequency field\n        \"\"\"\n        logger.debug(\"Adding frequency {0} with value {1} to variant {2}\".format(\n            name, value, self['variant_id']))\n        self['frequencies'].append({'label': name, 'value': value})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the max frequency for the variant and the highest frequency for the variant. If max_freq is None go through all frequencies and set the highest frequency for the variant.", "response": "def set_max_freq(self, max_freq=None):\n        \"\"\"Set the max frequency for the variant\n\n            If max_freq use this, otherwise go through all frequencies and\n            set the highest as self['max_freq']\n\n            Args:\n                max_freq (float): The max frequency\n        \"\"\"\n        if max_freq:\n            self['max_freq'] = max_freq\n        else:\n            for frequency in self['frequencies']:\n                if self['max_freq']:\n                    if frequency['value'] > self['max_freq']:\n                        self['max_freq'] = frequency['value']\n                else:\n                    self['max_freq'] = frequency['value']\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a severity to the variant", "response": "def add_severity(self, name, value):\n        \"\"\"Add a severity to the variant\n\n            Args:\n                name (str): The name of the severity\n                value : The value of the severity\n        \"\"\"\n        logger.debug(\"Adding severity {0} with value {1} to variant {2}\".format(\n            name, value, self['variant_id']))\n        self['severities'].append({name: value})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the information for a individual to the variant", "response": "def add_individual(self, genotype):\n        \"\"\"Add the information for a individual\n\n            This adds a genotype dict to variant['individuals']\n\n            Args:\n                genotype (dict): A genotype dictionary\n        \"\"\"\n        logger.debug(\"Adding genotype {0} to variant {1}\".format(\n            genotype, self['variant_id']))\n        self['individuals'].append(genotype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the information transcript to the variant s transcripts", "response": "def add_transcript(self, transcript):\n        \"\"\"Add the information transcript\n\n            This adds a transcript dict to variant['transcripts']\n\n            Args:\n                transcript (dict): A transcript dictionary\n        \"\"\"\n        logger.debug(\"Adding transcript {0} to variant {1}\".format(\n            transcript, self['variant_id']))\n        self['transcripts'].append(transcript)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the information of a gene to the variant s genes list", "response": "def add_gene(self, gene):\n        \"\"\"Add the information of a gene\n\n            This adds a gene dict to variant['genes']\n\n            Args:\n                gene (dict): A gene dictionary\n\n        \"\"\"\n        logger.debug(\"Adding gene {0} to variant {1}\".format(\n            gene, self['variant_id']))\n        self['genes'].append(gene)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_compound(self, compound):\n        logger.debug(\"Adding compound {0} to variant {1}\".format(\n            compound, self['variant_id']))\n        self['compounds'].append(compound)", "response": "Add the information of a compound to the variant"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_variant_id(self, variant_id=None):\n        if not variant_id:\n            variant_id = '_'.join([\n                self.CHROM,\n                str(self.POS),\n                self.REF,\n                self.ALT\n                ])\n\n        logger.debug(\"Updating variant id to {0}\".format(\n            variant_id))\n\n        self['variant_id'] = variant_id", "response": "Set the variant id for this variant"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_to_result(self, lst_idx):\n        self.in_result_idx.add(lst_idx)\n\n        if lst_idx in self.not_in_result_root_match_idx:\n            self.not_in_result_root_match_idx.remove(lst_idx)", "response": "Moves element from lst available at lst_idx."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a match for the elements avaialble at lst_idx and root_idx.", "response": "def add_root_match(self, lst_idx, root_idx):\n        \"\"\"Adds a match for the elements avaialble at lst_idx and root_idx.\"\"\"\n        self.root_matches[lst_idx] = root_idx\n        if lst_idx in self.in_result_idx:\n            return\n\n        self.not_in_result_root_match_idx.add(lst_idx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_transcripts(self, variant_obj, gemini_variant):\n        query = \"SELECT * from variant_impacts WHERE variant_id = {0}\".format(\n            gemini_variant['variant_id']\n        )\n\n        gq = GeminiQuery(self.db)\n        gq.run(query)\n        \n        for gemini_transcript in gq:\n            transcript = Transcript(\n                hgnc_symbol=gemini_transcript['gene'],\n                transcript_id=gemini_transcript['transcript'],\n                consequence=gemini_transcript['impact_so'],\n                biotype=gemini_transcript['biotype'],\n                polyphen=gemini_transcript['polyphen_pred'],\n                sift=gemini_transcript['sift_pred'],\n                HGVSc=gemini_transcript['codon_change'],\n                HGVSp=', '.join([gemini_transcript['aa_change'] or '', gemini_transcript['aa_length'] or ''])\n                )\n            variant_obj.add_transcript(transcript)", "response": "Add all transcripts for a variant"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mysql_pub(mysql_dsn, tables=None, blocking=False, **kwargs):\n    # parse mysql settings\n    parsed = urlparse(mysql_dsn)\n    mysql_settings = {\n        \"host\": parsed.hostname,\n        \"port\": parsed.port or 3306,\n        \"user\": parsed.username,\n        \"passwd\": parsed.password\n    }\n\n    # connect to binlog stream\n    stream = pymysqlreplication.BinLogStreamReader(\n        mysql_settings,\n        server_id=random.randint(1000000000, 4294967295),\n        blocking=blocking,\n        only_events=[DeleteRowsEvent, UpdateRowsEvent, WriteRowsEvent],\n        **kwargs\n    )\n\n    def _pk(values):\n        if isinstance(event.primary_key, str):\n            return values[event.primary_key]\n        return tuple(values[k] for k in event.primary_key)\n\n    for event in stream:\n        if not event.primary_key:\n            continue\n\n        if tables and event.table not in tables:\n            continue\n\n        try:\n            rows = event.rows\n        except (UnicodeDecodeError, ValueError) as e:\n            logger.exception(e)\n            continue\n\n        timestamp = datetime.datetime.fromtimestamp(event.timestamp)\n\n        if isinstance(event, WriteRowsEvent):\n            sg_name = \"%s_write\" % event.table\n            sg = signal(sg_name)\n            sg_raw = signal(\"%s_raw\" % sg_name)\n\n            for row in rows:\n                pk = _pk(row[\"values\"])\n                sg.send(pk)\n                sg_raw.send(row)\n\n                logger.debug(\"%s -> %s, %s\" % (sg_name, pk, timestamp))\n\n        elif isinstance(event, UpdateRowsEvent):\n            sg_name = \"%s_update\" % event.table\n            sg = signal(sg_name)\n            sg_raw = signal(\"%s_raw\" % sg_name)\n\n            for row in rows:\n                pk = _pk(row[\"after_values\"])\n                sg.send(pk)\n                sg_raw.send(row)\n\n                logger.debug(\"%s -> %s, %s\" % (sg_name, pk, timestamp))\n\n        elif isinstance(event, DeleteRowsEvent):\n            sg_name = \"%s_delete\" % event.table\n            sg = signal(sg_name)\n            sg_raw = signal(\"%s_raw\" % sg_name)\n\n            for row in rows:\n                pk = _pk(row[\"values\"])\n                sg.send(pk)\n                sg_raw.send(row)\n\n                logger.debug(\"%s -> %s, %s\" % (sg_name, pk, timestamp))\n\n        signal(\"mysql_binlog_pos\").send(\n            \"%s:%s\" % (stream.log_file, stream.log_pos))", "response": "MySQL row - based binlog events pub."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef propertyNames(self):\n        for k in self.data:\n            yield k\n        if self.defaults is not None:\n            for k in self.defaults.propertyNames():\n                if k not in self.data:\n                    yield k", "response": "r Returns a generator of all distinct keys in the Properties object and\n        s defaults and defaults in unspecified\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef store(self, out, comments=None):\n        dump(self.data, out, comments=comments)", "response": "Write the in - memory properties of the object to out."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the properties object to out in XML format.", "response": "def storeToXML(self, out, comment=None, encoding='UTF-8'):\n        \"\"\"\n        Write the `Properties` object's entries (in unspecified order) in XML\n        properties format to ``out``.\n\n        :param out: a file-like object to write the properties to\n        :type out: binary file-like object\n        :param comment: if non-`None`, ``comment`` will be output as a\n            ``<comment>`` element before the ``<entry>`` elements\n        :type comment: text string or `None`\n        :param string encoding: the name of the encoding to use for the XML\n            document (also included in the XML declaration)\n        :return: `None`\n        \"\"\"\n        dump_xml(self.data, out, comment=comment, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index_queryset(self, using=None):\n        return self.get_model().objects.filter(\n            modified__lte=datetime.datetime.now(),\n            status=STATUS.published\n        )", "response": "Used when the entire index for model is updated."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the molecule in rdkit with the given size and converts it to SMILES format.", "response": "def load_molecule_in_rdkit_smiles(self, molSize,kekulize=True,bonds=[],bond_color=None,atom_color = {}, size= {} ):\n        \"\"\"\n        Loads mol file in rdkit without the hydrogens - they do not have to appear in the final\n        figure. Once loaded, the molecule is converted to SMILES format which RDKit appears to\n        draw best - since we do not care about the actual coordinates of the original molecule, it\n        is sufficient to have just 2D information.\n        Some molecules can be problematic to import and steps such as stopping sanitize function can\n        be taken. This is done automatically if problems are observed. However, better solutions can\n        also be implemented and need more research.\n        The molecule is then drawn from SMILES in 2D representation without hydrogens. The drawing is\n        saved as an SVG file.\n        \"\"\"\n\n        mol_in_rdkit = self.topology_data.mol #need to reload without hydrogens\n        try:\n            mol_in_rdkit = Chem.RemoveHs(mol_in_rdkit)\n            self.topology_data.smiles = Chem.MolFromSmiles(Chem.MolToSmiles(mol_in_rdkit))\n        except ValueError:\n            mol_in_rdkit = Chem.RemoveHs(mol_in_rdkit, sanitize = False)\n            self.topology_data.smiles = Chem.MolFromSmiles(Chem.MolToSmiles(mol_in_rdkit), sanitize=False)\n        self.atom_identities = {}\n        i=0\n        for atom in self.topology_data.smiles.GetAtoms():\n            self.atom_identities[mol_in_rdkit.GetProp('_smilesAtomOutputOrder')[1:].rsplit(\",\")[i]] = atom.GetIdx()\n            i+=1\n        mc = Chem.Mol(self.topology_data.smiles.ToBinary())\n        if kekulize:\n            try:\n                Chem.Kekulize(mc)\n            except:\n                mc = Chem.Mol(self.topology_data.smiles.ToBinary())\n        if not mc.GetNumConformers():\n            rdDepictor.Compute2DCoords(mc)\n        atoms=[]\n        colors={}\n        for i in range(mol_in_rdkit.GetNumAtoms()):\n            atoms.append(i)\n            if len(atom_color)==0:\n                colors[i]=(1,1,1)\n            else:\n                colors = atom_color\n        drawer = rdMolDraw2D.MolDraw2DSVG(int(molSize[0]),int(molSize[1]))\n        drawer.DrawMolecule(mc,highlightAtoms=atoms,highlightBonds=bonds, highlightAtomColors=colors,highlightAtomRadii=size,highlightBondColors=bond_color)\n        drawer.FinishDrawing()\n        self.svg = drawer.GetDrawingText().replace('svg:','')\n        filesvg = open(\"molecule.svg\", \"w+\")\n        filesvg.write(self.svg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convex_hull(self):\n        #Get coordinates of ligand atoms (needed to draw the convex hull around)\n        self.ligand_atom_coords = []\n        ligand_atoms = [x.name for x in self.topology_data.universe.ligand_noH.atoms]\n        with open (\"molecule.svg\", \"r\") as f:\n            lines = f.readlines()\n            i=0\n            for line in lines:\n                if line.startswith(\"<ellipse\"):\n                    self.ligand_atom_coords.append([float(line.rsplit(\"'\",10)[1]), float(line.rsplit(\"'\",10)[3])])\n                    for atom_id in self.atom_identities:\n                        if i == self.atom_identities[atom_id]:\n                            self.ligand_atom_coords_from_diagr[ligand_atoms[int(atom_id)]]=[float(line.rsplit(\"'\",10)[1]), float(line.rsplit(\"'\",10)[3])]\n                    i+=1\n\n        self.ligand_atom_coords=np.array(self.ligand_atom_coords)\n        self.a = geometry.MultiPoint(self.ligand_atom_coords).convex_hull\n        self.b = self.a.boundary.buffer(130).convex_hull\n        self.b_for_all ={}\n        self.b_lenght = self.b.boundary.length\n        for residue in self.topology_data.closest_atoms:\n            mean_distance =np.array([x[1] for x in self.topology_data.closest_atoms[residue]]).mean()\n            b = self.a.boundary.parallel_offset(mean_distance*50+50,\"left\",join_style=2).convex_hull\n            projection =[]\n            projection_init = []\n            for atom in self.topology_data.closest_atoms[residue]:\n                point =geometry.Point((self.ligand_atom_coords_from_diagr[atom[0]][0],self.ligand_atom_coords_from_diagr[atom[0]][1]))\n                projection.append(abs(b.boundary.project(point) % b.boundary.length))\n                projection_init.append(abs(self.b.boundary.project(point) % self.b.boundary.length))\n            # Check whether projections are not crossing the boundary point (i.e. end of circle) - is one number in the projection very different from any other?\n            for (index1,number1), (index2,number2) in combinations(enumerate(projection),2):\n                if abs(number1-number2)>b.boundary.length/2:\n                    proj =[]\n                    for atom in projection:\n                        if atom == max([number1,number2]):\n                            proj.append(atom-b.boundary.length)\n                        else:\n                            proj.append(atom)\n                    projection = proj\n            for (index1,number1), (index2,number2) in combinations(enumerate(projection_init),2):\n                if abs(number1-number2)>self.b.boundary.length/2:\n                    proj =[]\n                    for atom in projection_init:\n                        if atom == max([number1,number2]):\n                            proj.append(atom-self.b.boundary.length)\n                        else:\n                            proj.append(atom)\n                    projection_init = proj\n            self.nearest_points_projection[residue] = np.array(projection).mean()\n            self.b_for_all[residue] = np.array(projection_init).mean()\n            self.nearest_points[residue] = b.boundary.interpolate(self.nearest_points_projection[residue] % b.boundary.length)\n            self.nearest_points_coords[residue]=self.nearest_points[residue].x,self.nearest_points[residue].y", "response": "Draw a convex hull around ligand atoms and expands it around the diagrams."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the overlap in 2D space for a given set of points.", "response": "def calc_2d_forces(self,x1,y1,x2,y2,width):\n        \"\"\"Calculate overlap in 2D space\"\"\"\n        #calculate a\n        if x1>x2:\n            a = x1-x2\n        else:\n            a = x2-x1\n\n        a_sq=a*a\n        #calculate b\n        if y1>y2:\n            b = y1-y2\n        else:\n            b = y2-y1\n\n        b_sq=b*b\n\n        #calculate c\n        from math import sqrt\n        c_sq = a_sq+b_sq\n\n        c = sqrt(c_sq)\n\n        if c > width:\n            return 0,0\n        else:\n            overlap = width-c\n        return -overlap/2, overlap/2"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates forces between two diagrams and pushes them apart by tenth of width", "response": "def do_step(self, values, xy_values,coeff, width):\n        \"\"\"Calculates forces between two diagrams and pushes them apart by tenth of width\"\"\"\n        forces = {k:[] for k,i in enumerate(xy_values)}\n        for (index1, value1), (index2,value2) in combinations(enumerate(xy_values),2):\n            f = self.calc_2d_forces(value1[0],value1[1],value2[0],value2[1],width)\n            if coeff[index1] < coeff[index2]:\n                if self.b_lenght-coeff[index2]<self.b_lenght/10: #a quick and dirty solution, but works\n                    forces[index1].append(f[1]) # push to left (smaller projection value)\n                    forces[index2].append(f[0])\n                else:\n                    #all is normal\n                    forces[index1].append(f[0]) # push to left (smaller projection value)\n                    forces[index2].append(f[1])\n            else:\n                if self.b_lenght-coeff[index1]<self.b_lenght/10: #a quick and dirty solution, but works\n                    forces[index1].append(f[0]) # push to left (smaller projection value)\n                    forces[index2].append(f[1])\n                else:\n                #if all is normal\n                    forces[index1].append(f[1]) # push to left (smaller projection value)\n                    forces[index2].append(f[0])\n        forces = {k:sum(v) for k,v in forces.items()}\n\n        energy = sum([abs(x) for x in forces.values()])\n\n        return [(forces[k]/10+v) for k, v in enumerate(values)], energy"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_new_projection_values(self,width=160):\n        #Make gap between residues bigger if plots have a lot of rings - each ring after the 4th\n        #give extra 12.5px space\n        start = timer()\n        if self.topology_data.ring_number>4:\n            width = width + (self.topology_data.ring_number-4)*12.5\n        values = [v for v in self.nearest_points_projection.values()]\n        xy_values = [v for v in self.nearest_points_coords.values()]\n        coeff_value = [v for v in self.b_for_all.values()]\n        energy = 100\n        while energy > 0.2:\n            values, energy = self.do_step(values,xy_values,coeff_value, width)\n            time = timer() - start\n            i=0\n            xy_values =[]\n            for residue in  self.nearest_points_coords:\n                b = self.a.boundary.parallel_offset(self.topology_data.closest_atoms[residue][0][1]*50+50,\"left\",join_style=2).convex_hull\n                self.nearest_points_projection[residue] = values[i]\n                self.nearest_points[residue] = b.boundary.interpolate(self.nearest_points_projection[residue] % b.boundary.length)\n                self.nearest_points_coords[residue] = self.nearest_points[residue].x, self.nearest_points[residue].y\n                xy_values.append(self.nearest_points_coords[residue])\n                i+=1\n            values = [v for v in self.nearest_points_projection.values()]\n            if time>30:\n                self.molsize1 = self.molsize1 + self.molsize1 * 0.2 #Increase molecule svg size\n                self.molsize2 = self.molsize2 + self.molsize2 * 0.2\n                self.draw_molecule()\n                break\n\n        #Calculate the borders of the final image\n        max_x = max(v[0] for k,v in self.nearest_points_coords.items())\n        min_x = min(v[0] for k,v in self.nearest_points_coords.items())\n        min_y = min(v[1] for k,v in self.nearest_points_coords.items())\n        max_y = max(v[1] for k,v in self.nearest_points_coords.items())\n        if min_x<0:\n            self.x_dim =(max_x-min_x)+600 #600 acts as buffer\n        elif max_x<self.molsize1 and min_x<0: #In case all residues are grouped on one end of the molecule\n            self.x_dim = (self.molsize1-min_x)+600\n        elif max_x<self.molsize1 and min_x>0:\n            self.x_dim = self.molsize1+600\n        else:\n            self.x_dim = max_x+600\n        if min_y<0:\n            self.y_dim = (max_y-min_y)+400 #400 acts as buffer\n        elif max_y<self.molsize2 and min_y<0:\n            self.y_dim = (self.molsize2-min_y)+400\n        elif max_y<self.molsize2 and min_y>0:\n            self.y_dim = self.molsize2+400\n        else:\n            self.y_dim = max_y+400\n        end = timer()\n        print \"Drawing molecule:\"+str(end-start)", "response": "Run do_step function until the diagramms have diverged from each other."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef variants(self, case_id, skip=0, count=1000, filters=None):\n        filters = filters or {}\n        logger.debug(\"Fetching case with case_id: {0}\".format(case_id))\n        case_obj = self.case(case_id)\n        plugin, case_id = self.select_plugin(case_obj)\n        self.filters = plugin.filters\n\n        gene_lists = (self.gene_list(list_id) for list_id\n                      in filters.get('gene_lists', []))\n        nested_geneids = (gene_list.gene_ids for gene_list in gene_lists)\n        gene_ids = set(itertools.chain.from_iterable(nested_geneids))\n\n        if filters.get('gene_ids'):\n            filters['gene_ids'].extend(gene_ids)\n        else:\n            filters['gene_ids'] = gene_ids\n        variants = plugin.variants(case_id, skip, count, filters)\n        return variants", "response": "Fetch variants for a case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch a single variant from variant source.", "response": "def variant(self, case_id, variant_id):\n        \"\"\"Fetch a single variant from variant source.\"\"\"\n        case_obj = self.case(case_id)\n        plugin, case_id = self.select_plugin(case_obj)\n        variant = plugin.variant(case_id, variant_id)\n        return variant"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef redis_es_sub(session, tables, redis_dsn, strict=False,\n                 namespace=None, ttl=3600*24*3, socket_timeout=1):\n    \"\"\"Redis EventSourcing sub.\n\n    This sub should be used together with sqlalchemy_es_pub, it will\n    use RedisEventStore as events storage layer and use the prepare-commit\n    pattern in :func:`sqlalchemy_es_pub` to ensure 100% security on\n    events recording.\n\n    :param session: the sqlalchemy to bind the signal\n    :param tables: tables to be event sourced.\n    :param redis_dsn: the redis server to store event sourcing events.\n    :param strict: arg to be passed to RedisPrepareCommit. If set to True,\n     the exception will not be silent and may cause the failure of sqlalchemy\n     transaction, user should handle the exception in the app side in this\n     case.\n    :param namespace: namespace string or func. If func passed, it should\n     accept timestamp as arg and return a string namespace.\n    :param ttl: expiration time for events stored, default to 3 days.\n    :param socket_timeout: redis socket timeout.\n    \"\"\"\n    logger = logging.getLogger(\"meepo.sub.redis_es_sub\")\n\n    if not isinstance(tables, (list, set)):\n        raise ValueError(\"tables should be list or set\")\n\n    # install event store hook for tables\n    event_store = RedisEventStore(\n        redis_dsn, namespace=namespace, ttl=ttl, socket_timeout=socket_timeout)\n\n    def _es_event_sub(pk, event):\n        if event_store.add(event, str(pk)):\n            logger.info(\"%s: %s -> %s\" % (\n                event, pk, datetime.datetime.now()))\n        else:\n            logger.error(\"event sourcing failed: %s\" % pk)\n\n    events = (\"%s_%s\" % (tb, action) for tb, action in\n              itertools.product(*[tables, [\"write\", \"update\",  \"delete\"]]))\n    for event in events:\n        sub_func = functools.partial(_es_event_sub, event=event)\n        signal(event).connect(sub_func, weak=False)\n\n    # install prepare-commit hook\n    prepare_commit = RedisPrepareCommit(\n        redis_dsn, strict=strict, namespace=namespace,\n        socket_timeout=socket_timeout)\n\n    signal(\"session_prepare\").connect(\n        prepare_commit.prepare, sender=session, weak=False)\n    signal(\"session_commit\").connect(\n        prepare_commit.commit, sender=session, weak=False)\n    signal(\"session_rollback\").connect(\n        prepare_commit.rollback, sender=session, weak=False)\n\n    return event_store, prepare_commit", "response": "Redis EventSourcing sub.\n\n    This sub should be used together with sqlalchemy_es_pub, it will\n    use RedisEventStore as events storage layer and use the prepare-commit\n    pattern in :func:`sqlalchemy_es_pub` to ensure 100% security on\n    events recording.\n\n    :param session: the sqlalchemy to bind the signal\n    :param tables: tables to be event sourced.\n    :param redis_dsn: the redis server to store event sourcing events.\n    :param strict: arg to be passed to RedisPrepareCommit. If set to True,\n     the exception will not be silent and may cause the failure of sqlalchemy\n     transaction, user should handle the exception in the app side in this\n     case.\n    :param namespace: namespace string or func. If func passed, it should\n     accept timestamp as arg and return a string namespace.\n    :param ttl: expiration time for events stored, default to 3 days.\n    :param socket_timeout: redis socket timeout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to a McDevice", "response": "def main():\n    \"\"\"Connect to a McDevice\"\"\"\n    args = setup_parser().parse_args()\n    host = getattr(args, \"host\")\n    port = getattr(args, \"port\")\n    ipv4 = socket.gethostbyname(host)\n    interval = getattr(args, \"interval\")\n\n    receiver = McDevice(ipv4, udp_port=port, mc_interval=interval)\n    receiver.handle_status()\n\n    # wait for UDP messages\n    while True:\n        time.sleep(0.2)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_sqlite(cls, database_path, base_url, version='auto', client_id='ghost-admin'):\n\n        import os\n        import sqlite3\n\n        fd = os.open(database_path, os.O_RDONLY)\n        connection = sqlite3.connect('/dev/fd/%d' % fd)\n        os.close(fd)\n\n        try:\n            row = connection.execute(\n                'SELECT secret FROM clients WHERE slug = ?',\n                (client_id,)\n            ).fetchone()\n\n            if row:\n                return cls(\n                    base_url, version=version,\n                    client_id=client_id, client_secret=row[0]\n                )\n\n            else:\n                raise GhostException(401, [{\n                    'errorType': 'InternalError',\n                    'message': 'No client_secret found for client_id: %s' % client_id\n                }])\n\n        finally:\n            connection.close()", "response": "Initialize a new Ghost API client instance from a SQLite database file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef version(self):\n\n        if self._version != 'auto':\n            return self._version\n\n        if self._version == 'auto':\n            try:\n                data = self.execute_get('configuration/about/')\n                self._version = data['configuration'][0]['version']\n            except GhostException:\n                return self.DEFAULT_VERSION\n\n        return self._version", "response": "Returns the version of the server when initialized as auto otherwise the version passed in at initialization\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nauthenticate with the server.", "response": "def login(self, username, password):\n        \"\"\"\n        Authenticate with the server.\n\n        :param username: The username of an existing user\n        :param password: The password for the user\n        :return: The authentication response from the REST endpoint\n        \"\"\"\n\n        data = self._authenticate(\n            grant_type='password',\n            username=username,\n            password=password,\n            client_id=self._client_id,\n            client_secret=self._client_secret\n        )\n\n        self._username = username\n        self._password = password\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef revoke_access_token(self):\n\n        if not self._access_token:\n            return\n\n        self.execute_post('authentication/revoke', json=dict(\n            token_type_hint='access_token',\n            token=self._access_token\n        ))\n\n        self._access_token = None", "response": "Revoke the access token currently in use."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef revoke_refresh_token(self):\n\n        if not self._refresh_token:\n            return\n\n        self.execute_post('authentication/revoke', json=dict(\n            token_type_hint='refresh_token',\n            token=self._refresh_token\n        ))\n\n        self._refresh_token = None", "response": "Revoke the refresh token currently active."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logout(self):\n\n        self.revoke_refresh_token()\n        self.revoke_access_token()\n\n        self._username, self._password = None, None", "response": "Log out and forgetting the login details."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upload(self, file_obj=None, file_path=None, name=None, data=None):\n\n        close = False\n\n        if file_obj:\n            file_name, content = os.path.basename(file_obj.name), file_obj\n\n        elif file_path:\n            file_name, content = os.path.basename(file_path), open(file_path, 'rb')\n            close = True\n\n        elif name and data:\n            file_name, content = name, data\n\n        else:\n            raise GhostException(\n                400,\n                'Either `file_obj` or `file_path` or '\n                '`name` and `data` needs to be specified'\n            )\n\n        try:\n            content_type, _ = mimetypes.guess_type(file_name)\n\n            file_arg = (file_name, content, content_type)\n\n            response = self.execute_post('uploads/', files={'uploadimage': file_arg})\n\n            return response\n\n        finally:\n            if close:\n                content.close()", "response": "Uploads an image to the server and returns its path on the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes an HTTP GET request against the API endpoints.", "response": "def execute_get(self, resource, **kwargs):\n        \"\"\"\n        Execute an HTTP GET request against the API endpoints.\n        This method is meant for internal use.\n\n        :param resource: The last part of the URI\n        :param kwargs: Additional query parameters (and optionally headers)\n        :return: The HTTP response as JSON or `GhostException` if unsuccessful\n        \"\"\"\n\n        url = '%s/%s' % (self.base_url, resource)\n\n        headers = kwargs.pop('headers', dict())\n\n        headers['Accept'] = 'application/json'\n        headers['Content-Type'] = 'application/json'\n\n        if kwargs:\n            separator = '&' if '?' in url else '?'\n\n            for key, value in kwargs.items():\n                if hasattr(value, '__iter__') and type(value) not in six.string_types:\n                    url = '%s%s%s=%s' % (url, separator, key, ','.join(value))\n\n                else:\n                    url = '%s%s%s=%s' % (url, separator, key, value)\n\n                separator = '&'\n\n        if self._access_token:\n            headers['Authorization'] = 'Bearer %s' % self._access_token\n\n        else:\n            separator = '&' if '?' in url else '?'\n            url = '%s%sclient_id=%s&client_secret=%s' % (\n                url, separator, self._client_id, self._client_secret\n            )\n\n        response = requests.get(url, headers=headers)\n\n        if response.status_code // 100 != 2:\n            raise GhostException(response.status_code, response.json().get('errors', []))\n\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes an HTTP POST request against the API endpoints.", "response": "def execute_post(self, resource, **kwargs):\n        \"\"\"\n        Execute an HTTP POST request against the API endpoints.\n        This method is meant for internal use.\n\n        :param resource: The last part of the URI\n        :param kwargs: Additional parameters for the HTTP call (`request` library)\n        :return: The HTTP response as JSON or `GhostException` if unsuccessful\n        \"\"\"\n\n        return self._request(resource, requests.post, **kwargs).json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute_put(self, resource, **kwargs):\n\n        return self._request(resource, requests.put, **kwargs).json()", "response": "Execute an HTTP PUT request against the API endpoints."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_delete(self, resource, **kwargs):\n\n        self._request(resource, requests.delete, **kwargs)", "response": "Execute an HTTP DELETE request against the API endpoints."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets http content :param url: contents url :param headers: http header :return: BeautifulSoup object", "response": "def _get_content(cls, url, headers=HTTP_HEADERS):\n        \"\"\"\n        Get http content\n        :param url: contents url\n        :param headers: http header\n        :return: BeautifulSoup object\n        \"\"\"\n        session = requests.Session()\n        return session.get(url, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_xml(cls, url, features):\n        req = cls._get_content(url)\n        if req.status_code in range(200, 300):\n            return BeautifulSoup(req.text, features)\n        else:\n            raise MlbAmHttpNotFound('HTTP Error url: {url} status: {status}'.format(url=url, status=req.status_code))", "response": "find xml\n        :param url: contents url\n        :param features: markup provider\n        :param headers: http header\n        :return: BeautifulSoup object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind xml ( list", "response": "def find_xml_all(cls, url, markup, tag, pattern):\n        \"\"\"\n        find xml(list)\n        :param url: contents url\n        :param markup: markup provider\n        :param tag: find tag\n        :param pattern: xml file pattern\n        :return: BeautifulSoup object list\n        \"\"\"\n        body = cls.find_xml(url, markup)\n        return body.find_all(tag, href=re.compile(pattern))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_attribute(cls, soup, key, unknown=None):\n        if key in soup.attrs:\n            return soup.get(key)\n        return unknown", "response": "Get attribute for Beautifulsoup object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_attribute_stats(cls, soup, key, data_type=str, unknown=None):\n        value = cls.get_attribute(soup, key, unknown)\n\n        if value and value != unknown:\n            return data_type(value)\n        return unknown", "response": "Get attribute for Beautifulsoup object containing key and data type"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef token_distance(t1, t2, initial_match_penalization):\n    if isinstance(t1, NameInitial) or isinstance(t2, NameInitial):\n        if t1.token == t2.token:\n            return 0\n        if t1 == t2:\n            return initial_match_penalization\n        return 1.0\n    return _normalized_edit_dist(t1.token, t2.token)", "response": "Calculates the edit distance between two tokens."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the descriptors of the ligands in a molecule and saves that in a dictionary.", "response": "def calculate_descriptors(self,mol):\n\t\t\"\"\"Calculates descriptors such as logP, charges and MR and saves that in a dictionary.\"\"\"\n\t\t#make dictionary\n\t\tself.ligand_atoms = {index:{\"name\":x.name} for index,x in enumerate(self.topology_data.universe.ligand_noH.atoms)}\n\n\t\t#Calculate logP and MR\n\t\tcontribs = self.calculate_logP(mol)\n\n\t\t#Calculate Gasteiger charges\n\t\tself.calculate_Gasteiger_charges(mol)\n\n\t\t#Calculate formal charges\n\t\tfcharges = self.calculate_formal_charge(mol)\n\n\t\tfor atom in self.ligand_atoms.keys():\n\t\t\tself.ligand_atoms[atom][\"logP\"]=contribs[atom][0]\n\t\t\tself.ligand_atoms[atom][\"MR\"]=contribs[atom][1]\n\t\t\tself.ligand_atoms[atom][\"Gasteiger_ch\"]=mol.GetAtomWithIdx(atom).GetProp(\"_GasteigerCharge\")\n\t\t\tself.ligand_atoms[atom][\"Formal charges\"]=fcharges[atom]\n\n\t\t#Determine rotatable bonds\n\t\tself.rot_bonds=self.get_rotatable_bonds(mol)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the rotatable bonds in a ligand molecule", "response": "def get_rotatable_bonds(self,mol):\n\t\t\"\"\"Determines rotatable bonds in a ligand molecule\n\t\tTakes:\n\t\t\t* mol * - mol file in rdkit environment\n\t\tOutput:\n\t\t\t* bonds * - tuples of atom ids\n\t\t\"\"\"\n\t\tRotatableBondSmarts = Chem.MolFromSmarts('[!$(*#*)&!D1]-&!@[!$(*#*)&!D1]')\n\t\tbonds = mol.GetSubstructMatches(RotatableBondSmarts,uniquify=1)\n\t\treturn bonds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the formal charge for each atom in the molecule.", "response": "def calculate_formal_charge(self,mol):\n\t\t\"\"\"Calculates formal charge for each atom.\n\t\tTakes:\n\t\t\t* mol * - mol file in rdkit environment\n\t\tOutput:\n\t\t\t* formal_charges * - list of charges\n\t\t\"\"\"\n\t\tformal_charges = []\n\t\tfor atom in mol.GetAtoms():\n\t\t\tformal_charges.append(atom.GetFormalCharge())\n\t\treturn formal_charges"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns geohash for given point with self. precision", "response": "def get_point_hash(self, point):\n        \"\"\"\n        return geohash for given point with self.precision\n        :param point: GeoPoint instance\n        :return: string\n        \"\"\"\n        return geohash.encode(point.latitude, point.longitude, self.precision)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds point to index", "response": "def add_point(self, point):\n        \"\"\"\n        add point to index, point must be a GeoPoint instance\n        :param point:\n        :return:\n        \"\"\"\n        assert isinstance(point, GeoPoint), \\\n            'point should be GeoPoint instance'\n        point_hash = self.get_point_hash(point)\n        points = self.data.setdefault(point_hash, [])\n        points.append(point)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_nearest_points_dirty(self, center_point, radius, unit='km'):\n        if unit == 'mi':\n            radius = utils.mi_to_km(radius)\n        grid_size = GEO_HASH_GRID_SIZE[self.precision]\n        if radius > grid_size / 2:\n            # radius is too big for current grid, we cannot use 9 neighbors\n            # to cover all possible points\n            suggested_precision = 0\n            for precision, max_size in GEO_HASH_GRID_SIZE.items():\n                if radius > max_size / 2:\n                    suggested_precision = precision - 1\n                    break\n            raise ValueError(\n                'Too large radius, please rebuild GeoHashGrid with '\n                'precision={0}'.format(suggested_precision)\n            )\n        me_and_neighbors = geohash.expand(self.get_point_hash(center_point))\n        return chain(*(self.data.get(key, []) for key in me_and_neighbors))", "response": "get nearest points from circle with given center and radius"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget nearest points from circle with given center and radius", "response": "def get_nearest_points(self, center_point, radius, unit='km'):\n        \"\"\"\n        return list of geo points from circle with given center and radius\n        :param center_point: GeoPoint with center of search circle\n        :param radius: radius of search circle\n        :return: generator with tuple with GeoPoints and distance\n        \"\"\"\n        assert isinstance(center_point, GeoPoint), \\\n            'point should be GeoPoint instance'\n        for point in self.get_nearest_points_dirty(center_point, radius):\n            distance = point.distance_to(center_point, unit)\n            if distance <= radius:\n                yield point, distance"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a specific variant.", "response": "def variant(self, case_id, variant_id):\n        \"\"\"Return a specific variant.\n\n            Args:\n                case_id (str): Path to vcf file\n                variant_id (str): A variant id\n\n            Returns:\n                variant (Variant): The variant object for the given id\n        \"\"\"\n        case_obj = self.case(case_id=case_id)\n        vcf_file_path = case_obj.variant_source\n        self.head = get_header(vcf_file_path)\n\n        self.vep_header = self.head.vep_columns\n        self.snpeff_header = self.head.snpeff_columns\n\n        handle = VCF(vcf_file_path)\n\n        for index, variant in enumerate(handle):\n            index += 1\n            line_id = get_variant_id(variant_line=str(variant)).lstrip('chrCHR')\n            if line_id == variant_id:\n                return self._format_variants(\n                    variant=variant,\n                    index=index,\n                    case_obj=case_obj,\n                    add_all_info=True\n                    )\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all variants in the VCF file.", "response": "def variants(self, case_id, skip=0, count=1000, filters=None):\n        \"\"\"Return all variants in the VCF.\n\n        This function will apply the given filter and return the 'count' first\n        variants. If skip the first 'skip' variants will not be regarded.\n\n            Args:\n                case_id (str): Path to a vcf file (for this adapter)\n                skip (int): Skip first variants\n                count (int): The number of variants to return\n                filters (dict): A dictionary with filters. Currently this will\n                look like: {\n                    gene_list: [] (list of hgnc ids),\n                    frequency: None (float),\n                    cadd: None (float),\n                    sv_len: None (float),\n                    consequence: [] (list of consequences),\n                    is_lof: None (Bool),\n                    genetic_models [] (list of genetic models)\n                    sv_type: List (list of sv types),\n                }\n            Returns:\n                puzzle.constants.Results : Named tuple with variants and\n                                           nr_of_variants\n\n        \"\"\"\n        filters = filters or {}\n        case_obj = self.case(case_id=case_id)\n\n        limit = count + skip\n\n        genes = set()\n        if filters.get('gene_ids'):\n            genes = set([gene_id.strip() for gene_id in filters['gene_ids']])\n\n        frequency = None\n        if filters.get('frequency'):\n            frequency = float(filters['frequency'])\n\n        cadd = None\n        if filters.get('cadd'):\n            cadd = float(filters['cadd'])\n\n        genetic_models = None\n        if filters.get('genetic_models'):\n            genetic_models = set(filters['genetic_models'])\n\n        sv_len = None\n        if filters.get('sv_len'):\n            sv_len = float(filters['sv_len'])\n\n        impact_severities = None\n        if filters.get('impact_severities'):\n            impact_severities = set(filters['impact_severities'])\n\n        vcf_file_path = case_obj.variant_source\n\n        self.head = get_header(vcf_file_path)\n\n        self.vep_header = self.head.vep_columns\n        self.snpeff_header = self.head.snpeff_columns\n\n        variants = self._get_filtered_variants(vcf_file_path, filters)\n\n        result = []\n        skip_index = 0\n        for index, variant in enumerate(variants):\n            index += 1\n            if skip_index >= skip:\n                variant_obj = self._format_variants(\n                     variant=variant,\n                     index=index,\n                     case_obj=case_obj,\n                )\n\n                if genes and variant_obj:\n                    if not set(variant_obj['gene_symbols']).intersection(genes):\n                        variant_obj = None\n\n                if impact_severities and variant_obj:\n                    if not variant_obj['impact_severity'] in impact_severities:\n                        variant_obj = None\n\n                if frequency and variant_obj:\n                    if variant_obj.max_freq > frequency:\n                        variant_obj = None\n\n                if cadd and variant_obj:\n                    if variant_obj['cadd_score'] < cadd:\n                        variant_obj = None\n\n                if genetic_models and variant_obj:\n                    models = set(variant_obj.genetic_models)\n                    if not models.intersection(genetic_models):\n                        variant_obj = None\n\n                if sv_len and variant_obj:\n                    if variant_obj.sv_len < sv_len:\n                        variant_obj = None\n\n                if variant_obj:\n                    skip_index += 1\n\n                    if skip_index <= limit:\n                        result.append(variant_obj)\n                    else:\n                        break\n            else:\n                skip_index += 1\n\n        return Results(result, len(result))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a generator that yields the variants that follow the filters", "response": "def _get_filtered_variants(self, vcf_file_path, filters={}):\n        \"\"\"Check if variants follows the filters\n\n            This function will try to make filters faster for the vcf adapter\n\n            Args:\n                vcf_file_path(str): Path to vcf\n                filters (dict): A dictionary with filters\n\n            Yields:\n                varian_line (str): A vcf variant line\n        \"\"\"\n\n        genes = set()\n        consequences = set()\n        sv_types = set()\n\n        if filters.get('gene_ids'):\n            genes = set([gene_id.strip() for gene_id in filters['gene_ids']])\n\n        if filters.get('consequence'):\n            consequences = set(filters['consequence'])\n\n        if filters.get('sv_types'):\n            sv_types = set(filters['sv_types'])\n\n        logger.info(\"Get variants from {0}\".format(vcf_file_path))\n\n        if filters.get('range'):\n            range_str = \"{0}:{1}-{2}\".format(\n                filters['range']['chromosome'],\n                filters['range']['start'],\n                filters['range']['end'])\n\n            vcf = VCF(vcf_file_path)\n            handle = vcf(range_str)\n        else:\n            handle = VCF(vcf_file_path)\n\n        for variant in handle:\n            variant_line = str(variant)\n            keep_variant = True\n\n            if genes and keep_variant:\n                keep_variant = False\n                for gene in genes:\n                    if \"{0}\".format(gene) in variant_line:\n                        keep_variant = True\n                        break\n\n            if consequences and keep_variant:\n                keep_variant = False\n                for consequence in consequences:\n                    if consequence in variant_line:\n                        keep_variant = True\n                        break\n\n            if sv_types and keep_variant:\n                keep_variant = False\n                for sv_type in sv_types:\n                    if sv_type in variant_line:\n                        keep_variant = True\n                        break\n\n            if keep_variant:\n                yield variant"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a single variant into a Variant object.", "response": "def _format_variants(self, variant, index, case_obj, add_all_info=False):\n        \"\"\"Return a Variant object\n\n        Format variant make a variant that includes enough information for\n        the variant view.\n        If add_all_info then all transcripts will be parsed\n\n        Args:\n            variant (cython2.Variant): A variant object\n            index (int): The index of the variant\n            case_obj (puzzle.models.Case): A case object\n\n        \"\"\"\n        header_line = self.head.header\n        # Get the individual ids for individuals in vcf file\n        vcf_individuals = set([ind_id for ind_id in self.head.individuals])\n\n        #Create a info dict:\n        info_dict = dict(variant.INFO)\n\n        chrom = variant.CHROM\n        if chrom.startswith('chr') or chrom.startswith('CHR'):\n            chrom = chrom[3:]\n\n        variant_obj = Variant(\n                CHROM=chrom,\n                POS=variant.POS,\n                ID=variant.ID,\n                REF=variant.REF,\n                ALT=variant.ALT[0],\n                QUAL=variant.QUAL,\n                FILTER=variant.FILTER,\n            )\n        variant_obj._set_variant_id()\n\n        logger.debug(\"Creating a variant object of variant {0}\".format(\n            variant_obj.variant_id))\n\n        variant_obj.index = index\n        logger.debug(\"Updating index to: {0}\".format(\n            index))\n\n        ########### Get the coordinates for the variant ##############\n        variant_obj.start = variant.start\n        variant_obj.stop = variant.end\n\n        #SV variants needs to be handeled a bit different since the can be huge\n        #it would take to much power to parse all vep/snpeff entrys for these.\n        if self.variant_type == 'sv':\n            variant_obj.stop = int(info_dict.get('END', variant_obj.POS))\n            self._add_sv_coordinates(variant_obj)\n            variant_obj.sv_type = info_dict.get('SVTYPE')\n\n            # Special for FindSV software:\n            # SV specific tag for number of occurances\n            occurances = info_dict.get('OCC')\n            if occurances:\n                logger.debug(\"Updating occurances to: {0}\".format(\n                    occurances))\n                variant_obj['occurances'] = float(occurances)\n                variant_obj.add_frequency('OCC', occurances)\n\n        else:\n            self._add_thousand_g(variant_obj, info_dict)\n            self._add_cadd_score(variant_obj, info_dict)\n            self._add_genetic_models(variant_obj, info_dict)\n            self._add_transcripts(variant_obj, info_dict)\n            self._add_exac(variant_obj, info_dict)\n            \n        self._add_hgnc_symbols(variant_obj)\n\n        if add_all_info:\n            self._add_genotype_calls(variant_obj, str(variant), case_obj)\n            self._add_compounds(variant_obj, info_dict)\n            self._add_gmaf(variant_obj, info_dict)\n            self._add_genes(variant_obj)\n\n\n        ##### Add consequences ####\n        self._add_consequences(variant_obj, str(variant))\n        self._add_most_severe_consequence(variant_obj)\n        self._add_impact_severity(variant_obj)\n        self._add_rank_score(variant_obj, info_dict)\n        variant_obj.set_max_freq()\n        return variant_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncore FNV hash algorithm used in FNV0 and FNV1.", "response": "def fnv(data, hval_init, fnv_prime, fnv_size):\n    \"\"\"\n    Core FNV hash algorithm used in FNV0 and FNV1.\n    \"\"\"\n    assert isinstance(data, bytes)\n\n    hval = hval_init\n    for byte in data:\n        hval = (hval * fnv_prime) % fnv_size\n        hval = hval ^ _get_byte(byte)\n    return hval"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend session_prepare signal in session before_commit.", "response": "def session_prepare(self, session, _):\n        \"\"\"Send session_prepare signal in session \"before_commit\".\n\n        The signal contains another event argument, which records whole info\n        of what's changed in this session, so the signal receiver can receive\n        and record the event.\n        \"\"\"\n        if not hasattr(session, 'meepo_unique_id'):\n            self._session_init(session)\n\n        evt = collections.defaultdict(set)\n        for action in (\"write\", \"update\", \"delete\"):\n            objs = getattr(session, \"pending_%s\" % action)\n            # filter tables if possible\n            if self.tables:\n                objs = [o for o in objs\n                        if o.__table__.fullname in self.tables]\n            for obj in objs:\n                evt_name = \"%s_%s\" % (obj.__table__.fullname, action)\n                evt[evt_name].add(obj)\n                self.logger.debug(\"%s - session_prepare: %s -> %s\" % (\n                    session.meepo_unique_id, evt_name, evt))\n\n        # only trigger signal when event exists\n        if evt:\n            signal(\"session_prepare\").send(session, event=evt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef session_commit(self, session):\n        # this may happen when there's nothing to commit\n        if not hasattr(session, 'meepo_unique_id'):\n            self.logger.debug(\"skipped - session_commit\")\n            return\n\n        # normal session pub\n        self.logger.debug(\"%s - session_commit\" % session.meepo_unique_id)\n        self._session_pub(session)\n        signal(\"session_commit\").send(session)\n        self._session_del(session)", "response": "Send session_commit signal in sqlalchemy before_commit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef session_rollback(self, session):\n        # this may happen when there's nothing to rollback\n        if not hasattr(session, 'meepo_unique_id'):\n            self.logger.debug(\"skipped - session_rollback\")\n            return\n\n        # del session meepo id after rollback\n        self.logger.debug(\"%s - after_rollback\" % session.meepo_unique_id)\n        signal(\"session_rollback\").send(session)\n        self._session_del(session)", "response": "Send session_rollback signal in sqlalchemy after_rollback phase."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\naugments the polar mesh for colormesh.", "response": "def augment_polar_mesh_for_colormesh(r_values, theta_values):\n    \"\"\"\n    Returns polar mesh for matplotlib.pyplot.pcolormesh() in polar coordinates.\n\n    polar coordinates of data points -> polar mesh for colormesh\n    polar coordinates is assumed to be equidistanced in a sense that\n    the r_values and theta_values are assumed to be equally-spaced.\n    \"\"\"\n\n    N_r = len(r_values)\n    N_theta = len(theta_values)\n\n    delta_r = (r_values[-1] - r_values[0]) / (N_r - 1)\n    delta_theta = (theta_values[-1] - theta_values[0]) / (N_theta - 1)\n\n    mesh_r_values = (np.arange(N_r + 1) - 0.5) * delta_r\n    mesh_r_values[0] = 0\n    mesh_theta_values = (np.arange(N_theta + 1) - 0.5) * delta_theta\n\n    mesh_R, mesh_Theta = np.meshgrid(mesh_r_values, mesh_theta_values, indexing='ij')\n\n    return mesh_R, mesh_Theta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef construct_polar_mesh_for_colormesh(r_values, theta_values):\n\n    mesh_R, mesh_Theta = augment_polar_mesh_for_colormesh(r_values, theta_values)\n\n    mesh_X = mesh_R * np.cos(mesh_Theta)\n    mesh_Y = mesh_R * np.sin(mesh_Theta)\n\n    return mesh_X, mesh_Y", "response": "Constructs a polar mesh for a colormesh with the given values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef construct_mesh_for_1d_array(x_values):\n\n    if not isinstance(x_values, np.ndarray):\n        try: x_values = np.array(x_values)\n        except: raise TypeError(\"Can not convert to numpy.ndarray: {0}\".format(x_values))\n    else: assert (x_values.ndim == 1) and (x_values.size >= 2)\n\n    x_mid_points = 0.5 * (x_values[1:] + x_values[:-1])\n\n    x_min_point = x_values[0] - (x_mid_points[0] - x_values[0])\n    x_max_point = x_values[-1] + (x_values[-1] - x_mid_points[-1])\n\n    x_mesh_values = np.empty((len(x_values)+1,), dtype=float)\n    x_mesh_values[0] = x_min_point\n    x_mesh_values[-1] = x_max_point\n    x_mesh_values[1:-1] = x_mid_points\n\n    return x_mesh_values", "response": "Construct the corresponding 1D mesh from 1D array for pcolormesh() in matplotlib."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_global_fontsize_from_fig(fig, scale=1.5):\n\n    fig_size_inch = fig.get_size_inches()\n    fig_size_len_geom_mean = (fig_size_inch[0] * fig_size_inch[1]) ** 0.5\n    rcParams['font.size'] = fig_size_len_geom_mean * scale\n    return rcParams['font.size']", "response": "Set the global font size to be the same as the figure s size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_fig_and_ax_argument(fig, ax, default_figsize=None):\n    if default_figsize is not None:\n        assert type(default_figsize) in [tuple, list]\n        assert len(default_figsize) == 2\n\n    if (fig is None) and (ax is None):\n        fig, ax = plt.subplots(figsize=default_figsize)\n    else:\n        assert (is_figure(fig)) and (is_axes(ax))\n    return fig, ax", "response": "Process fig and ax arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a tuple of axes limits for the given coords.", "response": "def get_square_axes_limits(coords, margin=0.05):\n    \"\"\"Return N-dimensional square's limits\n\n    ## Arguments\n    # 'coords': list of coordinates of poins to be plotted\n    # 'margin': margin to be added from boundaries of the square.\n    - 'margin' can be negative if one wants to reduce the square size.\n\n    ## Example\n    if 'coords' was given as [x,y,z],\n\n    then the resulting square's limits are given by:\n\n    (xlim, ylim, zlim)\n\n    where,\n\n    xlim == (x_mid - max_width, x_mid + max_width)\n    ylim == (y_mid - max_width, y_mid + max_width)\n    zlim == (z_mid - max_width, z_mid + max_width)\n\n    x_mid = 0.5 * (min(x) + max(x)) (and so on)\n\n    max_width = max([x_width, y_width, z_width])\n\n    where x_width = 0.5 * (max(x) - min(x)) (and so on)\n    \"\"\"\n    #coords = [x,y,z]\n    try: coords = [np.array(coord) for coord in coords]\n    except: raise Exception(\"Failed to convert elements of 'coords' into numpy.array\")\n    lims = [(coord.min(), coord.max()) for coord in coords]\n    mids = [0.5 * (lim[0] + lim[1]) for lim in lims]\n    widths = [0.5 * (lim[1] - lim[0]) for lim in lims]\n    max_width = max(widths)\n    max_width += max_width * margin\n    ax_lims = tuple((mid - max_width, mid + max_width) for mid in mids)\n    #xlim, ylim, zlim = ax_lims\n    return ax_lims"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SetPosition(self, track_id, position):\n        self.iface.SetPosition(convert2dbus(track_id, 'o'),\n                               convert2dbus(position, 'x'))", "response": "Sets the current position in microseconds."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_genes(self, variant):\n        ensembl_ids = []\n        hgnc_symbols = []\n        \n        for transcript in variant.transcripts:\n            if transcript.ensembl_id:\n                ensembl_ids.append(transcript.ensembl_id)\n            if transcript.hgnc_symbol:\n                hgnc_symbols.append(transcript.hgnc_symbol)\n        \n        genes = get_gene_info(\n                        ensembl_ids=ensembl_ids, \n                        hgnc_symbols=hgnc_symbols\n                        )\n        return genes", "response": "Get the genes from all transcripts and add them\n        to the variant"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_sv_coordinates(self, variant):\n        variant.stop_chrom = variant.CHROM\n        variant.start = int(variant.POS)\n        \n        # If we have a translocation:\n        if ':' in variant.ALT:\n            other_coordinates = variant.ALT.strip('ACGTN[]').split(':')\n            variant.stop_chrom = other_coordinates[0].lstrip('chrCHR')\n            other_position = other_coordinates[1]\n            # variant.stop = other_position\n\n            #Set 'infinity' to length if translocation\n            variant.sv_len = float('inf')\n            variant.sv_type = 'BND'\n        else:\n            variant.sv_len = variant.stop - variant.start\n\n        variant['cytoband_start'] = get_cytoband_coord(\n                                        chrom=variant.CHROM,\n                                        pos=variant.start\n                                        )\n\n        variant['cytoband_stop'] = get_cytoband_coord(\n                                    chrom=variant.stop_chrom,\n                                    pos=variant.stop\n                                    )", "response": "Add the neccesary sv coordinates for a variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes any preprocessing of the lists.", "response": "def process_lists(self):\n        \"\"\"Do any preprocessing of the lists.\"\"\"\n        for l1_idx, obj1 in enumerate(self.l1):\n            for l2_idx, obj2 in enumerate(self.l2):\n                if self.equal(obj1, obj2):\n                    self.matches.add((l1_idx, l2_idx))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_matches(self, src, src_idx):\n        if src not in ('l1', 'l2'):\n            raise ValueError('Must have one of \"l1\" or \"l2\" as src')\n        if src == 'l1':\n            target_list = self.l2\n        else:\n            target_list = self.l1\n        comparator = {\n            'l1': lambda s_idx, t_idx: (s_idx, t_idx) in self.matches,\n            'l2': lambda s_idx, t_idx: (t_idx, s_idx) in self.matches,\n        }[src]\n\n        return [(trg_idx, obj) for trg_idx, obj in enumerate(target_list)\n                if comparator(src_idx, trg_idx)]", "response": "Get elements equal to the idx t in src from the other list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the topology file and stores the resulting Universe object in self. uni", "response": "def load_topology(self,topology):\n        \"\"\"\n        Loads the topology file (e.g. GRO,PDB,INPCRD) as a MDAnalysis Universe,\n        checks if it can be loaded. Needs to be run before the equivalent function\n        topol.load_trajectory() and provides the snapshot that is going to be used\n        for final residue placement - i.e. the residue coordinates for placement\n        are taken from this file.\n            Takes:\n              * topology * - a topology file e.g. GRO, PDB, INPCRD, CARD, DMS\n            Output:\n              * self.universe * - MDAnalysis Universe\n        \"\"\"\n\n        try:\n            self.universe = MDAnalysis.Universe(topology)\n        except ValueError:\n            print \"Check your topology file - it is either missing or misspelled.\"\n            sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the trajectory file into the Universe and the topology.", "response": "def load_trajectory(self,trajectory):\n        \"\"\"\n        Loads the trajectory files e.g. XTC, DCD, TRJ together with the topology\n        file as a MDAnalysis Universe. This will only be run if a trajectory has\n        been submitted for analysis.\n            Takes:\n                * topology * - a topology file e.g. GRO, PDB, INPCRD, CARD, DMS\n                * trajectory * - a trajectory file e.g. XTC, DCD, TRJ\n            Output:\n                * self.universe * - an MDAnalysis Universe consisting from the\n                topology and trajectory file.\n        \"\"\"\n\n        try:\n            self.universe.load_new(trajectory)\n        except IOError, ValueError:\n            print \"Check you trajectory file \" + trajectory +\"- it might be missing or misspelled.\""}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_mol(self, mol_file):\n        #Check if MOL file has been provided correctly and can be loaded in MDAnalysis\n        if mol_file is None:\n            mol_file = \"lig.mol\"\n\n        self.mol = Chem.MolFromMolFile(mol_file,removeHs=False,sanitize=False)\n        try:\n            self.mol.UpdatePropertyCache(strict=False)\n        except AttributeError:\n            assert self.mol != None, \"The MOL file could not be imported in RDKit environment. Suggestion: Check the atomtypes.\"\n        assert self.mol != None, \"The MOL file could not be imported in RDKit environment.\"", "response": "Loads a MOL file into the RDKit environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenames the ligand with the given ligand name.", "response": "def rename_ligand(self,ligand_name,mol_file):\n        \"\"\"\n        Get an atom selection for the selected from both topology and trajectory. Rename the ligand LIG\n        to help with ligand names that are not standard, e.g. contain numbers.\n            Takes:\n                * ligand_name * - MDAnalysis atom selection for the ligand selected by user\n            Output:\n                * self.ligand * - renamed ligand with resname LIG,\n                * self.ligand_noH * - renamed ligand with resname LIG and without H atoms (these are not\n                present in the final 2D representation and are therefore excluded from some analysis scripts.)\n        \"\"\"\n\n        self.universe.ligand = self.universe.select_atoms(ligand_name)\n        #Both resname and resnames options need to be reset in order for complete renaming.\n        self.universe.ligand.residues.resnames = \"LIG\"\n        self.universe.ligand.resname = \"LIG\"\n        if mol_file is None:\n            self.universe.ligand.write(\"lig.pdb\")\n\n            os.system(\"babel -ipdb lig.pdb -omol lig.mol \")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrenumber the system with the given number", "response": "def renumber_system(self, offset):\n        \"\"\"\n        The residue numbers of the protein can be reformated in case of misallignment with the convention.\n            Takes:\n                 * offset * - a number that represents by how many residues the numbering has to be shifted.\n        \"\"\"\n\n        self.universe.protein = self.universe.select_atoms(\"protein\")\n        self.universe.protein.residues.resids = self.universe.protein.residues.resids+int(offset)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef define_residues_for_plotting_topology(self,cutoff):\n\n        #self.protein_selection = self.universe.select_atoms('all and around '+str(cutoff)+' (segid '+str(self.universe.ligand.segids[0])+' and resid '+str(self.universe.ligand.resids[0])+')')\n        #The previous line was not working on some examples for some reason - switch to more efficient Neighbour Search\n        n = AtomNeighborSearch(self.universe.select_atoms('protein and not name H* or (segid '+str(self.universe.ligand.segids[0])+' and resid '+str(self.universe.ligand.resids[0])+')'), bucket_size=10)\n        self.protein_selection = n.search(self.universe.ligand,cutoff,level=\"A\")\n        for atom in self.protein_selection.atoms:\n                #for non-analysis plots\n                residue = (atom.resname, str(atom.resid), atom.segid)\n                if residue not in self.dict_of_plotted_res.keys() and atom not in self.universe.ligand.atoms:\n                    self.dict_of_plotted_res[residue]=[1]\n        assert len(self.dict_of_plotted_res)!=0, \"Nothing to draw for this ligand (residue number: \"+ self.universe.ligand.resids[0] +\" on the chain \"+ self.universe.ligand.segids[0] +\") - check the position of your ligand within the topology file.\"", "response": "This function defines the residues for plotting in case only a topology file has been submitted. This function defines the residues for plotting in case only a topology file has been submitted."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_the_closest_atoms(self,topology):\n\n        # The measurements are made to ligand molecule without hydrogen atoms (ligand_noH) because the\n        # hydrogen atoms are not plotted in the final graph\n        self.universe.load_new(topology)\n        self.universe.ligand_noH = self.universe.ligand.select_atoms(\"not name H*\")\n        ligand_positions = self.universe.ligand_noH.positions\n\n        for residue in self.dict_of_plotted_res.keys():\n            residue_selection = self.universe.select_atoms(\"resname \"+residue[0]+\" and resid \"+residue[1]+\" and segid \"+ residue[2])\n            residue_positions = residue_selection.positions\n            dist_array = MDAnalysis.analysis.distances.distance_array(ligand_positions,residue_positions)\n            min_values_per_atom={}\n            i=0\n            for atom in self.universe.ligand_noH:\n                min_values_per_atom[atom.name]=dist_array[i].min()\n                i+=1\n            sorted_min_values = sorted(min_values_per_atom.items(), key=operator.itemgetter(1))\n            self.closest_atoms[residue]=[(sorted_min_values[0][0],sorted_min_values[0][1])]", "response": "This function defines the ligand atoms that are closest to the residues that will be plotted in the final graph. This function defines the ligand atoms that are closest to the residues that will be plotted in the final graph. This function defines the ligand atoms that are not closest to the residues that will be plotted in the final graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_data(self, topology, mol_file, ligand_name, offset=0):\n\n        self.load_topology(topology)\n        self.renumber_system(offset)\n        self.rename_ligand(ligand_name,mol_file)\n        self.load_mol(mol_file)", "response": "This function loads all relevant data and creates a Data object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analyse_topology(self,topology, cutoff=3.5):\n\n        self.define_residues_for_plotting_topology(cutoff)\n        self.find_the_closest_atoms(topology)", "response": "This function will be called by the plot_topology method of the ligand class. It will determine the residues that are closest to these residues and then find the residues that are plotted."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the header and return a header object", "response": "def get_header(vcf_file_path):\n    \"\"\"Parse the header and return a header object\n\n        Args:\n            vcf_file_path(str): Path to vcf\n\n        Returns:\n            head: A HeaderParser object\n    \"\"\"\n    logger.info(\"Parsing header of file {0}\".format(vcf_file_path))\n    head = HeaderParser()\n    handle = get_vcf_handle(infile=vcf_file_path)\n    # Parse the header\n    for line in handle:\n        line = line.rstrip()\n        if line.startswith('#'):\n            if line.startswith('##'):\n                head.parse_meta_data(line)\n            else:\n                head.parse_header_line(line)\n        else:\n            break\n\n    handle.close()\n\n    return head"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting start and end datetimes to a tuple of time range.", "response": "def parse_time_range(start_dt, end_dt):\n    \"\"\"\n    Convert the start/end datetimes specified by the user, specifically:\n    - truncate any minutes/seconds\n    - for a missing end time, use start + 24 hours\n    - for a missing start time, use end - 24 hours\n    - for missing start and end, use the last 24 hours\n    \"\"\"\n    now = datetime.now()\n\n    if start_dt and not end_dt:\n        end_dt = now\n\n    elif end_dt and not start_dt:\n        start_dt = _EARLIEST_DATE\n\n    elif not start_dt and not end_dt:  # last 24 hours\n        end_dt = now\n        start_dt = end_dt - timedelta(days=1)\n\n    return tuple(map(truncate_hour_fraction, (start_dt, end_dt)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef integrate(self, rmin=0, rmax=np.inf):\n        if rmin < 0: raise Exception('rmin must be >= 0')\n        integrand = lambda r: self._pdf(r) * 2*np.pi * r\n        return scipy.integrate.quad(integrand,rmin,rmax,full_output=True,epsabs=0)[0]", "response": "Calculates the 2D integral of the surface brightness profile \n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsample the radial distribution from the 2D stellar density.", "response": "def sample_radius(self, n):\n        \"\"\"\n        Sample the radial distribution (deg) from the 2D stellar density.\n        Output is elliptical radius in true projected coordinates.\n        \"\"\"\n        edge = self.edge if self.edge<20*self.extension else 20*self.extension\n        radius = np.linspace(0, edge, 1.e5)\n        pdf = self._pdf(radius) * np.sin(np.radians(radius))\n        cdf = np.cumsum(pdf)\n        cdf /= cdf[-1]\n        fn = scipy.interpolate.interp1d(cdf, list(range(0, len(cdf))))\n        index = np.floor(fn(np.random.uniform(size=n))).astype(int)\n        return radius[index]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample 2D distribution of points in lon lat", "response": "def sample_lonlat(self, n):\n        \"\"\"\n        Sample 2D distribution of points in lon, lat\n        \"\"\"\n        # From http://en.wikipedia.org/wiki/Ellipse#General_parametric_form\n        # However, Martin et al. (2009) use PA theta \"from North to East\"\n        # Definition of phi (position angle) is offset by pi/4\n        # Definition of t (eccentric anamoly) remains the same (x,y-frame usual)\n        # In the end, everything is trouble because we use glon, glat...\n\n        radius = self.sample_radius(n)\n        a = radius; b = self.jacobian * radius\n\n        t = 2. * np.pi * np.random.rand(n)\n        cost,sint = np.cos(t),np.sin(t)\n        phi = np.pi/2. - np.deg2rad(self.theta)\n        cosphi,sinphi = np.cos(phi),np.sin(phi)\n        x = a*cost*cosphi - b*sint*sinphi\n        y = a*cost*sinphi + b*sint*cosphi\n        \n        if self.projector  is None:\n            logger.debug(\"Creating AITOFF projector for sampling\")\n            projector = Projector(self.lon,self.lat,'ait')\n        else:\n            projector = self.projector\n        lon, lat = projector.imageToSphere(x, y)\n        return lon, lat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aggregate_count(keyname):\n    def inner(docs):\n        return sum(doc[keyname] for doc in docs)\n\n    return keyname, inner", "response": "Aggregate the count of the given keyname."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes an aggregate rate for rate_key weighted according to count_rate.", "response": "def aggregate_rate(rate_key, count_key):\n    \"\"\"\n    Compute an aggregate rate for `rate_key` weighted according to\n    `count_rate`.\n    \"\"\"\n    def inner(docs):\n        total = sum(doc[count_key] for doc in docs)\n        weighted_total = sum(doc[rate_key] * doc[count_key] for doc in docs)\n        total_rate = weighted_total / total\n        return total_rate\n\n    return rate_key, inner"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_aggregate(docs, aggregations):\n    new_doc = dict(docs[0])\n\n    for keyname, aggregation_function in aggregations:\n        new_doc[keyname] = aggregation_function(docs)\n\n    return new_doc", "response": "Given a list of documents and aggregations return a single document with the aggregations applied."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef json(value):\n    '''\n    Sanitize the JSON string using the Bleach HTML tag remover\n    '''\n    uncleaned = jsonlib.dumps(value)\n    clean = bleach.clean(uncleaned)\n    return mark_safe(clean)", "response": "Returns the JSON string as a string using the Bleach HTML tag remover\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_pareto_front(population):\n\n    pareto_front = set(range(len(population)))\n\n    for i in range(len(population)):\n        if i not in pareto_front:\n            continue\n\n        ind1 = population[i]\n        for j in range(i + 1, len(population)):\n            ind2 = population[j]\n\n            # if individuals are equal on all objectives, mark one of them (the first encountered one) as dominated\n            # to prevent excessive growth of the Pareto front\n            if ind2.fitness.dominates(ind1.fitness) or ind1.fitness == ind2.fitness:\n                pareto_front.discard(i)\n\n            if ind1.fitness.dominates(ind2.fitness):\n                pareto_front.discard(j)\n\n    return pareto_front", "response": "Finds a subset of nondominated individuals in a given list of individuals"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef t_ID(self, t):\n\n        # If the value is a reserved name, give it the appropriate type (not ID)\n        if t.value in self.reserved:\n            t.type = self.reserved[t.value]\n\n        # If it's a function, give it the FUNC type\n        elif t.value in self.functions:\n            t.type = 'FUNC'\n\n        return t", "response": "t_ID is the ID of the resource t_name is the name of the resource t_function is the function t_ID is the function t_function is the function t_ID is the name of the resource t_ID"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nterms : SUB factor | ADD factor", "response": "def p_factor_unary_operators(self, p):\n        \"\"\"\n        term : SUB factor\n             | ADD factor\n        \"\"\"\n\n        p[0] = p[2]\n        if p[1] == '-':\n            p[0] = Instruction('-x', context={'x': p[0]})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a factor ID instruction to the Instruction list p.", "response": "def p_factor_id(self, p):\n        \"\"\"\n        factor : ID\n        \"\"\"\n\n        def resolve_id(key, context):\n            try:\n                return context[key]\n            except KeyError:\n                raise NameError(\"name '{}' is not defined\".format(key))\n\n        p[0] = Instruction('resolve_id(key, context)', context={\n            'resolve_id': resolve_id,\n            'key': p[1],\n            'context': self.context\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfactoring : factor LBRACK conditional RBRACK", "response": "def p_factor_item(self, p):\n        \"\"\"\n        factor : factor LBRACK conditional RBRACK\n        \"\"\"\n\n        def resolve_item(obj, index):\n            if is_ndarray(obj) or isinstance(obj, (list, tuple)):\n                if not isinstance(index, int):\n                    raise TypeError(\"Not a valid array index: '{}'\".format(index))\n\n            elif isinstance(obj, dict):\n                if not isinstance(index, (six.string_types, int)):\n                    raise TypeError(\"Not a valid dictionary index: '{}'\".format(index))\n\n            else:\n                raise TypeError(\"Object does not support indexing: '{}'\".format(type(obj)))\n\n            return obj[index]\n\n        p[0] = Instruction('resolve_item(obj, index)', context={\n            'resolve_item': resolve_item,\n            'obj': p[1],\n            'index': p[3]\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_ndarray(self, a):\n\n        if isinstance(a, (list, tuple)):\n            a = numpy.array(a)\n\n        if not is_ndarray(a):\n            raise TypeError(\"Expected an ndarray but got object of type '{}' instead\".format(type(a)))\n\n        return a", "response": "Casts Python lists and tuples to a numpy array. Raises an AssertionError if the input is not an array."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the absolute value of a number.", "response": "def fn_abs(self, value):\n        \"\"\"\n        Return the absolute value of a number.\n\n        :param value: The number.\n        :return: The absolute value of the number.\n        \"\"\"\n\n        if is_ndarray(value):\n            return numpy.absolute(value)\n        else:\n            return abs(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fn_get_mask(self, value):\n\n        value = self._to_ndarray(value)\n\n        if numpy.ma.is_masked(value):\n            return value.mask\n        else:\n            return numpy.zeros(value.shape).astype(bool)", "response": "Returns an array mask."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fn_min(self, a, axis=None):\n\n        return numpy.nanmin(self._to_ndarray(a), axis=axis)", "response": "Returns the minimum value of an array ignoring any NaNs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the maximum value of an array ignoring any NaNs.", "response": "def fn_max(self, a, axis=None):\n        \"\"\"\n        Return the maximum of an array, ignoring any NaNs.\n\n        :param a: The array.\n        :return: The maximum value of the array\n        \"\"\"\n\n        return numpy.nanmax(self._to_ndarray(a), axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fn_median(self, a, axis=None):\n\n        return numpy.nanmedian(self._to_ndarray(a), axis=axis)", "response": "Compute the median of an array ignoring NaNs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fn_mean(self, a, axis=None):\n\n        return numpy.nanmean(self._to_ndarray(a), axis=axis)", "response": "Compute the arithmetic mean of an array ignoring NaNs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the standard deviation of an array ignoring NaNs.", "response": "def fn_std(self, a, axis=None):\n        \"\"\"\n        Compute the standard deviation of an array, ignoring NaNs.\n\n        :param a: The array.\n        :return: The standard deviation of the array.\n        \"\"\"\n\n        return numpy.nanstd(self._to_ndarray(a), axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fn_var(self, a, axis=None):\n\n        return numpy.nanvar(self._to_ndarray(a), axis=axis)", "response": "Compute the variance of an array ignoring NaNs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fn_floor(self, value):\n\n        if is_ndarray(value) or isinstance(value, (list, tuple)):\n            return numpy.floor(self._to_ndarray(value))\n        else:\n            return math.floor(value)", "response": "Returns the floor of a number. For positive numbers floor returns a lower value. Eg., floor returns a higher value. For negative numbers floor returns a lower value. Eg., floor returns a lower value. Eg., floor returns a higher value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the ceiling of a number.", "response": "def fn_ceil(self, value):\n        \"\"\"\n        Return the ceiling of a number.\n\n        :param value: The number.\n        :return: The ceiling of the number.\n        \"\"\"\n\n        if is_ndarray(value) or isinstance(value, (list, tuple)):\n            return numpy.ceil(self._to_ndarray(value))\n        else:\n            return math.ceil(value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fn_int(self, value):\n\n        if is_ndarray(value) or isinstance(value, (list, tuple)):\n            return self._to_ndarray(value).astype('int')\n        else:\n            return int(value)", "response": "Return the value cast to an int."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncasting the value to an 8 - bit signed integer or a Python int", "response": "def fn_int8(self, value):\n        \"\"\"\n        Return the value cast to an 8-bit signed integer (numpy array) or a Python int (single value)\n\n        :param value: The number or array\n        :return: The number or array as int/int8\n        \"\"\"\n\n        if is_ndarray(value) or isinstance(value, (list, tuple)):\n            return self._to_ndarray(value).astype(numpy.int8)\n        else:\n            return int(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fn_int16(self, value):\n\n        if is_ndarray(value) or isinstance(value, (list, tuple)):\n            return self._to_ndarray(value).astype(numpy.int16)\n        else:\n            return int(value)", "response": "Casts the value to an 16 - bit signed integer or a Python int"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncast the value to an 32 - bit signed integer or a Python int", "response": "def fn_int32(self, value):\n        \"\"\"\n        Return the value cast to an 32-bit signed integer (numpy array) or a Python int (single value)\n\n        :param value: The number or array\n        :return: The number or array as int/int8\n        \"\"\"\n\n        if is_ndarray(value) or isinstance(value, (list, tuple)):\n            return self._to_ndarray(value).astype(numpy.int32)\n        else:\n            return int(value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the value cast to a float.", "response": "def fn_float(self, value):\n        \"\"\"\n        Return the value cast to a float.\n\n        :param value: The number.\n        :return: The number as a float.\n        \"\"\"\n\n        if is_ndarray(value) or isinstance(value, (list, tuple)):\n            return self._to_ndarray(value).astype('float')\n        else:\n            return float(value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncoercing a datetime or string into a datetime. datetime object", "response": "def make_datetime(dt, date_parser=parse_date):\n    \"\"\"Coerce a datetime or string into datetime.datetime object\n\n    Arguments:\n      dt (str or datetime.datetime or atetime.time or numpy.Timestamp): time or date\n        to be coerced into a `datetime.date` object\n\n    Returns:\n      datetime.time: Time of day portion of a `datetime` string or object\n\n    >>> make_date('')\n    datetime.date(1970, 1, 1)\n    >>> make_date(None)\n    datetime.date(1970, 1, 1)\n    >>> make_date(\"11:59 PM\") == datetime.date.today()\n    True\n    >>> make_date(datetime.datetime(1999, 12, 31, 23, 59, 59))\n    datetime.date(1999, 12, 31)\n    >>> make_datetime(['1970-10-31', '1970-12-25'])  # doctest: +NORMALIZE_WHITESPACE\n    [datetime.datetime(1970, 10, 31, 0, 0), datetime.datetime(1970, 12, 25, 0, 0)]\n    \"\"\"\n    if (isinstance(dt, (datetime.datetime, datetime.date, datetime.time, pd.Timestamp, np.datetime64)) or\n            dt in (float('nan'), float('inf'), float('-inf'), None, '')):\n        return dt\n    if isinstance(dt, (float, int)):\n        return datetime_from_ordinal_float(dt)\n    if isinstance(dt, datetime.date):\n        return datetime.datetime(dt.year, dt.month, dt.day)\n    if isinstance(dt, datetime.time):\n        return datetime.datetime(1, 1, 1, dt.hour, dt.minute, dt.second, dt.microsecond)\n    if not dt:\n        return datetime.datetime(1970, 1, 1)\n    if isinstance(dt, basestring):\n        try:\n            return date_parser(dt)\n        except ValueError:\n            print('Unable to make_datetime({})'.format(dt))\n            raise\n    try:\n        return datetime.datetime(*dt.timetuple()[:7])\n    except AttributeError:\n        try:\n            dt = list(dt)\n            if 0 < len(dt) < 7:\n                try:\n                    return datetime.datetime(*dt[:7])\n                except (TypeError, IndexError, ValueError):\n                    pass\n        except (TypeError, IndexError, ValueError, AttributeError):\n            # dt is not iterable\n            return dt\n\n    return [make_datetime(val, date_parser=date_parser) for val in dt]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quantize_datetime(dt, resolution=None):\n    # FIXME: this automatically truncates off microseconds just because timtuple() only goes out to sec\n    resolution = int(resolution or 6)\n    if hasattr(dt, 'timetuple'):\n        dt = dt.timetuple()  # strips timezone info\n\n    if isinstance(dt, time.struct_time):\n        # strip last 3 fields (tm_wday, tm_yday, tm_isdst)\n        dt = list(dt)[:6]\n        # struct_time has no microsecond, but accepts float seconds\n        dt += [int((dt[5] - int(dt[5])) * 1000000)]\n        dt[5] = int(dt[5])\n        return datetime.datetime(*(dt[:resolution] + [1] * max(3 - resolution, 0)))\n\n    if isinstance(dt, tuple) and len(dt) <= 9 and all(isinstance(val, (float, int)) for val in dt):\n        dt = list(dt) + [0] * (max(6 - len(dt), 0))\n        # if the 6th element of the tuple looks like a float set of seconds need to add microseconds\n        if len(dt) == 6 and isinstance(dt[5], float):\n            dt = list(dt) + [1000000 * (dt[5] - int(dt[5]))]\n            dt[5] = int(dt[5])\n        dt = tuple(int(val) for val in dt)\n        return datetime.datetime(*(dt[:resolution] + [1] * max(resolution - 3, 0)))\n\n    return [quantize_datetime(value) for value in dt]", "response": "Quantize a datetime to integer years months days hours minutes seconds or microseconds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike datetime. ordinal but rather than integer allows fractional days", "response": "def ordinal_float(dt):\n    \"\"\"Like datetime.ordinal, but rather than integer allows fractional days (so float not ordinal at all)\n\n    Similar to the Microsoft Excel numerical representation of a datetime object\n\n    >>> ordinal_float(datetime.datetime(1970, 1, 1))\n    719163.0\n    >>> ordinal_float(datetime.datetime(1, 2, 3, 4, 5, 6, 7))  # doctest: +ELLIPSIS\n    34.1702083334143...\n    \"\"\"\n    try:\n        return dt.toordinal() + ((((dt.microsecond / 1000000.) + dt.second) / 60. + dt.minute) / 60 + dt.hour) / 24.\n    except AttributeError:\n        try:\n            return ordinal_float(make_datetime(dt))\n        except ValueError:\n            pass\n    dt = list(make_datetime(val) for val in dt)\n    assert(all(isinstance(val, datetime.datetime) for val in dt))\n    return [ordinal_float(val) for val in dt]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninverse of ordinal_float() converts a float number of days back to a datetime object", "response": "def datetime_from_ordinal_float(days):\n    \"\"\"Inverse of `ordinal_float()`, converts a float number of days back to a `datetime` object\n\n    >>> dt = datetime.datetime(1970, 1, 1)\n    >>> datetime_from_ordinal_float(ordinal_float(dt)) == dt\n    True\n    >>> dt = datetime.datetime(1, 2, 3, 4, 5, 6, 7)\n    >>> datetime_from_ordinal_float(ordinal_float(dt)) == dt\n    True\n    \"\"\"\n    if isinstance(days, (float, int)):\n        if np.isnan(days) or days in set((float('nan'), float('inf'), float('-inf'))):\n            return days\n        dt = datetime.datetime.fromordinal(int(days))\n        seconds = (days - int(days)) * 3600. * 24.\n        microseconds = (seconds - int(seconds)) * 1000000\n        return dt + datetime.timedelta(days=0, seconds=int(seconds), microseconds=int(round(microseconds)))\n    return [datetime_from_ordinal_float(d) for d in days]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a date - time tag suitable for appending to a file name.", "response": "def timetag_str(dt=None, sep='-', filler='0', resolution=6):\n    \"\"\"Generate a date-time tag suitable for appending to a file name.\n\n    >>> timetag_str(resolution=3) == '-'.join('{0:02d}'.format(i) for i in\n    ...             tuple(datetime.datetime.now().timetuple()[:3]))\n    True\n    >>> timetag_str(datetime.datetime(2004,12,8,1,2,3,400000))\n    '2004-12-08-01-02-03'\n    >>> timetag_str(datetime.datetime(2004,12,8))\n    '2004-12-08-00-00-00'\n    >>> timetag_str(datetime.datetime(2003,6,19), filler='')\n    '2003-6-19-0-0-0'\n    \"\"\"\n    resolution = int(resolution or 6)\n    if sep in (None, False):\n        sep = ''\n    sep = str(sep)\n    dt = datetime.datetime.now() if dt is None else dt\n    # FIXME: don't use timetuple which truncates microseconds\n    return sep.join(('{0:' + filler + ('2' if filler else '') + 'd}').format(i)\n                    for i in tuple(dt.timetuple()[:resolution]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd timezone information to a datetime object only if it is naive.", "response": "def make_tz_aware(dt, tz='UTC', is_dst=None):\n    \"\"\"Add timezone information to a datetime object, only if it is naive.\n\n    >>> make_tz_aware(datetime.datetime(2001, 9, 8, 7, 6))\n    datetime.datetime(2001, 9, 8, 7, 6, tzinfo=<UTC>)\n    >>> make_tz_aware(['2010-01-01'], 'PST')\n    [datetime.datetime(2010, 1, 1, 0, 0, tzinfo=<DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>)]\n    >>> make_tz_aware(['1970-10-31', '1970-12-25', '1971-07-04'], 'CDT')  # doctest: +NORMALIZE_WHITESPACE\n    [datetime.datetime(1970, 10, 31, 0, 0, tzinfo=<DstTzInfo 'US/Central' CST-1 day, 18:00:00 STD>),\n     datetime.datetime(1970, 12, 25, 0, 0, tzinfo=<DstTzInfo 'US/Central' CST-1 day, 18:00:00 STD>),\n     datetime.datetime(1971,  7,  4, 0, 0, tzinfo=<DstTzInfo 'US/Central' CDT-1 day, 19:00:00 DST>)]\n    >>> make_tz_aware([None, float('nan'), float('inf'), 1980, 1979.25*365.25, '1970-10-31',\n    ...                '1970-12-25', '1971-07-04'],\n    ...               'CDT')  # doctest: +NORMALIZE_WHITESPACE\n    [None, nan, inf,\n     datetime.datetime(6, 6, 3, 0, 0, tzinfo=<DstTzInfo 'US/Central' LMT-1 day, 18:09:00 STD>),\n     datetime.datetime(1980, 4, 16, 1, 30, tzinfo=<DstTzInfo 'US/Central' CST-1 day, 18:00:00 STD>),\n     datetime.datetime(1970, 10, 31, 0, 0, tzinfo=<DstTzInfo 'US/Central' CST-1 day, 18:00:00 STD>),\n     datetime.datetime(1970, 12, 25, 0, 0, tzinfo=<DstTzInfo 'US/Central' CST-1 day, 18:00:00 STD>),\n     datetime.datetime(1971, 7, 4, 0, 0, tzinfo=<DstTzInfo 'US/Central' CDT-1 day, 19:00:00 DST>)]\n    >>> make_tz_aware(datetime.time(22, 23, 59, 123456))\n    datetime.time(22, 23, 59, 123456, tzinfo=<UTC>)\n    >>> make_tz_aware(datetime.time(22, 23, 59, 123456), 'PDT', is_dst=True)\n    datetime.time(22, 23, 59, 123456, tzinfo=<DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)\n\n    \"\"\"\n    # make sure dt is a datetime, time, or list of datetime/times\n    dt = make_datetime(dt)\n    if not isinstance(dt, (list, datetime.datetime, datetime.date, datetime.time, pd.Timestamp)):\n        return dt\n    # TODO: deal with sequence of timezones\n    try:\n        tz = dt.tzinfo or tz\n    except (ValueError, AttributeError, TypeError):\n        pass\n    try:\n        tzstr = str(tz).strip().upper()\n        if tzstr in TZ_ABBREV_NAME:\n            is_dst = is_dst or tzstr.endswith('DT')\n            tz = TZ_ABBREV_NAME.get(tzstr, tz)\n    except (ValueError, AttributeError, TypeError):\n        pass\n    try:\n        tz = pytz.timezone(tz)\n    except (ValueError, AttributeError, TypeError):\n        # from traceback import print_exc\n        # print_exc()\n        pass\n    try:\n        return tz.localize(dt, is_dst=is_dst)\n    except (ValueError, AttributeError, TypeError):\n        # from traceback import print_exc\n        # print_exc()  # TypeError: unsupported operand type(s) for +: 'datetime.time' and 'datetime.timedelta'\n        pass\n    # could be datetime.time, which can't be localized. Insted `replace` the TZ\n    # don't try/except in case dt is not a datetime or time type -- should raise an exception\n    if not isinstance(dt, list):\n        return dt.replace(tzinfo=tz)\n\n    return [make_tz_aware(dt0, tz=tz, is_dst=is_dst) for dt0 in dt]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlimiting a datetime to a valid range for datetime64 and Timestamp objects.", "response": "def clip_datetime(dt, tz=DEFAULT_TZ, is_dst=None):\n    \"\"\"Limit a datetime to a valid range for datetime, datetime64, and Timestamp objects\n    >>> from datetime import timedelta\n    >>> clip_datetime(MAX_DATETIME + timedelta(100)) == pd.Timestamp(MAX_DATETIME, tz='utc') == MAX_TIMESTAMP\n    True\n    >>> MAX_TIMESTAMP\n    Timestamp('2262-04-11 23:47:16.854775+0000', tz='UTC')\n    \"\"\"\n    if isinstance(dt, datetime.datetime):\n        # TODO: this gives up a day of datetime range due to assumptions about timezone\n        #       make MIN/MAX naive and replace dt.replace(tz=None) before comparison\n        #       set it back when done\n        dt = make_tz_aware(dt, tz=tz, is_dst=is_dst)\n        try:\n            return pd.Timestamp(dt)\n        except (ValueError, AttributeError):\n            pass\n        if dt > MAX_DATETIME:\n            return MAX_TIMESTAMP\n        elif dt < MIN_DATETIME:\n            return MIN_TIMESTAMP\n        return NAT\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef translate_purposes(f):\n    @wraps(f)\n    def wr(r, pc):\n        tmp = []\n        for P in r[\"purposes\"]:\n            try:\n                tmp.append(POSTCODE_API_TYPEDEFS_PURPOSES[P])\n            except:\n                logger.warning(\"Warning: {}: \"\n                               \"cannot translate 'purpose': {}\".format(pc, P))\n                tmp.append(P)\n\n        r.update({\"purposes\": tmp})\n        return f(r, pc)\n\n    return wr", "response": "decorator to translate the values of the purposes field of the API response into\n    translated values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef jenks(data, num_breaks):\n\n    data = numpy.ma.compressed(data)\n    if len(data) > 1000:\n        data.sort()\n        ls = numpy.linspace(0, len(data)-1, 1000)\n        ls = [int(round(x)) for x in ls]\n        data_list = data[ls]\n    else:\n        data_list = data\n\n    data_list.sort()\n\n    mat1 = []\n    for i in range(0, len(data_list) + 1):\n        temp = []\n        for j in range(0, num_breaks + 1):\n            temp.append(0)\n        mat1.append(temp)\n\n    mat2 = []\n    for i in range(0, len(data_list) + 1):\n        temp = []\n        for j in range(0, num_breaks + 1):\n            temp.append(0)\n        mat2.append(temp)\n\n    for i in range(1, num_breaks + 1):\n        mat1[1][i] = 1\n        mat2[1][i] = 0\n        for j in range(2, len(data_list) + 1):\n            mat2[j][i] = float('inf')\n\n    v = 0.0\n    for l in range(2, len(data_list) + 1):\n        s1 = 0.0\n        s2 = 0.0\n        w = 0.0\n        for m in range(1, l + 1):\n            i3 = l - m + 1\n\n            val = float(data_list[i3-1])\n\n            s2 += val * val\n            s1 += val\n\n            w += 1\n            v = s2 - (s1 * s1) / w\n            i4 = i3 - 1\n\n            if i4 != 0:\n                for j in range(2, num_breaks + 1):\n                    if mat2[l][j] >= (v + mat2[i4][j - 1]):\n                        mat1[l][j] = i3\n                        mat2[l][j] = v + mat2[i4][j - 1]\n\n        mat1[l][1] = 1\n        mat2[l][1] = v\n\n    k = len(data_list)\n    kclass = []\n    for i in range(0, num_breaks + 1):\n        kclass.append(0)\n\n    kclass[num_breaks] = float(data_list[len(data_list) - 1])\n\n    count_num = num_breaks\n    while count_num >= 2:\n        id = int((mat1[k][count_num]) - 2)\n\n        kclass[count_num - 1] = data_list[id]\n        k = int((mat1[k][count_num] - 1))\n        count_num -= 1\n\n    return [float(x) for x in kclass][1:]", "response": "Calculate Jenks natural breaks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quantile(data, num_breaks):\n\n    def scipy_mquantiles(a, prob=list([.25,.5,.75]), alphap=.4, betap=.4, axis=None, limit=()):\n        \"\"\" function copied from scipy 0.13.3::scipy.stats.mstats.mquantiles \"\"\"\n\n        def _quantiles1D(data,m,p):\n            x = numpy.sort(data.compressed())\n            n = len(x)\n            if n == 0:\n                return numpy.ma.array(numpy.empty(len(p), dtype=float), mask=True)\n            elif n == 1:\n                return numpy.ma.array(numpy.resize(x, p.shape), mask=numpy.ma.nomask)\n            aleph = (n*p + m)\n            k = numpy.floor(aleph.clip(1, n-1)).astype(int)\n            gamma = (aleph-k).clip(0,1)\n            return (1.-gamma)*x[(k-1).tolist()] + gamma*x[k.tolist()]\n\n        # Initialization & checks ---------\n        data = numpy.ma.array(a, copy=False)\n        if data.ndim > 2:\n            raise TypeError(\"Array should be 2D at most !\")\n        #\n        if limit:\n            condition = (limit[0] < data) & (data < limit[1])\n            data[~condition.filled(True)] = numpy.ma.masked\n        #\n        p =  numpy.array(prob, copy=False, ndmin=1)\n        m = alphap + p*(1.-alphap-betap)\n        # Computes quantiles along axis (or globally)\n        if (axis is None):\n            return _quantiles1D(data, m, p)\n        return numpy.ma.apply_along_axis(_quantiles1D, axis, data, m, p)\n\n    return scipy_mquantiles(data, numpy.linspace(1.0 / num_breaks, 1, num_breaks))", "response": "Calculates the quantile breaks for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef equal(data, num_breaks):\n\n    step = (numpy.amax(data) - numpy.amin(data)) / num_breaks\n    return numpy.linspace(numpy.amin(data) + step, numpy.amax(data), num_breaks)", "response": "Calculate equal interval breaks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getCatalogFile(catalog_dir, mc_source_id):\n    catalog_infiles = sorted(glob.glob(catalog_dir + '/*catalog*.fits'))\n    mc_source_id_array = []\n    catalog_infile_index_array = []\n    for ii, catalog_infile in enumerate(catalog_infiles):\n        mc_source_id_min = int(os.path.basename(catalog_infile).split('.')[0].split('mc_source_id_')[-1].split('-')[0])\n        mc_source_id_max = int(os.path.basename(catalog_infile).split('.')[0].split('mc_source_id_')[-1].split('-')[1])\n        assert (mc_source_id_max > mc_source_id_min) & (mc_source_id_min >= 1), 'Found invalue MC_SOURCE_ID values in filenames'\n        mc_source_id_array.append(np.arange(mc_source_id_min, mc_source_id_max + 1))\n        catalog_infile_index_array.append(np.tile(ii, 1 + (mc_source_id_max - mc_source_id_min)))\n\n    mc_source_id_array = np.concatenate(mc_source_id_array)\n    catalog_infile_index_array = np.concatenate(catalog_infile_index_array)\n\n    assert len(mc_source_id_array) == len(np.unique(mc_source_id_array)), 'Found non-unique MC_SOURCE_ID values in filenames'\n    assert np.in1d(mc_source_id, mc_source_id_array), 'Requested MC_SOURCE_ID value not among files'\n    mc_source_id_index = np.nonzero(mc_source_id == mc_source_id_array)[0]\n    return catalog_infiles[catalog_infile_index_array[mc_source_id_index]]", "response": "Returns the file containing the stellar catalog containing the given MC_SOURCE_ID value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(filename,**kwargs):\n    base,ext = os.path.splitext(filename)\n    if ext in ('.fits','.fz'):\n        # Abstract fits here...\n        return fitsio.read(filename,**kwargs)\n    elif ext in ('.npy'):\n        return np.load(filename,**kwargs)\n    elif ext in ('.csv'):\n        return np.recfromcsv(filename,**kwargs)\n    elif ext in ('.txt','.dat'):\n        return np.genfromtxt(filename,**kwargs)\n\n    msg = \"Unrecognized file type: %s\"%filename\n    raise ValueError(msg)", "response": "Read a generic input file into a recarray."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(filename,data,**kwargs):\n    base,ext = os.path.splitext(filename)\n    if ext in ('.fits','.fz'):\n        # Abstract fits here...\n        return fitsio.write(filename,data,**kwargs)\n    elif ext in ('.npy'):\n        return np.save(filename,data,**kwargs)\n    elif ext in ('.csv'):\n        return np.savetxt(filename,data,header=','.join(data.dtype.names),delimiter=',',**kwargs)\n    elif ext in ('.txt','.dat'):\n        return np.savetxt(filename,data,**kwargs)\n\n    msg = \"Unrecognized file type: %s\"%filename\n    raise ValueError(msg)", "response": "Writes a recarray to a specific format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a column to a FITS file.", "response": "def add_column(filename,column,formula,force=False):\n    \"\"\" Add a column to a FITS file.\n\n    ADW: Could this be replaced by a ftool?\n    \"\"\"\n    columns = parse_formula(formula)\n    logger.info(\"Running file: %s\"%filename)\n    logger.debug(\"  Reading columns: %s\"%columns)\n    data = fitsio.read(filename,columns=columns)\n\n    logger.debug('  Evaluating formula: %s'%formula)\n    col = eval(formula)\n\n    col = np.asarray(col,dtype=[(column,col.dtype)])\n    insert_columns(filename,col,force=force)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a set of FITS files with kwargs.", "response": "def load_files(filenames,multiproc=False,**kwargs):\n    \"\"\" Load a set of FITS files with kwargs. \"\"\"\n    filenames = np.atleast_1d(filenames)\n    logger.debug(\"Loading %s files...\"%len(filenames))\n\n    kwargs = [dict(filename=f,**kwargs) for f in filenames]\n\n    if multiproc:\n        from multiprocessing import Pool\n        processes = multiproc if multiproc > 0 else None\n        p = Pool(processes,maxtasksperchild=1)\n        out = p.map(load_file,kwargs)\n    else:\n        out = [load_file(kw) for kw in kwargs]\n\n    dtype = out[0].dtype\n    for i,d in enumerate(out):\n        if d.dtype != dtype: \n            # ADW: Not really safe...\n            logger.warn(\"Casting input data to same type.\")\n            out[i] = d.astype(dtype,copy=False)\n\n    logger.debug('Concatenating arrays...')\n    return np.concatenate(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_membership(loglike,filename):\n\n    ra,dec = gal2cel(loglike.catalog.lon,loglike.catalog.lat)\n        \n    name_objid = loglike.config['catalog']['objid_field']\n    name_mag_1 = loglike.config['catalog']['mag_1_field']\n    name_mag_2 = loglike.config['catalog']['mag_2_field']\n    name_mag_err_1 = loglike.config['catalog']['mag_err_1_field']\n    name_mag_err_2 = loglike.config['catalog']['mag_err_2_field']\n\n    # Angular and isochrone separations\n    sep = angsep(loglike.source.lon,loglike.source.lat,\n                 loglike.catalog.lon,loglike.catalog.lat)\n    isosep = loglike.isochrone.separation(loglike.catalog.mag_1,loglike.catalog.mag_2)\n\n    data = odict()\n    data[name_objid] = loglike.catalog.objid\n    data['GLON'] = loglike.catalog.lon\n    data['GLAT'] = loglike.catalog.lat\n    data['RA']   = ra\n    data['DEC']  = dec\n    data[name_mag_1] = loglike.catalog.mag_1\n    data[name_mag_err_1] = loglike.catalog.mag_err_1\n    data[name_mag_2] = loglike.catalog.mag_2\n    data[name_mag_err_2] = loglike.catalog.mag_err_2\n    data['COLOR'] = loglike.catalog.color\n    data['ANGSEP'] = sep\n    data['ISOSEP'] = isosep\n    data['PROB'] = loglike.p\n\n    # HIERARCH allows header keywords longer than 8 characters\n    header = []\n    for param,value in loglike.source.params.items():\n        card = dict(name='HIERARCH %s'%param.upper(),\n                    value=value.value,\n                    comment=param)\n        header.append(card)\n    card = dict(name='HIERARCH %s'%'TS',value=loglike.ts(),\n                comment='test statistic')\n    header.append(card)\n    card = dict(name='HIERARCH %s'%'TIMESTAMP',value=time.asctime(),\n                comment='creation time')\n    header.append(card)\n    fitsio.write(filename,data,header=header,clobber=True)", "response": "Writes a catalog file of the likelihood region including the membership properties."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake the value from a two - dimensional histogram from the bin corresponding to x y.", "response": "def take2D(histogram, x, y, bins_x, bins_y):\n    \"\"\"\n    Take the value from a two-dimensional histogram from the bin corresponding to (x, y).\n\n    Parameters:\n    -----------\n    histogram : The values in the histogram (n,m) (ADW: is this ordering right?)\n    x : the x-value to take from the hist\n    y : the y-value to take from the hist\n    bins_x : the xbin edges, including upper edge (n-dim)\n    bins_y : the ybin edges, including upper edge (m-dim)\n    \"\"\"\n    histogram = np.array(histogram)\n    \n    if np.isscalar(x):\n        x = [x]\n    if np.isscalar(y):\n        y = [y]\n\n    bins_x[-1] += 1.e-10 * (bins_x[-1] - bins_x[-2]) # Numerical stability\n    bins_y[-1] += 1.e-10 * (bins_y[-1] - bins_y[-2])\n\n    #return np.take(histogram, (histogram.shape[1] * (np.digitize(y, bins_y) - 1)) + (np.digitize(x, bins_x) - 1))\n\n    # Return np.nan for entries which are outside the binning range on either axis\n    index = (histogram.shape[1] * (np.digitize(y, bins_y) - 1)) + (np.digitize(x, bins_x) - 1)\n    index_clipped = np.clip(index, 0, (histogram.shape[0] * histogram.shape[1]) - 1)\n    val = np.take(histogram, index_clipped)\n\n    outlier_x = np.logical_or(x < bins_x[0], x > bins_x[-1])\n    outlier_y = np.logical_or(y < bins_y[0], y > bins_y[-1])\n    outlier = np.logical_or(outlier_x, outlier_y)\n    val[outlier] = np.nan\n\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses cloud - in - cells binning algorithm. Only valid for equal - spaced linear bins.", "response": "def cloudInCells(x, y, bins, weights=None):\n    \"\"\"\n    Use cloud-in-cells binning algorithm. Only valid for equal-spaced linear bins.\n    http://ta.twi.tudelft.nl/dv/users/Lemmens/MThesis.TTH/chapter4.html#tth_sEc2\n    http://www.gnu.org/software/archimedes/manual/html/node29.html\n\n    INPUTS:\n        x: array of x-values\n        y: array or y-values\n        bins: [bins_x, bins_y] format, where bins_x corresponds to the bin edges along x-axis\n        weights[None]: optionally assign a weight to each entry\n    OUTPUTS:\n        histogram:\n        bins_x:\n        bins_y:\n    \"\"\"\n\n    # For consistency, the variable names should be changed in this function, but low priority...\n    \n    x_bins = np.array(bins[0])\n    delta_x = x_bins[1] - x_bins[0]\n    # Overflow and underflow bins\n    x_bins = np.insert(x_bins, 0, x_bins[0] - delta_x)\n    x_bins = np.append(x_bins, x_bins[-1] + delta_x)\n    y_bins = np.array(bins[1])\n    delta_y = y_bins[1] - y_bins[0]\n    y_bins = np.insert(y_bins, 0, y_bins[0] - delta_y)\n    y_bins = np.append(y_bins, y_bins[-1] + delta_y)\n\n    x_bound_cut = np.logical_and(x >= x_bins[0], x <= x_bins[-1])\n    y_bound_cut = np.logical_and(y >= y_bins[0], y <= y_bins[-1])\n    bound_cut = np.logical_and(x_bound_cut, y_bound_cut)\n\n    if not np.any(weights):\n        bound_weights = np.ones(len(x))[bound_cut]\n    else:\n        bound_weights = np.array(weights)[bound_cut]\n\n    x_vals = np.array(x)[bound_cut]\n    y_vals = np.array(y)[bound_cut]\n\n    x_width = x_bins[1] - x_bins[0]\n    y_width = y_bins[1] - y_bins[0]\n\n    x_centers = x_bins[0: -1] + (0.5 * x_width)  \n    y_centers = y_bins[0: -1] + (0.5 * y_width)\n\n    dx = x_vals - x_centers[np.digitize(x_vals, x_bins) - 1]\n    dy = y_vals - y_centers[np.digitize(y_vals, y_bins) - 1]\n\n    ux = ((dx / x_width) * (dx >= 0)) +\\\n         ((1. + (dx / x_width)) * (dx < 0))\n    lx = 1. - ux\n\n    uy = ((dy / y_width) * (dy >= 0)) +\\\n         ((1. + (dy / y_width)) * (dy < 0))\n    ly = 1. - uy\n\n    new_x_vals = []\n    new_y_vals = []\n    cell_weights = []\n\n    # 4 corners\n    new_x_vals.append(x_vals + (0.5 * x_width))\n    new_y_vals.append(y_vals + (0.5 * y_width))\n    cell_weights.append(bound_weights * ux * uy)\n\n    new_x_vals.append(x_vals + (0.5 * x_width))\n    new_y_vals.append(y_vals - (0.5 * y_width))\n    cell_weights.append(bound_weights * ux * ly)\n    \n    new_x_vals.append(x_vals - (0.5 * x_width))\n    new_y_vals.append(y_vals + (0.5 * y_width))\n    cell_weights.append(bound_weights * lx * uy)\n    \n    new_x_vals.append(x_vals - (0.5 * x_width))\n    new_y_vals.append(y_vals - (0.5 * y_width))\n    cell_weights.append(bound_weights * lx * ly)\n\n    new_x_vals = np.concatenate(new_x_vals)\n    new_y_vals = np.concatenate(new_y_vals)\n    cell_weights = np.concatenate(cell_weights)\n\n    result = np.histogram2d(new_x_vals, new_y_vals,\n                               bins = [x_bins, y_bins],\n                               weights = cell_weights)[0]\n \n    result = np.transpose(result[1: result.shape[0] - 1])[1: result.shape[1] - 1]\n\n    return result, x_bins, y_bins"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates a 2D kernel density estimate over a regular grid.", "response": "def fast_kde(x, y, gridsize=(200,200), extents=None, nocorrelation=False, weights=None):\n    \"\"\"\n    Performs a gaussian kernel density estimate over a regular grid using a\n    convolution of the gaussian kernel with a 2D histogram of the data.\n\n    This function is typically several orders of magnitude faster than \n    scipy.stats.kde.gaussian_kde for large (>1e7) numbers of points and \n    produces an essentially identical result.\n\n    Input:\n        x: The x-coords of the input data points\n        y: The y-coords of the input data points\n        gridsize: (default: 200x200) A (nx,ny) tuple of the size of the output \n            grid\n        extents: (default: extent of input data) A (xmin, xmax, ymin, ymax)\n            tuple of the extents of output grid\n        nocorrelation: (default: False) If True, the correlation between the\n            x and y coords will be ignored when preforming the KDE.\n        weights: (default: None) An array of the same shape as x & y that \n            weighs each sample (x_i, y_i) by each value in weights (w_i).\n            Defaults to an array of ones the same size as x & y.\n    Output:\n        A gridded 2D kernel density estimate of the input points. \n    \"\"\"\n    #---- Setup --------------------------------------------------------------\n    x, y = np.asarray(x), np.asarray(y)\n    x, y = np.squeeze(x), np.squeeze(y)\n    \n    if x.size != y.size:\n        raise ValueError('Input x & y arrays must be the same size!')\n\n    nx, ny = gridsize\n    n = x.size\n\n    if weights is None:\n        # Default: Weight all points equally\n        weights = np.ones(n)\n    else:\n        weights = np.squeeze(np.asarray(weights))\n        if weights.size != x.size:\n            raise ValueError('Input weights must be an array of the same size'\n                    ' as input x & y arrays!')\n\n    # Default extents are the extent of the data\n    if extents is None:\n        xmin, xmax = x.min(), x.max()\n        ymin, ymax = y.min(), y.max()\n    else:\n        xmin, xmax, ymin, ymax = list(map(float, extents))\n    dx = (xmax - xmin) / (nx - 1)\n    dy = (ymax - ymin) / (ny - 1)\n\n    #---- Preliminary Calculations -------------------------------------------\n\n    # First convert x & y over to pixel coordinates\n    # (Avoiding np.digitize due to excessive memory usage in numpy < v1.5!)\n    # http://stackoverflow.com/q/8805601/\n    xyi = np.vstack((x,y)).T\n    xyi -= [xmin, ymin]\n    xyi /= [dx, dy]\n    xyi = np.floor(xyi, xyi).T\n\n    # Next, make a 2D histogram of x & y\n    # Avoiding np.histogram2d due to excessive memory usage with many points\n    # http://stackoverflow.com/q/8805601/\n    grid = sp.sparse.coo_matrix((weights, xyi), shape=(nx, ny)).toarray()\n\n    # Calculate the covariance matrix (in pixel coords)\n    cov = np.cov(xyi)\n\n    if nocorrelation:\n        cov[1,0] = 0\n        cov[0,1] = 0\n\n    # Scaling factor for bandwidth\n    scotts_factor = np.power(n, -1.0 / 6) # For 2D\n\n    #---- Make the gaussian kernel -------------------------------------------\n\n    # First, determine how big the kernel needs to be\n    std_devs = np.diag(np.sqrt(cov))\n    kern_nx, kern_ny = np.round(scotts_factor * 2 * np.pi * std_devs)\n\n    # Determine the bandwidth to use for the gaussian kernel\n    inv_cov = np.linalg.inv(cov * scotts_factor**2) \n\n    # x & y (pixel) coords of the kernel grid, with <x,y> = <0,0> in center\n    xx = np.arange(kern_nx, dtype=np.float) - kern_nx / 2.0\n    yy = np.arange(kern_ny, dtype=np.float) - kern_ny / 2.0\n    xx, yy = np.meshgrid(xx, yy)\n\n    # Then evaluate the gaussian function on the kernel grid\n    kernel = np.vstack((xx.flatten(), yy.flatten()))\n    kernel = np.dot(inv_cov, kernel) * kernel \n    kernel = np.sum(kernel, axis=0) / 2.0 \n    kernel = np.exp(-kernel) \n    kernel = kernel.reshape((kern_ny, kern_nx))\n\n    #---- Produce the kernel density estimate --------------------------------\n\n    # Convolve the gaussian kernel with the 2D histogram, producing a gaussian\n    # kernel density estimate on a regular grid\n    grid = sp.signal.convolve2d(grid, kernel, mode='same', boundary='fill').T\n\n    ### ADW: Commented out for \n    ### # Normalization factor to divide result by so that units are in the same\n    ### # units as scipy.stats.kde.gaussian_kde's output.  \n    ### norm_factor = 2 * np.pi * cov * scotts_factor**2\n    ### norm_factor = np.linalg.det(norm_factor)\n    ### norm_factor = n * dx * dy * np.sqrt(norm_factor)\n    ###  \n    ### # Normalize the result\n    ### grid /= norm_factor\n\n    return grid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reverseHistogram(data,bins=None):\n    if bins is None: bins = np.arange(data.max()+2)\n    hist, edges = np.histogram(data, bins=bins)\n    digi = np.digitize(data.flat,bins=np.unique(data)).argsort()\n    rev = np.hstack( (len(edges), len(edges) + np.cumsum(hist), digi) )\n    return hist,edges,rev", "response": "This function calculates the histogram and the reverse indices of the entries in each bin using np. histogram and calculates the reverse indices for the entries like IDL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef angToPix(nside, lon, lat, nest=False):\n    theta = np.radians(90. - lat)\n    phi = np.radians(lon)\n    return hp.ang2pix(nside, theta, phi, nest=nest)", "response": "Convert from ang to pix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query_disc(nside, vec, radius, inclusive=False, fact=4, nest=False):\n    try: \n        # New-style call (healpy 1.6.3)\n        return hp.query_disc(nside, vec, np.radians(radius), inclusive, fact, nest)\n    except Exception as e: \n        print(e)\n        # Old-style call (healpy 0.10.2)\n        return hp.query_disc(nside, vec, np.radians(radius), nest, deg=False)", "response": "Wrapper around healpy. query_disc to deal with old healpy implementation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef angToDisc(nside, lon, lat, radius, inclusive=False, fact=4, nest=False):\n    vec = angToVec(lon,lat)\n    return query_disc(nside,vec,radius,inclusive,fact,nest)", "response": "Wrap query_disc to use lon lat and radius in degrees."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef angsep(lon1, lat1, lon2, lat2):\n    lon1,lat1 = np.radians([lon1,lat1])\n    lon2,lat2 = np.radians([lon2,lat2])\n    \n    sdlon = np.sin(lon2 - lon1)\n    cdlon = np.cos(lon2 - lon1)\n    slat1 = np.sin(lat1)\n    slat2 = np.sin(lat2)\n    clat1 = np.cos(lat1)\n    clat2 = np.cos(lat2)\n\n    num1 = clat2 * sdlon\n    num2 = clat1 * slat2 - slat1 * clat2 * cdlon\n    denominator = slat1 * slat2 + clat1 * clat2 * cdlon\n\n    return np.degrees(np.arctan2(np.hypot(num1,num2), denominator))", "response": "Returns the angular separation between two sky coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef meanFracdet(map_fracdet, lon_population, lat_population, radius_population):\n    nside_fracdet = hp.npix2nside(len(map_fracdet))\n    map_fracdet_zero = np.where(map_fracdet >= 0., map_fracdet, 0.)\n    fracdet_population = np.empty(len(lon_population))\n    for ii in range(0, len(lon_population)):\n        fracdet_population[ii] = np.mean(map_fracdet_zero[angToDisc(nside_fracdet, \n                                                                    lon_population[ii], \n                                                                    lat_population[ii], \n                                                                    radius_population if np.isscalar(radius_population) else radius_population[ii],\n                                                                    inclusive=True)])\n    return fracdet_population", "response": "Compute the mean fracdet within a circular aperture"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrains the classifier on the data set.", "response": "def trainClassifier(self):\n        \"\"\"\n        Self-consistently train the classifier\n        \"\"\"\n        \n        self.loadPopulationMetadata()\n        self.loadSimResults()\n        \n        cut_geometry, flags_geometry = self.applyGeometry(self.data_population['RA'], self.data_population['DEC'])\n        \n        cut_detect_sim_results_sig = (np.logical_or(np.logical_and(self.data_sim['SIG'] >=  self.config[self.algorithm]['sig_threshold'], \n                                      self.data_sim['DIFFICULTY'] == 0), np.logical_or(self.data_sim['DIFFICULTY'] == 1, self.data_sim['DIFFICULTY'] == 4)))\n        cut_detect_sim_results_ts = (np.logical_or(np.logical_and(self.data_sim['TS'] >=  self.config[self.algorithm]['ts_threshold'], \n                                      self.data_sim['DIFFICULTY'] == 0), np.logical_or(self.data_sim['DIFFICULTY'] == 1, self.data_sim['DIFFICULTY'] == 4)))\n        \n        mc_source_id_detect = self.data_sim['MC_SOURCE_ID'][cut_detect_sim_results_sig & cut_detect_sim_results_ts]\n        cut_detect = np.in1d(self.data_population['MC_SOURCE_ID'], mc_source_id_detect)\n\n        features = []\n        for key, operation in self.config['operation']['params_intrinsic']:\n            assert operation.lower() in ['linear', 'log'], 'ERROR'\n            if operation.lower() == 'linear':\n                features.append(self.data_population[key])\n            else:\n                features.append(np.log10(self.data_population[key]))\n\n        X = np.vstack(features).T\n        X = X[cut_geometry]\n        Y = cut_detect[cut_geometry]\n\n        # Create training and test sets\n        indices = np.arange(len(X))\n        X_train, X_test, Y_train, Y_test, cut_train, cut_test = train_test_split(X,Y,indices,test_size=0.2)\n\n        # Train random forest classifier\n        if True:\n            print 'Training the machine learning classifier. This may take a while ...'\n            t_start = time.time()\n            parameters = {'n_estimators':(500,1000)}#, 'criterion':[\"gini\",\"entropy\"], \"min_samples_leaf\": [1,2,4]}\n            rf = RandomForestClassifier(oob_score=True)\n            rf_tuned = GridSearchCV(rf, parameters, cv=10, verbose=1)\n            self.classifier = rf_tuned.fit(X_train, Y_train)\n            \n            #self.classifier = sklearn.gaussian_process.GaussianProcessClassifier(1.0 * sklearn.gaussian_process.kernels.RBF(0.5))\n            #self.classifier = sklearn.neighbors.KNeighborsClassifier(3, weights='uniform')\n            #self.classifier = sklearn.neighbors.KNeighborsClassifier(2, weights='distance') \n            #self.classifier = sklearn.svm.SVC(gamma=2, C=1)\n            \n            # Print the best score and estimator:\n            print('Best Score:', self.classifier.best_score_)\n            print(self.classifier.best_estimator_)\n            print(self.classifier.best_params_)\n            t_end = time.time()\n            print '  ... training took %.2f seconds'%(t_end - t_start)\n\n            # Save the trained classifier\n            classifier_data = pickle.dumps(self.classifier)\n            writer = open(self.config[self.algorithm]['classifier'], 'w')\n            writer.write(classifier_data)\n            writer.close()\n            print 'Saving machine learning classifier to %s ...'%(self.config[self.algorithm]['classifier'])\n        else:\n            self.loadClassifier()\n\n        y_pred = self.classifier.predict_proba(X_test)[:,1]\n        \n        #Confusion matrix\n        y_pred_label = self.classifier.predict(X_test)\n        cm = confusion_matrix(Y_test, y_pred_label)\n        nondet_frac = cm[0][0]/(1.0*cm[0][0]+1.0*cm[0][1])\n        det_frac = cm[1][1]/(1.0*cm[1][0]+1.0*cm[1][1])\n\n        print('Fraction of non-detections test set labeled correctly: %0.2f' % nondet_frac)\n        print('Fraction of detections in test set labeled correctly: %0.2f' % det_frac)\n\n        plt.figure(figsize=(8,6))\n        plt.matshow(cm)\n        plt.title('Confusion Matrix', fontsize=18, position = (0.5,1.1))\n        plt.colorbar()\n        plt.ylabel('True label', fontsize=16)\n        plt.xlabel('Predicted label', fontsize=16, position = (0.5, -10.5))\n        plt.tick_params(labelsize=12)\n        plt.show()\n        \n        # Compute ROC curve and area under curve (AUC) for each class:\n        BestRFselector = self.classifier.best_estimator_\n        y_pred_best = BestRFselector.predict_proba(X_test)\n        labels = BestRFselector.classes_\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        for i,label in enumerate(labels):\n            fpr[label], tpr[label], _ = roc_curve(Y_test, y_pred_best[:, i], pos_label=label)\n            roc_auc[label] = auc(fpr[label], tpr[label])\n            \n        plt.figure(figsize=(8,6))\n        plt.plot([0, 1], [1, 1], color='red', linestyle='-', linewidth=3, label='Perfect Classifier (AUC = %0.2f)' % (1.0))\n        plt.plot(fpr[1], tpr[1], lw=3, label='Random Forest (AUC = %0.2f)' % (roc_auc[1]), color='blue')\n        plt.plot([0, 1], [0, 1], color='black', linestyle=':', linewidth=2.5, label='Random Classifier (AUC = %0.2f)' % (0.5))\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.025])\n        plt.tick_params(labelsize=16)\n        plt.xlabel('False Positive Rate', fontsize=20, labelpad=8)\n        plt.ylabel('True Positive Rate', fontsize=20, labelpad=8)\n        plt.legend(loc=\"lower right\", fontsize=16)\n        plt.show()\n\n        self.validateClassifier(cut_detect, cut_train, cut_geometry, y_pred)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes some diagnostic plots", "response": "def validateClassifier(self, cut_detect, cut_train, cut_geometry, y_pred):\n        \"\"\"\n        Make some diagnostic plots\n        \"\"\"\n\n        color = {'detect': 'Red',\n                 'nondetect': 'Gold',\n                 'why_not': 'none',\n                 'actual': 'DodgerBlue',\n                 'hsc': 'lime'}\n        size = {'detect': 5,\n                'nondetect': 5,\n                'why_not': 35,\n                'actual': None,\n                'hsc': None}\n        marker = {'detect': 'o',\n                  'nondetect': 'o',\n                  'why_not': 'o',\n                  'actual': 's',\n                  'hsc': 's'}\n        alpha = {'detect': None,\n                 'nondetect': None,\n                 'why_not': None,\n                 'actual': None,\n                 'hsc': None}\n        edgecolor = {'detect': None,\n                     'nondetect': None,\n                     'why_not': 'magenta',\n                     'actual': 'black',\n                     'hsc': 'black'}\n\n        title = 'N_train = %i ; N_test = %i'%(len(cut_train),len(cut_test))        import matplotlib\n        cmap = matplotlib.colors.ListedColormap(['Gold', 'Orange', 'DarkOrange', 'OrangeRed', 'Red'])\n\n        pylab.figure()\n        pylab.xscale('log')\n        pylab.scatter(1.e3 * self.data_population['r_physical'][cut_train & cut_geometry], \n                      self.data_population['abs_mag'][cut_train & cut_geometry], \n                      c=cut_detect[cut_train & cut_geometry].astype(int), vmin=0., vmax=1., s=size['detect'], cmap=cmap, label=None) \n        pylab.scatter(1.e3 * self.data_population['r_physical'][~cut_train  & cut_geometry], \n                      self.data_population['abs_mag'][~cut_train & cut_geometry], \n                      c=y_pred, edgecolor='black', vmin=0., vmax=1., s=(3 * size['detect']), cmap=cmap, label=None) \n        colorbar = pylab.colorbar()\n        colorbar.set_label('ML Predicted Detection Probability')\n        pylab.scatter(0., 0., s=(3 * size['detect']), c='none', edgecolor='black', label='Test')\n        #pylab.scatter(1.e3 * r_physical_actual, abs_mag_actual, \n        #              c=color['actual'], s=size['actual'], marker=marker['actual'], edgecolor=edgecolor['actual'], alpha=alpha['actual'], label='Actual MW Satellites')\n        #pylab.scatter(1.e3 * r_physical_actual[cut_hsc], abs_mag_actual[cut_hsc], \n        #              c=color['hsc'], s=size['hsc'], marker=marker['hsc'], edgecolor=edgecolor['hsc'], alpha=alpha['hsc'], label='Actual MW Satellites: HSC')\n        pylab.xlim(1., 3.e3)\n        pylab.ylim(6., -12.)\n        pylab.xlabel('Half-light Radius (pc)')\n        pylab.ylabel('M_V (mag)')\n        pylab.legend(loc='upper left', markerscale=2)\n        pylab.title(title)\n\n        import ugali.utils.bayesian_efficiency # Replace with standalone util\n\n        bins = np.linspace(0., 1., 10 + 1)\n        centers = np.empty(len(bins) - 1)\n        bin_prob = np.empty(len(bins) - 1)\n        bin_prob_err_hi = np.empty(len(bins) - 1)\n        bin_prob_err_lo = np.empty(len(bins) - 1)\n        bin_counts = np.empty(len(bins) - 1)\n        for ii in range(0, len(centers)):\n            cut_bin = (y_pred > bins[ii]) & (y_pred < bins[ii + 1])\n            centers[ii] = np.mean(y_pred[cut_bin])\n    \n            n_trials = np.sum(cut_bin)\n            n_successes = np.sum(cut_detect[~cut_train & cut_geometry] & cut_bin)\n            efficiency, errorbar = ugali.utils.bayesian_efficiency.bayesianInterval(n_trials, n_successes, errorbar=True)\n            bin_prob[ii] = efficiency\n            bin_prob_err_hi[ii] = errorbar[1]\n            bin_prob_err_lo[ii] = errorbar[0]\n            #bin_prob[ii] = float(np.sum(cut_detect[~cut_train] & cut_bin)) / np.sum(cut_bin)\n            #bin_prob[ii] = np.mean(y[cut].astype(float))\n            bin_counts[ii] = np.sum(cut_bin)\n    \n        pylab.figure()\n        pylab.plot([0., 1.], [0., 1.], c='black', ls='--')\n        pylab.errorbar(centers, bin_prob, yerr=[bin_prob_err_lo, bin_prob_err_hi], c='red')\n        pylab.plot(centers, bin_prob, c='red')\n        pylab.scatter(centers, bin_prob, c=bin_counts, edgecolor='red', s=50, cmap='Reds', zorder=999)\n        colorbar = pylab.colorbar()\n        colorbar.set_label('Counts')\n        pylab.xlabel('ML Predicted Detection Probability')\n        pylab.ylabel('Fraction Detected')\n        pylab.xlim(0., 1.)\n        pylab.ylim(0., 1.)\n        pylab.title(title)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef applyFracdet(self, lon, lat):\n        self.loadFracdet()\n        fracdet_core = meanFracdet(self.m_fracdet, lon, lat, np.tile(0.1, len(lon)))\n        fracdet_wide = meanFracdet(self.m_fracdet, lon, lat, np.tile(0.5, len(lon)))\n        return (fracdet_core >= self.config[self.algorithm]['fracdet_core_threshold']) \\\n            & (fracdet_core >= self.config[self.algorithm]['fracdet_core_threshold'])", "response": "Returns True is passes fracdet False is passes fracdet cut\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef applyHotspot(self, lon, lat):\n        self.loadRealResults()\n        cut_detect_real = (self.data_real['SIG'] >= self.config[self.algorithm]['sig_threshold'])\n        lon_real = self.data_real['RA'][cut_detect_real]\n        lat_real = self.data_real['DEC'][cut_detect_real]\n\n        cut_hotspot = np.tile(True, len(lon))\n        for ii in range(0, len(lon)):\n            cut_hotspot[ii] = ~np.any(angsep(lon[ii], lat[ii], lon_real, lat_real) < self.config[self.algorithm]['hotspot_angsep_threshold'])\n\n        return cut_hotspot", "response": "Return a numpy array of the objects that are too close to hotspot True if passes hotspot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npredict the class of the object", "response": "def predict(self, lon, lat, **kwargs):\n        \"\"\"\n        distance, abs_mag, r_physical\n        \"\"\"\n        assert self.classifier is not None, 'ERROR'\n        \n        pred = np.zeros(len(lon))\n        cut_geometry, flags_geometry = self.applyGeometry(lon, lat)\n        \n        x_test = []\n        for key, operation in self.config['operation']['params_intrinsic']:\n            assert operation.lower() in ['linear', 'log'], 'ERROR'\n            if operation.lower() == 'linear':\n                x_test.append(kwargs[key])\n            else:\n                x_test.append(np.log10(kwargs[key]))\n\n        x_test = np.vstack(x_test).T\n        \n        #import pdb; pdb.set_trace()\n        pred[cut_geometry] = self.classifier.predict_proba(x_test[cut_geometry])[:,1]\n\n        self.validatePredict(pred, flags_geometry, lon, lat, kwargs['r_physical'], kwargs['abs_mag'], kwargs['distance'])\n\n        return pred, flags_geometry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef catalogFactory(name, **kwargs):\n    fn = lambda member: inspect.isclass(member) and member.__module__==__name__\n    catalogs = odict(inspect.getmembers(sys.modules[__name__], fn))\n\n    if name not in list(catalogs.keys()):\n        msg = \"%s not found in catalogs:\\n %s\"%(name,list(kernels.keys()))\n        logger.error(msg)\n        msg = \"Unrecognized catalog: %s\"%name\n        raise Exception(msg)\n\n    return catalogs[name](**kwargs)", "response": "Factory for various catalogs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef surfaceBrightness(abs_mag, r_physical, distance):\n    r_angle = np.degrees(np.arctan(r_physical / distance))\n    c_v = 19.78 # mag/arcsec^2\n    return abs_mag + dist2mod(distance) + c_v + 2.5 * np.log10(r_angle**2)", "response": "Compute the average surface brightness within the half - light radius."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an instance of the results class.", "response": "def createResults(config,srcfile,section='source',samples=None):\n    \"\"\" Create an MCMC instance \"\"\"\n    source = ugali.analysis.source.Source()\n    source.load(srcfile,section=section)\n    loglike = ugali.analysis.loglike.createLoglike(config,source)\n\n    results = Results(config,loglike,samples)\n\n    if samples is not None:\n        results.load_samples(samples)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites results to a file.", "response": "def write_results(filename,config,srcfile,samples):\n    \"\"\" Package everything nicely \"\"\" \n    results = createResults(config,srcfile,samples=samples)\n    results.write(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef estimate(self,param,burn=None,clip=10.0,alpha=0.32):\n        # FIXME: Need to add age and metallicity to composite isochrone params (currently properties)\n        if param not in list(self.samples.names) + list(self.source.params) + ['age','metallicity']:\n            msg = 'Unrecognized parameter: %s'%param\n            raise KeyError(msg)\n\n        # If the parameter is in the samples\n        if param in self.samples.names:\n            if param.startswith('position_angle'):\n                return self.estimate_position_angle(param,burn=burn,\n                                                    clip=clip,alpha=alpha)\n\n            return self.samples.peak_interval(param,burn=burn,clip=clip,alpha=alpha)\n \n        mle = self.get_mle()\n        errors = [np.nan,np.nan] \n\n        # Set default value to the MLE value\n        if param in self.source.params:\n            err = self.source.params[param].errors\n            if err is not None: errors = err\n\n        # For age and metallicity from composite isochrone\n        return [float(mle[param]),errors]", "response": "Estimate the parameter value and uncertainties for the specified parameter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef estimate_params(self,burn=None,clip=10.0,alpha=0.32):\n        mle = self.get_mle()\n        out = odict()\n        for param in mle.keys():\n            out[param] = self.estimate(param,burn=burn,clip=clip,alpha=alpha)\n        return out", "response": "Estimate all source parameters"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nestimate the position angle from the posterior dealing with periodicity.", "response": "def estimate_position_angle(self,param='position_angle',burn=None,clip=10.0,alpha=0.32):\n        \"\"\" Estimate the position angle from the posterior dealing\n        with periodicity.\n        \"\"\"\n        # Transform so peak in the middle of the distribution\n        pa = self.samples.get(param,burn=burn,clip=clip)\n        peak = ugali.utils.stats.kde_peak(pa)\n        shift = 180.*((pa+90-peak)>180)\n        pa -= shift\n        # Get the kde interval\n        ret = ugali.utils.stats.peak_interval(pa,alpha)\n        if ret[0] < 0: \n            ret[0] += 180.; ret[1][0] += 180.; ret[1][1] += 180.;\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the start and end formatted for query or the last hour for query .", "response": "def date_range_for_webtrends(cls, start_at=None, end_at=None):\n        \"\"\"\n        Get the start and end formatted for query\n        or the last hour if none specified.\n        Unlike reports, this does not aggregate periods\n        and so it is possible to just query a range and parse out the\n        individual hours.\n        \"\"\"\n        if start_at and end_at:\n            start_date = cls.parse_standard_date_string_to_date(\n                start_at)\n            end_date = cls.parse_standard_date_string_to_date(\n                end_at)\n            return [(\n                cls.parse_date_for_query(start_date),\n                cls.parse_date_for_query(end_date))]\n        else:\n            return [(\"current_hour-1\", \"current_hour-1\")]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_ugali_dir():\n\n    dirname = os.getenv('UGALIDIR')\n\n    # Get the HOME directory\n    if not dirname:\n        dirname=os.path.join(os.getenv('HOME'),'.ugali')\n\n    if not os.path.exists(dirname):\n        from ugali.utils.logger import logger\n        msg = \"Creating UGALIDIR:\\n%s\"%dirname\n        logger.warning(msg)\n\n    return mkdir(dirname)", "response": "Get the path to the ugali data directory from the environment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the ugali isochrone directory.", "response": "def get_iso_dir():\n    \"\"\"Get the ugali isochrone directory.\"\"\"\n    dirname = os.path.join(get_ugali_dir(),'isochrones')\n\n    if not os.path.exists(dirname):\n        from ugali.utils.logger import logger\n        msg = \"Isochrone directory not found:\\n%s\"%dirname\n        logger.warning(msg)\n\n    return dirname"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef registerParser(self, parser):\n\n        if not isinstance(parser, Subparser):\n            raise TypeError(\"%s is not an instance of a subparser.\" % parser)\n\n        self.parsers.append(parser)", "response": "Registers a parser to parse configuration inputs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new configuration option to the ConfigManager.", "response": "def addConfig(self, name, default=None, cast=None, required=False, description=None):\n        \"\"\"\n        Adds the given configuration option to the ConfigManager.\n\n        Inputs: name        - The configuration name to accept.\n                required    - A boolean indicating whether or not the configuration option is required or not.\n                cast        - A type (or function accepting 1 argument and returning an object) to cast the input as.\n                              If any error occurs during casting an InvalidConfigurationException will be raised.\n                default     - The default value to assign to this configuration option.  Note that None is not a valid\n                              default if required=True.\n                description - A human readable description of this configuration parameter.  Will be displayed when the\n                              program is run with a -h flag.\n        \"\"\"\n\n        # Validate the name\n        if not self.configNameRE.match(name):\n            raise InvalidConfigurationException(\"Invalid configuration name: %s\" % name)\n\n        self.configs[self._sanitizeName(name)] = {\n            'default': default,\n            'cast': cast,\n            'required': required,\n            'description': description\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self):\n\n        self._config = _Config()\n\n        self._setDefaults()\n\n        for parser in self.parsers:\n            for key, value in parser.parse(self, self._config).items():\n                key = self._sanitizeName(key)\n                if key not in self.configs:\n                    raise UnknownConfigurationException(key)\n                if value is not None:\n                    self._setConfig(key, value)\n\n        self._ensureRequired()\n        self._cast()\n\n        return self._config", "response": "Parses the input configurations and returns the parsed configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _setDefaults(self):\n\n        for configName, configDict in self.configs.items():\n            self._setConfig(configName, configDict['default'])", "response": "Sets all the expected configuration options on the config object as either the requested default value or None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that all configuration options marked as required have been passed.", "response": "def _ensureRequired(self):\n        \"\"\"\n        Ensures that all configuration options marked as being required have been passed (ie are non-None).\n        Raises a MissingConfigurationException if a required configuration option was not passed.\n        \"\"\"\n\n        for configName, configDict in self.configs.items():\n            if configDict['required']:\n                if getattr(self._config, configName) is None:\n                    raise MissingConfigurationException(configName)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncast all configuration options with marked cast types.", "response": "def _cast(self):\n        \"\"\"\n        Iterates through our parsed configuration options and cast any options with marked cast types.\n        \"\"\"\n\n        for configName, configDict in self.configs.items():\n            if configDict['cast'] is not None:\n                configValue = getattr(self._config, configName)\n                if configValue is not None:\n                    try:\n                        self._setConfig(configName, configDict['cast'](configValue))\n\n                    except:\n                        raise InvalidConfigurationException(\"%s: %r\" % (configName, configValue))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_models(self, macaroons):\n        return make_request(\"{}model\".format(self.url), timeout=self.timeout,\n                            client=self._client, cookies=self.cookies)", "response": "Get the list of models from the JIMM controller."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, novel_title='novel', filetype='txt'):\n        self._compose_chapters()\n        self._write_to_file(novel_title, filetype)", "response": "Write the novel to a text file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compose_chapters(self):\n        for count in range(self.chapter_count):\n            chapter_num = count + 1\n            c = Chapter(self.markov, chapter_num)\n            self.chapters.append(c)", "response": "Creates a chapters based on the markov and chapter count."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef valid_address(address):\n    if not address:\n        return False\n\n    components = str(address).split(':')\n    if len(components) > 2 or not valid_hostname(components[0]):\n        return False\n\n    if len(components) == 2 and not valid_port(components[1]):\n        return False\n\n    return True", "response": "Determines whether the specified address string is valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn whether the specified string is a valid hostname.", "response": "def valid_hostname(host):\n    \"\"\"\n    Returns whether the specified string is a valid hostname.\n    \"\"\"\n    if len(host) > 255:\n        return False\n\n    if host[-1:] == '.':\n        host = host[:-1]\n\n    return all(_hostname_re.match(c) for c in host.split('.'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nintegrating the IMF by integrating the mass between mass_min and mass_max.", "response": "def integrate(self, mass_min, mass_max, log_mode=True, weight=False, steps=1e4):\n        \"\"\" Numerical Riemannn integral of the IMF (stupid simple).\n\n        Parameters:\n        -----------\n        mass_min: minimum mass bound for integration (solar masses)\n        mass_max: maximum mass bound for integration (solar masses)\n        log_mode[True]: use logarithmic steps in stellar mass as oppose to linear\n            weight[False]: weight the integral by stellar mass\n            steps: number of numerical integration steps\n\n        Returns:\n        --------\n        result of integral\n        \"\"\"\n        if log_mode:\n            d_log_mass = (np.log10(mass_max) - np.log10(mass_min)) / float(steps)\n            log_mass = np.linspace(np.log10(mass_min), np.log10(mass_max), steps)\n            mass = 10.**log_mass\n\n            if weight:\n                return np.sum(mass * d_log_mass * self.pdf(mass, log_mode=True))\n            else:\n                return np.sum(d_log_mass * self.pdf(mass, log_mode=True))\n        else:\n            d_mass = (mass_max - mass_min) / float(steps)\n            mass = np.linspace(mass_min, mass_max, steps)\n\n            if weight:\n                return np.sum(mass * d_mass * self.pdf(mass, log_mode=False))\n            else:\n                return np.sum(d_mass * self.pdf(mass, log_mode=False))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws random values from the ISOchrone.", "response": "def sample(self, n, mass_min=0.1, mass_max=10., steps=10000, seed=None):\n        \"\"\"\n        Sample initial mass values between mass_min and mass_max,\n        following the IMF distribution.\n\n        ADW: Should this be `sample` or `simulate`?\n\n        Parameters:\n        -----------\n        n : number of samples to draw\n        mass_min : minimum mass to sample from\n        mass_max : maximum mass to sample from\n        steps    : number of steps for isochrone sampling\n        seed     : random seed (passed to np.random.seed)\n\n        Returns:\n        --------\n        mass     : array of randomly sampled mass values\n        \"\"\"\n        if seed is not None: np.random.seed(seed)\n        d_mass = (mass_max - mass_min) / float(steps)\n        mass = np.linspace(mass_min, mass_max, steps)\n        cdf = np.insert(np.cumsum(d_mass * self.pdf(mass[1:], log_mode=False)), 0, 0.)\n        cdf = cdf / cdf[-1]\n        f = scipy.interpolate.interp1d(cdf, mass)\n        return f(np.random.uniform(size=n))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pdf(cls, mass, log_mode=True):\n        alpha = 2.35\n\n        a = 0.060285569480482866\n        dn_dm  = a * mass**(-alpha)\n\n        if log_mode:\n            # Number per logarithmic mass range, i.e., dN/dlog(M)\n            return dn_dm * (mass * np.log(10))\n        else:\n            # Number per linear mass range, i.e., dN/dM\n            return dn_dm", "response": "PDF for the Salpeter IMF."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an open file descriptor to a configuration file to process in read mode if successful else None.", "response": "def _getConfigFile(self, config):\n        \"\"\"\n        Retrieves a file descriptor to a configuration file to process.\n\n        Inputs: config - The _Config object which is being populated.\n\n        Outputs: An open file descriptor to the configuration file to parse in read mode if successful, else None.\n        \"\"\"\n\n        joinPath = lambda p: (os.path.join(p) if isinstance(p, (tuple, list)) else p)\n\n        if self.filepathConfig is not None and self.filenameConfig is not None:\n            if hasattr(config, self.filepathConfig) and hasattr(config, self.filenameConfig):\n                path = joinPath(getattr(config, self.filepathConfig))\n                name = getattr(config, self.filenameConfig)\n\n                if os.path.isfile(os.path.join(path, name)):\n                    return open(os.path.join(path, name), 'r')\n\n        if self.filepath is not None and self.filename is not None:\n            path = joinPath(self.filepath)\n            name = self.filename\n\n            if os.path.isfile(os.path.join(path, name)):\n                return open(os.path.join(path, name), 'r')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncounts the citations in an aux - file.", "response": "def _count_citations(aux_file):\n    '''\n    Counts the citations in an aux-file.\n\n    @return: defaultdict(int) - {citation_name: number, ...}\n    '''\n    counter = defaultdict(int)\n    with open(aux_file) as fobj:\n        content = fobj.read()\n\n    for match in CITE_PATTERN.finditer(content):\n        name = match.groups()[0]\n        counter[name] += 1\n\n    return counter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset up \"optparse\" and pass the options to a new instance of L{LatexMaker}.", "response": "def main():\n    '''\n    Set up \"optparse\" and pass the options to\n    a new instance of L{LatexMaker}.\n    '''\n    prog = 'latexmk.py'\n    version = __version__\n    usage = '%prog [options] [filename]'\n\n    # Read description from doc\n    doc_text = ''\n    for line in __doc__.splitlines():\n        if line.find('#') == 0:\n            break\n        doc_text += '  %s\\n' % line\n\n    parser = OptionParser(prog=prog, usage=usage, version=version)\n    parser.add_option('-c', '--clean',\n                      action='store_true', dest='clean', default=False,\n                      help='clean all temporary files after converting')\n    parser.add_option('-q', '--quiet',\n                      action='store_false', dest='verbose', default=True,\n                      help='don\\'t print status messages to stdout')\n    parser.add_option('-n', '--no-exit',\n                      action='store_false', dest='exit_on_error', default=True,\n                      help='don\\'t exit if error occurs')\n    parser.add_option('-p', '--preview',\n                      action='store_true', dest='preview', default=False,\n                      help='try to open preview of generated document')\n    parser.add_option('--dvi', action='store_false', dest='pdf',\n                      default=True, help='use \"latex\" instead of pdflatex')\n    parser.add_option('--check-cite', action='store_true', dest='check_cite',\n                      default=False,\n                      help='check bibtex file for uncited entries')\n\n    opt, args = parser.parse_args()\n    if len(args) == 0:\n        tex_files = fnmatch.filter(os.listdir(os.getcwd()), '*.tex')\n        if len(tex_files) == 1:\n            name = tex_files[0]\n        else:\n            parser.error('could not find a single *.tex file in current dir')\n    elif len(args) == 1:\n        name = args[0]\n    else:\n        parser.error('incorrect number of arguments')\n\n    LatexMaker(name, opt).run()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup_logger(self):\n        '''Set up a logger.'''\n        log = logging.getLogger('latexmk.py')\n\n        handler = logging.StreamHandler()\n        log.addHandler(handler)\n\n        if self.opt.verbose:\n            log.setLevel(logging.INFO)\n        return log", "response": "Set up a logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_texlipse_config(self):\n        '''\n        Read the project name from the texlipse\n        config file \".texlipse\".\n        '''\n        # If Eclipse's workspace refresh, the\n        # \".texlipse\"-File will be newly created,\n        # so try again after short sleep if\n        # the file is still missing.\n        if not os.path.isfile('.texlipse'):\n            time.sleep(0.1)\n            if not os.path.isfile('.texlipse'):\n                self.log.error('! Fatal error: File .texlipse is missing.')\n                self.log.error('! Exiting...')\n                sys.exit(1)\n\n        with open('.texlipse') as fobj:\n            content = fobj.read()\n        match = TEXLIPSE_MAIN_PATTERN.search(content)\n        if match:\n            project_name = match.groups()[0]\n            self.log.info('Found inputfile in \".texlipse\": %s.tex'\n                          % project_name)\n            return project_name\n        else:\n            self.log.error('! Fatal error: Parsing .texlipse failed.')\n            self.log.error('! Exiting...')\n            sys.exit(1)", "response": "Parse the. texlipse config file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_latex_files(self):\n        '''\n        Check if some latex output files exist\n        before first latex run, process them and return\n        the generated data.\n\n            - Parsing *.aux for citations counter and\n              existing glossaries.\n            - Getting content of files to detect changes.\n                - *.toc file\n                - all available glossaries files\n        '''\n        if os.path.isfile('%s.aux' % self.project_name):\n            cite_counter = self.generate_citation_counter()\n            self.read_glossaries()\n        else:\n            cite_counter = {'%s.aux' % self.project_name:\n                            defaultdict(int)}\n\n        fname = '%s.toc' % self.project_name\n        if os.path.isfile(fname):\n            with open(fname) as fobj:\n                toc_file = fobj.read()\n        else:\n            toc_file = ''\n\n        gloss_files = dict()\n        for gloss in self.glossaries:\n            ext = self.glossaries[gloss][1]\n            filename = '%s.%s' % (self.project_name, ext)\n            if os.path.isfile(filename):\n                with open(filename) as fobj:\n                    gloss_files[gloss] = fobj.read()\n\n        return cite_counter, toc_file, gloss_files", "response": "Read all latex output files and return the citations counter toc file and gloss files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_toc_changed(self, toc_file):\n        '''\n        Test if the *.toc file has changed during\n        the first latex run.\n        '''\n        fname = '%s.toc' % self.project_name\n        if os.path.isfile(fname):\n            with open(fname) as fobj:\n                if fobj.read() != toc_file:\n                    return True", "response": "Test if the. toc file has changed during latex run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if we need to run bibtex.", "response": "def _need_bib_run(self, old_cite_counter):\n        '''\n        Determine if you need to run \"bibtex\".\n        1. Check if *.bib exists.\n        2. Check latex output for hints.\n        3. Test if the numbers of citations changed\n           during first latex run.\n        4. Examine *.bib for changes.\n        '''\n        with open('%s.aux' % self.project_name) as fobj:\n            match = BIB_PATTERN.search(fobj.read())\n            if not match:\n                return False\n            else:\n                self.bib_file = match.group(1)\n\n        if not os.path.isfile('%s.bib' % self.bib_file):\n            self.log.warning('Could not find *.bib file.')\n            return False\n\n        if (re.search('No file %s.bbl.' % self.project_name, self.out) or\n                re.search('LaTeX Warning: Citation .* undefined', self.out)):\n            return True\n\n        if old_cite_counter != self.generate_citation_counter():\n            return True\n\n        if os.path.isfile('%s.bib.old' % self.bib_file):\n            new = '%s.bib' % self.bib_file\n            old = '%s.bib.old' % self.bib_file\n            if not filecmp.cmp(new, old):\n                return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading all existing glossaries in the main aux - file.", "response": "def read_glossaries(self):\n        '''\n        Read all existing glossaries in the main aux-file.\n        '''\n        filename = '%s.aux' % self.project_name\n        with open(filename) as fobj:\n            main_aux = fobj.read()\n\n        pattern = r'\\\\@newglossary\\{(.*)\\}\\{.*\\}\\{(.*)\\}\\{(.*)\\}'\n        for match in re.finditer(pattern, main_aux):\n            name, ext_i, ext_o = match.groups()\n            self.glossaries[name] = (ext_i, ext_o)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if errors occured during a latex run by clf.", "response": "def check_errors(self):\n        '''\n        Check if errors occured during a latex run by\n        scanning the output.\n        '''\n        errors = ERROR_PATTTERN.findall(self.out)\n        # \"errors\" is a list of tuples\n        if errors:\n            self.log.error('! Errors occurred:')\n\n            self.log.error('\\n'.join(\n                [error.replace('\\r', '').strip() for error\n                 in chain(*errors) if error.strip()]\n            ))\n\n            self.log.error('! See \"%s.log\" for details.' % self.project_name)\n\n            if self.opt.exit_on_error:\n                self.log.error('! Exiting...')\n                sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_citation_counter(self):\n        '''\n        Generate dictionary with the number of citations in all\n        included files. If this changes after the first latex run,\n        you have to run \"bibtex\".\n        '''\n        cite_counter = dict()\n        filename = '%s.aux' % self.project_name\n        with open(filename) as fobj:\n            main_aux = fobj.read()\n        cite_counter[filename] = _count_citations(filename)\n\n        for match in re.finditer(r'\\\\@input\\{(.*.aux)\\}', main_aux):\n            filename = match.groups()[0]\n            try:\n                counter = _count_citations(filename)\n            except IOError:\n                pass\n            else:\n                cite_counter[filename] = counter\n\n        return cite_counter", "response": "Generate dictionary with the number of citations in all\n        included files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeindex_runs(self, gloss_files):\n        '''\n        Check for each glossary if it has to be regenerated\n        with \"makeindex\".\n\n        @return: True if \"makeindex\" was called.\n        '''\n        gloss_changed = False\n        for gloss in self.glossaries:\n            make_gloss = False\n            ext_i, ext_o = self.glossaries[gloss]\n            fname_in = '%s.%s' % (self.project_name, ext_i)\n            fname_out = '%s.%s' % (self.project_name, ext_o)\n            if re.search('No file %s.' % fname_in, self.out):\n                make_gloss = True\n            if not os.path.isfile(fname_out):\n                make_gloss = True\n            else:\n                with open(fname_out) as fobj:\n                    try:\n                        if gloss_files[gloss] != fobj.read():\n                            make_gloss = True\n                    except KeyError:\n                        make_gloss = True\n\n            if make_gloss:\n                self.log.info('Running makeindex (%s)...' % gloss)\n                try:\n                    cmd = ['makeindex', '-q', '-s',\n                           '%s.ist' % self.project_name,\n                           '-o', fname_in, fname_out]\n                    with open(os.devnull, 'w') as null:\n                        Popen(cmd, stdout=null).wait()\n                except OSError:\n                    self.log.error(NO_LATEX_ERROR % 'makeindex')\n                    sys.exit(1)\n                gloss_changed = True\n\n        return gloss_changed", "response": "Check for each glossary if it has to be regenerated\n        with makeindex."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to open a preview of the generated document.", "response": "def open_preview(self):\n        '''\n        Try to open a preview of the generated document.\n        Currently only supported on Windows.\n        '''\n        self.log.info('Opening preview...')\n        if self.opt.pdf:\n            ext = 'pdf'\n        else:\n            ext = 'dvi'\n        filename = '%s.%s' % (self.project_name, ext)\n        if sys.platform == 'win32':\n            try:\n                os.startfile(filename)\n            except OSError:\n                self.log.error(\n                    'Preview-Error: Extension .%s is not linked to a '\n                    'specific application!' % ext\n                )\n        elif sys.platform == 'darwin':\n            call(['open', filename])\n        else:\n            self.log.error(\n                'Preview-Error: Preview function is currently not '\n                'supported on Linux.'\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef need_latex_rerun(self):\n        '''\n        Test for all rerun patterns if they match the output.\n        '''\n        for pattern in LATEX_RERUN_PATTERNS:\n            if pattern.search(self.out):\n                return True\n        return False", "response": "Test for all rerun patterns that match the output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the LaTeX compilation.", "response": "def run(self):\n        '''Run the LaTeX compilation.'''\n        # store files\n        self.old_dir = []\n        if self.opt.clean:\n            self.old_dir = os.listdir('.')\n\n        cite_counter, toc_file, gloss_files = self._read_latex_files()\n\n        self.latex_run()\n        self.read_glossaries()\n\n        gloss_changed = self.makeindex_runs(gloss_files)\n        if gloss_changed or self._is_toc_changed(toc_file):\n            self.latex_run()\n\n        if self._need_bib_run(cite_counter):\n            self.bibtex_run()\n            self.latex_run()\n\n        while (self.latex_run_counter < MAX_RUNS):\n            if not self.need_latex_rerun():\n                break\n            self.latex_run()\n\n        if self.opt.check_cite:\n            cites = set()\n            with open('%s.aux' % self.project_name) as fobj:\n                aux_content = fobj.read()\n\n            for match in BIBCITE_PATTERN.finditer(aux_content):\n                name = match.groups()[0]\n                cites.add(name)\n\n            with open('%s.bib' % self.bib_file) as fobj:\n                bib_content = fobj.read()\n            for match in BIBENTRY_PATTERN.finditer(bib_content):\n                name = match.groups()[0]\n                if name not in cites:\n                    self.log.info('Bib entry not cited: \"%s\"' % name)\n\n        if self.opt.clean:\n            ending = '.dvi'\n            if self.opt.pdf:\n                ending = '.pdf'\n\n            for fname in os.listdir('.'):\n                if not (fname in self.old_dir or fname.endswith(ending)):\n                    try:\n                        os.remove(fname)\n                    except IOError:\n                        pass\n\n        if self.opt.preview:\n            self.open_preview()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the command to run the likelihood scan.", "response": "def command(self, outfile, configfile, pix):\n        \"\"\"\n        Generate the command for running the likelihood scan.\n        \"\"\"\n        params = dict(script=self.config['scan']['script'],\n                      config=configfile, outfile=outfile, \n                      nside=self.nside_likelihood, pix=pix,\n                      verbose='-v' if self.verbose else '')\n        cmd = '%(script)s %(config)s %(outfile)s --hpx %(nside)i %(pix)i %(verbose)s'%params\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a boolean array representing the pixels in the footprint.", "response": "def footprint(self, nside=None):\n        \"\"\"\n        UNTESTED.\n        Should return a boolean array representing the pixels in the footprint.\n        \"\"\"\n        if nside is None:\n            nside = self.nside_pixel\n        elif nside < self.nside_catalog: \n            raise Exception('Requested nside=%i is less than catalog_nside'%nside)\n        elif nside > self.nside_pixel:\n            raise Exception('Requested nside=%i is greater than pixel_nside'%nside)\n        pix = np.arange(hp.nside2npix(nside), dtype=int)\n        map = self.inFootprint(pix,nside)\n        return map"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inFootprint(self, pixels, nside=None):\n        if np.isscalar(pixels): pixels = np.array([pixels])\n        if nside is None: nside = self.nside_likelihood\n\n        inside = np.zeros( len(pixels), dtype='bool')\n        if not self.nside_catalog:\n            catalog_pix = [0]\n        else:\n            catalog_pix = superpixel(pixels,nside,self.nside_catalog)\n            catalog_pix = np.intersect1d(catalog_pix,self.catalog_pixels)\n\n        for filenames in self.filenames[catalog_pix]:\n            # ADW: Need to replace with healpix functions...\n            #logger.debug(\"Loading %s\"%filenames['mask_1'])\n            #subpix_1,val_1 = ugali.utils.skymap.readSparseHealpixMap(filenames['mask_1'],'MAGLIM',construct_map=False)\n            _n,subpix_1,val_1 = read_partial_map(filenames['mask_1'],'MAGLIM',fullsky=False)\n            #logger.debug(\"Loading %s\"%filenames['mask_2'])\n            #subpix_2,val_2 = ugali.utils.skymap.readSparseHealpixMap(filenames['mask_2'],'MAGLIM',construct_map=False)\n            _n,subpix_2,val_2 = read_partial_map(filenames['mask_2'],'MAGLIM',fullsky=False)\n            subpix = np.intersect1d(subpix_1,subpix_2)\n            superpix = np.unique(superpixel(subpix,self.nside_pixel,nside))\n            inside |= np.in1d(pixels, superpix)\n            \n        return inside", "response": "Determine the set of set \n        of subpixels with valid data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef submit_all(self, coords=None, queue=None, debug=False):\n        if coords is None:\n            pixels = np.arange(hp.nside2npix(self.nside_likelihood))\n        else:\n            coords = np.asarray(coords)\n            if coords.ndim == 1:\n                coords = np.array([coords])\n            if coords.shape[1] == 2:\n                lon,lat = coords.T\n                radius  = np.zeros(len(lon))\n            elif coords.shape[1] == 3:\n                lon,lat,radius = coords.T\n            else:\n                raise Exception(\"Unrecognized coords shape:\"+str(coords.shape))\n\n            #ADW: targets is still in glon,glat\n            if self.config['coords']['coordsys'].lower() == 'cel':\n                lon,lat = gal2cel(lon,lat)\n\n            vec = ang2vec(lon,lat)\n            pixels = np.zeros(0, dtype=int)\n            for v,r in zip(vec,radius):\n                pix = query_disc(self.nside_likelihood,v,r,inclusive=True,fact=32)\n                pixels = np.hstack([pixels, pix])\n            #pixels = np.unique(pixels)\n\n        inside = ugali.utils.skymap.inFootprint(self.config,pixels)\n        if inside.sum() != len(pixels):\n            logger.warning(\"Ignoring pixels outside survey footprint:\\n\"+str(pixels[~inside]))\n        if inside.sum() == 0:\n            logger.warning(\"No pixels inside footprint.\")\n            return\n\n        # Only write the configfile once\n        outdir = mkdir(self.config['output']['likedir'])\n        # Actually copy config instead of re-writing\n        shutil.copy(self.config.filename,outdir)\n        configfile = join(outdir,os.path.basename(self.config.filename))\n\n        pixels = pixels[inside]\n        self.submit(pixels,queue=queue,debug=debug,configfile=configfile)", "response": "Submit likelihood analyses on a set of target locations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef submit(self, pixels, queue=None, debug=False, configfile=None):\n        # For backwards compatibility\n        batch = self.config['scan'].get('batch',self.config['batch'])\n        queue = batch['cluster'] if queue is None else queue\n\n        # Need to develop some way to take command line arguments...\n        self.batch = ugali.utils.batch.batchFactory(queue,**batch['opts'])\n        self.batch.max_jobs = batch.get('max_jobs',200)\n\n        if np.isscalar(pixels): pixels = np.array([pixels])\n\n        outdir = mkdir(self.config['output']['likedir'])\n        logdir = mkdir(join(outdir,'log'))\n        subdir = mkdir(join(outdir,'sub'))\n\n        # Save the current configuation settings; avoid writing \n        # file multiple times if configfile passed as argument.\n        if configfile is None:\n            shutil.copy(self.config.filename,outdir)\n            configfile = join(outdir,os.path.basename(self.config.filename))\n                \n        lon,lat = pix2ang(self.nside_likelihood,pixels)\n        commands = []\n        chunk = batch['chunk']\n        istart = 0\n        logger.info('=== Submit Likelihood ===')\n        for ii,pix in enumerate(pixels):\n            msg = '  (%i/%i) pixel=%i nside=%i; (lon, lat) = (%.2f, %.2f)'\n            msg = msg%(ii+1,len(pixels),pix, self.nside_likelihood,lon[ii],lat[ii])\n            logger.info(msg)\n\n            # Create outfile name\n            outfile = self.config.likefile%(pix,self.config['coords']['coordsys'].lower())\n            outbase = os.path.basename(outfile)\n            jobname = batch['jobname']\n\n            # Submission command\n            sub = not os.path.exists(outfile)\n            cmd = self.command(outfile,configfile,pix)\n            commands.append([ii,cmd,lon[ii],lat[ii],sub])\n            \n            if chunk == 0:\n                # No chunking\n                command = cmd\n                submit = sub\n                logfile = join(logdir,os.path.splitext(outbase)[0]+'.log')\n            elif (len(commands)%chunk==0) or (ii+1 == len(pixels)):\n                # End of chunk, create submission script\n                commands = np.array(commands,dtype=object)\n                istart, iend = commands[0][0], commands[-1][0]\n                subfile = join(subdir,'submit_%08i_%08i.sh'%(istart,iend))\n                logfile = join(logdir,'submit_%08i_%08i.log'%(istart,iend))\n                command = \"sh %s\"%subfile\n\n                submit = np.any(commands[:,-1])\n                if submit: self.write_script(subfile,commands)\n            else:\n                # Not end of chunk\n                continue\n            commands=[]\n\n            # Actual job submission\n            if not submit:\n                logger.info(self.skip)\n                continue\n            else:\n                job = self.batch.submit(command,jobname,logfile)\n                logger.info(\"  \"+job)\n                time.sleep(0.5)", "response": "Submit a likelihood job for the given pixels."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclassing method to check every settings. Will raise an ``ImproperlyConfigured`` exception with explanation.", "response": "def check(cls):\n        \"\"\"\n        Class method to check every settings.\n\n        Will raise an ``ImproperlyConfigured`` exception with explanation.\n        \"\"\"\n        if cls == AppSettings:\n            return None\n\n        exceptions = []\n        for setting in cls.settings.values():\n            try:\n                setting.check()\n            # pylama:ignore=W0703\n            except Exception as e:\n                exceptions.append(str(e))\n        if exceptions:\n            raise ImproperlyConfigured(\"\\n\".join(exceptions))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rec_append_fields(rec, names, arrs, dtypes=None):\n    if (not isstring(names) and iterable(names) and len(names) and isstring(names[0])):\n        if len(names) != len(arrs):\n            raise ValueError(\"number of arrays do not match number of names\")\n    else:  # we have only 1 name and 1 array\n        names = [names]\n        arrs = [arrs]\n    arrs = list(map(np.asarray, arrs))\n    if dtypes is None:\n        dtypes = [a.dtype for a in arrs]\n    elif not iterable(dtypes):\n        dtypes = [dtypes]\n    if len(arrs) != len(dtypes):\n        if len(dtypes) == 1:\n            dtypes = dtypes * len(arrs)\n        else:\n            raise ValueError(\"dtypes must be None, a single dtype or a list\")\n    old_dtypes = rec.dtype.descr\n    if six.PY2:\n        old_dtypes = [(name.encode('utf-8'), dt) for name, dt in old_dtypes]\n    newdtype = np.dtype(old_dtypes + list(zip(names, dtypes)))\n    newrec = np.recarray(rec.shape, dtype=newdtype)\n    for field in rec.dtype.fields:\n        newrec[field] = rec[field]\n    for name, arr in zip(names, arrs):\n        newrec[name] = arr\n    return newrec", "response": "Append fields to a record array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_args(name=\"\", args=None):\n    def _load_json_file(path):\n        with open(path) as f:\n            json_data = json.load(f)\n            json_data['path_to_json_file'] = path\n            return json_data\n\n    parser = argparse.ArgumentParser(description=\"%s collector for sending\"\n                                                 \" data to the performance\"\n                                                 \" platform\" % name)\n    parser.add_argument('-c', '--credentials', dest='credentials',\n                        type=_load_json_file,\n                        help='JSON file containing credentials '\n                             'for the collector',\n                        required=True)\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument('-l', '--collector', dest='collector_slug',\n                       type=str,\n                       help='Collector slug to query the API for the '\n                            'collector config')\n    group.add_argument('-q', '--query', dest='query',\n                       type=_load_json_file,\n                       help='JSON file containing details '\n                            'about the query to make '\n                            'against the source API '\n                            'and the target data-set')\n    parser.add_argument('-t', '--token', dest='token',\n                        type=_load_json_file,\n                        help='JSON file containing token '\n                             'for the collector',\n                        required=True)\n    parser.add_argument('-b', '--performanceplatform',\n                        dest='performanceplatform',\n                        type=_load_json_file,\n                        help='JSON file containing the Performance Platform '\n                             'config for the collector',\n                        required=True)\n    parser.add_argument('-s', '--start', dest='start_at',\n                        type=parse_date,\n                        help='Date to start collection from')\n    parser.add_argument('-e', '--end', dest='end_at',\n                        type=parse_date,\n                        help='Date to end collection')\n    parser.add_argument('--console-logging', dest='console_logging',\n                        action='store_true',\n                        help='Output logging to the console rather than file')\n    parser.add_argument('--dry-run', dest='dry_run',\n                        action='store_true',\n                        help='Instead of pushing to the Performance Platform '\n                             'the collector will print out what would have '\n                             'been pushed')\n    parser.set_defaults(console_logging=False, dry_run=False)\n    args = parser.parse_args(args)\n\n    return args", "response": "Parse command line arguments for a collector and return a Namespace with config and query options"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the configuration options out of a YAML configuration file.", "response": "def parse(self, configManager, config):\n        \"\"\"\n        Parse configuration options out of a YAML configuration file.\n\n        Inputs: configManager - Our parent ConfigManager instance which is constructing the Config object.\n                config        - The _Config object containing configuration options populated thus far.\n\n        Outputs: A dictionary of new configuration options to add to the Config object.\n        \"\"\"\n\n        jsonConfigs = json.loads(self.jsonString)\n        if isinstance(jsonConfigs, dict):\n            return jsonConfigs\n\n        raise self.subparserException(\"JSON string parsed did not result in a dictionary, but instead a %s\"\n                                      % type(jsonConfigs))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_time_index_from_datetime(self, value, best_fit=True):\n\n        steps = self.variable.time_stops\n        if value in steps:\n            self.time_index = steps.index(value)\n        elif best_fit:\n            self.time_index = utils.best_fit(steps, value)\n        else:\n            raise ValueError(\"Invalid date\")", "response": "Sets the time_index parameter from a datetime using start end interval and units information from the service\n        configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a hash of the current render configuration from the variable renderer and time_index parameters.", "response": "def hash(self):\n        \"\"\"\n        Returns a hash of this render configuration from the variable, renderer, and time_index parameters. Used for\n        caching the full-extent, native projection render so that subsequent requests can be served by a warp\n        operation only.\n        \"\"\"\n\n        renderer_str = \"{}|{}|{}|{}\".format(\n            self.renderer.__class__.__name__, self.renderer.colormap, self.renderer.fill_value,\n            self.renderer.background_color\n        )\n        if isinstance(self.renderer, StretchedRenderer):\n            renderer_str = \"{}|{}|{}\".format(renderer_str, self.renderer.method, self.renderer.colorspace)\n        elif isinstance(self.renderer, UniqueValuesRenderer):\n            renderer_str = \"{}|{}\".format(renderer_str, self.renderer.labels)\n\n        return hash(\"{}/{}/{}\".format(self.variable.pk, renderer_str, self.time_index))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef factory(type, module=None, **kwargs):\n    cls = type\n    if module is None: module = __name__\n    fn = lambda member: inspect.isclass(member) and member.__module__==module\n    classes = odict(inspect.getmembers(sys.modules[module], fn))\n    members = odict([(k.lower(),v) for k,v in classes.items()])\n    lower = cls.lower()\n    if lower not in list(members.keys()):\n        msg = \"Unrecognized class: %s.%s\"%(module,cls)\n        raise KeyError(msg)\n    return members[lower](**kwargs)", "response": "Factory for creating objects of the specified type. Arguments are passed directly to the\n    constructor of the chosen class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jprecess(ra, dec, mu_radec=None, parallax=None, rad_vel=None, epoch=None):\n   if isinstance(ra, ndarray):\n      ra = array(ra)\n      dec = array(dec)\n   else:\n      ra = array([ra0])\n      dec = array([dec0])\n   n = ra.size\n    \n   if rad_vel is None:   \n      rad_vel = zeros(n,dtype=float)\n   else:\n      if not isinstance(rad_vel, ndarray):\n         rad_vel = array([rad_vel],dtype=float)\n      if rad_vel.size != n:   \n         raise Exception('ERROR - RAD_VEL keyword vector must be of the same length as RA and DEC')\n    \n   if (mu_radec is not None):   \n      if (array(mu_radec).size != 2 * n):   \n         raise Exception('ERROR - MU_RADEC keyword (proper motion) be dimensioned (2,' + strtrim(n, 2) + ')')\n      mu_radec = mu_radec * 1.\n    \n   if parallax is None:   \n      parallax = zeros(n,dtype=float)\n   else:   \n      if not isinstance(parallax, ndarray):\n         parallax = array([parallax],dtype=float)\n    \n   if epoch is None:   \n      epoch = 1950.0e0\n    \n   radeg = 180.e0 / pi\n   sec_to_radian = 1/radeg/3600.\n   #sec_to_radian = lambda x : deg2rad(x/3600.)\n    \n   m =  array([ \n      array([+0.9999256782e0, +0.0111820610e0, +0.0048579479e0,  \\\n                -0.000551e0,     +0.238514e0,     -0.435623e0      ]), \n      array([ -0.0111820611e0, +0.9999374784e0, -0.0000271474e0,  \\\n                 -0.238565e0,     -0.002667e0,      +0.012254e0    ]), \n      array([ -0.0048579477e0, -0.0000271765e0, +0.9999881997e0 , \\\n                 +0.435739e0,      -0.008541e0,      +0.002117e0   ]), \n      array([ +0.00000242395018e0, +0.00000002710663e0, +0.00000001177656e0, \\\n                 +0.99994704e0,    +0.01118251e0,    +0.00485767e0 ]), \n      array([ -0.00000002710663e0, +0.00000242397878e0, -0.00000000006582e0, \\\n                 -0.01118251e0,     +0.99995883e0,    -0.00002714e0]), \n      array([ -0.00000001177656e0, -0.00000000006587e0, 0.00000242410173e0, \\\n                 -0.00485767e0,   -0.00002718e0,     1.00000956e0  ]) ])\n    \n   a = 1e-6 * array([ -1.62557e0, -0.31919e0, -0.13843e0])        #in radians\n   a_dot = 1e-3 * array([1.244e0, -1.579e0, -0.660e0 ])           #in arc seconds per century\n    \n   ra_rad = deg2rad(ra)\n   dec_rad = deg2rad(dec)\n   cosra = cos(ra_rad)\n   sinra = sin(ra_rad)\n   cosdec = cos(dec_rad)\n   sindec = sin(dec_rad)\n    \n   ra_2000 = ra*0.\n   dec_2000 = dec*0.\n    \n   for i in range(n):\n      r0 = array([ cosra[i]*cosdec[i], sinra[i]*cosdec[i], sindec[i] ])\n    \n      if (mu_radec is None):   \n         mu_a = 0.\n         mu_d = 0.\n      else:\n         mu_a = mu_radec[ i, 0]\n         mu_d = mu_radec[ i, 1]\n    \n      #Velocity vector\n      r0_dot = array([-mu_a*sinra[i]*cosdec[i] - mu_d*cosra[i]*sindec[i], \\\n                         mu_a*cosra[i]*cosdec[i] - mu_d*sinra[i]*sindec[i] , \\\n                         mu_d*cosdec[i] ])  + 21.095e0 * rad_vel[i] * parallax[i] * r0\n    \n      r1 = r0 - a + ((a * r0).sum())*r0\n      r1_dot = r0_dot - a_dot + (( a * r0).sum())*r0\n    \n      r_1 = concatenate((r1, r1_dot))\n      r = transpose(dot(transpose(m),transpose(r_1)))\n    \n      if mu_radec is None:   \n         rr = r[0:3] \n         v =  r[3:6] \n         t = ((epoch - 1950.0e0) - 50.00021e0)/100.0\n         rr1 = rr + sec_to_radian*v*t\n         x = rr1[0]  ; y = rr1[1]  ; z = rr1[2]  \n      else:\n         x = r[0]  ; y = r[1]  ; z = r[2]  \n         x_dot = r[3]  ; y_dot= r[4]  ;  z_dot = r[5]\n    \n      r2 = x**2 + y**2 + z**2\n      rmag = sqrt( r2 )\n      dec_2000[i] = arcsin(z / rmag)\n      ra_2000[i] = arctan2(y, x)\n    \n      if mu_radec is not None:\n         mu_radec[i, 0] = ( x*y_dot - y*x_dot) / ( x**2 + y**2)\n         mu_radec[i, 1] = ( z_dot* (x**2 + y**2) - z*(x*x_dot + y*y_dot) ) /  \\\n                     ( r2*sqrt( x**2 + y**2) )\n    \n      if parallax[i] > 0.:\n         rad_vel[i] = ( x*x_dot + y*y_dot + z*z_dot )/ (21.095*parallax[i]*rmag)\n         parallax[i] = parallax[i] / rmag\n    \n   neg = (ra_2000 < 0)\n   if neg.any() > 0:\n      ra_2000[neg] = ra_2000[neg] + 2.0 * pi\n      \n   ra_2000 = ra_2000*radeg ; dec_2000 = dec_2000*radeg\n    \n   if ra.size == 1:\n      ra_2000 = ra_2000[0]     ; dec_2000 = dec_2000[0]\n    \n   return ra_2000, dec_2000", "response": "This function calculates the mean place of a star at a given J2000 position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnaming BPRECESS PURPOSE: Precess positions from J2000.0 (FK5) to B1950.0 (FK4) EXPLANATION: Calculates the mean place of a star at B1950.0 on the FK4 system from the mean place at J2000.0 on the FK5 system. CALLING SEQUENCE: bprecess, ra, dec, ra_1950, dec_1950, [ MU_RADEC = , PARALLAX = RAD_VEL =, EPOCH = ] INPUTS: RA,DEC - Input J2000 right ascension and declination in *degrees*. Scalar or N element vector OUTPUTS: RA_1950, DEC_1950 - The corresponding B1950 right ascension and declination in *degrees*. Same number of elements as RA,DEC but always double precision. OPTIONAL INPUT-OUTPUT KEYWORDS MU_RADEC - 2xN element double precision vector containing the proper motion in seconds of arc per tropical *century* in right ascension and declination. PARALLAX - N_element vector giving stellar parallax (seconds of arc) RAD_VEL - N_element vector giving radial velocity in km/s The values of MU_RADEC, PARALLAX, and RADVEL will all be modified upon output to contain the values of these quantities in the B1950 system. The parallax and radial velocity will have a very minor influence on the B1950 position. EPOCH - scalar giving epoch of original observations, default 2000.0d This keyword value is only used if the MU_RADEC keyword is not set. NOTES: The algorithm is taken from the Explanatory Supplement to the Astronomical Almanac 1992, page 186. Also see Aoki et al (1983), A&A, 128,263 BPRECESS distinguishes between the following two cases: (1) The proper motion is known and non-zero (2) the proper motion is unknown or known to be exactly zero (i.e. extragalactic radio sources). In this case, the reverse of the algorithm in Appendix 2 of Aoki et al. (1983) is used to ensure that the output proper motion is exactly zero. Better precision can be achieved in this case by inputting the EPOCH of the original observations. The error in using the IDL procedure PRECESS for converting between B1950 and J1950 can be up to 12\", mainly in right ascension. If better accuracy than this is needed then BPRECESS should be used. An unsystematic comparison of BPRECESS with the IPAC precession routine (http://nedwww.ipac.caltech.edu/forms/calculator.html) always gives differences less than 0.15\". EXAMPLE: The SAO2000 catalogue gives the J2000 position and proper motion for the star HD 119288. Find the B1950 position. RA(2000) = 13h 42m 12.740s Dec(2000) = 8d 23' 17.69'' Mu(RA) = -.0257 s/yr Mu(Dec) = -.090 ''/yr IDL> mu_radec = 100D* [ -15D*.0257, -0.090 ] IDL> ra = ten(13, 42, 12.740)*15.D IDL> dec = ten(8, 23, 17.69) IDL> bprecess, ra, dec, ra1950, dec1950, mu_radec = mu_radec IDL> print, adstring(ra1950, dec1950,2) ===> 13h 39m 44.526s +08d 38' 28.63\" REVISION HISTORY: Written, W. Landsman October, 1992 Vectorized, W. Landsman February, 1994 Treat case where proper motion not known or exactly zero November 1994 Handling of arrays larger than 32767 Lars L. Christensen, march, 1995 Converted to IDL V5.0 W. Landsman September 1997 Fixed bug where A term not initialized for vector input W. Landsman February 2000 Converted to python Sergey Koposov july 2010", "response": "def bprecess(ra0, dec0, mu_radec=None, parallax=None, rad_vel=None, epoch=None):\n   \"\"\"\n    NAME:\n          BPRECESS\n    PURPOSE:\n          Precess positions from J2000.0 (FK5) to B1950.0 (FK4)\n    EXPLANATION:\n          Calculates the mean place of a star at B1950.0 on the FK4 system from\n          the mean place at J2000.0 on the FK5 system.\n   \n    CALLING SEQUENCE:\n          bprecess, ra, dec, ra_1950, dec_1950, [ MU_RADEC = , PARALLAX =\n                                          RAD_VEL =, EPOCH =   ]\n   \n    INPUTS:\n          RA,DEC - Input J2000 right ascension and declination in *degrees*.\n                  Scalar or N element vector\n   \n    OUTPUTS:\n          RA_1950, DEC_1950 - The corresponding B1950 right ascension and\n                  declination in *degrees*.    Same number of elements as\n                  RA,DEC but always double precision.\n   \n    OPTIONAL INPUT-OUTPUT KEYWORDS\n          MU_RADEC - 2xN element double precision vector containing the proper\n                     motion in seconds of arc per tropical *century* in right\n                     ascension and declination.\n          PARALLAX - N_element vector giving stellar parallax (seconds of arc)\n          RAD_VEL  - N_element vector giving radial velocity in km/s\n   \n          The values of MU_RADEC, PARALLAX, and RADVEL will all be modified\n          upon output to contain the values of these quantities in the\n          B1950 system.  The parallax and radial velocity will have a very\n          minor influence on the B1950 position.\n   \n          EPOCH - scalar giving epoch of original observations, default 2000.0d\n              This keyword value is only used if the MU_RADEC keyword is not set.\n    NOTES:\n          The algorithm is taken from the Explanatory Supplement to the\n          Astronomical Almanac 1992, page 186.\n          Also see Aoki et al (1983), A&A, 128,263\n   \n          BPRECESS distinguishes between the following two cases:\n          (1) The proper motion is known and non-zero\n          (2) the proper motion is unknown or known to be exactly zero (i.e.\n                  extragalactic radio sources).   In this case, the reverse of\n                  the algorithm in Appendix 2 of Aoki et al. (1983) is used to\n                  ensure that the output proper motion is  exactly zero. Better\n                  precision can be achieved in this case by inputting the EPOCH\n                  of the original observations.\n   \n          The error in using the IDL procedure PRECESS for converting between\n          B1950 and J1950 can be up to 12\", mainly in right ascension.   If\n          better accuracy than this is needed then BPRECESS should be used.\n   \n          An unsystematic comparison of BPRECESS with the IPAC precession\n          routine (http://nedwww.ipac.caltech.edu/forms/calculator.html) always\n          gives differences less than 0.15\".\n    EXAMPLE:\n          The SAO2000 catalogue gives the J2000 position and proper motion for\n          the star HD 119288.   Find the B1950 position.\n   \n          RA(2000) = 13h 42m 12.740s      Dec(2000) = 8d 23' 17.69''\n          Mu(RA) = -.0257 s/yr      Mu(Dec) = -.090 ''/yr\n   \n          IDL> mu_radec = 100D* [ -15D*.0257, -0.090 ]\n          IDL> ra = ten(13, 42, 12.740)*15.D\n          IDL> dec = ten(8, 23, 17.69)\n          IDL> bprecess, ra, dec, ra1950, dec1950, mu_radec = mu_radec\n          IDL> print, adstring(ra1950, dec1950,2)\n                  ===> 13h 39m 44.526s    +08d 38' 28.63\"\n   \n    REVISION HISTORY:\n          Written,    W. Landsman                October, 1992\n          Vectorized, W. Landsman                February, 1994\n          Treat case where proper motion not known or exactly zero  November 1994\n          Handling of arrays larger than 32767   Lars L. Christensen, march, 1995\n          Converted to IDL V5.0   W. Landsman   September 1997\n          Fixed bug where A term not initialized for vector input\n               W. Landsman        February 2000\n         Converted to python \t\t\tSergey Koposov july 2010   \n   \"\"\"\n\n   scal = True\n   if isinstance(ra0, ndarray):\n      ra = ra0\n      dec = dec0\n      n = ra.size\n      scal = False\n   else:\n      n = 1\n      ra = array([ra0])\n      dec = array([dec0])\n      \n   if rad_vel is None:   \n      rad_vel = zeros(n)\n   else:\n      if not isinstance(rad_vel, ndarray):\n         rad_vel = array([rad_vel],dtype=float)\n      if rad_vel.size != n:   \n         raise Exception('ERROR - RAD_VEL keyword vector must be of the same length as RA and DEC')\n   \n   if (mu_radec is not None):   \n      if (array(mu_radec).size != 2 * n):   \n         raise Exception('ERROR - MU_RADEC keyword (proper motion) be dimensioned (2,' + strtrim(n, 2) + ')')\n      mu_radec = mu_radec * 1.\n   \n   if parallax is None:   \n      parallax = zeros(n)\n   else:   \n      if not isinstance(parallax, ndarray):\n         parallax = array([parallax],dtype=float)\n   \n   if epoch is None:   \n      epoch = 2000.0e0\n   \n   radeg = 180.e0 / pi\n   sec_to_radian = lambda x : deg2rad(x/3600.)\n   \n   m = array([array([+0.9999256795e0, -0.0111814828e0, -0.0048590040e0, -0.000551e0, -0.238560e0, +0.435730e0]),\n   array([+0.0111814828e0, +0.9999374849e0, -0.0000271557e0, +0.238509e0, -0.002667e0, -0.008541e0]),\n   array([+0.0048590039e0, -0.0000271771e0, +0.9999881946e0, -0.435614e0, +0.012254e0, +0.002117e0]),\n   array([-0.00000242389840e0, +0.00000002710544e0, +0.00000001177742e0, +0.99990432e0, -0.01118145e0, -0.00485852e0]),\n   array([-0.00000002710544e0, -0.00000242392702e0, +0.00000000006585e0, +0.01118145e0, +0.99991613e0, -0.00002716e0]),\n   array([-0.00000001177742e0, +0.00000000006585e0, -0.00000242404995e0, +0.00485852e0, -0.00002717e0, +0.99996684e0])])\n   \n   a_dot = 1e-3 * array([1.244e0, -1.579e0, -0.660e0])           #in arc seconds per century\n   \n   ra_rad = deg2rad(ra)\n   dec_rad = deg2rad(dec)\n   cosra = cos(ra_rad)\n   sinra = sin(ra_rad)\n   cosdec = cos(dec_rad)\n   sindec = sin(dec_rad)\n   \n   dec_1950 = dec * 0.\n   ra_1950 = ra * 0.\n   \n   for i in range(n):\n   \n   # Following statement moved inside loop in Feb 2000.\n      a = 1e-6 * array([-1.62557e0, -0.31919e0, -0.13843e0])        #in radians\n      \n      r0 = array([cosra[i] * cosdec[i], sinra[i] * cosdec[i], sindec[i]])\n      \n      if (mu_radec is not None):   \n         \n         mu_a = mu_radec[i,0]\n         mu_d = mu_radec[i,1]\n         r0_dot = array([-mu_a * sinra[i] * cosdec[i] - mu_d * cosra[i] * sindec[i], mu_a * cosra[i] * cosdec[i] - mu_d * sinra[i] * sindec[i], mu_d * cosdec[i]]) + 21.095e0 * rad_vel[i] * parallax[i] * r0\n         \n      else:   \n         r0_dot = array([0.0e0, 0.0e0, 0.0e0])\n      \n      r_0 = concatenate((r0, r0_dot))\n      r_1 = transpose(dot(transpose(m), transpose(r_0)))\n      \n      # Include the effects of the E-terms of aberration to form r and r_dot.\n      \n      r1 = r_1[0:3]\n      r1_dot = r_1[3:6]\n      \n      if mu_radec is None:   \n         r1 = r1 + sec_to_radian ( r1_dot * (epoch - 1950.0e0) / 100. )\n         a = a + sec_to_radian ( a_dot * (epoch - 1950.0e0) / 100. )\n      \n      x1 = r_1[0]   ;   y1 = r_1[1]    ;  z1 = r_1[2]\n      rmag = sqrt(x1 ** 2 + y1 ** 2 + z1 ** 2)\n      \n      \n      s1 = r1 / rmag    ; s1_dot = r1_dot / rmag\n      \n      s = s1\n      for j in arange(0, 3):\n         r = s1 + a - ((s * a).sum()) * s\n         s = r / rmag\n      x = r[0]          ; y = r[1]     ;  z = r[2]\n      r2 = x ** 2 + y ** 2 + z ** 2\n      rmag = sqrt(r2)\n      \n      if mu_radec is not None:   \n         r_dot = s1_dot + a_dot - ((s * a_dot).sum()) * s\n         x_dot = r_dot[0]  ; y_dot = r_dot[1]  ;  z_dot = r_dot[2]\n         mu_radec[i,0] = (x * y_dot - y * x_dot) / (x ** 2 + y ** 2)\n         mu_radec[i,1] = (z_dot * (x ** 2 + y ** 2) - z * (x * x_dot + y * y_dot)) / (r2 * sqrt(x ** 2 + y ** 2))\n      \n      dec_1950[i] = arcsin(z / rmag)\n      ra_1950[i] = arctan2(y, x)\n      \n      if parallax[i] > 0.:   \n         rad_vel[i] = (x * x_dot + y * y_dot + z * z_dot) / (21.095 * parallax[i] * rmag)\n         parallax[i] = parallax[i] / rmag\n   \n   neg = (ra_1950 < 0)\n   if neg.any() > 0:   \n      ra_1950[neg] = ra_1950[neg] + 2.e0 * pi\n   \n   ra_1950 = rad2deg(ra_1950)\n   dec_1950 = rad2deg(dec_1950)\n   \n   # Make output scalar if input was scalar\n   if scal:\n      return ra_1950[0],dec_1950[0]\n   else:\n      return ra_1950, dec_1950"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_renderer_from_definition(config):\n\n    options = config.get('options', {})\n\n    try:\n        renderer_type = config['type']\n        renderer_colors = [(float(x[0]), hex_to_color(x[1])) for x in config['colors']]\n        fill_value = options.get('fill_value')\n        if fill_value is not None:\n            fill_value = float(fill_value)\n    except KeyError:\n        raise ValueError(\"Missing required keys from renderer configuration\")\n\n    renderer_kwargs = {\n        'colormap': renderer_colors,\n        'fill_value': fill_value,\n    }\n\n    if renderer_type == \"stretched\":\n        color_space = options.get('color_space', 'hsv').lower().strip()\n        if not color_space in ('rgb', 'hsv'):\n            raise ValueError(\"Invalid color space: {}\".format(color_space))\n\n        renderer = StretchedRenderer(colorspace=color_space, **renderer_kwargs)\n    elif renderer_type == \"classified\":\n        renderer = ClassifiedRenderer(**renderer_kwargs)\n    elif renderer_type == \"unique\":\n        try:\n            labels = [six.text_type(x) for x in options.get('labels', [])]\n        except TypeError:\n            raise ValueError(\"Labels option must be an array\")\n\n        renderer = UniqueValuesRenderer(labels=labels, **renderer_kwargs)\n\n    return renderer", "response": "Returns a renderer object based on the configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary definition of the given renderer", "response": "def get_definition_from_renderer(renderer):\n    \"\"\"Returns a dictionary definition of the given renderer\"\"\"\n\n    config = {\n        'colors': [[x[0], x[1].to_hex()] for x in renderer.colormap],\n        'options': {}\n    }\n\n    if renderer.fill_value:\n        config['options']['fill_value'] = renderer.fill_value\n\n    if isinstance(renderer, StretchedRenderer):\n        config['type'] = 'stretched'\n        config['options']['color_space'] = renderer.colorspace\n    elif isinstance(renderer, UniqueValuesRenderer):\n        config['type'] = 'unique'\n        if renderer.labels:\n            config['options']['labels'] = renderer.labels\n    elif isinstance(renderer, ClassifiedRenderer):\n        config['type'] = 'classified'\n    else:\n        raise ValueError('{0} is not a valid renderer type'.format(renderer.__class__.__name__))\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a copy of the parameters.", "response": "def params(self):\n        \"\"\" Return a *copy* (we hope) of the parameters.\n        DANGER: Altering properties directly doesn't call model._cache\n        \"\"\"\n        params = odict([])\n        for key,model in self.models.items():\n            params.update(model.params)\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset a model instance for the current object.", "response": "def set_model(self, name, model):\n        \"\"\" Set a model.\n\n        Parameters\n        ----------\n        name  : name of the model -- e.g., richness, kernel, isochrone, etc.\n        model : the model instance\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # Careful to not use `hasattr`\n        # https://hynek.me/articles/hasattr/\n        try:\n            self.__getattribute__('models')\n        except AttributeError:\n            object.__setattr__(self, 'models',odict())\n        self.models[name] = model"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the parameter values", "response": "def set_params(self,**kwargs):\n        \"\"\" Set the parameter values \"\"\"\n        for key,value in list(kwargs.items()):\n            setattr(self,key,value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an odict of the parameter names and values", "response": "def get_params(self):\n        \"\"\" Get an odict of the parameter names and values \"\"\"\n        return odict([(key,param.value) for key,param in self.params.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_free_params(self):\n        return odict([(key,param.value) for key,param in self.params.items() if param.free])", "response": "Get an odict of free parameter names and values"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate all matches found within a string for a regex and yield each match as a string", "response": "def iter_finds(regex_obj, s):\n    \"\"\"Generate all matches found within a string for a regex and yield each match as a string\"\"\"\n    if isinstance(regex_obj, str):\n        for m in re.finditer(regex_obj, s):\n            yield m.group()\n    else:\n        for m in regex_obj.finditer(s):\n            yield m.group()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mergeCatalogs(catalog_list):\n    # Check the columns\n    for c in catalog_list:\n        if c.data.dtype.names != catalog_list[0].data.dtype.names:\n            msg = \"Catalog data columns not the same.\"\n            raise Exception(msg)\n    data = np.concatenate([c.data for c in catalog_list])\n    config = catalog_list[0].config\n    return Catalog(config,data=data)", "response": "Merge a list of Catalogs into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef applyCut(self, cut):\n        return Catalog(self.config, data=self.data[cut])", "response": "Returns a new catalog which is a subset of objects selectedby the input cut array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a random catalog by boostrapping the colors of the objects in the current catalog.", "response": "def bootstrap(self, mc_bit=0x10, seed=None):\n        \"\"\"\n        Return a random catalog by boostrapping the colors of the objects in the current catalog.\n        \"\"\"\n        if seed is not None: np.random.seed(seed)\n        data = copy.deepcopy(self.data)\n        idx = np.random.randint(0,len(data),len(data))\n        data[self.config['catalog']['mag_1_field']][:] = self.mag_1[idx]\n        data[self.config['catalog']['mag_err_1_field']][:] = self.mag_err_1[idx]\n        data[self.config['catalog']['mag_2_field']][:] = self.mag_2[idx]\n        data[self.config['catalog']['mag_err_2_field']][:] = self.mag_err_2[idx]\n        data[self.config['catalog']['mc_source_id_field']][:] |= mc_bit\n        return Catalog(self.config, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef project(self, projector = None):\n        msg = \"'%s.project': ADW 2018-05-05\"%self.__class__.__name__\n        DeprecationWarning(msg)\n        if projector is None:\n            try:\n                self.projector = ugali.utils.projector.Projector(self.config['coords']['reference'][0],\n                                                                 self.config['coords']['reference'][1])\n            except KeyError:\n                logger.warning('Projection reference point is median (lon, lat) of catalog objects')\n                self.projector = ugali.utils.projector.Projector(np.median(self.lon), np.median(self.lat))\n        else:\n            self.projector = projector\n\n        self.x, self.y = self.projector.sphereToImage(self.lon, self.lat)", "response": "Project coordinates on sphere to image plane using Projector class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spatialBin(self, roi):\n        if hasattr(self,'pixel_roi_index') and hasattr(self,'pixel'): \n            logger.warning('Catalog alread spatially binned')\n            return\n\n        # ADW: Not safe to set index = -1 (since it will access last entry); \n        # np.inf would be better...\n        self.pixel = ang2pix(self.config['coords']['nside_pixel'],self.lon,self.lat)\n        self.pixel_roi_index = roi.indexROI(self.lon,self.lat)\n\n        logger.info(\"Found %i objects outside ROI\"%(self.pixel_roi_index < 0).sum())", "response": "Calculate indices of ROI pixels corresponding to objects locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, outfile, clobber=True, **kwargs):\n        fitsio.write(outfile,self.data,clobber=True,**kwargs)", "response": "Writes the current object catalog to a FITS file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse(self, roi=None, filenames=None):\n        if (roi is not None) and (filenames is not None):\n            msg = \"Cannot take both roi and filenames\"\n            raise Exception(msg)\n\n        if roi is not None:\n            pixels = roi.getCatalogPixels()\n            filenames = self.config.getFilenames()['catalog'][pixels]\n        elif filenames is None:\n            filenames = self.config.getFilenames()['catalog'].compressed()\n        else:\n            filenames = np.atleast_1d(filenames)\n\n        if len(filenames) == 0:\n            msg = \"No catalog files found.\"\n            raise Exception(msg)\n\n        # Load the data\n        self.data = load_infiles(filenames)\n\n        # Apply a selection cut\n        self._applySelection()\n\n        # Cast data to recarray (historical reasons)\n        self.data = self.data.view(np.recarray)", "response": "Parse the FITS files into recarray."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _defineVariables(self):\n        logger.info('Catalog contains %i objects'%(len(self.data)))\n\n        mc_source_id_field = self.config['catalog']['mc_source_id_field']\n        if mc_source_id_field is not None:\n            if mc_source_id_field not in self.data.dtype.names:\n                array = np.zeros(len(self.data),dtype='>i8') # FITS byte-order convention\n                self.data = mlab.rec_append_fields(self.data,\n                                                   names=mc_source_id_field,\n                                                   arrs=array)\n            logger.info('Found %i simulated objects'%(np.sum(self.mc_source_id>0)))", "response": "Internal helper function to define pertinent variables from catalog data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a node to the workflow.", "response": "def add_node(self, node_id, task, inputs):\n        \"\"\"\n        Adds a node to the workflow.\n\n        :param node_id: A unique identifier for the new node.\n        :param task: The task to run.\n        :param inputs: A mapping of inputs from workflow inputs, or outputs from other nodes. The format should be\n            `{input_name: (source, value), ...}` where `input_name` is the parameter name for the task input, source is\n            \"input\" or \"dependency\" and `value` is either the workflow input name (if source is \"input\") or a 2-tuple\n            with a node id and an output parameter name from that node's task to map to the input.\n        \"\"\"\n\n        if node_id in self.nodes_by_id:\n            raise ValueError('The node {0} already exists in this workflow.'.format(node_id))\n\n        node = WorkflowNode(node_id, task, inputs)\n        self.nodes_by_id[node_id] = node\n\n        for source, value in six.itervalues(inputs):\n            if source == 'dependency':\n                dependents = self.dependents_by_node_id.get(value[0], set())\n                dependents.add(node_id)\n                self.dependents_by_node_id[value[0]] = dependents"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap the output from a node to a workflow output.", "response": "def map_output(self, node_id, node_output_name, parameter_name):\n        \"\"\"\n        Maps the output from a node to a workflow output.\n\n        :param node_id: The id of the node to map from.\n        :param node_output_name: The output parameter name for the node task to map to the workflow output.\n        :param parameter_name: The workflow output parameter name.\n        \"\"\"\n\n        self.output_mapping[parameter_name] = (node_id, node_output_name)\n\n        dependents = self.dependents_by_node_id.get(node_id, set())\n        dependents.add('output_{}'.format(parameter_name))\n        self.dependents_by_node_id[node_id] = dependents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_json(self, indent=None):\n\n        inputs = ParameterCollection(self.inputs)\n\n        d = {\n            'meta': {\n                'name': self.name,\n                'description': self.description\n            },\n            'inputs': [],\n            'workflow': [],\n            'outputs': [{'name': k, 'node': v} for k, v in six.iteritems(self.output_mapping)]\n        }\n\n        for parameter in self.inputs:\n            input_info = {\n                'name': parameter.name,\n                'type': parameter.id\n            }\n\n            args, kwargs = parameter.serialize_args()\n            args = list(args)\n            args.pop(0)  # 'name' is already taken care of\n            kwargs.pop('required', None)  # 'required' is assumed True for workflow inputs\n            if args or kwargs:\n                input_info['args'] = [args, kwargs]\n\n            d['inputs'].append(input_info)\n\n        for node in sorted(six.itervalues(self.nodes_by_id), key=lambda x: x.id):\n            task_name = node.task.name\n            if not task_name:\n                raise ValueError('The task {0} does not have a name and therefore cannot be serialized.'.format(\n                    node.task.__class__.__name__)\n                )\n\n            node_inputs = {}\n            for input_name, (source, value) in six.iteritems(node.inputs):\n                input_info = {'source': source}\n\n                if source == 'input':\n                    input_info['input'] = inputs.by_name[value].name\n                else:\n                    input_info['node'] = value\n\n                node_inputs[input_name] = input_info\n\n            d['workflow'].append({\n                'id': node.id,\n                'task': task_name,\n                'inputs': node_inputs\n            })\n\n        return json.dumps(d, indent=indent)", "response": "Serialize this workflow to JSON"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new workflow deserialized from a JSON string", "response": "def from_json(cls, text):\n        \"\"\"Return a new workflow, deserialized from a JSON string\"\"\"\n\n        d = json.loads(text)\n\n        meta = d.get('meta', {})\n        workflow = cls(name=meta.get('name'), description=meta.get('description'))\n\n        for workflow_input in d.get('inputs', []):\n            parameter_cls = Parameter.by_id(workflow_input['type'])\n\n            args = [workflow_input['name']]\n            kwargs = {'required': True}\n            if workflow_input.get('args'):\n                args = workflow_input['args'][0] + args\n                kwargs.update(workflow_input['args'][1])\n                args, kwargs = parameter_cls.deserialize_args(args, kwargs)\n\n            workflow.inputs.append(parameter_cls(*args, **kwargs))\n\n        for node in d.get('workflow', []):\n            node_inputs = {}\n            for k, v in six.iteritems(node.get('inputs', {})):\n                node_inputs[k] = (v['source'], v.get('input') or v.get('node'))\n\n            workflow.add_node(node['id'], Task.by_name(node['task'])(), node_inputs)\n\n        for output in d.get('outputs', []):\n            node = output['node']\n            node_parameters = ParameterCollection(workflow.nodes_by_id[node[0]].task.outputs)\n\n            # Add parameter to workflow output\n            output_param = copy.copy(node_parameters.by_name[node[1]])\n            output_param.name = output['name']\n            workflow.outputs.append(output_param)\n\n            workflow.map_output(node[0], node[1], output['name'])\n\n        return workflow"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the handler wrapping the default handler if static files should be served. Otherwise return the default handler.", "response": "def get_handler(self, *args, **options):\n        '''\n        Return the static files serving handler wrapping the default handler,\n        if static files should be served. Otherwise return the default handler.\n        '''\n        handler = super().get_handler(*args, **options)\n        use_static_handler = options['use_static_handler']\n        insecure_serving = options['insecure_serving']\n        if use_static_handler and (settings.DEBUG or insecure_serving):\n            return CRAStaticFilesHandler(handler)\n        return handler"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef surveyPixel(lon, lat, nside_pix, nside_subpix = None):\n    pix = np.unique(ang2pix(nside_pix, lon, lat))\n    if nside_subpix is None:\n        return pix\n    else:\n        subpix_array = []\n        for ii in range(0, len(pix)):\n            subpix = subpixel(pix[ii], nside_pix, nside_subpix)\n            subpix_array.append(subpix)\n        return pix, np.array(subpix_array)", "response": "Return the set of HEALPix pixels that cover the given coordinates at resolution nside."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inFootprint(config, pixels, nside=None):\n    config = Config(config)\n    nside_catalog    = config['coords']['nside_catalog']\n    nside_likelihood = config['coords']['nside_likelihood']\n    nside_pixel      = config['coords']['nside_pixel']\n\n    if np.isscalar(pixels): pixels = np.array([pixels])\n    if nside is None: nside = nside_likelihood\n\n    filenames = config.getFilenames()\n    catalog_pixels = filenames['pix'].compressed()\n\n    inside = np.zeros(len(pixels), dtype=bool)\n    if not nside_catalog:\n        catalog_pix = [0]\n    else:\n        catalog_pix = superpixel(pixels,nside,nside_catalog)\n        catalog_pix = np.intersect1d(catalog_pix,catalog_pixels)\n\n    for fnames in filenames[catalog_pix]:\n        logger.debug(\"Loading %s\"%filenames['mask_1'])\n        #subpix_1,val_1 = ugali.utils.skymap.readSparseHealpixMap(fnames['mask_1'],'MAGLIM',construct_map=False)\n        _nside,subpix_1,val_1 = ugali.utils.healpix.read_partial_map(fnames['mask_1'],'MAGLIM',fullsky=False)\n        logger.debug(\"Loading %s\"%fnames['mask_2'])\n        #subpix_2,val_2 = ugali.utils.skymap.readSparseHealpixMap(fnames['mask_2'],'MAGLIM',construct_map=False)\n        _nside,subpix_2,val_2 = ugali.utils.healpix.read_partial_map(fnames['mask_2'],'MAGLIM',fullsky=False)\n        subpix = np.intersect1d(subpix_1,subpix_2)\n        superpix = np.unique(superpixel(subpix,nside_pixel,nside))\n        inside |= np.in1d(pixels, superpix)\n        \n    return inside", "response": "Open each valid filename for the set of pixels and determine the set \n    of subpixels with valid data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a boolean array representing the pixels in the footprint.", "response": "def footprint(config, nside=None):\n    \"\"\"\n    UNTESTED.\n    Should return a boolean array representing the pixels in the footprint.\n    \"\"\"\n    config = Config(config)\n    if nside is None:\n        nside = config['coords']['nside_pixel']\n    elif nside < config['coords']['nside_catalog']:\n        raise Exception('Requested nside=%i is greater than catalog_nside'%nside)\n    elif nside > config['coords']['nside_pixel']:\n        raise Exception('Requested nside=%i is less than pixel_nside'%nside)\n    pix = np.arange(hp.nside2npix(nside), dtype=int)\n    return inFootprint(config,pix)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a set of coordinates at the centers of pixels of resolutions nside across the full sky.", "response": "def allSkyCoordinates(nside):\n    \"\"\"\n    Generate a set of coordinates at the centers of pixels of resolutions nside across the full sky. \n    \"\"\"\n    lon,lat = pix2ang(nside, np.arange(0, hp.nside2npix(nside)))\n    return lon, lat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating n random positions within a full HEALPix mask.", "response": "def randomPositions(input, nside_pix, n=1):\n    \"\"\"\n    Generate n random positions within a full HEALPix mask of booleans, or a set of (lon, lat) coordinates.\n\n    Parameters:\n    -----------\n    input :     (1) full HEALPix mask of booleans, or (2) a set of (lon, lat) coordinates for catalog objects that define the occupied pixels.\n    nside_pix : nside_pix is meant to be at coarser resolution than the input mask or catalog object positions\n    so that gaps from star holes, bleed trails, cosmic rays, etc. are filled in. \n\n    Returns:\n    --------\n    lon,lat,area : Return the longitude and latitude of the random positions (deg) and the total area (deg^2).\n\n    \"\"\"\n    input = np.array(input)\n    if len(input.shape) == 1:\n        if hp.npix2nside(len(input)) < nside_pix:\n            logger.warning('Expected coarser resolution nside_pix in skymap.randomPositions')\n        subpix = np.nonzero(input)[0] # All the valid pixels in the mask at the NSIDE for the input mask\n        lon, lat = pix2ang(hp.npix2nside(len(input)), subpix)\n    elif len(input.shape) == 2:\n        lon, lat = input[0], input[1] # All catalog object positions\n    else:\n        logger.warning('Unexpected input dimensions for skymap.randomPositions')\n    pix = surveyPixel(lon, lat, nside_pix)\n\n    # Area with which the random points are thrown\n    area = len(pix) * hp.nside2pixarea(nside_pix, degrees=True)\n\n    # Create mask at the coarser resolution\n    mask = np.tile(False, hp.nside2npix(nside_pix))\n    mask[pix] = True\n\n    # Estimate the number of points that need to be thrown based off\n    # coverage fraction of the HEALPix mask\n    coverage_fraction = float(np.sum(mask)) / len(mask) \n    n_throw = int(n / coverage_fraction)\n        \n    lon, lat = [], []\n    count = 0\n    while len(lon) < n:\n        lon_throw = np.random.uniform(0., 360., n_throw)\n        lat_throw = np.degrees(np.arcsin(np.random.uniform(-1., 1., n_throw)))\n\n        pix_throw = ugali.utils.healpix.angToPix(nside_pix, lon_throw, lat_throw)\n        cut = mask[pix_throw].astype(bool)\n\n        lon = np.append(lon, lon_throw[cut])\n        lat = np.append(lat, lat_throw[cut])\n\n        count += 1\n        if count > 10:\n            raise RuntimeError('Too many loops...')\n\n    return lon[0:n], lat[0:n], area"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef randomPositionsMask(mask, nside_pix, n):\n    \n    npix = len(mask)\n    nside = hp.npix2nside(npix)\n\n    # Estimate the number of points that need to be thrown based off\n    # coverage fraction of the HEALPix mask\n    coverage_fraction = float(np.sum(mask)) / len(mask) \n    n_throw = int(n / coverage_fraction)\n        \n    lon, lat = [], []\n    latch = True\n    count = 0\n    while len(lon) < n:\n        lon_throw = np.random.uniform(0., 360., n_throw)\n        lat_throw = np.degrees(np.arcsin(np.random.uniform(-1., 1., n_throw)))\n\n        pix = ugali.utils.healpix.angToPix(nside, lon_throw, lat_throw)\n        cut = mask[pix].astype(bool)\n\n        lon = np.append(lon, lon_throw[cut])\n        lat = np.append(lat, lat_throw[cut])\n\n        count += 1\n        if count > 10:\n            raise RuntimeError('Too many loops...')\n\n    return lon[0:n], lat[0:n]", "response": "Generate n random positions within a HEALPix mask."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nembeds a logical Ising model onto another graph via an embedding.", "response": "def embed_ising(source_linear, source_quadratic, embedding, target_adjacency, chain_strength=1.0):\n    \"\"\"Embeds a logical Ising model onto another graph via an embedding.\n\n    Args:\n        source_linear (dict): The linear biases to be embedded. Should be a dict of\n            the form {v: bias, ...} where v is a variable in the source model\n            and bias is the linear bias associated with v.\n        source_quadratic (dict): The quadratic biases to be embedded. Should be a dict\n            of the form {(u, v): bias, ...} where u, v are variables in the\n            source model and bias is the quadratic bias associated with (u, v).\n        embedding (dict): The mapping from the source graph to the target graph.\n            Should be of the form {v: {s, ...}, ...} where v is a variable in the\n            source model and s is a variable in the target model.\n        target_adjacency (dict/:class:`networkx.Graph`): The adjacency dict of the target\n            graph. Should be a dict of the form {s: Ns, ...} where s is a variable\n            in the target graph and Ns is the set of neighbours of s.\n        chain_strength (float, optional): The quadratic bias that should be used\n            to create chains.\n\n    Returns:\n        (dict, dict, dict): A 3-tuple containing:\n\n            dict: The linear biases of the target problem. In the form {s: bias, ...}\n            where s is a node in the target graph and bias is the associated linear bias.\n\n            dict: The quadratic biases of the target problem. A dict of the form\n            {(s, t): bias, ...} where (s, t) is an edge in the target graph and bias is\n            the associated quadratic bias.\n\n            dict: The quadratic biases that induce the variables in the target problem to\n            act as one. A dict of the form {(s, t): -chain_strength, ...} which\n            is the quadratic biases associated with the chains.\n\n    Examples:\n        >>> source_linear = {'a': 1, 'b': 1}\n        >>> source_quadratic = {('a', 'b'): -1}\n        >>> embedding = {'a': [0, 1], 'b': [2]}\n        >>> target_adjacency = {0: {1, 2}, 1: {0, 2}, 2: {0, 1}}\n        >>> target_linear, target_quadratic, chain_quadratic = embed_ising(\n        ...     source_linear, source_quadratic, embedding, target_adjacency)\n        >>> target_linear\n        {0: 0.5, 1: 0.5, 2: 1.0}\n        >>> target_quadratic\n        {(0, 2): -0.5, (1, 2): -0.5}\n        >>> chain_quadratic\n        {(0, 1): -1.0}\n\n    \"\"\"\n\n    # store variables in the target graph that the embedding hasn't used\n    unused = {v for v in target_adjacency} - set().union(*embedding.values())\n\n    # ok, let's begin with the linear biases.\n    # we spread the value of h evenly over the chain\n    target_linear = {v: 0. for v in target_adjacency}\n    for v, bias in iteritems(source_linear):\n        try:\n            chain_variables = embedding[v]\n        except KeyError:\n            # if our embedding doesn't deal with this variable, assume it's an isolated vertex and embed it to one of\n            # the unused variables. if this turns out to not be an isolated vertex, it will be caught below when\n            # handling quadratic biases\n            try:\n                embedding[v] = {unused.pop()}\n            except KeyError:\n                raise ValueError('no embedding provided for source variable {}'.format(v))\n            chain_variables = embedding[v]\n\n        b = bias / len(chain_variables)\n\n        for s in chain_variables:\n            try:\n                target_linear[s] += b\n            except KeyError:\n                raise ValueError('chain variable {} not in target_adjacency'.format(s))\n\n    # next up the quadratic biases.\n    # We spread the quadratic biases evenly over the edges\n    target_quadratic = {}\n    for (u, v), bias in iteritems(source_quadratic):\n        edges = set()\n\n        if u not in embedding:\n            raise ValueError('no embedding provided for source variable {}'.format(u))\n        if v not in embedding:\n            raise ValueError('no embedding provided for source variable {}'.format(v))\n\n        for s in embedding[u]:\n            for t in embedding[v]:\n                try:\n                    if s in target_adjacency[t] and (t, s) not in edges:\n                        edges.add((s, t))\n                except KeyError:\n                    raise ValueError('chain variable {} not in target_adjacency'.format(s))\n\n        if not edges:\n            raise ValueError(\"no edges in target graph between source variables {}, {}\".format(u, v))\n\n        b = bias / len(edges)\n\n        # in some cases the logical J can have (u, v) and (v, u) as inputs, so make\n        # sure we are not doubling them up with our choice of ordering\n        for s, t in edges:\n            if (s, t) in target_quadratic:\n                target_quadratic[(s, t)] += b\n            elif (t, s) in target_quadratic:\n                target_quadratic[(t, s)] += b\n            else:\n                target_quadratic[(s, t)] = b\n\n    # finally we need to connect the nodes in the chains\n    chain_quadratic = {}\n    for chain in itervalues(embedding):\n        chain_quadratic.update(chain_to_quadratic(chain, target_adjacency, chain_strength))\n\n    return target_linear, target_quadratic, chain_quadratic"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines the frequency of chain breaks in the given samples.", "response": "def chain_break_frequency(samples, embedding):\n    \"\"\"Determines the frequency of chain breaks in the given samples.\n\n    Args:\n        samples (iterable): An iterable of samples where each sample\n            is a dict of the form {v: val, ...} where v is a variable\n            in the target graph and val is the associated value as\n            determined by a binary quadratic model sampler.\n        embedding (dict): The mapping from the source graph to the target graph.\n            Should be of the form {v: {s, ...}, ...} where v is a variable in the\n            source model and s is a variable in the target model.\n\n    Returns:\n        dict: The frequency of chain breaks in the form {v: f, ...} where v\n        is a variable in the source graph and frequency is the fraction\n        of chains that were broken as a float.\n\n    \"\"\"\n    counts = {v: 0 for v in embedding}\n    total = 0\n    for sample in samples:\n        for v, chain in iteritems(embedding):\n            vals = [sample[u] for u in chain]\n\n            if not _all_equal(vals):\n                counts[v] += 1\n        total += 1\n\n    return {v: counts[v] / total for v in embedding}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn samples over the variables in the source graph that are not in the target graph.", "response": "def unembed_samples(samples, embedding, chain_break_method=None):\n    \"\"\"Return samples over the variables in the source graph.\n\n    Args:\n        samples (iterable): An iterable of samples where each sample\n            is a dict of the form {v: val, ...} where v is a variable\n            in the target model and val is the associated value as\n            determined by a binary quadratic model sampler.\n        embedding (dict): The mapping from the source graph to the target graph.\n            Should be of the form {v: {s, ...}, ...} where v is a node in the\n            source graph and s is a node in the target graph.\n        chain_break_method (function, optional): The method used to resolve chain\n            breaks. Default is :method:`majority_vote`.\n\n    Returns:\n        list: A list of unembedded samples. Each sample is a dict of the form\n        {v: val, ...} where v is a variable in the source graph and val\n        is the value associated with the variable.\n\n    \"\"\"\n    if chain_break_method is None:\n        chain_break_method = majority_vote\n    return list(itertools.chain(*(chain_break_method(sample, embedding) for sample in samples)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndiscards the sample if broken.", "response": "def discard(sample, embedding):\n    \"\"\"Discards the sample if broken.\n\n    Args:\n        sample (dict): A sample of the form {v: val, ...} where v is\n            a variable in the target graph and val is the associated value as\n            determined by a binary quadratic model sampler.\n        embedding (dict): The mapping from the source graph to the target graph.\n            Should be of the form {v: {s, ...}, ...} where v is a node in the\n            source graph and s is a node in the target graph.\n\n    Yields:\n        dict: The unembedded sample is no chains were broken.\n\n    \"\"\"\n    unembeded = {}\n\n    for v, chain in iteritems(embedding):\n        vals = [sample[u] for u in chain]\n\n        if _all_equal(vals):\n            unembeded[v] = vals.pop()\n        else:\n            return\n\n    yield unembeded"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef majority_vote(sample, embedding):\n    unembeded = {}\n\n    for v, chain in iteritems(embedding):\n        vals = [sample[u] for u in chain]\n\n        if _all_equal(vals):\n            unembeded[v] = vals.pop()\n        else:\n            unembeded[v] = _most_common(vals)\n\n    yield unembeded", "response": "Determines the sample values by majority vote."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the sample values by weighed random choice.", "response": "def weighted_random(sample, embedding):\n    \"\"\"Determines the sample values by weighed random choice.\n\n    Args:\n        sample (dict): A sample of the form {v: val, ...} where v is\n            a variable in the target graph and val is the associated value as\n            determined by a binary quadratic model sampler.\n        embedding (dict): The mapping from the source graph to the target graph.\n            Should be of the form {v: {s, ...}, ...} where v is a node in the\n            source graph and s is a node in the target graph.\n\n    Yields:\n        dict: The unembedded sample. When there is a chain break, the value\n        is chosen randomly, weighted by the frequency of the values\n        within the chain.\n\n    \"\"\"\n    unembeded = {}\n\n    for v, chain in iteritems(embedding):\n        vals = [sample[u] for u in chain]\n\n        # pick a random element uniformly from all vals, this weights them by\n        # the proportion of each\n        unembeded[v] = random.choice(vals)\n\n    yield unembeded"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntrues if all values in iterable are equal.", "response": "def _all_equal(iterable):\n    \"\"\"True if all values in `iterable` are equal, else False.\"\"\"\n    iterator = iter(iterable)\n    first = next(iterator)\n    return all(first == rest for rest in iterator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _most_common(iterable):\n    data = Counter(iterable)\n    return max(data, key=data.__getitem__)", "response": "Returns the most common element in iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef createMCMC(config,srcfile,section='source',samples=None):\n\n    source = ugali.analysis.source.Source()\n    source.load(srcfile,section=section)\n    loglike = ugali.analysis.loglike.createLoglike(config,source)\n\n    mcmc = MCMC(config,loglike)\n    if samples is not None:\n        mcmc.load_samples(samples)\n\n    return mcmc", "response": "Create an MCMC instance from a source file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lnprior(self,theta):\n        params,priors = self.params,self.priors\n        kwargs = dict(list(zip(params,theta)))\n        err = np.seterr(invalid='raise')\n        try:\n            lnprior = np.sum(np.log([priors[k](v) for k,v in list(kwargs.items())]))\n        except (FloatingPointError,ValueError):\n            lnprior = -np.inf\n        np.seterr(**err)\n        return lnprior", "response": "Logarithm of the prior"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lnprob(self,theta):\n        global niter\n        params,priors,loglike = self.params,self.priors,self.loglike\n        # Avoid extra likelihood calls with bad priors\n        _lnprior = self.lnprior(theta)\n        if np.isfinite(_lnprior):\n            _lnlike = self.lnlike(theta)\n        else:\n            _lnprior = -np.inf\n            _lnlike = -np.inf\n\n        _lnprob = _lnprior + _lnlike\n     \n        if (niter%100==0):\n            msg = \"%i function calls ...\\n\"%niter\n            msg+= ', '.join('%s: %.3f'%(k,v) for k,v in zip(params,theta))\n            msg+= '\\nlog(like): %.3f, log(prior): %.3f'%(_lnprior,_lnlike)\n            logger.debug(msg)\n        niter+=1\n        return _lnprob", "response": "Logarithm of the probability of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the membership of a log - like model to a file.", "response": "def write_membership(filename,config,srcfile,section=None):\n    \"\"\"\n    Top level interface to write the membership from a config and source model.\n    \"\"\"\n    source = Source()\n    source.load(srcfile,section=section)\n    loglike = createLoglike(config,source)\n    loglike.write_membership(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createCatalog(config,roi=None,lon=None,lat=None):\n    import ugali.observation.catalog\n    if roi is None: roi = createROI(config,lon,lat)\n    catalog = ugali.observation.catalog.Catalog(config,roi=roi)\n    return catalog", "response": "Create a catalog object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsimulating a catalog object.", "response": "def simulateCatalog(config,roi=None,lon=None,lat=None):\n    \"\"\"\n    Simulate a catalog object.\n    \"\"\"\n    import ugali.simulation.simulator\n    if roi is None: roi = createROI(config,lon,lat)\n    sim = ugali.simulation.simulator.Simulator(config,roi)\n    return sim.catalog()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_observable_fraction(self,distance_modulus):\n        # This is the observable fraction after magnitude cuts in each \n        # pixel of the ROI.\n        observable_fraction = self.isochrone.observableFraction(self.mask,distance_modulus)\n        if not observable_fraction.sum() > 0:\n            msg = \"No observable fraction\"\n            msg += (\"\\n\"+str(self.source.params))\n            logger.error(msg)\n            raise ValueError(msg)\n        return observable_fraction", "response": "Calculates the observable fraction within each pixel of the source region."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating signal color probability for each catalog object on the fly.", "response": "def calc_signal_color1(self, distance_modulus, mass_steps=10000):\n        \"\"\"\n        Compute signal color probability (u_color) for each catalog object on the fly.\n        \"\"\"\n        mag_1, mag_2 = self.catalog.mag_1,self.catalog.mag_2\n        mag_err_1, mag_err_2 = self.catalog.mag_err_1,self.catalog.mag_err_2\n        u_density = self.isochrone.pdf(mag_1,mag_2,mag_err_1,mag_err_2,distance_modulus,self.delta_mag,mass_steps)\n\n        #u_color = u_density * self.delta_mag**2\n        u_color = u_density\n\n        return u_color"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_signal_color2(self, distance_modulus, mass_steps=1000):\n        logger.info('Calculating signal color from MMD')\n\n        mag_1, mag_2 = self.catalog.mag_1,self.catalog.mag_2\n        lon, lat = self.catalog.lon,self.catalog.lat\n        u_density = self.isochrone.pdf_mmd(lon,lat,mag_1,mag_2,distance_modulus,self.mask,self.delta_mag,mass_steps)\n\n        #u_color = u_density * self.delta_mag**2\n        u_color = u_density\n\n        # ADW: Should calculate observable fraction here as well...\n\n        return u_color", "response": "Calculates signal color probability for each catalog object on the fly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the surface intensity for each pixel in the interior of the ROI.", "response": "def calc_surface_intensity(self, factor=10):\n        \"\"\"Calculate the surface intensity for each pixel in the interior\n        region of the ROI. Pixels are adaptively subsampled around the\n        kernel centroid out to a radius of 'factor * max_pixrad'.\n\n        Parameters:\n        -----------\n        factor : the radius of the oversample region in units of max_pixrad\n\n        Returns:\n        --------\n        surface_intensity : the surface intensity at each pixel\n        \"\"\"\n        # First we calculate the surface intensity at native resolution\n        pixels = self.roi.pixels_interior\n        nside_in = self.config['coords']['nside_pixel']\n        surface_intensity = self.kernel.pdf(pixels.lon,pixels.lat)\n\n        # Then we recalculate the surface intensity around the kernel\n        # centroid at higher resolution\n        for i in np.arange(1,5):\n            # Select pixels within the region of interest\n            nside_out = 2**i * nside_in\n            radius = factor*np.degrees(hp.max_pixrad(nside_out))\n            pix = ang2disc(nside_in,self.kernel.lon,self.kernel.lat,\n                           radius,inclusive=True)\n\n            # Select pix within the interior region of the ROI\n            idx = ugali.utils.healpix.index_pix_in_pixels(pix,pixels)\n            pix = pix[(idx >= 0)]; idx = idx[(idx >= 0)]\n\n            # Reset the surface intensity for the subsampled pixels\n            subpix = ugali.utils.healpix.ud_grade_ipix(pix,nside_in,nside_out)\n            pix_lon,pix_lat = pix2ang(nside_out,subpix)\n            surface_intensity[idx]=np.mean(self.kernel.pdf(pix_lon,pix_lat),axis=1)\n\n        return surface_intensity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_signal_spatial(self):\n        # Calculate the surface intensity\n        self.surface_intensity_sparse = self.calc_surface_intensity()\n\n        # Calculate the probability per object-by-object level\n        self.surface_intensity_object = self.kernel.pdf(self.catalog.lon,\n                                                        self.catalog.lat)\n        \n        # Spatial component of signal probability\n        u_spatial = self.surface_intensity_object\n        return u_spatial", "response": "Calculates the spatial signal probability for each object - by - object level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit_richness(self, atol=1.e-3, maxiter=50):\n        # Check whether the signal probability for all objects are zero\n        # This can occur for finite kernels on the edge of the survey footprint\n        if np.isnan(self.u).any():\n            logger.warning(\"NaN signal probability found\")\n            return 0., 0., None\n        \n        if not np.any(self.u):\n            logger.warning(\"Signal probability is zero for all objects\")\n            return 0., 0., None\n\n        if self.f == 0:\n            logger.warning(\"Observable fraction is zero\")\n            return 0., 0., None\n\n        # Richness corresponding to 0, 1, and 10 observable stars\n        richness = np.array([0., 1./self.f, 10./self.f])\n        loglike = np.array([self.value(richness=r) for r in richness])\n\n        found_maximum = False\n        iteration = 0\n        while not found_maximum:\n            parabola = ugali.utils.parabola.Parabola(richness, 2.*loglike)\n            if parabola.vertex_x < 0.:\n                found_maximum = True\n            else:\n                richness = np.append(richness, parabola.vertex_x)\n                loglike  = np.append(loglike, self.value(richness=richness[-1]))\n\n                if np.fabs(loglike[-1] - np.max(loglike[0: -1])) < atol:\n                    found_maximum = True\n            iteration+=1\n            if iteration > maxiter:\n                logger.warning(\"Maximum number of iterations reached\")\n                break\n            \n        index = np.argmax(loglike)\n        return loglike[index], richness[index], parabola", "response": "Fits the log - likelihood of the current object to a function of richness."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_membership(self,filename):\n        # Column names\n        name_objid = self.config['catalog']['objid_field']\n        name_mag_1 = self.config['catalog']['mag_1_field']\n        name_mag_2 = self.config['catalog']['mag_2_field']\n        name_mag_err_1 = self.config['catalog']['mag_err_1_field']\n        name_mag_err_2 = self.config['catalog']['mag_err_2_field']\n\n        # Coordinate conversion\n        #ra,dec = gal2cel(self.catalog.lon,self.catalog.lat)\n        glon,glat = self.catalog.glon_glat\n        ra,dec    = self.catalog.ra_dec\n\n        # Angular and isochrone separations\n        sep = angsep(self.source.lon,self.source.lat,\n                     self.catalog.lon,self.catalog.lat)\n        isosep = self.isochrone.separation(self.catalog.mag_1,self.catalog.mag_2)\n\n        # If size becomes an issue we can make everything float32\n        data = odict()\n        data[name_objid]     = self.catalog.objid\n        data['GLON']         = glon\n        data['GLAT']         = glat\n        data['RA']           = ra\n        data['DEC']          = dec\n        data[name_mag_1]     = self.catalog.mag_1\n        data[name_mag_err_1] = self.catalog.mag_err_1\n        data[name_mag_2]     = self.catalog.mag_2\n        data[name_mag_err_2] = self.catalog.mag_err_2\n        data['COLOR']        = self.catalog.color\n        data['ANGSEP']       = sep.astype(np.float32)\n        data['ISOSEP']       = isosep.astype(np.float32)\n        data['PROB']         = self.p.astype(np.float32)\n     \n        # HIERARCH allows header keywords longer than 8 characters\n        header = []\n        for param,value in self.source.params.items():\n            card = dict(name='HIERARCH %s'%param.upper(),\n                        value=value.value,\n                        comment=param)\n            header.append(card)\n        card = dict(name='HIERARCH %s'%'TS',value=self.ts(),\n                    comment='test statistic')\n        header.append(card)\n        card = dict(name='HIERARCH %s'%'TIMESTAMP',value=time.asctime(),\n                    comment='creation time')\n        header.append(card)\n        fitsio.write(filename,data,header=header,clobber=True)", "response": "Writes a catalog file of the likelihood region including the membership properties."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve_reference(self, ref):\n        url, resolved = self.resolver.resolve(ref)\n        return resolved", "response": "Resolve a JSON Pointer object reference to the object itself."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_path(self, path):\n        mapping = self.get_path_mapping(path)\n        return self.path_class(api=self, path=path, mapping=mapping)", "response": "Construct a Path object from a path string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_file(cls, filename):\n        with open(filename) as infp:\n            if filename.endswith('.yaml') or filename.endswith('.yml'):\n                import yaml\n                data = yaml.safe_load(infp)\n            else:\n                import json\n                data = json.load(infp)\n        return cls.from_data(data)", "response": "Construct an APIDefinition by parsing the given filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maybe_resolve(object, resolve):\n    if isinstance(object, dict) and object.get('$ref'):\n        return resolve(object['$ref'])\n    return object", "response": "Resolve the object s ID if it has one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, configManager, config):\n\n        configFile = self._getConfigFile(config)\n\n        if not configFile:\n            return dict()\n\n        yamlConfigs = yaml.load(configFile)\n        if isinstance(yamlConfigs, dict):\n            return yamlConfigs\n\n        raise self.subparserException(\"YAML config parsed did not result in a dictionary, but instead a: %s\"\n                                      % type(yamlConfigs))", "response": "Parse configuration options out of a YAML configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _server_error_message(url, message):\n    msg = _error_message.format(url=url, message=message)\n    log.error(msg)\n    return msg", "response": "Log and return a server error message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a request to the specified url with the provided data.", "response": "def make_request(\n        url, method='GET', query=None, body=None, auth=None, timeout=10,\n        client=None, macaroons=None):\n    \"\"\"Make a request with the provided data.\n\n    @param url The url to make the request to.\n    @param method The HTTP request method (defaulting to \"GET\").\n    @param query A dict of the query key and values.\n    @param body The optional body as a string or as a JSON decoded dict.\n    @param auth The optional username and password as a tuple,\n    not used if client is not None\n    @param timeout The request timeout in seconds, defaulting to 10 seconds.\n    @param client (httpbakery.Client) holds a context for making http\n    requests with macaroons.\n    @param macaroons Optional JSON serialized, base64 encoded macaroons to be\n        included in the request header.\n\n    POST/PUT request bodies are assumed to be in JSON format.\n    Return the response content as a JSON decoded object, or an empty dict.\n    Raise a ServerError if a problem occurs in the request/response process.\n    Raise a ValueError if invalid parameters are provided.\n    \"\"\"\n    headers = {}\n    kwargs = {'timeout': timeout, 'headers': headers}\n    # Handle the request body.\n    if body is not None:\n        if isinstance(body, collections.Mapping):\n            body = json.dumps(body)\n        kwargs['data'] = body\n    # Handle request methods.\n    if method in ('GET', 'HEAD'):\n        if query:\n            url = '{}?{}'.format(url, urlencode(query, True))\n    elif method in ('DELETE', 'PATCH', 'POST', 'PUT'):\n        headers['Content-Type'] = 'application/json'\n    else:\n        raise ValueError('invalid method {}'.format(method))\n    if macaroons is not None:\n        headers['Macaroons'] = macaroons\n\n    kwargs['auth'] = auth if client is None else client.auth()\n\n    api_method = getattr(requests, method.lower())\n    # Perform the request.\n    try:\n        response = api_method(url, **kwargs)\n    except requests.exceptions.Timeout:\n        raise timeout_error(url, timeout)\n    except Exception as err:\n        msg = _server_error_message(url, err)\n        raise ServerError(msg)\n    # Handle error responses.\n    try:\n        response.raise_for_status()\n    except HTTPError as err:\n        msg = _server_error_message(url, err.response.text)\n        raise ServerError(err.response.status_code, msg)\n    except requests.exceptions.RequestException as err:\n        msg = _server_error_message(url, err.message)\n        raise ServerError(msg)\n    # Some requests just result in a status with no response body.\n    if not response.content:\n        return {}\n    # Assume the response body is a JSON encoded string.\n    try:\n        return response.json()\n    except Exception as err:\n        msg = 'Error decoding JSON response: {} message: {}'.format(url, err)\n        log.error(msg)\n        raise ServerError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_plans(self, reference):\n        response = make_request(\n            '{}charm?charm-url={}'.format(self.url,\n                                          'cs:' + reference.path()),\n            timeout=self.timeout, client=self._client)\n        try:\n            return tuple(map(lambda plan: Plan(\n                url=plan['url'], plan=plan['plan'],\n                created_on=datetime.datetime.strptime(\n                    plan['created-on'],\n                    \"%Y-%m-%dT%H:%M:%SZ\"\n                ),\n                description=plan.get('description'),\n                price=plan.get('price')), response))\n        except Exception as err:\n            log.error(\n                'cannot process plans: invalid JSON response: {!r}'.format(\n                    response))\n            raise ServerError(\n                'unable to get list of plans for {}: {}'.format(\n                    reference.path(), err))", "response": "Get the plans for a given charm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the list of wallets.", "response": "def list_wallets(self):\n        \"\"\"Get the list of wallets.\n\n        @return an dict containing a list of wallets, a total, and available\n            credit.\n        @raise ServerError\n        \"\"\"\n        response = make_request(\n            '{}wallet'.format(self.url),\n            timeout=self.timeout,\n            client=self._client)\n        try:\n            total = response['total']\n            return {\n                'credit': response['credit'],\n                'total': WalletTotal(\n                    limit=total['limit'],\n                    budgeted=total['budgeted'],\n                    available=total['available'],\n                    unallocated=total['unallocated'],\n                    usage=total['usage'],\n                    consumed=total['consumed']),\n                'wallets': tuple(Wallet(\n                    owner=wallet['owner'],\n                    wallet=wallet['wallet'],\n                    limit=wallet['limit'],\n                    budgeted=wallet['budgeted'],\n                    unallocated=wallet['unallocated'],\n                    available=wallet['available'],\n                    consumed=wallet['consumed'],\n                    default='default' in wallet)\n                    for wallet in response['wallets']),\n            }\n        except Exception as err:\n            log.error(\n                'cannot process wallets: invalid JSON response: {!r}'.format(\n                    response))\n            raise ServerError(\n                'unable to get list of wallets: {!r}'.format(err))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_wallet(self, wallet_name):\n        response = make_request(\n            '{}wallet/{}'.format(self.url, wallet_name),\n            timeout=self.timeout,\n            client=self._client)\n        try:\n            total = response['total']\n            return {\n                'credit': response['credit'],\n                'limit': response['limit'],\n                'total': WalletTotal(\n                    limit=total['limit'],\n                    budgeted=total['budgeted'],\n                    available=total['available'],\n                    unallocated=total['unallocated'],\n                    usage=total['usage'],\n                    consumed=total['consumed'])\n            }\n        except Exception as exc:\n            log.error(\n                'cannot get wallet from server: {!r}'.format(exc))\n            raise ServerError(\n                'unable to get list of wallets: {!r}'.format(exc))", "response": "Get a single wallet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a wallet with a new limit.", "response": "def update_wallet(self, wallet_name, limit):\n        \"\"\"Update a wallet with a new limit.\n\n        @param the name of the wallet.\n        @param the new value of the limit.\n        @return a success string from the plans server.\n        @raise ServerError via make_request.\n        \"\"\"\n        request = {\n            'update': {\n                'limit': str(limit),\n            }\n        }\n        return make_request(\n            '{}wallet/{}'.format(self.url, wallet_name),\n            method='PATCH',\n            body=request,\n            timeout=self.timeout,\n            client=self._client)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a wallet. @param the name of the wallet. @return a success string from the plans server. @raise ServerError via make_request.", "response": "def delete_wallet(self, wallet_name):\n        \"\"\"Delete a wallet.\n\n        @param the name of the wallet.\n        @return a success string from the plans server.\n        @raise ServerError via make_request.\n        \"\"\"\n        return make_request(\n            '{}wallet/{}'.format(self.url, wallet_name),\n            method='DELETE',\n            timeout=self.timeout,\n            client=self._client)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_budget(self, wallet_name, model_uuid, limit):\n        request = {\n            'model': model_uuid,\n            'limit': limit,\n        }\n        return make_request(\n            '{}wallet/{}/budget'.format(self.url, wallet_name),\n            method='POST',\n            body=request,\n            timeout=self.timeout,\n            client=self._client)", "response": "Create a new budget for a model and wallet."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a budget. @param the name of the wallet. @param the model UUID. @return a success string from the plans server. @raise ServerError via make_request.", "response": "def delete_budget(self, model_uuid):\n        \"\"\"Delete a budget.\n\n        @param the name of the wallet.\n        @param the model UUID.\n        @return a success string from the plans server.\n        @raise ServerError via make_request.\n        \"\"\"\n        return make_request(\n            '{}model/{}/budget'.format(self.url, model_uuid),\n            method='DELETE',\n            timeout=self.timeout,\n            client=self._client)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef safe_div(a, b, inf=INF):\n    with np.errstate(divide='raise', invalid='raise'):\n        try:\n            return 1. * a / b\n        except (ZeroDivisionError, FloatingPointError):\n            return inf\n        except TypeError:\n            try:\n                1. / b\n            except TypeError:\n                return b\n            return a\n        except RuntimeWarning:\n            try:\n                1. / b\n            except RuntimeWarning:\n                return b\n            return a\n        return 1. * a / b", "response": "Safely divide a and b into a single object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates Chi from True Positive TN and False Negative TN and FN.", "response": "def tptnfpfn_chi(*args, **kwargs):\n    \"\"\"Calculate Chi from True Positive (tp), True Negative (tn), False Positive/Negative counts.\n    Assumes that the random variable being measured is continuous rather than discrete.\n    Reference:\n      https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\n    >>> round(tptnfpfn_chi(1000, 2000, 30, 40))\n    2765.0\n    \"\"\"\n    tp, tn, fp, fn = args_tptnfpfn(*args, **kwargs)\n    return tptnfpfn_mcc(tp=tp, tn=tn, fp=fp, fn=fn) ** 2. * (tp + tn + fp + fn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef args_tptnfpfn(*args, **kwargs):\n    if len(args) == 4:\n        tp, tn, fp, fn = args\n    elif len(kwargs) == 0:\n        if len(args) == 1:\n            args = listify(args[0])\n        tp, tn, fp, fn = list(list(args) + [0] * (4 - len(args)))\n    else:\n        args = list(args)\n        tp = kwargs['tp'] if 'tp' in kwargs else args.pop(0) if len(args) else 0\n        tn = kwargs['tn'] if 'tn' in kwargs else args.pop(0) if len(args) else 0\n        fp = kwargs['fp'] if 'fp' in kwargs else args.pop(0) if len(args) else 0\n        fn = kwargs['fn'] if 'fn' in kwargs else args.pop(0) if len(args) else 0\n    return tp, tn, fp, fn", "response": "Convert kwargs for tp tn fp fn to ordered tuple of args\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the TPTN FPN and FPN samples within a confusions matrx.", "response": "def dataframe_tptnfpfn(df, pos_label=True, labels=None):\n    \"\"\"Count the True Pos, True Neg, False Pos, False Neg samples within a confusions matrx (potentiall larger than 2x2)\n    >>> matrix = [[5, 3, 0], [2, 3, 1], [0, 2, 11]]\n    >>> columns=['Cat', 'Dog', 'Rabbit']\n    >>> x = np.array([[tc, pc] for (tc, row) in enumerate(matrix) for (pc, n) in enumerate(row) for i in range(n)])\n    >>> c = Confusion([(columns[i], columns[j]) for (i, j) in x], columns=['Actual', 'Predicted'])\n    >>> c\n    Predicted  Cat  Dog  Rabbit\n    Actual\n    Cat          5    3       0\n    Dog          2    3       1\n    Rabbit       0    2      11\n    >>> dataframe_tptnfpfn(c, 'Rabbit')\n    (11, 13, 1, 2)\n    >>> dataframe_tptnfpfn(c.T, 'Rabbit')\n    (11, 13, 2, 1)\n    >>> c.mcc[2]\n    0.77901...\n    \"\"\"\n    labels = df.columns if labels is None else labels\n    neg_labels = [label for label in labels if label != pos_label]\n    tp = df[pos_label][pos_label]\n    tn = sum(df[pred_label][true_label] for true_label in neg_labels for pred_label in neg_labels)\n    fp = df[pos_label][neg_labels].sum()\n    fn = sum(df[label][pos_label] for label in neg_labels)\n    return tp, tn, fp, fn"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_pos_label(neg_label=None):\n    # A class label should be a None, bool, int, or str\n    if neg_label is None:\n        return True\n    typ = type(neg_label)\n    # If class label isn't a bool or None then make it an int or str\n    try:\n        neg_label = int(float(neg_label))\n        if neg_label in (0, 1):\n            return typ(int(not neg_label))\n        if neg_label < 0:\n            return typ(-neg_label)\n        return typ(neg_label + 1)\n    except (ValueError, TypeError):\n        neg_label = stringify(neg_label).strip()\n    for xform, label_dict in zip((lambda x: x, lambda x: x, str.lower, str.lower,),\n                                 (POS_LABELS, POS_LABELS_INVERSE, POS_LABELS_LOWER, POS_LABELS_LOWER_INVERSE)):\n        try:\n            return typ(label_dict[xform(neg_label)])\n        except KeyError:\n            pass\n    # neg_label = neg.lower()\n    # for labels in (POS_LABELS_LOWER, POS_LABELS_LOWER_INVERSE):\n    #     try:\n    #         return typ(labels[neg_label])\n    #     except KeyError:\n    #         pass\n    # if not neg_label:\n    #     return True\n    # neg = neg[0]\n    # try:\n    #     return POS_LABELS_LOWER_FIRST[neg]\n    # except KeyError:\n    #     pass\n    return 'P'", "response": "Try to infer a positive classification label from a negative label\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef graph_from_cov_df(df, threshold=.5, gain=2., n=None, class_dict=CLASSES):\n    n = n or len(df)\n    nodes = [{'group': class_dict.get(name, 0), \"name\": name} for name in df.index.values][:n]\n    edges = []\n    for i, (row_name, row) in enumerate(df.iterrows()):\n        for j, value in enumerate(row.values):\n            if i > j and value * gain > threshold and i < n and j < n:\n                edges += [{'source': i, 'target': j, 'value': gain * value}]\n    return nodes, edges", "response": "Compose a graph from a DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nproduces a json string describing the graph from a square auto - correlation covariance matrix.", "response": "def json_from_cov_df(df, threshold=.5, gain=2., n=None, indent=1):\n    \"\"\"Produce a json string describing the graph (list of edges) from a square auto-correlation/covariance matrix\n       { \"nodes\": [{\"group\": 1, \"name\": \"the\"},\n                {\"group\": 1, \"name\": \"and\"},\n                {\"group\": 1, \"name\": \"our\"},\n                {\"group\": 2, \"name\": \"that\"},...\n         \"links\": [{\"source\": 0, \"target\": 0, \"value\": 2.637520131294177},\n                   {\"source\": 0, \"target\": 1, \"value\": 1.343999676850537}, ...\n    \"\"\"\n    nodes, edges = graph_from_cov_df(df=df, threshold=threshold, gain=gain, n=n)\n    return json.dumps({'nodes': nodes, 'links': edges}, indent=indent)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spec_from_thresh(thresh, labels, scores, *args, **kwargs):\n    df = pd.DataFrame(list(zip(labels, np.array(scores > thresh).astype(int))))\n    c = Confusion(df, *args, **kwargs)\n    return c._binary_specificity", "response": "r Compute the specifity that a particular threshold on the scores can acheive\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sens_from_spec(spec, labels, scores, *args, **kwargs):\n    thresh = thresh_from_spec(spec, labels, scores)\n    df = pd.DataFrame(list(zip(labels, scores[scores > thresh].astype(int))))\n    c = Confusion(df, *args, **kwargs)\n    return c._binary_sensitivity", "response": "r Find the sensitivity that corresponds to the indicated specificity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cost_fun(x, *args, **kwargs):\n    cost_fun.regularization_weight = kwargs.pop('regularization_weight', cost_fun.regularization_weight)\n    cost_fun.verbose = verbose = kwargs.pop('verbose', cost_fun.verbose)\n    fun = cost_fun.fun = kwargs.get('fun', None) or cost_fun.fun\n    target = cost_fun.target = cost_fun.target if kwargs.get('target', None) is None else kwargs.pop('target')\n    delta = target - fun(x, *args)\n    cost = np.abs(delta) + getattr(cost_fun, 'regularization_weight', 0) * np.abs(x)\n    if verbose:\n        print(\"x, target = {}, {} => delta = {} - {} => cost = abs({}) + abs({}) = {}\".format(\n            x, target, target, fun(x, *args), delta, x, cost))\n    return cost", "response": "Enables minimization for non - zero targets\n    Returns regularized absolute error between target and output of fun."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the cost function topology", "response": "def plot_cost(scores=np.random.rand(100), thresh=0.5, noise=0):\n    \"\"\"Plot the cost function topology (contours for each of several targets)\"\"\"\n    c = pd.DataFrame(index=np.arange(0, 1, 0.01))\n    if isinstance(thresh, (int, float)):\n        thresh = [thresh]\n    elif not isinstance(thresh, (pd.Series, np.ndarray, list, tuple)):\n        thresh = np.arange(0, 1, .2)\n    cost_fun.fun = spec_from_thresh\n    for t in thresh:\n        labels = (scores / t / scores.max() / 1.00001).astype(int)\n        cost_fun.target = t\n        c['target=0.{}'.format(int(t * 10))] = np.array([cost_fun(x, labels, scores, verbose=True) for x in c.index])\n    c.plot()\n    plt.show(block=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fun_inverse(fun=None, y=0, x0=None, args=(), disp=False, method='Nelder-Mead', **kwargs):\n    fun_inverse.fun = cost_fun.fun = fun if fun is not None else getattr(fun_inverse, 'fun', lambda x: x)\n    fun_inverse.target = cost_fun.target = y or 0\n    fun_inverse.verbose = verbose = cost_fun.verbose = kwargs.pop(\n        'verbose', getattr(cost_fun, 'verbose', getattr(fun_inverse, 'verbose', False)))\n    fun_inverse.x0 = x0 = x0 if x0 is not None else getattr(fun_inverse, 'x0', 0) or 0\n\n    if verbose:\n        print('    x0: {}\\ntarget: {}\\n'.format(fun_inverse.x0, fun_inverse.target))\n\n    res = minimize(cost_fun,\n                   x0=x0,\n                   args=args,\n                   options=kwargs.pop('options', {}),\n                   method=method,\n                   **kwargs\n                   )\n    if isinstance(x0, NUMERIC_TYPES):\n        return res.x[0]\n    return res.x", "response": "r Finds the threshold level that accomplishes the desired specificity of a function at a given target y and x0."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dyno_hist(x, window=None, probability=True, edge_weight=1.):\n    x = np.sort(x)\n    if probability:\n        # normalize x first\n        x = x - x[0]\n        x = x / float(x[-1] or 1)\n    window = window or 1\n    window = min(max(window, 1), int(len(x) / 1.5))\n    window += 1\n    # Empirical Densitites (PDF) based on diff of sorted values\n    delta = x[(window - 1):] - x[:(1 - window)]\n    densities = float(window - 1) / (len(delta) + window - 2) / delta\n    h = pd.Series(densities, index=x[window // 2:][:len(delta)])\n    if probability:\n        if h.index[0] > 0:\n            h = pd.Series(edge_weight * densities[0], index=[0]).append(h)\n        if h.index[-1] < 1:\n            h = h.append(pd.Series(edge_weight * densities[-1], index=[1.]))\n    return h", "response": "Returns the probability distribution function from values\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef refresh_meta(self):\n\n        # these calcs are duplicated in __init__()\n        setattr(self, '_num_classes', len(self.index))\n        setattr(self, '_colnums', np.arange(0, self._num_classes))\n        try:\n            setattr(self, '_neg_label',\n                    next(label for label in self.columns if str(label).strip().lower()[0] in ('-nh0')))\n        except StopIteration:\n            setattr(self, '_neg_label', self.columns[-1])\n        try:\n            setattr(self, '_pos_label', next(label for label in self.columns if label != self._neg_label))\n        except StopIteration:\n            setattr(self, '_pos_label', infer_pos_label(self._neg_label))\n\n        # TODO: reorder columns with newly guessed pos and neg class labels first\n\n        # TODO: gather up additional meta calculations here so\n        #       a Confusion matrix can be build from an existing DataFrame that contains confusion counts\n        #       rather than just two columns of labels.\n        self.__setattr__('_hist_labels', self.sum().astype(int))\n        self.__setattr__('_num_total', self._hist_labels.sum())\n        assert(self._num_total == self.sum().sum())\n        setattr(self, '_num_pos_labels', self._hist_labels.get(self._pos_label, 0))\n        # everything that isn't positive is negative\n        setattr(self, '_num_neg_labels', self._num_total - self._num_pos_labels)\n        setattr(self, '_hist_classes', self.T.sum())\n        setattr(self, '_num_pos', self._hist_classes.get(self._pos_label, 0))\n        setattr(self, '_num_neg', self._hist_classes.sum() - self._num_pos)  # everything that isn't positive is negative\n        setattr(self, '_tp', self.get(self._pos_label, pd.Series()).get(self._pos_label, 0))\n        setattr(self, '_tpr', safe_div(float(self._tp), self._num_pos))\n        setattr(self, '_tn', np.diag(self).sum() - self._tp)\n        setattr(self, '_tnr', safe_div(float(self._tn), self._num_neg))\n        setattr(self, '_fp', self.get(self._pos_label, pd.Series()).sum() - self._tp)\n        setattr(self, '_fpr', safe_div(float(self._fp), self._num_neg))\n        setattr(self, '_fn', self._num_neg_labels - self._tn)\n        setattr(self, '_fnr', safe_div(float(self._fn), self._num_pos))\n        setattr(self, '_plr', safe_div(float(self._tpr), self._fpr))\n        setattr(self, '_nlr', safe_div(float(self._fnr), self._tnr))\n        setattr(self, '_binary_accuracy', safe_div(self._tp + self._tn, self._num_samples))\n        setattr(self, '_binary_sensitivity', safe_div(self._tp, self._tp + self._fn))\n        setattr(self, '_binary_specificity', safe_div(self._tn, self._tn + self._fp))", "response": "Refreshes the meta data for the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_existing(cls, confusion, *args, **kwargs):\n        # Extremely brute-force to recreate data from a confusion matrix!\n\n        df = []\n        for t, p in product(confusion.index.values, confusion.columns.values):\n            df += [[t, p]] * confusion[p][t]\n        if confusion.index.name is not None and confusion.columns.name is not None:\n            return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name]))\n        return Confusion(pd.DataFrame(df))", "response": "Creates a new confusion matrix from an existing DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the accuracy of the class", "response": "def get_accuracy(self, scalar=None):\n        \"\"\"Num_True_Positive / Num_Samples\"\"\"\n        # to get a Series instead of a dict:\n        # (np.diag(c).astype(float) / c.T.sum())\n        #     == pd.Series(self.sensitivity)\n        if ((not self._scalar_stats and not scalar and self._num_classes > 2) or\n                ((scalar is False or self._scalar_stats is False) and self._num_classes > 1)):\n            return pd.Series(PrettyDict([(k, safe_div(self[k][k], self._num_total)) for k in self.columns]))\n        return self._binary_accuracy"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sensitivity(self, scalar=None):\n        # to get a Series instead of a dict:\n        # (np.diag(c).astype(float) / c.T.sum())\n        #     == pd.Series(self.sensitivity)\n        if ((not self._scalar_stats and not scalar and self._num_classes > 2) or\n                ((scalar is False or self._scalar_stats is False) and self._num_classes > 1)):\n            return pd.Series(PrettyDict([(k, safe_div(self[k][k], self.loc[k].sum())) for k in self.columns]))\n        return self._binary_sensitivity", "response": "Returns the sensitivity of the class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a pandas Series of specificity of the binary class.", "response": "def get_specificity(self, scalar=None):\n        \"\"\"True_Negative / (True_Negative + False_Positive)\"\"\"\n        if ((not self._scalar_stats and not scalar and self._num_classes > 2) or\n                ((scalar is False or self._scalar_stats is False) and self._num_classes > 1)):\n            spec = PrettyDict()\n            for pos_label in self.columns:\n                neg_labels = [label for label in self.columns if label != pos_label]\n                tn = sum(self[label][label] for label in neg_labels)\n                # fp = self[pos_label][neg_labels].sum()\n                fp = self.loc[neg_labels].sum()[pos_label]\n                assert(self[pos_label][neg_labels].sum() == fp)\n                spec[pos_label] = float(tn) / (tn + fp)\n            return pd.Series(spec)\n        return self._binary_specificity"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the phi coefficient for the class with respect to the class labels.", "response": "def get_phi(self, scalar=None):\n        \"\"\"Phi (\u03c6) Coefficient -- lack of confusion\n        Arguments:\n          scalar (bool or None): Whether to return a scalar Phi coefficient\n          (assume binary classification) rather than a multiclass vector\n        Measure of the lack of confusion in a single value\n        References:\n          [MCC on wikipedia](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n          [docs on R implementation](http://www.personality-project.org/r/html/phi.html)\n        \u03c6 =   (TP*TN - FP*FN) / sqrt((TP+FP) * (TP+FN) * (TN+FP) * (TN+FN))\n        mcc = (tp*tn - fp*fn) / sqrt((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn))\n        \"\"\"\n        # If requested, compute the phi coeffients for all possible 'positive' and 'negative' class labels\n        # (multiclass problem)\n        if ((not self._scalar_stats and not scalar and self._num_classes > 2) or\n                ((scalar is False or self._scalar_stats is False) and self._num_classes > 1)):\n            phi = PrettyDict()\n            # count of predictions labeled with pred_label for a slice of data that was actually labeled true_label:\n            # `count = self[pred_label][true_label]`\n            for pos_label in self.columns:\n                tp, tn, fp, fn = dataframe_tptnfpfn(self, pos_label=pos_label, labels=self.columns)\n                phi[pos_label] = tptnfpfn_mcc(tp=tp, tn=tn, fp=fp, fn=fn)\n            return pd.Series(phi)\n        # A scalar phi value was requested, so compute it for the \"inferred\" positive classification class\n        return tptnfpfn_mcc(self._tp, self._tn, self._fp, self._fn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_chi(self, scalar=None):\n        phi = self.get_phi(scalar=scalar)\n        return mcc_chi(phi, self._num_samples)", "response": "get the Chi_Squared statistic"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the normalized false positive rate.", "response": "def get_false_negative(self, scalar=True):\n        \"\"\"Normalized false positive rate (0 <= fp <= 1)\"\"\"\n        ans = pd.Series(PrettyDict([(k, safe_div(np.sum(self.loc[k]) - self[k][k],\n                                                 np.sum(self.sum() - self.loc[k]))) for k in self.columns]))\n        if (not self._scalar_stats and not scalar) or self._num_classes != 2:\n            return ans\n        return ans[self._pos_label]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading isochrone from the server and return the path to the downloaded file.", "response": "def download(self,age,metallicity,outdir=None,force=False):\n        \"\"\"\n        Check valid parameter range and download isochrones from:\n        http://stev.oapd.inaf.it/cgi-bin/cmd\n        \"\"\"\n        if outdir is None: outdir = './'\n        basename = self.isochrone.params2filename(age,metallicity)\n        outfile = os.path.join(outdir,basename)\n            \n        if os.path.exists(outfile) and not force:\n            try:\n                self.verify(outfile,self.survey,age,metallicity)\n                logger.info(\"Found %s; skipping...\"%(outfile))\n                return\n            except Exception as e:\n                msg = \"Overwriting corrupted %s...\"%(outfile)\n                logger.warn(msg)\n                #os.remove(outfile)\n                \n        mkdir(outdir)\n\n        self.print_info(age,metallicity)\n\n        try:\n            self.query_server(outfile,age,metallicity)\n        except Exception as e:\n            logger.debug(str(e))\n            raise RuntimeError('Bad server response')\n\n        if not os.path.exists(outfile):\n            raise RuntimeError('Download failed')\n\n        try:\n            self.verify(outfile,self.survey,age,metallicity)\n        except Exception as e:\n            msg = \"Output file is corrupted.\"\n            logger.error(msg)\n            #os.remove(outfile)\n            raise(e)\n\n        return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging two dicts of addable values", "response": "def add_dicts(d1, d2):\n    \"\"\" Merge two dicts of addable values \"\"\"\n    if d1 is None:\n        return d2\n    if d2 is None:\n        return d1\n    keys = set(d1)\n    keys.update(set(d2))\n    ret = {}\n    for key in keys:\n        v1 = d1.get(key)\n        v2 = d2.get(key)\n        if v1 is None:\n            ret[key] = v2\n        elif v2 is None:\n            ret[key] = v1\n        else:\n            ret[key] = v1 + v2\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_capacity(self, data):\n        if 'ConsumedCapacity' in data:\n            # This is all for backwards compatibility\n            consumed = data['ConsumedCapacity']\n            if not isinstance(consumed, list):\n                consumed = [consumed]\n            for cap in consumed:\n                self.capacity += cap.get('CapacityUnits', 0)\n                self.table_capacity += cap.get('Table',\n                                               {}).get('CapacityUnits', 0)\n                local_indexes = cap.get('LocalSecondaryIndexes', {})\n                for k, v in six.iteritems(local_indexes):\n                    self.indexes.setdefault(k, 0)\n                    self.indexes[k] += v['CapacityUnits']\n                global_indexes = cap.get('GlobalSecondaryIndexes', {})\n                for k, v in six.iteritems(global_indexes):\n                    self.global_indexes.setdefault(k, 0)\n                    self.global_indexes[k] += v['CapacityUnits']", "response": "Update the consumed capacity metrics"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch(self):\n        self.limit.set_request_args(self.kwargs)\n        data = self.connection.call(*self.args, **self.kwargs)\n        self.limit.post_fetch(data)\n        self.last_evaluated_key = data.get('LastEvaluatedKey')\n        if self.last_evaluated_key is None:\n            self.kwargs.pop('ExclusiveStartKey', None)\n        else:\n            self.kwargs['ExclusiveStartKey'] = self.last_evaluated_key\n        self._update_capacity(data)\n        if 'consumed_capacity' in data:\n            self.consumed_capacity += data['consumed_capacity']\n        for raw_item in data['Items']:\n            item = self.connection.dynamizer.decode_keys(raw_item)\n            if self.limit.accept(item):\n                yield item", "response": "Fetch more results from Dynamo"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_kwargs(self):\n        keys, self.keys = self.keys[:MAX_GET_BATCH], self.keys[MAX_GET_BATCH:]\n        query = {'ConsistentRead': self.consistent}\n        if self.attributes is not None:\n            query['ProjectionExpression'] = self.attributes\n        if self.alias:\n            query['ExpressionAttributeNames'] = self.alias\n        query['Keys'] = keys\n        return {\n            'RequestItems': {\n                self.tablename: query,\n            },\n            'ReturnConsumedCapacity': self.return_capacity,\n        }", "response": "Construct the kwargs to pass to batch_get_item"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch(self):\n        kwargs = self.build_kwargs()\n        data = self.connection.call('batch_get_item', **kwargs)\n        if 'UnprocessedKeys' in data:\n            for items in six.itervalues(data['UnprocessedKeys']):\n                self.keys.extend(items['Keys'])\n            # Getting UnprocessedKeys indicates that we are exceeding our\n            # throughput. So sleep for a bit.\n            self._attempt += 1\n            self.connection.exponential_sleep(self._attempt)\n        else:\n            # No UnprocessedKeys means our request rate is fine, so we can\n            # reset the attempt number.\n            self._attempt = 0\n        self._update_capacity(data)\n        if 'consumed_capacity' in data:\n            # Comes back as a list from BatchWriteItem\n            self.consumed_capacity = \\\n                sum(data['consumed_capacity'], self.consumed_capacity)\n        return iter(data['Responses'][self.tablename])", "response": "Fetch a set of items from their keys"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self):\n        return Limit(self.scan_limit, self.item_limit, self.min_scan_limit,\n                     self.strict, self.filter)", "response": "Return a copy of the limit"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the Limit parameter into the request args", "response": "def set_request_args(self, args):\n        \"\"\" Set the Limit parameter into the request args \"\"\"\n        if self.scan_limit is not None:\n            args['Limit'] = self.scan_limit\n        elif self.item_limit is not None:\n            args['Limit'] = max(self.item_limit, self.min_scan_limit)\n        else:\n            args.pop('Limit', None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef complete(self):\n        if self.scan_limit is not None and self.scan_limit == 0:\n            return True\n        if self.item_limit is not None and self.item_limit == 0:\n            return True\n        return False", "response": "Return True if the limit has been reached False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the filter and item_limit and return True if the item is acceptable False otherwise.", "response": "def accept(self, item):\n        \"\"\" Apply the filter and item_limit, and return True to accept \"\"\"\n        accept = self.filter(item)\n        if accept and self.item_limit is not None:\n            if self.item_limit > 0:\n                self.item_limit -= 1\n            elif self.strict:\n                return False\n        return accept"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef returned(n):\n\t## `takei` yield lazily so we can short-circuit and avoid computing the rest of the walk\n\tfor pos in randwalk() >> drop(1) >> takei(xrange(n-1)):\n\t\tif pos == Origin:\n\t\t\treturn True\n\treturn False", "response": "Generate a random walk and return True if the walker has returned to\n\tthe origin after taking n steps."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef first_return():\n\twalk = randwalk() >> drop(1) >> takewhile(lambda v: v != Origin) >> list\n\treturn len(walk)", "response": "Generate a random walk and return its length upto the moment\n\t that the walker first returns to the origin."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reduce(function, initval=None):\n\tif initval is None:\n\t\treturn lambda s: __builtin__.reduce(function, s)\n\telse:\n\t\treturn lambda s: __builtin__.reduce(function, s, initval)", "response": "Function to reduce the elements of a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting inpipe and outpipe.", "response": "def pipe(inpipe, outpipe):\n\t\t\"\"\"Connect inpipe and outpipe.  If outpipe is not a Stream instance,\n\t\tit should be an function callable on an iterable.\n\t\t\"\"\"\n\t\tif hasattr(outpipe, '__pipe__'):\n\t\t\treturn outpipe.__pipe__(inpipe)\n\t\telif hasattr(outpipe, '__call__'):\n\t\t\treturn outpipe(inpipe)\n\t\telse:\n\t\t\traise BrokenPipe('No connection mechanism defined')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extend(self, outpipe):\n\t\tself.iterator = outpipe.__call__(self.iterator)\n\t\treturn self", "response": "Extend the current Stream with the contents of another Stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsubmitting the given items.", "response": "def submit(self, *items):\n\t\t\"\"\"Return job ids assigned to the submitted items.\"\"\"\n\t\twith self.lock:\n\t\t\tif self.closed:\n\t\t\t\traise BrokenPipe('Job submission has been closed.')\n\t\t\tid = self.jobcount\n\t\t\tself._status += ['SUBMITTED'] * len(items)\n\t\t\tself.jobcount += len(items)\n\t\t\tfor item in items:\n\t\t\t\tself.waitqueue.put((id, item))\n\t\t\t\tid += 1\n\t\tif len(items) == 1:\n\t\t\treturn id - 1\n\t\telse:\n\t\t\treturn range(id - len(items), id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to cancel jobs with associated ids. Returns the actual number of jobs cancelled.", "response": "def cancel(self, *ids):\n\t\t\"\"\"Try to cancel jobs with associated ids.\n\t\t\n\t\tReturn the actual number of jobs cancelled.\n\t\t\"\"\"\n\t\tncancelled = 0\n\t\twith self.lock:\n\t\t\tfor id in ids:\n\t\t\t\ttry:\n\t\t\t\t\tif self._status[id] == 'SUBMITTED':\n\t\t\t\t\t\tself._status[id] = 'CANCELLED'\n\t\t\t\t\t\tncancelled += 1\n\t\t\t\texcept IndexError:\n\t\t\t\t\tpass\n\t\treturn ncancelled"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the statuses of the jobs with associated ids at the time of call.", "response": "def status(self, *ids):\n\t\t\"\"\"Return the statuses of jobs with associated ids at the\n\t\ttime of call:  either 'SUBMITED', 'CANCELLED', 'RUNNING',\n\t\t'COMPLETED' or 'FAILED'.\n\t\t\"\"\"\n\t\twith self.lock:\n\t\t\tif len(ids) > 1:\n\t\t\t\treturn [self._status[i] for i in ids]\n\t\t\telse:\n\t\t\t\treturn self._status[ids[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n\t\twith self.lock:\n\t\t\tif self.closed:\n\t\t\t\treturn\n\t\t\tself.waitqueue.put(StopIteration)\n\t\t\tself.closed = True", "response": "Signal that the executor will no longer accept jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\njoins the threads that are waiting for the executor to finish.", "response": "def join(self):\n\t\t\"\"\"Note that the Executor must be close()'d elsewhere,\n\t\tor join() will never return.\n\t\t\"\"\"\n\t\tself.inputfeeder_thread.join()\n\t\tself.pool.join()\n\t\tself.resulttracker_thread.join()\n\t\tself.failuretracker_thread.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshuts down the Executor.", "response": "def shutdown(self):\n\t\t\"\"\"Shut down the Executor.  Suspend all waiting jobs.\n\t\t\n\t\tRunning workers will terminate after finishing their current job items.\n\t\tThe call will block until all workers are terminated.\n\t\t\"\"\"\n\t\twith self.lock:\n\t\t\tself.pool.inqueue.put(StopIteration)   # Stop the pool workers\n\t\t\tself.waitqueue.put(StopIteration)      # Stop the input_feeder\n\t\t\t_iterqueue(self.waitqueue) >> item[-1] # Exhaust the waitqueue\n\t\t\tself.closed = True\n\t\tself.join()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_requirements(fname):\n    packages = _read(fname).split('\\n')\n    packages = (p.strip() for p in packages)\n    packages = (p for p in packages if p and not p.startswith('#'))\n    return list(packages)", "response": "Get a list of requirements from a pip freeze command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef TermsProcessor(instance, placeholder, rendered_content, original_context):\n    if 'terms' in original_context:\n        return rendered_content\n\n    return mark_safe(replace_terms(rendered_content))", "response": "Processes the terms template."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef install_isochrones(self):\n        cmd_obj = self.distribution.get_command_obj('isochrones')\n        cmd_obj.force = self.force\n        if self.ugali_dir: cmd_obj.ugali_dir = self.ugali_dir\n        self.run_command('isochrones')", "response": "Call to isochrone install command:\n        http://stackoverflow.com/a/24353921/4075339"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall the nagios catalogs", "response": "def install_catalogs(self):\n        \"\"\"\n        Call to catalog install command:\n        http://stackoverflow.com/a/24353921/4075339\n        \"\"\"\n        cmd_obj = self.distribution.get_command_obj('catalogs')\n        cmd_obj.force = self.force\n        if self.ugali_dir: cmd_obj.ugali_dir = self.ugali_dir\n        self.run_command('catalogs')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_reason(response, content):\n    def first_error(data):\n        errors = data['error']['errors']\n        if len(errors) > 1:\n            # we have more than one error. We should capture that?\n            logging.info('Received {} errors'.format(len(errors)))\n        return errors[0]\n\n    try:\n        json_data = json.loads(content)\n        return first_error(json_data)['reason']\n    except:\n        return response.reason", "response": "Parse the error reason from the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef time_stops(self):\n\n        if not self.supports_time:\n            return []\n\n        if self.service.calendar == 'standard':\n            units = self.service.time_interval_units\n            interval = self.service.time_interval\n            steps = [self.time_start]\n\n            if units in ('years', 'decades', 'centuries'):\n                if units == 'years':\n                    years = interval\n                elif units == 'decades':\n                    years = 10 * interval\n                else:\n                    years = 100 * interval\n\n                next_value = lambda x: x.replace(year=x.year + years)\n            elif units == 'months':\n                def _fn(x):\n                    year = x.year + (x.month+interval-1) // 12\n                    month = (x.month+interval) % 12 or 12\n                    day = min(x.day, calendar.monthrange(year, month)[1])\n\n                    return x.replace(year=year, month=month, day=day)\n                next_value = _fn\n            else:\n                if units == 'milliseconds':\n                    delta = timedelta(milliseconds=interval)\n                elif units == 'seconds':\n                    delta = timedelta(seconds=interval)\n                elif units == 'minutes':\n                    delta = timedelta(minutes=interval)\n                elif units == 'hours':\n                    delta = timedelta(hours=interval)\n                elif units == 'days':\n                    delta = timedelta(days=interval)\n                elif units == 'weeks':\n                    delta = timedelta(weeks=interval)\n                else:\n                    raise ValidationError(\n                        \"Service has an invalid time_interval_units: {}\".format(self.service.time_interval_units)\n                    )\n\n                next_value = lambda x: x + delta\n\n            while steps[-1] < self.time_end:\n                value = next_value(steps[-1])\n                if value > self.time_end:\n                    break\n                steps.append(value)\n            return steps\n\n        else:\n            # TODO\n            raise NotImplementedError", "response": "Returns a list of datetime objects that represent the time steps for this service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing target coordinates in various ways...", "response": "def _parse_coords(self,opts):\n        \"\"\" Parse target coordinates in various ways...\n        \"\"\"\n        # The coordinates are mutually exclusive, so\n        # shouldn't have to worry about over-writing them.\n        if 'coords' in vars(opts): return\n        radius = vars(opts).get('radius',0)\n        gal = None\n        if vars(opts).get('gal') is not None: \n            gal = opts.gal\n        elif vars(opts).get('cel') is not None: \n            gal = cel2gal(*opts.cel)\n        elif vars(opts).get('hpx') is not None: \n            gal = pix2ang(*opts.hpx)\n\n        if gal is not None:\n            opts.coords = [(gal[0],gal[1],radius)]\n            opts.names = [vars(opts).get('name','')]\n        else:\n            opts.coords = None\n            opts.names = None\n\n        if vars(opts).get('targets') is not None:\n            opts.names,opts.coords = self.parse_targets(opts.targets)\n            if vars(opts).get('radius') is not None:\n                opts.coords['radius'] = vars(opts).get('radius')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_targets(filename):\n        base,ext = os.path.splitext(filename)\n        if (ext=='.fits'):\n            import fitsio\n            data = fitsio.read(filename)\n        elif (ext=='.txt'):\n            from numpy.lib import NumpyVersion\n            if NumpyVersion(np.__version__) < '1.14.0':\n                data = np.genfromtxt(filename,names=True,dtype=None)\n            else:\n                data = np.genfromtxt(filename,names=True,dtype=None,encoding=None)\n            #data = np.genfromtxt(filename,unpack=True,usecols=list(range(5)),dtype=object,names=True)\n        elif (ext=='.yaml'):\n            import yaml\n            data = [(k,v['kernel']['lon']['value'],v['kernel']['lat']['value'],0.5,'CEL') for k,v in yaml.load(open(filename)).items()]\n            data = np.rec.fromrecords(data,names=['name','lon','lat','radius','coord'])\n        else:\n            msg = \"Unrecognized file type: %s\"%filename\n            raise IOError(msg)\n\n        data = np.atleast_1d(data)\n        data.dtype.names = list(map(str.lower,data.dtype.names))\n\n        # Deal with one-line input files\n        #if data.ndim == 1: data = np.array([data]).T\n        names = data['name']\n        out   = data[['lon','lat','radius']].copy()\n         \n        coord = np.char.lower(data['coord'])\n        gal = (coord=='gal')\n        cel = (coord=='cel')\n        hpx = (coord=='hpx')\n         \n        if cel.any():\n            glon,glat = cel2gal(data['lon'][cel],data['lat'][cel])\n            out['lon'][cel] = glon\n            out['lat'][cel] = glat\n        if hpx.any():\n            glon,glat = pix2ang(data['lat'][hpx],data['lon'][hpx])\n            out['lon'][hpx] = glon\n            out['lat'][hpx] = glat\n         \n        return names,out.view(np.ndarray)", "response": "Load a text file with target coordinates. Returns an array of target locations in Galactic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_data(self, special_fields=None):\n        docs = build_document_set(self.data, self.data_type, self.mappings,\n                                  special_fields,\n                                  self.idMapping,\n                                  additionalFields=self.additionalFields)\n\n        if self.plugins:\n            docs = run_plugins(self.plugins, list(docs))\n\n        return docs", "response": "This method builds the data for the collector type and returns a dict of data that can be used to parse the data into the instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef meanFracdet(map_fracdet, lon_population, lat_population, radius_population):\n    nside_fracdet = healpy.npix2nside(len(map_fracdet))\n    map_fracdet_zero = np.where(map_fracdet >= 0., map_fracdet, 0.)\n    fracdet_population = np.empty(len(lon_population))\n    for ii in range(0, len(lon_population)):\n        fracdet_population[ii] = np.mean(map_fracdet_zero[ugali.utils.healpix.ang2disc(nside_fracdet, \n                                                                                       lon_population[ii], \n                                                                                       lat_population[ii], \n                                                                                       radius_population if np.isscalar(radius_population) else radius_population[ii],\n                                                                                       inclusive=True)])\n    return fracdet_population", "response": "Compute the mean fracdet within a circular aperture"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsimulating a single satellite.", "response": "def catsimSatellite(config, lon_centroid, lat_centroid, distance, stellar_mass, r_physical, \n                    m_maglim_1, m_maglim_2, m_ebv,\n                    plot=False, title='test'):\n    \"\"\"\n    Simulate a single satellite. This is currently only valid for band_1 = g and band_2 = r.\n    r_physical is azimuthally averaged half-light radius, kpc\n    \"\"\"\n\n    # Probably don't want to parse every time\n    completeness = getCompleteness(config)\n    log_photo_error = getPhotoError(config)\n\n    s = ugali.analysis.source.Source()\n\n    # Following McConnachie 2012, ellipticity = 1 - (b/a) , where a is semi-major axis and b is semi-minor axis\n    \n    r_h = np.degrees(np.arcsin(r_physical / distance)) # Azimuthally averaged half-light radius\n    #ellipticity = 0.3 # Semi-arbitrary default for testing purposes\n    # See http://iopscience.iop.org/article/10.3847/1538-4357/833/2/167/pdf\n    # Based loosely on https://arxiv.org/abs/0805.2945\n    ellipticity = np.random.uniform(0.1, 0.8)\n    position_angle = np.random.uniform(0., 180.) # Random position angle (deg)\n    a_h = r_h / np.sqrt(1. - ellipticity) # semi-major axis (deg)\n    \n        \n    # Elliptical kernels take the \"extension\" as the semi-major axis\n    ker = ugali.analysis.kernel.EllipticalPlummer(lon=lon_centroid, lat=lat_centroid, ellipticity=ellipticity, position_angle=position_angle)\n\n    flag_too_extended = False\n    if a_h >= 1.0:\n        print 'Too extended: a_h = %.2f'%(a_h)\n        a_h = 1.0\n        flag_too_extended = True\n    ker.setp('extension', value=a_h, bounds=[0.0,1.0])\n    s.set_kernel(ker)\n    \n    age = np.random.choice([10., 12.0, 13.5])\n    metal_z = np.random.choice([0.0001, 0.0002])\n    distance_modulus = ugali.utils.projector.distanceToDistanceModulus(distance)\n    iso = isochrone_factory('Bressan2012', survey=config['survey'], age=age, z=metal_z, distance_modulus=distance_modulus)\n    s.set_isochrone(iso)\n    # Simulate takes stellar mass as an argument, NOT richness\n    mag_1, mag_2 = s.isochrone.simulate(stellar_mass) \n\n    lon, lat = s.kernel.sample_lonlat(len(mag_2))\n\n    nside = healpy.npix2nside(len(m_maglim_1)) # Assuming that the two maglim maps have same resolution\n    pix = ugali.utils.healpix.angToPix(nside, lon, lat)\n    maglim_1 = m_maglim_1[pix]\n    maglim_2 = m_maglim_2[pix]\n    if config['survey'] == 'des':\n        # DES Y3 Gold fiducial\n        mag_extinction_1 = 3.186 * m_ebv[pix]\n        mag_extinction_2 = 2.140 * m_ebv[pix]\n    elif config['survey'] == 'ps1':\n        # From Table 6 in Schlafly 2011 with Rv = 3.1\n        # http://iopscience.iop.org/article/10.1088/0004-637X/737/2/103/pdf\n        mag_extinction_1 = 3.172 * m_ebv[pix]\n        mag_extinction_2 = 2.271 * m_ebv[pix]\n    \n    # Photometric uncertainties are larger in the presence of interstellar dust reddening\n    mag_1_error = 0.01 + 10**(log_photo_error((mag_1 + mag_extinction_1) - maglim_1))\n    mag_2_error = 0.01 + 10**(log_photo_error((mag_2 + mag_extinction_2) - maglim_2))\n\n    # It would be better to convert to a flux uncertainty and then transform back to a magnitude\n    #mag_1_meas = mag_1 + np.random.normal(scale=mag_1_error)\n    #mag_2_meas = mag_2 + np.random.normal(scale=mag_2_error)\n    flux_1_meas = magToFlux(mag_1) + np.random.normal(scale=getFluxError(mag_1, mag_1_error))\n    mag_1_meas = np.where(flux_1_meas > 0., fluxToMag(flux_1_meas), 99.)\n    flux_2_meas = magToFlux(mag_2) + np.random.normal(scale=getFluxError(mag_2, mag_2_error))\n    mag_2_meas = np.where(flux_2_meas > 0., fluxToMag(flux_2_meas), 99.)\n\n    # In the HSC SXDS ultra-deep field:\n    # mean maglim_r_sof_gold_2.0 = 23.46\n    # median maglim_r_sof_gold_2.0 = 23.47\n    # m = healpy.read_map('/Users/keithbechtol/Documents/DES/projects/mw_substructure/des/y3a1/data/maps/y3a2_gold_1.0_cmv02-001_v1_nside4096_nest_r_depth.fits.gz')\n    # np.mean(m[ugali.utils.healpix.angToDisc(4096, 34.55, -4.83, 0.75)])\n    # np.median(m[ugali.utils.healpix.angToDisc(4096, 34.55, -4.83, 0.75)])\n\n    # Includes penalty for interstellar extinction and also include variations in depth\n    if config['survey'] == 'des':\n        cut_detect = (np.random.uniform(size=len(mag_2)) < completeness(mag_2 + mag_extinction_2 + (23.46 - np.clip(maglim_2, 20., 26.))))\n    elif config['survey'] == 'ps1':\n        cut_detect = (np.random.uniform(size=len(mag_2)) < completeness(mag_2 + mag_extinction_2))\n\n    n_g22 = np.sum(cut_detect & (mag_1 < 22.))\n    n_g24 = np.sum(cut_detect & (mag_1 < 24.))\n    print '  n_sim = %i, n_detect = %i, n_g24 = %i, n_g22 = %i'%(len(mag_1),np.sum(cut_detect),n_g24,n_g22)\n    \n    richness = stellar_mass / s.isochrone.stellarMass()\n    #abs_mag = s.isochrone.absolute_magnitude()\n    #abs_mag_martin = s.isochrone.absolute_magnitude_martin(richness=richness, n_trials=10)[0] # 100 trials seems to be sufficient for rough estimate\n    #print 'abs_mag_martin = %.2f mag'%(abs_mag_martin)\n\n    # The more clever thing to do would be to sum up the actual simulated stars\n    if config['survey'] == 'des':\n        v = mag_1 - 0.487*(mag_1 - mag_2) - 0.0249 # See https://github.com/DarkEnergySurvey/ugali/blob/master/ugali/isochrone/model.py\n    elif config['survey'] == 'ps1':\n        # https://arxiv.org/pdf/1706.06147.pdf\n        # V - g = C_0 + C_1 * (g - r)\n        C_0 = -0.017\n        C_1 = -0.508\n        v = mag_1 + C_0 + C_1 * (mag_1 - mag_2)\n    flux = np.sum(10**(-v/2.5))\n    abs_mag = -2.5*np.log10(flux) - distance_modulus\n\n    #print abs_mag, abs_mag_martin\n\n    #distance = ugali.utils.projector.distanceModulusToDistance(distance_modulus)\n    #r_h = extension * np.sqrt(1. - ellipticity) # Azimuthally averaged half-light radius\n    r_physical = distance * np.tan(np.radians(r_h)) # Azimuthally averaged half-light radius, kpc\n    #print 'distance = %.3f kpc'%(distance)\n    #print 'r_physical = %.3f kpc'%(r_physical)\n    surface_brightness = ugali.analysis.results.surfaceBrightness(abs_mag, r_physical, distance) # Average within azimuthally averaged half-light radius\n    #print 'surface_brightness = %.3f mag arcsec^-2'%(surface_brightness)\n    \n    if plot:\n        import pylab\n        pylab.ion()\n\n        n_sigma_p = np.sum(cut_detect & (mag_1 < 23.))\n\n        pylab.figure(figsize=(6., 6.))\n        pylab.scatter(mag_1_meas[cut_detect] - mag_2_meas[cut_detect], mag_1_meas[cut_detect], edgecolor='none', c='black', s=5)\n        pylab.xlim(-0.5, 1.)\n        pylab.ylim(26., 16.)\n        pylab.xlabel('g - r')\n        pylab.ylabel('g')\n        pylab.title('Number of stars with g < 23: %i'%(n_sigma_p))\n        pylab.savefig('y3_sat_sim_cmd_%s.png'%(title), dpi=150.)\n        \n        print 'n_Sigma_p = %i'%(n_sigma_p)\n        raw_input('WAIT')\n        \n    #if flag_too_extended:\n    #    # This is a kludge to remove these satellites. fragile!!\n    #    n_g24 = 1.e6\n\n    return lon[cut_detect], lat[cut_detect], mag_1_meas[cut_detect], mag_2_meas[cut_detect], mag_1_error[cut_detect], mag_2_error[cut_detect], mag_extinction_1[cut_detect], mag_extinction_2[cut_detect], n_g22, n_g24, abs_mag, surface_brightness, ellipticity, position_angle, age, metal_z, flag_too_extended"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef catsimPopulation(tag, mc_source_id_start=1, n=5000, n_chunk=100, config='simulate_population.yaml'):\n\n    assert mc_source_id_start >= 1, \"Starting mc_source_id must be >= 1\" \n    assert n % n_chunk == 0, \"Total number of satellites must be divisible by the chunk size\"\n    nside_pix = 256 # NSIDE = 128 -> 27.5 arcmin, NSIDE = 256 -> 13.7 arcmin \n    \n    if not os.path.exists(tag): os.makedirs(tag)\n\n    if isinstance(config,str): config = yaml.load(open(config))\n    assert config['survey'] in ['des', 'ps1']\n\n    infile_ebv = config['ebv']\n    infile_fracdet = config['fracdet']\n    infile_maglim_g = config['maglim_g']\n    infile_maglim_r = config['maglim_r']\n    infile_density = config['stellar_density']\n\n    range_distance = config.get('range_distance',[5., 500.])\n    range_stellar_mass = config.get('range_stellar_mass',[1.e1, 1.e6])\n    range_r_physical = config.get('range_r_physical',[1.e-3, 2.0])\n    \n    m_density = np.load(infile_density)\n    nside_density = healpy.npix2nside(len(m_density))\n    m_fracdet = read_map(infile_fracdet, nest=False) #.astype(np.float16)\n    nside_fracdet = healpy.npix2nside(len(m_fracdet))\n\n    m_maglim_g = read_map(infile_maglim_g, nest=False) #.astype(np.float16)\n    m_maglim_r = read_map(infile_maglim_r, nest=False) #.astype(np.float16)\n\n    m_ebv = read_map(infile_ebv, nest=False) #.astype(np.float16)\n    \n    #m_foreground = healpy.read_map(infile_foreground)\n\n    mask = (m_fracdet > 0.5)\n\n    kwargs = dict(range_distance = range_distance,\n                  range_stellar_mass = range_stellar_mass,\n                  range_r_physical = range_r_physical)\n    print kwargs\n    # r_physical is azimuthally-averaged half-light radius, kpc\n    simulation_area, lon_population, lat_population, distance_population, stellar_mass_population, r_physical_population = ugali.simulation.population.satellitePopulation(mask, nside_pix, n, **kwargs)\n    n_g22_population = np.tile(np.nan, n)\n    n_g24_population = np.tile(np.nan, n)\n    abs_mag_population = np.tile(np.nan, n)\n    surface_brightness_population = np.tile(np.nan, n)\n    ellipticity_population = np.tile(np.nan, n)\n    position_angle_population = np.tile(np.nan, n)\n    age_population = np.tile(np.nan, n)\n    metal_z_population = np.tile(np.nan, n)\n    mc_source_id_population = np.arange(mc_source_id_start, mc_source_id_start + n)\n    #cut_difficulty_population = np.tile(False, n)\n    difficulty_population = np.tile(0, n)\n\n    lon_array = []\n    lat_array = []\n    mag_1_array = []\n    mag_2_array = []\n    mag_1_error_array = []\n    mag_2_error_array = []\n    mag_extinction_1_array = []\n    mag_extinction_2_array = []\n    mc_source_id_array = []\n    for ii, mc_source_id in enumerate(mc_source_id_population):\n        print '  Simulating satellite (%i/%i) ... MC_SOURCE_ID = %i'%(ii + 1, n, mc_source_id)\n        print '  distance=%.2e, stellar_mass=%.2e, rhalf=%.2e'%(distance_population[ii],stellar_mass_population[ii],r_physical_population[ii])\n        lon, lat, mag_1, mag_2, mag_1_error, mag_2_error, mag_extinction_1, mag_extinction_2, n_g22, n_g24, abs_mag, surface_brightness, ellipticity, position_angle, age, metal_z, flag_too_extended = catsimSatellite(config,\n                                                                                                                                                                             lon_population[ii], \n                                                                                                                                                                             lat_population[ii], \n                                                                                                                                                                             distance_population[ii], \n                                                                                                                                                                             stellar_mass_population[ii], \n                                                                                                                                                                             r_physical_population[ii],\n                                                                                                                                                                             m_maglim_g,\n                                                                                                                                                                             m_maglim_r,\n                                                                                                                                                                             m_ebv)\n        print '  ', len(lon)\n        \n        n_g22_population[ii] = n_g22\n        n_g24_population[ii] = n_g24\n        abs_mag_population[ii] = abs_mag\n        surface_brightness_population[ii] = surface_brightness\n        ellipticity_population[ii] = ellipticity\n        position_angle_population[ii] = position_angle\n        age_population[ii] = age\n        metal_z_population[ii] = metal_z\n\n        #print \"Difficulty masking...\"  \n\n        # These objects are too extended and are not simulated\n        if (flag_too_extended):\n            difficulty_population[ii] |= 0b0001\n\n        # We assume that these objects would be easily detected and\n        # remove them to reduce data volume\n        if (surface_brightness_population[ii]<25.)&(n_g22_population[ii]>1e2):\n            difficulty_population[ii] |= 0b0010\n        if (surface_brightness_population[ii]<28.)&(n_g22_population[ii]>1e4):\n            difficulty_population[ii] |= 0b0100\n        if (surface_brightness_population[ii]<30.)&(n_g22_population[ii]>1e5):\n            difficulty_population[ii] |= 0b1000\n        \n        #cut_easy = (surface_brightness_population[ii]<25.)&(n_g22_population[ii]>1.e2) \\\n        #           | ((surface_brightness_population[ii] < 30.) & (n_g24_population[ii] > 1.e4)) \\\n        #           | ((surface_brightness_population[ii] < 31.) & (n_g24_population[ii] > 1.e5))\n        #cut_hard = (surface_brightness_population[ii] > 35.) | (n_g24_population[ii] < 1.)\n        #cut_difficulty_population[ii] = ~cut_easy & ~cut_hard\n        #if cut_easy:\n        #    difficulty_population[ii] += 1 # TOO EASY\n        #if cut_hard:\n        #    difficulty_population[ii] += 2 # TOO HARD\n        #if flag_too_extended:\n        #    difficulty_population[ii] += 3 # TOO EXTENDED\n\n        if difficulty_population[ii] == 0:\n            lon_array.append(lon)\n            lat_array.append(lat)\n            mag_1_array.append(mag_1)\n            mag_2_array.append(mag_2)\n            mag_1_error_array.append(mag_1_error)\n            mag_2_error_array.append(mag_2_error)\n            mag_extinction_1_array.append(mag_extinction_1)\n            mag_extinction_2_array.append(mag_extinction_2)\n            mc_source_id_array.append(np.tile(mc_source_id, len(lon)))\n\n    # Concatenate all the arrays\n\n    print \"Concatenating arrays...\"\n    lon_array = np.concatenate(lon_array)\n    lat_array = np.concatenate(lat_array)\n    mag_1_array = np.concatenate(mag_1_array)\n    mag_2_array = np.concatenate(mag_2_array)\n    mag_1_error_array = np.concatenate(mag_1_error_array)\n    mag_2_error_array = np.concatenate(mag_2_error_array)\n    mag_extinction_1_array = np.concatenate(mag_extinction_1_array)\n    mag_extinction_2_array = np.concatenate(mag_extinction_2_array)\n    mc_source_id_array = np.concatenate(mc_source_id_array)\n\n    # Now do the masking all at once\n\n    print \"Fracdet masking...\"\n    pix_array = ugali.utils.healpix.angToPix(nside_fracdet, lon_array, lat_array)\n    cut_fracdet = (np.random.uniform(size=len(lon_array)) < m_fracdet[pix_array])\n\n    lon_array = lon_array[cut_fracdet]\n    lat_array = lat_array[cut_fracdet]\n    mag_1_array = mag_1_array[cut_fracdet]\n    mag_2_array = mag_2_array[cut_fracdet]\n    mag_1_error_array = mag_1_error_array[cut_fracdet]\n    mag_2_error_array = mag_2_error_array[cut_fracdet]\n    mag_extinction_1_array = mag_extinction_1_array[cut_fracdet]\n    mag_extinction_2_array = mag_extinction_2_array[cut_fracdet]\n    mc_source_id_array = mc_source_id_array[cut_fracdet]\n\n    # Cut out the entries that are easily detectable\n    \"\"\"\n    lon_population = lon_population[cut_difficulty_population]\n    lat_population = lat_population[cut_difficulty_population]\n    distance_population = distance_population[cut_difficulty_population]\n    stellar_mass_population = stellar_mass_population[cut_difficulty_population]\n    r_physical_population = r_physical_population[cut_difficulty_population]\n    n_g24_population = n_g24_population[cut_difficulty_population]\n    abs_mag_population = abs_mag_population[cut_difficulty_population]\n    surface_brightness_population = surface_brightness_population[cut_difficulty_population]\n    ellipticity_population = ellipticity_population[cut_difficulty_population]\n    position_angle_population = position_angle_population[cut_difficulty_population]\n    age_population = age_population[cut_difficulty_population]\n    metal_z_population = metal_z_population[cut_difficulty_population]\n    mc_source_id_population = mc_source_id_population[cut_difficulty_population]\n    \"\"\"\n    \n    # Create bonus columns\n    \n    print \"Creating bonus columns...\"\n    distance_modulus_population = ugali.utils.projector.distanceToDistanceModulus(distance_population)\n    hpix_32_population = ugali.utils.healpix.angToPix(32, lon_population, lat_population) # Make sure this matches the dataset\n\n    # Local stellar density\n    pixarea = healpy.nside2pixarea(nside_density, degrees=True) * 60.**2 # arcmin^2\n    density_population = m_density[ugali.utils.healpix.angToPix(nside_density, lon_population, lat_population)] / pixarea # arcmin^-2\n\n    # Average fracdet within the azimuthally averaged half-light radius\n    #m_fracdet_zero = np.where(m_fracdet >= 0., m_fracdet, 0.)\n    #m_fracdet_zero = m_fracdet\n    r_half = np.degrees(np.arctan2(r_physical_population, distance_population)) # Azimuthally averaged half-light radius in degrees\n    fracdet_half_population = meanFracdet(m_fracdet, lon_population, lat_population, r_half)\n    fracdet_core_population = meanFracdet(m_fracdet, lon_population, lat_population, 0.1)\n    fracdet_wide_population = meanFracdet(m_fracdet, lon_population, lat_population, 0.5)\n\n    # Magnitude limits\n    nside_maglim = healpy.npix2nside(len(m_maglim_g))\n    pix_population = ugali.utils.healpix.angToPix(nside_maglim, lon_population, lat_population)\n    maglim_g_population = m_maglim_g[pix_population]\n    maglim_r_population = m_maglim_r[pix_population]\n    \n    # E(B-V)\n    nside_ebv = healpy.npix2nside(len(m_ebv))\n    pix_population = ugali.utils.healpix.angToPix(nside_ebv, lon_population, lat_population)\n    ebv_population = m_ebv[pix_population]\n\n    # Survey\n    survey_population = np.tile(config['survey'], len(lon_population))\n\n    # Number of surviving catalog stars\n    n_catalog_population = np.histogram(mc_source_id_array, bins=np.arange(mc_source_id_population[0] - 0.5, mc_source_id_population[-1] + 0.51))[0]\n\n    # Faked-up coadd_object_ids\n    coadd_object_id_array = []\n    for mc_source_id in mc_source_id_population:\n        coadd_object_id_array.append((1000000 * mc_source_id) + 1 + np.arange(np.sum(mc_source_id == mc_source_id_array)))\n    coadd_object_id_array = -1 * np.concatenate(coadd_object_id_array) # Assign negative numbers to distinguish from real objects\n\n    # Catalog output file\n\n    # for ii in range(0, len(d.formats)): print '\\'%s\\': [ , \\'%s\\'],'%(d.names[ii], d.formats[ii])\n    \n    # See: \n    # https://github.com/sidneymau/simple/blob/master/search_algorithm.py \n    # https://github.com/sidneymau/simple/blob/master/config.yaml\n    # /home/s1/kadrlica/projects/y3a2/dsphs/v2/skim/ , e.g., /home/s1/kadrlica/projects/y3a2/dsphs/v2/skim/y3a2_ngmix_cm_11755.fits\n\n    #default_array = np.tile(np.nan, len(mc_source_id_array)) # To recognize that those values are synthetic filler\n    default_array = np.tile(-9999., len(mc_source_id_array))\n\n    \"\"\"\n    # Column name, data, fits format\n    # Y3A2 pre-Gold\n    key_map = {'CM_MAG_ERR_G': [mag_1_error_array, 'D'],\n               'CM_MAG_ERR_R': [mag_2_error_array, 'D'],\n               'CM_MAG_G': [mag_1_array, 'D'],\n               'CM_MAG_R': [mag_2_array, 'D'],\n               'CM_T': [default_array, 'D'],\n               'CM_T_ERR': [default_array, 'D'],\n               'COADD_OBJECT_ID': [coadd_object_id_array, 'K'],\n               'DEC': [lat_array, 'D'],\n               'FLAGS': [default_array, 'K'],\n               'PSF_MAG_ERR_G': [mag_1_error_array, 'D'],\n               'PSF_MAG_ERR_R': [mag_2_error_array, 'D'],\n               'PSF_MAG_G': [mag_1_array, 'D'],\n               'PSF_MAG_R': [mag_2_array, 'D'],\n               'RA': [lon_array, 'D'],\n               'SEXTRACTOR_FLAGS_G': [np.tile(0, len(mc_source_id_array)), 'I'],\n               'SEXTRACTOR_FLAGS_R': [np.tile(0, len(mc_source_id_array)), 'I'],\n               'WAVG_MAG_PSF_G': [mag_1_array, 'E'],\n               'WAVG_MAG_PSF_R': [mag_2_array, 'E'],\n               'WAVG_MAGERR_PSF_G': [mag_1_error_array, 'E'],\n               'WAVG_MAGERR_PSF_R': [mag_2_error_array, 'E'],\n               'WAVG_SPREAD_MODEL_I': [default_array, 'E'],\n               'WAVG_SPREADERR_MODEL_I': [default_array, 'E'],\n               'EXT_SFD98_G': [default_array, 'E'],\n               'EXT_SFD98_R': [default_array, 'E'],\n               'CM_MAG_SFD_G': [mag_1_array, 'D'],\n               'CM_MAG_SFD_R': [mag_2_array, 'D'],\n               'FLAG_FOOTPRINT': [np.tile(1, len(mc_source_id_array)), 'J'],\n               'FLAG_FOREGROUND': [np.tile(0, len(mc_source_id_array)), 'J'],\n               'EXTENDED_CLASS_MASH': [np.tile(0, len(mc_source_id_array)), 'K'],\n               'PSF_MAG_SFD_G': [mag_1_array, 'D'],\n               'PSF_MAG_SFD_R': [mag_2_array, 'D'],\n               'WAVG_MAG_PSF_SFD_G': [mag_1_array, 'E'],\n               'WAVG_MAG_PSF_SFD_R': [mag_2_array, 'E']}\n    \"\"\"\n    \n    if config['survey'] == 'des':\n        # Y3 Gold v2.0\n        key_map = odict([\n                ('COADD_OBJECT_ID', [coadd_object_id_array, 'K']),\n                ('RA', [lon_array, 'D']),\n                ('DEC', [lat_array, 'D']),\n                ('SOF_PSF_MAG_CORRECTED_G', [mag_1_array, 'D']),\n                ('SOF_PSF_MAG_CORRECTED_R', [mag_2_array, 'D']),\n                ('SOF_PSF_MAG_ERR_G', [mag_1_error_array, 'D']),\n                ('SOF_PSF_MAG_ERR_R', [mag_2_error_array, 'D']),\n                ('A_SED_SFD98_G', [mag_extinction_1_array, 'E']),\n                ('A_SED_SFD98_R', [mag_extinction_2_array, 'E']),\n                ('WAVG_MAG_PSF_G', [mag_1_array+mag_extinction_1_array, 'E']),\n                ('WAVG_MAG_PSF_R', [mag_2_array+mag_extinction_2_array, 'E']),\n                ('WAVG_MAGERR_PSF_G', [mag_1_error_array, 'E']),\n                ('WAVG_MAGERR_PSF_R', [mag_2_error_array, 'E']),\n                ('WAVG_SPREAD_MODEL_I', [default_array, 'E']),\n                ('WAVG_SPREADERR_MODEL_I', [default_array, 'E']),\n                ('SOF_CM_T', [default_array, 'D']),\n                ('SOF_CM_T_ERR', [default_array, 'D']),\n                ('FLAGS_GOLD', [np.tile(0, len(mc_source_id_array)), 'J']),\n                ('EXTENDED_CLASS_MASH_SOF', [np.tile(0, len(mc_source_id_array)), 'I']),\n                ])\n    elif config['survey'] == 'ps1':\n        # PS1\n        key_map = odict([\n                ('OBJID', [coadd_object_id_array, 'K']),\n                ('RA', [lon_array, 'D']),\n                ('DEC', [lat_array, 'D']),\n                #('UNIQUEPSPSOBID', [coadd_object_id_array, 'K']),\n                #('OBJINFOFLAG', [default_array, 'E']),\n                #('QUALITYFLAG', [np.tile(16, len(mc_source_id_array)), 'I']),\n                #('NSTACKDETECTIONS', [np.tile(99, len(mc_source_id_array)), 'I']),\n                #('NDETECTIONS', [np.tile(99, len(mc_source_id_array)), 'I']),\n                #('NG', [default_array, 'E']),\n                #('NR', [default_array, 'E']),\n                #('NI', [default_array, 'E']),\n                ('GFPSFMAG', [mag_1_array+mag_extinction_1_array, 'E']),\n                ('RFPSFMAG', [mag_2_array+mag_extinction_2_array, 'E']),\n                #('IFPSFMAG', [np.tile(0., len(mc_source_id_array)), 'E'], # Too pass star selection\n                ('GFPSFMAGERR', [mag_1_error_array, 'E']),\n                ('RFPSFMAGERR', [mag_2_error_array, 'E']),\n                #('IFPSFMAGERR', [default_array, 'E']),\n                #('GFKRONMAG', [mag_1_array, 'E']),\n                #('RFKRONMAG', [mag_2_array, 'E']),\n                #('IFKRONMAG', [np.tile(0., len(mc_source_id_array)), 'E'], # Too pass star selection\n                #('GFKRONMAGERR', [mag_1_error_array, 'E']),\n                #('RFKRONMAGERR', [mag_2_error_array, 'E']),\n                #('IFKRONMAGERR', [default_array, 'E']),\n                #('GFLAGS', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('RFLAGS', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('IFLAGS', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('GINFOFLAG', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('RINFOFLAG', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('IINFOFLAG', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('GINFOFLAG2', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('RINFOFLAG2', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('IINFOFLAG2', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('GINFOFLAG3', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('RINFOFLAG3', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('IINFOFLAG3', [np.tile(0, len(mc_source_id_array)), 'I']),\n                #('PRIMARYDETECTION', [default_array, 'E']),\n                #('BESTDETECTION', [default_array, 'E']),\n                #('EBV', [default_array, 'E']),\n                #('EXTSFD_G', [mag_extinction_1_array 'E']),\n                #('EXTSFD_R', [mag_extinction_2_array, 'E']),\n                #('EXTSFD_I', [default_array, 'E']),\n                ('GFPSFMAG_SFD', [mag_1_array, 'E']),\n                ('RFPSFMAG_SFD', [mag_2_array, 'E']),\n                ('EXTENDED_CLASS', [np.tile(0, len(mc_source_id_array)), 'I']),\n                ])\n    key_map['MC_SOURCE_ID'] = [mc_source_id_array, 'K']\n\n    print \"Writing catalog files...\"\n    columns = []\n    for key in key_map:\n        columns.append(pyfits.Column(name=key, format=key_map[key][1], array=key_map[key][0]))\n    tbhdu = pyfits.BinTableHDU.from_columns(columns)\n    tbhdu.header.set('AREA', simulation_area, 'Simulation area (deg^2)')\n\n    for mc_source_id_chunk in np.split(np.arange(mc_source_id_start, mc_source_id_start + n), n / n_chunk):\n        print '  writing MC_SOURCE_ID values from %i to %i'%(mc_source_id_chunk[0], mc_source_id_chunk[-1])\n        cut_chunk = np.in1d(mc_source_id_array, mc_source_id_chunk)\n        outfile = '%s/sim_catalog_%s_mc_source_id_%07i-%07i.fits'%(tag, tag, mc_source_id_chunk[0], mc_source_id_chunk[-1])\n        header = copy.deepcopy(tbhdu.header)\n        header.set('IDMIN',mc_source_id_chunk[0], 'Minimum MC_SOURCE_ID')\n        header.set('IDMAX',mc_source_id_chunk[-1], 'Maximum MC_SOURCE_ID')\n        pyfits.writeto(outfile, tbhdu.data[cut_chunk], header, clobber=True)\n\n    # Population metadata output file\n    \n    print \"Writing population metadata file...\"\n    tbhdu = pyfits.BinTableHDU.from_columns([\n        pyfits.Column(name='RA', format='E', array=lon_population, unit='deg'),\n        pyfits.Column(name='DEC', format='E', array=lat_population, unit='deg'),\n        pyfits.Column(name='DISTANCE', format='E', array=distance_population, unit='kpc'),\n        pyfits.Column(name='DISTANCE_MODULUS', format='E', array=distance_modulus_population, unit='kpc'),\n        pyfits.Column(name='STELLAR_MASS', format='E', array=stellar_mass_population, unit='m_solar'),\n        pyfits.Column(name='R_PHYSICAL', format='E', array=r_physical_population, unit='kpc'),\n        pyfits.Column(name='N_G22', format='J', array=n_g22_population, unit=''),\n        pyfits.Column(name='N_G24', format='J', array=n_g24_population, unit=''),\n        pyfits.Column(name='N_CATALOG', format='J', array=n_catalog_population, unit=''),\n        pyfits.Column(name='DIFFICULTY', format='J', array=difficulty_population, unit=''),\n        pyfits.Column(name='ABS_MAG', format='E', array=abs_mag_population, unit='mag'),\n        pyfits.Column(name='SURFACE_BRIGHTNESS', format='E', array=surface_brightness_population, unit='mag arcsec^-2'),\n        pyfits.Column(name='ELLIPTICITY', format='E', array=ellipticity_population, unit=''),\n        pyfits.Column(name='POSITION_ANGLE', format='E', array=position_angle_population, unit='deg'),\n        pyfits.Column(name='AGE', format='E', array=age_population, unit='deg'),\n        pyfits.Column(name='METAL_Z', format='E', array=metal_z_population, unit=''),\n        pyfits.Column(name='MC_SOURCE_ID', format='K', array=mc_source_id_population, unit=''),\n        pyfits.Column(name='HPIX_32', format='E', array=hpix_32_population, unit=''),\n        pyfits.Column(name='DENSITY', format='E', array=density_population, unit='arcmin^-2'),\n        pyfits.Column(name='FRACDET_HALF', format='E', array=fracdet_half_population, unit=''),\n        pyfits.Column(name='FRACDET_CORE', format='E', array=fracdet_core_population, unit=''),\n        pyfits.Column(name='FRACDET_WIDE', format='E', array=fracdet_wide_population, unit=''),\n        pyfits.Column(name='MAGLIM_G', format='E', array=maglim_g_population, unit='mag'),\n        pyfits.Column(name='MAGLIM_R', format='E', array=maglim_r_population, unit='mag'),\n        pyfits.Column(name='EBV', format='E', array=ebv_population, unit='mag'),\n        pyfits.Column(name='SURVEY', format='A12', array=survey_population, unit=''),\n    ])\n    tbhdu.header.set('AREA', simulation_area, 'Simulation area (deg^2)')\n    tbhdu.writeto('%s/sim_population_%s_mc_source_id_%07i-%07i.fits'%(tag, tag, mc_source_id_start, mc_source_id_start + n - 1), clobber=True)\n\n    # 5284.2452461023322\n\n    # Mask output file\n\n    print \"Writing population mask file...\"\n    outfile_mask = '%s/sim_mask_%s_cel_nside_%i.fits'%(tag, tag, healpy.npix2nside(len(mask)))\n    if not os.path.exists(outfile_mask):\n        healpy.write_map(outfile_mask, mask.astype(int), nest=True, coord='C', overwrite=True)\n        os.system('gzip -f %s'%(outfile_mask))", "response": "Create a population of a single MCMC source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raw_value(self):\n        if self.parent_setting is not None:\n            return self.parent_setting.raw_value[self.full_name]\n        else:\n            return getattr(settings, self.full_name)", "response": "Property to return the value defined in django. conf. settings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_value(self):\n        try:\n            value = self.raw_value\n        except (AttributeError, KeyError) as err:\n            self._reraise_if_required(err)\n            default_value = self.default_value\n            if self.transform_default:\n                return self.transform(default_value)\n            return default_value\n        else:\n            return self.transform(value)", "response": "Returns the transformed raw or default value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_validators(self, value):\n        errors = []\n        for validator in self.validators:\n            try:\n                validator(value)\n            except ValidationError as error:\n                errors.extend(error.messages)\n        if errors:\n            raise ValidationError(errors)", "response": "Run the validators on the setting value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the checker against the raw value.", "response": "def check(self):\n        \"\"\"\n        Run the setting checker against the setting raw value.\n\n        Raises:\n            AttributeError: if the setting is missing and required.\n            ValueError: if the raw value is invalid.\n        \"\"\"\n        try:\n            value = self.raw_value\n        except (AttributeError, KeyError) as err:\n            self._reraise_if_required(err)\n        else:\n            if self.checker:\n                self.checker(self.full_name, value)\n            try:\n                self.validate(value)\n                self.run_validators(value)\n            except ValidationError as error:\n                raise ValueError(\"Setting {} has an invalid value: {}\".format(self.full_name, error))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming a path into an actual Python object.", "response": "def transform(self, path):\n        \"\"\"\n        Transform a path into an actual Python object.\n\n        The path can be arbitrary long. You can pass the path to a package,\n        a module, a class, a function or a global variable, as deep as you\n        want, as long as the deepest module is importable through\n        ``importlib.import_module`` and each object is obtainable through\n        the ``getattr`` method. Local objects will not work.\n\n        Args:\n            path (str): the dot-separated path of the object.\n\n        Returns:\n            object: the imported module or obtained object.\n        \"\"\"\n        if path is None or not path:\n            return None\n\n        obj_parent_modules = path.split(\".\")\n        objects = [obj_parent_modules.pop(-1)]\n\n        while True:\n            try:\n                parent_module_path = \".\".join(obj_parent_modules)\n                parent_module = importlib.import_module(parent_module_path)\n                break\n            except ImportError:\n                if len(obj_parent_modules) == 1:\n                    raise ImportError(\"No module named '%s'\" % obj_parent_modules[0])\n                objects.insert(0, obj_parent_modules.pop(-1))\n\n        current_object = parent_module\n        for obj in objects:\n            current_object = getattr(current_object, obj)\n        return current_object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_value(self):\n        try:\n            self.raw_value\n        except (AttributeError, KeyError) as err:\n            self._reraise_if_required(err)\n            default_value = self.default_value\n            if self.transform_default:\n                return self.transform(default_value)\n            return default_value\n        else:\n            # If setting is defined, load values of all subsettings.\n            value = {}\n            for key, subsetting in self.settings.items():\n                value[key] = subsetting.get_value()\n            return value", "response": "Get the value of the attribute in the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check(self):\n        super(NestedSetting, self).check()\n        errors = []\n        for subsetting in self.settings.values():\n            try:\n                subsetting.check()\n            except ValidationError as error:\n                errors.extend(error.messages)\n        if errors:\n            raise ValidationError(errors)", "response": "Run the setting checker against the raw value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsum an array of magnitudes in flux space.", "response": "def sum_mags(mags, weights=None):\n    \"\"\"\n    Sum an array of magnitudes in flux space.\n\n    Parameters:\n    -----------\n    mags    : array of magnitudes\n    weights : array of weights for each magnitude (i.e. from a pdf)\n    \n    Returns:\n    --------\n    sum_mag : the summed magnitude of all the stars\n    \"\"\"\n    flux = 10**(-np.asarray(mags) / 2.5)\n    if weights is None:\n        return -2.5 * np.log10(np.sum(flux))\n    else:\n        return -2.5 * np.log10(np.sum(weights*flux))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef absolute_magnitude(distance_modulus,g,r,prob=None):\n    V = g - 0.487*(g - r) - 0.0249\n        \n    flux = np.sum(10**(-(V-distance_modulus)/2.5))\n    Mv = -2.5*np.log10(flux)\n    return Mv", "response": "Calculate the absolute magnitude of a single node in a set of bands."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsamples the isochrone in steps of mass interpolating between the isochrone points.", "response": "def sample(self, mode='data', mass_steps=1000, mass_min=0.1, full_data_range=False):\n        \"\"\"Sample the isochrone in steps of mass interpolating between\n        the originally defined isochrone points.\n\n        Parameters:\n        -----------\n        mode : \n        mass_steps : \n        mass_min : Minimum mass [Msun]\n        full_data_range :\n        \n        Returns:\n        --------\n        mass_init : Initial mass of each point\n        mass_pdf : PDF of number of stars in each point\n        mass_act : Actual (current mass) of each stellar point\n        mag_1 : Array of absolute magnitudes in first band (no distance modulus applied)\n        mag_2 : Array of absolute magnitudes in second band (no distance modulus applied)\n        \"\"\"\n\n        if full_data_range:\n            # ADW: Might be depricated 02/10/2015\n            # Generate points over full isochrone data range\n            select = slice(None)\n        else:\n            # Not generating points for the post-AGB stars,\n            # but still count those stars towards the normalization\n            select = slice(self.index)\n\n        mass_steps = int(mass_steps)\n\n        mass_init = self.mass_init[select]\n        mass_act = self.mass_act[select]\n        mag_1 = self.mag_1[select]\n        mag_2 = self.mag_2[select]\n        \n        # ADW: Assume that the isochrones are pre-sorted by mass_init\n        # This avoids some numerical instability from points that have the same\n        # mass_init value (discontinuities in the isochrone).\n        # ADW: Might consider using np.interp for speed\n        mass_act_interpolation = scipy.interpolate.interp1d(mass_init, mass_act,assume_sorted=True)\n        mag_1_interpolation = scipy.interpolate.interp1d(mass_init, mag_1,assume_sorted=True)\n        mag_2_interpolation = scipy.interpolate.interp1d(mass_init, mag_2,assume_sorted=True)\n\n        # ADW: Any other modes possible?\n        if mode=='data':\n            # Mass interpolation with uniform coverage between data points from isochrone file \n            mass_interpolation = scipy.interpolate.interp1d(np.arange(len(mass_init)), mass_init)\n            mass_array = mass_interpolation(np.linspace(0, len(mass_init)-1, mass_steps+1))\n            d_mass = mass_array[1:] - mass_array[:-1]\n            mass_init_array = np.sqrt(mass_array[1:] * mass_array[:-1])\n            mass_pdf_array = d_mass * self.imf.pdf(mass_init_array, log_mode=False)\n            mass_act_array = mass_act_interpolation(mass_init_array)\n            mag_1_array = mag_1_interpolation(mass_init_array)\n            mag_2_array = mag_2_interpolation(mass_init_array)\n\n        # Horizontal branch dispersion\n        if self.hb_spread and (self.stage==self.hb_stage).any():\n            logger.debug(\"Performing dispersion of horizontal branch...\")\n            mass_init_min = self.mass_init[self.stage==self.hb_stage].min()\n            mass_init_max = self.mass_init[self.stage==self.hb_stage].max()\n            cut = (mass_init_array>mass_init_min)&(mass_init_array<mass_init_max)\n            if isinstance(self.hb_spread,collections.Iterable):\n                # Explicit dispersion spacing\n                dispersion_array = self.hb_spread\n                n = len(dispersion_array)\n            else:\n                # Default dispersion spacing\n                dispersion = self.hb_spread\n                spacing = 0.025\n                n = int(round(2.0*self.hb_spread/spacing))\n                if n % 2 != 1: n += 1\n                dispersion_array = np.linspace(-dispersion, dispersion, n)\n\n            # Reset original values\n            mass_pdf_array[cut] = mass_pdf_array[cut] / float(n)\n\n            # Isochrone values for points on the HB\n            mass_init_hb = mass_init_array[cut]\n            mass_pdf_hb = mass_pdf_array[cut]\n            mass_act_hb = mass_act_array[cut]\n            mag_1_hb = mag_1_array[cut]\n            mag_2_hb = mag_2_array[cut]\n\n            # Add dispersed values\n            for dispersion in dispersion_array:\n                if dispersion == 0.: continue\n                msg = 'Dispersion=%-.4g, HB Points=%i, Iso Points=%i'%(dispersion,cut.sum(),len(mass_init_array))\n                logger.debug(msg)\n\n                mass_init_array = np.append(mass_init_array, mass_init_hb) \n                mass_pdf_array = np.append(mass_pdf_array, mass_pdf_hb)\n                mass_act_array = np.append(mass_act_array, mass_act_hb) \n                mag_1_array = np.append(mag_1_array, mag_1_hb + dispersion)\n                mag_2_array = np.append(mag_2_array, mag_2_hb + dispersion)\n\n        # Note that the mass_pdf_array is not generally normalized to unity\n        # since the isochrone data range typically covers a different range\n        # of initial masses\n        #mass_pdf_array /= np.sum(mass_pdf_array) # ORIGINAL\n        # Normalize to the number of stars in the satellite with mass > mass_min\n        mass_pdf_array /= self.imf.integrate(mass_min, self.mass_init_upper_bound)\n        out = np.vstack([mass_init_array,mass_pdf_array,mass_act_array,mag_1_array,mag_2_array])\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stellar_mass(self, mass_min=0.1, steps=10000):\n        mass_max = self.mass_init_upper_bound\n            \n        d_log_mass = (np.log10(mass_max) - np.log10(mass_min)) / float(steps)\n        log_mass = np.linspace(np.log10(mass_min), np.log10(mass_max), steps)\n        mass = 10.**log_mass\n\n        if mass_min < np.min(self.mass_init):\n            mass_act_interpolation = scipy.interpolate.interp1d(np.insert(self.mass_init, 0, mass_min),\n                                                                np.insert(self.mass_act, 0, mass_min))\n        else:\n           mass_act_interpolation = scipy.interpolate.interp1d(self.mass_init, self.mass_act) \n\n        mass_act = mass_act_interpolation(mass)\n        return np.sum(mass_act * d_log_mass * self.imf.pdf(mass, log_mode=True))", "response": "Compute the stellar mass of the isochrone."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the stellar luminosity of the isochrone.", "response": "def stellar_luminosity(self, steps=10000):\n        \"\"\"\n        Compute the stellar luminosity (Lsun; average per star). PDF\n        comes from IMF.  The range of integration only covers the\n        input isochrone data (no extrapolation used), but this seems\n        like a sub-percent effect if the isochrone goes to 0.15 Msun\n        for the old and metal-poor stellar populations of interest.\n\n        Note that the stellar luminosity is very sensitive to the\n        post-AGB population.\n\n        Parameters:\n        -----------\n        steps : Number of steps to sample the isochrone.\n\n        Returns:\n        --------\n        lum   : The stellar luminosity [Lsun]\n        \"\"\"\n        mass_min = np.min(self.mass_init)\n        mass_max = self.mass_init_upper_bound\n        \n        d_log_mass = (np.log10(mass_max) - np.log10(mass_min)) / float(steps)\n        log_mass = np.linspace(np.log10(mass_min), np.log10(mass_max), steps)\n        mass = 10.**log_mass\n        \n        luminosity_interpolation = scipy.interpolate.interp1d(self.mass_init, self.luminosity,fill_value=0,bounds_error=False)\n        luminosity = luminosity_interpolation(mass)\n\n        return np.sum(luminosity * d_log_mass * self.imf.pdf(mass, log_mode=True))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the stellar luminosity of the current species.", "response": "def stellar_luminosity2(self, steps=10000):\n        \"\"\"\n        DEPRECATED: ADW 2017-09-20\n\n        Compute the stellar luminosity (L_Sol; average per star).\n        Uses \"sample\" to generate mass sample and pdf.  The range of\n        integration only covers the input isochrone data (no\n        extrapolation used), but this seems like a sub-percent effect\n        if the isochrone goes to 0.15 Msun for the old and metal-poor\n        stellar populations of interest.\n\n        Note that the stellar luminosity is very sensitive to the\n        post-AGB population.\n        \"\"\"\n        msg = \"'%s.stellar_luminosity2': ADW 2017-09-20\"%self.__class__.__name__\n        DeprecationWarning(msg)\n        mass_init, mass_pdf, mass_act, mag_1, mag_2 = self.sample(mass_steps=steps)\n        luminosity_interpolation = scipy.interpolate.interp1d(self.mass_init, self.luminosity,fill_value=0,bounds_error=False)\n        luminosity = luminosity_interpolation(mass_init)\n        return np.sum(luminosity * mass_pdf)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the absolute visual magnitude from the isochrone in the SDSS system and using the Jester 2005", "response": "def absolute_magnitude(self, richness=1, steps=1e4):\n        \"\"\"\n        Calculate the absolute visual magnitude (Mv) from the richness\n        by transforming the isochrone in the SDSS system and using the\n        g,r -> V transform equations from Jester 2005\n        [astro-ph/0506022].\n\n\n        Parameters:\n        -----------\n        richness : isochrone normalization parameter\n        steps    : number of isochrone sampling steps\n\n        Returns:\n        --------\n        abs_mag : Absolute magnitude (Mv)\n        \"\"\"\n        # Using the SDSS g,r -> V from Jester 2005 [astro-ph/0506022]\n        # for stars with R-I < 1.15\n        # V = g_sdss - 0.59*(g_sdss - r_sdss) - 0.01\n\n        # Create a copy of the isochrone in the SDSS system\n        params = {k:v.value for k,v in self._params.items()}\n        params.update(band_1='g',band_2='r',survey='sdss')\n        iso = self.__class__(**params)\n\n        # g, r are absolute magnitude\n        mass_init, mass_pdf, mass_act, sdss_g, sdss_r = iso.sample(mass_steps=steps)\n        V = jester_mag_v(sdss_g,sdss_r)\n\n        # Sum the V-band absolute magnitudes\n        return sum_mags(V,weights=mass_pdf*richness)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the absolute magnitude (Mv) of the isochrone using the prescription of Martin et al. 2008. ADW: Seems like the faint and bright limits should depend on the survey maglim? Parameters: ----------- richness : Isochrone nomalization factor steps : Number of steps for sampling the isochrone. n_trials : Number of bootstrap samples mag_bright : Bright magnitude limit [SDSS g-band] for luminosity calculation mag_faint : Faint magnitude limit [SDSS g-band] for luminosity calculation alpha : Output confidence interval (1-alpha) seed : Random seed Returns: -------- med,lo,hi : Total absolute magnitude interval", "response": "def absolute_magnitude_martin(self, richness=1, steps=1e4, n_trials=1000, mag_bright=None, mag_faint=23., alpha=0.32, seed=None):\n        \"\"\"\n        Calculate the absolute magnitude (Mv) of the isochrone using\n        the prescription of Martin et al. 2008.\n        \n        ADW: Seems like the faint and bright limits should depend on the survey maglim?\n\n        Parameters:\n        -----------\n        richness   : Isochrone nomalization factor\n        steps      : Number of steps for sampling the isochrone.\n        n_trials   : Number of bootstrap samples\n        mag_bright : Bright magnitude limit [SDSS g-band] for luminosity calculation\n        mag_faint  : Faint magnitude limit [SDSS g-band] for luminosity calculation\n        alpha      : Output confidence interval (1-alpha)\n        seed       : Random seed\n\n        Returns:\n        --------\n        med,lo,hi : Total absolute magnitude interval\n        \"\"\"\n        # ADW: This function is not quite right. It should restrict\n        # the catalog to the obsevable space using the mask in each\n        # pixel.  This becomes even more complicated when we transform\n        # the isochrone into SDSS g,r...\n        if seed is not None: np.random.seed(seed)\n\n        # Create a copy of the isochrone in the SDSS system\n        params = {k:v.value for k,v in self._params.items()}\n        params.update(band_1='g',band_2='r',survey='sdss')\n        iso = self.__class__(**params)\n\n        # Analytic part (below detection threshold)\n        # g, r are absolute magnitudes\n        mass_init, mass_pdf, mass_act, sdss_g, sdss_r = iso.sample(mass_steps = steps)\n        V = jester_mag_v(sdss_g, sdss_r)\n        cut = ( (sdss_g + iso.distance_modulus) > mag_faint)\n        mag_unobs = sum_mags(V[cut], weights = richness * mass_pdf[cut])\n\n        # Stochastic part (above detection threshold)\n        abs_mag_v = np.zeros(n_trials)\n        for i in range(n_trials):\n            if i%100==0: logger.debug('%i absolute magnitude trials'%i)\n            # g,r are apparent magnitudes\n            sdss_g, sdss_r = iso.simulate(richness * iso.stellar_mass())\n            cut = (sdss_g < mag_faint) \n            # V is absolute magnitude\n            V = jester_mag_v(sdss_g[cut]-iso.distance_modulus,\n                             sdss_r[cut]-iso.distance_modulus)\n            mag_obs = sum_mags(V)\n            abs_mag_v[i] = sum_mags([mag_obs,mag_unobs])\n\n        # ADW: Careful, fainter abs mag is larger (less negative) number\n        q = [100*alpha/2., 50, 100*(1-alpha/2.)]\n        hi,med,lo = np.percentile(abs_mag_v,q)\n        return ugali.utils.stats.interval(med,lo,hi)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simulate(self, stellar_mass, distance_modulus=None, **kwargs):\n        if distance_modulus is None: distance_modulus = self.distance_modulus\n        # Total number of stars in system\n        n = int(round(stellar_mass / self.stellar_mass()))\n        f_1 = scipy.interpolate.interp1d(self.mass_init, self.mag_1)\n        f_2 = scipy.interpolate.interp1d(self.mass_init, self.mag_2)\n        mass_init_sample = self.imf.sample(n, np.min(self.mass_init), np.max(self.mass_init), **kwargs)\n        mag_1_sample, mag_2_sample = f_1(mass_init_sample), f_2(mass_init_sample) \n        return mag_1_sample + distance_modulus, mag_2_sample + distance_modulus", "response": "Simulate a set of stellar magnitudes for a given satellite and distance modulus."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef observableFractionCMDX(self, mask, distance_modulus, mass_min=0.1):\n        mass_init_array,mass_pdf_array,mass_act_array,mag_1_array,mag_2_array = self.sample(mass_min=mass_min,full_data_range=False)\n        mag = mag_1_array if self.band_1_detection else mag_2_array\n        color = mag_1_array - mag_2_array\n\n        # ADW: Only calculate observable fraction over interior pixels...\n        pixels = mask.roi.pixels_interior\n        mag_1_mask = mask.mask_1.mask_roi_sparse[mask.roi.pixel_interior_cut]\n        mag_2_mask = mask.mask_2.mask_roi_sparse[mask.roi.pixel_interior_cut]\n\n        # ADW: Restrict mag and color to range of mask with sufficient solid angle\n        cmd_cut = ugali.utils.binning.take2D(mask.solid_angle_cmd,color,mag+distance_modulus,\n                                             mask.roi.bins_color, mask.roi.bins_mag) > 0\n        # Pre-apply these cuts to the 1D mass_pdf_array to save time\n        mass_pdf_cut = mass_pdf_array*cmd_cut\n\n        # Create 2D arrays of cuts for each pixel\n        mask_1_cut = (mag_1_array+distance_modulus)[:,np.newaxis] < mag_1_mask\n        mask_2_cut = (mag_2_array+distance_modulus)[:,np.newaxis] < mag_2_mask\n        mask_cut_repeat = mask_1_cut & mask_2_cut\n\n        observable_fraction = (mass_pdf_cut[:,np.newaxis]*mask_cut_repeat).sum(axis=0)\n        return observable_fraction", "response": "Compute observable fraction of stars with mass greater than mass_min in each \n        pixel in the interior region of the mask."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute observable fraction of stars with masses greater than mass_min in each pixel in the interior region of the mask.", "response": "def observableFractionCMD(self, mask, distance_modulus, mass_min=0.1):\n        \"\"\"\n        Compute observable fraction of stars with masses greater than mass_min in each \n        pixel in the interior region of the mask.\n\n        ADW: Careful, this function is fragile! The selection here should\n             be the same as mask.restrictCatalogToObservable space. However,\n             for technical reasons it is faster to do the calculation with\n             broadcasting here.\n        ADW: Could this function be even faster / more readable?\n        ADW: Should this include magnitude error leakage?\n        \"\"\"\n        if distance_modulus is None: distance_modulus = self.distance_modulus\n        mass_init,mass_pdf,mass_act,mag_1,mag_2 = self.sample(mass_min=mass_min,full_data_range=False)\n\n        mag = mag_1 if self.band_1_detection else mag_2\n        color = mag_1 - mag_2\n\n        # ADW: Only calculate observable fraction for unique mask values\n        mag_1_mask,mag_2_mask = mask.mask_roi_unique.T\n\n        # ADW: Restrict mag and color to range of mask with sufficient solid angle\n        cmd_cut = ugali.utils.binning.take2D(mask.solid_angle_cmd,color,mag+distance_modulus,\n                                             mask.roi.bins_color, mask.roi.bins_mag) > 0\n        # Pre-apply these cuts to the 1D mass_pdf_array to save time\n        mass_pdf_cut = mass_pdf*cmd_cut\n\n        # Create 2D arrays of cuts for each pixel\n        mask_1_cut = (mag_1+distance_modulus)[:,np.newaxis] < mag_1_mask\n        mask_2_cut = (mag_2+distance_modulus)[:,np.newaxis] < mag_2_mask\n        mask_cut_repeat = (mask_1_cut & mask_2_cut)\n\n        # Condense back into one per digi\n        observable_fraction = (mass_pdf_cut[:,np.newaxis]*mask_cut_repeat).sum(axis=0)\n\n        # Expand to the roi and multiply by coverage fraction\n        return observable_fraction[mask.mask_roi_digi[mask.roi.pixel_interior_cut]] * mask.frac_interior_sparse"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the observable fraction of stars with masses greater than mass_min in each pixel in the interior region of the mask.", "response": "def observableFractionCDF(self, mask, distance_modulus, mass_min=0.1):\n        \"\"\"\n        Compute observable fraction of stars with masses greater than mass_min in each \n        pixel in the interior region of the mask. Incorporates simplistic\n        photometric errors.\n\n        ADW: Careful, this function is fragile! The selection here should\n             be the same as mask.restrictCatalogToObservable space. However,\n             for technical reasons it is faster to do the calculation with\n             broadcasting here.\n        ADW: This function is currently a rate-limiting step in the likelihood \n             calculation. Could it be faster?\n        \"\"\"\n        method = 'step'\n\n        mass_init,mass_pdf,mass_act,mag_1,mag_2 = self.sample(mass_min=mass_min,full_data_range=False)\n         \n        mag_1 = mag_1+distance_modulus\n        mag_2 = mag_2+distance_modulus\n         \n        mask_1,mask_2 = mask.mask_roi_unique.T\n         \n        mag_err_1 = mask.photo_err_1(mask_1[:,np.newaxis]-mag_1)\n        mag_err_2 = mask.photo_err_2(mask_2[:,np.newaxis]-mag_2)\n         \n        # \"upper\" bound set by maglim\n        delta_hi_1 = (mask_1[:,np.newaxis]-mag_1)/mag_err_1\n        delta_hi_2 = (mask_2[:,np.newaxis]-mag_2)/mag_err_2\n         \n        # \"lower\" bound set by bins_mag (maglim shouldn't be 0)\n        delta_lo_1 = (mask.roi.bins_mag[0]-mag_1)/mag_err_1\n        delta_lo_2 = (mask.roi.bins_mag[0]-mag_2)/mag_err_2\n         \n        cdf_1 = norm_cdf(delta_hi_1) - norm_cdf(delta_lo_1)\n        cdf_2 = norm_cdf(delta_hi_2) - norm_cdf(delta_lo_2)\n        cdf = cdf_1*cdf_2\n         \n        if method is None or method == 'none':\n            comp_cdf = cdf\n        elif self.band_1_detection == True:\n            comp = mask.mask_1.completeness(mag_1, method=method)\n            comp_cdf = comp*cdf\n        elif self.band_1_detection == False:\n            comp =mask.mask_2.completeness(mag_2, method=method)\n            comp_cdf = comp*cdf\n        else:\n            comp_1 = mask.mask_1.completeness(mag_1, method=method)\n            comp_2 = mask.mask_2.completeness(mag_2, method=method)\n            comp_cdf = comp_1*comp_2*cdf\n         \n        observable_fraction = (mass_pdf[np.newaxis]*comp_cdf).sum(axis=-1)\n        return observable_fraction[mask.mask_roi_digi[mask.roi.pixel_interior_cut]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a 2D histogram of the isochrone in mag - mag space.", "response": "def histogram2d(self,distance_modulus=None,delta_mag=0.03,steps=10000):\n        \"\"\"\n        Return a 2D histogram the isochrone in mag-mag space.\n\n        Parameters:\n        -----------\n        distance_modulus : distance modulus to calculate histogram at\n        delta_mag : magnitude bin size\n        mass_steps : number of steps to sample isochrone at\n\n        Returns:\n        --------\n        bins_mag_1 : bin edges for first magnitude\n        bins_mag_2 : bin edges for second magnitude\n        isochrone_pdf : weighted pdf of isochrone in each bin\n        \"\"\"\n        if distance_modulus is not None:\n            self.distance_modulus = distance_modulus\n\n        # Isochrone will be binned, so might as well sample lots of points\n        mass_init,mass_pdf,mass_act,mag_1,mag_2 = self.sample(mass_steps=steps)\n\n        #logger.warning(\"Fudging intrinisic dispersion in isochrone.\")\n        #mag_1 += np.random.normal(scale=0.02,size=len(mag_1))\n        #mag_2 += np.random.normal(scale=0.02,size=len(mag_2))\n\n        # We cast to np.float32 to save memory\n        bins_mag_1 = np.arange(self.mod+mag_1.min() - (0.5*delta_mag),\n                               self.mod+mag_1.max() + (0.5*delta_mag),\n                               delta_mag).astype(np.float32)\n        bins_mag_2 = np.arange(self.mod+mag_2.min() - (0.5*delta_mag),\n                               self.mod+mag_2.max() + (0.5*delta_mag),\n                               delta_mag).astype(np.float32)\n \n        # ADW: Completeness needs to go in mass_pdf here...\n        isochrone_pdf = np.histogram2d(self.mod + mag_1,\n                                       self.mod + mag_2,\n                                       bins=[bins_mag_1, bins_mag_2],\n                                       weights=mass_pdf)[0].astype(np.float32)\n \n        return isochrone_pdf, bins_mag_1, bins_mag_2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pdf_mmd(self, lon, lat, mag_1, mag_2, distance_modulus, mask, delta_mag=0.03, steps=1000):\n        logger.info('Running MMD pdf')\n \n        roi = mask.roi\n        mmd = self.signalMMD(mask,distance_modulus,delta_mag=delta_mag,mass_steps=steps)\n        \n        # This is fragile, store this information somewhere else...\n        nedges = np.rint((roi.bins_mag[-1]-roi.bins_mag[0])/delta_mag)+1\n        edges_mag,delta_mag = np.linspace(roi.bins_mag[0],roi.bins_mag[-1],nedges,retstep=True)\n                                    \n        idx_mag_1 = np.searchsorted(edges_mag,mag_1)\n        idx_mag_2 = np.searchsorted(edges_mag,mag_2)\n \n        if np.any(idx_mag_1 > nedges) or np.any(idx_mag_1 == 0):\n            msg = \"Magnitude out of range...\"\n            raise Exception(msg)\n        if np.any(idx_mag_2 > nedges) or np.any(idx_mag_2 == 0):\n            msg = \"Magnitude out of range...\"\n            raise Exception(msg)\n \n        idx = mask.roi.indexROI(lon,lat)\n        u_color = mmd[(mask.mask_roi_digi[idx],idx_mag_1,idx_mag_2)]\n \n        # Remove the bin size to convert the pdf to units of mag^-2\n        u_color /= delta_mag**2\n \n        return u_color", "response": "This function is used to compute the MMD of a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the pdf of the given magnitude and error of the given stars.", "response": "def pdf(self, mag_1, mag_2, mag_err_1, mag_err_2, \n            distance_modulus=None, delta_mag=0.03, steps=10000):\n        \"\"\"\n        Compute isochrone probability for each catalog object.\n \n        ADW: This is a memory intensive function, so try as much as\n        possible to keep array types at `float32` or smaller (maybe\n        using add.at would be good?)\n        ADW: Still a little speed to be gained here (broadcasting)\n        ADW: Units? [mag^-2] [per sr?]\n\n        Parameters:\n        -----------\n        mag_1 : magnitude of stars (pdf sample points) in first band\n        mag_2 : magnitude of stars (pdf sample points) in second band\n        mag_err_1 : magnitude error of stars (pdf sample points) in first band\n        mag_err_2 : magnitude error of stars (pdf sample points) in second band\n        distance_modulus : distance modulus of isochrone\n        delta_mag : magnitude binning for evaluating the pdf\n        steps : number of isochrone sample points\n\n        Returns:\n        --------\n        u_color : probability that the star belongs to the isochrone [mag^-2]\n        \"\"\"\n        nsigma = 5.0\n        #pad = 1. # mag\n\n        if distance_modulus is None: \n            distance_modulus = self.distance_modulus\n\n        # ADW: HACK TO ADD SYSTEMATIC UNCERTAINTY (0.010 mag)\n        mag_err_1 = np.sqrt(mag_err_1**2 + 0.01**2)\n        mag_err_2 = np.sqrt(mag_err_2**2 + 0.01**2)\n \n        # Binned pdf of the isochrone\n        histo_pdf,bins_mag_1,bins_mag_2 = self.histogram2d(distance_modulus,delta_mag,steps)\n         \n        # Keep only isochrone bins that are within the magnitude\n        # space of the sample\n        mag_1_mesh, mag_2_mesh = np.meshgrid(bins_mag_2[1:], bins_mag_1[1:])\n         \n        # pdf contribution only calculated out to nsigma,\n        # so padding shouldn't be necessary.\n        mag_1_max = np.max(mag_1+nsigma*mag_err_1)# +pad \n        mag_1_min = np.min(mag_1-nsigma*mag_err_1)# -pad \n        mag_2_max = np.max(mag_2+nsigma*mag_err_2)# +pad \n        mag_2_min = np.min(mag_2-nsigma*mag_err_2)# -pad \n         \n        in_mag_space = ((mag_1_mesh>=mag_1_min)&(mag_1_mesh<=mag_1_max))\n        in_mag_space*= ((mag_2_mesh>=mag_2_min)&(mag_2_mesh<=mag_2_max))\n        histo_pdf *= in_mag_space\n \n        idx_mag_1, idx_mag_2 = np.nonzero(histo_pdf)\n        isochrone_pdf = histo_pdf[idx_mag_1, idx_mag_2]\n \n        n_catalog = len(mag_1)\n        n_isochrone_bins = len(idx_mag_1)\n\n        mag_1 = mag_1.reshape([n_catalog, 1])\n        mag_err_1 = mag_err_1.reshape([n_catalog, 1])\n        mag_2 = mag_2.reshape([n_catalog, 1])\n        mag_err_2 = mag_err_2.reshape([n_catalog, 1])\n\n        # Calculate (normalized) distance between each catalog object\n        # and isochrone bin. Assume normally distributed photometric\n        # uncertainties so that the normalized distance is:\n        #   norm_dist = (mag_1 - bins_mag_1)/mag_err_1\n\n        # ADW: Creating the dist arrays is memory intensive.\n        # Can we cut it down (maybe with add.at)?\n        dist_mag_1_hi = (mag_1-bins_mag_1[idx_mag_1])/mag_err_1\n        dist_mag_1_lo = (mag_1-bins_mag_1[idx_mag_1+1])/mag_err_1\n\n        dist_mag_2_hi = (mag_2-bins_mag_2[idx_mag_2])/mag_err_2\n        dist_mag_2_lo = (mag_2-bins_mag_2[idx_mag_2+1])/mag_err_2\n         \n        # Only calculate the PDF using bins that are < nsigma from the\n        # data point (i.e., where it is ~nonzero).\n        idx_nonzero_0,idx_nonzero_1 = np.nonzero((dist_mag_1_hi > -nsigma) \\\n                                                *(dist_mag_1_lo < nsigma)\\\n                                                *(dist_mag_2_hi > -nsigma)\\\n                                                *(dist_mag_2_lo < nsigma))\n\n        # Now calculate the pdf as the delta of the normalized cdf\n        # (more accurate than the point evaluation of the pdf)\n        pdf_mag_1 = np.zeros([n_catalog, n_isochrone_bins],dtype=np.float32)\n        pdf_mag_1[idx_nonzero_0,idx_nonzero_1] = norm_cdf(dist_mag_1_hi[idx_nonzero_0,idx_nonzero_1]) \\\n            - norm_cdf(dist_mag_1_lo[idx_nonzero_0,idx_nonzero_1])\n\n        pdf_mag_2 = np.zeros([n_catalog, n_isochrone_bins],dtype=np.float32)\n        pdf_mag_2[idx_nonzero_0,idx_nonzero_1] = norm_cdf(dist_mag_2_hi[idx_nonzero_0,idx_nonzero_1]) \\\n            - norm_cdf(dist_mag_2_lo[idx_nonzero_0,idx_nonzero_1])\n\n        # Signal \"color probability\" (as opposed to \"spatial\n        # probability\", but more accurately \"isochrone probability\")\n        # is the product of PDFs for each object-bin pair summed over\n        # isochrone bins \n\n        #ADW: Here is where add.at would be good...\n        u_color = np.sum(pdf_mag_1 * pdf_mag_2 * isochrone_pdf, axis=1)\n \n        # Remove the bin size to convert the pdf to units of mag^-2\n        u_color /= delta_mag**2\n\n        return u_color.astype(np.float32)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef raw_separation(self,mag_1,mag_2,steps=10000):\n     \n        # http://stackoverflow.com/q/12653120/\n        mag_1 = np.array(mag_1,copy=False,ndmin=1)\n        mag_2 = np.array(mag_2,copy=False,ndmin=1)\n     \n        init,pdf,act,iso_mag_1,iso_mag_2 = self.sample(mass_steps=steps)\n        iso_mag_1+=self.distance_modulus\n        iso_mag_2+=self.distance_modulus\n     \n        iso_cut = (iso_mag_1<np.max(mag_1))&(iso_mag_1>np.min(mag_1)) | \\\n                  (iso_mag_2<np.max(mag_2))&(iso_mag_2>np.min(mag_2))\n        iso_mag_1 = iso_mag_1[iso_cut]\n        iso_mag_2 = iso_mag_2[iso_cut]\n         \n        dist_mag_1 = mag_1[:,np.newaxis]-iso_mag_1\n        dist_mag_2 = mag_2[:,np.newaxis]-iso_mag_2\n        \n        return np.min(np.sqrt(dist_mag_1**2 + dist_mag_2**2),axis=1)", "response": "Calculates the raw separation between two test points."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef separation(self, mag_1, mag_2):\n\n        iso_mag_1 = self.mag_1 + self.distance_modulus\n        iso_mag_2 = self.mag_2 + self.distance_modulus\n        \n        def interp_iso(iso_mag_1,iso_mag_2,mag_1,mag_2):\n            interp_1 = scipy.interpolate.interp1d(iso_mag_1,iso_mag_2,bounds_error=False)\n            interp_2 = scipy.interpolate.interp1d(iso_mag_2,iso_mag_1,bounds_error=False)\n\n            dy = interp_1(mag_1) - mag_2\n            dx = interp_2(mag_2) - mag_1\n\n            dmag_1 = np.fabs(dx*dy) / (dx**2 + dy**2) * dy\n            dmag_2 = np.fabs(dx*dy) / (dx**2 + dy**2) * dx\n\n            return dmag_1, dmag_2\n\n        # Separate the various stellar evolution stages\n        if np.issubdtype(self.stage.dtype,np.number):\n            sel = (self.stage < self.hb_stage)\n        else:\n            sel = (self.stage != self.hb_stage)\n\n        # First do the MS/RGB\n        rgb_mag_1 = iso_mag_1[sel]\n        rgb_mag_2 = iso_mag_2[sel]\n        dmag_1,dmag_2 = interp_iso(rgb_mag_1,rgb_mag_2,mag_1,mag_2)\n\n        # Then do the HB (if it exists)\n        if not np.all(sel):\n            hb_mag_1 = iso_mag_1[~sel]\n            hb_mag_2 = iso_mag_2[~sel]\n\n            hb_dmag_1,hb_dmag_2 = interp_iso(hb_mag_1,hb_mag_2,mag_1,mag_2)\n\n            dmag_1 = np.nanmin([dmag_1,hb_dmag_1],axis=0)\n            dmag_2 = np.nanmin([dmag_2,hb_dmag_2],axis=0)\n\n        #return dmag_1,dmag_2\n        return np.sqrt(dmag_1**2 + dmag_2**2)", "response": "Calculates the separation between two test points in magnitude - magnitude space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download(self,age=None,metallicity=None,outdir=None,force=False):\n        try:\n            from urllib.error import URLError\n        except ImportError:\n            from urllib2 import URLError\n\n        if age is None: age = float(self.age)\n        if metallicity is None: metallicity = float(self.metallicity)\n\n        if outdir is None: outdir = './'\n        basename = self.params2filename(age,metallicity)\n        outfile = os.path.join(outdir,basename)\n            \n        if os.path.exists(outfile) and not force:\n            try:\n                self.verify(outfile,self.survey,age,metallicity)\n                logger.info(\"Found %s; skipping...\"%(outfile))\n                return\n            except Exception as e:\n                msg = \"Overwriting corrupted %s...\"%(outfile)\n                logger.warn(msg)\n                os.remove(outfile)\n                \n        mkdir(outdir)\n\n        self.print_info(age,metallicity)\n        self.query_server(outfile,age,metallicity)\n\n        if not os.path.exists(outfile):\n            raise RuntimeError('Download failed')\n\n        try:\n            self.verify(outfile,self.survey,age,metallicity)\n        except Exception as e:\n            msg = \"Output file is corrupted.\"\n            logger.error(msg)\n            msg = \"Removing %s.\"%outfile\n            logger.info(msg)\n            os.remove(outfile)\n            raise(e)\n\n        return outfile", "response": "Download isochrones from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the router s URLs ready to be installed in urlpatterns.", "response": "def get_urls(\n        self,\n        root_view_name=None,\n        optional_trailing_slash=False,\n        decorate=(),\n        name_template='{name}',\n    ):\n        \"\"\"\n        Get the router's URLs, ready to be installed in `urlpatterns` (directly or via `include`).\n\n        :param root_view_name: The optional url name for an API root view.\n                               This may be useful for projects that do not explicitly know where the\n                               router is mounted; those projects can then use `reverse('api:root')`,\n                               for instance, if they need to construct URLs based on the API's root URL.\n        :type root_view_name: str|None\n\n        :param optional_trailing_slash: Whether to fix up the regexen for the router to make any trailing\n                                        slashes optional.\n        :type optional_trailing_slash: bool\n\n        :param decorate: A function to decorate view functions with, or an iterable of such decorators.\n                         Use `(lepo.decorators.csrf_exempt,)` to mark all API views as CSRF exempt.\n        :type decorate: function|Iterable[function]\n\n        :param name_template: A `.format()` template for view naming.\n        :type name_template: str\n\n        :return: List of URL tuples.\n        :rtype: list[tuple]\n        \"\"\"\n        if isinstance(decorate, Iterable):\n            decorators = decorate\n\n            def decorate(view):\n                return reduce(lambda view, decorator: decorator(view), decorators, view)\n\n        urls = []\n        for path in self.api.get_paths():\n            regex = path.regex\n            if optional_trailing_slash:\n                regex = regex.rstrip('$')\n                if not regex.endswith('/'):\n                    regex += '/'\n                regex += '?$'\n            view = decorate(path.get_view_class(router=self).as_view())\n            urls.append(url(regex, view, name=name_template.format(name=path.name)))\n\n        if root_view_name:\n            urls.append(url(r'^$', root_view, name=name_template.format(name=root_view_name)))\n        return urls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_handler(self, operation_id):\n        handler = (\n            self.handlers.get(operation_id)\n            or self.handlers.get(snake_case(operation_id))\n        )\n        if handler:\n            return handler\n        raise MissingHandler(\n            'Missing handler for operation %s (tried %s too)' % (operation_id, snake_case(operation_id))\n        )", "response": "Get the handler function for a given operation ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_handlers(self, namespace):\n        if isinstance(namespace, str):\n            namespace = import_module(namespace)\n\n        if isinstance(namespace, dict):\n            namespace = namespace.items()\n        else:\n            namespace = vars(namespace).items()\n\n        for name, value in namespace:\n            if name.startswith('_'):\n                continue\n            if isfunction(value) or ismethod(value):\n                self.handlers[name] = value", "response": "Adds handler functions from the given namespace for instance a module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary that can be used in evaluating the expression.", "response": "def get_context(self, arr, expr, context):\n        \"\"\"\n        Returns a context dictionary for use in evaluating the expression.\n\n        :param arr: The input array.\n        :param expr: The input expression.\n        :param context: Evaluation context.\n        \"\"\"\n\n        expression_names = [x for x in self.get_expression_names(expr) if x not in set(context.keys()).union(['i'])]\n\n        if len(expression_names) != 1:\n            raise ValueError('The expression must have exactly one variable.')\n\n        return {expression_names[0]: arr}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self, array_in, expression, **kwargs):\n\n        context = self.get_context(array_in, expression, kwargs)\n        context.update(kwargs)\n        return ma.masked_where(self.evaluate_expression(expression, context), array_in)", "response": "Creates and returns a masked view of the input array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef timeout_error(url, timeout):\n    msg = 'Request timed out: {} timeout: {}s'.format(url, timeout)\n    log.warning(msg)\n    return ServerError(msg)", "response": "Raise a server error indicating a request timed out."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Gregory(type=float):\n\treturn seq(type(1), step=2) >> map(lambda x: 1/x) >> alt_sign >> fold(operator.add)", "response": "Return partial sums of the Gregory series converging to atan ( 1 ) == pi"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naccelerating convergence of the a series using Aitken s delta - squared process.", "response": "def Aitken(s):\n\t\"\"\"Accelerate the convergence of the a series\n\tusing Aitken's delta-squared process (SCIP calls it Euler).\n\t\"\"\"\n\tdef accel():\n\t\ts0, s1, s2 = s >> item[:3]\n\t\twhile 1:\n\t\t\tyield s2 - (s2 - s1)**2 / (s0 - 2*s1 + s2)\n\t\t\ts0, s1, s2 = s1, s2, next(s)\n\treturn accel()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nuploading an array to a personal database using the SOAP POST protocol.", "response": "def upload(self, array, fields=None, table=\"MyDB\", configfile=None):\n        \"\"\"\n        Upload an array to a personal database using SOAP POST protocol.\n        http://skyserver.sdss3.org/casjobs/services/jobs.asmx?op=UploadData\n        \"\"\"\n\n        wsid=''\n        password=''\n        if configfile is None:\n            configfile = \"CasJobs.config\"\n        logger.info(\"Reading config file: %s\"%configfile)\n        lines = open(configfile,'r').readlines()\n        for line in lines:\n            k,v = line.strip().split('=')\n            if k == 'wsid': wsid = v\n            if k == 'password': password = v\n\n        logger.info(\"Attempting to drop table: %s\"%table)\n        self.drop(table)\n     \n        SOAP_TEMPLATE = \"\"\"\n        <soap12:Envelope xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n                         xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" \n                         xmlns:soap12=\"http://www.w3.org/2003/05/soap-envelope\">\n          <soap12:Body>\n            <UploadData xmlns=\"http://Services.Cas.jhu.edu\">\n              <wsid>%s</wsid>\n              <pw>%s</pw>\n              <tableName>%s</tableName>\n              <data>%s</data>\n              <tableExists>%s</tableExists>\n            </UploadData>\n          </soap12:Body>\n        </soap12:Envelope>\n        \"\"\"\n     \n        logger.info(\"Writing array...\")\n        s = io.StringIO()\n        np.savetxt(s,array,delimiter=',',fmt=\"%.10g\")\n        tb_data = ''\n        if fields is not None: \n            tb_data += ','.join(f for f in fields)+'\\n'\n        tb_data += s.getvalue()\n     \n        message = SOAP_TEMPLATE % (wsid, password, table, tb_data, \"false\")\n        \n        #construct and send the header\n        webservice = httpcl.HTTP(\"skyserver.sdss3.org\")\n        webservice.putrequest(\"POST\", \"/casjobs/services/jobs.asmx\")\n        webservice.putheader(\"Host\", \"skyserver.sdss3.org\")\n        webservice.putheader(\"Content-type\", \"text/xml; charset=\\\"UTF-8\\\"\")\n        webservice.putheader(\"Content-length\", \"%d\" % len(message))\n        webservice.endheaders()\n        logger.info(\"Sending SOAP POST message...\")\n        webservice.send(message)\n         \n        # get the response\n        statuscode, statusmessage, header = webservice.getreply()\n        print(\"Response: \", statuscode, statusmessage)\n        print(\"headers: \", header)\n        res = webservice.getfile().read()\n        print(res)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef footprint(self,nside):\n        import healpy\n        import ugali.utils.projector\n        if nside > 2**9: raise Exception(\"Overflow error: nside must be <=2**9\")\n        pix = np.arange(healpy.nside2npix(nside),dtype='int')\n        footprint = np.zeros(healpy.nside2npix(nside),dtype='bool')\n        ra,dec = ugali.utils.projector.pixToAng(nside,pix)\n        table_name = 'Pix%i'%nside\n        self.upload(np.array([pix,ra,dec]).T, ['pix','ra','dec'], name=table_name)\n        radius = healpy.nside2resol(nside_superpix,arcmin=True)\n\n        query=\"\"\"\n        SELECT t.pix, dbo.fInFootprintEq(t.ra, t.dec, %g)\n        FROM %s AS t\n        \"\"\"%(radius, table_name)", "response": "Download the survey footprint for HEALpix pixels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef histogram(title, title_x, title_y,\n              x, bins_x):\n    \"\"\"\n    Plot a basic histogram.\n    \"\"\"\n    plt.figure()\n    plt.hist(x, bins_x)\n    plt.xlabel(title_x)\n    plt.ylabel(title_y)\n    plt.title(title)", "response": "Plots a histogram plot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a two - dimension histogram plot.", "response": "def twoDimensionalHistogram(title, title_x, title_y,\n                            z, bins_x, bins_y,\n                            lim_x=None, lim_y=None,\n                            vmin=None, vmax=None):\n    \"\"\"\n    Create a two-dimension histogram plot or binned map.\n\n    If using the outputs of np.histogram2d, remember to transpose the histogram.\n\n    INPUTS\n    \"\"\"\n    plt.figure()\n\n    mesh_x, mesh_y = np.meshgrid(bins_x, bins_y)\n\n    if vmin != None and vmin == vmax:\n        plt.pcolor(mesh_x, mesh_y, z)\n    else:\n        plt.pcolor(mesh_x, mesh_y, z, vmin=vmin, vmax=vmax)\n    plt.xlabel(title_x)\n    plt.ylabel(title_y)\n    plt.title(title)\n    plt.colorbar()\n\n    if lim_x:\n        plt.xlim(lim_x[0], lim_x[1])\n    if lim_y:\n        plt.ylim(lim_y[0], lim_y[1])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef twoDimensionalScatter(title, title_x, title_y,\n                          x, y,\n                          lim_x = None, lim_y = None,\n                          color = 'b', size = 20, alpha=None):\n    \"\"\"\n    Create a two-dimensional scatter plot.\n\n    INPUTS\n    \"\"\"\n    plt.figure()\n\n    plt.scatter(x, y, c=color, s=size, alpha=alpha, edgecolors='none')\n    \n    plt.xlabel(title_x)\n    plt.ylabel(title_y)\n    plt.title(title)\n    if type(color) is not str:\n        plt.colorbar()\n\n    if lim_x:\n        plt.xlim(lim_x[0], lim_x[1])\n    if lim_y:\n        plt.ylim(lim_y[0], lim_y[1])", "response": "Create a two - dimensional scatter plot."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nzoom a HEALPix map in a Zoomed HealPix map.", "response": "def zoomedHealpixMap(title, map, lon, lat, radius,\n                     xsize=1000, **kwargs):\n    \"\"\"\n    Inputs: lon (deg), lat (deg), radius (deg)\n    \"\"\"\n    reso = 60. * 2. * radius / xsize # Deg to arcmin\n    hp.gnomview(map=map, rot=[lon, lat, 0], title=title, xsize=xsize, reso=reso, degree=False, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef projScatter(lon, lat, **kwargs):\n    hp.projscatter(lon, lat, lonlat=True, **kwargs)", "response": "projScatter is a wrapper around hp. projscatter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sparseHealpixFiles(title, infiles, field='MAGLIM',**kwargs):\n                       \n    \"\"\"\n    Inputs: field\n    \"\"\"\n    #map = ugali.utils.skymap.readSparseHealpixMaps(infiles,field)\n    map = ugali.utils.skymap.read_partial_map(infiles,field)\n    ax = hp.mollview(map=map, title=title, **kwargs)\n    return ax, map", "response": "Returns a plot of a sparse HEALPix map."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing a healpix map.", "response": "def drawHealpixMap(hpxmap, lon, lat, size=1.0, xsize=501, coord='GC', **kwargs):\n    \"\"\"\n    Draw local projection of healpix map.\n    \"\"\"\n    ax = plt.gca()\n    x = np.linspace(-size,size,xsize)\n    y = np.linspace(-size,size,xsize)\n    xx, yy = np.meshgrid(x,y)\n    \n    coord = coord.upper()\n        \n    if coord == 'GC':\n        #Assumes map and (lon,lat) are Galactic, but plotting celestial\n        llon, llat = image2sphere(*gal2cel(lon,lat),x=xx.flat,y=yy.flat)\n        pix = ang2pix(get_nside(hpxmap),*cel2gal(llon,llat))\n    elif coord == 'CG':\n        #Assumes map and (lon,lat) are celestial, but plotting Galactic\n        llon, llat = image2sphere(*cel2gal(lon,lat),x=xx.flat,y=yy.flat)\n        pix = ang2pix(get_nside(hpxmap),*gal2cel(llon,llat))\n    else:\n        #Assumes plotting the native coordinates\n        llon, llat = image2sphere(lon,lat,xx.flat,yy.flat)\n        pix = ang2pix(get_nside(hpxmap),llon,llat)\n\n    values = hpxmap[pix].reshape(xx.shape)\n    zz = np.ma.array(values,mask=(values==hp.UNSEEN),fill_value=np.nan)\n\n    return drawProjImage(xx,yy,zz,coord=coord,**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getSDSSImage(ra,dec,radius=1.0,xsize=800,opt='GML',**kwargs):\n    import subprocess\n    import tempfile\n\n    url=\"http://skyservice.pha.jhu.edu/DR10/ImgCutout/getjpeg.aspx?\"\n    scale = 2. * radius * 3600. / xsize\n    params=dict(ra=ra,dec=dec,\n                width=xsize,height=xsize,\n                scale=scale,opt=opt)\n    query='&'.join(\"%s=%s\"%(k,v) for k,v in params.items())\n    \n    tmp = tempfile.NamedTemporaryFile(suffix='.jpeg')\n    cmd='wget --progress=dot:mega -O %s \"%s\"'%(tmp.name,url+query)\n    subprocess.call(cmd,shell=True)\n    im = plt.imread(tmp.name)\n    tmp.close()\n    return im", "response": "Download and return the Sloan Digital Sky Survey images from the DSS server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getDSSImage(ra,dec,radius=1.0,xsize=800,**kwargs):\n    import subprocess\n    import tempfile\n    service = 'skyview'\n\n    if service == 'stsci':\n        url=\"https://archive.stsci.edu/cgi-bin/dss_search?\"\n        scale = 2.0 * radius * 60.\n        params=dict(ra='%.3f'%ra,dec='%.3f'%dec,width=scale,height=scale,\n                    format='gif',version=1)\n        #v='poss2ukstu_red'\n    elif service == 'skyview':\n        url=\"https://skyview.gsfc.nasa.gov/cgi-bin/images?\"\n        params=dict(survey='DSS',position='%.3f,%.3f'%(ra,dec),scaling='Linear',\n                    Return='GIF',size=2*radius,projection='Car',pixels=xsize)\n    else:\n        raise Exception(\"Unrecognized service.\")\n\n    query='&'.join(\"%s=%s\"%(k,v) for k,v in params.items())\n    \n    tmp = tempfile.NamedTemporaryFile(suffix='.gif')\n    cmd='wget --progress=dot:mega -O %s \"%s\"'%(tmp.name,url+query)\n    subprocess.call(cmd,shell=True)\n    im = plt.imread(tmp.name)\n    tmp.close()\n    if service == 'stsci' and xsize: \n        im = scipy.misc.imresize(im,size=(xsize,xsize))\n    return im", "response": "Download Digitized Sky Survey images and display them in a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_slices(hist, func=np.sum, **kwargs):\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n    kwargs.setdefault('ls','-')\n    ax = plt.gca()\n\n    data = hist\n\n    # Slices\n    vslice = func(data,axis=0)\n    hslice = func(data,axis=1)\n\n    npix = np.array(data.shape)\n    #xlim,ylim = plt.array(zip([0,0],npix-1))\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    #extent = ax.get_extent()\n    #xlim =extent[:2]\n    #ylim = extent[2:]\n\n    # Bin centers\n    xbin = np.linspace(xlim[0],xlim[1],len(vslice))#+0.5 \n    ybin = np.linspace(ylim[0],ylim[1],len(hslice))#+0.5\n    divider = make_axes_locatable(ax)\n    \n    #gh2 = pywcsgrid2.GridHelperSimple(wcs=self.header, axis_nums=[2, 1])\n    hax = divider.append_axes(\"right\", size=1.2, pad=0.05,sharey=ax,\n                              axes_class=axes_divider.LocatableAxes)\n    hax.axis[\"left\"].toggle(label=False, ticklabels=False)\n    #hax.plot(hslice, plt.arange(*ylim)+0.5,'-') # Bin center\n    hax.plot(hslice, ybin, **kwargs) # Bin center\n    hax.xaxis.set_major_locator(MaxNLocator(4,prune='both'))\n    hax.set_ylim(*ylim)\n\n    #gh1 = pywcsgrid2.GridHelperSimple(wcs=self.header, axis_nums=[0, 2])\n    vax = divider.append_axes(\"top\", size=1.2, pad=0.05, sharex=ax,\n                              axes_class=axes_divider.LocatableAxes)\n    vax.axis[\"bottom\"].toggle(label=False, ticklabels=False)\n    vax.plot(xbin, vslice, **kwargs) \n    vax.yaxis.set_major_locator(MaxNLocator(4,prune='lower'))\n    vax.set_xlim(*xlim)\n\n    return vax,hax", "response": "Draw horizontal and vertical slices through histogram"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plotSkymapCatalog(lon,lat,**kwargs):\n    fig = plt.figure()\n    ax = plt.subplot(111,projection=projection)\n    drawSkymapCatalog(ax,lon,lat,**kwargs)", "response": "Plot a catalog of coordinates on a full - sky map."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a path from two arrays of x and y.", "response": "def makePath(x_path, y_path, epsilon=1.e-10):\n    \"\"\"\n    Create closed path.\n    \"\"\"\n    x_path_closed = np.concatenate([x_path, x_path[::-1]])\n    y_path_closed = np.concatenate([y_path, epsilon + y_path[::-1]])\n    path = matplotlib.path.Path(list(zip(x_path_closed, y_path_closed)))\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncut the path from g to r_err and r_err to the isochrone.", "response": "def cutIsochronePath(g, r, g_err, r_err, isochrone, radius=0.1, return_all=False):\n    \"\"\"\n    Cut to identify objects within isochrone cookie-cutter.\n\n    ADW: This should be moved into the isochrone class.\n    \"\"\"\n    import scipy.interpolate\n    from ugali.isochrone import CompositeIsochrone\n\n    if isinstance(isochrone, CompositeIsochrone):\n        isochrone = isochrone.isochrones[0]\n\n    if len(g) == 0:\n        return np.array([],dtype=bool)\n\n    try:\n        if np.all(isochrone.stage == 'Main'):\n            # Dotter case\n            index_transition = len(isochrone.stage)\n        else:\n            # Other cases\n            index_transition = np.nonzero(isochrone.stage > 3)[0][0] + 1    \n    except AttributeError:\n        index_transition = 1\n\n    mag_1_rgb = isochrone.mag_1[0: index_transition] + isochrone.distance_modulus\n    mag_2_rgb = isochrone.mag_2[0: index_transition] + isochrone.distance_modulus\n    mag_1_rgb = mag_1_rgb[::-1]\n    mag_2_rgb = mag_2_rgb[::-1]\n    \n    # Cut one way...\n    f_isochrone = scipy.interpolate.interp1d(mag_2_rgb, mag_1_rgb - mag_2_rgb, bounds_error=False, fill_value = 999.)\n    color_diff = np.fabs((g - r) - f_isochrone(r))\n    cut_2 = (color_diff < np.sqrt(0.1**2 + r_err**2 + g_err**2))\n\n     # ...and now the other\n    f_isochrone = scipy.interpolate.interp1d(mag_1_rgb, mag_1_rgb - mag_2_rgb, bounds_error=False, fill_value = 999.)\n    color_diff = np.fabs((g - r) - f_isochrone(g))\n    cut_1 = (color_diff < np.sqrt(0.1**2 + r_err**2 + g_err**2))\n\n    cut = np.logical_or(cut_1, cut_2)\n    \n    # Include horizontal branch if it exists\n    if not np.any(isochrone.stage == isochrone.hb_stage):\n        index_transition = np.nonzero(isochrone.stage==isochrone.hb_stage)[0][0]+1\n        mag_1_hb = isochrone.mag_1[index_transition:] + isochrone.distance_modulus\n        mag_2_hb = isochrone.mag_2[index_transition:] + isochrone.distance_modulus\n        path_hb = makePath(mag_1_hb, mag_2_hb)\n        cut_hb = path_hb.contains_points(list(zip(g, r)), radius=0.1)\n        logger.debug('Applying HB selection')\n        logger.debug(np.sum(cut))\n        cut = np.logical_or(cut, cut_hb)\n        logger.debug(np.sum(cut))\n\n    mag_bins = np.arange(16., 24.1, 0.1)\n    mag_centers = 0.5 * (mag_bins[1:] + mag_bins[0:-1])\n    magerr = np.tile(0., len(mag_centers))\n    for ii in range(0, len(mag_bins) - 1):\n        cut_mag_bin = (g > mag_bins[ii]) & (g < mag_bins[ii + 1])\n        magerr[ii] = np.median(np.sqrt(0.1**2 + r_err[cut_mag_bin]**2 + g_err[cut_mag_bin]**2))\n\n    if return_all:\n        return cut, mag_centers[f_isochrone(mag_centers) < 100], (f_isochrone(mag_centers) + magerr)[f_isochrone(mag_centers) < 100], (f_isochrone(mag_centers) - magerr)[f_isochrone(mag_centers) < 100]\n    else:\n        return cut"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef drawMask(self,ax=None, mask=None, mtype='maglim'):\n        if not ax: ax = plt.gca()\n        if mask is None:\n            mask = ugali.analysis.loglike.createMask(self.config,roi=self.roi)\n\n        mask_map = hp.UNSEEN*np.ones(hp.nside2npix(self.nside))\n        if mtype.lower() == 'maglim':\n            mask_map[mask.roi.pixels] = mask.mask_1.mask_roi_sparse\n        elif mtype.lower() == 'fracdet':\n            mask_map[mask.roi.pixels] = mask.mask_1.frac_roi_sparse\n        else:\n            raise Exception(\"Unrecognized type: %s\"%mtype)\n        masked = (mask_map==hp.UNSEEN) | (mask_map==0)\n        mask_map = np.ma.array(mask_map,mask=masked,fill_value=np.nan)\n\n        im = drawHealpixMap(mask_map,self.lon,self.lat,self.radius,coord=self.coord)\n\n        try: cbar = ax.cax.colorbar(im)\n        except: cbar = plt.colorbar(im)\n        cbar.ax.set_xticklabels(cbar.ax.get_xticklabels(),rotation=90)\n        ax.annotate(mtype,**self.label_kwargs)\n        return im", "response": "Draw the maglim from the mask."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing configuration options out of an. ini file.", "response": "def parse(self, configManager, config):\n        \"\"\"\n        Parse configuration options out of an .ini configuration file.\n\n        Inputs: configManager - Our parent ConfigManager instance which is constructing the Config object.\n                config        - The _Config object containing configuration options populated thus far.\n\n        Outputs: A dictionary of new configuration options to add to the Config object.\n        \"\"\"\n\n        parser = ConfigParser.RawConfigParser()\n        configOptions = dict()\n        configFile = self._getConfigFile(config)\n\n        if configFile:\n            parser.readfp(configFile)\n\n            for section in parser.sections():\n                if self.sections is None or section in self.sections:\n                    configOptions.update(parser.items(section))\n\n        return configOptions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef createimage(app, srcdir, buildpath):\n    text = app.config.sphinxmark_text\n\n    # draw transparent background\n    width = app.config.sphinxmark_text_width\n    height = app.config.sphinxmark_text_spacing\n    img = Image.new('RGBA', (width, height), (255, 255, 255, 0))\n    d = ImageDraw.Draw(img)\n\n    # set font\n    fontfile = os.path.join(srcdir, 'arial.ttf')\n    font = ImageFont.truetype(fontfile, app.config.sphinxmark_text_size)\n\n    # set x y location for text\n    xsize, ysize = d.textsize(text, font)\n    LOG.debug('[sphinxmark] x = ' + str(xsize) + '\\ny = ' + str(ysize))\n    x = (width / 2) - (xsize / 2)\n    y = (height / 2) - (ysize / 2)\n\n    # add text to image\n    color = app.config.sphinxmark_text_color\n    d.text((x, y), text, font=font, fill=color)\n\n    # set opacity\n    img.putalpha(app.config.sphinxmark_text_opacity)\n\n    # rotate image\n    img = img.rotate(app.config.sphinxmark_text_rotation)\n\n    # save image\n    imagefile = 'textmark_' + text + '.png'\n    imagepath = os.path.join(buildpath, imagefile)\n    img.save(imagepath, 'PNG')\n    LOG.debug('[sphinxmark] Image saved to: ' + imagepath)\n\n    return(imagefile)", "response": "Create PNG image from string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef watermark(app, env):\n    if app.config.sphinxmark_enable is True:\n        LOG.info('adding watermark...', nonl=True)\n        buildpath, imagefile = getimage(app)\n        cssname = buildcss(app, buildpath, imagefile)\n        app.add_css_file(cssname)\n        LOG.info(' done')", "response": "Add watermark to the Sphinx application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring setup for Sphinx extension. :param app: Sphinx application context.", "response": "def setup(app):\n    \"\"\"\n    Configure setup for Sphinx extension.\n\n    :param app: Sphinx application context.\n    \"\"\"\n    app.add_config_value('sphinxmark_enable', False, 'html')\n    app.add_config_value('sphinxmark_div', 'default', 'html')\n    app.add_config_value('sphinxmark_border', None, 'html')\n    app.add_config_value('sphinxmark_repeat', True, 'html')\n    app.add_config_value('sphinxmark_fixed', False, 'html')\n    app.add_config_value('sphinxmark_image', 'default', 'html')\n    app.add_config_value('sphinxmark_text', 'default', 'html')\n    app.add_config_value('sphinxmark_text_color', (255, 0, 0), 'html')\n    app.add_config_value('sphinxmark_text_size', 100, 'html')\n    app.add_config_value('sphinxmark_text_width', 1000, 'html')\n    app.add_config_value('sphinxmark_text_opacity', 20, 'html')\n    app.add_config_value('sphinxmark_text_spacing', 400, 'html')\n    app.add_config_value('sphinxmark_text_rotation', 0, 'html')\n    app.connect('env-updated', watermark)\n\n    return {\n        'version': '0.1.18',\n        'parallel_read_safe': True,\n        'parallel_write_safe': True,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_view(cls, method_name):\n        method = getattr(cls, method_name)\n\n        @wraps(method)\n        def view(request, **kwargs):\n            handler = cls(request, kwargs)\n            handler.call_processors('view')\n            response = None\n            try:\n                response = method(handler)\n                return response\n            finally:\n                handler.call_processors('post_view', response=response)\n\n        return view", "response": "Get a Django function view calling the given method on the class\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gammalnStirling(z):\n    return (0.5 * (np.log(2. * np.pi) - np.log(z))) \\\n           + (z * (np.log(z + (1. / ((12. * z) - (1. / (10. * z))))) - 1.))", "response": "Returns the log - gamma function for a given number of times z."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef confidenceInterval(n, k, alpha = 0.68, errorbar=False):\n    try:\n        e = float(k) / float(n)\n    except ZeroDivisionError:\n        return np.nan, [np.nan, np.nan]\n\n    bins = 1000001\n    dx = 1. / bins\n\n    efficiency = np.linspace(0, 1, bins)\n\n    # MODIFIED FOR LARGE NUMBERS\n    if n + 2 > 1000:\n        a = gammalnStirling(n + 2)\n    else:\n        a = scipy.special.gammaln(n + 2)\n    if k + 1 > 1000:\n        b = gammalnStirling(k + 1)\n    else:\n        b = scipy.special.gammaln(k + 1)\n    if n - k + 1 > 1000:\n        c = gammalnStirling(n - k + 1)\n    else:\n        c = scipy.special.gammaln(n - k + 1)\n\n    if k == 0:\n        p = np.concatenate([[np.exp(a - b - c)],\n                               np.exp(a - b - c + (k * np.log(efficiency[1: -1])) + (n - k) * np.log(1. - efficiency[1: -1])),\n                               [0.]])\n    elif k == n:\n        p = np.concatenate([[0.],\n                               np.exp(a - b - c + (k * np.log(efficiency[1: -1])) + (n - k) * np.log(1. - efficiency[1: -1])),\n                               [np.exp(a - b - c)]])\n    else:\n        p = np.concatenate([[0.],\n                               np.exp(a - b - c + (k * np.log(efficiency[1: -1])) + (n - k) * np.log(1. - efficiency[1: -1])),\n                               [0.]])\n\n    i = np.argsort(p)[::-1]\n    p_i = np.take(p, i)\n\n    s = i[np.cumsum(p_i * dx) < alpha]\n\n    low = min(np.min(s) * dx, e)\n    high = max(np.max(s) * dx, e)\n\n    if not errorbar:\n        return e, [low, high]\n    else:\n        return e, [e - low, high - e]", "response": "Given n tests and k successes return efficiency and confidence interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives n tests and k successes return efficiency and confidence interval.", "response": "def binomialInterval(n, k, alpha = 0.68):\n    \"\"\"\n    Given n tests and k successes, return efficiency and confidence interval.\n    \"\"\"\n    e = float(k)/n\n    delta_e = 1/float(n) * np.sqrt(e * (1 - e) * float(n)) * alpha/0.68\n    return e, [e - delta_e, e + delta_e]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps the isochrone and kernel simulate functions.", "response": "def satellite(isochrone, kernel, stellar_mass, distance_modulus,**kwargs):\n    \"\"\"\n    Wrapping the isochrone and kernel simulate functions.\n    \"\"\"\n    mag_1, mag_2 = isochrone.simulate(stellar_mass, distance_modulus)\n    lon, lat     = kernel.simulate(len(mag_1))\n\n    return mag_1, mag_2, lon, lat"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef detectability(self,**kwargs):\n        distance_modulus = kwargs.get('distance_modulus')\n        distance = mod2dist(distance_modulus)\n        stellar_mass = kwargs.get('stellar_mass')\n        extension = kwargs.get('extension')\n\n        # Normalized to 10^3 Msolar at mod=18\n        norm = 10**3/mod2dist(18)**2\n        detect = stellar_mass / distance**2\n        detect /= norm", "response": "Returns the detectability of a priori detectability proxy."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_catalog(self,catalog=None):\n        if catalog is None:\n            catalog = ugali.analysis.loglike.createCatalog(self.config,self.roi)\n        cut = self.mask.restrictCatalogToObservableSpace(catalog)\n        self.catalog = catalog.applyCut(cut)", "response": "Create a new catalog."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the realistic photometric errors estimated from catalog objects and mask objects and mask.", "response": "def _photometricErrors(self, n_per_bin=100, plot=False):\n        \"\"\"\n        Realistic photometric errors estimated from catalog objects and mask.\n        Extend below the magnitude threshold with a flat extrapolation.\n        \"\"\"\n\n        self.catalog.spatialBin(self.roi)\n\n        if len(self.catalog.mag_1) < n_per_bin:\n            logger.warning(\"Catalog contains fewer objects than requested to calculate errors.\")\n            n_per_bin = int(len(self.catalog.mag_1) / 3)\n\n        # Band 1\n        mag_1_thresh = self.mask.mask_1.mask_roi_sparse[self.catalog.pixel_roi_index] - self.catalog.mag_1\n        sorting_indices = np.argsort(mag_1_thresh)\n        mag_1_thresh_sort = mag_1_thresh[sorting_indices]\n        mag_err_1_sort = self.catalog.mag_err_1[sorting_indices]\n\n        # ADW: Can't this be done with np.median(axis=?)\n        mag_1_thresh_medians = []\n        mag_err_1_medians = []\n        for i in range(0, int(len(mag_1_thresh) / float(n_per_bin))):\n            mag_1_thresh_medians.append(np.median(mag_1_thresh_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n            mag_err_1_medians.append(np.median(mag_err_1_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n        \n        if mag_1_thresh_medians[0] > 0.:\n            mag_1_thresh_medians = np.insert(mag_1_thresh_medians, 0, -99.)\n            mag_err_1_medians = np.insert(mag_err_1_medians, 0, mag_err_1_medians[0])\n        \n        self.photo_err_1 = scipy.interpolate.interp1d(mag_1_thresh_medians, mag_err_1_medians,\n                                                      bounds_error=False, fill_value=mag_err_1_medians[-1])\n\n        # Band 2\n        mag_2_thresh = self.mask.mask_2.mask_roi_sparse[self.catalog.pixel_roi_index] - self.catalog.mag_2\n        sorting_indices = np.argsort(mag_2_thresh)\n        mag_2_thresh_sort = mag_2_thresh[sorting_indices]\n        mag_err_2_sort = self.catalog.mag_err_2[sorting_indices]\n\n        mag_2_thresh_medians = []\n        mag_err_2_medians = []\n        for i in range(0, int(len(mag_2_thresh) / float(n_per_bin))):\n            mag_2_thresh_medians.append(np.median(mag_2_thresh_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n            mag_err_2_medians.append(np.median(mag_err_2_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n\n        if mag_2_thresh_medians[0] > 0.:\n            mag_2_thresh_medians = np.insert(mag_2_thresh_medians, 0, -99.)\n            mag_err_2_medians = np.insert(mag_err_2_medians, 0, mag_err_2_medians[0])\n        \n        self.photo_err_2 = scipy.interpolate.interp1d(mag_2_thresh_medians, mag_err_2_medians,\n                                                      bounds_error=False, fill_value=mag_err_2_medians[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup_cmd(self,mode='cloud-in-cells'):\n        # Only setup once...\n        if hasattr(self,'bkg_lambda'): return\n\n        logger.info(\"Setup color...\")\n        # In the limit theta->0: 2*pi*(1-cos(theta)) -> pi*theta**2\n        # (Remember to convert from sr to deg^2) \n        #solid_angle_roi = sr2deg(2*np.pi*(1-np.cos(np.radians(self.roi_radius))))\n        solid_angle_roi = self.roi.area_pixel*len(self.roi.pixels)\n\n        # Large CMD bins cause problems when simulating\n        config = Config(self.config) \n        config['color']['n_bins'] *= 5 #10\n        config['mag']['n_bins']   *= 1 #2\n        #config['mask']['minimum_solid_angle'] = 0\n        roi = ugali.analysis.loglike.createROI(config,self.roi.lon,self.roi.lat)\n        mask = ugali.analysis.loglike.createMask(config,roi)\n\n        self.bkg_centers_color  = roi.centers_color\n        self.bkg_centers_mag    = roi.centers_mag\n\n        # Background CMD has units: [objs / deg^2 / mag^2]\n        cmd_background = mask.backgroundCMD(self.catalog,mode)\n        \n        self.bkg_lambda=cmd_background*solid_angle_roi*roi.delta_color*roi.delta_mag\n        np.sum(self.bkg_lambda)\n\n        # Clean up \n        del config, roi, mask", "response": "Create background CMD to sample from."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a random HEALPix map from the main HEALPix pixels.", "response": "def toy_background(self,mc_source_id=2,seed=None):\n        \"\"\"\n        Quick uniform background generation.\n        \"\"\"\n\n        logger.info(\"Running toy background simulation...\")\n        size = 20000\n        nstar = np.random.poisson(size)\n        #np.random.seed(0)\n        logger.info(\"Simulating %i background stars...\"%nstar)\n\n        ### # Random points from roi pixels\n        ### idx = np.random.randint(len(self.roi.pixels)-1,size=nstar)\n        ### pix = self.roi.pixels[idx]\n\n        # Random points drawn from subpixels\n        logger.info(\"Generating uniform positions...\")\n        idx = np.random.randint(0,len(self.subpix)-1,size=nstar)\n        lon,lat = pix2ang(self.nside_subpixel,self.subpix[idx])\n\n        pix = ang2pix(self.nside_pixel, lon, lat)\n        lon,lat = pix2ang(self.nside_pixel,pix)\n\n        # Single color\n        #mag_1 = 19.05*np.ones(len(pix))\n        #mag_2 = 19.10*np.ones(len(pix))\n\n        # Uniform in color\n        logger.info(\"Generating uniform CMD...\")\n        mag_1 = np.random.uniform(self.config['mag']['min'],self.config['mag']['max'],size=nstar)\n        color = np.random.uniform(self.config['color']['min'],self.config['color']['max'],size=nstar)\n\n        mag_2 = mag_1 - color\n\n        # There is probably a better way to do this step without creating the full HEALPix map\n        mask = -1. * np.ones(hp.nside2npix(self.nside_pixel))\n        mask[self.roi.pixels] = self.mask.mask_1.mask_roi_sparse\n        mag_lim_1 = mask[pix]\n        mask = -1. * np.ones(hp.nside2npix(self.nside_pixel))\n        mask[self.roi.pixels] = self.mask.mask_2.mask_roi_sparse\n        mag_lim_2 = mask[pix]\n        \n        #mag_err_1 = 1.0*np.ones(len(pix))\n        #mag_err_2 = 1.0*np.ones(len(pix))\n        mag_err_1 = self.photo_err_1(mag_lim_1 - mag_1)\n        mag_err_2 = self.photo_err_2(mag_lim_2 - mag_2)\n        mc_source_id = mc_source_id * np.ones(len(mag_1))\n\n        select = (mag_lim_1>mag_1)&(mag_lim_2>mag_2)\n\n        hdu = ugali.observation.catalog.makeHDU(self.config,mag_1[select],mag_err_1[select],\n                                                mag_2[select],mag_err_2[select],\n                                                lon[select],lat[select],mc_source_id[select])\n        catalog = ugali.observation.catalog.Catalog(self.config, data=hdu.data)\n        return catalog"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef background(self,mc_source_id=2,seed=None):\n        if seed is not None: np.random.seed(seed)\n        self._setup_cmd()\n\n        # Randomize the number of stars per bin according to Poisson distribution\n        nstar_per_bin = np.random.poisson(lam=self.bkg_lambda)\n        nstar = nstar_per_bin.sum()\n\n        logger.info(\"Simulating %i background stars...\"%nstar)\n\n        if not self.config['simulate'].get('uniform'):\n            logger.info(\"Generating colors from background CMD.\")\n\n            # Distribute the stars within each CMD bin\n            delta_color = self.bkg_centers_color[1]-self.bkg_centers_color[0]\n            delta_mag   = self.bkg_centers_mag[1]-self.bkg_centers_mag[0]\n\n            # Distribute points within each color-mag bins\n            xx,yy = np.meshgrid(self.bkg_centers_color,self.bkg_centers_mag)\n            color = np.repeat(xx.flatten(),repeats=nstar_per_bin.flatten())\n            color += np.random.uniform(-delta_color/2.,delta_color/2.,size=nstar)\n            mag_1 = np.repeat(yy.flatten(),repeats=nstar_per_bin.flatten())\n            mag_1 += np.random.uniform(-delta_mag/2.,delta_mag/2.,size=nstar)\n        else:\n            # Uniform color-magnitude distribution\n            logger.info(\"Generating uniform CMD.\")\n            mag_1 = np.random.uniform(self.config['mag']['min'],self.config['mag']['max'],size=nstar)\n            color = np.random.uniform(self.config['color']['min'],self.config['color']['max'],size=nstar)\n\n        mag_2 = mag_1 - color\n\n        # Random points drawn from healpix subpixels\n        logger.info(\"Generating uniform positions...\")\n        idx = np.random.randint(0,len(self.subpix)-1,size=nstar)\n        lon,lat = pix2ang(self.nside_subpixel,self.subpix[idx])\n\n        nside_pixel = self.nside_pixel\n        pix = ang2pix(nside_pixel, lon, lat)\n\n        # There is probably a better way to do this step without creating the full HEALPix map\n        mask = -1. * np.ones(hp.nside2npix(nside_pixel))\n        mask[self.roi.pixels] = self.mask.mask_1.mask_roi_sparse\n        mag_lim_1 = mask[pix]\n        mask = -1. * np.ones(hp.nside2npix(nside_pixel))\n        mask[self.roi.pixels] = self.mask.mask_2.mask_roi_sparse\n        mag_lim_2 = mask[pix]\n\n        mag_err_1 = self.photo_err_1(mag_lim_1 - mag_1)\n        mag_err_2 = self.photo_err_2(mag_lim_2 - mag_2)\n        mc_source_id = mc_source_id * np.ones(len(mag_1))\n\n        # ADW: Should magnitudes be randomized by the erros?\n        #mag_1 += (np.random.normal(size=len(mag_1)) * mag_err_1)\n        #mag_2 += (np.random.normal(size=len(mag_2)) * mag_err_2)\n\n        select = (mag_lim_1>mag_1)&(mag_lim_2>mag_2)\n\n        ### # Make sure objects lie within the original cmd (should be done later...)\n        ### select &= (ugali.utils.binning.take2D(self.mask.solid_angle_cmd, color, mag_1,\n        ###                                       self.roi.bins_color, self.roi.bins_mag) > 0)\n\n        logger.info(\"Clipping %i simulated background stars...\"%(~select).sum())\n        \n        hdu = ugali.observation.catalog.makeHDU(self.config,mag_1[select],mag_err_1[select],\n                                                mag_2[select],mag_err_2[select],\n                                                lon[select],lat[select],mc_source_id[select])\n        catalog = ugali.observation.catalog.Catalog(self.config, data=hdu.data)\n        return catalog", "response": "Create a simulation of the background object colors for each CMD in the main command set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef satellite(self,stellar_mass,distance_modulus,mc_source_id=1,seed=None,**kwargs):\n        if seed is not None: np.random.seed(seed)\n\n        isochrone = kwargs.pop('isochrone',self.isochrone)\n        kernel    = kwargs.pop('kernel',self.kernel)\n\n        for k,v in kwargs.items():\n            if k in kernel.params.keys(): setattr(kernel,k,v)\n\n        mag_1, mag_2 = isochrone.simulate(stellar_mass, distance_modulus)\n        lon, lat     = kernel.simulate(len(mag_1))\n \n        logger.info(\"Simulating %i satellite stars...\"%len(mag_1))\n        pix = ang2pix(self.config['coords']['nside_pixel'], lon, lat)\n\n        # There is probably a better way to do this step without creating the full HEALPix map\n        mask = -1. * np.ones(hp.nside2npix(self.config['coords']['nside_pixel']))\n        mask[self.roi.pixels] = self.mask.mask_1.mask_roi_sparse\n        mag_lim_1 = mask[pix]\n        mask = -1. * np.ones(hp.nside2npix(self.config['coords']['nside_pixel']))\n        mask[self.roi.pixels] = self.mask.mask_2.mask_roi_sparse\n        mag_lim_2 = mask[pix]\n\n        mag_err_1 = self.photo_err_1(mag_lim_1 - mag_1)\n        mag_err_2 = self.photo_err_2(mag_lim_2 - mag_2)\n\n        # Randomize magnitudes by their errors\n        mag_obs_1 = mag_1+np.random.normal(size=len(mag_1))*mag_err_1\n        mag_obs_2 = mag_2+np.random.normal(size=len(mag_2))*mag_err_2\n        #mag_obs_1 = mag_1\n        #mag_obs_2 = mag_2\n\n        #select = np.logical_and(mag_obs_1 < mag_lim_1, mag_obs_2 < mag_lim_2)\n        select = (mag_lim_1>mag_obs_1)&(mag_lim_2>mag_obs_2)\n\n        # Make sure objects lie within the original cmd (should also be done later...)\n        #select &= (ugali.utils.binning.take2D(self.mask.solid_angle_cmd, mag_obs_1 - mag_obs_2, mag_obs_1,self.roi.bins_color, self.roi.bins_mag) > 0)\n\n        #return mag_1_obs[cut], mag_2_obs[cut], lon[cut], lat[cut]\n        logger.info(\"Clipping %i simulated satellite stars...\"%(~select).sum())\n        mc_source_id = mc_source_id * np.ones(len(mag_1))\n        \n        hdu = ugali.observation.catalog.makeHDU(self.config,mag_obs_1[select],mag_err_1[select],\n                                                mag_obs_2[select],mag_err_2[select], \n                                                lon[select],lat[select],mc_source_id[select])\n        catalog = ugali.observation.catalog.Catalog(self.config, data=hdu.data)\n        return catalog", "response": "Create a simulated satellite."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a catalog FITS file object based on input data.", "response": "def makeHDU(self, mag_1, mag_err_1, mag_2, mag_err_2, lon, lat, mc_source_id):\n        \"\"\"\n        Create a catalog fits file object based on input data.\n\n        ADW: This should be combined with the write_membership\n        function of loglike.\n        \"\"\"\n        if self.config['catalog']['coordsys'].lower() == 'cel' \\\n           and self.config['coords']['coordsys'].lower() == 'gal':\n            lon, lat = ugali.utils.projector.gal2cel(lon, lat)\n        elif self.config['catalog']['coordsys'].lower() == 'gal' \\\n           and self.config['coords']['coordsys'].lower() == 'cel':\n            lon, lat = ugali.utils.projector.cel2gal(lon, lat)\n\n        columns = [\n            pyfits.Column(name=self.config['catalog']['objid_field'],\n                          format = 'D',array = np.arange(len(lon))),\n            pyfits.Column(name=self.config['catalog']['lon_field'],\n                          format = 'D',array = lon),\n            pyfits.Column(name = self.config['catalog']['lat_field'],          \n                          format = 'D',array = lat), \n            pyfits.Column(name = self.config['catalog']['mag_1_field'],        \n                          format = 'E',array = mag_1),\n            pyfits.Column(name = self.config['catalog']['mag_err_1_field'],    \n                          format = 'E',array = mag_err_1),\n            pyfits.Column(name = self.config['catalog']['mag_2_field'],        \n                          format = 'E',array = mag_2),\n            pyfits.Column(name = self.config['catalog']['mag_err_2_field'],    \n                          format = 'E',array = mag_err_2),\n            pyfits.Column(name = self.config['catalog']['mc_source_id_field'], \n                          format = 'I',array = mc_source_id),\n        ]\n\n        hdu = pyfits.new_table(columns)\n        return hdu"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean_columns(columns, valid_regex=r'\\w', lower=True, max_len=32):\n    rettype = None\n    if isinstance(columns, str):\n        rettype = type(columns)\n        columns = [columns]\n\n    columns = [c.strip() for c in columns]\n    # # unnecessary because these are invalid characters removed below\n    # columns = [(c[1:-1] if c[0] in '\\'\"' and c[-1] == c[0] else c) for c in columns]\n    # columns = [(c[1:-1] if c[0] in '{([<' and c[-1] in '})]>' else c) for c in columns]\n    columns = [re.sub('\\s+', '_', c).lower() for c in columns]\n    columns = remove_invalid_chars(columns, valid_regex=r'\\w')\n    columns = [c[:max_len] for c in columns]\n    columns = np.array(columns) if rettype is None else rettype(columns[0])\n    return columns", "response": "Ensure all column names in a list are valid python variable names and attribute names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef force_hashable(obj, recursive=True):\n    # if it's already hashable, and isn't a generator (which are also hashable, but also mutable?)\n    if hasattr(obj, '__hash__') and not hasattr(obj, 'next') and not hasattr(obj, '__next__'):\n        try:\n            hash(obj)\n            return obj\n        except (IndexError, ValueError, AttributeError, TypeError):\n            pass\n    if hasattr(obj, '__iter__'):\n        # looks like a Mapping if it has .get() and .items(), so should treat it like one\n        if hasattr(obj, 'get') and hasattr(obj, 'items'):\n            # FIXME: prevent infinite recursion:\n            #        tuples don't have 'items' method so this will recurse forever\n            #        if elements within new tuple aren't hashable and recurse has not been set!\n            return force_hashable(tuple(obj.items()))\n        if recursive:\n            return tuple(force_hashable(item) for item in obj)\n        return tuple(obj)\n    # strings are hashable so this ends the recursion for any object without an __iter__ method (strings do not)\n    return str(obj)", "response": "Force a frozen set of mutables and iterables like lists dicts and generators to freeze the order and contents of mutables and iterables like lists dicts and generators."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inverted_dict(d):\n    return dict((force_hashable(v), k) for (k, v) in viewitems(dict(d)))", "response": "Return a dict with swapped keys and values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inverted_dict_of_lists(d):\n    new_dict = {}\n    for (old_key, old_value_list) in viewitems(dict(d)):\n        for new_key in listify(old_value_list):\n            new_dict[new_key] = old_key\n    return new_dict", "response": "Return a dict where the keys are all the values listed in the values of the original dict\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsorts a list of strings according to the provided sorted list of string prefixes.", "response": "def sort_strings(strings, sort_order=None, reverse=False, case_sensitive=False, sort_order_first=True):\n    \"\"\"Sort a list of strings according to the provided sorted list of string prefixes\n\n    TODO:\n        - Provide an option to use `.startswith()` rather than a fixed prefix length (will be much slower)\n\n    Arguments:\n        sort_order_first (bool): Whether strings in sort_order should always preceed \"unknown\" strings\n        sort_order (sequence of str): Desired ordering as a list of prefixes to the strings\n            If sort_order strings have varying length, the max length will determine the prefix length compared\n        reverse (bool): whether to reverse the sort orded. Passed through to `sorted(strings, reverse=reverse)`\n        case_senstive (bool): Whether to sort in lexographic rather than alphabetic order\n         and whether the prefixes  in sort_order are checked in a case-sensitive way\n\n    Examples:\n        >>> sort_strings(['morn32', 'morning', 'unknown', 'date', 'dow', 'doy', 'moy'],\n        ...              ('dat', 'dow', 'moy', 'dom', 'doy', 'mor'))\n        ['date', 'dow', 'moy', 'doy', 'morn32', 'morning', 'unknown']\n        >>> sort_strings(['morn32', 'morning', 'unknown', 'less unknown', 'lucy', 'date', 'dow', 'doy', 'moy'],\n        ...              ('dat', 'dow', 'moy', 'dom', 'doy', 'mor'), reverse=True)\n        ['unknown', 'lucy', 'less unknown', 'morning', 'morn32', 'doy', 'moy', 'dow', 'date']\n\n        Strings whose prefixes don't exist in `sort_order` sequence can be interleaved into the\n        sorted list in lexical order by setting `sort_order_first=False`\n        >>> sort_strings(['morn32', 'morning', 'unknown', 'lucy', 'less unknown', 'date', 'dow', 'doy', 'moy'],\n        ...              ('dat', 'dow', 'moy', 'dom', 'moy', 'mor'),\n        ...              sort_order_first=False)  # doctest: +NORMALIZE_WHITESPACE\n        ['date', 'dow', 'doy', 'less unknown', 'lucy', 'moy', 'morn32', 'morning', 'unknown']\n    \"\"\"\n    if not case_sensitive:\n        sort_order = tuple(s.lower() for s in sort_order)\n        strings = tuple(s.lower() for s in strings)\n    prefix_len = max(len(s) for s in sort_order)\n\n    def compare(a, b, prefix_len=prefix_len):\n        if prefix_len:\n            if a[:prefix_len] in sort_order:\n                if b[:prefix_len] in sort_order:\n                    comparison = sort_order.index(a[:prefix_len]) - sort_order.index(b[:prefix_len])\n                    comparison = int(comparison / abs(comparison or 1))\n                    if comparison:\n                        return comparison * (-2 * reverse + 1)\n                elif sort_order_first:\n                    return -1 * (-2 * reverse + 1)\n            # b may be in sort_order list, so it should be first\n            elif sort_order_first and b[:prefix_len] in sort_order:\n                return -2 * reverse + 1\n        return (-1 * (a < b) + 1 * (a > b)) * (-2 * reverse + 1)\n\n    return sorted(strings, key=functools.cmp_to_key(compare))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize field values by stripping whitespace from strings localizing datetimes to a timezone etc.", "response": "def clean_field_dict(field_dict, cleaner=str.strip, time_zone=None):\n    r\"\"\"Normalize field values by stripping whitespace from strings, localizing datetimes to a timezone, etc\n\n    >>> (sorted(clean_field_dict({'_state': object(), 'x': 1, 'y': \"\\t  Wash Me! \\n\" }).items()) ==\n    ... [('x', 1), ('y', 'Wash Me!')])\n    True\n    \"\"\"\n    d = {}\n    if time_zone is None:\n        tz = DEFAULT_TZ\n    for k, v in viewitems(field_dict):\n        if k == '_state':\n            continue\n        if isinstance(v, basestring):\n            d[k] = cleaner(str(v))\n        elif isinstance(v, (datetime.datetime, datetime.date)):\n            d[k] = tz.localize(v)\n        else:\n            d[k] = v\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce_vocab(tokens, similarity=.85, limit=20, sort_order=-1):\n    if 0 <= similarity <= 1:\n        similarity *= 100\n    if sort_order:\n        tokens = set(tokens)\n        tokens_sorted = sorted(list(tokens), reverse=bool(sort_order < 0))\n    else:\n        tokens_sorted = list(tokens)\n        tokens = set(tokens)\n    # print(tokens)\n    thesaurus = {}\n    for tok in tokens_sorted:\n        try:\n            tokens.remove(tok)\n        except (KeyError, ValueError):\n            continue\n        # FIXME: this is slow because the tokens list must be regenerated and reinstantiated with each iteration\n        matches = fuzzy.extractBests(tok, list(tokens), score_cutoff=int(similarity), limit=limit)\n        if matches:\n            thesaurus[tok] = list(zip(*matches))[0]\n        else:\n            thesaurus[tok] = ()\n        for syn in thesaurus[tok]:\n            tokens.discard(syn)\n    return thesaurus", "response": "This function is used to reduce the vocabulary of similar words within a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreduce the vocabulary of similar words within a list of tokens.", "response": "def reduce_vocab_by_len(tokens, similarity=.87, limit=20, reverse=True):\n    \"\"\"Find spelling variations of similar words within a list of tokens to reduce token set size\n\n    Sorted by length (longest first unless reverse=False) before running through fuzzy-wuzzy\n    which results in longer key tokens.\n\n    Arguments:\n      tokens (list or set or tuple of str): token strings from which to eliminate similar spellings\n\n    Returns:\n      dict: { 'token': ('similar_token', 'similar_token2', ...), ...}\n\n    Examples:\n      FIXME: needs to return a dict of sets rather than dict of tuples so order doesn't matter\n      >> tokens = set(('on', 'hon', 'honey', 'ones', 'one', 'two', 'three'))\n      {'honey': ('on', 'hon', 'one'), 'ones': (), 'three': (), 'two': ()}\n    \"\"\"\n    tokens = set(tokens)\n    tokens_sorted = list(zip(*sorted([(len(tok), tok) for tok in tokens], reverse=reverse)))[1]\n    return reduce_vocab(tokens=tokens_sorted, similarity=similarity, limit=limit, sort_order=0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating through a sequence or generator in batches of length batch_len", "response": "def generate_batches(sequence, batch_len=1, allow_partial=True, ignore_errors=True, verbosity=1):\n    \"\"\"Iterate through a sequence (or generator) in batches of length `batch_len`\n\n    http://stackoverflow.com/a/761125/623735\n    >>> [batch for batch in generate_batches(range(7), 3)]\n    [[0, 1, 2], [3, 4, 5], [6]]\n    \"\"\"\n    it = iter(sequence)\n    last_value = False\n    # An exception will be thrown by `.next()` here and caught in the loop that called this iterator/generator\n    while not last_value:\n        batch = []\n        for n in range(batch_len):\n            try:\n                batch += (next(it),)\n            except StopIteration:\n                last_value = True\n                if batch:\n                    break\n                else:\n                    raise StopIteration\n            except Exception:\n                # 'Error: new-line character seen in unquoted field -\n                # do you need to open the file in universal-newline mode?'\n                if verbosity > 0:\n                    print_exc()\n                if not ignore_errors:\n                    raise\n        yield batch"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_tuple_batches(qs, batch_len=1):\n    num_items, batch = 0, []\n    for item in qs:\n        if num_items >= batch_len:\n            yield tuple(batch)\n            num_items = 0\n            batch = []\n        num_items += 1\n        batch += [item]\n    if num_items:\n        yield tuple(batch)", "response": "Iterate through a queryset in batches of length batch_len"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates through a sequence of objects or generator in batches of length batch_len.", "response": "def generate_slices(sliceable_set, batch_len=1, length=None, start_batch=0):\n    \"\"\"Iterate through a sequence (or generator) in batches of length `batch_len`\n\n    See Also:\n      pug.dj.db.generate_queryset_batches\n\n    References:\n      http://stackoverflow.com/a/761125/623735\n\n    Examples:\n      >>  [batch for batch in generate_slices(range(7), 3)]\n      [(0, 1, 2), (3, 4, 5), (6,)]\n      >>  from django.contrib.auth.models import User, Permission\n      >>  len(list(generate_slices(User.objects.all(), 2)))       == max(math.ceil(User.objects.count() / 2.), 1)\n      True\n      >>  len(list(generate_slices(Permission.objects.all(), 2))) == max(math.ceil(Permission.objects.count() / 2.), 1)\n      True\n    \"\"\"\n    if length is None:\n        try:\n            length = sliceable_set.count()\n        except (IndexError, ValueError, AttributeError, TypeError):\n            length = len(sliceable_set)\n    length = int(length)\n\n    for i in range(int(length / batch_len + 1)):\n        if i < start_batch:\n            continue\n        start = i * batch_len\n        end = min((i + 1) * batch_len, length)\n        if start != end:\n            yield tuple(sliceable_set[start:end])\n    raise StopIteration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_count_label(d):\n    for name in COUNT_NAMES:\n        if name in d:\n            return name\n    for name in COUNT_NAMES:\n        if str(name).lower() in d:\n            return name", "response": "Find the label of the count."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the key for a given value in a dictionary.", "response": "def get_key_for_value(dict_obj, value, default=None):\n    \"\"\"\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you')\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you', default='Not Found')\n    'Not Found'\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'other', default='Not Found')\n    'Not Found'\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'want')\n    'you'\n    >>> get_key_for_value({0: 'what', '': 'ever', 'you': 'want', 'to find': None, 'you': 'too'}, 'what')\n    0\n    >>> get_key_for_value({0: 'what', '': 'ever', 'you': 'want', 'to find': None, 'you': 'too', ' ': 'want'}, 'want')\n    ' '\n    \"\"\"\n    for k, v in viewitems(dict_obj):\n        if v == value:\n            return k\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_set(seq):\n    new_list = []\n    for i in seq:\n        if i not in new_list:\n            new_list += [i]\n    return type(seq)(new_list)", "response": "Similar to list ( set ( seq ) but maintains the order of the original list while eliminating duplicates\nMimeType"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fuzzy_get(possible_keys, approximate_key, default=None, similarity=0.6, tuple_joiner='|',\n              key_and_value=False, dict_keys=None):\n    r\"\"\"Find the closest matching key in a dictionary (or element in a list)\n\n    For a dict, optionally retrieve the associated value associated with the closest key\n\n    Notes:\n      `possible_keys` must have all string elements or keys!\n      Argument order is in reverse order relative to `fuzzywuzzy.process.extractOne()`\n        but in the same order as get(self, key) method on dicts\n\n    Arguments:\n      possible_keys (dict): object to run the get method on using the key that is most similar to one within the dict\n      approximate_key (str): key to look for a fuzzy match within the dict keys\n      default (obj): the value to return if a similar key cannote be found in the `possible_keys`\n      similarity (float): fractional similiarity between the approximate_key and the dict key\n        (0.9 means 90% of characters must be identical)\n      tuple_joiner (str): Character to use as delimitter/joiner between tuple elements.\n        Used to create keys of any tuples to be able to use fuzzywuzzy string matching on it.\n      key_and_value (bool): Whether to return both the key and its value (True) or just the value (False).\n        Default is the same behavior as dict.get (i.e. key_and_value=False)\n      dict_keys (list of str): if you already have a set of keys to search,\n        this will save this funciton a little time and RAM\n\n    See Also:\n      get_similar: Allows nonstring keys and searches object attributes in addition to keys\n\n    Examples:\n\n    >>> fuzzy_get({'seller': 2.7, 'sailor': set('e')}, 'sail') == set(['e'])\n    True\n    >>> fuzzy_get({'seller': 2.7, 'sailor': set('e'), 'camera': object()}, 'SLR')\n    2.7\n    >>> fuzzy_get({'seller': 2.7, 'sailor': set('e'), 'camera': object()}, 'I') == set(['e'])\n    True\n    >>> fuzzy_get({'word': tuple('word'), 'noun': tuple('noun')}, 'woh!', similarity=.3, key_and_value=True)\n    ('word', ('w', 'o', 'r', 'd'))\n    >>> fuzzy_get({'word': tuple('word'), 'noun': tuple('noun')}, 'woh!', similarity=.9, key_and_value=True)\n    (None, None)\n    >>> fuzzy_get({'word': tuple('word'), 'noun': tuple('noun')}, 'woh!', similarity=.9,\n    ...           default='darn :-()', key_and_value=True)\n    (None, 'darn :-()')\n    >>> possible_keys = ('alerts astronomy conditions currenthurricane forecast forecast10day geolookup history ' +\n    ...                  'hourly hourly10day planner rawtide satellite tide webcams yesterday').split()\n    >>> fuzzy_get(possible_keys, \"cond\")\n    'conditions'\n    >>> fuzzy_get(possible_keys, \"Tron\")\n    'astronomy'\n    >>> df = pd.DataFrame(np.arange(6*2).reshape(2,6), columns=('alpha','beta','omega','begin','life','end'))\n    >>> fuzzy_get(df, 'beg')  # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS\n    0    3\n    1    9\n    Name: begin, dtype: int...\n    >>> fuzzy_get(df, 'get')\n    >>> fuzzy_get(df, 'et')[1]\n    7\n    >>> fuzzy_get(df, 'get')\n    \"\"\"\n    dict_obj = copy.copy(possible_keys)\n    if not isinstance(dict_obj, (Mapping, pd.DataFrame, pd.Series)):\n        dict_obj = dict((x, x) for x in dict_obj)\n\n    fuzzy_key, value = None, default\n    if approximate_key in dict_obj:\n        fuzzy_key, value = approximate_key, dict_obj[approximate_key]\n    else:\n        strkey = str(approximate_key)\n        if approximate_key and strkey and strkey.strip():\n            # print 'no exact match was found for {0} in {1} so preprocessing keys'.format(approximate_key, dict_obj.keys())\n            if any(isinstance(k, (tuple, list)) for k in dict_obj):\n                dict_obj = dict((tuple_joiner.join(str(k2) for k2 in k), v) for (k, v) in viewitems(dict_obj))\n                if isinstance(approximate_key, (tuple, list)):\n                    strkey = tuple_joiner.join(approximate_key)\n            # fuzzywuzzy requires that dict_keys be a list (sets and tuples fail!)\n            dict_keys = list(set(dict_keys if dict_keys else dict_obj))\n            if strkey in dict_keys:\n                fuzzy_key, value = strkey, dict_obj[strkey]\n            else:\n                strkey = strkey.strip()\n                if strkey in dict_keys:\n                    fuzzy_key, value = strkey, dict_obj[strkey]\n                else:\n                    fuzzy_key_scores = fuzzy.extractBests(strkey, dict_keys,\n                                                          score_cutoff=min(max(similarity * 100.0 - 1, 0), 100),\n                                                          limit=6)\n                    if fuzzy_key_scores:\n                        fuzzy_score_keys = []\n                        # add length similarity as part of score\n                        for (i, (k, score)) in enumerate(fuzzy_key_scores):\n                            fuzzy_score_keys += [(score * math.sqrt(len(strkey)**2 /\n                                                                    float((len(k)**2 + len(strkey)**2) or 1)), k)]\n                        fuzzy_score, fuzzy_key = sorted(fuzzy_score_keys)[-1]\n                        value = dict_obj[fuzzy_key]\n    if key_and_value:\n        if key_and_value in ('v', 'V', 'value', 'VALUE', 'Value'):\n            return value\n        return fuzzy_key, value\n    else:\n        return value", "response": "r Returns the closest matching key in a dictionary or element in a list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nliking fuzzy_get, but assume the obj is dict-like and return the value without the key Notes: Argument order is in reverse order relative to `fuzzywuzzy.process.extractOne()` but in the same order as get(self, key) method on dicts Arguments: obj (dict-like): object to run the get method on using the key that is most similar to one within the dict approximate_key (str): key to look for a fuzzy match within the dict keys default (obj): the value to return if a similar key cannote be found in the `possible_keys` similarity (str): fractional similiarity between the approximate_key and the dict key (0.9 means 90% of characters must be identical) tuple_joiner (str): Character to use as delimitter/joiner between tuple elements. Used to create keys of any tuples to be able to use fuzzywuzzy string matching on it. key_and_value (bool): Whether to return both the key and its value (True) or just the value (False). Default is the same behavior as dict.get (i.e. key_and_value=False) dict_keys (list of str): if you already have a set of keys to search, this will save this funciton a little time and RAM Examples: >>> fuzzy_get_value({'seller': 2.7, 'sailor': set('e')}, 'sail') == set(['e']) True >>> fuzzy_get_value({'seller': 2.7, 'sailor': set('e'), 'camera': object()}, 'SLR') 2.7 >>> fuzzy_get_value({'seller': 2.7, 'sailor': set('e'), 'camera': object()}, 'I') == set(['e']) True >>> fuzzy_get_value({'word': tuple('word'), 'noun': tuple('noun')}, 'woh!', similarity=.3) ('w', 'o', 'r', 'd') >>> df = pd.DataFrame(np.arange(6*2).reshape(2,6), columns=('alpha','beta','omega','begin','life','end')) >>> fuzzy_get_value(df, 'life')[0], fuzzy_get(df, 'omega')[0] (4, 2)", "response": "def fuzzy_get_value(obj, approximate_key, default=None, **kwargs):\n    \"\"\" Like fuzzy_get, but assume the obj is dict-like and return the value without the key\n\n    Notes:\n      Argument order is in reverse order relative to `fuzzywuzzy.process.extractOne()`\n        but in the same order as get(self, key) method on dicts\n\n    Arguments:\n      obj (dict-like): object to run the get method on using the key that is most similar to one within the dict\n      approximate_key (str): key to look for a fuzzy match within the dict keys\n      default (obj): the value to return if a similar key cannote be found in the `possible_keys`\n      similarity (str): fractional similiarity between the approximate_key and the dict key\n        (0.9 means 90% of characters must be identical)\n      tuple_joiner (str): Character to use as delimitter/joiner between tuple elements.\n        Used to create keys of any tuples to be able to use fuzzywuzzy string matching on it.\n      key_and_value (bool): Whether to return both the key and its value (True) or just the value (False).\n        Default is the same behavior as dict.get (i.e. key_and_value=False)\n      dict_keys (list of str): if you already have a set of keys to search, this will save this funciton\n        a little time and RAM\n\n    Examples:\n      >>> fuzzy_get_value({'seller': 2.7, 'sailor': set('e')}, 'sail') == set(['e'])\n      True\n      >>> fuzzy_get_value({'seller': 2.7, 'sailor': set('e'), 'camera': object()}, 'SLR')\n      2.7\n      >>> fuzzy_get_value({'seller': 2.7, 'sailor': set('e'), 'camera': object()}, 'I') == set(['e'])\n      True\n      >>> fuzzy_get_value({'word': tuple('word'), 'noun': tuple('noun')}, 'woh!', similarity=.3)\n      ('w', 'o', 'r', 'd')\n      >>> df = pd.DataFrame(np.arange(6*2).reshape(2,6), columns=('alpha','beta','omega','begin','life','end'))\n      >>> fuzzy_get_value(df, 'life')[0], fuzzy_get(df, 'omega')[0]\n      (4, 2)\n    \"\"\"\n    dict_obj = OrderedDict(obj)\n    try:\n        return dict_obj[list(dict_obj.keys())[int(approximate_key)]]\n    except (ValueError, IndexError):\n        pass\n    return fuzzy_get(dict_obj, approximate_key, key_and_value=False, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the closest matching key and value in a dictionary.", "response": "def fuzzy_get_tuple(dict_obj, approximate_key, dict_keys=None, key_and_value=False, similarity=0.6, default=None):\n    \"\"\"Find the closest matching key and/or value in a dictionary (must have all string keys!)\"\"\"\n    return fuzzy_get(dict(('|'.join(str(k2) for k2 in k), v) for (k, v) in viewitems(dict_obj)),\n                     '|'.join(str(k) for k in approximate_key), dict_keys=dict_keys,\n                     key_and_value=key_and_value, similarity=similarity, default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sod_transposed(seq_of_dicts, align=True, pad=True, filler=None):\n    result = {}\n    if isinstance(seq_of_dicts, Mapping):\n        seq_of_dicts = [seq_of_dicts]\n    it = iter(seq_of_dicts)\n    # if you don't need to align and/or fill, then just loop through and return\n    if not (align and pad):\n        for d in it:\n            for k in d:\n                result[k] = result.get(k, []) + [d[k]]\n        return result\n    # need to align and/or fill, so pad as necessary\n    for i, d in enumerate(it):\n        for k in d:\n            result[k] = result.get(k, [filler] * (i * int(align))) + [d[k]]\n        for k in result:\n            if k not in d:\n                result[k] += [filler]\n    return result", "response": "Return sequence of dictionaries transposed into a dictionary of lists"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef joined_seq(seq, sep=None):\n    joined_seq = tuple(seq)\n    if isinstance(sep, basestring):\n        joined_seq = sep.join(str(item) for item in joined_seq)\n    return joined_seq", "response": "r Join a sequence into a tuple or a concatenated string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconsolidate stats from a dict of sequences into a single tree of new tree items.", "response": "def consolidate_stats(dict_of_seqs, stats_key=None, sep=','):\n    \"\"\"Join (stringify and concatenate) keys (table fields) in a dict (table) of sequences (columns)\n\n    >>> consolidate_stats(dict([('c', [1, 1, 1]), ('cm', ['P', 6, 'Q']), ('cn', [0, 'MUS', 'ROM']),\n    ...                        ('ct', [0, 2, 0])]), stats_key='c')\n    [{'P,0,0': 1}, {'6,MUS,2': 1}, {'Q,ROM,0': 1}]\n    >>> consolidate_stats([{'c': 1, 'cm': 'P', 'cn': 0, 'ct': 0}, {'c': 1, 'cm': 6, 'cn': 'MUS', 'ct': 2},\n    ...                    {'c': 1, 'cm': 'Q', 'cn': 'ROM', 'ct': 0}], stats_key='c')\n    [{'P,0,0': 1}, {'6,MUS,2': 1}, {'Q,ROM,0': 1}]\n    \"\"\"\n    if isinstance(dict_of_seqs, dict):\n        stats = dict_of_seqs[stats_key]\n        keys = joined_seq(sorted([k for k in dict_of_seqs if k is not stats_key]), sep=None)\n        joined_key = joined_seq(keys, sep=sep)\n        result = {stats_key: [], joined_key: []}\n        for i, statistic in enumerate(stats):\n            result[stats_key] += [statistic]\n            result[joined_key] += [joined_seq((dict_of_seqs[k][i] for k in keys if k is not stats_key), sep)]\n        return list({k: result[stats_key][i]} for i, k in enumerate(result[joined_key]))\n    return [{joined_seq((d[k] for k in sorted(d) if k is not stats_key), sep): d[stats_key]} for d in dict_of_seqs]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nproduces dictionary of sequences from sequence of sequences optionally with a header row.", "response": "def dos_from_table(table, header=None):\n    \"\"\"Produce dictionary of sequences from sequence of sequences, optionally with a header \"row\".\n\n    >>> dos_from_table([['hello', 'world'], [1, 2], [3,4]]) == {'hello': [1, 3], 'world': [2, 4]}\n    True\n    \"\"\"\n    start_row = 0\n    if not table:\n        return table\n    if not header:\n        header = table[0]\n        start_row = 1\n    header_list = header\n    if header and isinstance(header, basestring):\n        header_list = header.split('\\t')\n        if len(header_list) != len(table[0]):\n            header_list = header.split(',')\n        if len(header_list) != len(table[0]):\n            header_list = header.split(' ')\n    ans = {}\n    for i, k in enumerate(header):\n        ans[k] = [row[i] for row in table[start_row:]]\n    return ans"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transposed_lists(list_of_lists, default=None):\n    if default is None or default is [] or default is tuple():\n        default = []\n    elif default is 'None':\n        default = [None]\n    else:\n        default = [default]\n\n    N = len(list_of_lists)\n    Ms = [len(row) for row in list_of_lists]\n    M = max(Ms)\n    ans = []\n    for j in range(M):\n        ans += [[]]\n        for i in range(N):\n            if j < Ms[i]:\n                ans[-1] += [list_of_lists[i][j]]\n            else:\n                ans[-1] += list(default)\n    return ans", "response": "Like numpy. transposed but allows uneven row lengths\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transposed_matrix(matrix, filler=None, row_type=list, matrix_type=list, value_type=None):\n    matrix_type = matrix_type or type(matrix)\n\n    try:\n        row_type = row_type or type(matrix[0])\n    except (IndexError, ValueError, AttributeError, TypeError):\n        pass\n    if not row_type or row_type is None:\n        row_type = list\n\n    try:\n        if matrix[0][0] is None:\n            value_type = value_type or float\n        else:\n            value_type = value_type or type(matrix[0][0]) or float\n    except (IndexError, ValueError, AttributeError, TypeError):\n        pass\n    if not value_type or value_type is None:\n        value_type = float\n\n    # original matrix is NxM, new matrix will be MxN\n    N = len(matrix)\n    Ms = [len(row) for row in matrix]\n    M = 0 if not Ms else max(Ms)\n\n    ans = []\n    # for each row in the new matrix (column in old matrix)\n    for j in range(M):\n        # add a row full of copies the `fill` value up to the maximum width required\n        ans += [row_type([filler] * N)]\n        for i in range(N):\n            try:\n                ans[j][i] = value_type(matrix[i][j])\n            except IndexError:\n                ans[j][i] = filler\n            except TypeError:\n                ans[j][i] = filler\n\n    return matrix_type(ans) if isinstance(ans[0], row_type) else matrix_type([row_type(row) for row in ans])", "response": "Like numpy. transposed but evens up row lengths that aren t uniform filling with None."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hist_from_counts(counts, normalize=False, cumulative=False, to_str=False, sep=',', min_bin=None, max_bin=None):\n    counters = [dict((i, c)for i, c in enumerate(counts))]\n\n    intkeys_list = [[c for c in counts_dict if (isinstance(c, int) or (isinstance(c, float) and int(c) == c))]\n                    for counts_dict in counters]\n    min_bin, max_bin = min_bin or 0, max_bin or len(counts) - 1\n\n    histograms = []\n    for intkeys, counts in zip(intkeys_list, counters):\n        histograms += [OrderedDict()]\n        if not intkeys:\n            continue\n        if normalize:\n            N = sum(counts[c] for c in intkeys)\n            for c in intkeys:\n                counts[c] = float(counts[c]) / N\n        if cumulative:\n            for i in range(min_bin, max_bin + 1):\n                histograms[-1][i] = counts.get(i, 0) + histograms[-1].get(i - 1, 0)\n        else:\n            for i in range(min_bin, max_bin + 1):\n                histograms[-1][i] = counts.get(i, 0)\n    if not histograms:\n        histograms = [OrderedDict()]\n\n    # fill in the zero counts between the integer bins of the histogram\n    aligned_histograms = []\n\n    for i in range(min_bin, max_bin + 1):\n        aligned_histograms += [tuple([i] + [hist.get(i, 0) for hist in histograms])]\n\n    if to_str:\n        # FIXME: add header row\n        return str_from_table(aligned_histograms, sep=sep, max_rows=365 * 2 + 1)\n\n    return aligned_histograms", "response": "Compute an emprical histogram PMF or CDF in a list of lists\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes an emprical histogram PMF or CDF in a list of lists or csv string.", "response": "def hist_from_values_list(values_list, fillers=(None,), normalize=False,\n                          cumulative=False, to_str=False, sep=',', min_bin=None, max_bin=None):\n    \"\"\"Compute an emprical histogram, PMF or CDF in a list of lists or a csv string\n\n    Only works for discrete (integer) values (doesn't bin real values).\n    `fillers`: list or tuple of values to ignore in computing the histogram\n\n    >>> hist_from_values_list([1,1,2,1,1,1,2,3,2,4,4,5,7,7,9])  # doctest: +NORMALIZE_WHITESPACE\n    [(1, 5), (2, 3), (3, 1), (4, 2), (5, 1), (6, 0), (7, 2), (8, 0), (9, 1)]\n    >>> hist_from_values_list([(1,9),(1,8),(2,),(1,),(1,4),(2,5),(3,3),(5,0),(2,2)])  # doctest: +NORMALIZE_WHITESPACE\n    [[(1, 4), (2, 3), (3, 1), (4, 0), (5, 1)], [(0, 1), (1, 0), ... (6, 0), (7, 0), (8, 1), (9, 1)]]\n    >>> hist_from_values_list(transposed_matrix([(8,),(1,3,5),(2,),(3,4,5,8)]))  # doctest: +NORMALIZE_WHITESPACE\n    [[(8, 1)], [(1, 1), (2, 0), (3, 1), (4, 0), (5, 1)], [(2, 1)], [(3, 1), (4, 1), (5, 1), (6, 0), (7, 0), (8, 1)]]\n    \"\"\"\n    value_types = tuple([int, float] + [type(filler) for filler in fillers])\n\n    if all(isinstance(value, value_types) for value in values_list):\n        # ignore all fillers and convert all floats to ints when doing counting\n        counters = [Counter(int(value) for value in values_list if isinstance(value, (int, float)))]\n    elif all(len(row) == 1 for row in values_list) and all(isinstance(row[0], value_types) for row in values_list):\n        return hist_from_values_list([values[0] for values in values_list], fillers=fillers,\n                                     normalize=normalize, cumulative=cumulative,\n                                     to_str=to_str, sep=sep, min_bin=min_bin, max_bin=max_bin)\n    else:  # assume it's a row-wise table (list of rows)\n        return [\n            hist_from_values_list(col, fillers=fillers, normalize=normalize, cumulative=cumulative, to_str=to_str, sep=sep,\n                                  min_bin=min_bin, max_bin=max_bin)\n            for col in transposed_matrix(values_list)\n        ]\n\n    if not values_list:\n        return []\n\n    intkeys_list = [[c for c in counts if (isinstance(c, int) or (isinstance(c, float) and int(c) == c))]\n                    for counts in counters]\n    try:\n        min_bin = int(min_bin)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        min_bin = min(min(intkeys) for intkeys in intkeys_list)\n    try:\n        max_bin = int(max_bin)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        max_bin = max(max(intkeys) for intkeys in intkeys_list)\n\n    # FIXME: this looks slow and hazardous (like it's ignore min/max bin):\n    # TODO: reuse min(intkeys)\n    min_bin = max(min_bin, min((min(intkeys) if intkeys else 0) for intkeys in intkeys_list))\n    max_bin = min(max_bin, max((max(intkeys) if intkeys else 0) for intkeys in intkeys_list))\n\n    histograms = []\n    for intkeys, counts in zip(intkeys_list, counters):\n        histograms += [OrderedDict()]\n        if not intkeys:\n            continue\n        if normalize:\n            N = sum(counts[c] for c in intkeys)\n            for c in intkeys:\n                counts[c] = float(counts[c]) / N\n        if cumulative:\n            for i in range(min_bin, max_bin + 1):\n                histograms[-1][i] = counts.get(i, 0) + histograms[-1].get(i - 1, 0)\n        else:\n            for i in range(min_bin, max_bin + 1):\n                histograms[-1][i] = counts.get(i, 0)\n    if not histograms:\n        histograms = [OrderedDict()]\n\n    # fill in the zero counts between the integer bins of the histogram\n    aligned_histograms = []\n\n    for i in range(min_bin, max_bin + 1):\n        aligned_histograms += [tuple([i] + [hist.get(i, 0) for hist in histograms])]\n\n    if to_str:\n        # FIXME: add header row\n        return str_from_table(aligned_histograms, sep=sep, max_rows=365 * 2 + 1)\n\n    return aligned_histograms"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_similar(obj, labels, default=None, min_similarity=0.5):\n    raise NotImplementedError(\n        \"Unfinished implementation, needs to be in fuzzy_get where list of scores & keywords is sorted.\")\n    labels = listify(labels)\n\n    def not_found(*args, **kwargs):\n        return 0\n\n    min_score = int(min_similarity * 100)\n    for similarity_score in [100, 95, 90, 80, 70, 50, 30, 10, 5, 0]:\n        if similarity_score <= min_score:\n            similarity_score = min_score\n        for label in labels:\n            try:\n                result = obj.get(label, not_found)\n            except AttributeError:\n                try:\n                    result = obj.__getitem__(label)\n                except (IndexError, TypeError):\n                    result = not_found\n            if result is not not_found:\n                return result\n        if similarity_score == min_score:\n            if result is not not_found:\n                return result", "response": "Similar to fuzzy_get but allows non - string keys and a list of possible keys\nvms."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_dict(d, u=None, depth=-1, take_new=True, default_mapping_type=dict, prefer_update_type=False, copy=False):\n    u = u or {}\n    orig_mapping_type = type(d)\n    if prefer_update_type and isinstance(u, Mapping):\n        dictish = type(u)\n    elif isinstance(d, Mapping):\n        dictish = orig_mapping_type\n    else:\n        dictish = default_mapping_type\n    if copy:\n        d = dictish(d)\n    for k, v in viewitems(u):\n        if isinstance(d, Mapping):\n            if isinstance(v, Mapping) and not depth == 0:\n                r = update_dict(d.get(k, dictish()), v, depth=max(depth - 1, -1), copy=copy)\n                d[k] = r\n            elif take_new:\n                d[k] = u[k]\n        elif take_new:\n            d = dictish([(k, u[k])])\n    return d", "response": "Recursively updates a dict - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_name(s, camel=None, lower=None, space='_', remove_prefix=None, language='python', string_type=str):\n    if camel is None and lower is None:\n        lower = True\n    if not s:\n        return None\n    ecma_languages = ['ecma', 'javasc']\n    unicode_languages = ecma_languages\n    language = language or 'python'\n    language = language.lower().strip()[:6]\n    string_type = string_type or str\n    if language in unicode_languages:\n        string_type = str\n    s = string_type(s)  # TODO: encode in ASCII, UTF-8, or the charset used for this file!\n    if remove_prefix and s.startswith(remove_prefix):\n        s = s[len(remove_prefix):]\n    if camel:\n        if space and space == '_':\n            space = ''\n        if any(c in ' \\t\\n\\r' + string.punctuation for c in s) or s.lower() == s:\n            if lower:\n                s = s.lower()\n            s = s.title()\n    elif lower:\n        s = s.lower()\n    # TODO: add language Regexes to filter characters appropriately for python or javascript\n    space_escape = '\\\\' if space and space not in ' _' else ''\n    if language not in ecma_languages:\n        invalid_char_regex = re.compile('[^a-zA-Z0-9' + space_escape + space + ']+')\n    else:\n        # FIXME: Unicode categories and properties only works in Perl Regexes!\n        invalid_char_regex = re.compile('[\\W' + space_escape + space + ']+', re.UNICODE)\n    if space is not None:\n        # get rid of all invalid characters, substitting the space-filler for them all\n        s = invalid_char_regex.sub(space, s)\n        # get rid of duplicate space-filler characters\n        if space:\n            s = re.sub('[' + space_escape + space + ']{2,}', space, s)\n    return s", "response": "Generates a valid python variable name from a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a filename from a string.", "response": "def make_filename(s, space=None, language='msdos', strict=False, max_len=None, repeats=1024):\n    r\"\"\"Process string to remove any characters not allowed by the language specified (default: MSDOS)\n\n    In addition, optionally replace spaces with the indicated \"space\" character\n    (to make the path useful in a copy-paste without quoting).\n\n    Uses the following regular expression to substitute spaces for invalid characters:\n\n        re.sub(r'[ :\\\\/?*&\"<>|~`!]{1}', space, s)\n\n    >>> make_filename(r'Whatever crazy &s $h!7 n*m3 ~\\/ou/ can come up. with.`txt`!', strict=False)\n    'Whatever-crazy-s-$h-7-n-m3-ou-can-come-up.-with.-txt-'\n    >>> make_filename(r'Whatever crazy &s $h!7 n*m3 ~\\/ou/ can come up. with.`txt`!', strict=False, repeats=1)\n    'Whatever-crazy--s-$h-7-n-m3----ou--can-come-up.-with.-txt--'\n    >>> make_filename(r'Whatever crazy &s $h!7 n*m3 ~\\/ou/ can come up. with.`txt`!', repeats=1)\n    'Whatever-crazy--s-$h-7-n-m3----ou--can-come-up.-with.-txt--'\n    >>> make_filename(r'Whatever crazy &s $h!7 n*m3 ~\\/ou/ can come up. with.`txt`!')\n    'Whatever-crazy-s-$h-7-n-m3-ou-can-come-up.-with.-txt-'\n    >>> make_filename(r'Whatever crazy &s $h!7 n*m3 ~\\/ou/ can come up. with.`txt`!', strict=True, repeats=1)\n    'Whatever_crazy_s_h_7_n_m3_ou_can_come_up_with_txt_'\n    >>> make_filename(r'Whatever crazy &s $h!7 n*m3 ~\\/ou/ can come up. with.`txt`!', strict=True, repeats=1, max_len=14)\n    'Whatever_crazy'\n    >>> make_filename(r'Whatever crazy &s $h!7 n*m3 ~\\/ou/ can come up. with.`txt`!', max_len=14)\n    'Whatever-crazy'\n    \"\"\"\n    filename = None\n    if strict or language.lower().strip() in ('strict', 'variable', 'expression', 'python'):\n        if space is None:\n            space = '_'\n        elif not space:\n            space = ''\n        filename = make_name(s, space=space, lower=False)\n    else:\n        if space is None:\n            space = '-'\n        elif not space:\n            space = ''\n    if not filename:\n        if language.lower().strip() in ('posix', 'unix', 'linux', 'centos', 'ubuntu', 'fedora',\n                                        'redhat', 'rhel', 'debian', 'deb'):\n            filename = re.sub(r'[^0-9A-Za-z._-]' + '\\{1,{0}\\}'.format(repeats), space, s)\n        else:\n            filename = re.sub(r'[ :\\\\/?*&\"<>|~`!]{' + ('1,{0}'.format(repeats)) + r'}', space, s)\n    if max_len and int(max_len) > 0 and filename:\n        return filename[:int(max_len)]\n    else:\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_file_ext(filename, ext='txt', sep='.'):\n    path, filename = os.path.split(filename)\n\n    if ext and ext[0] == sep:\n        ext = ext[1:]\n    return os.path.join(path, sep.join(filename.split(sep)[:-1 if filename.count(sep) > 1 else 1] + [ext]))", "response": "r Force the file or path str to end with the indicated extension\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to convert value to one of the desired_types.", "response": "def tryconvert(value, desired_types=SCALAR_TYPES, default=None, empty='', strip=True):\n    \"\"\"\n    Convert value to one of the desired_types specified (in order of preference) without raising an exception.\n\n    If value is empty is a string and Falsey, then return the `empty` value specified.\n    If value can't be converted to any of the desired_types requested, then return the `default` value specified.\n\n    >>> tryconvert('MILLEN2000', desired_types=float, default='GENX')\n    'GENX'\n    >>> tryconvert('1.23', desired_types=[int,float], default='default')\n    1.23\n    >>> tryconvert('-1.0', desired_types=[int,float])  # assumes you want a float if you have a trailing .0 in a str\n    -1.0\n    >>> tryconvert(-1.0, desired_types=[int,float])  # assumes you want an int if int type listed first\n    -1\n    >>> repr(tryconvert('1+1', desired_types=[int,float]))\n    'None'\n    \"\"\"\n    if value in tryconvert.EMPTY:\n        if isinstance(value, basestring):\n            return type(value)(empty)\n        return empty\n    if isinstance(value, basestring):\n        # there may not be any \"empty\" strings that won't be caught by the `is ''` check above, but just in case\n        if not value:\n            return type(value)(empty)\n        if strip:\n            value = value.strip()\n    if isinstance(desired_types, type):\n        desired_types = (desired_types,)\n    if desired_types is not None and len(desired_types) == 0:\n        desired_types = tryconvert.SCALAR\n    if len(desired_types):\n        if (isinstance(desired_types, (list, tuple)) and\n                len(desired_types) and\n                isinstance(desired_types[0], (list, tuple))):\n            desired_types = desired_types[0]\n        elif isinstance(desired_types, type):\n            desired_types = [desired_types]\n    for t in desired_types:\n        try:\n            return t(value)\n        except (ValueError, TypeError, InvalidOperation, InvalidContext):\n            continue\n        # if any other weird exception happens then need to get out of here\n        return default\n    # if no conversions happened successfully then return the default value requested\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transcode(infile, outfile=None, incoding=\"shift-jis\", outcoding=\"utf-8\"):\n    if not outfile:\n        outfile = os.path.basename(infile) + '.utf8'\n    with codecs.open(infile, \"rb\", incoding) as fpin:\n        with codecs.open(outfile, \"wb\", outcoding) as fpout:\n            fpout.write(fpin.read())", "response": "Change encoding of text file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef strip_br(s):\n\n    if isinstance(s, basestring):\n        return re.sub(r'\\s*<\\s*[Bb][Rr]\\s*[/]?\\s*>\\s*$', '', s)\n    elif isinstance(s, (tuple, list)):\n        # strip just the last element in a list or tuple\n        try:\n            return type(s)(list(s)[:-1] + [strip_br(s[-1])])\n        except (IndexError, ValueError, AttributeError, TypeError):  # len(s) == 0\n            return s\n    else:\n        try:\n            return type(s)(strip_br(str(s)))\n        except (IndexError, ValueError, AttributeError, TypeError):  # s is None\n            return s", "response": "r Strip the trailing html linebreak character from a string or sequence of strings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_csv(csv_file, ext='.csv', format=None, delete_empty_keys=False,\n             fieldnames=[], rowlimit=100000000, numbers=False, normalize_names=True, unique_names=True,\n             verbosity=0):\n    r\"\"\"\n    Read a csv file from a path or file pointer, returning a dict of lists, or list of lists (according to `format`)\n\n    filename: a directory or list of file paths\n    numbers: whether to attempt to convert strings in csv to numbers\n\n    TODO:\n        merge with `nlp.util.make_dataframe` function\n\n    Handles unquoted and quoted strings, quoted commas, quoted newlines (EOLs), complex numbers,\n        times, dates, datetimes\n\n    >>> read_csv('\"name\\r\\n\",rank,\"serial\\nnumber\",date <BR />\\t\\n\"McCain, John\",\"1\",\"123456789\",9/11/2001\\n' +\n    ...          'Bob,big cheese,1-23,1/1/2001 12:00 GMT', format='header+values list', numbers=True)\n    [['name', 'rank', 'serial\\nnumber', 'date'], ['McCain, John', 1.0, 123456789.0, '9/11/2001'],\n    ['Bob', 'big cheese', '1-23', '1/1/2001 12:00 GMT']]\n    \"\"\"\n    if not csv_file:\n        return\n    if isinstance(csv_file, basestring):\n        # truncate `csv_file` in case it is a string buffer containing GBs of data\n        path = csv_file[:1025]\n        try:\n            # see http://stackoverflow.com/a/4169762/623735 before trying 'rU'\n            fpin = open(path, 'rUb')  # U = universal EOL reader, b = binary\n        except (IOError, FileNotFoundError):\n            # truncate path more, in case path is used later as a file description:\n            path = csv_file[:128]\n            fpin = StringIO(str(csv_file))\n    else:\n        fpin = csv_file\n        try:\n            path = csv_file.name\n        except (IndexError, ValueError, AttributeError, TypeError):\n            path = 'unknown file buffer path'\n\n    format = format or 'h'\n    format = format[0].lower()\n\n    # if fieldnames not specified then assume that first row of csv contains headings\n    csvr = csv.reader(fpin, dialect=csv.excel)\n    if not fieldnames:\n        while not fieldnames or not any(fieldnames):\n            fieldnames = strip_br([str(s).strip() for s in next(csvr)])\n        if verbosity > 0:\n            logger.info('Column Labels: ' + repr(fieldnames))\n    if unique_names:\n        norm_names = OrderedDict([(fldnm, fldnm) for fldnm in fieldnames])\n    else:\n        norm_names = OrderedDict([(num, fldnm) for num, fldnm in enumerate(fieldnames)])\n    if normalize_names:\n        norm_names = OrderedDict([(num, make_name(fldnm, **make_name.DJANGO_FIELD)) for num, fldnm in enumerate(fieldnames)])\n        # required for django-formatted json files\n        model_name = make_name(path, **make_name.DJANGO_MODEL)\n    if format in 'c':  # columnwise dict of lists\n        recs = OrderedDict((norm_name, []) for norm_name in list(norm_names.values()))\n    elif format in 'vh':\n        recs = [fieldnames]\n    else:\n        recs = []\n    if verbosity > 0:\n        logger.info('Field Names: ' + repr(norm_names if normalize_names else fieldnames))\n    rownum = 0\n    eof = False\n    pbar = None\n    start_seek_pos = fpin.tell() or 0\n    if verbosity > 1:\n        print('Starting at byte {} in file buffer.'.format(start_seek_pos))\n    fpin.seek(0, os.SEEK_END)\n    file_len = fpin.tell() - start_seek_pos  # os.fstat(fpin.fileno()).st_size\n    fpin.seek(start_seek_pos)\n\n    if verbosity > 1:\n        print(('There appear to be {} bytes remaining in the file buffer.' +\n               'Resetting (seek) to starting position in file.').format(file_len))\n    # if verbosity > 0:\n    #     pbar = progressbar.ProgressBar(maxval=file_len)\n    #     pbar.start()\n    while csvr and rownum < rowlimit and not eof:\n        if pbar:\n            pbar.update(fpin.tell() - start_seek_pos)\n        rownum += 1\n        row = []\n        row_dict = OrderedDict()\n        # skip rows with all empty strings as values,\n        while not row or not any(len(x) for x in row):\n            try:\n                row = next(csvr)\n                if verbosity > 1:\n                    logger.info('  row content: ' + repr(row))\n            except StopIteration:\n                eof = True\n                break\n        if eof:\n            break\n        if len(row) and isinstance(row[-1], basestring) and len(row[-1]):\n            row = strip_br(row)\n        if numbers:\n            # try to convert the type to a numerical scalar type (int, float etc)\n            row = [tryconvert(v, desired_types=NUMBERS_AND_DATETIMES, empty=None, default=v) for v in row]\n        if row:\n            N = min(max(len(row), 0), len(norm_names))\n            row_dict = OrderedDict(\n                ((field_name, field_value) for field_name, field_value in zip(\n                    list(list(norm_names.values()) if unique_names else norm_names)[:N], row[:N])\n                    if (str(field_name).strip() or delete_empty_keys is False))\n            )\n            if format in 'dj':  # django json format\n                recs += [{\"pk\": rownum, \"model\": model_name, \"fields\": row_dict}]\n            elif format in 'vhl':  # list of lists of values, with header row (list of str)\n                recs += [[value for field_name, value in viewitems(row_dict)\n                          if (field_name.strip() or delete_empty_keys is False)]]\n            elif format in 'c':  # columnwise dict of lists\n                for field_name in row_dict:\n                    recs[field_name] += [row_dict[field_name]]\n                if verbosity > 2:\n                    print([recs[field_name][-1] for field_name in row_dict])\n            else:\n                recs += [row_dict]\n            if verbosity > 2 and format not in 'c':\n                print(recs[-1])\n\n    if file_len > fpin.tell():\n        logger.info(\"Only %d of %d bytes were read and processed.\" % (fpin.tell(), file_len))\n    if pbar:\n        pbar.finish()\n    fpin.close()\n    if not unique_names:\n        return recs, norm_names\n    return recs", "response": "r Reads a CSV file into a dict of lists or lists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a dict to an object or namespace", "response": "def dict2obj(d):\n    \"\"\"Convert a dict to an object or namespace\n\n\n    >>> d = {'a': 1, 'b': {'c': 2}, 'd': [\"hi\", {'foo': \"bar\"}]}\n    >>> obj = dict2obj(d)\n    >>> obj.b.c\n    2\n    >>> obj.d\n    ['hi', {'foo': 'bar'}]\n    >>> d = {'a': 1, 'b': {'c': 2}, 'd': [(\"hi\", {'foo': \"bar\"})]}\n    >>> obj = dict2obj(d)\n    >>> obj.d.hi.foo\n    'bar'\n    \"\"\"\n    if isinstance(d, (Mapping, list, tuple)):\n        try:\n            d = dict(d)\n        except (ValueError, TypeError):\n            return d\n    else:\n        return d\n    obj = Object()\n    for k, v in viewitems(d):\n        obj.__dict__[k] = dict2obj(v)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncoerces a provided array or sequence generator into a pandas. Series", "response": "def make_series(x, *args, **kwargs):\n    \"\"\"Coerce a provided array/sequence/generator into a pandas.Series object\n    FIXME: Deal with CSR, COO, DOK and other sparse matrices like this:\n       pd.Series(csr.toarray()[:,0])\n         or, if csr.shape[1] == 2\n       pd.Series(csr.toarray()[:,1], index=csr.toarray()[:,0])\n    >>> make_series(range(1, 4))\n    0    1\n    1    2\n    2    3\n    dtype: int64\n    >>> make_series(range(1, 4))\n    0    1\n    1    2\n    2    3\n    dtype: int64\n    >>> make_series(list('ABC'))\n    0    A\n    1    B\n    2    C\n    dtype: object\n    >>> make_series({'a': .8, 'be': .6}, name=None)\n    a     0.8\n    be    0.6\n    dtype: float64\n    \"\"\"\n    if isinstance(x, pd.Series):\n        return x\n    try:\n        if len(args) == 1 and 'pk' not in args:\n            # args is a tuple, so needs to be turned into a list to prepend pk for Series index\n            args = ['pk'] + list(args)\n        df = pd.DataFrame.from_records(getattr(x, 'objects', x).values(*args))\n        if len(df.columns) == 1:\n            return df[df.columns[0]]\n        elif len(df.columns) >= 2:\n            return df.set_index(df.columns[0], drop=False)[df.columns[1]]\n        logger.warn('Unable to coerce {} into a pd.Series using args {} and kwargs {}.'.format(x, args, kwargs))\n        return pd.Series()\n    except (AttributeError, TypeError):\n        kwargs['name'] = getattr(x, 'name', None) if 'name' not in kwargs else kwargs['name']\n        if 'index' in kwargs:\n            x = list(x)\n        try:\n            return pd.Series(x, **kwargs)\n        except (IndexError, ValueError, AttributeError, TypeError):\n            logger.debug(format_exc())\n            try:\n                return pd.Series(np.array(x), **kwargs)\n            except (IndexError, ValueError, AttributeError, TypeError):\n                logger.debug(format_exc())\n                return pd.Series(x, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode(obj):\n    try:\n        return obj.encode(encode.encoding)\n    except AttributeError:\n        pass\n    except UnicodeDecodeError:\n        logger.warning('Problem with byte sequence of type {}.'.format(type(obj)))\n        # TODO: Check PG for the proper encoding and fix Django ORM settings so that unicode can be UTF-8 encoded!\n        return str('').join([c for c in obj if c < MAX_CHR])\n    # TODO: encode sequences of strings and dataframes of strings\n    return obj", "response": "r Encode all unicode strings in a dataframe in the encoding indicated as a fun attribute and return a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_series(series, *args, **kwargs):\n    if not series.dtype == np.dtype('O'):\n        return series\n    if any_generated((isinstance(v, datetime.datetime) for v in series)):\n        series = series.apply(clip_datetime)\n    if any_generated((isinstance(v, basestring) for v in series)):\n        series = series.apply(encode)\n    series = series.apply(try_float_int)\n    return series", "response": "Ensure all datetimes are valid Timestamp objects and dtype is np. datetime64 [ ns ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncoerces a provided table to a Pandas DataFrame.", "response": "def make_dataframe(table, clean=True, verbose=False, **kwargs):\n    \"\"\"Coerce a provided table (QuerySet, list of lists, list of Series)\n    >>> dt = datetime.datetime\n    >>> make_dataframe([[1,2,3],[4,5,6]])\n       0  1  2\n    0  1  2  3\n    1  4  5  6\n    >>> make_dataframe([])\n    Empty DataFrame\n    Columns: []\n    Index: []\n    >>> make_dataframe([OrderedDict([('a', 2), ('b', 3)]), PrettyDict([('a', 4), ('b', 5)])])\n       a  b\n    0  2  3\n    1  4  5\n    >>> make_dataframe([[dt(2700, 1, 1), dt(2015, 11, 2)], [(2700 - 2015) * 365.25 + 60, 1]]).T\n                                         0       1\n    0  2262-04-11 23:47:16.854775+00:00  250256\n    1            2015-11-02 00:00:00+00:00       1\n    \"\"\"\n    if hasattr(table, 'objects') and not callable(table.objects):\n        table = table.objects\n    if hasattr(table, 'filter') and callable(table.values):\n        table = pd.DataFrame.from_records(list(table.values()).all())\n    elif isinstance(table, basestring) and os.path.isfile(table):\n        table = pd.DataFrame.from_csv(table)\n    # elif isinstance(table, ValuesQuerySet) or (isinstance(table, (list, tuple)) and\n    #                                            len(table) and all(isinstance(v, Mapping) for v in table)):\n    #     table = pd.DataFrame.from_records(table)\n    try:\n        table = pd.DataFrame(table, **kwargs)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        table = pd.DataFrame(table)\n    if clean and len(table) and isinstance(table, pd.DataFrame):\n        if verbose:\n            print('Cleaning up OutOfBoundsDatetime values...')\n        for col in table.columns:\n            if any_generated((isinstance(v, DATETIME_TYPES) for v in table[col])):\n                table[col] = clean_series(table[col])\n        table = table.dropna(how='all')\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef column_name_to_date(name):\n    month_nums = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7,\n                  'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n    year_month = re.split(r'[^0-9a-zA-Z]{1}', name.strip().strip('_-/*=:+'))\n    try:\n        year = int(year_month[0])\n        month = year_month[-1]\n    except (ValueError, IndexError):\n        year = int(year_month[-1])\n        month = year_month[0]\n    month = int(month_nums.get(str(month).lower().title(), month or 0))\n    if 0 <= year <= 2100 and 1 <= month <= 12:\n        return datetime.date(year, month, 1)\n    try:\n        year = int(year_month[1])\n        month = int(year_month[0])\n    except (ValueError, IndexError):\n        year. month = 0, 0\n    if 0 <= year <= 2100 and 1 <= month <= 12:\n        return datetime.date(year, month, 1)\n    try:\n        month = int(year_month[1])\n        year = int(year_month[0])\n    except (ValueError, IndexError):\n        year. month = 0, 0\n    if 0 <= year <= 2100 and 1 <= month <= 12:\n        return datetime.date(year, month, 1)", "response": "Convert a column name to a date."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the first digits in a string ignoring sign ( + - ).", "response": "def first_digits(s, default=0):\n    \"\"\"Return the fist (left-hand) digits in a string as a single integer, ignoring sign (+/-).\n    >>> first_digits('+123.456')\n    123\n    \"\"\"\n    s = re.split(r'[^0-9]+', str(s).strip().lstrip('+-' + charlist.whitespace))\n    if len(s) and len(s[0]):\n        return int(s[0])\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef int_pair(s, default=(0, None)):\n    s = re.split(r'[^0-9]+', str(s).strip())\n    if len(s) and len(s[0]):\n        if len(s) > 1 and len(s[1]):\n            return (int(s[0]), int(s[1]))\n        return (int(s[0]), default[1])\n    return default", "response": "Return the digits to either side of a single non - digit character as a 2 - tuple of integers"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_us_postal_code(s, allowed_lengths=(), allowed_digits=()):\n    allowed_lengths = allowed_lengths or tuple(N if N < 6 else N + 1 for N in allowed_digits)\n    allowed_lengths = allowed_lengths or (2, 3, 5, 10)\n    ints = int_pair(s)\n    z = str(ints[0]) if ints[0] else ''\n    z4 = '-' + str(ints[1]) if ints[1] else ''\n    if len(z) == 4:\n        z = '0' + z\n    if len(z + z4) in allowed_lengths:\n        return z + z4\n    elif len(z) in (min(l, 5) for l in allowed_lengths):\n        return z\n    return ''", "response": "Make a US - postal code from a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncoerce a string into a float", "response": "def make_float(s, default='', ignore_commas=True):\n    r\"\"\"Coerce a string into a float\n\n    >>> make_float('12,345')\n    12345.0\n    >>> make_float('12.345')\n    12.345\n    >>> make_float('1+2')\n    3.0\n    >>> make_float('+42.0')\n    42.0\n    >>> make_float('\\r\\n-42?\\r\\n')\n    -42.0\n    >>> make_float('$42.42')\n    42.42\n    >>> make_float('B-52')\n    -52.0\n    >>> make_float('1.2 x 10^34')\n    1.2e+34\n    >>> make_float(float('nan'))\n    nan\n    >>> make_float(float('-INF'))\n    -inf\n    \"\"\"\n    if ignore_commas and isinstance(s, basestring):\n        s = s.replace(',', '')\n    try:\n        return float(s)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        try:\n            return float(str(s))\n        except ValueError:\n            try:\n                return float(normalize_scientific_notation(str(s), ignore_commas))\n            except ValueError:\n                try:\n                    return float(first_digits(s))\n                except ValueError:\n                    return default"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncoercing a string into an integer.", "response": "def make_int(s, default='', ignore_commas=True):\n    r\"\"\"Coerce a string into an integer (long ints will fail)\n\n    TODO:\n    - Ignore dashes and other punctuation within a long string of digits,\n       like a telephone number, partnumber, datecode or serial number.\n    - Use the Decimal type to allow infinite precision\n    - Use regexes to be more robust\n\n    >>> make_int('12345')\n    12345\n    >>> make_int('0000012345000       ')\n    12345000\n    >>> make_int(' \\t\\n123,450,00\\n')\n    12345000\n    \"\"\"\n    if ignore_commas and isinstance(s, basestring):\n        s = s.replace(',', '')\n    try:\n        return int(s)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        pass\n    try:\n        return int(re.split(str(s), '[^-0-9,.Ee]')[0])\n    except ValueError:\n        try:\n            return int(float(normalize_scientific_notation(str(s), ignore_commas)))\n        except (ValueError, TypeError):\n            try:\n                return int(first_digits(s))\n            except (ValueError, TypeError):\n                return default"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nproduces a string convertable with float s if possible fixing some common scientific notations and allows addition.", "response": "def normalize_scientific_notation(s, ignore_commas=True, verbosity=1):\n    \"\"\"Produce a string convertable with float(s), if possible, fixing some common scientific notations\n\n    Deletes commas and allows addition.\n    >>> normalize_scientific_notation(' -123 x 10^-45 ')\n    '-123e-45'\n    >>> normalize_scientific_notation(' -1+1,234 x 10^-5,678 ')\n    '1233e-5678'\n    >>> normalize_scientific_notation('$42.42')\n    '42.42'\n    \"\"\"\n    s = s.lstrip(charlist.not_digits_nor_sign)\n    s = s.rstrip(charlist.not_digits)\n    # print s\n    # TODO: substitute ** for ^ and just eval the expression rather than insisting on a base-10 representation\n    num_strings = rex.scientific_notation_exponent.split(s, maxsplit=2)\n    # print num_strings\n    # get rid of commas\n    s = rex.re.sub(r\"[^.0-9-+\" + \",\" * int(not ignore_commas) + r\"]+\", '', num_strings[0])\n    # print s\n    # if this value gets so large that it requires an exponential notation, this will break the conversion\n    if not s:\n        return None\n    try:\n        s = str(eval(s.strip().lstrip('0')))\n    except (IndexError, ValueError, AttributeError, TypeError):\n        if verbosity > 1:\n            print('Unable to evaluate %s' % repr(s))\n        try:\n            s = str(float(s))\n        except (IndexError, ValueError, AttributeError, TypeError):\n            print('Unable to float %s' % repr(s))\n            s = ''\n    # print s\n    if len(num_strings) > 1:\n        if not s:\n            s = '1'\n        s += 'e' + rex.re.sub(r'[^.0-9-+]+', '', num_strings[1])\n    if s:\n        return s\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncoerce a string or nested list of strings into a flat list of strings.", "response": "def normalize_names(names):\n    \"\"\"Coerce a string or nested list of strings into a flat list of strings.\"\"\"\n    if isinstance(names, basestring):\n        names = names.split(',')\n    names = listify(names)\n    return [str(name).strip() for name in names]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncount the occurrence of a category of valid characters within an iterable of serial numbers.", "response": "def string_stats(strs, valid_chars='012346789', left_pad='0', right_pad='', strip=True):\n    \"\"\" Count the occurrence of a category of valid characters within an iterable of serial/model no, etc \"\"\"\n    if left_pad is None:\n        left_pad = ''.join(c for c in rex.ASCII_CHARACTERS if c not in valid_chars)\n    if right_pad is None:\n        right_pad = ''.join(c for c in rex.ASCII_CHARACTERS if c not in valid_chars)\n\n    def normalize(s):\n        if strip:\n            s = s.strip()\n        s = s.lstrip(left_pad)\n        s = s.rstrip(right_pad)\n        return s\n\n    # should probably check to make sure memory not exceeded\n    strs = [normalize(s) for s in strs]\n    lengths = Counter(len(s) for s in strs)\n    counts = {}\n    max_length = max(lengths.keys())\n\n    for i in range(max_length):\n        # print i\n        for s in strs:\n            if i < len(s):\n                counts[i] = counts.get(i, 0) + int(s[i] in valid_chars)\n                counts[-i - 1] = counts.get(-i - 1, 0) + int(s[-i - 1] in valid_chars)\n        long_enough_strings = float(sum(c for n, c in list(lengths.items()) if n >= i))\n        counts[i] = counts[i] / long_enough_strings\n        counts[-i - 1] = counts[-i - 1] / long_enough_strings\n\n    return counts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multisplit(s, seps=list(string.punctuation) + list(string.whitespace), blank=True):\n    seps = str().join(seps)\n    return [s2 for s2 in s.translate(str().join([(chr(i) if chr(i) not in seps else seps[0])\n                                                 for i in range(256)])).split(seps[0]) if (blank or s2)]", "response": "A simple split function that returns a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_words(s, splitter_regex=rex.word_sep_except_external_appostrophe,\n              preprocessor=strip_HTML, postprocessor=strip_edge_punc, min_len=None,\n              max_len=None, blacklist=None, whitelist=None, lower=False,\n              filter_fun=None, str_type=str):\n    r\"\"\"Segment words (tokens), returning a list of all tokens\n\n    Does not return any separating whitespace or punctuation marks.\n    Attempts to return external apostrophes at the end of words.\n    Comparable to `nltk.word_toeknize`.\n\n    Arguments:\n      splitter_regex (str or re): compiled or uncompiled regular expression\n        Applied to the input string using `re.split()`\n      preprocessor (function): defaults to a function that strips out all HTML tags\n      postprocessor (function): a function to apply to each token before return it as an element in the word list\n        Applied using the `map()` builtin\n      min_len (int): delete all words shorter than this number of characters\n      max_len (int): delete all words longer than this number of characters\n      blacklist and whitelist (list of str): words to delete or preserve\n      lower (bool): whether to convert all words to lowercase\n      str_type (type): typically `str` or `unicode`, any type constructor that should can be applied\n        to all words before returning the list\n\n    Returns:\n      list of str: list of tokens\n\n    >>> get_words('He said, \"She called me \\'Hoss\\'!\". I didn\\'t hear.')\n    ['He', 'said', 'She', 'called', 'me', 'Hoss', 'I', \"didn't\", 'hear']\n    >>> get_words('The foxes\\' oh-so-tiny den was 2empty!')\n    ['The', 'foxes', 'oh-so-tiny', 'den', 'was', '2empty']\n    \"\"\"\n    # TODO: Get rid of `lower` kwarg (and make sure code that uses it doesn't break)\n    #       That and other simple postprocessors can be done outside of get_words\n    postprocessor = postprocessor or str_type\n    preprocessor = preprocessor or str_type\n    if min_len is None:\n        min_len = get_words.min_len\n    if max_len is None:\n        max_len = get_words.max_len\n    blacklist = blacklist or get_words.blacklist\n    whitelist = whitelist or get_words.whitelist\n    filter_fun = filter_fun or get_words.filter_fun\n    lower = lower or get_words.lower\n    try:\n        s = open(s, 'r')\n    except (IOError, FileNotFoundError):\n        pass\n    try:\n        s = s.read()\n    except (IOError, AttributeError, TypeError):\n        pass\n    if not isinstance(s, basestring):\n        try:\n            # flatten the list of lists of words from each obj (file or string)\n            return [word for obj in s for word in get_words(obj)]\n        except (IOError, IndexError, ValueError, AttributeError, TypeError):\n            pass\n    try:\n        s = preprocessor(s)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        pass\n    if isinstance(splitter_regex, basestring):\n        splitter_regex = re.compile(splitter_regex)\n    s = list(map(postprocessor, splitter_regex.split(s)))\n    s = list(map(str_type, s))\n    if not filter_fun:\n        return s\n    return [word for word in s if\n            filter_fun(word, min_len=min_len, max_len=max_len,\n                       blacklist=blacklist, whitelist=whitelist, lower=lower)]", "response": "r Returns a list of words in a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields a list of strings in a tabulate format.", "response": "def tabulate(lol, headers, eol='\\n'):\n    \"\"\"Use the pypi tabulate package instead!\"\"\"\n    yield '| %s |' % ' | '.join(headers) + eol\n    yield '| %s:|' % ':| '.join(['-' * len(w) for w in headers]) + eol\n    for row in lol:\n        yield '| %s |' % '  |  '.join(str(c) for c in row) + eol"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intify(obj, str_fun=str, use_ord=True, use_hash=True, use_len=True):\n    try:\n        return int(obj)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        pass\n    try:\n        float_obj = float(obj)\n        if float('-inf') < float_obj < float('inf'):\n            # WARN: This will increment sys.maxint by +1 and decrement sys.maxint by -1!!!!\n            #       But hopefully these cases will be dealt with as expected, above\n            return int(float_obj)\n    except (IndexError, ValueError, AttributeError, TypeError):\n        pass\n    if not str_fun:\n        def str_fun(x):\n            return x\n    if use_ord:\n        try:\n            return ord(str_fun(obj)[0].lower())\n        except (IndexError, ValueError, AttributeError, TypeError):\n            pass\n    if use_hash:\n        try:\n            return hash(str_fun(obj))\n        except (IndexError, ValueError, AttributeError, TypeError):\n            pass\n    if use_len:\n        try:\n            return len(obj)\n        except (IndexError, ValueError, AttributeError, TypeError):\n            pass\n        try:\n            return len(str_fun(obj))\n        except (IndexError, ValueError, AttributeError, TypeError):\n            pass\n    return None", "response": "Convert an object into an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an N - length list with elements values extrapolating as necessary.", "response": "def listify(values, N=1, delim=None):\n    \"\"\"Return an N-length list, with elements values, extrapolating as necessary.\n\n    >>> listify(\"don't split into characters\")\n    [\"don't split into characters\"]\n    >>> listify(\"len = 3\", 3)\n    ['len = 3', 'len = 3', 'len = 3']\n    >>> listify(\"But split on a delimeter, if requested.\", delim=',')\n    ['But split on a delimeter', ' if requested.']\n    >>> listify([\"obj 1\", \"obj 2\", \"len = 4\"], N=4)\n    ['obj 1', 'obj 2', 'len = 4', 'len = 4']\n    >>> listify(iter(\"len=7\"), N=7)\n    ['l', 'e', 'n', '=', '7', '7', '7']\n    >>> listify(iter(\"len=5\"))\n    ['l', 'e', 'n', '=', '5']\n    >>> listify(None, 3)\n    [[], [], []]\n    >>> listify([None],3)\n    [None, None, None]\n    >>> listify([], 3)\n    [[], [], []]\n    >>> listify('', 2)\n    ['', '']\n    >>> listify(0)\n    [0]\n    >>> listify(False, 2)\n    [False, False]\n    \"\"\"\n    ans = [] if values is None else values\n\n    # convert non-string non-list iterables into a list\n    if hasattr(ans, '__iter__') and not isinstance(ans, basestring):\n        ans = list(ans)\n    else:\n        # split the string (if possible)\n        if isinstance(delim, basestring) and isinstance(ans, basestring):\n            try:\n                ans = ans.split(delim)\n            except (IndexError, ValueError, AttributeError, TypeError):\n                ans = [ans]\n        else:\n            ans = [ans]\n\n    # pad the end of the list if a length has been specified\n    if len(ans):\n        if len(ans) < N and N > 1:\n            ans += [ans[-1]] * (N - len(ans))\n    else:\n        if N > 1:\n            ans = [[]] * N\n\n    return ans"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unlistify(n, depth=1, typ=list, get=None):\n    i = 0\n    if depth is None:\n        depth = 1\n    index_desired = get or 0\n    while i < depth and isinstance(n, typ):\n        if len(n):\n            if len(n) > index_desired:\n                n = n[index_desired]\n                i += 1\n        else:\n            return n\n    return n", "response": "Return the desired element in a list ignoring the rest."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef strip_keys(d, nones=False, depth=0):\n    ans = type(d)((str(k).strip(), v) for (k, v) in viewitems(OrderedDict(d))\n                  if (not nones or (str(k).strip() and str(k).strip() != 'None')))\n    if int(depth) < 1:\n        return ans\n    if int(depth) > strip_keys.MAX_DEPTH:\n        warnings.warn(RuntimeWarning(\"Maximum recursion depth allowance (%r) exceeded.\" % strip_keys.MAX_DEPTH))\n    for k, v in viewitems(ans):\n        if isinstance(v, Mapping):\n            ans[k] = strip_keys(v, nones=nones, depth=int(depth) - 1)\n    return ans", "response": "r Strip whitespace from all dictionary keys to the depth indicated\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_table_from_csv(filename='ssg_report_aarons_returns.csv', delimiter=',', dos=False):\n    table = []\n    with open(filename, 'rb') as f:\n        reader = csv.reader(f, dialect='excel', delimiter=delimiter)\n        for row in reader:\n            table += [row]\n    if not dos:\n        return table\n    return dos_from_table(table)", "response": "Returns a dictionary of sequences from a CSV file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shorten(s, max_len=16):\n    short = s\n    words = [abbreviate(word) for word in get_words(s)]\n    for i in range(len(words), 0, -1):\n        short = ' '.join(words[:i])\n        if len(short) <= max_len:\n            break\n    return short[:max_len]", "response": "Shorten a phrase by deleting words at the end of the phrase."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_kmers(seq, k=4):\n    if isinstance(seq, basestring):\n        for i in range(len(seq) - k + 1):\n            yield seq[i:i + k]\n    elif isinstance(seq, (int, float, Decimal)):\n        for s in generate_kmers(str(seq)):\n            yield s\n    else:\n        for s in seq:\n            yield generate_kmers(s, k)", "response": "Return a generator of all the k - mers within a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a Counter object for all the k - mer substrings within a short symbol.", "response": "def kmer_counter(seq, k=4):\n    \"\"\"Return a sequence of all the unique substrings (k-mer or q-gram) within a short (<128 symbol) string\n\n    Used for algorithms like UniqTag for genome unique identifier locality sensitive hashing.\n\n    jellyfish is a C implementation of k-mer counting\n\n    If seq is a string generate a sequence of k-mer string\n    If seq is a sequence of strings then generate a sequence of generators or sequences of k-mer strings\n    If seq is a sequence of sequences of strings generate a sequence of sequence of generators ...\n\n    Default k = 4 because that's the length of a gene base-pair?\n\n    >>> kmer_counter('AGATAGATAGACACAGAAATGGGACCACAC') == Counter({'ACAC': 2, 'ATAG': 2, 'CACA': 2,\n    ...     'TAGA': 2, 'AGAT': 2, 'GATA': 2, 'AGAC': 1, 'ACAG': 1, 'AGAA': 1, 'AAAT': 1, 'TGGG': 1, 'ATGG': 1,\n    ...     'ACCA': 1, 'GGAC': 1, 'CCAC': 1, 'CAGA': 1, 'GAAA': 1, 'GGGA': 1, 'GACA': 1, 'GACC': 1, 'AATG': 1})\n    True\n    \"\"\"\n    if isinstance(seq, basestring):\n        return Counter(generate_kmers(seq, k))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kmer_set(seq, k=4):\n    if isinstance(seq, basestring):\n        return set(generate_kmers(seq, k))", "response": "Return the set of k - length substrings within a sequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dict of objects and thier counts but only count > 1", "response": "def count_duplicates(items):\n    \"\"\"Return a dict of objects and thier counts (like a Counter), but only count > 1\"\"\"\n    c = Counter(items)\n    return dict((k, v) for (k, v) in viewitems(c) if v > 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef slug_from_dict(d, max_len=128, delim='-'):\n    return slug_from_iter(list(d.values()), max_len=max_len, delim=delim)", "response": "Produce a short URI - friendly string from an iterable Mapping"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nproducing a slug from an iterable", "response": "def slug_from_iter(it, max_len=128, delim='-'):\n    \"\"\"Produce a slug (short URI-friendly string) from an iterable (list, tuple, dict)\n\n    >>> slug_from_iter(['.a.', '=b=', '--alpha--'])\n    'a-b-alpha'\n    \"\"\"\n\n    nonnull_values = [str(v) for v in it if v or ((isinstance(v, (int, float, Decimal)) and str(v)))]\n    return slugify(delim.join(shorten(v, max_len=int(float(max_len) / len(nonnull_values)))\n                              for v in nonnull_values), word_boundary=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef slash_product(string_or_seq, slash='/', space=' '):\n    # Terminating case is a sequence of strings without any slashes\n    if not isinstance(string_or_seq, basestring):\n        # If it's not a string and has no slashes, we're done\n        if not any(slash in s for s in string_or_seq):\n            return list(string_or_seq)\n        ans = []\n        for s in string_or_seq:\n            # slash_product of a string will always return a flat list\n            ans += slash_product(s)\n        return slash_product(ans)\n    # Another terminating case is a single string without any slashes\n    if slash not in string_or_seq:\n        return [string_or_seq]\n    # The third case is a string with some slashes in it\n    i = string_or_seq.index(slash)\n    head, tail = string_or_seq[:i].split(space), string_or_seq[i + 1:].split(space)\n    alternatives = head[-1], tail[0]\n    head, tail = space.join(head[:-1]), space.join(tail[1:])\n    return slash_product([space.join([head, word, tail]).strip(space) for word in alternatives])", "response": "Return a list of all possible meanings of a phrase containing slashes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a header a new image", "response": "def create_header(coord, radius, proj='ZEA', npix=30):\n    \"\"\" Create a header a new image \"\"\"\n    gal = coord.name == 'galactic'\n    values = [\n            [\"NAXIS\",  2,          ],\n \n            [\"NAXIS1\", npix,       ],\n            [\"NAXIS2\", npix,       ],\n \n            [\"CTYPE1\", 'GLON-%s'%proj if gal else 'RA---%s'%proj ],\n            [\"CTYPE2\", 'GLAT-%s'%proj if gal else 'DEC--%s'%proj ],\n \n            [\"CRPIX1\", npix/2. + 0.5,       ],\n            [\"CRPIX2\", npix/2. + 0.5,       ],\n \n            [\"CRVAL1\", coord.l.deg if gal else coord.ra.deg,        ],\n            [\"CRVAL2\", coord.b.deg if gal else coord.dec.deg,       ],\n \n            [\"CDELT1\", -3.*radius/npix,       ],\n            [\"CDELT2\",  3.*radius/npix,        ],\n    ]\n \n    if not gal:\n        values += [\n            ['RADECSYS','FK5'],\n            ['EQUINOX',2000],\n        ]\n \n    cards = [pyfits.Card(*i) for i in values]\n    header=pyfits.Header(cards=cards)\n \n    return header"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an empty sky region at location of skydir", "response": "def create_header_data(coord, radius=10., **kwargs):\n    \"\"\" Make an empty sky region at location of skydir\n    skydir : skymaps.SkyDir object\n    size   : size of region (deg.)\n    kwargs : arguments passed to create_header\n    \"\"\"\n    header = create_header(coord, radius=radius, **kwargs)\n    data = np.zeros( (header['NAXIS1'],header['NAXIS2']) )\n    return header, data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_skyreg(header, data, **kwargs):\n    kwargs.setdefault('cmap','binary')\n    fig = plt.figure()\n    ax = pywcsgrid2.subplot(111, header=header)\n    ax.set_ticklabel_type(\"dms\")\n    im = ax.imshow(data, origin=\"center\", **kwargs)\n    ax.grid()\n    ax.add_compass(loc=1,coord='fk5')\n    ax.add_compass(loc=4,coord='gal')\n    return ax, im", "response": "Plot a sky region defined by header and data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimate_angle(coord, angle, new_frame, offset=1e-7):\n    delta = delta_coord(coord, angle, offset)\n    new_coord = coord.transform_to(new_frame)\n    new_delta = delta.transform_to(new_frame)\n    return new_coord.position_angle(new_delta).deg", "response": "Estimate the angle of a given coordinate."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits string text into word tokens using the Penn Treebank rules", "response": "def word_tokenize(text):\n    \"\"\"\n    Split string `text` into word tokens using the Penn Treebank rules\n    \"\"\"\n    for (regexp, replacement) in RULES1:\n        text = sub(regexp, replacement, text)\n    # add extra space to make things easier\n    text = \" \" + text + \" \"\n    for (regexp, replacement) in RULES2:\n        text = sub(regexp, replacement, text)\n    for regexp in CONTRACTIONS:\n        text = sub(regexp, r\"\\1 \\2 \", text)\n    # split and return\n    return text.split()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_python(self, value):\n\n        if not value or isinstance(value, BaseGeometry):\n            return value\n\n        geometry_type = value['type']\n        geometry = value['geometry']\n\n        try:\n            if geometry_type == 'esriGeometryPoint':\n                if 'x' in geometry:\n                    data = json.loads(geometry)\n                    x, y = data['x'], data['y']\n                else:\n                    x, y = [float(val) for val in geometry.split(',')]\n                return Point(x, y)\n\n            elif geometry_type == 'esriGeometryMultipoint':\n                data = json.loads(geometry)\n                return MultiPoint([(p['0'], p['1']) for p in data['points']])\n\n            elif geometry_type == 'esriGeometryPolyline':\n                data = json.loads(geometry)\n                return MultiLineString([((l[0][0], l[0][1]), (l[1][0], l[1][1])) for l in data['paths']])\n\n            elif geometry_type == 'esriGeometryPolygon':\n                data = json.loads(geometry)\n                rings = [LinearRing([(p[0], p[1]) for p in r]) for r in data['rings']]\n                return Polygon([r for r in rings if not r.is_ccw], interiors=[r for r in rings if r.is_ccw])\n\n            elif geometry_type == 'esriGeometryEnvelope':\n                if 'xmin' in geometry:\n                    data = json.loads(geometry)\n                    xmin, ymin, xmax, ymax = [data[k] for k in ('xmin', 'ymin', 'xmax', 'ymax')]\n                else:\n                    xmin, ymin, xmax, ymax = [float(val) for val in geometry.split(',')]\n                return MultiPoint([(xmin, ymin), (xmax, ymax)]).envelope\n\n            else:\n                raise ValueError\n        except ValueError:\n            raise ValidationError('Invalid geometry')", "response": "Converts the value into a Python object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_signalcheck(self, sar, **params):\n        params = sar\n        endpoint = 'rest/signal/check'\n\n        # The 'sar'-request dictionary should be sent as valid JSON data, so\n        # we need to convert it to JSON\n        # when we construct the request in API.request\n        retValue = self._API__request(endpoint, 'POST',\n                                      params=params, convJSON=True)\n\n        return retValue", "response": "This is the main function for the signal check API."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequests - Returns dict of response from postcode.nl API. This method is called only by the EndpointMixin methods.", "response": "def __request(self, endpoint, method='GET', params=None, convJSON=False):\n        \"\"\"request - Returns dict of response from postcode.nl API.\n\n        This method is called only by the EndpointMixin methods.\n        \"\"\"\n        url = '%s/%s' % (self.api_url, endpoint)\n\n        method = method.lower()\n        params = params or {}\n        if convJSON:\n            params = json.dumps(params)\n\n        func = getattr(self.client, method)\n        request_args = {}\n        if method == 'get':\n            request_args['params'] = params\n        else:\n            request_args['data'] = params\n\n        try:\n            # Normally some valid HTTP-response will be the case\n            # if not some exception regarding the request / connection has\n            # occurred\n            # this will be one of the exceptions of the request module\n            # if so, we will a PostcodeError exception and pass the request\n            # exception message\n            response = func(url, **request_args)\n        except requests.RequestException as e:\n            raise PostcodeError(\"ERRrequest\", {\"exception\": e.__doc__})\n\n        content = response.content.decode('utf-8')\n\n        content = json.loads(content)\n\n        if response.status_code == 200:\n            return content\n\n        # Errors, otherwise we did not get here ...\n        if 'exceptionId' in content:\n            raise PostcodeError(content['exceptionId'], content)\n\n        raise PostcodeError(\"UnknownExceptionFromPostcodeNl\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validateSatellite(config, isochrone, kernel, stellar_mass, distance_modulus, trials=1, debug=False, seed=0):\n    logger.info('=== Validate Satellite ===')\n\n    config.params['kernel']['params'] = [kernel.r_h] # TODO: Need better solution to update size??\n    logger.debug('Using Plummer profile spatial model with half-light radius %.2f deg'%(config.params['kernel']['params'][0]))\n    roi = ugali.observation.roi.ROI(config, kernel.lon, kernel.lat)\n    simulator = ugali.simulation.simulator.Simulator(config, roi=roi)\n    catalog_base = ugali.observation.catalog.Catalog(config,roi=roi)\n    mask = ugali.observation.mask.Mask(config, roi)\n\n    coords = (kernel.lon, kernel.lat)\n    \n    results = {'mc_lon': [],\n               'mc_lat': [],\n               'mc_distance_modulus': [],\n               'mc_stellar_mass': [],\n               'mc_radius': [],\n               'richness': [],\n               'log_likelihood': [],\n               'richness_lower': [],\n               'richness_upper': [],\n               'richness_limit': [],\n               'f': [],\n               'stellar_mass': []}\n\n    numpy.random.seed(seed)\n\n    for ii in range(0, trials):\n        logger.info('=== Running Satellite %i ==='%ii)\n\n        # Simulate\n        catalog_satellite = simulator.satellite(isochrone, kernel, stellar_mass, distance_modulus, mc_source_id=1)\n        #catalog_bootstrap = catalog_base.bootstrap()\n        #catalog_merge = ugali.observation.catalog.mergeCatalogs([catalog_bootstrap, catalog_satellite])\n        catalog_background = simulator.satellite(mc_source_id=1)\n        catalog_merge = ugali.observation.catalog.mergeCatalogs([catalog_background, catalog_satellite])\n\n        # Analyze\n        likelihood = ugali.analysis.likelihood.Likelihood(config, roi, mask, catalog_merge, isochrone, kernel)\n                                                               \n        likelihood.precomputeGridSearch([distance_modulus])\n        richness, log_likelihood, richness_lower, richness_upper, richness_upper_limit, richness_raw, log_likelihood_raw, p, f = likelihood.gridSearch(coords=coords, distance_modulus_index=0)\n\n        results['mc_lon'].append(kernel.lon)\n        results['mc_lat'].append(kernel.lat)\n        results['mc_distance_modulus'].append(distance_modulus)\n        results['mc_stellar_mass'].append(stellar_mass)\n        results['mc_radius'].append(kernel.r_h)\n        results['richness'].append(richness)\n        results['log_likelihood'].append(log_likelihood)\n        results['richness_lower'].append(richness_lower)\n        results['richness_upper'].append(richness_upper)\n        results['richness_limit'].append(richness_upper_limit)\n        results['f'].append(f)\n        results['stellar_mass'].append(richness * isochrone.stellarMass())\n\n        logger.info('MC Stellar Mass = %.2f, Measured Stellar Mass = %.2f'%(stellar_mass,richness * isochrone.stellarMass()))\n        if debug:\n            return likelihood, richness, log_likelihood, richness_lower, richness_upper, richness_upper_limit, richness_raw, log_likelihood_raw, p, f\n\n    return results", "response": "Tool for simple MC validation studies -- specifically to create multiple realizations of\n    a satellite given an CompositeIsochrone object, Kernel object, stellar mass (M_sol) for normalization,\n    and distance_modulus."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate membership probability and accuracy of a likelihood.", "response": "def validateMembership(likelihood, p, mc_source_id=1):\n    \"\"\"\n    Plot membership probabilities and MC source ids together.\n    \"\"\"\n\n    cut_mc_source_id = (likelihood.catalog.mc_source_id == mc_source_id)\n\n    # Spatial\n\n    projector = ugali.utils.projector.Projector(likelihood.kernel.lon, likelihood.kernel.lat)\n    x, y = projector.sphereToImage(likelihood.catalog.lon, likelihood.catalog.lat)\n    \n    pylab.figure()\n    pylab.scatter(x[cut_mc_source_id], y[cut_mc_source_id], c='gray', s=100, edgecolors='none')\n    pylab.scatter(x, y, c=p, edgecolors='none')\n    pylab.colorbar()\n    pylab.title('Membership Probability')\n    pylab.xlabel(r'$\\delta$ Lon (deg)')\n    pylab.ylabel(r'$\\delta$ Lat (deg)')\n    \n    # Spectral\n    \n    pylab.figure()\n    pylab.scatter(likelihood.catalog.color[cut_mc_source_id], likelihood.catalog.mag[cut_mc_source_id], c='gray', s=100, edgecolors='none')\n    pylab.scatter(likelihood.catalog.color, likelihood.catalog.mag, c=p, edgecolors='none')\n    pylab.colorbar()\n    pylab.title('Membership Probability')\n    pylab.xlim(likelihood.roi.bins_color[0], likelihood.roi.bins_color[-1])\n    pylab.ylim(likelihood.roi.bins_mag[-1], likelihood.roi.bins_mag[0])\n    pylab.xlabel('Color (mag)')\n    pylab.ylabel('Magnitude (mag)')\n    \n    # Membership accuracy\n\n    p_array = []\n    frac_members_array = []\n    frac_members_array_low = []\n    frac_members_array_high = []\n\n    probability_bins = numpy.arange(0., 1. + 1.e-10, 0.1)\n    for ii in range(0, len(probability_bins) - 1):\n\n        if not numpy.any(numpy.logical_and(p >= probability_bins[ii], p < probability_bins[ii + 1])):\n            continue\n        \n        #p_array.append(0.5 * (probability_bins[ii] + probability_bins[ii + 1]))\n        cut = numpy.logical_and(p >= probability_bins[ii], p < probability_bins[ii + 1])\n        p_array.append(numpy.median(p[cut]))\n\n        n_members = numpy.sum(numpy.all([likelihood.catalog.mc_source_id == mc_source_id,\n                                         p >= probability_bins[ii],\n                                         p < probability_bins[ii + 1]], axis=0))\n        n_not_members = numpy.sum(numpy.all([likelihood.catalog.mc_source_id != mc_source_id,\n                                             p >= probability_bins[ii],\n                                             p < probability_bins[ii + 1]], axis=0))\n\n        frac_members, (frac_members_low, frac_members_high) = ugali.utils.bayesian_efficiency.confidenceInterval(n_members + n_not_members,\n                                                                                                                 n_members,\n                                                                                                                 0.68)\n        frac_members_array.append(frac_members)\n        frac_members_array_low.append(frac_members - frac_members_low)\n        frac_members_array_high.append(frac_members_high - frac_members)\n        \n    pylab.figure()\n    pylab.plot([0,1], [0,1], c='red')\n    pylab.errorbar(p_array, frac_members_array, yerr=[frac_members_array_low, frac_members_array_high],\n                   marker='o', color='blue', linestyle='none')\n    pylab.xlim(0, 1)\n    pylab.ylim(0, 1)\n    pylab.xlabel('Membership Probability')\n    pylab.ylabel('Fraction of True Satellite Members')\n\n    # Where does richness come from?\n    \n    p_array = []\n    p_sum_array = []\n    n_members_array = []\n\n    for ii in range(0, len(probability_bins) - 1):\n        p_array.append(0.5 * (probability_bins[ii] + probability_bins[ii + 1]))\n        p_sum_array.append(numpy.sum(p[numpy.logical_and(p >= probability_bins[ii],\n                                                         p < probability_bins[ii + 1])]))\n        n_members_array.append(numpy.sum(numpy.all([likelihood.catalog.mc_source_id == mc_source_id,\n                                                    p >= probability_bins[ii],\n                                                    p < probability_bins[ii + 1]], axis=0)))\n        \n    pylab.figure()\n    pylab.scatter(p_array, p_sum_array, marker='o', c='blue')\n    pylab.scatter(p_array, n_members_array, marker='o', c='red')\n    pylab.xlim(0, 1)\n    #pylab.ylim(0, 1)\n    pylab.xlabel('Membership Probability')\n    pylab.ylabel('Membership Probability Sum')\n\n    # Purity and completeness\n\n    x = numpy.linspace(0., 1., 1001) \n    purity = []\n    completeness = []\n\n    purity_reconstructed = []\n    completeness_reconstructed = []\n\n    for ii in range(0, len(x)):\n        cut = p > (1 - x[ii])\n\n        if numpy.sum(cut) < 1:\n            purity.append(1.)\n            completeness.append(0.)\n            purity_reconstructed.append(1.)\n            completeness_reconstructed.append(0.)\n            continue\n        \n        purity_cut = numpy.logical_and(cut, likelihood.catalog.mc_source_id == mc_source_id)\n        completeness_cut = likelihood.catalog.mc_source_id == mc_source_id\n        \n        purity.append(float(numpy.sum(purity_cut)) / numpy.sum(cut))\n        completeness.append(float(numpy.sum(purity_cut)) / numpy.sum(completeness_cut))\n\n        purity_reconstructed.append(numpy.mean(p[cut]))\n        completeness_reconstructed.append(numpy.sum(p[cut]) / numpy.sum(p))\n        \n    pylab.figure()\n    pylab.plot(x, purity, c='red', label='Purity')\n    pylab.plot(x, completeness, c='blue', label='Completeness')\n    pylab.plot(x, purity_reconstructed, c='red', linestyle='--', label='Purity Reconstructed')\n    pylab.plot(x, completeness_reconstructed, c='blue', linestyle='--', label='Completeness Reconstructed')\n    pylab.xlim(0, 1)\n    pylab.ylim(0, 1)\n    pylab.xlabel('1 - Membership Probability')\n    #pylab.ylabel('Purity or Completeness')\n    pylab.legend(loc='lower center')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef regression_and_plot(x, y=None):\n    if y is None:\n        y = x\n        x = range(len(x))\n    if not isinstance(x[0], (float, int, np.float64, np.float32)):\n        x = [row[0] for row in x]\n    A = np.vstack([np.array(x), np.ones(len(x))]).T\n    fit = np.linalg.lstsq(A, y, rcond=None)\n    # if fit is None:\n    #     fit = [(1, 0), None, None, None]\n    poly = fit[0][0], fit[0][-1]\n    poly = regressionplot(x, y, poly)\n    return poly", "response": "Fit a line to the x y data supplied and plot it along with teh raw samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a 2 - D linear regression of the raw data", "response": "def regressionplot(x, y, poly=None):\n    \"\"\"\n    Plot a 2-D linear regression (y = slope * x + offset) overlayed over the raw data samples\n    \"\"\"\n    if not isinstance(x[0], (float, int, np.float64, np.float32)):\n        x = [row[0] for row in x]\n    y_regression = poly[0] * np.array(x) + poly[-1]\n    try:\n        plt.ion()\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        plt.plot(x, y_regression, 'r-', x, y, 'o', markersize=5)\n        plt.legend(['%+.2g * x + %.2g' % poly, 'Samples'])\n        ax.grid(True)\n        plt.draw()\n    except:\n        logger.warn('No display available')\n    return y_regression"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scatmat(df, category=None, colors='rgob',\n            num_plots=4, num_topics=100, num_columns=4,\n            show=False, block=False, data_path=DATA_PATH, save=False, verbose=1):\n    \"\"\"Scatter plot with colored markers depending on the discrete values in a \"category\" column\n\n    FIXME: empty plots that dont go away, Plot and/save scatter matrix in groups of num_columns topics\"\"\"\n    if category is None:\n        category = list(df.columns)[-1]\n    if isinstance(category, (str, bytes, int)) and category in df.columns:\n        category = df[category]\n    else:\n        category = pd.Series(category)\n\n    suffix = '{}x{}'.format(*list(df.shape))\n    # suffix = compose_suffix(len(df), num_topics, save)\n    # save = bool(save)\n    for i in range(min(num_plots * num_columns, num_topics) / num_plots):\n        scatter_matrix(df[df.columns[i * num_columns:(i + 1) * num_columns]],\n                       marker='+', c=[colors[int(x) % len(colors)] for x in category.values],\n                       figsize=(18, 12))\n    if save:\n        name = 'scatmat_topics_{}-{}.jpg'.format(i * num_columns, (i + 1) * num_columns) + suffix\n        plt.savefig(os.path.join(data_path, name + '.jpg'))\n    if show:\n        if block:\n            plt.show()\n        else:\n            plt.show(block=False)", "response": "Scatter plot with colored markers depending on the discrete values in a category column."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef point_cloud(df, columns=[0, 1, 2]):\n    df = df if isinstance(df, pd.DataFrame) else pd.DataFrame(df)\n    if not all(c in df.columns for c in columns):\n        columns = list(df.columns)[:3]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')  # noqa\n    Axes3D.scatter(*[df[columns[i]] for i in range(3)], zdir='z', s=20, c=None, depthshade=True)\n    return ax", "response": "3 - D Point cloud for plotting things like mesh models of horses ;"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a sequence of bools ( True / False into a list of the start and stop of the True sections or spans", "response": "def mask2spans(mask, index=None):\n    \"\"\"Convert a sequence of bools (True/False) into a list of the start and stop of the True \"sections\" or spans\"\"\"\n    index = list(range(len(mask))) if index is None else index\n\n    mask = pd.Series(mask)\n    mask = mask.astype(int).fillna(0).diff().fillna(0)\n    starts = list(index[mask > 0])\n    stops = list(index[mask < 0])\n    if len(stops) == len(starts) - 1:\n        stops += [index.values[-1]]\n    spans = join_spans(join_spans(zip(starts, stops)))\n    return spans"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show(self, block=False):\n        try:\n            plt.show(block=block)\n        except ValueError:\n            plt.show()", "response": "Display the last image drawn"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves colormap to file", "response": "def save(self, filename):\n        \"\"\" save colormap to file\"\"\"\n        plt.savefig(filename, fig=self.fig, facecolor='black', edgecolor='black')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an array to a numpy array.", "response": "def asscalar(a):\n    \"\"\" https://github.com/numpy/numpy/issues/4701 \"\"\"\n    # Do we want to check that the value is numeric?\n    #if   isinstance(value, (int, long, float)): return value\n    try:\n        return np.asscalar(a)\n    except AttributeError as e:\n        return np.asscalar(np.asarray(a))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the named parameter.", "response": "def getp(self, name):\n        \"\"\" \n        Get the named parameter.\n\n        Parameters\n        ----------\n        name : string\n            The parameter name.\n\n        Returns\n        -------\n        param : \n            The parameter object.\n        \"\"\"\n        name = self._mapping.get(name,name)\n        return self.params[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the value and bounds of the named parameter.", "response": "def setp(self, name, value=None, bounds=None, free=None, errors=None):\n        \"\"\" \n        Set the value (and bounds) of the named parameter.\n\n        Parameters\n        ----------\n        name : string\n            The parameter name.\n        value: \n            The value of the parameter\n        bounds: None\n            The bounds on the parameter\n        Returns\n        -------\n        None\n        \"\"\"\n        name = self._mapping.get(name,name)\n        self.params[name].set(value,bounds,free,errors)\n        self._cache(name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_attributes(self, **kwargs):\n        kwargs = dict(kwargs)\n        for name,value in kwargs.items():\n            # Raise AttributeError if param not found\n            self.__getattr__(name) \n            # Set attributes\n            try: self.setp(name,**value)\n            except TypeError:\n                try:  self.setp(name,*value)\n                except (TypeError,KeyError):  \n                    self.__setattr__(name,value)", "response": "Set a group of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef representer(dumper, data):\n        tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n        return dumper.represent_mapping(tag,list(data.todict().items()),flow_style=True)", "response": "Represent a dictionary of object identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a color - probability look - up table to a file.", "response": "def writeColorLUT2(config,\n                  outfile=None, isochrone=None, distance_modulus_array=None,\n                  delta_mag=None, mag_err_array=None,\n                  mass_steps=10000, plot=False):\n    \"\"\"\n    Precompute a 4-dimensional signal color probability look-up table to speed up the likelihood evaluation.\n    Inputs are a Config object (or file name), an Isochrone object,\n    an array of distance moduli at which to evaluate the signal color probability,\n    and an array of magnitude uncertainties which set the bin edges of those dimensions (zero implicity included).\n    Finally there is an outfile name.\n    \"\"\"\n    if plot: import ugali.utils.plotting\n\n    if type(config) == str:\n        config = ugali.utils.config.Config(config)\n    if outfile is None:\n        outfile = config.params['color_lut']['filename']\n    if isochrone is None:\n        isochrones = []\n        for ii, name in enumerate(config.params['isochrone']['infiles']):\n            isochrones.append(ugali.isochrone.Isochrone(config, name))\n        isochrone = ugali.isochrone.CompositeIsochrone(isochrones, config.params['isochrone']['weights'])\n    if distance_modulus_array is None:\n        distance_modulus_array = config.params['color_lut']['distance_modulus_array']\n    if delta_mag is None:\n        delta_mag = config.params['color_lut']['delta_mag']\n    if mag_err_array is None:\n        mag_err_array = config.params['color_lut']['mag_err_array']\n\n    mag_buffer = 0.5 # Safety buffer in magnitudes around the color-magnitude space defined by the ROI\n    epsilon = 1.e-10\n    if config.params['catalog']['band_1_detection']:\n        bins_mag_1 = numpy.arange(config.params['mag']['min'] - mag_buffer,\n                                  config.params['mag']['max'] + mag_buffer + epsilon,\n                                  delta_mag)\n        bins_mag_2 = numpy.arange(config.params['mag']['min'] - config.params['color']['max'] - mag_buffer,\n                                  config.params['mag']['max'] - config.params['color']['min'] + mag_buffer + epsilon,\n                                  delta_mag)\n    else:\n        bins_mag_1 = numpy.arange(config.params['mag']['min'] + config.params['color']['min'] - mag_buffer,\n                                  config.params['mag']['max'] + config.params['color']['max'] + mag_buffer + epsilon,\n                                  delta_mag)\n        bins_mag_2 = numpy.arange(config.params['mag']['min'] - mag_buffer,\n                                  config.params['mag']['max'] + mag_buffer + epsilon,\n                                  delta_mag)\n\n    # Output binning configuration\n    #print config.params['catalog']['band_1_detection']\n    #print config.params['mag']['min'], config.params['mag']['max']\n    #print config.params['color']['min'], config.params['color']['max']\n\n    #print bins_mag_1[0], bins_mag_1[-1], len(bins_mag_1)\n    #print bins_mag_2[0], bins_mag_2[-1], len(bins_mag_2)\n\n    isochrone_mass_init, isochrone_mass_pdf, isochrone_mass_act, isochrone_mag_1, isochrone_mag_2 = isochrone.sample(mass_steps=mass_steps)\n\n    hdul = pyfits.HDUList()\n\n    for index_distance_modulus, distance_modulus in enumerate(distance_modulus_array):\n\n        logger.debug('(%i/%i)'%(index_distance_modulus, len(distance_modulus_array)))\n\n        columns_array = []\n        \n        time_start = time.time()\n\n        histo_isochrone_pdf = numpy.histogram2d(distance_modulus + isochrone_mag_1,\n                                                distance_modulus + isochrone_mag_2,\n                                                bins=[bins_mag_1, bins_mag_2],\n                                                weights=isochrone_mass_pdf)[0]\n        \n        if plot:\n            # Checked that axis are plotted correctly\n            ugali.utils.plotting.twoDimensionalHistogram('Isochrone', 'mag_1', 'mag_2',\n                                                         numpy.log10(histo_isochrone_pdf + epsilon).transpose(),\n                                                         bins_mag_1, bins_mag_2,\n                                                         lim_x=None, lim_y=None,\n                                                         vmin=None, vmax=None)\n\n            \n        \n        for index_mag_err_1, mag_err_1 in enumerate(mag_err_array):\n            for index_mag_err_2, mag_err_2 in enumerate(mag_err_array):\n                logger.debug('  Distance modulus = %.2f mag_err_1 = %.2f mag_err_2 = %.2f'%(distance_modulus, mag_err_1, mag_err_2))\n\n                mag_1_sigma_step = delta_mag / mag_err_1\n                n = int(numpy.ceil(4. / mag_1_sigma_step))\n                mag_1_sigma = numpy.arange(-1. * (n + 0.5) * mag_1_sigma_step,\n                                           ((n + 0.5) * mag_1_sigma_step) + epsilon,\n                                           mag_1_sigma_step)\n                mag_1_pdf_array = scipy.stats.norm.cdf(mag_1_sigma[1:]) - scipy.stats.norm.cdf(mag_1_sigma[0:-1])\n\n                mag_2_sigma_step = delta_mag / mag_err_2\n                n = int(numpy.ceil(4. / mag_2_sigma_step))\n                mag_2_sigma = numpy.arange(-1. * (n + 0.5) * mag_2_sigma_step,\n                                           ((n + 0.5) * mag_2_sigma_step) + epsilon,\n                                           mag_2_sigma_step)\n                mag_2_pdf_array = scipy.stats.norm.cdf(mag_2_sigma[1:]) - scipy.stats.norm.cdf(mag_2_sigma[0:-1])\n\n                mag_1_pdf, mag_2_pdf = numpy.meshgrid(mag_2_pdf_array, mag_1_pdf_array)\n                \n                pdf = mag_1_pdf * mag_2_pdf\n\n                histo_isochrone_pdf_convolve = scipy.signal.convolve2d(histo_isochrone_pdf, pdf, mode='same')\n\n                if plot:\n                    # Checked that axis are plotted correctly\n                    ugali.utils.plotting.twoDimensionalHistogram('Convolved Isochrone', 'mag_1', 'mag_2',\n                                                                 numpy.log10(histo_isochrone_pdf_convolve + epsilon).transpose(),\n                                                                 bins_mag_1, bins_mag_2,\n                                                                 lim_x=None, lim_y=None,\n                                                                 vmin=None, vmax=None)\n\n                columns_array.append(pyfits.Column(name = '%i%i'%(index_mag_err_1, index_mag_err_2),\n                                                   format = '%iE'%(histo_isochrone_pdf_convolve.shape[1]),\n                                                   array = histo_isochrone_pdf_convolve))\n\n        hdu = pyfits.new_table(columns_array)\n        hdu.header.update('DIST_MOD', distance_modulus)\n        hdu.name = '%.2f'%(distance_modulus)\n        hdul.append(hdu)\n\n        time_end = time.time()\n        logger.debug('%.2f s'%(time_end - time_start))\n\n    # Store distance modulus info\n    columns_array = [pyfits.Column(name = 'DISTANCE_MODULUS',\n                                   format = 'E',\n                                   array = distance_modulus_array)]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'DISTANCE_MODULUS'\n    hdul.append(hdu)\n\n    # Store magnitude error info\n    columns_array = [pyfits.Column(name = 'BINS_MAG_ERR',\n                                   format = 'E',\n                                   array = numpy.insert(mag_err_array, 0, 0.))]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'BINS_MAG_ERR'\n    hdul.append(hdu)\n\n    # Store magnitude 1 info\n    columns_array = [pyfits.Column(name = 'BINS_MAG_1',\n                                   format = 'E',\n                                   array = bins_mag_1)]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'BINS_MAG_1'\n    hdul.append(hdu)\n\n    # Store magnitude 2 info\n    columns_array = [pyfits.Column(name = 'BINS_MAG_2',\n                                   format = 'E',\n                                   array = bins_mag_2)]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'BINS_MAG_2'\n    hdul.append(hdu)\n\n    logger.info('Writing look-up table to %s'%(outfile))\n    hdul.writeto(outfile, clobber = True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef writeColorLUT(config,\n                  outfile=None, isochrone=None, distance_modulus_array=None,\n                  delta_mag=None, mag_err_array=None,\n                  mass_steps=1000000, plot=False):\n    \"\"\"\n    Precompute a 4-dimensional signal color probability look-up table to speed up the likelihood evaluation.\n    Inputs are a Config object (or file name), an Isochrone object,\n    an array of distance moduli at which to evaluate the signal color probability,\n    and an array of magnitude uncertainties which set the bin edges of those dimensions (zero implicity included).\n    Finally there is an outfile name.\n    \"\"\"\n    if plot: import ugali.utils.plotting\n    if type(config) == str:\n        config = ugali.utils.config.Config(config)\n    if outfile is None:\n        outfile = config.params['color_lut']['filename']\n    if isochrone is None:\n        isochrones = []\n        for ii, name in enumerate(config.params['isochrone']['infiles']):\n            isochrones.append(ugali.isochrone.Isochrone(config, name))\n        isochrone = ugali.isochrone.CompositeIsochrone(isochrones, config.params['isochrone']['weights'])\n    if distance_modulus_array is None:\n        distance_modulus_array = config.params['color_lut']['distance_modulus_array']\n    if delta_mag is None:\n        delta_mag = config.params['color_lut']['delta_mag']\n    if mag_err_array is None:\n        mag_err_array = config.params['color_lut']['mag_err_array']\n\n    mag_buffer = 0.5 # Safety buffer in magnitudes around the color-magnitude space defined by the ROI\n    epsilon = 1.e-10\n    if config.params['catalog']['band_1_detection']:\n        bins_mag_1 = numpy.arange(config.params['mag']['min'] - mag_buffer,\n                                  config.params['mag']['max'] + mag_buffer + epsilon,\n                                  delta_mag)\n        bins_mag_2 = numpy.arange(config.params['mag']['min'] - config.params['color']['max'] - mag_buffer,\n                                  config.params['mag']['max'] - config.params['color']['min'] + mag_buffer + epsilon,\n                                  delta_mag)\n    else:\n        bins_mag_1 = numpy.arange(config.params['mag']['min'] + config.params['color']['min'] - mag_buffer,\n                                  config.params['mag']['max'] + config.params['color']['max'] + mag_buffer + epsilon,\n                                  delta_mag)\n        bins_mag_2 = numpy.arange(config.params['mag']['min'] - mag_buffer,\n                                  config.params['mag']['max'] + mag_buffer + epsilon,\n                                  delta_mag)\n\n    # Output binning configuration\n    #print config.params['catalog']['band_1_detection']\n    #print config.params['mag']['min'], config.params['mag']['max']\n    #print config.params['color']['min'], config.params['color']['max']\n\n    #print bins_mag_1[0], bins_mag_1[-1], len(bins_mag_1)\n    #print bins_mag_2[0], bins_mag_2[-1], len(bins_mag_2)\n\n    isochrone_mass_init, isochrone_mass_pdf, isochrone_mass_act, isochrone_mag_1, isochrone_mag_2 = isochrone.sample(mass_steps=mass_steps)\n\n    # make randoms\n    randoms_1 = numpy.random.normal(0., 1., len(isochrone_mass_pdf))\n    randoms_2 = numpy.random.normal(0., 1., len(isochrone_mass_pdf))\n\n    hdul = pyfits.HDUList()\n\n    for index_distance_modulus, distance_modulus in enumerate(distance_modulus_array):\n\n        logger.debug('(%i/%i)'%(index_distance_modulus, len(distance_modulus_array)))\n\n        columns_array = []\n        \n        time_start = time.time()\n        \n        for index_mag_err_1, mag_err_1 in enumerate(mag_err_array):\n            for index_mag_err_2, mag_err_2 in enumerate(mag_err_array):\n                logger.debug('  (%i/%i) Distance modulus = %.2f mag_err_1 = %.3f mag_err_2 = %.3f'%(index_mag_err_1 * len(mag_err_array) + index_mag_err_2,\n                                                                                             len(mag_err_array)**2,\n                                                                                             distance_modulus,\n                                                                                             mag_err_1,\n                                                                                             mag_err_2))\n                \n                # Add randoms\n                histo_isochrone_pdf = numpy.histogram2d(distance_modulus + isochrone_mag_1 + randoms_1 * mag_err_1,\n                                                        distance_modulus + isochrone_mag_2 + randoms_2 * mag_err_2,\n                                                        bins=[bins_mag_1, bins_mag_2],\n                                                        weights=isochrone_mass_pdf)[0]\n                \n                if plot:\n                    # Checked that axis are plotted correctly\n                    ugali.utils.plotting.twoDimensionalHistogram('Convolved Isochrone', 'mag_1', 'mag_2',\n                                                                 numpy.log10(histo_isochrone_pdf + epsilon).transpose(),\n                                                                 bins_mag_1, bins_mag_2,\n                                                                 lim_x=None, lim_y=None,\n                                                                 vmin=None, vmax=None)\n                    input('WAIT')\n\n                columns_array.append(pyfits.Column(name = '%i%i'%(index_mag_err_1, index_mag_err_2),\n                                                   format = '%iE'%(histo_isochrone_pdf.shape[1]),\n                                                   array = histo_isochrone_pdf))\n\n        hdu = pyfits.new_table(columns_array)\n        hdu.header.update('DIST_MOD', distance_modulus)\n        hdu.name = '%.2f'%(distance_modulus)\n        hdul.append(hdu)\n\n        time_end = time.time()\n        logger.debug('%.2f s'%(time_end - time_start))\n\n    # Store distance modulus info\n    columns_array = [pyfits.Column(name = 'DISTANCE_MODULUS',\n                                   format = 'E',\n                                   array = distance_modulus_array)]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'DISTANCE_MODULUS'\n    hdul.append(hdu)\n\n    # Store magnitude error info\n    columns_array = [pyfits.Column(name = 'BINS_MAG_ERR',\n                                   format = 'E',\n                                   array = numpy.insert(mag_err_array, 0, 0.))]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'BINS_MAG_ERR'\n    hdul.append(hdu)\n\n    # Store magnitude 1 info\n    columns_array = [pyfits.Column(name = 'BINS_MAG_1',\n                                   format = 'E',\n                                   array = bins_mag_1)]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'BINS_MAG_1'\n    hdul.append(hdu)\n\n    # Store magnitude 2 info\n    columns_array = [pyfits.Column(name = 'BINS_MAG_2',\n                                   format = 'E',\n                                   array = bins_mag_2)]\n    hdu = pyfits.new_table(columns_array)\n    hdu.name = 'BINS_MAG_2'\n    hdul.append(hdu)\n\n    logger.info('Writing look-up table to %s'%(outfile))\n    hdul.writeto(outfile, clobber = True)", "response": "Write a color - probability look - up table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a color look - up table and return the signal color evaluated for each object.", "response": "def readColorLUT(infile, distance_modulus, mag_1, mag_2, mag_err_1, mag_err_2):\n    \"\"\"\n    Take in a color look-up table and return the signal color evaluated for each object.\n    Consider making the argument a Catalog object rather than magnitudes and uncertainties.\n    \"\"\"\n\n    reader = pyfits.open(infile)\n\n    distance_modulus_array = reader['DISTANCE_MODULUS'].data.field('DISTANCE_MODULUS')\n    if not numpy.any(numpy.fabs(distance_modulus_array - distance_modulus) < 1.e-3):\n        logger.warning(\"Distance modulus %.2f not available in file %s\"%(distance_modulus, infile))\n        logger.warning('         available distance moduli:'+str(distance_modulus_array))\n        return False\n\n    distance_modulus_key = '%.2f'%(distance_modulus_array[numpy.argmin(numpy.fabs(distance_modulus_array - distance_modulus))])\n\n    bins_mag_err = reader['BINS_MAG_ERR'].data.field('BINS_MAG_ERR') \n    bins_mag_1 = reader['BINS_MAG_1'].data.field('BINS_MAG_1') \n    bins_mag_2 = reader['BINS_MAG_2'].data.field('BINS_MAG_2') \n    # Note that magnitude uncertainty is always assigned by rounding up, is this the right thing to do?\n    index_mag_err_1 = numpy.clip(numpy.digitize(mag_err_1, bins_mag_err) - 1,\n                                 0, len(bins_mag_err) - 2)\n    index_mag_err_2 = numpy.clip(numpy.digitize(mag_err_2, bins_mag_err) - 1,\n                                 0, len(bins_mag_err) - 2)\n\n    u_color = numpy.zeros(len(mag_1))\n\n    for index_mag_err_1_select in range(0, len(bins_mag_err) - 1):\n        for index_mag_err_2_select in range(0, len(bins_mag_err) - 1):\n            cut = numpy.logical_and(index_mag_err_1 == index_mag_err_1_select,\n                                    index_mag_err_2 == index_mag_err_2_select)\n            if numpy.sum(cut) < 1:\n                continue\n            histo = reader[distance_modulus_key].data.field('%i%i'%(index_mag_err_1_select, index_mag_err_2_select))\n            u_color[cut] = ugali.utils.binning.take2D(histo,\n                                                      mag_2[cut], mag_1[cut],\n                                                      bins_mag_2, bins_mag_1)\n    \n    reader.close()\n    return u_color"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auto_memoize(func):\n\n    @wraps(func)\n    def wrapper(*args):\n        inst = args[0]\n        inst._memoized_values = getattr(inst, '_memoized_values', {})\n        key = (func, args[1:])\n        if key not in inst._memoized_values:\n            inst._memoized_values[key] = func(*args)\n        return inst._memoized_values[key]\n    return wrapper", "response": "A decorator that automatically memoizes instace methods for the lifespan of an object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattempt to convert a PROJ4 projection object to an EPSG code and returns None if conversion fails", "response": "def proj4_to_epsg(projection):\n    \"\"\"Attempts to convert a PROJ4 projection object to an EPSG code and returns None if conversion fails\"\"\"\n\n    def make_definition(value):\n        return {x.strip().lower() for x in value.split('+') if x}\n\n    # Use the EPSG in the definition if available\n    match = EPSG_RE.search(projection.srs)\n    if match:\n        return int(match.group(1))\n\n    # Otherwise, try to look up the EPSG from the pyproj data file\n    pyproj_data_dir = os.path.join(os.path.dirname(pyproj.__file__), 'data')\n    pyproj_epsg_file = os.path.join(pyproj_data_dir, 'epsg')\n    if os.path.exists(pyproj_epsg_file):\n        definition = make_definition(projection.srs)\n        f = open(pyproj_epsg_file, 'r')\n        for line in f.readlines():\n            match = PYPROJ_EPSG_FILE_RE.search(line)\n            if match:\n                file_definition = make_definition(match.group(2))\n                if definition == file_definition:\n                    return int(match.group(1))\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wkt_to_proj4(wkt):\n\n    srs = osgeo.osr.SpatialReference()\n    srs.ImportFromWkt(wkt)\n\n    return pyproj.Proj(str(srs.ExportToProj4()))", "response": "Converts a well - known text string to a pyproj. Proj object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a pyproj. Proj object to a well - known text string", "response": "def proj4_to_wkt(projection):\n    \"\"\"Converts a pyproj.Proj object to a well-known text string\"\"\"\n\n    srs = osgeo.osr.SpatialReference()\n    srs.ImportFromProj4(projection.srs)\n\n    return srs.ExportToWkt()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project_geometry(geometry, source, target):\n\n    project = partial(\n        pyproj.transform,\n        source,\n        target\n    )\n\n    return transform(project, geometry)", "response": "Projects a shapely geometry object from the source to the target projection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload this config from an existing config object or dict.", "response": "def _load(self, config):\n        \"\"\" Load this config from an existing config \n\n        Parameters:\n        -----------\n        config : filename, config object, or dict to load\n\n        Returns:\n        --------\n        params : configuration parameters\n        \"\"\"\n        if isstring(config):\n            self.filename = config\n            params = yaml.load(open(config))\n        elif isinstance(config, Config):\n            # This is the copy constructor...\n            self.filename = config.filename\n            params = copy.deepcopy(config)\n        elif isinstance(config, dict):\n            params = copy.deepcopy(config)\n        elif config is None:\n            params = {}\n        else:\n            raise Exception('Unrecognized input')\n\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nenforce some structure to the config file", "response": "def _validate(self):\n        \"\"\" Enforce some structure to the config file \"\"\"\n        # This could be done with a default config\n\n        # Check that specific keys exist\n        sections = odict([\n                ('catalog',['dirname','basename',\n                            'lon_field','lat_field','objid_field',\n                            'mag_1_band', 'mag_1_field', 'mag_err_1_field',\n                            'mag_2_band', 'mag_2_field', 'mag_err_2_field',\n                            ]),\n                ('mask',[]),\n                ('coords',['nside_catalog','nside_mask','nside_likelihood',\n                           'nside_pixel','roi_radius','roi_radius_annulus',\n                           'roi_radius_interior','coordsys',\n                           ]),\n                ('likelihood',[]),\n                ('output',[]),\n                ('batch',[]),\n                ])  \n\n        keys = np.array(list(sections.keys()))\n        found = np.in1d(keys,list(self.keys()))\n\n        if not np.all(found):\n            msg = 'Missing sections: '+str(keys[~found])\n            raise Exception(msg)\n\n        for section,keys in sections.items():\n            keys = np.array(keys)\n            found = np.in1d(keys,list(self[section].keys()))\n            if not np.all(found):\n                msg = 'Missing keys in %s: '%(section)+str(keys[~found])\n                raise Exception(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat the file paths for the current instance of the class.", "response": "def _formatFilepaths(self):\n        \"\"\"\n        Join dirnames and filenames from config.\n        \"\"\"\n        likedir=self['output']['likedir']\n        self.likefile  = join(likedir,self['output']['likefile'])\n        self.mergefile = join(likedir,self['output']['mergefile'])\n        self.roifile   = join(likedir,self['output']['roifile'])\n\n        searchdir=self['output']['searchdir']\n        self.labelfile  = join(searchdir,self['output']['labelfile'])\n        self.objectfile = join(searchdir,self['output']['objectfile'])\n        self.assocfile  = join(searchdir,self['output']['assocfile'])\n        self.candfile   = join(searchdir,self['output']['candfile'])\n\n        mcmcdir=self['output']['mcmcdir']\n        self.mcmcfile   = join(mcmcdir,self['output']['mcmcfile'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a copy of this config object to a file.", "response": "def write(self, filename):\n        \"\"\"\n        Write a copy of this config object.\n\n        Parameters:\n        -----------\n        outfile : output filename\n        \n        Returns:\n        --------\n        None\n        \"\"\"\n        ext = os.path.splitext(filename)[1]\n        writer = open(filename, 'w')\n        if ext == '.py':\n            writer.write(pprint.pformat(self))\n        elif ext == '.yaml':\n            writer.write(yaml.dump(self))\n        else:\n            writer.close()\n            raise Exception('Unrecognized config format: %s'%ext)\n        writer.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _createFilenames(self,pixels=None):\n        nside_catalog = self['coords']['nside_catalog']\n\n        # Deprecated: ADW 2018-06-17\n        #if nside_catalog is None:\n        #    pixels = [None]\n        if pixels is not None:\n            pixels = [pixels] if np.isscalar(pixels) else pixels\n        else:\n            pixels = np.arange(hp.nside2npix(nside_catalog))   \n\n        npix = len(pixels)\n\n        catalog_dir = self['catalog']['dirname']\n        catalog_base = self['catalog']['basename']\n         \n        mask_dir = self['mask']['dirname']\n        mask_base_1 = self['mask']['basename_1']\n        mask_base_2 = self['mask']['basename_2']\n         \n        data = np.ma.empty(npix,dtype=[('pix',int), ('catalog',object), \n                                       ('mask_1',object), ('mask_2',object)])\n        mask = np.ma.empty(npix,dtype=[('pix',bool), ('catalog',bool), \n                                       ('mask_1',bool), ('mask_2',bool)])\n        for ii,pix in enumerate(pixels):\n            if pix is None:\n                # DEPRECTATED: ADW 2018-06-17\n                # This is not really being used anymore\n                catalog = os.path.join(catalog_dir,catalog_base)\n                mask_1 = os.path.join(mask_dir,mask_base_1)\n                mask_2 = os.path.join(mask_dir,mask_base_2)\n            else:\n                catalog = os.path.join(catalog_dir,catalog_base%pix)\n                mask_1 = os.path.join(mask_dir,mask_base_1%pix)\n                mask_2 = os.path.join(mask_dir,mask_base_2%pix)\n            data[ii]['pix'] = pix if pix is not None else -1\n            data[ii]['catalog'] = catalog\n            data[ii]['mask_1']  = mask_1\n            data[ii]['mask_2']  = mask_2\n         \n            mask[ii]['catalog'] = not os.path.exists(catalog)\n            mask[ii]['mask_1']  = not os.path.exists(mask_1)\n            mask[ii]['mask_2']  = not os.path.exists(mask_2)\n\n        for name in ['catalog','mask_1','mask_2']:\n            if np.all(mask[name]): logger.warn(\"All '%s' files masked\"%name)\n\n        # mask 'pix' if all files not present\n        mask['pix'] = mask['catalog'] | mask['mask_1'] | mask['mask_2']\n\n        if np.all(mask['pix']): logger.warn(\"All pixels masked\")\n                \n        #return np.ma.mrecords.MaskedArray(data, mask, fill_value=[-1,None,None,None])\n        #return np.ma.mrecords.MaskedArray(data, mask, fill_value=[-1,'','',''])\n        return np.ma.MaskedArray(data, mask, fill_value=[-1,'','',''])", "response": "Create a masked record array of all filenames for the given set of pixels and store existence of those files in the mask values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _createFilenames(self):\n        nside_catalog = self['coords']['nside_catalog']\n        npix = hp.nside2npix(nside_catalog)\n        pixels = np.arange(npix)\n\n        catalog_dir = self['catalog']['dirname']\n        catalog_base = self['catalog']['basename']\n        catalog_path = os.path.join(catalog_dir,catalog_base)\n\n        mask_dir    = self['mask']['dirname']\n        mask_base_1 = self['mask']['basename_1']\n        mask_base_2 = self['mask']['basename_2']\n        mask_path_1 = os.path.join(mask_dir,mask_base_1)\n        mask_path_2 = os.path.join(mask_dir,mask_base_2)\n\n        data = np.ma.empty(npix,dtype=[('pix',int), ('catalog',object), \n                                       ('mask_1',object), ('mask_2',object)])\n        mask = np.ma.empty(npix,dtype=[('pix',bool), ('catalog',bool), \n                                       ('mask_1',bool), ('mask_2',bool)])\n\n        # Build the filenames\n        data['pix']     = pixels\n        data['catalog'] = np.char.mod(catalog_path,pixels)\n        data['mask_1']  = np.char.mod(mask_path_1,pixels)\n        data['mask_2']  = np.char.mod(mask_path_2,pixels)\n\n        # Build the mask of existing files using glob\n        mask['catalog'] = ~np.in1d(data['catalog'],glob.glob(catalog_dir+'/*'))\n        mask['mask_1']  = ~np.in1d(data['mask_1'],glob.glob(mask_dir+'/*'))\n        mask['mask_2']  = ~np.in1d(data['mask_2'],glob.glob(mask_dir+'/*'))\n\n        for name in ['catalog','mask_1','mask_2']:\n            if np.all(mask[name]): logger.warn(\"All '%s' files masked\"%name)\n\n        # mask 'pix' if all files not present\n        mask['pix'] = mask['catalog'] | mask['mask_1'] | mask['mask_2']\n\n        if np.all(mask['pix']): logger.warn(\"All pixels masked\")\n\n        return np.ma.MaskedArray(data, mask, fill_value=[-1,'','',''])", "response": "Create a masked records array of all filenames for the given set of pixels and store existence of those files in the mask values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the requested filenames.", "response": "def getFilenames(self,pixels=None):\n        \"\"\"\n        Return the requested filenames.\n\n        Parameters:\n        -----------\n        pixels : requeseted pixels\n\n        Returns:\n        --------\n        filenames : recarray\n        \"\"\"\n        logger.debug(\"Getting filenames...\")\n        if pixels is None:\n            return self.filenames\n        else:\n            return self.filenames[np.in1d(self.filenames['pix'],pixels)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef superpixel(subpix, nside_subpix, nside_superpix):\n    if nside_subpix==nside_superpix: return subpix\n    theta, phi =  hp.pix2ang(nside_subpix, subpix)\n    return hp.ang2pix(nside_superpix, theta, phi)", "response": "Return the indices of the super - pixels which contain each of the sub - pixels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subpixel(superpix, nside_superpix, nside_subpix):\n    if nside_superpix==nside_subpix: return superpix\n    vec = hp.pix2vec(nside_superpix, superpix)\n    radius = np.degrees(2. * hp.max_pixrad(nside_superpix))\n    subpix = query_disc(nside_subpix, vec, radius)\n    pix_for_subpix = superpixel(subpix,nside_subpix,nside_superpix)\n    # Might be able to speed up array indexing...\n    return subpix[pix_for_subpix == superpix]", "response": "Return the indices of sub - pixels within a given super - pixel with resolution nside_subpix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the indices of the super - pixels which contain each of the subpixels ipix.", "response": "def d_grade_ipix(ipix, nside_in, nside_out, nest=False):\n    \"\"\"\n    Return the indices of the super-pixels which contain each of the\n    sub-pixels (nside_in > nside_out).\n\n    Parameters:\n    -----------\n    ipix      : index of the input subpixels\n    nside_in  : nside of the input subpix\n    nside_out : nside of the desired superpixels\n\n    Returns:\n    --------\n    ipix_out  : superpixels for each subpixel\n    \"\"\"\n\n    if nside_in==nside_out: return ipix\n    if not (nside_in > nside_out): \n        raise ValueError(\"nside_out must be less than nside_in\")\n\n    return hp.vec2pix(nside_out, *hp.pix2vec(nside_in, ipix, nest), nest=nest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef u_grade_ipix(ipix, nside_in, nside_out, nest=False):\n\n    if nside_in==nside_out: return ipix\n    if not (nside_in < nside_out): \n        raise ValueError(\"nside_in must be less than nside_out\")\n\n    if nest: nest_ipix = ipix\n    else:    nest_ipix = hp.ring2nest(nside_in, ipix)\n\n    factor = (nside_out//nside_in)**2\n    if np.isscalar(ipix):\n        nest_ipix_out = factor*nest_ipix + np.arange(factor)\n    else:\n        nest_ipix_out = factor*np.asarray(nest_ipix)[:,np.newaxis]+np.arange(factor)\n\n    if nest: return nest_ipix_out\n    else:    return hp.nest2ring(nside_out, nest_ipix_out)", "response": "This function takes a superpixel and returns the indices of sub - pixels within the base image of the base image of the base image of the super - pixel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ud_grade_ipix(ipix, nside_in, nside_out, nest=False):\n    if nside_in == nside_out: return ipix\n    elif nside_in < nside_out:\n        return u_grade_ipix(ipix, nside_in, nside_out, nest)\n    elif nside_in > nside_out:\n        return d_grade_ipix(ipix, nside_in, nside_out, nest)", "response": "Upgrade or degrade resolution of a pixel list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pix2ang(nside, pix, nest=False):\n    theta, phi =  hp.pix2ang(nside, pix, nest=nest)\n    lon = phi2lon(phi)\n    lat = theta2lat(theta)\n    return lon, lat", "response": "Convert pixel coordinates to degrees and degrees."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ang2pix(nside, lon, lat, nest=False):\n    theta = np.radians(90. - lat)\n    phi = np.radians(lon)\n    return healpy.ang2pix(nside, theta, phi, nest=nest)", "response": "Convert from ang to pix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a HEALPix map at the desired resolution.", "response": "def healpixMap(nside, lon, lat, fill_value=0., nest=False):\n    \"\"\"\n    Input (lon, lat) in degrees instead of (theta, phi) in radians.\n    Returns HEALPix map at the desired resolution \n    \"\"\"\n\n    lon_median, lat_median = np.median(lon), np.median(lat)\n    max_angsep = np.max(ugali.utils.projector.angsep(lon, lat, lon_median, lat_median))\n    \n    pix = angToPix(nside, lon, lat, nest=nest)\n    if max_angsep < 10:\n        # More efficient histograming for small regions of sky\n        m = np.tile(fill_value, healpy.nside2npix(nside))\n        pix_subset = ugali.utils.healpix.angToDisc(nside, lon_median, lat_median, max_angsep, nest=nest)\n        bins = np.arange(np.min(pix_subset), np.max(pix_subset) + 1)\n        m_subset = np.histogram(pix, bins=bins - 0.5)[0].astype(float)\n        m[bins[0:-1]] = m_subset\n    else:\n        m = np.histogram(pix, np.arange(hp.nside2npix(nside) + 1))[0].astype(float)\n    if fill_value != 0.:\n        m[m == 0.] = fill_value\n    return m"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a given longitude latitude and nside are contained in a set of pixels.", "response": "def in_pixels(lon,lat,pixels,nside):\n    \"\"\"\n    Check if (lon,lat) in pixel list. Assumes RING formatting.\n\n    Parameters:\n    -----------\n    lon    : longitude (deg)\n    lat    : latitude (deg)\n    pixels : pixel list [RING format] to check for inclusion\n    nside  : nside of pixel list \n\n    Returns:\n    --------\n    inpix : boolean array for inclusion\n    \"\"\"\n    pix = ang2pix(nside,lon,lat)\n    return np.in1d(pix,pixels)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef index_pix_in_pixels(pix,pixels,sort=False,outside=-1):\n    # ADW: Not really safe to set index = -1 (accesses last entry); \n    # -np.inf would be better, but breaks other code...\n\n    # ADW: Are the pixels always sorted? Is there a quick way to check?\n    if sort: pixels = np.sort(pixels)\n\n    # Assumes that 'pixels' is pre-sorted, otherwise...???\n    index = np.searchsorted(pixels,pix)\n    if np.isscalar(index):\n        if not np.in1d(pix,pixels).any(): index = outside\n    else:\n        # Find objects that are outside the pixels\n        index[~np.in1d(pix,pixels)] = outside\n    return index", "response": "Find the indices of a set of pixels into another set of pixels."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index_lonlat_in_pixels(lon,lat,pixels,nside,sort=False,outside=-1):\n\n    pix = ang2pix(nside,lon,lat)\n    return index_pix_in_pixels(pix,pixels,sort,outside)", "response": "Find the indices of a set of angles into a set of pixels"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ang2disc(nside, lon, lat, radius, inclusive=False, fact=4, nest=False):\n    vec = ang2vec(lon,lat)\n    return query_disc(nside,vec,radius,inclusive,fact,nest)", "response": "Wrap query_disc to use lon lat and radius in degrees."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmimic the healpy header keywords.", "response": "def header_odict(nside,nest=False,coord=None, partial=True):\n    \"\"\"Mimic the healpy header keywords.\"\"\"\n    hdr = odict([])\n    hdr['PIXTYPE']=odict([('name','PIXTYPE'),\n                          ('value','HEALPIX'),\n                          ('comment','HEALPIX pixelisation')])\n\n    ordering = 'NEST' if nest else 'RING'\n    hdr['ORDERING']=odict([('name','ORDERING'),\n                           ('value',ordering),\n                           ('comment','Pixel ordering scheme, either RING or NESTED')])\n    hdr['NSIDE']=odict([('name','NSIDE'),\n                        ('value',nside),\n                        ('comment','Resolution parameter of HEALPIX')])\n    if coord:\n        hdr['COORDSYS']=odict([('name','COORDSYS'), \n                               ('value',coord), \n                               ('comment','Ecliptic, Galactic or Celestial (equatorial)')])\n    \n    if not partial:\n        hdr['FIRSTPIX']=odict([('name','FIRSTPIX'),\n                               ('value',0), \n                               ('comment','First pixel # (0 based)')])\n        hdr['LASTPIX']=odict([('name','LASTPIX'),\n                              ('value',hp.nside2npix(nside)-1),\n                              ('comment','Last pixel # (0 based)')])\n    hdr['INDXSCHM']=odict([('name','INDXSCHM'),\n                           ('value','EXPLICIT' if partial else 'IMPLICIT'),\n                           ('comment','Indexing: IMPLICIT or EXPLICIT')])\n    hdr['OBJECT']=odict([('name','OBJECT'), \n                         ('value','PARTIAL' if partial else 'FULLSKY'),\n                         ('comment','Sky coverage, either FULLSKY or PARTIAL')])\n    return hdr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite out a HEALPix map of the same HEALPix image.", "response": "def write_partial_map(filename, data, nside, coord=None, nest=False,\n                      header=None,dtype=None,**kwargs):\n    \"\"\"\n    Partial HEALPix maps are used to efficiently store maps of the sky by only\n    writing out the pixels that contain data.\n\n    Three-dimensional data can be saved by supplying a distance modulus array\n    which is stored in a separate extension.\n\n    Parameters:\n    -----------\n    filename : output file name\n    data     : dictionary or recarray of data to write (must contain 'PIXEL')\n    nside    : healpix nside of data\n    coord    : 'G'alactic, 'C'elestial, 'E'cliptic\n    ordering : 'RING' or 'NEST'\n    kwargs   : Passed to fitsio.write\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    # ADW: Do we want to make everything uppercase?\n\n    if isinstance(data,dict):\n        names = list(data.keys())\n    else:\n        names = data.dtype.names\n\n    if 'PIXEL' not in names:\n        msg = \"'PIXEL' column not found.\"\n        raise ValueError(msg)\n\n    hdr = header_odict(nside=nside,coord=coord,nest=nest)\n    fitshdr = fitsio.FITSHDR(list(hdr.values()))\n    if header is not None:\n        for k,v in header.items():\n            fitshdr.add_record({'name':k,'value':v})\n\n    logger.info(\"Writing %s\"%filename)\n    fitsio.write(filename,data,extname='PIX_DATA',header=fitshdr,clobber=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a partial HEALPix file and return pixels and values and map.", "response": "def read_partial_map(filenames, column, fullsky=True, **kwargs):\n    \"\"\"\n    Read a partial HEALPix file(s) and return pixels and values/map. Can\n    handle 3D healpix maps (pix, value, zdim). Returned array has\n    shape (dimz,npix).\n\n    Parameters:\n    -----------\n    filenames     : list of input filenames\n    column        : column of interest\n    fullsky       : partial or fullsky map\n    kwargs        : passed to fitsio.read\n\n    Returns:\n    --------\n    (nside,pix,map) : pixel array and healpix map (partial or fullsky)\n    \"\"\"\n    # Make sure that PIXEL is in columns\n    #kwargs['columns'] = ['PIXEL',column]\n    kwargs['columns'] = ['PIXEL'] + np.atleast_1d(column).tolist()\n\n    filenames = np.atleast_1d(filenames)\n    header = fitsio.read_header(filenames[0],ext=kwargs.get('ext',1))\n    data = ugali.utils.fileio.load_files(filenames,**kwargs)\n\n    pix = data['PIXEL']\n    value = data[column]\n    nside = header['NSIDE']\n    npix = hp.nside2npix(nside)\n\n    ndupes = len(pix) - len(np.unique(pix))\n    if ndupes > 0:\n        msg = '%i duplicate pixels during load.'%(ndupes)\n        raise Exception(msg)\n\n    if fullsky and not np.isscalar(column):\n        raise Exception(\"Cannot make fullsky map from list of columns.\")\n    \n    if fullsky:\n        shape = list(value.shape)\n        shape[0] = npix\n        hpxmap = hp.UNSEEN * np.ones(shape,dtype=value.dtype)\n        hpxmap[pix] = value\n        return (nside,pix,hpxmap.T)\n    else:\n        return (nside,pix,value.T)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_likelihood_headers(filenames, outfile):\n\n    filenames = np.atleast_1d(filenames)\n\n    ext='PIX_DATA'\n    nside = fitsio.read_header(filenames[0],ext=ext)['LKDNSIDE']\n    \n    keys=['STELLAR','NINSIDE','NANNULUS']\n    data_dict = odict(PIXEL=[])\n    for k in keys:\n        data_dict[k] = []\n\n    for i,filename in enumerate(filenames):\n        logger.debug('(%i/%i) %s'%(i+1, len(filenames), filename))\n        header = fitsio.read_header(filename,ext=ext)\n        data_dict['PIXEL'].append(header['LKDPIX'])\n        for key in keys:\n            data_dict[key].append(header[key])\n\n        del header\n        \n    data_dict['PIXEL'] = np.array(data_dict['PIXEL'],dtype=int)\n    for key in keys:\n        data_dict[key] = np.array(data_dict[key],dtype='f4')\n\n    #import pdb; pdb.set_trace()\n    write_partial_map(outfile, data_dict, nside)\n    return data_dict", "response": "Merge header information from likelihood files into one single dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a healpix map from a fits file.", "response": "def read_map(filename, nest=False, hdu=None, h=False, verbose=True):\n    \"\"\"Read a healpix map from a fits file.  Partial-sky files,\n    if properly identified, are expanded to full size and filled with UNSEEN.\n    Uses fitsio to mirror much (but not all) of the functionality of healpy.read_map\n    \n    Parameters:\n    -----------\n    filename : str \n      the fits file name\n    nest : bool, optional\n      If True return the map in NEST ordering, otherwise in RING ordering;\n      use fits keyword ORDERING to decide whether conversion is needed or not\n      If None, no conversion is performed.\n    hdu : int, optional\n      the header number to look at (start at 0)\n    h : bool, optional\n      If True, return also the header. Default: False.\n    verbose : bool, optional\n      If True, print a number of diagnostic messages\n    \n    Returns\n    -------\n    m [, header] : array, optionally with header appended\n      The map read from the file, and the header if *h* is True.\n    \"\"\"\n    \n    data,hdr = fitsio.read(filename,header=True,ext=hdu)\n\n    nside = int(hdr.get('NSIDE'))\n    if verbose: print('NSIDE = {0:d}'.format(nside))\n\n    if not healpy.isnsideok(nside):\n        raise ValueError('Wrong nside parameter.')\n    sz=healpy.nside2npix(nside)\n\n    ordering = hdr.get('ORDERING','UNDEF').strip()\n    if verbose: print('ORDERING = {0:s} in fits file'.format(ordering))\n\n    schm = hdr.get('INDXSCHM', 'UNDEF').strip()\n    if verbose: print('INDXSCHM = {0:s}'.format(schm))\n    if schm == 'EXPLICIT':\n        partial = True\n    elif schm == 'IMPLICIT':\n        partial = False\n\n    # monkey patch on a field method\n    fields = data.dtype.names\n\n    # Could be done more efficiently (but complicated) by reordering first\n    if hdr['INDXSCHM'] == 'EXPLICIT':\n        m = healpy.UNSEEN*np.ones(sz,dtype=data[fields[1]].dtype)\n        m[data[fields[0]]] = data[fields[1]]\n    else:\n        m = data[fields[0]].ravel()\n\n    if (not healpy.isnpixok(m.size) or (sz>0 and sz != m.size)) and verbose:\n        print('nside={0:d}, sz={1:d}, m.size={2:d}'.format(nside,sz,m.size))\n        raise ValueError('Wrong nside parameter.')\n    if not nest is None:\n        if nest and ordering.startswith('RING'):\n            idx = healpy.nest2ring(nside,np.arange(m.size,dtype=np.int32))\n            if verbose: print('Ordering converted to NEST')\n            m = m[idx]\n            return  m[idx]\n        elif (not nest) and ordering.startswith('NESTED'):\n            idx = healpy.ring2nest(nside,np.arange(m.size,dtype=np.int32))\n            m = m[idx]\n            if verbose: print('Ordering converted to RING')\n\n    if h:\n        return m, header\n    else:\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a number to float or int as appropriate", "response": "def _convert_number(self, number):\n        \"\"\"Converts a number to float or int as appropriate\"\"\"\n\n        number = float(number)\n        return int(number) if number.is_integer() else float(number)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_results(args):\n    config,name,label,coord = args\n\n    filenames = make_filenames(config,label)\n    srcfile = filenames['srcfile']\n    samples = filenames['samfile']\n\n    if not exists(srcfile):\n        logger.warning(\"Couldn't find %s; skipping...\"%srcfile)\n        return\n    if not exists(samples):\n        logger.warning(\"Couldn't find %s; skipping...\"%samples)\n        return\n\n    logger.info(\"Writing %s...\"%srcfile)\n    from ugali.analysis.results import write_results\n    write_results(srcfile,config,srcfile,samples)", "response": "Write the results output file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_membership(args):\n    config,name,label,coord = args\n\n    filenames = make_filenames(config,label)\n    srcfile = filenames['srcfile']\n    memfile = filenames['memfile']\n\n    logger.info(\"Writing %s...\"%memfile)\n    from ugali.analysis.loglike import write_membership\n    write_membership(memfile,config,srcfile,section='source')", "response": "Write the membership output file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_plot(args):\n    import ugali.utils.plotting\n    import pylab as plt\n\n    config,name,label,coord = args\n    filenames = make_filenames(config,label)\n    srcfile = filenames['srcfile']\n    samfile = filenames['samfile']\n    memfile = filenames['memfile']\n\n    if not exists(srcfile):\n        logger.warning(\"Couldn't find %s; skipping...\"%srcfile)\n        return\n    if not exists(samfile):\n        logger.warning(\"Couldn't find %s; skipping...\"%samfile)\n        return\n\n    config = ugali.utils.config.Config(config)\n    burn = config['mcmc']['nburn']*config['mcmc']['nwalkers']\n\n    source = ugali.analysis.source.Source()\n    source.load(srcfile,section='source')\n\n    outfile = samfile.replace('.npy','.png')\n    ugali.utils.plotting.plotTriangle(srcfile,samfile,burn=burn)\n    logger.info(\"  Writing %s...\"%outfile)\n    plt.savefig(outfile,bbox_inches='tight',dpi=60)\n    plt.close()\n\n    plotter = ugali.utils.plotting.SourcePlotter(source,config,radius=0.5)\n\n    data = fitsio.read(memfile,trim_strings=True) if exists(memfile) else None\n    if data is not None:\n        plt.figure()\n        kernel,isochrone = source.kernel,source.isochrone\n        ugali.utils.plotting.plotMembership(config,data,kernel,isochrone)\n        outfile = samfile.replace('.npy','_mem.png')\n        logger.info(\"  Writing %s...\"%outfile)\n        plt.savefig(outfile,bbox_inches='tight',dpi=60)\n        plt.close()\n            \n        plotter.plot6(data)\n\n        outfile = samfile.replace('.npy','_6panel.png')\n        logger.info(\"  Writing %s...\"%outfile)\n        plt.savefig(outfile,bbox_inches='tight',dpi=60)\n\n        outfile = samfile.replace('.npy','_6panel.pdf')\n        logger.info(\"  Writing %s...\"%outfile)\n        plt.savefig(outfile,bbox_inches='tight',dpi=60)\n\n        plt.close()\n\n    try:\n        title = name\n        plotter.plot4()\n        outfile = samfile.replace('.npy','_4panel.png')\n        logger.info(\"  Writing %s...\"%outfile)\n        plt.suptitle(title)\n        plt.savefig(outfile,bbox_inches='tight',dpi=60)\n        plt.close()\n    except:\n        logger.warning(\"  Failed to create plotter.plot4()\")", "response": "Create plots of mcmc output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning our initialized dictionary arguments.", "response": "def parse(self, *args):\n        \"\"\"\n        Return our initialized dictionary arguments.\n        \"\"\"\n\n        if isinstance(self.dictionary, dict):\n            return self.dictionary\n\n        raise self.subparserException(\"Argument passed to Dictionary SubParser is not a dict: %s\" % type(self.dictionary))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _caches_dicts(self):\n\n        qs = (self.get_query_set() if django.VERSION < (1, 6)\n              else self.get_queryset())\n\n        variants_dict = self._get_variants_dict(qs)\n        cache.set(VARIANTS_DICT_CACHE_KEY, variants_dict)\n\n        replace_dict = self._get_replace_dict(qs)\n        cache.set(REPLACE_DICT_CACHE_KEY, replace_dict)\n\n        return variants_dict, replace_dict", "response": "Caches variants_dict and replace_dict in a single database hit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves the path and address of a resource based on consecutive optional arguments.", "response": "def resolve(path_or_address=None, address=None, *ignored):\n    \"\"\"\n    Returns (path, address) based on consecutive optional arguments,\n    [path] [address].\n    \"\"\"\n    if path_or_address is None or address is not None:\n        return path_or_address, address\n\n    path = None\n    if split_address(path_or_address)[1] is not None:\n        address = path_or_address\n    else:\n        path = path_or_address\n\n    return path, address"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_address(address):\n    invalid = None, None\n\n    if not address and address != 0:\n        return invalid\n\n    components = str(address).split(':')\n    if len(components) > 2:\n        return invalid\n\n    if components[0] and not valid_hostname(components[0]):\n        return invalid\n\n    if len(components) == 2 and not valid_port(components[1]):\n        return invalid\n\n    if len(components) == 1:\n        components.insert(0 if valid_port(components[0]) else 1, None)\n\n    host, port = components\n    port = int(port) if port else None\n\n    return host, port", "response": "Splits an address into host and port."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef translate_exception(exc, kwargs):\n    error = exc.response['Error']\n    error.setdefault('Message', '')\n    err_class = EXC.get(error['Code'], DynamoDBError)\n    return err_class(exc.response['ResponseMetadata']['HTTPStatusCode'],\n                     exc_info=sys.exc_info(), args=pformat(kwargs), **error)", "response": "Translate a botocore. exceptions. ClientError into a dynamo3 error"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef re_raise(self):\n        if self.exc_info is not None:\n            six.reraise(type(self), self, self.exc_info[2])\n        else:\n            raise self", "response": "Raise this exception with the original traceback"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef segment_text(text=os.path.join(DATA_PATH, 'goodreads-omniscient-books.txt'),\n                 start=None, stop=r'^Rate\\ this', ignore=r'^[\\d]'):\n    \"\"\" Split text into segments (sections, paragraphs) using regular expressions to trigger breaks.start\n    \"\"\"\n    start = start if hasattr(start, 'match') else re.compile(start) if start else None\n    stop = stop if hasattr(stop, 'match') else re.compile(stop) if stop else None\n    ignore = ignore if hasattr(ignore, 'match') else re.compile(ignore) if ignore else None\n\n    segments = []\n    segment = []\n    with open(text) as fin:\n        for line in fin:\n            if start is not None and start.match(line):\n                segments += [segment] if len(segment) else []\n                segment = [line]\n            elif stop is not None and stop.match(line):\n                segments += [segment]\n                segment = []\n            elif ignore is not None and ignore.match(line):\n                continue\n            else:\n                segment += [segment]", "response": "Splits text into segments using regular expressions to trigger breaks. end\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_ngrams(token_list, n=1, join=' '):\n    join = ' ' if join is True else join\n    if isinstance(join, str):\n        return [join.join(ng) for ng in list_ngrams(token_list, n=n, join=False)]\n    return list(zip(*[token_list[i:] for i in range(n)]))", "response": "Return a list of n - tuples one for each possible sequence of n items in the token_list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of n - tuples one for each possible sequence of n items in the token_list", "response": "def list_ngram_range(token_list, *args, **kwargs):\n    \"\"\"Return a list of n-tuples, one for each possible sequence of n items in the token_list\n\n    Arguments:\n      join (bool or str): if str, then join ngrom tuples on it before returning\n         True is equivalent to join=' '\n         default = True\n\n    >>> list_ngram_range('goodbye cruel world'.split(), 0, 2, join=False)\n    [('goodbye',), ('cruel',), ('world',), ('goodbye', 'cruel'), ('cruel', 'world')]\n    >>> list_ngram_range('goodbye cruel world'.split(), 2, join=False)\n    [('goodbye',), ('cruel',), ('world',), ('goodbye', 'cruel'), ('cruel', 'world')]\n    >>> list_ngram_range('goodbye cruel world'.split(), 0, 2, join='|')\n    ['goodbye', 'cruel', 'world', 'goodbye|cruel', 'cruel|world']\n    >>> list_ngram_range('goodbye cruel world'.split(), 0, 2, join=True)\n    ['goodbye', 'cruel', 'world', 'goodbye cruel', 'cruel world']\n    \"\"\"\n    m, n = (args if len(args) > 1 else ((0, args[0]) if args else (0, 1)))\n    join = args[2] if len(args) > 2 else kwargs.pop('join', True)\n    return list(chain(*(list_ngrams(token_list, i + 1, join=join) for i in range(0, n))))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_sentences(text='', train_path=None, case_sensitive=True, ext=['.md', '.txt', '.asc', '.asciidoc'],\n                       normalize_ordinals=1, normalize_newlines=1, normalize_sentence_boundaries=1,\n                       epochs=20, classifier=nlup.BinaryAveragedPerceptron,\n                       re_eol=r'\\r\\n|\\r|\\n', **kwargs):\n    \"\"\"Generate sentences from a sequence of characters (text)\n\n    Wrapped text (newlines at column 80, for instance) will break this, breaking up sentences.\n    Wrapper and preprocessor for Kyle Gorman's \"DetectorMorse\" module\n\n    Arguments:\n      preprocess (bool): whether to assume common sentence delimitters in markdown and asciidoc formatting\n                         using r'[.?!][ \\t]*\\n\\n|[.?!][ \\t]*\\r\\n\\r\\n|[.?!][ \\t]*\\r\\r|[.?!][ ][ ][A-Z]'\n      case_sensitive (int): whether to consider case to make decisions about sentence boundaries\n      epochs (int): number of epochs (iterations for classifier training)\n\n    \"\"\"\n    ext = [ext] if isinstance(ext, basestring) else ext\n    if isinstance(text, basestring) and len(text) <= 256:\n        if os.path.isfile(text) and os.path.splitext(text)[-1].lower() in ext:\n            text = open(text)\n        elif os.path.isdir(text):\n            return chain.from_iterable((\n                generate_sentences(text=stat['path'], train_path=train_path, ext=ext,\n                                   normalize_ordinals=normalize_ordinals, normalize_newlines=normalize_ordinals,\n                                   normalize_sentence_boundaries=normalize_sentence_boundaries,\n                                   epochs=epochs, classifier=classifier, re_eol=re_eol, **kwargs)\n                for stat in find_files(text, ext=ext)))\n    if isinstance(text, basestring):\n        texts = Split(text=text, re_delim=re_eol)\n    else:\n        texts = chain.from_iterable(Split(text=doc, re_delm=re_eol) for doc in text)\n\n    if normalize_newlines:\n        re_eol = re.compile(r'\\r\\n|\\r')\n        texts = (re_eol.sub(r'\\n', doc) for doc in texts)\n    if normalize_ordinals:\n        re_ord = re.compile(r'\\b([0-9]+|[A-Za-z])[.?!][ \\t]{1,4}([A-Za-z])')\n        texts = (re_ord.sub(r'\\1) \\2', doc) for doc in texts)\n    if normalize_sentence_boundaries:\n        re_eos = re.compile(r'([.?!])([ ][ ])[\\n]?([A-Z])')\n        texts = (re_eos.sub(r'\\1\\n\\3', doc) for doc in texts)\n\n    if train_path:\n        generate_sentences.detector = Detector(slurp(train_path), epochs=epochs, nocase=not case_sensitive)\n    elif not isinstance(getattr(generate_sentences, 'detector', None), Detector):\n        generate_sentences.detector = Detector.load(\n            os.path.join(DATA_PATH, 'wsj_pugnlp.detector_morse.Detector.json.gz'))\n    # generate_sentences.detector = SentenceDetector(text=text, nocase=not case_sensitive,\n    # epochs=epochs, classifier=classifier)\n    return iter(chain.from_iterable((s.lstrip() for s in generate_sentences.detector.segments(text)) for text in texts))", "response": "Generate sentences from a sequence of text."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, now, num):\n        if num == 0:\n            return\n        self.points.append((now, num))", "response": "Add a timestamp and date to the data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef value(self):\n        now = time.time()\n        cutoff = now - self.window\n        while self.points and self.points[0][0] < cutoff:\n            self.points.pop(0)\n        return sum([p[1] for p in self.points])", "response": "Get the sum of all non - expired points"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_consumed(self, key):\n        if key not in self._consumed:\n            self._consumed[key] = {\n                'read': DecayingCapacityStore(),\n                'write': DecayingCapacityStore(),\n            }\n        return self._consumed[key]", "response": "Get a consumed capacity storage dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhooking that runs in response to a returned capacity event", "response": "def on_capacity(self, connection, command, query_kwargs, response,\n                    capacity):\n        \"\"\" Hook that runs in response to a 'returned capacity' event \"\"\"\n        now = time.time()\n        args = (connection, command, query_kwargs, response, capacity)\n        # Check total against the total_cap\n        self._wait(args, now, self.total_cap, self._total_consumed,\n                   capacity.total)\n\n        # Increment table consumed capacity & check it\n        if capacity.tablename in self.table_caps:\n            table_cap = self.table_caps[capacity.tablename]\n        else:\n            table_cap = self.default_cap\n        consumed_history = self.get_consumed(capacity.tablename)\n        if capacity.table_capacity is not None:\n            self._wait(args, now, table_cap, consumed_history,\n                       capacity.table_capacity)\n        # The local index consumed capacity also counts against the table\n        if capacity.local_index_capacity is not None:\n            for consumed in six.itervalues(capacity.local_index_capacity):\n                self._wait(args, now, table_cap, consumed_history, consumed)\n\n        # Increment global indexes\n        # check global indexes against the table+index cap or default\n        gic = capacity.global_index_capacity\n        if gic is not None:\n            for index_name, consumed in six.iteritems(gic):\n                full_name = capacity.tablename + ':' + index_name\n                if index_name in table_cap:\n                    index_cap = table_cap[index_name]\n                elif full_name in self.table_caps:\n                    index_cap = self.table_caps[full_name]\n                else:\n                    # If there's no specified capacity for the index,\n                    # use the cap on the table\n                    index_cap = table_cap\n                consumed_history = self.get_consumed(full_name)\n                self._wait(args, now, index_cap, consumed_history, consumed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting for the user to see if the capacity is exceeded.", "response": "def _wait(self, args, now, cap, consumed_history, consumed_capacity):\n        \"\"\" Check the consumed capacity against the limit and sleep \"\"\"\n        for key in ['read', 'write']:\n            if key in cap and cap[key] > 0:\n                consumed_history[key].add(now, consumed_capacity[key])\n                consumed = consumed_history[key].value\n                if consumed > 0 and consumed >= cap[key]:\n                    seconds = math.ceil(float(consumed) / cap[key])\n                    LOG.debug(\"Rate limited throughput exceeded. Sleeping \"\n                              \"for %d seconds.\", seconds)\n                    if callable(self.callback):\n                        callback_args = args + (seconds,)\n                        if self.callback(*callback_args):\n                            continue\n                    time.sleep(seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_case(self, name, email, subject, description, businessImpact,\n                    priority, phone):\n        \"\"\" Send a case creation to SalesForces to create a ticket.\n\n        @param name of the person creating the case.\n        @param email of the person creating the case.\n        @param subject of the case.\n        @param description of the case.\n        @param businessImpact of the case.\n        @param priority of the case.\n        @param phone of the person creating the case.\n        @return Nothing if this is ok.\n        @raise ServerError when something goes wrong.\n        @raise ValueError when data passed in are invalid\n        \"\"\"\n\n        if not('@' in parseaddr(email)[1]):\n            raise ValueError('invalid email: {}'.format(email))\n        if '' == name or name is None:\n            raise ValueError('empty name')\n        if '' == subject or subject is None:\n            raise ValueError('empty subject')\n        if '' == description or description is None:\n            raise ValueError('empty description')\n        if '' == businessImpact or businessImpact is None:\n            raise ValueError('empty business impact')\n        if priority is None:\n            raise ValueError('Ensure the priority is from the set of '\n                             'known priorities')\n        if '' == phone or phone is None:\n            raise ValueError('empty phone')\n\n        try:\n            r = requests.post(self.url, data={\n                'orgid': self.orgId,\n                'recordType': self.recordType,\n                'name': name,\n                'email': email,\n                'subject': subject,\n                'description': description,\n                self.BUSINESS_IMPACT: businessImpact,\n                'priority': priority,\n                'phone': phone,\n                'external': 1\n                }, timeout=self.timeout)\n            r.raise_for_status()\n        except Timeout:\n            message = 'Request timed out: {url} timeout: {timeout}'\n            message = message.format(url=self.url, timeout=self.timeout)\n            log.error(message)\n            raise ServerError(message)\n        except RequestException as err:\n            log.info('cannot create case: {}'.format(err))\n            raise ServerError(\n                'cannot create case: {}'.format(err))", "response": "Create a new case in SalesForces."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the day dates in between start and end formatted for query. This returns dates inclusive e.g. final day is (end_at, end_at+1 day)", "response": "def date_range_for_webtrends(cls, start_at=None, end_at=None):\n        \"\"\"\n        Get the day dates in between start and end formatted for query.\n        This returns dates inclusive e.g. final day is (end_at, end_at+1 day)\n        \"\"\"\n        if start_at and end_at:\n            start_date = cls.parse_standard_date_string_to_date(\n                start_at)\n            end_date = cls.parse_standard_date_string_to_date(\n                end_at)\n            numdays = (end_date - start_date).days + 1\n            start_dates = [end_date - timedelta(days=x)\n                           for x in reversed(range(0, numdays))]\n            date_range = []\n            for i, date in enumerate(start_dates):\n                query_date = cls.parse_date_for_query(date)\n                date_range.append((query_date, query_date))\n            return date_range\n        else:\n            return [(\"current_day-1\", \"current_day-1\")]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the sum of spending for this category up to and including the given month.", "response": "def get_cumulative_spend(key):\n    \"\"\"\n    Get the sum of spending for this category up to and including the given\n    month.\n    \"\"\"\n    query = ('ROUND(SUM(total_ex_vat), 2) AS total '\n             'FROM {table} '\n             'WHERE date <= \"{year}-{month:02}-01\" '\n             'AND lot=\"{lot}\" '\n             'AND customer_sector=\"{sector}\" '\n             'AND supplier_type=\"{sme_large}\"'.format(\n                 table=_RAW_SALES_TABLE,\n                 year=key.year,\n                 month=key.month,\n                 lot=key.lot,\n                 sector=key.sector,\n                 sme_large=key.sme_large))\n    logging.debug(query)\n    result = scraperwiki.sqlite.select(query)\n    logging.debug(result)\n    value = result[0]['total']\n    return float(result[0]['total']) if value is not None else 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot(self, value=None, pixel=None):\n        # DEPRECATED\n        import ugali.utils.plotting\n\n        map_roi = np.array(hp.UNSEEN \\\n                              * np.ones(hp.nside2npix(self.config.params['coords']['nside_pixel'])))\n        \n        if value is None:\n            #map_roi[self.pixels] = ugali.utils.projector.angsep(self.lon, self.lat, self.centers_lon, self.centers_lat)\n            map_roi[self.pixels] = 1\n            map_roi[self.pixels_annulus] = 0\n            map_roi[self.pixels_target] = 2\n        elif value is not None and pixel is None:\n            map_roi[self.pixels] = value\n        elif value is not None and pixel is not None:\n            map_roi[pixel] = value\n        else:\n            logger.error(\"Can't parse input\")\n        \n        ugali.utils.plotting.zoomedHealpixMap('Region of Interest',\n                                              map_roi,\n                                              self.lon, self.lat,\n                                              self.config.params['coords']['roi_radius'])", "response": "Plot the ROI and the ROI of the region of Interest."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests if coordintes in set of HEALPix pixels.", "response": "def inPixels(self,lon,lat,pixels):\n        \"\"\" Function for testing if coordintes in set of ROI pixels. \"\"\"\n        nside = self.config.params['coords']['nside_pixel']\n        return ugali.utils.healpix.in_pixels(lon,lat,pixels,nside)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the catalog pixels spanned by this ROI.", "response": "def getCatalogPixels(self):\n        \"\"\"\n        Return the catalog pixels spanned by this ROI.\n        \"\"\"\n        filenames = self.config.getFilenames()\n\n        nside_catalog = self.config.params['coords']['nside_catalog']\n        nside_pixel = self.config.params['coords']['nside_pixel']\n        # All possible catalog pixels spanned by the ROI\n        superpix = ugali.utils.skymap.superpixel(self.pixels,nside_pixel,nside_catalog)\n        superpix = np.unique(superpix)\n        # Only catalog pixels that exist in catalog files\n        pixels = np.intersect1d(superpix, filenames['pix'].compressed())\n        return pixels"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the index schema for the specified hash key.", "response": "def schema(self, hash_key):\n        \"\"\"\n        Create the index schema\n\n        Parameters\n        ----------\n        hash_key : :class:`~.DynamoKey`\n            The hash key of the table\n\n        \"\"\"\n        key_schema = [hash_key.hash_schema()]\n        if self.range_key is not None:\n            key_schema.append(self.range_key.range_schema())\n        schema_data = {\n            'IndexName': self.name,\n            'KeySchema': key_schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            }\n        }\n        if self.include_fields is not None:\n            schema_data['Projection']['NonKeyAttributes'] = self.include_fields\n        return schema_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef include(cls, name, range_key, includes):\n        return cls(cls.INCLUDE, name, range_key, includes)", "response": "Create an index that projects key attributes plus some others"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an index from a Dynamo response.", "response": "def from_response(cls, response, attrs):\n        \"\"\" Create an index from returned Dynamo data \"\"\"\n        proj = response['Projection']\n        index = cls(proj['ProjectionType'], response['IndexName'],\n                    attrs[response['KeySchema'][1]['AttributeName']],\n                    proj.get('NonKeyAttributes'))\n        index.response = response\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all(cls, name, hash_key, range_key=None, throughput=None):\n        return cls(cls.ALL, name, hash_key, range_key, throughput=throughput)", "response": "Create an index that projects all attributes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef keys(cls, name, hash_key, range_key=None, throughput=None):\n        return cls(cls.KEYS, name, hash_key, range_key,\n                   throughput=throughput)", "response": "Create an index that projects only key attributes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an index that projects key attributes plus some others", "response": "def include(cls, name, hash_key, range_key=None, includes=None,\n                throughput=None):\n        \"\"\" Create an index that projects key attributes plus some others \"\"\"\n        return cls(cls.INCLUDE, name, hash_key, range_key, includes,\n                   throughput=throughput)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef schema(self):\n        schema_data = super(GlobalIndex, self).schema(self.hash_key)\n        schema_data['ProvisionedThroughput'] = self.throughput.schema()\n        return schema_data", "response": "Construct the schema definition for this index"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_response(cls, response, attrs):\n        proj = response['Projection']\n        hash_key = attrs.get(response['KeySchema'][0]['AttributeName'])\n        range_key = None\n        if len(response['KeySchema']) > 1:\n            range_key = attrs[response['KeySchema'][1]['AttributeName']]\n        throughput = Throughput.from_response(\n            response['ProvisionedThroughput'])\n        index = cls(proj['ProjectionType'], response['IndexName'], hash_key,\n                    range_key, proj.get('NonKeyAttributes'), throughput)\n        index.response = response\n        return index", "response": "Create an index from a Dynamo response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Table from a Dynamo response.", "response": "def from_response(cls, response):\n        \"\"\" Create a Table from returned Dynamo data \"\"\"\n        hash_key = None\n        range_key = None\n        # KeySchema may not be in the response if the TableStatus is DELETING.\n        if 'KeySchema' in response:\n            attrs = dict(((d['AttributeName'],\n                           DynamoKey(d['AttributeName'], d['AttributeType']))\n                          for d in response['AttributeDefinitions']))\n            hash_key = attrs[response['KeySchema'][0]['AttributeName']]\n            if len(response['KeySchema']) > 1:\n                range_key = attrs[response['KeySchema'][1]['AttributeName']]\n\n        indexes = []\n        for idx in response.get('LocalSecondaryIndexes', []):\n            indexes.append(LocalIndex.from_response(idx, attrs))\n        global_indexes = []\n        for idx in response.get('GlobalSecondaryIndexes', []):\n            global_indexes.append(GlobalIndex.from_response(idx, attrs))\n\n        table = cls(\n            name=response['TableName'],\n            hash_key=hash_key,\n            range_key=range_key,\n            indexes=indexes,\n            global_indexes=global_indexes,\n            throughput=Throughput.from_response(\n                response['ProvisionedThroughput']),\n            status=response['TableStatus'],\n            size=response['TableSizeBytes'],\n        )\n        table.response = response\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_attrs(self):\n        if self.action != 'Create':\n            return []\n        index = self.extra['index']\n        ret = [index.hash_key]\n        if index.range_key is not None:\n            ret.append(index.range_key)\n        return ret", "response": "Get all the attrs needed for the update"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the serialized Dynamo format for the update", "response": "def serialize(self):\n        \"\"\" Get the serialized Dynamo format for the update \"\"\"\n        if self.action == 'Create':\n            payload = self.extra['index'].schema()\n        else:\n            payload = {\n                'IndexName': self.index_name,\n            }\n            if self.action == 'Update':\n                payload['ProvisionedThroughput'] = \\\n                    self.extra['throughput'].schema()\n        return {\n            self.action: payload\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_string(cls, string, pset):\n        tokens = re.split(\"[ \\t\\n\\r\\f\\v(),]\", string)\n        expr = []\n\n        def get_parts(token_string):\n            parts = tokens[i].split('_')\n            return parts[1], parts[2], parts[3]\n        i = 0\n        while i < len(tokens):\n            if tokens[i] == '':\n                i += 1\n                continue\n            if tokens[i] in pset.mapping:\n                primitive = pset.mapping[tokens[i]]\n                expr.append(primitive)\n            elif RangeOperationTerminal.NAME in tokens[i]:\n                operation, begin_range_name, end_range_name = get_parts(tokens[i])\n                range_operation_terminal = RangeOperationTerminal()\n                range_operation_terminal.initialize_parameters(pset.variable_type_indices, pset.variable_names,\n                                                               operation, begin_range_name, end_range_name)\n                expr.append(range_operation_terminal)\n            elif MomentFindingTerminal.NAME in tokens[i]:\n                operation, begin_range_name, end_range_name = get_parts(tokens[i])\n                moment_operation_terminal = MomentFindingTerminal()\n                moment_operation_terminal.initialize_parameters(pset.variable_type_indices, pset.variable_names,\n                                                                operation, begin_range_name, end_range_name)\n                expr.append(moment_operation_terminal)\n            else:\n                try:\n                    token = eval(tokens[i])\n                except NameError:\n                    raise TypeError(\"Unable to evaluate terminal: {}.\".format(tokens[i]))\n                expr.append(gp.Terminal(token, False, gp.__type__))\n            i += 1\n        return cls(expr)", "response": "Try to convert a string expression into a PrimitiveTree given a PrimitiveSet."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the parameters of the object based on the provided parameters.", "response": "def initialize_parameters(self, variable_type_indices, names, operation=None, begin_range_name=None,\n                              end_range_name=None, *args):\n        \"\"\"\n        :param variable_type_indices: A sequence of variable type indices where each entry defines the\n        index of a variable type in the design matrix. For example a design matrix with two variable types will have\n        indices [j,n] where variable type A spans 0 to j and variable type B spans j + 1 to n.\n        :param names:\n        :param args:\n        :param operation\n        :param begin_range_name\n        :param end_range_name\n        :return:\n        \"\"\"\n        self.names = names\n        for r in variable_type_indices:\n            if r[1] - r[0] < 2:\n                raise ValueError('Invalid range provided to Range Terminal: ' + str(r))\n        rng = random.choice(variable_type_indices)\n        self.lower_bound = rng[0]\n        self.upper_bound = rng[1]\n        if operation is not None and begin_range_name is not None and end_range_name is not None:\n            if self.operations.get(operation) is None:\n                raise ValueError('Invalid operation provided to Range Terminal: ' + operation)\n            if begin_range_name not in self.names:\n                raise ValueError('Invalid range name provided to Range Termnial: ' + str(begin_range_name))\n            if end_range_name not in names:\n                raise ValueError('Invalid range name provided to Range Termnial: ' + str(end_range_name))\n            begin_range = self.names.index(begin_range_name)\n            end_range = self.names.index(end_range_name)\n            valid = False\n            for r in variable_type_indices:\n                if r[0] <= begin_range < end_range <= r[1]:\n                    valid = True\n            if not valid:\n                raise ValueError('Invalid range provided to Range Terminal: (' + str(begin_range) + ',' +\n                                 str(end_range) + ')')\n            self.operation = self.operations[operation]\n            self.begin_range = begin_range\n            self.end_range = end_range\n        else:\n            self.operation = random.choice(list(self.operations.values()))\n            self.begin_range = np.random.randint(self.lower_bound, self.upper_bound - 1)\n            self.end_range = np.random.randint(self.begin_range + 1, self.upper_bound)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef instance(cls, interval=5):\n        '''\n        Returns existing instance of messenger. If one does not exist it will\n        be created and returned.\n\n        :param int interval:\n            Number of miliseconds that represents interval when messages will\n            be processed.\n            Note that this parameter will be used only the first time when\n            instance is requested, every other time it will be ignored\n            because existing instance of :class:`._Messenger` is returned.\n        '''\n        if not cls._instance:\n            cls._instance = _Messenger(interval)\n        return cls._instance", "response": "Returns existing instance of _Messenger."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends provided message to all listeners.", "response": "def send(self, message, *args, **kwargs):\n        '''\n        Sends provided message to all listeners. Message is only added to\n        queue and will be processed on next tick.\n\n        :param Message message:\n            Message to send.\n        '''\n        self._messages.put((message, args, kwargs), False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subscribe(self, message, handler):\n        '''\n        Adds hander for specified message.\n\n        :param str message:\n            Name of message to subscribe to.\n\n        :param callable handler:\n            Handler for this message type. Handler must receive single parameter\n            and that parameter will be instance of sent message.\n        '''\n        with self._lock:\n            ref = WeakCallable(handler, self._on_collect)\n            self._subscribers[message].append(ref)", "response": "Adds a handler to be called when a message is received."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unsubscribe(self, message, handler):\n        '''\n        Removes handler from message listeners.\n\n        :param str message:\n            Name of message to unsubscribe handler from.\n\n        :param callable handler:\n            Callable that should be removed as handler for `message`.\n        '''\n        with self._lock:\n            self._subscribers[message].remove(WeakCallable(handler))", "response": "Unsubscribe handler from a message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _execute(self, sender, event_args):\n        '''\n        Event handler for timer that processes all queued messages.\n        '''\n        with self._lock:\n            while not self._messages.empty():\n                msg, args, kwargs = self._messages.get(False)\n                for subscriber in self._subscribers[msg]:\n                    try:\n                        subscriber(*args, **kwargs)\n                    except weakref.ReferenceError:\n                        # Reference to handler is lost and it is OK to silence it\n                        pass", "response": "Execute all queued messages."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nemitting this signal. As result, all handlers will be invoked.", "response": "def emit(self, *args, **kwargs):\n        '''\n        Emits this signal. As result, all handlers will be invoked.\n        '''\n        self._messanger.send(self, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nraise event that property value has changed for provided property name.", "response": "def RaisePropertyChanged(self, property_name):\n        '''\n        Raises event that property value has changed for provided property name.\n\n        :param str property_name:\n            Name of property whose value has changed.\n        '''\n        args = PropertyChangedEventArgs(property_name)\n        for handler in self.property_chaged_handlers:\n            handler(self, args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expand_path(path, follow_links=False):\n    path = os.path.expandvars(os.path.expanduser(path))\n    if follow_links:\n        return os.path.realpath(path)\n    return os.path.abspath(path)", "response": "Expand shell variables user directory symbols and return absolute path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef walk_level(path, level=1):\n    if level is None:\n        level = float('inf')\n    path = expand_path(path)\n    if os.path.isdir(path):\n        root_level = path.count(os.path.sep)\n        for root, dirs, files in os.walk(path):\n            yield root, dirs, files\n            if root.count(os.path.sep) >= root_level + level:\n                del dirs[:]\n    elif os.path.isfile(path):\n        yield os.path.dirname(path), [], [os.path.basename(path)]\n    else:\n        raise RuntimeError(\"Can't find a valid folder or file for path {0}\".format(repr(path)))", "response": "Like os. walk but takes level kwarg that indicates how deep the recursion will go at the root level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the type of the file or dir for the provided path", "response": "def get_type(full_path):\n    \"\"\"Get the type (socket, file, dir, symlink, ...) for the provided path\"\"\"\n    status = {'type': []}\n    if os.path.ismount(full_path):\n        status['type'] += ['mount-point']\n    elif os.path.islink(full_path):\n        status['type'] += ['symlink']\n    if os.path.isfile(full_path):\n        status['type'] += ['file']\n    elif os.path.isdir(full_path):\n        status['type'] += ['dir']\n    if not status['type']:\n        if os.stat.S_ISSOCK(status['mode']):\n            status['type'] += ['socket']\n        elif os.stat.S_ISCHR(status['mode']):\n            status['type'] += ['special']\n        elif os.stat.S_ISBLK(status['mode']):\n            status['type'] += ['block-device']\n        elif os.stat.S_ISFIFO(status['mode']):\n            status['type'] += ['pipe']\n    if not status['type']:\n        status['type'] += ['unknown']\n    elif status['type'] and status['type'][-1] == 'symlink':\n        status['type'] += ['broken']\n    return status['type']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_stat(full_path):\n    status = {}\n    status['size'] = os.path.getsize(full_path)\n    status['accessed'] = datetime.datetime.fromtimestamp(os.path.getatime(full_path))\n    status['modified'] = datetime.datetime.fromtimestamp(os.path.getmtime(full_path))\n    status['changed_any'] = datetime.datetime.fromtimestamp(os.path.getctime(full_path))\n    # first 3 digits are User, Group, Other permissions: 1=execute,2=write,4=read\n    status['mode'] = os.stat(full_path).st_mode\n    status['type'] = get_type(full_path)\n    return status", "response": "Use python builtin equivalents to unix stat command and return dict containing stat data about a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef path_status(path, filename='', status=None, deep=False, verbosity=0):\n    status = {} if status is None else status\n\n    path = expand_path(path)\n    if filename:\n        dir_path = path\n    else:\n        dir_path, filename = os.path.split(path)  # this will split off a dir as `filename` if path doesn't end in a /\n    full_path = os.path.join(dir_path, filename)\n    if verbosity > 1:\n        print('stat: {}'.format(full_path))\n    status['name'] = filename\n    status['path'] = full_path\n    status['dir'] = dir_path\n    status['type'] = []\n    try:\n        status.update(get_stat(full_path))\n    except OSError:\n        status['type'] = ['nonexistent'] + status['type']\n        logger.info(\"Unable to stat path '{}'\".format(full_path))\n    status['type'] = '->'.join(status['type'])\n\n    return status", "response": "Retrieve the access modify and create timetags for a path along with its size"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates files in the specified directory tree.", "response": "def generate_files(path='', ext='', level=None, dirs=False, files=True, verbosity=0):\n    \"\"\" Recursively generate files (and thier stats) in the indicated directory\n\n    Filter by the indicated file name extension (ext)\n\n    Args:\n        path (str):  Root/base path to search.\n        ext (str or list of str):  File name extension(s).\n            Only file paths that \".endswith()\" this string will be returned\n        level (int, optional): Depth of file tree to halt recursion at.\n            None = full recursion to as deep as it goes\n            0 = nonrecursive, just provide a list of files at the root level of the tree\n            1 = one level of depth deeper in the tree\n        typ (type):  output type (default: list). if a mapping type is provided the keys will be the full paths (unique)\n        dirs (bool):  Whether to yield dir paths along with file paths (default: False)\n        files (bool): Whether to yield file paths (default: True)\n            `dirs=True`, `files=False` is equivalent to `ls -d`\n\n    Returns:\n        list of dicts: dict keys are { 'path', 'name', 'bytes', 'created', 'modified', 'accessed', 'permissions' }\n        path (str): Full, absolute paths to file beneath the indicated directory and ending with `ext`\n        name (str): File name only (everythin after the last slash in the path)\n        size (int): File size in bytes\n        changed_any (datetime): Timestamp for modification of either metadata (e.g. permissions) or content\n        modified (datetime): File content modification timestamp from file system\n        accessed (datetime): File access timestamp from file system\n        permissions (int): File permissions bytes as a chown-style integer with a maximum of 4 digits\n        type (str): One of 'file', 'dir', 'symlink->file', 'symlink->dir', 'symlink->broken'\n                e.g.: 777 or 1755\n\n    Examples:\n        >>> 'util.py' in [d['name'] for d in generate_files(os.path.dirname(__file__), ext='.py', level=0)]\n        True\n        >>> next(d for d in generate_files(os.path.dirname(__file__), ext='.py')\n        ...      if d['name'] == 'util.py')['size'] > 1000\n        True\n        >>> sorted(next(generate_files()).keys())\n        ['accessed', 'changed_any', 'dir', 'mode', 'modified', 'name', 'path', 'size', 'type']\n\n        There should be an __init__ file in the same directory as this script.\n        And it should be at the top of the list.\n        >>> sorted(d['name'] for d in generate_files(os.path.dirname(__file__), ext='.py', level=0))[0]\n        '__init__.py'\n        >>> len(list(generate_files(__file__, ext='.')))\n        0\n        >>> len(list(generate_files(__file__, ext=['invalidexttesting123', False])))\n        0\n        >>> len(list(generate_files(__file__, ext=['.py', '.pyc', 'invalidexttesting123', False]))) > 0\n        True\n        >>> sorted(generate_files(__file__))[0]['name'] == os.path.basename(__file__)\n        True\n        >>> sorted(list(generate_files())[0].keys())\n        ['accessed', 'changed_any', 'dir', 'mode', 'modified', 'name', 'path', 'size', 'type']\n        >>> all(d['type'] in ('file', 'dir',\n        ...                   'symlink->file', 'symlink->dir', 'symlink->broken',\n        ...                   'mount-point->file', 'mount-point->dir',\n        ...                   'block-device', 'pipe', 'special', 'socket', 'unknown')\n        ...     for d in generate_files(level=1, files=True, dirs=True))\n        True\n    \"\"\"\n    path = expand_path(path or '.')\n    # None interpreted as '', False is interpreted as '.' (no ext will be accepted)\n    ext = '.' if ext is False else ext\n    # multiple extensions can be specified in a list or tuple\n    ext = ext if ext and isinstance(ext, (list, tuple)) else [ext]\n    # case-insensitive extensions, '.' ext means only no-extensions are accepted\n    ext = set(x.lower() if x else '.' if x is False else '' for x in ext)\n\n    if os.path.isfile(path):\n        fn = os.path.basename(path)\n        # only yield the stat dict if the extension is among those that match or files without any ext are desired\n        if not ext or any(path.lower().endswith(x) or (x == '.' and '.' not in fn) for x in ext):\n            yield path_status(os.path.dirname(path), os.path.basename(path), verbosity=verbosity)\n    else:\n        for dir_path, dir_names, filenames in walk_level(path, level=level):\n            if verbosity > 0:\n                print('Checking path \"{}\"'.format(dir_path))\n            if files:\n                for fn in filenames:  # itertools.chain(filenames, dir_names)\n                    if ext and not any((fn.lower().endswith(x) or (x == '.' and x not in fn) for x in ext)):\n                        continue\n                    stat = path_status(dir_path, fn, verbosity=verbosity)\n                    if stat and stat['name'] and stat['path']:\n                        yield stat\n            if dirs:\n                for fn in dir_names:\n                    if ext and not any((fn.lower().endswith(x) or (x == '.' and x not in fn) for x in ext)):\n                        continue\n                    yield path_status(dir_path, fn, verbosity=verbosity)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the modify and change timestamps of a file", "response": "def touch(filepath, times=None, mkdir=False):\n    \"\"\" Update the modify (modify) and change (ctime) timestamps of a file, create if necessary\n\n    >>> from pugnlp.constants import DATA_PATH\n    >>> filepath = os.path.join(DATA_PATH, 'tmpfilefortouch.txt')\n    >>> touch(filepath).endswith('tmpfilefortouch.txt')\n    True\n    >>> os.path.isfile(filepath)\n    True\n    >>> os.remove(filepath)\n    \"\"\"\n    filepath = expand_path(filepath)\n    if mkdir:\n        mkdir_p(os.path.dirname(filepath))\n    with open(filepath, 'a'):\n        if times or times is None:\n            os.utime(filepath, times)\n    return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef touch_p(filepath, times=None, mkdir=True):\n    return touch(filepath=filepath, times=times, mkdir=mkdir)", "response": "mkdir_p(filepath) then touch(filepath)\n\n    >>> from pugnlp.constants import DATA_PATH\n    >>> filepath = os.path.join(DATA_PATH, 'tmpdirfortouch', 'tmpfilefortouch.txt')\n    >>> touch_p(filepath).endswith(os.path.join('tmpdirfortouch', 'tmpfilefortouch.txt'))\n    True\n    >>> os.path.isfile(filepath)\n    True\n    >>> os.remove(filepath)\n    >>> os.rmdir(os.path.dirname(filepath))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields lines from a file in a node s network - connection tree", "response": "def sudo_yield_file_lines(file_path='/etc/NetworkManager/system-connections/*'):\n    r\"\"\"Cat a file iterating/yielding one line at a time,\n\n    shell will execute: `sudo cat $file_path` so if your shell doesn't have sudo or cat, no joy\n    Input:\n                    file_path(str): glob stars are fine\n\n    >> for line in sudo_yield_file_lines('/etc/NetworkManager/system-connections/*')\n\n\n    \"\"\"\n    # substitute your Windoze/DOS/PowerlessShell command here:\n    sudo_cat_cmd = 'sudo cat {}'.format(file_path)\n\n    process = subprocess.Popen(sudo_cat_cmd, stdout=subprocess.PIPE, shell=True)\n    # read one line at a time, as it becomes available\n    for line in iter(process.stdout.readline, ''):\n        yield line"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_json(file_or_path):\n    try:\n        with (open(file_or_path, 'r') if isinstance(file_or_path, (str, bytes)) else file_or_path) as f:\n            obj = json.load(f)\n    except IOError:\n        obj = json.loads(file_or_path)\n    return obj", "response": "Parse json contents of string or file object or file path and return python nested dict / lists"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inFootprint(footprint,ra,dec):\n    if footprint is None:\n        return np.ones(len(ra),dtype=bool)\n    \n    try:\n        if isinstance(footprint,str) and os.path.exists(footprint):\n            filename = footprint\n            #footprint = hp.read_map(filename,verbose=False)\n            #footprint = fitsio.read(filename)['I'].ravel()\n            footprint = read_map(filename)\n        nside = hp.npix2nside(len(footprint))\n        pix = ang2pix(nside,ra,dec)\n        inside = (footprint[pix] > 0)\n    except IOError:\n        logger.warning(\"Failed to load healpix footprint; trying to use mangle...\")\n        inside = inMangle(filename,ra,dec)\n    return inside", "response": "Check if set of ra dec combinations are in footprint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit a pre - existing maglim map into chunks consistent with the catalog pixels.", "response": "def split(config,dirname='split',force=False):\n    \"\"\" Take a pre-existing maglim map and divide it into\n    chunks consistent with the catalog pixels. \"\"\"\n\n    config = Config(config)\n    filenames = config.getFilenames()\n    #healpix = filenames['pix'].compressed()\n\n    # Check that things are ok\n    basedir,basename = os.path.split(config['mask']['dirname'])\n    #if basename == dirname:\n    #    msg = \"Input and output directory are the same.\"\n    #    raise Exception(msg)\n    outdir = mkdir(os.path.join(basedir,dirname))\n    \n    nside_catalog = config['coords']['nside_catalog']\n    nside_pixel = config['coords']['nside_pixel']\n\n    release = config['data']['release'].lower()\n    band1 = config['catalog']['mag_1_band']\n    band2 = config['catalog']['mag_2_band']\n\n    # Read the magnitude limits\n    maglimdir = config['maglim']['dirname']\n\n    maglimfile_1 = join(maglimdir,config['maglim']['filename_1'])\n    logger.info(\"Reading %s...\"%maglimfile_1)\n    maglim1 = read_map(maglimfile_1)\n    \n    maglimfile_2 = join(maglimdir,config['maglim']['filename_2'])\n    logger.info(\"Reading %s...\"%maglimfile_2)\n    maglim2 = read_map(maglimfile_2)\n\n    # Read the footprint\n    footfile = config['data']['footprint']\n    logger.info(\"Reading %s...\"%footfile)\n    footprint = read_map(footfile)\n\n    # Output mask names\n    mask1 = os.path.basename(config['mask']['basename_1'])\n    mask2 = os.path.basename(config['mask']['basename_2'])\n\n    for band,maglim,base in [(band1,maglim1,mask1),(band2,maglim2,mask2)]:\n        nside_maglim = hp.npix2nside(len(maglim))\n        if nside_maglim != nside_pixel:\n            msg = \"Mask nside different from pixel nside\"\n            logger.warning(msg)\n            #raise Exception(msg)\n\n        pixels = np.nonzero(maglim>0)[0]\n        superpix = superpixel(pixels,nside_maglim,nside_catalog)\n        healpix = np.unique(superpix)\n        for hpx in healpix:\n            outfile = join(outdir,base)%hpx\n            if os.path.exists(outfile) and not force:\n                logger.warning(\"Found %s; skipping...\"%outfile)\n                continue\n\n            pix = pixels[superpix == hpx]\n            print(hpx, len(pix))\n\n            logger.info('Writing %s...'%outfile)\n            data = odict()\n            data['PIXEL']=pix\n            data['MAGLIM']=maglim[pix].astype('f4')\n            data['FRACDET']=footprint[pix].astype('f4')\n            ugali.utils.healpix.write_partial_map(outfile,data,nside_pixel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloops through pixels containing catalog objects and calculate the magnitude limit. This gets a bit convoluted due to all the different pixel resolutions...", "response": "def run(self,field=None,simple=False,force=False):\n        \"\"\"\n        Loop through pixels containing catalog objects and calculate\n        the magnitude limit. This gets a bit convoluted due to all\n        the different pixel resolutions...\n        \"\"\"\n        if field is None: fields = [1,2]\n        else:             fields = [field]\n        for filenames in self.filenames.compress(~self.filenames.mask['catalog']).data:\n            infile = filenames['catalog']\n            for f in fields:\n                outfile = filenames['mask_%i'%f]\n                if os.path.exists(outfile) and not force:\n                    logger.info(\"Found %s; skipping...\"%outfile)\n                    continue\n                \n                pixels,maglims=self.calculate(infile,f,simple)\n                logger.info(\"Creating %s\"%outfile)\n                outdir = mkdir(os.path.dirname(outfile))\n                data = odict()\n                data['PIXEL']=pixels\n                data['MAGLIM']=maglims.astype('f4')\n                ugali.utils.healpix.write_partial_map(outfile,data,\n                                                      self.nside_pixel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters the given variable set based on the request parameters.", "response": "def get_variable_set(self, variable_set, data):\n        \"\"\"Filters the given variable set based on request parameters\"\"\"\n\n        if data.get('dynamic_layers'):\n            variable_set = []  # TODO\n        elif data.get('layers'):\n            op, layer_ids = data['layers'].split(':', 1)\n            op = op.lower()\n            layer_ids = [int(x) for x in layer_ids.split(',')]\n\n            if op in ('show', 'include'):\n                variable_set = [x for x in variable_set if x.index in layer_ids]\n            elif op in ('hide', 'exclude'):\n                variable_set = [x for x in variable_set if x.index not in layer_ids]\n        elif self.service.render_top_layer_only:\n            variable_set = [variable_set[0]]\n\n        return variable_set"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying the correct time index to configurations", "response": "def apply_time_to_configurations(self, configurations, data):\n        \"\"\"Applies the correct time index to configurations\"\"\"\n\n        time_value = None\n        if data.get('time'):\n            time_value = data['time']\n\n            # Only single time values are supported. For extents, just grab the first value\n            if isinstance(data['time'], (tuple, list)):\n                time_value = time_value[0]\n\n        if time_value:\n            for config in configurations:\n                config.set_time_index_from_datetime(time_value, best_fit=ALLOW_BEST_FIT_TIME_INDEX)\n\n        return configurations"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_form_defaults(self):\n\n        return {\n            'response_format': 'html',\n            'bbox': self.service.full_extent,\n            'size': '400,400',\n            'dpi': 200,\n            'image_projection': pyproj.Proj(str(self.service.projection)),\n            'bbox_projection': pyproj.Proj(str(self.service.projection)),\n            'image_format': 'png',\n            'transparent': True\n        }", "response": "Returns default values for the get image form"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an image in the request format", "response": "def format_image(self, image, image_format, **kwargs):\n        \"\"\"Returns an image in the request format\"\"\"\n\n        image_format = image_format.lower()\n        accept = self.request.META['HTTP_ACCEPT'].split(',')\n\n        if FORCE_WEBP and 'image/webp' in accept:\n            image_format = 'webp'\n        elif image_format == 'png8':\n            alpha = image.split()[-1]\n            image = image.convert('RGB')\n            image = image.convert('P', palette=Image.ADAPTIVE, colors=255)\n            image.paste(255, Image.eval(alpha, lambda x: 255 if x <= 128 else 0))\n            image_format = 'png'\n            kwargs['transparency'] = 255\n        elif image_format in ('png32', 'png24'):\n            image_format = 'png'\n\n        return super(GetImageView, self).format_image(image, image_format, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn default values for the identify form", "response": "def _get_form_defaults(self):\n        \"\"\"Returns default values for the identify form\"\"\"\n\n        return {\n            'response_format': 'html',\n            'geometry_type': 'esriGeometryPoint',\n            'projection': pyproj.Proj(str(self.service.projection)),\n            'return_geometry': True,\n            'maximum_allowable_offset': 2,\n            'geometry_precision': 3,\n            'return_z': False,\n            'return_m': False\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_terms(self, name, revision=None):\n        url = '{}terms/{}'.format(self.url, name)\n        if revision:\n            url = '{}?revision={}'.format(url, revision)\n        json = make_request(url, timeout=self.timeout, client=self._client)\n        try:\n            # This is always a list of one element.\n            data = json[0]\n            return Term(name=data['name'],\n                        title=data.get('title'),\n                        revision=data['revision'],\n                        created_on=datetime.datetime.strptime(\n                            data['created-on'],\n                            \"%Y-%m-%dT%H:%M:%SZ\"\n                            ),\n                        content=data['content'])\n        except (KeyError, TypeError, ValueError, IndexError) as err:\n            log.info(\n                'cannot process terms: invalid JSON response: {!r}'.format(\n                    json))\n            raise ServerError(\n                'unable to get terms for {}: {}'.format(name, err))", "response": "Retrieve a specific term and condition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_dataset(self, service):\n\n        if not self.dataset:\n            path = os.path.join(SERVICE_DATA_ROOT, service.data_path)\n            self.dataset = netCDF4.Dataset(path, 'r')\n        return self.dataset", "response": "Opens and returns the NetCDF dataset associated with a service or returns a previously - opened dataset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_grid_spatial_dimensions(self, variable):\n\n        data = self.open_dataset(self.service).variables[variable.variable]\n        dimensions = list(data.dimensions)\n        return data.shape[dimensions.index(variable.x_dimension)], data.shape[dimensions.index(variable.y_dimension)]", "response": "Returns the width and height for the given variable"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns this bbox normalized to match the ratio of the given size.", "response": "def _normalize_bbox(self, bbox, size):\n        \"\"\"Returns this bbox normalized to match the ratio of the given size.\"\"\"\n\n        bbox_ratio = float(bbox.width) / float(bbox.height)\n        size_ratio = float(size[0]) / float(size[1])\n\n        if round(size_ratio, 4) == round(bbox_ratio, 4):\n            return bbox\n        else:\n            if bbox.height * size_ratio >= bbox.width:\n                diff = bbox.height*size_ratio - bbox.width\n                return BBox((bbox.xmin - diff/2, bbox.ymin, bbox.xmax + diff/2, bbox.ymax), bbox.projection)\n            else:\n                diff = abs(bbox.width/size_ratio - bbox.height)\n                return BBox((bbox.xmin, bbox.ymin - diff/2, bbox.xmax, bbox.ymax + diff/2), bbox.projection)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an image in the request format", "response": "def format_image(self, image, image_format, **kwargs):\n        \"\"\"Returns an image in the request format\"\"\"\n\n        if image_format in ('png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'):\n            if image_format != 'webp' and FORCE_WEBP:\n                # Always return WebP when supported by the browser\n                accept = self.request.META['HTTP_ACCEPT'].split(',')\n                if 'image/webp' in accept:\n                    image = image.convert('RGBA')\n                    image_format = 'webp'\n                    kwargs = {'lossless': True}\n\n            if image_format == 'png':\n                kwargs['optimize'] = True\n            elif image_format == 'jpg':\n                image.convert('RGB')\n                kwargs['progressive'] = True\n\n            buffer = six.BytesIO()\n            image.save(buffer, image_format, **kwargs)\n            return buffer.getvalue(), \"image/{}\".format(image_format)\n        else:\n            raise ValueError('Unsupported format: {}'.format(image_format))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_response(self, request, image, content_type):\n\n        return HttpResponse(content=image, content_type=content_type)", "response": "Returns a response object for the given image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a response object for the request. Can be overridden to return different responses.", "response": "def create_response(self, request, content, content_type):\n        \"\"\"Returns a response object for the request. Can be overridden to return different responses.\"\"\"\n\n        return HttpResponse(content=content, content_type=content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntruncating the filename if necessary saves the model and returns a response", "response": "def process_temporary_file(self, tmp_file):\n        \"\"\"Truncates the filename if necessary, saves the model, and returns a response\"\"\"\n\n        #Truncate filename if necessary\n        if len(tmp_file.filename) > 100:\n            base_filename = tmp_file.filename[:tmp_file.filename.rfind(\".\")]\n            tmp_file.filename = \"%s.%s\" % (base_filename[:99-len(tmp_file.extension)], tmp_file.extension)\n\n        tmp_file.save()\n\n        data = {\n            'uuid': str(tmp_file.uuid)\n        }\n\n        response = HttpResponse(json.dumps(data), status=201)\n        response['Content-type'] = \"text/plain\"\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates 2D labels at zmax", "response": "def createLabels2D(self):\n        \"\"\" 2D labeling at zmax \"\"\"\n        logger.debug(\"  Creating 2D labels...\")\n        self.zmax = np.argmax(self.values,axis=1)\n        self.vmax = self.values[np.arange(len(self.pixels),dtype=int),self.zmax]\n\n        kwargs=dict(pixels=self.pixels,values=self.vmax,nside=self.nside,\n                    threshold=self.threshold,xsize=self.xsize)\n        labels,nlabels = CandidateSearch.labelHealpix(**kwargs)\n        self.nlabels = nlabels\n        self.labels = np.repeat(labels,len(self.distances)).reshape(len(labels),len(self.distances))\n        return self.labels, self.nlabels"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlabel contiguous regions of a HEALPix array.", "response": "def labelHealpix(pixels, values, nside, threshold=0, xsize=1000):\n        \"\"\"\n        Label contiguous regions of a (sparse) HEALPix map. Works by mapping \n        HEALPix array to a Mollweide projection and applying scipy.ndimage.label\n     \n        Assumes non-nested HEALPix map.\n        \n        Parameters:\n        pixels    : Pixel values associated to (sparse) HEALPix array\n        values    : (Sparse) HEALPix array of data values\n        nside     : HEALPix dimensionality\n        threshold : Threshold value for object detection\n        xsize     : Size of Mollweide projection\n        \n        Returns:\n        labels, nlabels\n        \"\"\"\n        proj = healpy.projector.MollweideProj(xsize=xsize)\n        vec = healpy.pix2vec(nside,pixels)\n        xy = proj.vec2xy(vec)\n        ij = proj.xy2ij(xy)\n        xx,yy = proj.ij2xy()\n     \n        # Convert to Mollweide\n        searchims = []\n        if values.ndim < 2: iterate = [values]\n        else:               iterate = values.T\n        for i,value in enumerate(iterate):\n            logger.debug(\"Labeling slice %i...\")\n            searchim = numpy.zeros(xx.shape,dtype=bool)\n            select = (value > threshold)\n            yidx = ij[0][select]; xidx = ij[1][select]\n            searchim[yidx,xidx] |= True\n            searchims.append( searchim )\n        searchims = numpy.array(searchims)\n\n        # Full binary structure\n        s = ndimage.generate_binary_structure(searchims.ndim,searchims.ndim)\n     \n        ### # Dilate in the z-direction\n        logger.info(\"  Dilating image...\")\n        searchims = ndimage.binary_dilation(searchims,s,1)\n        \n        # Do the labeling\n        logger.info(\"  Labeling image...\")\n        labels,nlabels = ndimage.label(searchims,structure=s)\n\n        # Convert back to healpix\n        pix_labels = labels[:,ij[0],ij[1]].T\n        pix_labels = pix_labels.reshape(values.shape)\n        pix_labels *= (values > threshold) # re-trim\n\n        return pix_labels, nlabels"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the objects in a multi - dimensional HEALPix map.", "response": "def findObjects(pixels, values, nside, zvalues, rev, good):\n        \"\"\"\n        Characterize labelled candidates in a multi-dimensional HEALPix map.\n     \n        Parameters:\n        values    : (Sparse) HEALPix array of data values\n        nside     : HEALPix dimensionality\n        pixels    : Pixel values associated to (sparse) HEALPix array\n        zvalues   : Values of the z-dimension (usually distance modulus)\n        rev       : Reverse indices for pixels in each \"island\"\n        good      : Array containg labels for each \"island\"\n     \n        Returns:\n        objs      : numpy.recarray of object characteristics\n        \"\"\"\n     \n        ngood = len(good)\n        objs = numpy.recarray((ngood,),\n                           dtype=[('LABEL','i4'),\n                                  ('NPIX','i4'),\n                                  ('VAL_MAX','f4'),\n                                  ('IDX_MAX','i4'),\n                                  ('ZIDX_MAX','i4'),\n                                  ('PIX_MAX','i4'),\n                                  ('X_MAX','f4'),\n                                  ('Y_MAX','f4'),\n                                  ('Z_MAX','f4'),\n                                  ('X_CENT','f4'),\n                                  ('Y_CENT','f4'),\n                                  ('Z_CENT','f4'),\n                                  ('X_BARY','f4'),\n                                  ('Y_BARY','f4'),\n                                  ('Z_BARY','f4'),\n                                  ('CUT','i2'),])\n        objs['CUT'][:] = 0\n     \n        shape = values.shape\n        ncol = shape[1]\n        for i in range(0,ngood):\n            logger.debug(\"i=%i\",i)\n            # This code could use some cleanup...\n            indices=rev[rev[good[i]]:rev[good[i]+1]]\n            npix = len(indices)\n            idx = indices // ncol # This is the spatial index\n            zidx = indices % ncol  # This is the distance index\n     \n            pix = pixels[idx] # This is the healpix pixel\n            xval,yval = pix2ang(nside, pix)\n            zval = zvalues[zidx]\n            \n            objs[i]['LABEL'] = good[i]\n            objs[i]['NPIX'] = npix\n            logger.debug(\"LABEL=%i\"%objs[i]['LABEL'])\n            logger.debug(\"NPIX=%i\"%objs[i]['NPIX'])\n     \n            island = values[idx,zidx]\n            idxmax = island.argmax()\n            xval_max,yval_max,zval_max = xval[idxmax],yval[idxmax],zval[idxmax]\n     \n            objs[i]['VAL_MAX'] = island[idxmax]\n            objs[i]['IDX_MAX']  = idx[idxmax]\n            objs[i]['ZIDX_MAX']  = zidx[idxmax]\n            objs[i]['PIX_MAX']   = pix[idxmax]\n            objs[i]['X_MAX']  = xval_max\n            objs[i]['Y_MAX']  = yval_max\n            objs[i]['Z_MAX']  = zval_max\n\n            proj = Projector(xval_max,yval_max)\n            xpix,ypix = proj.sphereToImage(xval,yval)\n\n            # Projected centroid\n            x_cent,y_cent,zval_cent = numpy.average([xpix,ypix,zval],axis=1)\n            xval_cent, yval_cent = proj.imageToSphere(x_cent,y_cent)\n            objs[i]['X_CENT'] = xval_cent\n            objs[i]['Y_CENT'] = yval_cent\n            objs[i]['Z_CENT'] = zval_cent\n\n            # Projected barycenter\n            weights=[island,island,island]\n            x_bary,y_bary,zval_bary = numpy.average([xpix,ypix,zval],weights=weights,axis=1)\n            xval_bary,yval_bary = proj.imageToSphere(x_bary, y_bary)\n            objs[i]['X_BARY'] = xval_bary\n            objs[i]['Y_BARY'] = yval_bary\n            objs[i]['Z_BARY'] = zval_bary\n     \n        return objs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a population of n random satellites within a survey mask.", "response": "def satellitePopulation(mask, nside_pix, n,\n                        range_distance=[5., 500.],\n                        range_stellar_mass=[1.e1, 1.e6],\n                        range_r_physical=[1.e-3, 2.],\n                        plot=False):\n    \"\"\"\n    Create a population of n randomly placed satellites within a survey mask.\n    Satellites are distributed uniformly in log(distance) (kpc), uniformly in log(stellar_mass) (M_sol), and uniformly in\n    physical half-light radius log(r_physical) (kpc). The ranges can be set by the user.\n\n    Returns the simulated area (deg^2) as well as the\n    lon (deg), lat (deg), distance modulus, stellar mass (M_sol), and half-light radius (deg) for each satellite\n\n    Parameters:\n    -----------\n    mask : the survey mask of available area\n    nside_pix : coarse resolution npix for avoiding small gaps in survey\n    n : number of satellites to simulate\n\n    Returns:\n    --------\n    area, lon, lat, distance, stellar_mass, r_physical    \n    \"\"\"\n    \n    distance = 10**np.random.uniform(np.log10(range_distance[0]),\n                                     np.log10(range_distance[1]),\n                                     n)\n\n    stellar_mass = 10**np.random.uniform(np.log10(range_stellar_mass[0]), \n                                         np.log10(range_stellar_mass[1]), \n                                         n)\n    \n    # Physical half-light radius (kpc)\n    r_physical = 10**np.random.uniform(np.log10(range_r_physical[0]), \n                                       np.log10(range_r_physical[1]), \n                                       n)\n\n    # Call positions last because while loop has a variable number of calls to np.random (thus not preserving seed information)\n    lon, lat, simulation_area = ugali.utils.skymap.randomPositions(mask, nside_pix, n=n)\n\n    #half_light_radius = np.degrees(np.arcsin(half_light_radius_physical \\\n    #                                         / ugali.utils.projector.distanceModulusToDistance(distance_modulus)))\n\n    # One choice of theory prior\n    #half_light_radius_physical = ugali.analysis.kernel.halfLightRadius(stellar_mass) # kpc\n    #half_light_radius = np.degrees(np.arcsin(half_light_radius_physical \\\n    #                                               / ugali.utils.projector.distanceModulusToDistance(distance_modulus)))\n\n    if plot:\n        pylab.figure()\n        #pylab.scatter(lon, lat, c=distance_modulus, s=500 * half_light_radius)\n        #pylab.colorbar()\n        pylab.scatter(lon, lat, edgecolors='none')\n        xmin, xmax = pylab.xlim() # Reverse azimuthal axis\n        pylab.xlim([xmax, xmin])\n        pylab.title('Random Positions in Survey Footprint')\n        pylab.xlabel('Longitude (deg)')\n        pylab.ylabel('Latitude (deg)')\n\n        pylab.figure()\n        pylab.scatter(stellar_mass, ugali.utils.projector.distanceModulusToDistance(distance_modulus),\n                      c=(60. * half_light_radius), s=500 * half_light_radius, edgecolors='none')\n        pylab.xscale('log')\n        pylab.yscale('log')\n        pylab.xlim([0.5 * range_stellar_mass[0], 2. * range_stellar_mass[1]])\n        pylab.colorbar()\n        pylab.title('Half-light Radius (arcmin)')\n        pylab.xlabel('Stellar Mass (arcmin)')\n        pylab.ylabel('Distance (kpc)')\n\n    return simulation_area, lon, lat, distance, stellar_mass, r_physical"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new satellite population with n random positions within a survey mask or catalog.", "response": "def satellitePopulationOrig(config, n,\n                            range_distance_modulus=[16.5, 24.],\n                            range_stellar_mass=[1.e2, 1.e5],\n                            range_r_physical=[5.e-3, 1.],\n                            mode='mask',\n                            plot=False):\n    \"\"\"\n    Create a population of n randomly placed satellites within a survey mask or catalog specified in the config file.\n    Satellites are distributed uniformly in distance modulus, uniformly in log(stellar_mass) (M_sol), and uniformly in\n    log(r_physical) (kpc). The ranges can be set by the user.\n\n    Returns the simulated area (deg^2) as well as the\n    lon (deg), lat (deg), distance modulus, stellar mass (M_sol), and half-light radius (deg) for each satellite\n    \"\"\"\n    \n    if type(config) == str:\n        config = ugali.utils.config.Config(config)\n\n    if mode == 'mask':\n        mask_1 = ugali.utils.skymap.readSparseHealpixMap(config.params['mask']['infile_1'], 'MAGLIM')\n        mask_2 = ugali.utils.skymap.readSparseHealpixMap(config.params['mask']['infile_2'], 'MAGLIM')\n        input = (mask_1 > 0.) * (mask_2 > 0.)\n    elif mode == 'catalog':\n        catalog = ugali.observation.catalog.Catalog(config)\n        input = np.array([catalog.lon, catalog.lat])\n    \n    lon, lat, simulation_area = ugali.utils.skymap.randomPositions(input,\n                                                                   config.params['coords']['nside_likelihood_segmentation'],\n                                                                   n=n)\n    distance_modulus = np.random.uniform(range_distance_modulus[0], \n                                         range_distance_modulus[1], \n                                         n)\n    stellar_mass = 10**np.random.uniform(np.log10(range_stellar_mass[0]), \n                                         np.log10(range_stellar_mass[1]), \n                                         n)\n    \n    half_light_radius_physical = 10**np.random.uniform(np.log10(range_half_light_radius_physical[0]), \n                                                       np.log10(range_half_light_radius_physical[0]), \n                                                       n) # kpc\n\n    half_light_radius = np.degrees(np.arcsin(half_light_radius_physical \\\n                                             / ugali.utils.projector.distanceModulusToDistance(distance_modulus)))\n    \n    # One choice of theory prior\n    #half_light_radius_physical = ugali.analysis.kernel.halfLightRadius(stellar_mass) # kpc\n    #half_light_radius = np.degrees(np.arcsin(half_light_radius_physical \\\n    #                                               / ugali.utils.projector.distanceModulusToDistance(distance_modulus)))\n\n    if plot:\n        pylab.figure()\n        #pylab.scatter(lon, lat, c=distance_modulus, s=500 * half_light_radius)\n        #pylab.colorbar()\n        pylab.scatter(lon, lat, edgecolors='none')\n        xmin, xmax = pylab.xlim() # Reverse azimuthal axis\n        pylab.xlim([xmax, xmin])\n        pylab.title('Random Positions in Survey Footprint')\n        pylab.xlabel('Longitude (deg)')\n        pylab.ylabel('Latitude (deg)')\n\n        pylab.figure()\n        pylab.scatter(stellar_mass, ugali.utils.projector.distanceModulusToDistance(distance_modulus),\n                      c=(60. * half_light_radius), s=500 * half_light_radius, edgecolors='none')\n        pylab.xscale('log')\n        pylab.yscale('log')\n        pylab.xlim([0.5 * range_stellar_mass[0], 2. * range_stellar_mass[1]])\n        pylab.colorbar()\n        pylab.title('Half-light Radius (arcmin)')\n        pylab.xlabel('Stellar Mass (arcmin)')\n        pylab.ylabel('Distance (kpc)')\n\n    return simulation_area, lon, lat, distance_modulus, stellar_mass, half_light_radius"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the query params from a query dict.", "response": "def _get_query_params(query):\n    \"\"\"\n    >>> _get_query_params({'query': {'a': 1, 'c': 3, 'b': 5}})\n    'a=1 b=5 c=3'\n    \"\"\"\n    query_params = OrderedDict(sorted(query['query'].items()))\n    return ' '.join(['{}={}'.format(k, v) for k, v in query_params.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_extra_json_fields(args):\n    extra_json_fields = {\n        'data_group': _get_data_group(args.query),\n        'data_type': _get_data_type(args.query),\n        'data_group_data_type': _get_data_group_data_type(args.query),\n        'query': _get_query_params(args.query),\n    }\n    if \"path_to_json_file\" in args.query:\n        extra_json_fields['path_to_query'] = _get_path_to_json_file(args.query)\n    return extra_json_fields", "response": "Generate a dictionary of additional json fields to be inserted into JSON logs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_csv(f):\n    reader = unicodecsv.DictReader(f, encoding=_ENCODING)\n    for row in reader:\n        month, year = parse_month_year(row['Return Month'])\n\n        yield OrderedDict([\n            ('customer_name', row['CustomerName']),\n            ('supplier_name', row['SupplierName']),\n            ('month', month),\n            ('year', year),\n            ('date', datetime.date(year, month, 1)),\n            ('total_ex_vat', parse_price(row['EvidencedSpend'])),\n            ('lot', parse_lot_name(row['LotDescription'])),\n            ('customer_sector', parse_customer_sector(row['Sector'])),\n            ('supplier_type', parse_sme_or_large(row['SME or Large'])),\n        ])", "response": "Takes a file - like object and yields OrderedDicts to be inserted into raw\n    spending database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_month_year(date_string):\n    match = re.match('\\d{2}/(?P<month>\\d{2})/(?P<year>\\d{4})$',\n                     date_string.lower())\n    if not match:\n        raise ValueError(\"Not format 'dd/mm/yyyy': '{}'\".format(date_string))\n    month = int(match.group('month'))\n    year = int(match.group('year'))\n    return month, year", "response": "Parse a date string into a month and year tuple."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_price(price_string):\n    return float(re.sub('[\u00a3,]', '', price_string))\n\n    match = re.match('(?P<amount>-?\u00a3[\\d,]+(\\.\\d{2})?)', price_string)\n    if not match:\n        raise ValueError(\"Charge not in format '(-)\u00a316,000(.00)' : {}\".format(\n            repr(price_string)))\n    return float(re.sub('[\u00a3,]', '', match.group('amount')))", "response": "Parse a price string into a sequence of numbers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a file exists and if not skip it", "response": "def file_found(filename,force):\n    \"\"\"Check if a file exists\"\"\"\n    if os.path.exists(filename) and not force:\n        logger.info(\"Found %s; skipping...\"%filename)\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_next_population(individuals, toolbox):\n    individuals = [toolbox.clone(ind) for ind in individuals]\n    individuals.sort(key=lambda x: x.error)\n\n    offspring = []\n    pop_size = len(individuals)\n    num_top = math.floor(pop_size / 2)\n    parents = individuals[0:num_top + 1]\n    for _ in range(pop_size - 1):\n        off = toolbox.clone(random.choice(parents))\n        off = toolbox.mutate(off)[0]\n        offspring.append(off)\n    offspring.append(individuals[0])\n    return offspring", "response": "Perform truncated selection with elitism."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noptimizes a population of individuals.", "response": "def optimize(population, toolbox, ngen, archive=None, stats=None, verbose=False, history=None):\n    \"\"\"\n    Optimize a population of individuals.\n    :param population:\n    :param toolbox:\n    :param mut_prob:\n    :param ngen:\n    :param archive:\n    :param stats:\n    :param verbose:\n    :param history:\n    :return:\n    \"\"\"\n    start = time.time()\n    if history is not None:\n        history.update(population)\n    logbook = tools.Logbook()\n    logbook.header = ['gen', 'nevals', 'cpu_time'] + (stats.fields if stats else [])\n    render_fitness(population, toolbox, history)\n    record_information(population, stats, start, archive, logbook, verbose)\n    for gen in range(1, ngen + 1):\n        offspring = generate_next_population(population, toolbox)\n        render_fitness(offspring, toolbox, history)\n        population = offspring\n        record_information(population, stats, start, archive, logbook, verbose)\n    return population, logbook, history"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to cast the string value to an int or float.", "response": "def try_number(value):\n    \"\"\"\n    Attempt to cast the string `value` to an int, and failing that, a float,\n    failing that, raise a ValueError.\n    \"\"\"\n\n    for cast_function in [int, float]:\n        try:\n            return cast_function(value)\n        except ValueError:\n            pass\n\n    raise ValueError(\"Unable to use value as int or float: {0!r}\"\n                     .format(value))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_durations(metric):\n    if metric[0] == 'avgSessionDuration' and metric[1]:\n        new_metric = (metric[0], metric[1] * 1000)\n    else:\n        new_metric = metric\n    return new_metric", "response": "Convert session duration metrics from seconds to milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the parameters from the request.", "response": "def read_parameters(request, view_kwargs=None, capture_errors=False):  # noqa: C901\n    \"\"\"\n    :param request: HttpRequest with attached api_info\n    :type request: HttpRequest\n    :type view_kwargs: dict[str, object]\n    :type capture_errors: bool\n    :rtype: dict[str, object]\n    \"\"\"\n    if view_kwargs is None:\n        view_kwargs = {}\n    params = {}\n    errors = {}\n    for param in request.api_info.operation.parameters:\n        try:\n            value = param.get_value(request, view_kwargs)\n            if value is NO_VALUE:\n                if param.has_default:\n                    params[param.name] = param.default\n                elif param.required:  # Required but missing\n                    errors[param.name] = MissingParameter('parameter %s is required but missing' % param.name)\n                continue  # No value, or a default was added, or an error was added.\n            params[param.name] = param.cast(request.api_info.api, value)\n        except (NotImplementedError, ImproperlyConfigured):\n            raise\n        except Exception as e:\n            if not capture_errors:\n                raise\n            errors[param.name] = e\n    if errors:\n        raise ErroneousParameters(errors, params)\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_datetime(date_key):\n        '''\n        Extract the first date from 'key' matching YYYY-MM-DD\n        or YYYY-MM, and convert to datetime.\n        '''\n        match = re.search(r'\\d{4}-\\d{2}(-\\d{2})?', date_key)\n        formatter = '%Y-%m'\n        if len(match.group()) == 10:\n            formatter += '-%d'\n        return datetime.strptime(\n            match.group(), formatter).replace(tzinfo=pytz.UTC)", "response": "Extract the first date from key matching YYYY - MM - DD or YYYY - MM and convert to datetime."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a float to a 38 - precision Decimal", "response": "def float_to_decimal(f):\n    \"\"\" Convert a float to a 38-precision Decimal \"\"\"\n    n, d = f.as_integer_ratio()\n    numerator, denominator = Decimal(n), Decimal(d)\n    return DECIMAL_CONTEXT.divide(numerator, denominator)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the value is a Dynamo - formatted value.", "response": "def is_dynamo_value(value):\n    \"\"\" Returns True if the value is a Dynamo-formatted value \"\"\"\n    if not isinstance(value, dict) or len(value) != 1:\n        return False\n    subkey = six.next(six.iterkeys(value))\n    return subkey in TYPES_REV"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_set(dynamizer, value):\n    inner_value = next(iter(value))\n    inner_type = dynamizer.raw_encode(inner_value)[0]\n    return inner_type + 'S', [dynamizer.raw_encode(v)[1] for v in value]", "response": "Encode a set for the DynamoDB format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode a list for the DynamoDB format", "response": "def encode_list(dynamizer, value):\n    \"\"\" Encode a list for the DynamoDB format \"\"\"\n    encoded_list = []\n    dict(map(dynamizer.raw_encode, value))\n    for v in value:\n        encoded_type, encoded_value = dynamizer.raw_encode(v)\n        encoded_list.append({\n            encoded_type: encoded_value,\n        })\n    return 'L', encoded_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding a dict for the DynamoDB format", "response": "def encode_dict(dynamizer, value):\n    \"\"\" Encode a dict for the DynamoDB format \"\"\"\n    encoded_dict = {}\n    for k, v in six.iteritems(value):\n        encoded_type, encoded_value = dynamizer.raw_encode(v)\n        encoded_dict[k] = {\n            encoded_type: encoded_value,\n        }\n    return 'M', encoded_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef raw_encode(self, value):\n        if type(value) in self.encoders:\n            encoder = self.encoders[type(value)]\n            return encoder(self, value)\n        raise ValueError(\"No encoder for value '%s' of type '%s'\" %\n                         (value, type(value)))", "response": "Run the encoder on a value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the encoder on a dict of keys", "response": "def encode_keys(self, keys):\n        \"\"\" Run the encoder on a dict of values \"\"\"\n        return dict(((k, self.encode(v)) for k, v in six.iteritems(keys) if not\n                     is_null(v)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maybe_encode_keys(self, keys):\n        ret = {}\n        for k, v in six.iteritems(keys):\n            if is_dynamo_value(v):\n                return keys\n            elif not is_null(v):\n                ret[k] = self.encode(v)\n        return ret", "response": "Encode keys if they are already in Dynamo format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_keys(self, keys):\n        return dict(((k, self.decode(v)) for k, v in six.iteritems(keys)))", "response": "Run the decoder on a dict of keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode a dynamo value into a python value", "response": "def decode(self, dynamo_value):\n        \"\"\" Decode a dynamo value into a python value \"\"\"\n        type, value = next(six.iteritems(dynamo_value))\n        if type == STRING:\n            return value\n        elif type == BINARY:\n            return Binary(value)\n        elif type == NUMBER:\n            return Decimal(value)\n        elif type == STRING_SET:\n            return set(value)\n        elif type == BINARY_SET:\n            return set((Binary(v) for v in value))\n        elif type == NUMBER_SET:\n            return set((Decimal(v) for v in value))\n        elif type == BOOL:\n            return value\n        elif type == LIST:\n            return [self.decode(v) for v in value]\n        elif type == MAP:\n            decoded_dict = {}\n            for k, v in six.iteritems(value):\n                decoded_dict[k] = self.decode(v)\n            return decoded_dict\n        elif type == NULL:\n            return None\n        else:\n            raise TypeError(\"Received unrecognized type %r from dynamo\", type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_paragraph(self):\n        self.text = ''\n        for x in range(randint(5, 12)):\n            sentence = self._write_sentence()\n            self.text = self.text + sentence\n        return self.text", "response": "Write a paragraph of 5 sentences."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_job_line(line):\n    parsed = None\n\n    if not ignore_line_re.match(line):\n        parsed = tuple(line.strip().split(','))\n\n    return parsed", "response": "Parses a single line of a job into a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_crontab(current_crontab, path_to_jobs, path_to_app, unique_id):\n    set_disable_envar = ''\n    if os.environ.get('DISABLE_COLLECTORS') == 'true':\n        set_disable_envar = 'DISABLE_COLLECTORS={} '.format(\n            os.environ.get('DISABLE_COLLECTORS'))\n    job_template = '{schedule} ' \\\n                   '{set_disable_envar}' \\\n                   '{app_path}/venv/bin/pp-collector ' \\\n                   '-l {collector_slug} ' \\\n                   '-c {app_path}/config/{credentials} ' \\\n                   '-t {app_path}/config/{token} ' \\\n                   '-b {app_path}/config/{performanceplatform} ' \\\n                   '>> {app_path}/log/out.log 2>> {app_path}/log/error.log'\n\n    crontab = [line.strip() for line in current_crontab]\n    crontab = remove_existing_crontab_for_app(crontab, unique_id)\n    additional_crontab = []\n\n    job_number = 0\n    with open(path_to_jobs) as jobs:\n        try:\n            for job in jobs:\n                parsed = parse_job_line(job)\n\n                if parsed is not None:\n                    job_number += 1\n                    if skip_job(job_number):\n                        continue\n\n                    schedule, collector_slug, credentials, \\\n                        token, performanceplatform = parsed\n\n                    cronjob = job_template.format(\n                        schedule=schedule,\n                        set_disable_envar=set_disable_envar,\n                        app_path=path_to_app,\n                        collector_slug=collector_slug,\n                        credentials=credentials,\n                        token=token,\n                        performanceplatform=performanceplatform\n                    )\n\n                    additional_crontab.append(cronjob)\n\n        except ValueError as e:\n            raise ParseError(str(e))\n\n    if additional_crontab:\n        crontab.append(crontab_begin_comment(unique_id))\n        crontab.extend(additional_crontab)\n        crontab.append(crontab_end_comment(unique_id))\n\n    return crontab", "response": "Generates a crontab from a file path"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmaps parameters to form field names", "response": "def map_parameters(cls, params):\n        \"\"\"Maps parameters to form field names\"\"\"\n\n        d = {}\n        for k, v in six.iteritems(params):\n            d[cls.FIELD_MAP.get(k.lower(), k)] = v\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef world_to_image(bbox, size):\n\n    px_per_unit = (float(size[0])/bbox.width, float(size[1])/bbox.height)\n    return lambda x,y: ((x-bbox.xmin) * px_per_unit[0], size[1] - (y-bbox.ymin)*px_per_unit[1])", "response": "Function generator to create functions for converting from world coordinates to image coordinates"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a copy of this image warped to a target size and bounding box.", "response": "def warp(self, target_bbox, target_size=None):\n        \"\"\"Returns a copy of this image warped to a target size and bounding box\"\"\"\n\n        # Determine target size based on pixels per unit of the source image and the target bounding box reprojected\n        # to the source projection.\n        if not target_size:\n            px_per_unit = (float(self.image.size[0])/self.bbox.width, float(self.image.size[1])/self.bbox.height)\n            src_bbox = target_bbox.project(self.bbox.projection)\n            target_size = (int(round(src_bbox.width*px_per_unit[0])), int(round(src_bbox.height*px_per_unit[1])))\n\n        canvas_size = (\n            max(target_size[0], self.image.size[0]),\n            max(target_size[1], self.image.size[1])\n        )\n\n        # If target and source bounds are the same and source and target sizes are the same, return a reference to\n        # this image.\n        if self.bbox == target_bbox and self.image.size == target_size:\n            return self\n\n        # If target and source projections are the same, perform a simple resize\n        elif self.bbox.projection.srs == target_bbox.projection.srs:\n            to_source_image = world_to_image(self.bbox, self.image.size)\n            upper_left = to_source_image(*(target_bbox.xmin, target_bbox.ymax))\n            lower_right = to_source_image(*(target_bbox.xmax, target_bbox.ymin))\n\n            if canvas_size == self.image.size:\n                im = self.image\n            else:\n                im = Image.new(\"RGBA\", canvas_size, (0, 0, 0, 0))\n                im.paste(self.image, (0, 0))\n\n            new_image = im.transform(\n                target_size, Image.EXTENT, (upper_left[0], upper_left[1], lower_right[0], lower_right[1]),\n                Image.NEAREST\n            )\n\n        # Full warp\n        else:\n            if canvas_size == self.image.size:\n                im = self.image\n            else:\n                im = Image.new(\"RGBA\", canvas_size, (0, 0, 0, 0))\n                im.paste(self.image, (0, 0))\n\n            new_image = im.transform(\n                target_size, Image.MESH, self._create_mesh(target_bbox, target_size), Image.NEAREST\n            )\n\n        return GeoImage(new_image, target_bbox)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to take the first department code or fall back to string as passed", "response": "def try_get_department(department_or_code):\n    \"\"\"\n    Try to take the first department code, or fall back to string as passed\n    \"\"\"\n    try:\n        value = take_first_department_code(department_or_code)\n    except AssertionError:\n        value = department_or_code\n\n    if value in DEPARTMENT_MAPPING:\n        value = DEPARTMENT_MAPPING[value]\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user(self, username, macaroons):\n        url = '{}u/{}'.format(self.url, username)\n        return make_request(url, timeout=self.timeout, macaroons=macaroons)", "response": "Fetch user data.\n\n        Raise a ServerError if an error occurs in the request process.\n\n        @param username the user's name.\n        @param macaroons the encoded macaroons string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef debug(self):\n        url = '{}debug/status'.format(self.url)\n        try:\n            return make_request(url, timeout=self.timeout)\n        except ServerError as err:\n            return {\"error\": str(err)}", "response": "Retrieve the debug information from the identity manager."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend user identity information to the identity manager.", "response": "def login(self, username, json_document):\n        \"\"\"Send user identity information to the identity manager.\n\n        Raise a ServerError if an error occurs in the request process.\n\n        @param username The logged in user.\n        @param json_document The JSON payload for login.\n        \"\"\"\n        url = '{}u/{}'.format(self.url, username)\n        make_request(\n            url, method='PUT', body=json_document, timeout=self.timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef discharge(self, username, macaroon):\n        caveats = macaroon.third_party_caveats()\n        if len(caveats) != 1:\n            raise InvalidMacaroon(\n                'Invalid number of third party caveats (1 != {})'\n                ''.format(len(caveats)))\n        url = '{}discharger/discharge?discharge-for-user={}&id={}'.format(\n            self.url, quote(username), caveats[0][1])\n        logging.debug('Sending identity info to {}'.format(url))\n        logging.debug('data is {}'.format(caveats[0][1]))\n        response = make_request(url, method='POST', timeout=self.timeout)\n        try:\n            macaroon = response['Macaroon']\n            json_macaroon = json.dumps(macaroon)\n        except (KeyError, UnicodeDecodeError) as err:\n            raise InvalidMacaroon(\n                'Invalid macaroon from discharger: {}'.format(err.message))\n\n        return base64.urlsafe_b64encode(json_macaroon.encode('utf-8'))", "response": "Discharge the macarooon for the identity."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef discharge_token(self, username):\n        url = '{}discharge-token-for-user?username={}'.format(\n            self.url, quote(username))\n        logging.debug('Sending identity info to {}'.format(url))\n        response = make_request(url, method='GET', timeout=self.timeout)\n        try:\n            macaroon = response['DischargeToken']\n            json_macaroon = json.dumps(macaroon)\n        except (KeyError, UnicodeDecodeError) as err:\n            raise InvalidMacaroon(\n                'Invalid macaroon from discharger: {}'.format(err.message))\n        return base64.urlsafe_b64encode(\"[{}]\".format(\n            json_macaroon).encode('utf-8'))", "response": "Discharge a user s identity token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_extra_info(self, username, extra_info):\n        url = self._get_extra_info_url(username)\n        make_request(url, method='PUT', body=extra_info, timeout=self.timeout)", "response": "Set extra info for the given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_extra_info(self, username):\n        url = self._get_extra_info_url(username)\n        return make_request(url, timeout=self.timeout)", "response": "Get extra info for the given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads an ISOchrone from a file and parses the data into a dictionary of properties.", "response": "def _parse(self,filename):\n        \"\"\"\n        Reads an isochrone in the old Padova format (Girardi 2002,\n        Marigo 2008) and determines the age (log10 yrs and Gyr),\n        metallicity (Z and [Fe/H]), and creates arrays with the\n        initial stellar mass and corresponding magnitudes for each\n        step along the isochrone.\n        http://stev.oapd.inaf.it/cgi-bin/cmd\n        \"\"\"\n        try:\n            columns = self.columns[self.survey.lower()]\n        except KeyError as e:\n            logger.warning('did not recognize survey %s'%(survey))\n            raise(e)\n\n        kwargs = dict(delimiter='\\t',usecols=list(columns.keys()),dtype=list(columns.values()))\n        self.data = np.genfromtxt(filename,**kwargs)\n        \n        self.mass_init = self.data['mass_init']\n        self.mass_act  = self.data['mass_act']\n        self.luminosity = 10**self.data['log_lum']\n        self.mag_1 = self.data[self.band_1]\n        self.mag_2 = self.data[self.band_2]\n        self.stage = np.char.array(self.data['stage']).strip()\n        for i,s in enumerate(self.stage):\n            if i>0 and s=='' and self.stage[i-1]!='':\n                self.stage[i] = self.stage[i-1]\n\n        # Check where post-AGB isochrone data points begin\n        self.mass_init_upper_bound = np.max(self.mass_init)\n        if np.any(self.stage == 'LTP'):\n            self.index = np.nonzero(self.stage == 'LTP')[0][0]\n        else:\n            self.index = len(self.mass_init)\n\n        self.mag = self.mag_1 if self.band_1_detection else self.mag_2\n        self.color = self.mag_1 - self.mag_2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_complete(self):\n\n        return all(p.name in self.values for p in self.parameters if p.required)", "response": "Do all required parameters have values?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncleaning and returns the given value", "response": "def clean(self, value):\n        \"\"\"Cleans and returns the given value, or raises a ParameterNotValidError exception\"\"\"\n\n        for parameter_obj in self.types:\n            try:\n                return parameter_obj.clean(value)\n            except ParameterNotValidError:\n                continue\n\n        raise ParameterNotValidError"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serialize_args(self):\n\n        args, kwargs = super(MultiParameter, self).serialize_args()\n        args.insert(0, [[t.id, t.serialize_args()] for t in self.types])\n\n        return args, kwargs", "response": "Returns ( args kwargs ) to be used when deserializing this parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean(self, value):\n\n        if not isinstance(value, (list, tuple, set, GeneratorType)):\n            value = [value]\n\n        gen = (self.param_type.clean(x) for x in value)\n\n        if isinstance(value, GeneratorType):\n            return gen\n        else:\n            return list(gen)", "response": "Cleans and returns the given value or raises a ParameterNotValidError exception"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn ( args kwargs ) to be used when deserializing this parameter.", "response": "def serialize_args(self):\n        \"\"\"Returns (args, kwargs) to be used when deserializing this parameter.\"\"\"\n\n        args, kwargs = super(ListParameter, self).serialize_args()\n        args.insert(0, [self.param_type.id, self.param_type.serialize_args()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean(self, value):\n\n        if isinstance(value, six.string_types):\n            return value\n        elif isinstance(value, numbers.Number):\n            return str(value)\n\n        raise ParameterNotValidError", "response": "Cleans and returns the given value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean(self, value):\n\n        if isinstance(value, numbers.Number):\n            return value\n        elif isinstance(value, six.string_types):\n            try:\n                value = float(value)\n                return int(value) if value.is_integer() else value\n            except ValueError:\n                raise ParameterNotValidError\n\n        raise ParameterNotValidError", "response": "Cleans and returns the given value or raises a ParameterNotValidError exception"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans and returns the given value raises a ParameterNotValidError exception", "response": "def clean(self, value):\n        \"\"\"Cleans and returns the given value, or raises a ParameterNotValidError exception\"\"\"\n\n        if isinstance(value, six.string_types) and value.lower() == 'false':\n            return False\n\n        return bool(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean(self, value):\n\n        if isinstance(value, numpy.ndarray):\n            return value\n        elif isinstance(value, (list, tuple)):\n            return numpy.array(value)\n\n        raise ParameterNotValidError", "response": "Cleans and returns the given value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclean and returns the given value or raises a ParameterNotValidError exception", "response": "def clean(self, value):\n        \"\"\"Cleans and returns the given value, or raises a ParameterNotValidError exception\"\"\"\n\n        if isinstance(value, (list, tuple)):\n            return [super(FeatureCollectionParameter, self).clean(x) for x in value]\n\n        raise ParameterNotValidError"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean(self, value):\n\n        if not isinstance(value, six.string_types):\n            raise ParameterNotValidError\n\n        try:\n            source, value = value.split('://', 1)\n        except ValueError:\n            raise ParameterNotValidError\n\n        if source == 'service':\n            if '@' in value:\n                value, timestamp = value.split('@', 1)\n\n                try:\n                    service_time = timestamp_to_date(int(timestamp))\n                except ValueError:\n                    raise ParameterNotValidError\n            else:\n                service_time = None\n\n            if ':' in value:\n                service_name, variable_name = value.split(':', 1)\n            else:\n                service_name = value\n                variable_name = None\n\n            try:\n                self.service = Service.objects.get(name=service_name)\n            except ObjectDoesNotExist:\n                raise ParameterNotValidError(\"Service '{}' not found\".format(service_name))\n\n            if variable_name:\n                try:\n                    variable = self.service.variable_set.all().get(variable=variable_name)\n                except ObjectDoesNotExist:\n                    raise ParameterNotValidError(\"Variable '{}' not found\".format(variable_name))\n\n                if service_time is not None:\n                    time_index = best_fit(variable.time_stops, service_time)\n                else:\n                    time_index = None\n\n                data = self.get_grid_for_variable(variable, time_index=time_index)\n                return Raster(data, variable.full_extent, 1, 0, self.is_y_increasing(variable))\n            else:\n                return self.dataset\n        else:\n            raise ParameterNotValidError('Invalid source: {}'.format(source))", "response": "Cleans and returns the given value or raises a ParameterNotValidError exception"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencode an item write command", "response": "def _encode_write(dynamizer, data, action, key):\n    \"\"\" Encode an item write command \"\"\"\n    # Strip null values out of data\n    data = dict(((k, dynamizer.encode(v)) for k, v in six.iteritems(data) if\n                 not is_null(v)))\n    return {\n        action: {\n            key: data,\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode query constraints in Dynamo format", "response": "def encode_query_kwargs(dynamizer, kwargs):\n    \"\"\" Encode query constraints in Dynamo format \"\"\"\n    ret = {}\n    for k, v in six.iteritems(kwargs):\n        if '__' not in k:\n            raise TypeError(\"Invalid query argument '%s'\" % k)\n        name, condition_key = k.split('__')\n        # Convert ==None to IS_NULL\n        if condition_key == 'eq' and is_null(v):\n            condition_key = 'null'\n            v = True\n        # null is a special case\n        if condition_key == 'null':\n            ret[name] = {\n                'ComparisonOperator': 'NULL' if v else 'NOT_NULL'\n            }\n            continue\n        elif condition_key not in ('in', 'between'):\n            v = (v,)\n        ret[name] = {\n            'AttributeValueList': [dynamizer.encode(value) for value in v],\n            'ComparisonOperator': CONDITIONS[condition_key],\n        }\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the attributes for the update", "response": "def attrs(self, dynamizer):\n        \"\"\" Get the attributes for the update \"\"\"\n        ret = {\n            self.key: {\n                'Action': self.action,\n            }\n        }\n        if not is_null(self.value):\n            ret[self.key]['Value'] = dynamizer.encode(self.value)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expected(self, dynamizer):\n        if self._expect_kwargs:\n            return encode_query_kwargs(dynamizer, self._expect_kwargs)\n        if self._expected is not NO_ARG:\n            ret = {}\n            if is_null(self._expected):\n                ret['Exists'] = False\n            else:\n                ret['Value'] = dynamizer.encode(self._expected)\n                ret['Exists'] = True\n            return {self.key: ret}\n        return {}", "response": "Get the expected values for the update"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put(self, data):\n        self._to_put.append(data)\n\n        if self.should_flush():\n            self.flush()", "response": "Writes an item to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes an item from the cache.", "response": "def delete(self, kwargs):\n        \"\"\"\n        Delete an item\n\n        Parameters\n        ----------\n        kwargs : dict\n            The primary key of the item to delete\n\n        \"\"\"\n        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nflushes pending items to Dynamo", "response": "def flush(self):\n        \"\"\" Flush pending items to Dynamo \"\"\"\n        items = []\n\n        for data in self._to_put:\n            items.append(encode_put(self.connection.dynamizer, data))\n\n        for data in self._to_delete:\n            items.append(encode_delete(self.connection.dynamizer, data))\n        self._write(items)\n        self._to_put = []\n        self._to_delete = []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write(self, items):\n        response = self._batch_write_item(items)\n        if 'consumed_capacity' in response:\n            # Comes back as a list from BatchWriteItem\n            self.consumed_capacity = \\\n                sum(response['consumed_capacity'], self.consumed_capacity)\n\n        if response.get('UnprocessedItems'):\n            unprocessed = response['UnprocessedItems'].get(self.tablename, [])\n\n            # Some items have not been processed. Stow them for now &\n            # re-attempt processing on ``__exit__``.\n            LOG.info(\"%d items were unprocessed. Storing for later.\",\n                     len(unprocessed))\n            self._unprocessed.extend(unprocessed)\n            # Getting UnprocessedItems indicates that we are exceeding our\n            # throughput. So sleep for a bit.\n            self._attempt += 1\n            self.connection.exponential_sleep(self._attempt)\n        else:\n            # No UnprocessedItems means our request rate is fine, so we can\n            # reset the attempt number.\n            self._attempt = 0\n\n        return response", "response": "Perform a batch write and handle the response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nresend all unprocessed items.", "response": "def resend_unprocessed(self):\n        \"\"\" Resend all unprocessed items \"\"\"\n        LOG.info(\"Re-sending %d unprocessed items.\", len(self._unprocessed))\n\n        while self._unprocessed:\n            to_resend = self._unprocessed[:MAX_WRITE_BATCH]\n            self._unprocessed = self._unprocessed[MAX_WRITE_BATCH:]\n            LOG.info(\"Sending %d items\", len(to_resend))\n            self._write(to_resend)\n            LOG.info(\"%d unprocessed items left\", len(self._unprocessed))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a BatchWriteItem call to Dynamo", "response": "def _batch_write_item(self, items):\n        \"\"\" Make a BatchWriteItem call to Dynamo \"\"\"\n        kwargs = {\n            'RequestItems': {\n                self.tablename: items,\n            },\n            'ReturnConsumedCapacity': self.return_capacity,\n            'ReturnItemCollectionMetrics': self.return_item_collection_metrics,\n        }\n        return self.connection.call('batch_write_item', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_path(entity_id):\n    '''Get the entity_id as a string if it is a Reference.\n\n    @param entity_id The ID either a reference or a string of the entity\n          to get.\n    @return entity_id as a string\n    '''\n    try:\n        path = entity_id.path()\n    except AttributeError:\n        path = entity_id\n    if path.startswith('cs:'):\n        path = path[3:]\n    return path", "response": "Get the entity_id as a string if it is a Reference.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a get request against the charmstore.", "response": "def _get(self, url):\n        \"\"\"Make a get request against the charmstore.\n\n        This method is used by other API methods to standardize querying.\n        @param url The full url to query\n            (e.g. https://api.jujucharms.com/charmstore/v4/macaroon)\n        \"\"\"\n        try:\n            response = requests.get(url, verify=self.verify,\n                                    cookies=self.cookies, timeout=self.timeout,\n                                    auth=self._client.auth())\n            response.raise_for_status()\n            return response\n        except HTTPError as exc:\n            if exc.response.status_code in (404, 407):\n                raise EntityNotFound(url)\n            else:\n                message = ('Error during request: {url} '\n                           'status code:({code}) '\n                           'message: {message}').format(\n                               url=url,\n                               code=exc.response.status_code,\n                               message=exc.response.text)\n                logging.error(message)\n                raise ServerError(exc.response.status_code,\n                                  exc.response.text,\n                                  message)\n        except Timeout:\n            message = 'Request timed out: {url} timeout: {timeout}'\n            message = message.format(url=url, timeout=self.timeout)\n            logging.error(message)\n            raise ServerError(message)\n        except RequestException as exc:\n            message = ('Error during request: {url} '\n                       'message: {message}').format(\n                           url=url,\n                           message=exc)\n            logging.error(message)\n            raise ServerError(exc.args[0][1].errno,\n                              exc.args[0][1].strerror,\n                              message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve metadata about an entity in the charmstore.", "response": "def _meta(self, entity_id, includes, channel=None):\n        '''Retrieve metadata about an entity in the charmstore.\n\n        @param entity_id The ID either a reference or a string of the entity\n               to get.\n        @param includes Which metadata fields to include in the response.\n        @param channel Optional channel name, e.g. `stable`.\n        '''\n        queries = []\n        if includes is not None:\n            queries.extend([('include', include) for include in includes])\n        if channel is not None:\n            queries.append(('channel', channel))\n        if len(queries):\n            url = '{}/{}/meta/any?{}'.format(self.url, _get_path(entity_id),\n                                             urlencode(queries))\n        else:\n            url = '{}/{}/meta/any'.format(self.url, _get_path(entity_id))\n        data = self._get(url)\n        return data.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef entity(self, entity_id, get_files=False, channel=None,\n               include_stats=True, includes=None):\n        '''Get the default data for any entity (e.g. bundle or charm).\n\n        @param entity_id The entity's id either as a reference or a string\n        @param get_files Whether to fetch the files for the charm or not.\n        @param channel Optional channel name.\n        @param include_stats Optionally disable stats collection.\n        @param includes An optional list of meta info to include, as a\n            sequence of strings. If None, the default include list is used.\n        '''\n        if includes is None:\n            includes = DEFAULT_INCLUDES[:]\n        if get_files and 'manifest' not in includes:\n            includes.append('manifest')\n        if include_stats and 'stats' not in includes:\n            includes.append('stats')\n        return self._meta(entity_id, includes, channel=channel)", "response": "Get the default data for the given entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the default data for entities.", "response": "def entities(self, entity_ids):\n        '''Get the default data for entities.\n\n        @param entity_ids A list of entity ids either as strings or references.\n        '''\n        url = '%s/meta/any?include=id&' % self.url\n        for entity_id in entity_ids:\n            url += 'id=%s&' % _get_path(entity_id)\n        # Remove the trailing '&' from the URL.\n        url = url[:-1]\n        data = self._get(url)\n        return data.json()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bundle(self, bundle_id, channel=None):\n        '''Get the default data for a bundle.\n\n        @param bundle_id The bundle's id.\n        @param channel Optional channel name.\n        '''\n        return self.entity(bundle_id, get_files=True, channel=channel)", "response": "Get the default data for a bundle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef charm(self, charm_id, channel=None):\n        '''Get the default data for a charm.\n\n        @param charm_id The charm's id.\n        @param channel Optional channel name.\n        '''\n        return self.entity(charm_id, get_files=True, channel=channel)", "response": "Get the default data for a charm."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the path to the icon for charms.", "response": "def charm_icon_url(self, charm_id, channel=None):\n        '''Generate the path to the icon for charms.\n\n        @param charm_id The ID of the charm.\n        @param channel Optional channel name.\n        @return The url to the icon.\n        '''\n        url = '{}/{}/icon.svg'.format(self.url, _get_path(charm_id))\n        return _add_channel(url, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef charm_icon(self, charm_id, channel=None):\n        '''Get the charm icon.\n\n        @param charm_id The ID of the charm.\n        @param channel Optional channel name.\n        '''\n        url = self.charm_icon_url(charm_id, channel=channel)\n        response = self._get(url)\n        return response.content", "response": "Get the charm icon."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the bundle visualization.", "response": "def bundle_visualization(self, bundle_id, channel=None):\n        '''Get the bundle visualization.\n\n        @param bundle_id The ID of the bundle.\n        @param channel Optional channel name.\n        '''\n        url = self.bundle_visualization_url(bundle_id, channel=channel)\n        response = self._get(url)\n        return response.content"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bundle_visualization_url(self, bundle_id, channel=None):\n        '''Generate the path to the visualization for bundles.\n\n        @param charm_id The ID of the bundle.\n        @param channel Optional channel name.\n        @return The url to the visualization.\n        '''\n        url = '{}/{}/diagram.svg'.format(self.url, _get_path(bundle_id))\n        return _add_channel(url, channel)", "response": "Generate the path to the visualization for bundles."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef entity_readme_url(self, entity_id, channel=None):\n        '''Generate the url path for the readme of an entity.\n\n        @entity_id The id of the entity (i.e. charm, bundle).\n        @param channel Optional channel name.\n        '''\n        url = '{}/{}/readme'.format(self.url, _get_path(entity_id))\n        return _add_channel(url, channel)", "response": "Generate the url path for the readme of an entity."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef entity_readme_content(self, entity_id, channel=None):\n        '''Get the readme for an entity.\n\n        @entity_id The id of the entity (i.e. charm, bundle).\n        @param channel Optional channel name.\n        '''\n        readme_url = self.entity_readme_url(entity_id, channel=channel)\n        response = self._get(readme_url)\n        return response.text", "response": "Get the readme for an entity."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a URL for the archive of an entity.", "response": "def archive_url(self, entity_id, channel=None):\n        '''Generate a URL for the archive of an entity..\n\n        @param entity_id The ID of the entity to look up as a string\n               or reference.\n        @param channel Optional channel name.\n        '''\n        url = '{}/{}/archive'.format(self.url, _get_path(entity_id))\n        return _add_channel(url, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef file_url(self, entity_id, filename, channel=None):\n        '''Generate a URL for a file in an archive without requesting it.\n\n        @param entity_id The ID of the entity to look up.\n        @param filename The name of the file in the archive.\n        @param channel Optional channel name.\n        '''\n        url = '{}/{}/archive/{}'.format(self.url, _get_path(entity_id),\n                                        filename)\n        return _add_channel(url, channel)", "response": "Generate a URL for a file in an archive without requesting it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef files(self, entity_id, manifest=None, filename=None,\n              read_file=False, channel=None):\n        '''\n        Get the files or file contents of a file for an entity.\n\n        If all files are requested, a dictionary of filenames and urls for the\n        files in the archive are returned.\n\n        If filename is provided, the url of just that file is returned, if it\n        exists.\n\n        If filename is provided and read_file is true, the *contents* of the\n        file are returned, if the file exists.\n\n        @param entity_id The id of the entity to get files for\n        @param manifest The manifest of files for the entity. Providing this\n            reduces queries; if not provided, the manifest is looked up in the\n            charmstore.\n        @param filename The name of the file in the archive to get.\n        @param read_file Whether to get the url for the file or the file\n            contents.\n        @param channel Optional channel name.\n        '''\n        if manifest is None:\n            manifest_url = '{}/{}/meta/manifest'.format(self.url,\n                                                        _get_path(entity_id))\n            manifest_url = _add_channel(manifest_url, channel)\n            manifest = self._get(manifest_url)\n            manifest = manifest.json()\n        files = {}\n        for f in manifest:\n            manifest_name = f['Name']\n            file_url = self.file_url(_get_path(entity_id), manifest_name,\n                                     channel=channel)\n            files[manifest_name] = file_url\n\n        if filename:\n            file_url = files.get(filename, None)\n            if file_url is None:\n                raise EntityNotFound(entity_id, filename)\n            if read_file:\n                data = self._get(file_url)\n                return data.text\n            else:\n                return file_url\n        else:\n            return files", "response": "Get the files or file contents of a file for an entity."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the resource url for a given resource on an entity.", "response": "def resource_url(self, entity_id, name, revision):\n        '''\n        Return the resource url for a given resource on an entity.\n\n        @param entity_id The id of the entity to get resource for.\n        @param name The name of the resource.\n        @param revision The revision of the resource.\n        '''\n        return '{}/{}/resource/{}/{}'.format(self.url,\n                                             _get_path(entity_id),\n                                             name,\n                                             revision)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef config(self, charm_id, channel=None):\n        '''Get the config data for a charm.\n\n        @param charm_id The charm's id.\n        @param channel Optional channel name.\n        '''\n        url = '{}/{}/meta/charm-config'.format(self.url, _get_path(charm_id))\n        data = self._get(_add_channel(url, channel))\n        return data.json()", "response": "Get the config data for a charm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, text, includes=None, doc_type=None, limit=None,\n               autocomplete=False, promulgated_only=False, tags=None,\n               sort=None, owner=None, series=None):\n        '''\n        Search for entities in the charmstore.\n\n        @param text The text to search for.\n        @param includes What metadata to return in results (e.g. charm-config).\n        @param doc_type Filter to this type: bundle or charm.\n        @param limit Maximum number of results to return.\n        @param autocomplete Whether to prefix/suffix match search terms.\n        @param promulgated_only Whether to filter to only promulgated charms.\n        @param tags The tags to filter; can be a list of tags or a single tag.\n        @param sort Sorting the result based on the sort string provided\n            which can be name, author, series and - in front for descending.\n        @param owner Optional owner. If provided, search results will only\n            include entities that owner can view.\n        @param series The series to filter; can be a list of series or a\n            single series.\n        '''\n        queries = self._common_query_parameters(doc_type, includes, owner,\n                                                promulgated_only, series, sort)\n        if len(text):\n            queries.append(('text', text))\n        if limit is not None:\n            queries.append(('limit', limit))\n        if autocomplete:\n            queries.append(('autocomplete', 1))\n        if tags is not None:\n            if type(tags) is list:\n                tags = ','.join(tags)\n            queries.append(('tags', tags))\n        if len(queries):\n            url = '{}/search?{}'.format(self.url, urlencode(queries))\n        else:\n            url = '{}/search'.format(self.url)\n        data = self._get(url)\n        return data.json()['Results']", "response": "Search for entities in the charmstore."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists the entities in the charmstore.", "response": "def list(self, includes=None, doc_type=None, promulgated_only=False,\n             sort=None, owner=None, series=None):\n        '''\n        List entities in the charmstore.\n\n        @param includes What metadata to return in results (e.g. charm-config).\n        @param doc_type Filter to this type: bundle or charm.\n        @param promulgated_only Whether to filter to only promulgated charms.\n        @param sort Sorting the result based on the sort string provided\n            which can be name, author, series and - in front for descending.\n        @param owner Optional owner. If provided, search results will only\n            include entities that owner can view.\n        @param series The series to filter; can be a list of series or a\n            single series.\n        '''\n        queries = self._common_query_parameters(doc_type, includes, owner,\n                                                promulgated_only, series, sort)\n        if len(queries):\n            url = '{}/list?{}'.format(self.url, urlencode(queries))\n        else:\n            url = '{}/list'.format(self.url)\n        data = self._get(url)\n        return data.json()['Results']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _common_query_parameters(self, doc_type, includes, owner,\n                                 promulgated_only, series, sort):\n        '''\n        Extract common query parameters between search and list into slice.\n\n        @param includes What metadata to return in results (e.g. charm-config).\n        @param doc_type Filter to this type: bundle or charm.\n        @param promulgated_only Whether to filter to only promulgated charms.\n        @param sort Sorting the result based on the sort string provided\n            which can be name, author, series and - in front for descending.\n        @param owner Optional owner. If provided, search results will only\n            include entities that owner can view.\n        @param series The series to filter; can be a list of series or a\n            single series.\n        '''\n        queries = []\n        if includes is not None:\n            queries.extend([('include', include) for include in includes])\n        if doc_type is not None:\n            queries.append(('type', doc_type))\n        if promulgated_only:\n            queries.append(('promulgated', 1))\n        if owner is not None:\n            queries.append(('owner', owner))\n        if series is not None:\n            if type(series) is list:\n                series = ','.join(series)\n            queries.append(('series', series))\n        if sort is not None:\n            queries.append(('sort', sort))\n        return queries", "response": "Extract common query parameters between search and list into slice."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch related entity information for the supplied entity ids.", "response": "def fetch_related(self, ids):\n        \"\"\"Fetch related entity information.\n\n        Fetches metadata, stats and extra-info for the supplied entities.\n\n        @param ids The entity ids to fetch related information for. A list of\n            entity id dicts from the charmstore.\n        \"\"\"\n        if not ids:\n            return []\n        meta = '&id='.join(id['Id'] for id in ids)\n        url = ('{url}/meta/any?id={meta}'\n               '&include=bundle-metadata&include=stats'\n               '&include=supported-series&include=extra-info'\n               '&include=bundle-unit-count&include=owner').format(\n                   url=self.url, meta=meta)\n        data = self._get(url)\n        return data.json().values()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of charms that provides or requires this interface.", "response": "def fetch_interfaces(self, interface, way):\n        \"\"\"Get the list of charms that provides or requires this interface.\n\n        @param interface The interface for the charm relation.\n        @param way The type of relation, either \"provides\" or \"requires\".\n        @return List of charms\n        \"\"\"\n        if not interface:\n            return []\n        if way == 'requires':\n            request = '&requires=' + interface\n        else:\n            request = '&provides=' + interface\n        url = (self.url + '/search?' +\n               'include=charm-metadata&include=stats&include=supported-series'\n               '&include=extra-info&include=bundle-unit-count'\n               '&limit=1000&include=owner' + request)\n        data = self._get(url)\n        return data.json().values()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef debug(self):\n        '''Retrieve the debug information from the charmstore.'''\n        url = '{}/debug/status'.format(self.url)\n        data = self._get(url)\n        return data.json()", "response": "Retrieve the debug information from the charmstore."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query_server(self,outfile,age,metallicity):\n        params = copy.deepcopy(self.download_defaults)\n\n        epsilon = 1e-4\n        lage = np.log10(age*1e9)\n        lage_min,lage_max = params['isoc_lage0'],params['isoc_lage1']\n        if not (lage_min-epsilon < lage <lage_max+epsilon):\n            msg = 'Age outside of valid range: %g [%g < log(age) < %g]'%(lage,lage_min,lage_max)\n            raise RuntimeError(msg)\n\n        z_min,z_max = params['isoc_z0'],params['isoc_z1']\n        if not (z_min <= metallicity <= z_max):\n            msg = 'Metallicity outside of valid range: %g [%g < z < %g]'%(metallicity,z_min,z_max)\n            raise RuntimeError(msg)\n        \n        params['photsys_file'] = photsys_dict[self.survey]\n        params['isoc_age']     = age * 1e9\n        params['isoc_zeta']    = metallicity\n\n        server = self.download_url\n        url = server + '/cgi-bin/cmd_%s'%params['cmd_version']\n        # First check that the server is alive\n        logger.debug(\"Accessing %s...\"%url)\n        urlopen(url,timeout=2)\n\n        q = urlencode(params).encode('utf-8')\n        logger.debug(url+'?'+q)\n        c = str(urlopen(url, q).read())\n        aa = re.compile('output\\d+')\n        fname = aa.findall(c)\n        \n        if len(fname) == 0:\n            msg = \"Output filename not found\"\n            raise RuntimeError(msg)\n\n        out = '{0}/tmp/{1}.dat'.format(server, fname[0])\n        \n        cmd = 'wget --progress dot:binary %s -O %s'%(out,outfile)\n        logger.debug(cmd)\n        stdout = subprocess.check_output(cmd,shell=True,stderr=subprocess.STDOUT)\n        logger.debug(stdout)\n\n        return outfile", "response": "Query the server for the isochrone file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the likelihood grid search", "response": "def run(self, coords=None, debug=False):\n        \"\"\"\n        Run the likelihood grid search\n        \"\"\"\n        #self.grid.precompute()\n        self.grid.search(coords=coords)\n        return self.grid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, coords=None, distance_modulus=None, tolerance=1.e-2):\n        nmoduli = len(self.distance_modulus_array)\n        npixels    = len(self.roi.pixels_target)\n        self.log_likelihood_sparse_array       = np.zeros([nmoduli, npixels])\n        self.richness_sparse_array             = np.zeros([nmoduli, npixels])\n        self.richness_lower_sparse_array       = np.zeros([nmoduli, npixels])\n        self.richness_upper_sparse_array       = np.zeros([nmoduli, npixels])\n        self.richness_upper_limit_sparse_array = np.zeros([nmoduli, npixels])\n        self.stellar_mass_sparse_array         = np.zeros([nmoduli, npixels])\n        self.fraction_observable_sparse_array  = np.zeros([nmoduli, npixels])\n\n        # Specific pixel/distance_modulus\n        coord_idx, distance_modulus_idx = None, None\n        if coords is not None:\n            # Match to nearest grid coordinate index\n            coord_idx = self.roi.indexTarget(coords[0],coords[1])\n        if distance_modulus is not None:\n            # Match to nearest distance modulus index\n            distance_modulus_idx=np.fabs(self.distance_modulus_array-distance_modulus).argmin()\n\n        lon, lat = self.roi.pixels_target.lon, self.roi.pixels_target.lat\n            \n        logger.info('Looping over distance moduli in grid search ...')\n        for ii, distance_modulus in enumerate(self.distance_modulus_array):\n\n            # Specific pixel\n            if distance_modulus_idx is not None:\n                if ii != distance_modulus_idx: continue\n\n            logger.info('  (%-2i/%i) Distance Modulus=%.1f ...'%(ii+1,nmoduli,distance_modulus))\n\n            # Set distance_modulus once to save time\n            self.loglike.set_params(distance_modulus=distance_modulus)\n\n            for jj in range(0, npixels):\n                # Specific pixel\n                if coord_idx is not None:\n                    if jj != coord_idx: continue\n\n                # Set kernel location\n                self.loglike.set_params(lon=lon[jj],lat=lat[jj])\n                # Doesn't re-sync distance_modulus each time\n                self.loglike.sync_params()\n                                         \n                args = (jj+1, npixels, self.loglike.source.lon, self.loglike.source.lat)\n                msg = '    (%-3i/%i) Candidate at (%.2f, %.2f) ... '%(args)\n\n                self.log_likelihood_sparse_array[ii][jj], self.richness_sparse_array[ii][jj], parabola = self.loglike.fit_richness()\n                self.stellar_mass_sparse_array[ii][jj] = self.stellar_mass_conversion * self.richness_sparse_array[ii][jj]\n                self.fraction_observable_sparse_array[ii][jj] = self.loglike.f\n                if self.config['scan']['full_pdf']:\n                    #n_pdf_points = 100\n                    #richness_range = parabola.profileUpperLimit(delta=25.) - self.richness_sparse_array[ii][jj]\n                    #richness = np.linspace(max(0., self.richness_sparse_array[ii][jj] - richness_range),\n                    #                          self.richness_sparse_array[ii][jj] + richness_range,\n                    #                          n_pdf_points)\n                    #if richness[0] > 0.:\n                    #    richness = np.insert(richness, 0, 0.)\n                    #    n_pdf_points += 1\n                    # \n                    #log_likelihood = np.zeros(n_pdf_points)\n                    #for kk in range(0, n_pdf_points):\n                    #    log_likelihood[kk] = self.loglike.value(richness=richness[kk])\n                    #parabola = ugali.utils.parabola.Parabola(richness, 2.*log_likelihood)\n                    #self.richness_lower_sparse_array[ii][jj], self.richness_upper_sparse_array[ii][jj] = parabola.confidenceInterval(0.6827)\n                    self.richness_lower_sparse_array[ii][jj], self.richness_upper_sparse_array[ii][jj] = self.loglike.richness_interval(0.6827)\n                    \n                    self.richness_upper_limit_sparse_array[ii][jj] = parabola.bayesianUpperLimit(0.95)\n\n                    args = (\n                        2. * self.log_likelihood_sparse_array[ii][jj],\n                        self.stellar_mass_conversion*self.richness_sparse_array[ii][jj],\n                        self.stellar_mass_conversion*self.richness_lower_sparse_array[ii][jj],\n                        self.stellar_mass_conversion*self.richness_upper_sparse_array[ii][jj],\n                        self.stellar_mass_conversion*self.richness_upper_limit_sparse_array[ii][jj]\n                    )\n                    msg += 'TS=%.1f, Stellar Mass=%.1f (%.1f -- %.1f @ 0.68 CL, < %.1f @ 0.95 CL)'%(args)\n                else:\n                    args = (\n                        2. * self.log_likelihood_sparse_array[ii][jj], \n                        self.stellar_mass_conversion * self.richness_sparse_array[ii][jj],\n                        self.fraction_observable_sparse_array[ii][jj]\n                    )\n                    msg += 'TS=%.1f, Stellar Mass=%.1f, Fraction=%.2g'%(args)\n                logger.debug(msg)\n                \n                #if coords is not None and distance_modulus is not None:\n                #    results = [self.richness_sparse_array[ii][jj],\n                #               self.log_likelihood_sparse_array[ii][jj],\n                #               self.richness_lower_sparse_array[ii][jj],\n                #               self.richness_upper_sparse_array[ii][jj],\n                #               self.richness_upper_limit_sparse_array[ii][jj],\n                #               richness, log_likelihood, self.loglike.p, self.loglike.f]\n                #    return results\n\n            jj_max = self.log_likelihood_sparse_array[ii].argmax()\n            args = (\n                jj_max+1, npixels, lon[jj_max], lat[jj_max],\n                2. * self.log_likelihood_sparse_array[ii][jj_max], \n                self.stellar_mass_conversion * self.richness_sparse_array[ii][jj_max]\n            )\n            msg = '  (%-3i/%i) Maximum at (%.2f, %.2f) ... TS=%.1f, Stellar Mass=%.1f'%(args)\n            logger.info(msg)", "response": "Search over the distance moduli in distance_modulus_array and store the results in self. log_likelihood_sparse_array loggers and self. richness_lower_sparse_array loggers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, outfile):\n        data = odict()\n        data['PIXEL']=self.roi.pixels_target\n        # Full data output (too large for survey)\n        if self.config['scan']['full_pdf']:\n            data['LOG_LIKELIHOOD']=self.log_likelihood_sparse_array.T\n            data['RICHNESS']=self.richness_sparse_array.T\n            data['RICHNESS_LOWER']=self.richness_lower_sparse_array.T\n            data['RICHNESS_UPPER']=self.richness_upper_sparse_array.T\n            data['RICHNESS_LIMIT']=self.richness_upper_limit_sparse_array.T\n            #data['STELLAR_MASS']=self.stellar_mass_sparse_array.T\n            data['FRACTION_OBSERVABLE']=self.fraction_observable_sparse_array.T\n        else:\n            data['LOG_LIKELIHOOD']=self.log_likelihood_sparse_array.T\n            data['RICHNESS']=self.richness_sparse_array.T\n            data['FRACTION_OBSERVABLE']=self.fraction_observable_sparse_array.T\n\n        # Convert to 32bit float\n        for k in list(data.keys())[1:]:\n            data[k] = data[k].astype('f4',copy=False)\n            \n        # Stellar mass can be calculated from STELLAR * RICHNESS\n        header = odict()\n        header['STELLAR']=round(self.stellar_mass_conversion,8)\n        header['LKDNSIDE']=self.config['coords']['nside_likelihood']\n        header['LKDPIX']=ang2pix(self.config['coords']['nside_likelihood'],\n                                 self.roi.lon,self.roi.lat)\n        header['NROI']=self.roi.inROI(self.loglike.catalog_roi.lon,\n                                      self.loglike.catalog_roi.lat).sum()\n        header['NANNULUS']=self.roi.inAnnulus(self.loglike.catalog_roi.lon,\n                                              self.loglike.catalog_roi.lat).sum()\n        header['NINSIDE']=self.roi.inInterior(self.loglike.catalog_roi.lon,\n                                              self.loglike.catalog_roi.lat).sum()\n        header['NTARGET']=self.roi.inTarget(self.loglike.catalog_roi.lon,\n                                            self.loglike.catalog_roi.lat).sum()\n\n        # Flatten if there is only a single distance modulus\n        # ADW: Is this really what we want to do?\n        if len(self.distance_modulus_array) == 1:\n            for key in data:\n                data[key] = data[key].flatten()\n\n        logger.info(\"Writing %s...\"%outfile)\n        write_partial_map(outfile,data,\n                          nside=self.config['coords']['nside_pixel'],\n                          header=header,\n                          clobber=True\n                          )\n        \n        fitsio.write(outfile,\n                     dict(DISTANCE_MODULUS=self.distance_modulus_array.astype('f4',copy=False)),\n                     extname='DISTANCE_MODULUS',\n                     clobber=False)", "response": "Write the likelihood results to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_uri(cls, uri, socket_timeout=None, auto_decode=False):\n        parts = six.moves.urllib.parse.urlparse(uri)\n        if parts.scheme.lower() not in ('beanstalk', 'beanstalkd'):\n            raise ValueError('Invalid scheme %s' % parts.scheme)\n        ipv6_md = re.match(r'^\\[([0-9a-fA-F:]+)\\](:[0-9]+)?$', parts.netloc)\n        if ipv6_md:\n            host = ipv6_md.group(1)\n            port = ipv6_md.group(2) or '11300'\n            port = port.lstrip(':')\n        elif ':' in parts.netloc:\n            host, port = parts.netloc.rsplit(':', 1)\n        else:\n            host = parts.netloc\n            port = 11300\n        port = int(port)\n        return cls(host, port, socket_timeout=socket_timeout, auto_decode=auto_decode)", "response": "Construct a synchronous Beanstalk Client from a URI."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _re_establish_use_watch(self):\n        if self.current_tube != self.desired_tube:\n            self.use(self.desired_tube)\n        if self._watchlist != self.desired_watchlist:\n            self.watchlist = self.desired_watchlist", "response": "Call after a close or re - connect."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_tubes(self):\n        with self._sock_ctx() as sock:\n            self._send_message('list-tubes', sock)\n            body = self._receive_data_with_prefix(b'OK', sock)\n            tubes = yaml_load(body)\n            return tubes", "response": "Return a list of tubes that this beanstalk instance knows about."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stats(self):\n        with self._sock_ctx() as socket:\n            self._send_message('stats', socket)\n            body = self._receive_data_with_prefix(b'OK', socket)\n            stats = yaml_load(body)\n            return stats", "response": "Return a bunch of instance - wide statistics for the current user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_job(self, data, pri=65536, delay=0, ttr=120):\n        with self._sock_ctx() as socket:\n            message = 'put {pri} {delay} {ttr} {datalen}\\r\\n'.format(\n                pri=pri, delay=delay, ttr=ttr, datalen=len(data), data=data\n            ).encode('utf-8')\n            if not isinstance(data, bytes):\n                data = data.encode('utf-8')\n            message += data\n            message += b'\\r\\n'\n            self._send_message(message, socket)\n            return self._receive_id(socket)", "response": "Insert a new job into the queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert a new job into a specific queue.", "response": "def put_job_into(self, tube_name, data, pri=65536, delay=0, ttr=120):\n        \"\"\"Insert a new job into a specific queue. Wrapper around :func:`put_job`.\n\n        :param tube_name: Tube name\n        :type tube_name: str\n        :param data: Job body\n        :type data: Text (either str which will be encoded as utf-8, or bytes which are already utf-8\n        :param pri: Priority for the job\n        :type pri: int\n        :param delay: Delay in seconds before the job should be placed on the ready queue\n        :type delay: int\n        :param ttr: Time to reserve (how long a worker may work on this job before we assume the worker is blocked\n            and give the job to another worker\n        :type ttr: int\n\n        .. seealso::\n\n           :func:`put_job()`\n              Put a job into whatever the current tube is\n\n           :func:`using()`\n              Insert a job using an external guard\n        \"\"\"\n        with self.using(tube_name) as inserter:\n            return inserter.put_job(data=data, pri=pri, delay=delay, ttr=ttr)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef watchlist(self, tubes):\n        tubes = set(tubes)\n        for tube in tubes - self._watchlist:\n            self.watch(tube)\n        for tube in self._watchlist - tubes:\n            self.ignore(tube)", "response": "Sets the watchlist to the given tubes\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the given tube to the watchlist.", "response": "def watch(self, tube):\n        \"\"\"Add the given tube to the watchlist.\n\n        :param tube: Name of the tube to add to the watchlist\n\n        Note: Initially, all connections are watching a tube named \"default\". If\n        you manually call :func:`watch()`, we will un-watch the \"default\" tube.\n        To keep it in your list, first call :func:`watch()` with the other tubes, then\n        call :func:`watch()` with \"default\".\n        \"\"\"\n        with self._sock_ctx() as socket:\n            self.desired_watchlist.add(tube)\n            if tube not in self._watchlist:\n                self._send_message('watch {0}'.format(tube), socket)\n                self._receive_id(socket)\n                self._watchlist.add(tube)\n                if self.initial_watch:\n                    if tube != 'default':\n                        self.ignore('default')\n                    self.initial_watch = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ignore(self, tube):\n        with self._sock_ctx() as socket:\n            if tube not in self._watchlist:\n                raise KeyError(tube)\n            if tube != 'default':\n                self.desired_watchlist.remove(tube)\n            if tube in self._watchlist:\n                self._send_message('ignore {0}'.format(tube), socket)\n                self._receive_id(socket)\n                self._watchlist.remove(tube)\n            if not self._watchlist:\n                self._watchlist.add('default')", "response": "Remove the given tube from the watchlist."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches statistics about a single job.", "response": "def stats_job(self, job_id):\n        \"\"\"Fetch statistics about a single job\n\n        :rtype: dict\n        \"\"\"\n        with self._sock_ctx() as socket:\n            if hasattr(job_id, 'job_id'):\n                job_id = job_id.job_id\n            self._send_message('stats-job {0}'.format(job_id), socket)\n            body = self._receive_data_with_prefix(b'OK', socket)\n            job_status = yaml_load(body)\n            return job_status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch statistics about a single tube", "response": "def stats_tube(self, tube_name):\n        \"\"\"Fetch statistics about a single tube\n\n        :param tube_name: Tube to fetch stats about\n        :rtype: dict\n        \"\"\"\n        with self._sock_ctx() as socket:\n            self._send_message('stats-tube {0}'.format(tube_name), socket)\n            body = self._receive_data_with_prefix(b'OK', socket)\n            return yaml_load(body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reserve_job(self, timeout=5):\n        timeout = int(timeout)\n        if self.socket_timeout is not None:\n            if timeout >= self.socket_timeout:\n                raise ValueError('reserve_job timeout must be < socket timeout')\n        if not self._watchlist:\n            raise ValueError('Select a tube or two before reserving a job')\n        with self._sock_ctx() as socket:\n            self._send_message('reserve-with-timeout {0}'.format(timeout), socket)\n            job_id, job_data = self._receive_id_and_data_with_prefix(b'RESERVED', socket)\n            return Job(job_id, job_data)", "response": "Reserves a job for this connection. Blocks for TIMEOUT secionds and raises TIMED_OUT if no job was available."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes the given job id.", "response": "def delete_job(self, job_id):\n        \"\"\"Delete the given job id. The job must have been previously reserved by this connection\"\"\"\n        if hasattr(job_id, 'job_id'):\n            job_id = job_id.job_id\n        with self._sock_ctx() as socket:\n            self._send_message('delete {0}'.format(job_id), socket)\n            self._receive_word(socket, b'DELETED')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmarks the given job_id as buried.", "response": "def bury_job(self, job_id, pri=65536):\n        \"\"\"Mark the given job_id as buried. The job must have been previously reserved by this connection\n\n        :param job_id: Job to bury\n        :param pri: Priority for the newly-buried job. If not passed, will keep its current priority\n        :type pri: int\n        \"\"\"\n        if hasattr(job_id, 'job_id'):\n            job_id = job_id.job_id\n        with self._sock_ctx() as socket:\n            self._send_message('bury {0} {1}'.format(job_id, pri), socket)\n            return self._receive_word(socket, b'BURIED')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef release_job(self, job_id, pri=65536, delay=0):\n        if hasattr(job_id, 'job_id'):\n            job_id = job_id.job_id\n        with self._sock_ctx() as socket:\n            self._send_message('release {0} {1} {2}\\r\\n'.format(job_id, pri, delay), socket)\n            return self._receive_word(socket, b'RELEASED', b'BURIED')", "response": "Release a reserved job from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nkick the given job id.", "response": "def kick_job(self, job_id):\n        \"\"\"Kick the given job id. The job must either be in the DELAYED or BURIED state and will be immediately moved to\n        the READY state.\"\"\"\n        if hasattr(job_id, 'job_id'):\n            job_id = job_id.job_id\n        with self._sock_ctx() as socket:\n            self._send_message('kick-job {0}'.format(job_id), socket)\n            self._receive_word(socket, b'KICKED')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts producing jobs into the given tube.", "response": "def use(self, tube):\n        \"\"\"Start producing jobs into the given tube.\n\n        :param tube: Name of the tube to USE\n\n        Subsequent calls to :func:`put_job` insert jobs into this tube.\n        \"\"\"\n        with self._sock_ctx() as socket:\n            if self.current_tube != tube:\n                self.desired_tube = tube\n                self._send_message('use {0}'.format(tube), socket)\n                self._receive_name(socket)\n                self.current_tube = tube"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkicks some number of jobs from the buried queue onto the ready queue.", "response": "def kick_jobs(self, num_jobs):\n        \"\"\"Kick some number of jobs from the buried queue onto the ready queue.\n\n        :param num_jobs: Number of jobs to kick\n        :type num_jobs: int\n\n        If not that many jobs are in the buried queue, it will kick as many as it can.\"\"\"\n        with self._sock_ctx() as socket:\n            self._send_message('kick {0}'.format(num_jobs), socket)\n            return self._receive_id(socket)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pause_tube(self, tube, delay=3600):\n        with self._sock_ctx() as socket:\n            delay = int(delay)\n            self._send_message('pause-tube {0} {1}'.format(tube, delay), socket)\n            return self._receive_word(socket, b'PAUSED')", "response": "Pause a tube for some number of seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unpause_tube(self, tube):\n        with self._sock_ctx() as socket:\n            self._send_message('pause-tube {0} 0'.format(tube), socket)\n            return self._receive_word(socket, b'PAUSED')", "response": "Unpause a tube which was previously paused with :func:`pause_tube()`.\n\n        .. seealso::\n\n           :func:`pause_tube()`"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interval(best,lo=np.nan,hi=np.nan):\n    return [float(best),[float(lo),float(hi)]]", "response": "Pythonized interval for easy output to yaml\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mean_interval(data, alpha=_alpha):\n    mean =np.mean(data)\n    sigma = np.std(data)\n    scale = scipy.stats.norm.ppf(1-alpha/2.)\n    return interval(mean,mean-scale*sigma,mean+scale*sigma)", "response": "Returns an interval assuming gaussian posterior."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef median_interval(data, alpha=_alpha):\n    q = [100*alpha/2., 50, 100*(1-alpha/2.)]\n    lo,med,hi = np.percentile(data,q)\n    return interval(med,lo,hi)", "response": "Returns a median interval."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kde(data, npoints=_npoints):\n    # Clipping of severe outliers to concentrate more KDE samples in the parameter range of interest\n    mad = np.median(np.fabs(np.median(data) - data))\n    cut = (data > np.median(data) - 5. * mad) & (data < np.median(data) + 5. * mad)\n    x = data[cut]\n    kde = scipy.stats.gaussian_kde(x)\n    # No penalty for using a finer sampling for KDE evaluation except computation time\n    values = np.linspace(np.min(x), np.max(x), npoints)\n    kde_values = kde.evaluate(values)\n    peak = values[np.argmax(kde_values)]\n    return values[np.argmax(kde_values)], kde.evaluate(peak)", "response": "Identify peak using Gaussian kernel density estimator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef peak_interval(data, alpha=_alpha, npoints=_npoints):\n    peak = kde_peak(data,npoints)\n    x = np.sort(data.flat); n = len(x)\n    # The number of entries in the interval\n    window = int(np.rint((1.0-alpha)*n))\n    # The start, stop, and width of all possible intervals\n    starts = x[:n-window]; ends = x[window:]\n    widths = ends - starts\n    # Just the intervals containing the peak\n    select = (peak >= starts) & (peak <= ends)\n    widths = widths[select]\n    if len(widths) == 0:\n        raise ValueError('Too few elements for interval calculation')\n    min_idx = np.argmin(widths)\n    lo = x[min_idx]\n    hi = x[min_idx+window]\n    return interval(peak,lo,hi)", "response": "Identify interval using Gaussian kernel density estimator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sky(lon=None,lat=None,size=1):\n    if lon is None:\n        umin,umax = 0,1\n    else:\n        lon = np.asarray(lon)\n        lon = np.radians(lon + 360.*(lon<0))\n        if   lon.size==1: umin=umax=lon/(2*np.pi)\n        elif lon.size==2: umin,umax=lon/(2*np.pi)\n        else: raise Exception('...')\n        \n    if lat is None:\n        vmin,vmax = -1,1\n    else:\n        lat = np.asarray(lat)\n        lat = np.radians(90 - lat)\n        if   lat.size==1: vmin=vmax=np.cos(lat)\n        elif lat.size==2: vmin,vmax=np.cos(lat)\n        else: raise Exception('...')\n\n    phi = 2*np.pi*np.random.uniform(umin,umax,size=size)\n    theta = np.arcsin(np.random.uniform(vmin,vmax,size=size))\n    return np.degrees(phi),np.degrees(theta)", "response": "Outputs uniform points on sphere from lon lat"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef supplement(self,coordsys='gal'):\n        from ugali.utils.projector import gal2cel, gal2cel_angle\n        from ugali.utils.projector import cel2gal, cel2gal_angle\n\n        coordsys = coordsys.lower()\n        kwargs = dict(usemask=False, asrecarray=True)\n        out = copy.deepcopy(self)\n\n        if ('lon' in out.names) and ('lat' in out.names):\n            # Ignore entries that are all zero\n            zeros = np.all(self.ndarray==0,axis=1)\n\n            if coordsys == 'gal':\n                ra,dec = gal2cel(out.lon,out.lat)\n                glon,glat = out.lon,out.lat\n            else:\n                ra,dec = out.lon,out.lat\n                glon,glat = cel2gal(out.lon,out.lat)\n\n            ra[zeros] = 0; dec[zeros] = 0\n            glon[zeros] = 0; glat[zeros] = 0\n\n            names = ['ra','dec','glon','glat']\n            arrs = [ra,dec,glon,glat]\n            out = mlab.rec_append_fields(out,names,arrs).view(Samples)\n            #out = recfuncs.append_fields(out,names,arrs,**kwargs).view(Samples)\n\n            if 'position_angle' in out.names:\n                if coordsys == 'gal':\n                    pa_gal = out.position_angle\n                    pa_cel = gal2cel_angle(out.lon,out.lat,out.position_angle)\n                    pa_cel = pa_cel - 180.*(pa_cel > 180.)\n                else:\n                    pa_gal = cel2gal_angle(out.lon,out.lat,out.position_angle)\n                    pa_cel = out.position_angle\n                    pa_gal = pa_gal - 180.*(pa_gal > 180.)\n                    \n                pa_gal[zeros] = 0; pa_cel[zeros] = 0\n                names = ['position_angle_gal','position_angle_cel']\n                arrs = [pa_gal,pa_cel]\n                out = recfuncs.append_fields(out,names,arrs,**kwargs).view(Samples)\n        \n        return out", "response": "Add some supplemental columns to the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mean(self, name, **kwargs):\n        return np.mean(self.get(name,**kwargs))", "response": "Mean of the distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the mean interval of a set of items.", "response": "def mean_interval(self, name, alpha=_alpha, **kwargs):\n        \"\"\"\n        Interval assuming gaussian posterior.\n        \"\"\"\n        data = self.get(name,**kwargs)\n        #return ugali.utils.stats.mean_interval(data,alpha)\n        return mean_interval(data,alpha)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef median(self, name, **kwargs):\n        data = self.get(name,**kwargs)\n        return np.percentile(data,[50])", "response": "Return the median of the data for the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the median interval of the named entry.", "response": "def median_interval(self, name, alpha=_alpha, **kwargs):\n        \"\"\"\n        Median including bayesian credible interval.\n        \"\"\"\n        data = self.get(name,**kwargs)\n        return median_interval(data,alpha)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the peak of kernel density estimator for a given set of points", "response": "def kde_peak(self, name, npoints=_npoints, **kwargs):\n        \"\"\" \n        Calculate peak of kernel density estimator\n        \"\"\"\n        data = self.get(name,**kwargs)\n        return kde_peak(data,npoints)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates kernel density estimator for a parameter set.", "response": "def kde(self, name, npoints=_npoints, **kwargs):\n        \"\"\" \n        Calculate kernel density estimator for parameter\n        \"\"\"\n        data = self.get(name,**kwargs)\n        return kde(data,npoints)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef peak_interval(self, name, alpha=_alpha, npoints=_npoints, **kwargs):\n        data = self.get(name, **kwargs)\n        return peak_interval(data,alpha,npoints)", "response": "Calculate peak interval for a given resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef min_interval(self,name, alpha=_alpha, **kwargs):\n        data = self.get(name, **kwargs)\n        return min_interval(data,alpha)", "response": "Calculates minimum interval for a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef results(self, names=None, alpha=_alpha, mode='peak', **kwargs):\n        if names is None: names = self.names\n        ret = odict()\n        for n in names:\n            ret[n] = getattr(self,'%s_interval'%mode)(n, **kwargs)\n        return ret", "response": "Calculate the results for a set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nincreases the density of points along the parabolic curve.", "response": "def densify(self, factor=10):\n        \"\"\"\n        Increase the density of points along the parabolic curve.\n        \"\"\"\n        x = []\n        y = []\n        for ii in range(0, len(self.x) - 2):\n            p = Parabola(self.x[ii: ii + 3], self.y[ii: ii + 3])\n            x.append(np.linspace(self.x[ii], self.x[ii + 1], factor)[0: -1])\n            y.append(p(x[-1]))\n\n        p = Parabola(self.x[len(self.x) - 3:], self.y[len(self.y) - 3:])\n        x.append(np.linspace(self.x[-2], self.x[-1], factor)[0: -1])\n        y.append(p(x[-1]))\n\n        x.append([self.x[-1]])\n        y.append([self.y[-1]])\n\n        #f = scipy.interpolate.interp1d(np.concatenate(x), np.concatenate(y))\n        #x = np.linspace(self.x[0], self.x[-1], len(x) * factor)   \n        #return x, f(x)\n        \n        return np.concatenate(x), np.concatenate(y)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute one - sided upperlimit via profile method.", "response": "def profileUpperLimit(self, delta = 2.71):\n        \"\"\"\n        Compute one-sided upperlimit via profile method.\n        \"\"\"\n        a = self.p_2\n        b = self.p_1\n        if self.vertex_x < 0:\n            c = self.p_0 + delta\n        else:\n            c = self.p_0 - self.vertex_y + delta\n\n        if b**2 - 4. * a * c < 0.:\n            print('WARNING')\n            print(a, b, c)\n            return 0.\n            \n        return max((np.sqrt(b**2 - 4. * a * c) - b) / (2. * a), (-1. * np.sqrt(b**2 - 4. * a * c) - b) / (2. * a))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bayesianUpperLimit(self, alpha, steps=1.e5, plot=False):\n        x_dense, y_dense = self.densify()\n        y_dense -= np.max(y_dense) # Numeric stability\n        f = scipy.interpolate.interp1d(x_dense, y_dense, kind='linear')\n        x = np.linspace(0., np.max(x_dense), steps)\n        pdf = np.exp(f(x) / 2.)\n        cut = (pdf / np.max(pdf)) > 1.e-10\n        x = x[cut]\n        pdf = pdf[cut]\n        #pdf /= pdf[0]\n        #forbidden = np.nonzero(pdf < 1.e-10)[0]\n        #if len(forbidden) > 0:\n        #    index = forbidden[0] # Numeric stability\n        #    x = x[0: index]\n        #    pdf = pdf[0: index]\n        cdf = np.cumsum(pdf)\n        cdf /= cdf[-1]\n        cdf_reflect = scipy.interpolate.interp1d(cdf, x)\n\n        return cdf_reflect(alpha)", "response": "Compute one - sided upper limit using Bayesian Method of Helene."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute one - sided upper limit using Bayesian Method of Helene.", "response": "def bayesianUpperLimit2(self, alpha, steps=1.e5, plot=False):\n        \"\"\"\n        Compute one-sided upper limit using Bayesian Method of Helene.\n        \"\"\"\n        cut = ((self.y / 2.) > -30.) # Numeric stability\n        try:\n            f = scipy.interpolate.interp1d(self.x[cut], self.y[cut], kind='cubic')\n        except:\n            f = scipy.interpolate.interp1d(self.x[cut], self.y[cut], kind='linear')\n        x = np.linspace(0., np.max(self.x[cut]), steps)\n        y = np.exp(f(x) / 2.)\n        #forbidden = np.nonzero((y / np.exp(self.vertex_y / 2.)) < 1.e-10)[0]\n        forbidden = np.nonzero((y / self.vertex_y) < 1.e-10)[0]\n        if len(forbidden) > 0:\n            index = forbidden[0] # Numeric stability\n            x = x[0: index]\n            y = y[0: index]\n        cdf = np.cumsum(y)\n        cdf /= cdf[-1]\n        cdf_reflect = scipy.interpolate.interp1d(cdf, x)\n\n        return cdf_reflect(alpha)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef confidenceInterval(self, alpha=0.6827, steps=1.e5, plot=False):\n        x_dense, y_dense = self.densify()\n        y_dense -= np.max(y_dense) # Numeric stability\n        f = scipy.interpolate.interp1d(x_dense, y_dense, kind='linear')\n        x = np.linspace(0., np.max(x_dense), steps)\n        # ADW: Why does this start at 0, which often outside the input range?\n        # Wouldn't starting at xmin be better:\n        #x = np.linspace(np.min(x_dense), np.max(x_dense), steps)\n        pdf = np.exp(f(x) / 2.)\n        cut = (pdf / np.max(pdf)) > 1.e-10\n        x = x[cut]\n        pdf = pdf[cut]\n\n        sorted_pdf_indices = np.argsort(pdf)[::-1] # Indices of PDF in descending value\n        cdf = np.cumsum(pdf[sorted_pdf_indices])\n        cdf /= cdf[-1]\n        sorted_pdf_index_max = np.argmin((cdf - alpha)**2)\n        x_select = x[sorted_pdf_indices[0: sorted_pdf_index_max]]\n            \n        return np.min(x_select), np.max(x_select)", "response": "Compute two - sided confidence interval by taking x - values corresponding to the largest PDF - values first."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts aitoff spherical coordinates to image coordinates.", "response": "def aitoffSphereToImage(lon, lat):\n    \"\"\"\n    Hammer-Aitoff projection (deg).\n    \"\"\"\n    lon = lon - 360.*(lon>180)\n    lon = np.radians(lon)\n    lat = np.radians(lat)\n\n    half_lon = lon/2.\n    cos_lat = np.cos(lat)\n     \n    gamma = (180. / np.pi) * np.sqrt(2. / (1. + (cos_lat * np.cos(half_lon))))\n    x = 2. * gamma * cos_lat * np.sin(half_lon)\n    y = gamma * np.sin(lat)\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert an image from Hammer - Aitoff to Sphere.", "response": "def aitoffImageToSphere(x, y):\n    \"\"\"\n    Inverse Hammer-Aitoff projection (deg).\n    \"\"\"\n    x = x - 360.*(x>180)\n    x = np.asarray(np.radians(x))\n    y = np.asarray(np.radians(y))\n    z = np.sqrt(1. - (x / 4.)**2 - (y / 2.)**2) # rad\n    lon = 2. * np.arctan2((2. * z**2) - 1, (z / 2.) * x)\n    lat = np.arcsin( y * z)\n    return ((180. - np.degrees(lon)) % 360.), np.degrees(lat)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gnomonicSphereToImage(lon, lat):\n    # Convert angle to [-180, 180] interval\n    lon = lon - 360.*(lon>180)\n    lon = np.radians(lon)\n    lat = np.radians(lat)\n    r_theta = (180. / np.pi) / np.tan(lat)\n    x = r_theta * np.cos(lon)\n    y = r_theta * np.sin(lon)\n    return x, y", "response": "Convert a point on the gnomonic sphere to an image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gnomonicImageToSphere(x, y):\n    # Convert angle to [-180, 180] interval\n    x = x - 360.*(x>180)\n    x = np.asarray(x)\n    y = np.asarray(y)\n    lon = np.degrees(np.arctan2(y, x))\n    r_theta = np.sqrt(x**2 + y**2)\n    lat = np.degrees(np.arctan(180. / (np.pi * r_theta)))\n    return lon, lat", "response": "Convert a Gnomonic image to a Sphere."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef angsep2(lon_1, lat_1, lon_2, lat_2):\n    import healpy\n\n    v10, v11, v12 = healpy.ang2vec(np.radians(90. - lat_1), np.radians(lon_1)).transpose()\n    v20, v21, v22 = healpy.ang2vec(np.radians(90. - lat_2), np.radians(lon_2)).transpose()\n    val = (v10 * v20) + (v11 * v21) + (v12 * v22)\n    val = np.clip(val, -1., 1.)\n    return np.degrees(np.arccos(val))", "response": "Returns the angle between two sky coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef galToCel(ll, bb):\n    bb = np.radians(bb)\n    sin_bb = np.sin(bb)\n    cos_bb = np.cos(bb)\n\n    ll = np.radians(ll)\n    ra_gp = np.radians(192.85948)\n    de_gp = np.radians(27.12825)\n    lcp = np.radians(122.932)\n\n    sin_lcp_ll = np.sin(lcp - ll)\n    cos_lcp_ll = np.cos(lcp - ll)\n\n    sin_d = (np.sin(de_gp) * sin_bb) \\\n            + (np.cos(de_gp) * cos_bb * cos_lcp_ll)\n    ramragp = np.arctan2(cos_bb * sin_lcp_ll,\n                            (np.cos(de_gp) * sin_bb) \\\n                            - (np.sin(de_gp) * cos_bb * cos_lcp_ll))\n    dec = np.arcsin(sin_d)\n    ra = (ramragp + ra_gp + (2. * np.pi)) % (2. * np.pi)\n    return np.degrees(ra), np.degrees(dec)", "response": "Converts Galactic to Celestial J2000 coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef celToGal(ra, dec):\n    dec = np.radians(dec)\n    sin_dec = np.sin(dec)\n    cos_dec = np.cos(dec)\n\n    ra = np.radians(ra)    \n    ra_gp = np.radians(192.85948)\n    de_gp = np.radians(27.12825)\n\n    sin_ra_gp = np.sin(ra - ra_gp)\n    cos_ra_gp = np.cos(ra - ra_gp)\n\n    lcp = np.radians(122.932)    \n    sin_b = (np.sin(de_gp) * sin_dec) \\\n            + (np.cos(de_gp) * cos_dec * cos_ra_gp)\n    lcpml = np.arctan2(cos_dec * sin_ra_gp,\n                          (np.cos(de_gp) * sin_dec) \\\n                          - (np.sin(de_gp) * cos_dec * cos_ra_gp))\n    bb = np.arcsin(sin_b)\n    ll = (lcp - lcpml + (2. * np.pi)) % (2. * np.pi)\n    return np.degrees(ll), np.degrees(bb)", "response": "Converts a Celestial J2000 to a Calactic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef estimate_angle(angle, origin, new_frame, offset=1e-7):\n    from astropy.coordinates import SkyCoord\n    import astropy.units as u\n    angle_deg = angle*np.pi/180\n    newlat = offset * np.cos(angle_deg) + origin.data.lat.degree\n    newlon = (offset * np.sin(angle_deg) / np.cos(newlat * np.pi/180) + origin.data.lon.degree)\n    sc = SkyCoord(newlon, newlat, unit='degree', frame=origin.frame.name)\n    new_origin = origin.transform_to(new_frame)\n    new_sc = sc.transform_to(new_frame)\n    return new_origin.position_angle(new_sc).deg", "response": "Estimate the angle of a node in a new frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert decimal degrees to hours minutes and seconds.", "response": "def dec2hms(dec):\n    \"\"\"\n    ADW: This should really be replaced by astropy\n    \"\"\"\n    DEGREE = 360.\n    HOUR = 24.\n    MINUTE = 60.\n    SECOND = 3600.\n    \n    dec = float(dec)\n    fhour = dec*(HOUR/DEGREE)\n    hour = int(fhour)\n\n    fminute = (fhour - hour)*MINUTE\n    minute = int(fminute)\n    \n    second = (fminute - minute)*MINUTE\n    return (hour, minute, second)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dec2dms(dec):\n    DEGREE = 360.\n    HOUR = 24.\n    MINUTE = 60.\n    SECOND = 3600.\n\n    dec = float(dec)\n    sign = np.copysign(1.0,dec)\n\n    fdeg = np.abs(dec)\n    deg = int(fdeg)\n    \n    fminute = (fdeg - deg)*MINUTE\n    minute = int(fminute)\n    \n    second = (fminute - minute)*MINUTE\n\n    deg = int(deg * sign)\n    return (deg, minute, second)", "response": "Convert a decimal degrees to degrees minutes and seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert longitude from hours minutes seconds in string or 3 - array format to decimal degrees.", "response": "def hms2dec(hms):\n    \"\"\"\n    Convert longitude from hours,minutes,seconds in string or 3-array\n    format to decimal degrees.\n\n    ADW: This really should be replaced by astropy\n    \"\"\"\n    DEGREE = 360.\n    HOUR = 24.\n    MINUTE = 60.\n    SECOND = 3600.\n\n    if isstring(hms):\n        hour,minute,second = np.array(re.split('[hms]',hms))[:3].astype(float)\n    else:\n        hour,minute,second = hms.T\n\n    decimal = (hour + minute * 1./MINUTE + second * 1./SECOND)*(DEGREE/HOUR)\n    return decimal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dms2dec(dms):\n    DEGREE = 360.\n    HOUR = 24.\n    MINUTE = 60.\n    SECOND = 3600.\n\n    # Be careful here, degree needs to be a float so that negative zero\n    # can have its signbit set:\n    # http://docs.scipy.org/doc/numpy-1.7.0/reference/c-api.coremath.html#NPY_NZERO\n\n    if isstring(dms):\n        degree,minute,second = np.array(re.split('[dms]',hms))[:3].astype(float)\n    else:\n        degree,minute,second = dms.T\n\n    sign = np.copysign(1.0,degree)\n    decimal = np.abs(degree) + minute * 1./MINUTE + second * 1./SECOND\n    decimal *= sign\n    return decimal", "response": "Convert latitude from degrees minutes seconds in string or 3 - array\n   format to decimal degrees."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new tree that matches the given two points.", "response": "def match(lon1, lat1, lon2, lat2, tol=None, nnearest=1):\n    \"\"\"\n    Adapted from Eric Tollerud.\n    Finds matches in one catalog to another.\n \n    Parameters\n    lon1 : array-like\n        Longitude of the first catalog (degrees)\n    lat1 : array-like\n        Latitude of the first catalog (shape of array must match `lon1`)\n    lon2 : array-like\n        Longitude of the second catalog\n    lat2 : array-like\n        Latitude of the second catalog (shape of array must match `lon2`)\n    tol : float or None, optional\n        Proximity (degrees) of a match to count as a match.  If None,\n        all nearest neighbors for the first catalog will be returned.\n    nnearest : int, optional\n        The nth neighbor to find.  E.g., 1 for the nearest nearby, 2 for the\n        second nearest neighbor, etc.  Particularly useful if you want to get\n        the nearest *non-self* neighbor of a catalog.  To do this, use:\n        ``spherematch(lon, lat, lon, lat, nnearest=2)``\n \n    Returns\n    -------\n    idx1 : int array\n        Indices into the first catalog of the matches. Will never be\n        larger than `lon1`/`lat1`.\n    idx2 : int array\n        Indices into the second catalog of the matches. Will never be\n        larger than `lon2`/`lat2`.\n    ds : float array\n        Distance (in degrees) between the matches\n    \"\"\"\n    from scipy.spatial import cKDTree\n \n    lon1 = np.asarray(lon1)\n    lat1 = np.asarray(lat1)\n    lon2 = np.asarray(lon2)\n    lat2 = np.asarray(lat2)\n \n    if lon1.shape != lat1.shape:\n        raise ValueError('lon1 and lat1 do not match!')\n    if lon2.shape != lat2.shape:\n        raise ValueError('lon2 and lat2 do not match!')\n\n    rotator = SphericalRotator(0,0)\n\n \n    # This is equivalent, but faster than just doing np.array([x1, y1, z1]).T\n    x1, y1, z1 = rotator.cartesian(lon1.ravel(),lat1.ravel())\n    coords1 = np.empty((x1.size, 3))\n    coords1[:, 0] = x1\n    coords1[:, 1] = y1\n    coords1[:, 2] = z1\n \n    x2, y2, z2 = rotator.cartesian(lon2.ravel(),lat2.ravel())\n    coords2 = np.empty((x2.size, 3))\n    coords2[:, 0] = x2\n    coords2[:, 1] = y2\n    coords2[:, 2] = z2\n \n    tree = cKDTree(coords2)\n    if nnearest == 1:\n        idxs2 = tree.query(coords1)[1]\n    elif nnearest > 1:\n        idxs2 = tree.query(coords1, nnearest)[1][:, -1]\n    else:\n        raise ValueError('invalid nnearest ' + str(nnearest))\n \n    ds = angsep(lon1, lat1, lon2[idxs2], lat2[idxs2])\n \n    idxs1 = np.arange(lon1.size)\n \n    if tol is not None:\n        msk = ds < tol\n        idxs1 = idxs1[msk]\n        idxs2 = idxs2[msk]\n        ds = ds[msk]\n \n    return idxs1, idxs2, ds"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse commandline arguments and returns a dictionary of new configuration options.", "response": "def parse(self, configManager, config):\n        \"\"\"\n        Parses commandline arguments, given a series of configuration options.\n\n        Inputs: configManager - Our parent ConfigManager instance which is constructing the Config object.\n                config        - The _Config object containing configuration options populated thus far.\n\n        Outputs: A dictionary of new configuration options to add to the Config object.\n        \"\"\"\n\n        argParser = self.getArgumentParser(configManager, config)\n        return vars(argParser.parse_args())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a text string get candidates and context for feature extraction and classification", "response": "def candidates(text):\n        \"\"\"\n        Given a `text` string, get candidates and context for feature\n        extraction and classification\n        \"\"\"\n        for Pmatch in finditer(TARGET, text):\n            # the punctuation mark itself\n            P = Pmatch.group(1)\n            # is it a boundary?\n            B = bool(match(NEWLINE, Pmatch.group(5)))\n            # L & R\n            start = Pmatch.start()\n            end = Pmatch.end()\n            Lmatch = search(LTOKEN, text[max(0, start - BUFSIZE):start])\n            if not Lmatch:  # this happens when a line begins with '.'\n                continue\n            L = word_tokenize(\" \" + Lmatch.group(1))[-1]\n            Rmatch = search(RTOKEN, text[end:end + BUFSIZE])\n            if not Rmatch:  # this happens at the end of the file, usually\n                continue\n            R = word_tokenize(Rmatch.group(1) + \" \")[0]\n            # complete observation\n            yield Observation(L, P, R, B, end)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives left context L punctuation mark P and right context R extract features.", "response": "def extract_one(self, L, P, R):\n        \"\"\"\n        Given left context `L`, punctuation mark `P`, and right context\n        R`, extract features. Probability distributions for any\n        quantile-based features will not be modified.\n        \"\"\"\n        yield \"*bias*\"\n        # L feature(s)\n        if match(QUOTE, L):\n            L = QUOTE_TOKEN\n        elif isnumberlike(L):\n            L = NUMBER_TOKEN\n        else:\n            yield \"len(L)={}\".format(min(len(L), CLIP))\n            if \".\" in L:\n                yield \"L:*period*\"\n            if not self.nocase:\n                cf = case_feature(R)\n                if cf:\n                    yield \"L:{}'\".format(cf)\n            L = L.upper()\n            if not any(char in VOWELS for char in L):\n                yield \"L:*no-vowel*\"\n        L_feat = \"L='{}'\".format(L)\n        yield L_feat\n        # P feature(s)\n        yield \"P='{}'\".format(P)\n        # R feature(s)\n        if match(QUOTE, R):\n            R = QUOTE_TOKEN\n        elif isnumberlike(R):\n            R = NUMBER_TOKEN\n        else:\n            if not self.nocase:\n                cf = case_feature(R)\n                if cf:\n                    yield \"R:{}'\".format(cf)\n            R = R.upper()\n        R_feat = \"R='{}'\".format(R)\n        yield R_feat\n        # the combined L,R feature\n        yield \"{},{}\".format(L_feat, R_feat)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(self, text, epochs=EPOCHS):\n        logger.debug(\"Extracting features and classifications.\")\n        Phi = []\n        Y = []\n        for (L, P, R, gold, _) in Detector.candidates(text):\n            Phi.append(self.extract_one(L, P, R))\n            Y.append(gold)\n        self.classifier.fit(Y, Phi, epochs)\n        logger.debug(\"Fitting complete.\")", "response": "Given a string text use it to train the segmentation classifier for epochs iterations."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an observation L P and R return True iff this observation is hypothesized to be a sentence boundary.", "response": "def predict(self, L, P, R):\n        \"\"\"\n        Given an left context `L`, punctuation mark `P`, and right context\n        `R`, return True iff this observation is hypothesized to be a\n        sentence boundary.\n        \"\"\"\n        phi = self.extract_one(L, P, R)\n        return self.classifier.predict(phi)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef segments(self, text):\n        start = 0\n        for (L, P, R, B, end) in Detector.candidates(text):\n            # if there's already a newline there, we have nothing to do\n            if B:\n                continue\n            if self.predict(L, P, R):\n                yield text[start:end].rstrip()\n                start = end\n            # otherwise, there's probably not a sentence boundary here\n        yield text[start:].rstrip()", "response": "Given a string of text return a generator yielding each hypothesized sentence string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a string of text compute the confusion matrix for the classification task.", "response": "def evaluate(self, text):\n        \"\"\"\n        Given a string of `text`, compute confusion matrix for the\n        classification task.\n        \"\"\"\n        cx = BinaryConfusion()\n        for (L, P, R, gold, _) in Detector.candidates(text):\n            guess = self.predict(L, P, R)\n            cx.update(gold, guess)\n            if not gold and guess:\n                logger.debug(\"False pos.: L='{}', R='{}'.\".format(L, R))\n            elif gold and not guess:\n                logger.debug(\"False neg.: L='{}', R='{}'.\".format(L, R))\n        return cx"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readMangleFile(infile, lon, lat, index = None):\n    msg = \"'mask.readMangleFile': ADW 2018-05-05\"\n    DeprecationWarning(msg)\n\n    if index is None:\n        index = np.random.randint(0, 1.e10)\n    \n    coordinate_file = 'temp_coordinate_%010i.dat'%(index)\n    maglim_file = 'temp_maglim_%010i.dat'%(index)\n\n    writer = open(coordinate_file, 'w')\n    for ii in range(0, len(lon)):\n        writer.write('%12.5f%12.5f\\n'%(lon[ii], lat[ii]))\n    writer.close()\n\n    os.system('polyid -W %s %s %s || exit'%(infile,\n                                            coordinate_file,\n                                            maglim_file))\n\n    reader = open(maglim_file)\n    lines = reader.readlines()\n    reader.close()\n\n    os.remove(maglim_file)\n    os.remove(coordinate_file)\n\n    maglim = []\n    for ii in range(1, len(lines)):\n        if len(lines[ii].split()) == 3:\n            maglim.append(float(lines[ii].split()[2]))\n        elif len(lines[ii].split()) == 2:\n            maglim.append(0.) # Coordinates outside of the MANGLE ploygon\n        elif len(lines[ii].split()) > 3:\n            msg = 'Coordinate inside multiple polygons, masking that coordinate.'\n            logger.warning(msg)\n            maglim.append(0.)\n        else:\n            msg = 'Cannot parse maglim file, unexpected number of columns.'\n            logger.error(msg)\n            break\n            \n    maglim = np.array(maglim)\n    return maglim", "response": "Reads a Mangle file and returns a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scale(mask, mag_scale, outfile=None):\n    msg = \"'mask.scale': ADW 2018-05-05\"\n    DeprecationWarning(msg)\n    mask_new = hp.UNSEEN * np.ones(len(mask))\n    mask_new[mask == 0.] = 0.\n    mask_new[mask > 0.] = mask[mask > 0.] + mag_scale\n\n    if outfile is not None:\n        pix = np.nonzero(mask_new > 0.)[0]\n        data_dict = {'MAGLIM': mask_new[pix]}\n        nside = hp.npix2nside(len(mask_new))\n        ugali.utils.skymap.writeSparseHealpixMap(pix, data_dict, nside, outfile)\n\n    return mask_new", "response": "Scale the completeness depth of a HEALPix map."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassemble a set of unique magnitude tuples for the ROI", "response": "def mask_roi_unique(self):\n        \"\"\"\n        Assemble a set of unique magnitude tuples for the ROI\n        \"\"\"\n        # There is no good inherent way in numpy to do this...\n        # http://stackoverflow.com/q/16970982/\n\n        # Also possible and simple:\n        #return np.unique(zip(self.mask_1.mask_roi_sparse,self.mask_2.mask_roi_sparse))\n\n        A = np.vstack([self.mask_1.mask_roi_sparse,self.mask_2.mask_roi_sparse]).T\n        B = A[np.lexsort(A.T[::-1])]\n        return B[np.concatenate(([True],np.any(B[1:]!=B[:-1],axis=1)))]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the index of the unique magnitude tuple for each pixel in the ROI.", "response": "def mask_roi_digi(self):\n        \"\"\"\n        Get the index of the unique magnitude tuple for each pixel in the ROI.\n        \"\"\"\n        # http://stackoverflow.com/q/24205045/#24206440\n        A = np.vstack([self.mask_1.mask_roi_sparse,self.mask_2.mask_roi_sparse]).T\n        B = self.mask_roi_unique\n\n        AA = np.ascontiguousarray(A)\n        BB = np.ascontiguousarray(B)\n         \n        dt = np.dtype((np.void, AA.dtype.itemsize * AA.shape[1]))\n        a = AA.view(dt).ravel()\n        b = BB.view(dt).ravel()\n         \n        idx = np.argsort(b)\n        indices = np.searchsorted(b[idx],a)\n        return idx[indices]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating an approximate pixel coverage fraction from the two masks.", "response": "def _fracRoiSparse(self):\n        \"\"\"\n        Calculate an approximate pixel coverage fraction from the two masks.\n\n        We have no way to know a priori how much the coverage of the\n        two masks overlap in a give pixel. For example, masks that each\n        have frac = 0.5 could have a combined frac = [0.0 to 0.5]. \n        The limits will be: \n          max:  min(frac1,frac2)\n          min:  max((frac1+frac2)-1, 0.0)\n\n        Sometimes we are lucky and our fracdet is actually already\n        calculated for the two masks combined, so that the max\n        condition is satisfied. That is what we will assume...\n        \"\"\"\n        self.frac_roi_sparse = np.min([self.mask_1.frac_roi_sparse,self.mask_2.frac_roi_sparse],axis=0)\n        return self.frac_roi_sparse"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _solidAngleMMD(self):\n        # Take upper corner of the magnitude bin\n        mag_2,mag_1 = np.meshgrid(self.roi.bins_mag[1:],self.roi.bins_mag[1:])\n\n        # Havent tested since adding fracdet\n        unmasked_mag_1 = (self.mask_1.mask_annulus_sparse[:,np.newaxis]>mag_1[:,np.newaxis])\n        unmasked_mag_2 = (self.mask_2.mask_annulus_sparse[:,np.newaxis]>mag_2[:,np.newaxis])\n        n_unmasked_pixels = (unmasked_mag_1*unmasked_mag_2*self.frac_annulus_sparse).sum(axis=1)\n\n        self.solid_angle_mmd = self.roi.area_pixel * n_unmasked_pixels\n\n        if self.solid_angle_mmd.sum() == 0:\n            msg = \"Mask annulus contains no solid angle.\"\n            logger.error(msg)\n            raise Exception(msg)", "response": "Compute solid angle within the mask annulus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _pruneMMD(self, minimum_solid_angle):\n\n        logger.info('Pruning mask based on minimum solid angle of %.2f deg^2'%(minimum_solid_angle))\n\n        solid_angle_mmd = self.solid_angle_mmd*(self.solid_angle_mmd > minimum_solid_angle)\n        if solid_angle_mmd.sum() == 0:\n            msg = \"Pruned mask contains no solid angle.\"\n            logger.error(msg)\n            raise Exception(msg)\n\n        self.solid_angle_mmd = solid_angle_mmd\n\n        # Compute which magnitudes the clipping correspond to\n        index_mag_1, index_mag_2 = np.nonzero(self.solid_angle_mmd)\n        self.mag_1_clip = self.roi.bins_mag[1:][np.max(index_mag_1)]\n        self.mag_2_clip = self.roi.bins_mag[1:][np.max(index_mag_2)]\n\n        logger.info('Clipping mask 1 at %.2f mag'%(self.mag_1_clip) )\n        logger.info('Clipping mask 2 at %.2f mag'%(self.mag_2_clip) )\n        self.mask_1.mask_roi_sparse = np.clip(self.mask_1.mask_roi_sparse, 0., self.mag_1_clip)\n        self.mask_2.mask_roi_sparse = np.clip(self.mask_2.mask_roi_sparse, 0., self.mag_2_clip)", "response": "Remove regions of magnitude - magnitude space where the unmasked solid angle isometric insufficient to estimate the background."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute solid angle within the mask annulus.", "response": "def _solidAngleCMD(self):\n        \"\"\"\n        Compute solid angle within the mask annulus (deg^2) as a\n        function of color and magnitude.\n        \"\"\"\n\n        self.solid_angle_cmd = np.zeros([len(self.roi.centers_mag),\n                                            len(self.roi.centers_color)])\n\n        for index_mag in np.arange(len(self.roi.centers_mag)):\n            for index_color in np.arange(len(self.roi.centers_color)):\n                # mag and color at bin center\n                mag = self.roi.centers_mag[index_mag]\n                color = self.roi.centers_color[index_color]\n\n                if self.config.params['catalog']['band_1_detection']:\n                    # Evaluating at the center of the color-mag bin, be consistent!\n                    #mag_1 = self.roi.centers_mag[index_mag]\n                    #color = self.roi.centers_color[index_color]\n                    #mag_2 = mag_1 - color\n                    # Evaluating at corner of the color-mag bin, be consistent!\n                    mag_1 = mag + (0.5 * self.roi.delta_mag)\n                    mag_2 = mag - color + (0.5 * self.roi.delta_color)\n                else:\n                    # Evaluating at the center of the color-mag bin, be consistent!\n                    #mag_2 = self.roi.centers_mag[index_mag]\n                    #color = self.roi.centers_color[index_color]\n                    #mag_1 = mag_2 + color\n                    # Evaluating at corner of the color-mag bin, be consistent!\n                    mag_1 = mag + color + (0.5 * self.roi.delta_color)\n                    mag_2 = mag + (0.5 * self.roi.delta_mag)\n\n                # ADW: Is there a problem here?\n                #self.solid_angle_cmd[index_mag, index_color] = self.roi.area_pixel * np.sum((self.mask_1.mask > mag_1) * (self.mask_2.mask > mag_2))\n\n                # ADW: I think we want to keep pixels that are >= mag\n                unmasked_mag_1 = (self.mask_1.mask_annulus_sparse >= mag_1)\n                unmasked_mag_2 = (self.mask_2.mask_annulus_sparse >= mag_2)\n                n_unmasked_pixels = np.sum(unmasked_mag_1*unmasked_mag_2*self.frac_annulus_sparse)\n\n                #n_unmasked_pixels = np.sum((self.mask_1.mask_annulus_sparse > mag_1) \\\n                #                               * (self.mask_2.mask_annulus_sparse > mag_2))\n\n                self.solid_angle_cmd[index_mag, index_color] = self.roi.area_pixel * n_unmasked_pixels\n        if self.solid_angle_cmd.sum() == 0:\n            msg = \"Mask annulus contains no solid angle.\"\n            logger.error(msg)\n            raise Exception(msg)\n\n        return self.solid_angle_cmd"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pruneCMD(self, minimum_solid_angle):\n\n        logger.info('Pruning mask based on minimum solid angle of %.2f deg^2'%(minimum_solid_angle))\n        self.solid_angle_cmd *= self.solid_angle_cmd > minimum_solid_angle\n\n        if self.solid_angle_cmd.sum() == 0:\n            msg = \"Pruned mask contains no solid angle.\"\n            logger.error(msg)\n            raise Exception(msg)\n\n        # Compute which magnitudes the clipping correspond to\n        index_mag, index_color = np.nonzero(self.solid_angle_cmd)\n        mag = self.roi.centers_mag[index_mag]\n        color = self.roi.centers_color[index_color]\n        if self.config.params['catalog']['band_1_detection']:\n            mag_1 = mag\n            mag_2 = mag_1 - color\n            self.mag_1_clip = np.max(mag_1) + (0.5 * self.roi.delta_mag)\n            self.mag_2_clip = np.max(mag_2) + (0.5 * self.roi.delta_color)\n        else:\n            mag_2 = mag\n            mag_1 = color + mag_2\n            self.mag_1_clip = np.max(mag_1) + (0.5 * self.roi.delta_color)\n            self.mag_2_clip = np.max(mag_2) + (0.5 * self.roi.delta_mag)\n\n        logger.info('Clipping mask 1 at %.2f mag'%(self.mag_1_clip) )\n        logger.info('Clipping mask 2 at %.2f mag'%(self.mag_2_clip) )\n        self.mask_1.mask_roi_sparse = np.clip(self.mask_1.mask_roi_sparse, 0., self.mag_1_clip)\n        self.mask_2.mask_roi_sparse = np.clip(self.mask_2.mask_roi_sparse, 0., self.mag_2_clip)", "response": "Remove regions of color - magnitude space where the unmasked solid angle is not insufficient to estimate the background."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the completeness as a function of magnitude.", "response": "def completeness(self, delta, method='step'):\n        \"\"\"\n        Return the completeness as a function of magnitude.\n\n        ADW: Eventually want a completeness mask to set overall efficiency.\n        \"\"\"\n        delta = np.asarray(delta)\n        if method == 'step':\n            func = lambda delta: (delta > 0).astype(float)\n        elif method == 'erf':\n            # Trust the SDSS EDR???\n            # 95% completeness: \n            def func(delta):\n                # Efficiency at bright end (assumed to be 100%)\n                e = 1.0\n                # EDR says full width is ~0.5 mag\n                width = 0.2 \n                # This should be the halfway point in the curve\n                return (e/2.0)*(1/np.sqrt(2*width))*(np.sqrt(2*width)-scipy.special.erf(-delta))\n        elif method == 'flemming':\n            # Functional form taken from Fleming et al. AJ 109, 1044 (1995)\n            # http://adsabs.harvard.edu/abs/1995AJ....109.1044F\n            # f = 1/2 [1 - alpha(V - Vlim)/sqrt(1 + alpha^2 (V - Vlim)^2)]\n            # CAREFUL: This definition is for Vlim = 50% completeness\n            def func(delta):\n                alpha = 2.0\n                return 0.5 * (1 - (alpha * delta)/np.sqrt(1+alpha**2 * delta**2))\n        else:\n            raise Exception('...')\n        return func(delta)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the photometric errors for a given catalog object.", "response": "def _photometricErrors(self, catalog=None, n_per_bin=100):\n        \"\"\"\n        Realistic photometric errors estimated from catalog objects and mask.\n        Extend below the magnitude threshold with a flat extrapolation.\n        \"\"\"\n\n        if catalog is None:\n            # Simple proxy for photometric errors\n            release = self.config['data']['release']\n            band_1 = self.config['catalog'].get('mag_1_band')\n            if not band_1: band_1 = self.config['isochrone']['mag_1_field']\n            band_2 = self.config['catalog'].get('mag_2_band')\n            if not band_2: band_2 = self.config['isochrone']['mag_2_field']\n            \n            DELMIN = 0.0\n            pars_1 = MAGERR_PARAMS[release][band_1]\n            \n            def photo_err_1(delta):\n                p = pars_1\n                return np.clip(np.exp(p[0]*delta+p[1])+p[2], 0, np.exp(p[0]*(DELMIN)+p[1])+p[2])\n\n            pars_2 = MAGERR_PARAMS[release][band_2]\n            def photo_err_2(delta):\n                p = pars_2\n                return np.clip(np.exp(p[0]*delta+p[1])+p[2], 0, np.exp(p[0]*(DELMIN)+p[1])+p[2])\n\n        else:\n            catalog.spatialBin(self.roi)\n\n            if len(catalog.mag_1) < n_per_bin:\n                logger.warning(\"Catalog contains fewer objects than requested to calculate errors.\")\n                #n_per_bin = int(len(catalog.mag_1) / 3)\n                return self._photometricErrors(catalog=None)\n             \n            # Band 1\n            mag_1_thresh = self.mask_1.mask_roi_sparse[catalog.pixel_roi_index] - catalog.mag_1\n            sorting_indices = np.argsort(mag_1_thresh)\n            mag_1_thresh_sort = mag_1_thresh[sorting_indices]\n            mag_err_1_sort = catalog.mag_err_1[sorting_indices]\n             \n            # ADW: Can't this be done with np.median(axis=?)\n            mag_1_thresh_medians = []\n            mag_err_1_medians = []\n            for i in range(0, int(len(mag_1_thresh) / float(n_per_bin))):\n                mag_1_thresh_medians.append(np.median(mag_1_thresh_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n                mag_err_1_medians.append(np.median(mag_err_1_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n             \n            if mag_1_thresh_medians[0] > 0.:\n                mag_1_thresh_medians = np.insert(mag_1_thresh_medians, 0, -99.)\n                mag_err_1_medians = np.insert(mag_err_1_medians, 0, mag_err_1_medians[0])\n             \n            photo_err_1 = scipy.interpolate.interp1d(mag_1_thresh_medians, mag_err_1_medians,\n                                                     bounds_error=False, fill_value=mag_err_1_medians[-1])\n             \n            # Band 2\n            mag_2_thresh = self.mask_2.mask_roi_sparse[catalog.pixel_roi_index] - catalog.mag_2\n            sorting_indices = np.argsort(mag_2_thresh)\n            mag_2_thresh_sort = mag_2_thresh[sorting_indices]\n            mag_err_2_sort = catalog.mag_err_2[sorting_indices]\n             \n            mag_2_thresh_medians = []\n            mag_err_2_medians = []\n            for i in range(0, int(len(mag_2_thresh) / float(n_per_bin))):\n                mag_2_thresh_medians.append(np.median(mag_2_thresh_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n                mag_err_2_medians.append(np.median(mag_err_2_sort[n_per_bin * i: n_per_bin * (i + 1)]))\n             \n            if mag_2_thresh_medians[0] > 0.:\n                mag_2_thresh_medians = np.insert(mag_2_thresh_medians, 0, -99.)\n                mag_err_2_medians = np.insert(mag_err_2_medians, 0, mag_err_2_medians[0])\n             \n            photo_err_2 = scipy.interpolate.interp1d(mag_2_thresh_medians, mag_err_2_medians,\n                                                     bounds_error=False, fill_value=mag_err_2_medians[-1])\n             \n        self.photo_err_1=photo_err_1\n        self.photo_err_2=photo_err_2\n\n        return self.photo_err_1, self.photo_err_2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plotSolidAngleCMD(self):\n        msg = \"'%s.plotSolidAngleCMD': ADW 2018-05-05\"%self.__class__.__name__\n        DeprecationWarning(msg)\n\n        import ugali.utils.plotting        \n        ugali.utils.plotting.twoDimensionalHistogram('mask', 'color', 'magnitude',\n                                                     self.solid_angle_cmd,\n                                                     self.roi.bins_color,\n                                                     self.roi.bins_mag,\n                                                     lim_x = [self.roi.bins_color[0],\n                                                              self.roi.bins_color[-1]],\n                                                     lim_y = [self.roi.bins_mag[-1],\n                                                              self.roi.bins_mag[0]])", "response": "Function to plot solid angle within the mask."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plotSolidAngleMMD(self):\n        msg = \"'%s.plotSolidAngleMMD': ADW 2018-05-05\"%self.__class__.__name__\n        DeprecationWarning(msg)\n\n        import ugali.utils.plotting        \n\n        ugali.utils.plotting.twoDimensionalHistogram('mask', 'mag_2', 'mag_1',\n                                                     self.solid_angle_mmd,\n                                                     self.roi.bins_mag,\n                                                     self.roi.bins_mag,\n                                                     lim_x = [self.roi.bins_mag[0],\n                                                              self.roi.bins_mag[-1]],\n                                                     lim_y = [self.roi.bins_mag[-1],\n                                                              self.roi.bins_mag[0]])", "response": "Plots the solid angle within the mask as a function of color and magnitude."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating an empirical background model in magnitude - magnitude space.", "response": "def backgroundMMD(self, catalog, method='cloud-in-cells', weights=None):\n        \"\"\"\n        Generate an empirical background model in magnitude-magnitude space.\n        \n        INPUTS:\n            catalog: Catalog object\n        OUTPUTS:\n            background\n        \"\"\"\n\n        # Select objects in annulus\n        cut_annulus = self.roi.inAnnulus(catalog.lon,catalog.lat)\n        mag_1 = catalog.mag_1[cut_annulus]\n        mag_2 = catalog.mag_2[cut_annulus]\n\n        # Units are (deg^2)\n        solid_angle = ugali.utils.binning.take2D(self.solid_angle_mmd, mag_2, mag_1,\n                                                 self.roi.bins_mag, self.roi.bins_mag)\n\n        # Weight each object before binning\n        # Divide by solid angle and bin size in magnitudes to get number density \n        # [objs / deg^2 / mag^2]\n        if weights is None:\n            number_density = (solid_angle*self.roi.delta_mag**2)**(-1)\n        else:\n            number_density = weights*(solid_angle*self.roi.delta_mag**2)**(-1)\n\n        method = str(method).lower()\n        if method == 'cloud-in-cells':\n            # Apply cloud-in-cells algorithm\n            mmd_background = ugali.utils.binning.cloudInCells(mag_2,mag_1,\n                                                              [self.roi.bins_mag,self.roi.bins_mag],\n                                                              weights=number_density)[0]\n        elif method == 'bootstrap':\n            # Not implemented\n            raise ValueError(\"Bootstrap method not implemented\")\n            mag_1 + (mag_1_err * np.random.normal(0, 1., len(mag_1)))\n            mag_2 + (mag_2_err * np.random.normal(0, 1., len(mag_2)))\n\n        elif method == 'histogram':\n            # Apply raw histogram\n            mmd_background = np.histogram2d(mag_1,mag_2,bins=[self.roi.bins_mag,self.roi.bins_mag],\n                                            weights=number_density)[0]\n\n        elif method == 'kde':\n            # Gridded kernel density estimator\n            logger.warning(\"### KDE not implemented properly\")\n            mmd_background = ugali.utils.binning.kernelDensity(mag_2,mag_1,\n                                                               [self.roi.bins_mag,self.roi.bins_mag],\n                                                               weights=number_density)[0]\n        elif method == 'uniform':\n            logger.warning(\"### WARNING: Uniform CMD\")\n            hist = np.histogram2d(mag_1,mag_2,bins=[self.roi.bins_mag,self.roi.bins_mag], \n                                  weights=number_density)[0]\n            mmd_background = np.mean(hist)*np.ones(hist.shape)\n            observable = (self.solid_angle_mmd > self.minimum_solid_angle)\n            mmd_background *= observable\n            return mmd_background\n        else:\n            raise ValueError(\"Unrecognized method: %s\"%method)\n        ## Account for the objects that spill out of the observable space\n        ## But what about the objects that spill out to red colors??\n        #for index_color in range(0, len(self.roi.centers_color)):\n        #    for index_mag in range(0, len(self.roi.centers_mag)):\n        #        if self.solid_angle_cmd[index_mag][index_color] < self.minimum_solid_angle:\n        #            cmd_background[index_mag - 1][index_color] += cmd_background[index_mag][index_color]\n        #            cmd_background[index_mag][index_color] = 0.\n        #            break\n\n        mmd_area = self.solid_angle_mmd*self.roi.delta_mag**2 # [deg^2 * mag^2]\n\n        # ADW: This accounts for leakage to faint magnitudes\n        # But what about the objects that spill out to red colors??\n        # Maximum obsevable magnitude index for each color (uses the fact that\n        # np.argmin returns first minimum (zero) instance found.\n        # NOTE: More complicated maps may have holes causing problems\n\n        observable = (self.solid_angle_mmd > self.minimum_solid_angle)\n        index_mag_1 = observable.argmin(axis=0) - 1\n        index_mag_2 = np.arange(len(self.roi.centers_mag))\n        # Add the cumulative leakage back into the last bin of the CMD\n        leakage = (mmd_background * ~observable).sum(axis=0)\n        ### mmd_background[[index_mag_1,index_mag_2]] += leakage\n        # Zero out all non-observable bins\n        ### mmd_background *= observable\n\n        # Avoid dividing by zero by setting empty bins to the value of the \n        # minimum filled bin of the CMD. This choice is arbitrary and \n        # could be replaced by a static minimum, some fraction of the \n        # CMD maximum, some median clipped minimum, etc. However, should \n        # be robust against outliers with very small values.\n        min_mmd_background = max(mmd_background[mmd_background > 0.].min(),\n                                 1e-4*mmd_background.max())\n        mmd_background[observable] = mmd_background[observable].clip(min_mmd_background)\n\n        ### # ADW: This is a fudge factor introduced to renormalize the CMD\n        ### # to the number of input stars in the annulus. While leakage\n        ### # will still smooth the distribution, it shouldn't result in \n        ### fudge_factor = len(mag) / float((cmd_background*cmd_area).sum())\n        ### cmd_background *= fudge_factor\n\n        return mmd_background"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef backgroundCMD(self, catalog, mode='cloud-in-cells', weights=None):\n\n        # Select objects in annulus\n        cut_annulus = self.roi.inAnnulus(catalog.lon,catalog.lat)\n        color = catalog.color[cut_annulus]\n        mag   = catalog.mag[cut_annulus]\n\n        # Units are (deg^2)\n        solid_angle = ugali.utils.binning.take2D(self.solid_angle_cmd, color, mag,\n                                                 self.roi.bins_color, self.roi.bins_mag)\n\n        # Weight each object before binning\n        # Divide by solid angle and bin size in magnitudes to get number density \n        # [objs / deg^2 / mag^2]\n        if weights is None:\n            number_density = (solid_angle*self.roi.delta_color*self.roi.delta_mag)**(-1)\n        else:\n            number_density = weights*(solid_angle*self.roi.delta_color*self.roi.delta_mag)**(-1)\n\n        mode = str(mode).lower()\n        if mode == 'cloud-in-cells':\n            # Apply cloud-in-cells algorithm\n            cmd_background = ugali.utils.binning.cloudInCells(color,mag,\n                                                              [self.roi.bins_color,self.roi.bins_mag],\n                                                              weights=number_density)[0]\n        elif mode == 'bootstrap':\n            # Not implemented\n            raise ValueError(\"Bootstrap mode not implemented\")\n            mag_1_array = catalog.mag_1\n            mag_2_array = catalog.mag_2\n\n            catalog.mag_1 + (catalog.mag_1_err * np.random.normal(0, 1., len(catalog.mag_1)))\n            catalog.mag_2 + (catalog.mag_2_err * np.random.normal(0, 1., len(catalog.mag_2)))\n\n        elif mode == 'histogram':\n            # Apply raw histogram\n            cmd_background = np.histogram2d(mag,color,bins=[self.roi.bins_mag,self.roi.bins_color],\n                                            weights=number_density)[0]\n\n        elif mode == 'kde':\n            # Gridded kernel density estimator\n            logger.warning(\"### KDE not implemented properly\")\n            cmd_background = ugali.utils.binning.kernelDensity(color,mag,\n                                                               [self.roi.bins_color,self.roi.bins_mag],\n                                                               weights=number_density)[0]\n        elif mode == 'uniform':\n            logger.warning(\"### WARNING: Uniform CMD\")\n            hist = np.histogram2d(mag,color,bins=[self.roi.bins_mag,self.roi.bins_color], weights=number_density)[0]\n            cmd_background = np.mean(hist)*np.ones(hist.shape)\n            observable = (self.solid_angle_cmd > self.minimum_solid_angle)\n            cmd_background *= observable\n            return cmd_background\n        else:\n            raise ValueError(\"Unrecognized mode: %s\"%mode)\n        ## Account for the objects that spill out of the observable space\n        ## But what about the objects that spill out to red colors??\n        #for index_color in range(0, len(self.roi.centers_color)):\n        #    for index_mag in range(0, len(self.roi.centers_mag)):\n        #        if self.solid_angle_cmd[index_mag][index_color] < self.minimum_solid_angle:\n        #            cmd_background[index_mag - 1][index_color] += cmd_background[index_mag][index_color]\n        #            cmd_background[index_mag][index_color] = 0.\n        #            break\n\n        cmd_area = self.solid_angle_cmd*self.roi.delta_color*self.roi.delta_mag # [deg^2 * mag^2]\n\n        # ADW: This accounts for leakage to faint magnitudes\n        # But what about the objects that spill out to red colors??\n        # Maximum obsevable magnitude index for each color (uses the fact that\n        # np.argmin returns first minimum (zero) instance found.\n        # NOTE: More complicated maps may have holes causing problems\n\n        observable = (self.solid_angle_cmd > self.minimum_solid_angle)\n        index_mag = observable.argmin(axis=0) - 1\n        index_color = np.arange(len(self.roi.centers_color))\n        # Add the cumulative leakage back into the last bin of the CMD\n        leakage = (cmd_background * ~observable).sum(axis=0)\n        cmd_background[[index_mag,index_color]] += leakage\n        # Zero out all non-observable bins\n        cmd_background *= observable\n\n        # Avoid dividing by zero by setting empty bins to the value of the \n        # minimum filled bin of the CMD. This choice is arbitrary and \n        # could be replaced by a static minimum, some fraction of the \n        # CMD maximum, some median clipped minimum, etc. However, should \n        # be robust against outliers with very small values.\n        min_cmd_background = max(cmd_background[cmd_background > 0.].min(),\n                                 1e-4*cmd_background.max())\n        cmd_background[observable] = cmd_background[observable].clip(min_cmd_background)\n\n        ### # ADW: This is a fudge factor introduced to renormalize the CMD\n        ### # to the number of input stars in the annulus. While leakage\n        ### # will still smooth the distribution, it shouldn't result in \n        ### fudge_factor = len(mag) / float((cmd_background*cmd_area).sum())\n        ### cmd_background *= fudge_factor\n\n        return cmd_background", "response": "Generates an empirical background model in color - magnitude space."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretain only the catalog objects which fall within the observable (i.e., unmasked) space. Parameters: catalog: a Catalog object Returns: sel : boolean selection array where True means the object would be observable (i.e., unmasked). ADW: Careful, this function is fragile! The selection here should be the same as isochrone.observableFraction space. However, for technical reasons it is faster to do the calculation with broadcasting there.", "response": "def restrictCatalogToObservableSpaceMMD(self, catalog):\n        \"\"\"\n        Retain only the catalog objects which fall within the observable (i.e., unmasked) space.\n\n        Parameters:\n        catalog: a Catalog object\n        Returns:\n        sel    : boolean selection array where True means the object would be observable (i.e., unmasked).\n\n        ADW: Careful, this function is fragile! The selection here should\n             be the same as isochrone.observableFraction space. However,\n             for technical reasons it is faster to do the calculation with\n             broadcasting there.\n        \"\"\"\n\n        # ADW: This creates a slope in color-magnitude space near the magnitude limit\n        # i.e., if color=g-r then you can't have an object with g-r=1 and mag_r > mask_r-1\n        # Depending on which is the detection band, this slope will appear at blue\n        # or red colors. When it occurs at blue colors, it effects very few objects.\n        # However, when occuring for red objects it can cut many objects. It is \n        # unclear that this is being correctly accounted for in the likelihood\n\n        catalog.spatialBin(self.roi)\n        sel_roi = (catalog.pixel_roi_index >= 0) # Objects outside ROI have pixel_roi_index of -1\n        sel_mag_1 = catalog.mag_1 < self.mask_1.mask_roi_sparse[catalog.pixel_roi_index]\n        sel_mag_2 = catalog.mag_2 < self.mask_2.mask_roi_sparse[catalog.pixel_roi_index]\n\n        # and are located in the region of mag-mag space where background can be estimated\n        sel_mmd = ugali.utils.binning.take2D(self.solid_angle_mmd,\n                                             catalog.mag_2, catalog.mag_1,\n                                             self.roi.bins_mag, self.roi.bins_mag) > 0.\n\n        sel = np.all([sel_roi,sel_mag_1,sel_mag_2,sel_mmd], axis=0)\n        return sel"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretain only the catalog objects which fall within the observable (i.e., unmasked) space. NOTE: This returns a *selection* (i.e., objects are retained if the value of the output array is True). Parameters: catalog: a Catalog object Returns: sel : boolean selection array where True means the object would be observable (i.e., unmasked). ADW: Careful, this function is fragile! The selection here should be the same as isochrone.observableFraction space. However, for technical reasons it is faster to do the calculation with broadcasting there.", "response": "def restrictCatalogToObservableSpaceCMD(self, catalog):\n        \"\"\"\n        Retain only the catalog objects which fall within the\n        observable (i.e., unmasked) space.  NOTE: This returns a\n        *selection* (i.e., objects are retained if the value of the\n        output array is True).\n\n        Parameters:\n        catalog: a Catalog object\n        Returns:\n        sel    : boolean selection array where True means the object would be observable (i.e., unmasked).\n\n        ADW: Careful, this function is fragile! The selection here should\n             be the same as isochrone.observableFraction space. However,\n             for technical reasons it is faster to do the calculation with\n             broadcasting there.\n        \"\"\"\n\n        # ADW: This creates a slope in color-magnitude space near the magnitude limit\n        # i.e., if color=g-r then you can't have an object with g-r=1 and mag_r > mask_r-1\n        # Depending on which is the detection band, this slope will appear at blue\n        # or red colors. When it occurs at blue colors, it effects very few objects.\n        # However, when occuring for red objects it can cut many objects. It is \n        # unclear that this is being correctly accounted for in the likelihood\n\n        ### # Check that the objects fall in the color-magnitude space of the ROI\n        ### # ADW: I think this is degenerate with the cut_cmd\n        ### sel_mag = np.logical_and(catalog.mag > self.roi.bins_mag[0],\n        ###                             catalog.mag < self.roi.bins_mag[-1])\n        ### sel_color = np.logical_and(catalog.color > self.roi.bins_color[0],\n        ###                               catalog.color < self.roi.bins_color[-1])\n\n        # and are observable in the ROI-specific mask for both bands\n        #if not hasattr(catalog, 'pixel_roi_index'): # TODO: An attempt to save computations, but not robust\n        #    catalog.spatialBin(self.roi)\n        catalog.spatialBin(self.roi)\n        sel_roi = (catalog.pixel_roi_index >= 0) # Objects outside ROI have pixel_roi_index of -1\n        sel_mag_1 = catalog.mag_1 < self.mask_1.mask_roi_sparse[catalog.pixel_roi_index]\n        sel_mag_2 = catalog.mag_2 < self.mask_2.mask_roi_sparse[catalog.pixel_roi_index]\n\n        # and are located in the region of color-magnitude space where background can be estimated\n        sel_cmd = ugali.utils.binning.take2D(self.solid_angle_cmd,\n                                             catalog.color, catalog.mag,\n                                             self.roi.bins_color, self.roi.bins_mag) > 0.\n\n        sel = np.all([sel_roi,sel_mag_1,sel_mag_2,sel_cmd], axis=0)\n        return sel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef completeness(self, mags, method='step'):\n        if method == 'step':\n            func = lambda x: (x < self.mask_roi_unique[:,np.newaxis]).astype(float)\n        elif method == 'erf':\n            # Trust the ERD???\n            # 95% completeness: \n            def func(x):\n                # Efficiency at bright end (assumed to be 100%)\n                e = 1.0\n                # SDSS EDR says full width is ~0.5 mag\n                width = 0.2 \n                # This should be the halfway point in the curve\n                maglim = self.mask_roi_unique[:,np.newaxis]\n                return (e/2.0)*(1/np.sqrt(2*width))*(np.sqrt(2*width)-scipy.special.erf((x-maglim)))\n        else:\n            raise Exception('...')\n\n        return func(mags)", "response": "Return the completeness as a function of magnitude."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the magnitude depth.", "response": "def plot(self):\n        \"\"\"\n        Plot the magnitude depth.\n        \"\"\"\n        msg = \"'%s.plot': ADW 2018-05-05\"%self.__class__.__name__\n        DeprecationWarning(msg)\n\n        import ugali.utils.plotting\n\n        mask = hp.UNSEEN * np.ones(hp.nside2npix(self.nside))\n        mask[self.roi.pixels] = self.mask_roi_sparse\n        mask[mask == 0.] = hp.UNSEEN\n        ugali.utils.plotting.zoomedHealpixMap('Completeness Depth',\n                                              mask,\n                                              self.roi.lon, self.roi.lat,\n                                              self.roi.config.params['coords']['roi_radius'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_expected(dynamizer, expected):\n    ret = {}\n    for k, v in six.iteritems(expected):\n        if is_null(v):\n            ret[k] = {\n                'Exists': False,\n            }\n        else:\n            ret[k] = {\n                'Exists': True,\n                'Value': dynamizer.encode(v),\n            }\n    return ret", "response": "Builds the Expected parameters from a dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild ExpresionAttributeValues from a value or kwargs", "response": "def build_expression_values(dynamizer, expr_values, kwargs):\n    \"\"\" Build ExpresionAttributeValues from a value or kwargs \"\"\"\n    if expr_values:\n        values = expr_values\n        return dynamizer.encode_keys(values)\n    elif kwargs:\n        values = dict(((':' + k, v) for k, v in six.iteritems(kwargs)))\n        return dynamizer.encode_keys(values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to an AWS region.", "response": "def connect_to_region(cls, region, session=None, access_key=None,\n                          secret_key=None, **kwargs):\n        \"\"\"\n        Connect to an AWS region.\n\n        This method has been deprecated in favor of :meth:`~.connect`\n\n        Parameters\n        ----------\n        region : str\n            Name of an AWS region\n        session : :class:`~botocore.session.Session`, optional\n            The Session object to use for the connection\n        access_key : str, optional\n            If session is None, set this access key when creating the session\n        secret_key : str, optional\n            If session is None, set this secret key when creating the session\n        **kwargs : dict\n            Keyword arguments to pass to the constructor\n\n        \"\"\"\n        warnings.warn(\"connect_to_region is deprecated and will be removed. \"\n                      \"Use connect instead.\")\n        if session is None:\n            session = botocore.session.get_session()\n            if access_key is not None:\n                session.set_credentials(access_key, secret_key)\n        client = session.create_client('dynamodb', region)\n        return cls(client, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect_to_host(cls, host='localhost', port=8000, is_secure=False,\n                        session=None, access_key=None, secret_key=None,\n                        **kwargs):\n        \"\"\"\n        Connect to a specific host.\n\n        This method has been deprecated in favor of :meth:`~.connect`\n\n        Parameters\n        ----------\n        host : str, optional\n            Address of the host (default 'localhost')\n        port : int, optional\n            Connect to the host on this port (default 8000)\n        is_secure : bool, optional\n            Enforce https connection (default False)\n        session : :class:`~botocore.session.Session`, optional\n            The Session object to use for the connection\n        access_key : str, optional\n            If session is None, set this access key when creating the session\n        secret_key : str, optional\n            If session is None, set this secret key when creating the session\n        **kwargs : dict\n            Keyword arguments to pass to the constructor\n\n        \"\"\"\n        warnings.warn(\"connect_to_host is deprecated and will be removed. \"\n                      \"Use connect instead.\")\n        if session is None:\n            session = botocore.session.get_session()\n            if access_key is not None:\n                session.set_credentials(access_key, secret_key)\n        url = \"http://%s:%d\" % (host, port)\n        client = session.create_client('dynamodb', 'local', endpoint_url=url,\n                                       use_ssl=is_secure)\n        return cls(client, **kwargs)", "response": "Connect to a specific host."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconnect to an AWS region.", "response": "def connect(cls, region, session=None, access_key=None, secret_key=None,\n                host=None, port=80, is_secure=True, **kwargs):\n        \"\"\"\n        Connect to an AWS region.\n\n        Parameters\n        ----------\n        region : str\n            Name of an AWS region\n        session : :class:`~botocore.session.Session`, optional\n            The Session object to use for the connection\n        access_key : str, optional\n            If session is None, set this access key when creating the session\n        secret_key : str, optional\n            If session is None, set this secret key when creating the session\n        host : str, optional\n            Address of the host. Use this to connect to a local instance.\n        port : int, optional\n            Connect to the host on this port (default 80)\n        is_secure : bool, optional\n            Enforce https connection (default True)\n        **kwargs : dict\n            Keyword arguments to pass to the constructor\n\n        \"\"\"\n        if session is None:\n            session = botocore.session.get_session()\n            if access_key is not None:\n                session.set_credentials(access_key, secret_key)\n        url = None\n        if host is not None:\n            protocol = 'https' if is_secure else 'http'\n            url = \"%s://%s:%d\" % (protocol, host, port)\n        client = session.create_client('dynamodb', region, endpoint_url=url,\n                                       use_ssl=is_secure)\n        return cls(client, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a command on DynamoDB and returns the response.", "response": "def call(self, command, **kwargs):\n        \"\"\"\n        Make a request to DynamoDB using the raw botocore API\n\n        Parameters\n        ----------\n        command : str\n            The name of the Dynamo command to execute\n        **kwargs : dict\n            The parameters to pass up in the request\n\n        Raises\n        ------\n        exc : :class:`~.DynamoDBError`\n\n        Returns\n        -------\n        data : dict\n\n        \"\"\"\n        for hook in self._hooks['precall']:\n            hook(self, command, kwargs)\n        op = getattr(self.client, command)\n        attempt = 0\n        while True:\n            try:\n                data = op(**kwargs)\n                break\n            except ClientError as e:\n                exc = translate_exception(e, kwargs)\n                attempt += 1\n                if isinstance(exc, ThroughputException):\n                    if attempt > self.request_retries:\n                        exc.re_raise()\n                    self.exponential_sleep(attempt)\n                else:\n                    exc.re_raise()\n        for hook in self._hooks['postcall']:\n            hook(self, command, kwargs, data)\n        if 'ConsumedCapacity' in data:\n            is_read = command in READ_COMMANDS\n            consumed = data['ConsumedCapacity']\n            if isinstance(consumed, list):\n                data['consumed_capacity'] = [\n                    ConsumedCapacity.from_response(cap, is_read)\n                    for cap in consumed\n                ]\n            else:\n                capacity = ConsumedCapacity.from_response(consumed, is_read)\n                data['consumed_capacity'] = capacity\n        if 'consumed_capacity' in data:\n            if isinstance(data['consumed_capacity'], list):\n                all_caps = data['consumed_capacity']\n            else:\n                all_caps = [data['consumed_capacity']]\n            for hook in self._hooks['capacity']:\n                for cap in all_caps:\n                    hook(self, command, kwargs, data, cap)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubscribe a callback to an event.", "response": "def subscribe(self, event, hook):\n        \"\"\"\n        Subscribe a callback to an event\n\n        Parameters\n        ----------\n        event : str\n            Available events are 'precall', 'postcall', and 'capacity'.\n            precall is called with: (connection, command, query_kwargs)\n            postcall is called with: (connection, command, query_kwargs, response)\n            capacity is called with: (connection, command, query_kwargs, response, capacity)\n        hook : callable\n\n        \"\"\"\n        if hook not in self._hooks[event]:\n            self._hooks[event].append(hook)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unsubscribe(self, event, hook):\n        if hook in self._hooks[event]:\n            self._hooks[event].remove(hook)", "response": "Unsubscribe a hook from an event"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_rate_limit(self, limiter):\n        if limiter not in self.rate_limiters:\n            self.subscribe('capacity', limiter.on_capacity)\n            self.rate_limiters.append(limiter)", "response": "Add a RateLimit to the connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a RateLimit from the connection", "response": "def remove_rate_limit(self, limiter):\n        \"\"\" Remove a RateLimit from the connection \"\"\"\n        if limiter in self.rate_limiters:\n            self.unsubscribe('capacity', limiter.on_capacity)\n            self.rate_limiters.remove(limiter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the value for ReturnConsumedCapacity from provided value", "response": "def _default_capacity(self, value):\n        \"\"\" Get the value for ReturnConsumedCapacity from provided value \"\"\"\n        if value is not None:\n            return value\n        if self.default_return_capacity or self.rate_limiters:\n            return INDEXES\n        return NONE"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms a scan or query and aggregate the results into a Count", "response": "def _count(self, method, limit, keywords):\n        \"\"\" Do a scan or query and aggregate the results into a Count \"\"\"\n        # The limit will be mutated, so copy it and leave the original intact\n        limit = limit.copy()\n        has_more = True\n        count = None\n        while has_more:\n            limit.set_request_args(keywords)\n            response = self.call(method, **keywords)\n            limit.post_fetch(response)\n            count += Count.from_response(response)\n            last_evaluated_key = response.get('LastEvaluatedKey')\n            has_more = last_evaluated_key is not None and not limit.complete\n            if has_more:\n                keywords['ExclusiveStartKey'] = last_evaluated_key\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the details about a table.", "response": "def describe_table(self, tablename):\n        \"\"\"\n        Get the details about a table\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table\n\n        Returns\n        -------\n        table : :class:`~dynamo3.fields.Table`\n\n        \"\"\"\n        try:\n            response = self.call(\n                'describe_table', TableName=tablename)['Table']\n            return Table.from_response(response)\n        except DynamoDBError as e:\n            if e.kwargs['Code'] == 'ResourceNotFoundException':\n                return None\n            else:  # pragma: no cover\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new table in DynamoDB.", "response": "def create_table(self, tablename, hash_key, range_key=None, indexes=None,\n                     global_indexes=None, throughput=None, wait=False):\n        \"\"\"\n        Create a table\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table\n        hash_key : :class:`~dynamo3.fields.DynamoKey`\n            The key to use as the Hash key\n        range_key : :class:`~dynamo3.fields.DynamoKey`, optional\n            The key to use as the Range key\n        indexes : list, optional\n            List of :class:`~dynamo3.fields.LocalIndex`\n        global_indexes : list, optional\n            List of :class:`~dynamo3.fields.GlobalIndex`\n        throughput : :class:`~dynamo3.fields.Throughput`, optional\n            The throughput of the table\n\n        \"\"\"\n        if throughput is None:\n            throughput = Throughput()\n        all_attrs = set([hash_key])\n        if range_key is not None:\n            all_attrs.add(range_key)\n        key_schema = [hash_key.hash_schema()]\n        if range_key is not None:\n            key_schema.append(range_key.range_schema())\n\n        kwargs = {\n            'TableName': tablename,\n            'KeySchema': key_schema,\n            'ProvisionedThroughput': throughput.schema(),\n        }\n        if indexes:\n            kwargs['LocalSecondaryIndexes'] = [\n                idx.schema(hash_key) for idx in indexes\n            ]\n            for idx in indexes:\n                all_attrs.add(idx.range_key)\n\n        if global_indexes:\n            kwargs['GlobalSecondaryIndexes'] = [\n                idx.schema() for idx in global_indexes\n            ]\n            for idx in global_indexes:\n                all_attrs.add(idx.hash_key)\n                if idx.range_key is not None:\n                    all_attrs.add(idx.range_key)\n\n        kwargs['AttributeDefinitions'] = [attr.definition() for attr in\n                                          all_attrs]\n        result = self.call('create_table', **kwargs)\n        if wait:\n            self.client.get_waiter('table_exists').wait(\n                TableName=tablename\n            )\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a table and all related data.", "response": "def delete_table(self, tablename, wait=False):\n        \"\"\"\n        Delete a table\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to delete\n\n        Returns\n        -------\n        response : bool\n            True if the table was deleted, False if no table exists\n\n        \"\"\"\n        try:\n            self.call('delete_table', TableName=tablename)\n            if wait:\n                self.client.get_waiter('table_not_exists').wait(\n                    TableName=tablename\n                )\n            return True\n        except DynamoDBError as e:\n            if e.kwargs['Code'] == 'ResourceNotFoundException':\n                return False\n            else:  # pragma: no cover\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_item(self, tablename, item, expected=None, returns=NONE,\n                 return_capacity=None, expect_or=False, **kwargs):\n        \"\"\"\n        Store an item, overwriting existing data\n\n        This uses the older version of the DynamoDB API.\n        See also: :meth:`~.put_item2`.\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to write\n        item : dict\n            Item data\n        expected : dict, optional\n            DEPRECATED (use **kwargs instead).\n            If present, will check the values in Dynamo before performing the\n            write. If values do not match, will raise an exception. (Using None\n            as a value checks that the field does not exist).\n        returns : {NONE, ALL_OLD}, optional\n            If ALL_OLD, will return any data that was overwritten (default\n            NONE)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        expect_or : bool, optional\n            If True, the **kwargs conditionals will be OR'd together. If False,\n            they will be AND'd. (default False).\n        **kwargs : dict, optional\n            Conditional filter on the PUT. Same format as the kwargs for\n            :meth:`~.scan`.\n\n        \"\"\"\n        keywords = {}\n        if kwargs:\n            keywords['Expected'] = encode_query_kwargs(self.dynamizer, kwargs)\n            if len(keywords['Expected']) > 1:\n                keywords['ConditionalOperator'] = 'OR' if expect_or else 'AND'\n        elif expected is not None:\n            keywords['Expected'] = build_expected(self.dynamizer, expected)\n        keywords['ReturnConsumedCapacity'] = \\\n            self._default_capacity(return_capacity)\n        item = self.dynamizer.encode_keys(item)\n        ret = self.call('put_item', TableName=tablename, Item=item,\n                        ReturnValues=returns, **keywords)\n        if ret:\n            return Result(self.dynamizer, ret, 'Attributes')", "response": "Stores an item in DynamoDB."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_item(self, tablename, key, attributes=None, consistent=False,\n                 return_capacity=None):\n        \"\"\"\n        Fetch a single item from a table\n\n        This uses the older version of the DynamoDB API.\n        See also: :meth:`~.get_item2`.\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to fetch from\n        key : dict\n            Primary key dict specifying the hash key and, if applicable, the\n            range key of the item.\n        attributes : list, optional\n            If present, only fetch these attributes from the item\n        consistent : bool, optional\n            Perform a strongly consistent read of the data (default False)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n\n        \"\"\"\n        kwargs = {\n            'TableName': tablename,\n            'Key': self.dynamizer.encode_keys(key),\n            'ConsistentRead': consistent,\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n        }\n        if attributes is not None:\n            kwargs['AttributesToGet'] = attributes\n        data = self.call('get_item', **kwargs)\n        return Result(self.dynamizer, data, 'Item')", "response": "Fetch a single item from a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_item2(self, tablename, key, attributes=None, alias=None,\n                  consistent=False, return_capacity=None):\n        \"\"\"\n        Fetch a single item from a table\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to fetch from\n        key : dict\n            Primary key dict specifying the hash key and, if applicable, the\n            range key of the item.\n        attributes : str or list, optional\n            See docs for ProjectionExpression. If list, it will be joined by\n            commas.\n        alias : dict, optional\n            See docs for ExpressionAttributeNames\n        consistent : bool, optional\n            Perform a strongly consistent read of the data (default False)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n\n        \"\"\"\n        kwargs = {\n            'TableName': tablename,\n            'Key': self.dynamizer.encode_keys(key),\n            'ConsistentRead': consistent,\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n        }\n        if attributes is not None:\n            if not isinstance(attributes, six.string_types):\n                attributes = ', '.join(attributes)\n            kwargs['ProjectionExpression'] = attributes\n        if alias:\n            kwargs['ExpressionAttributeNames'] = alias\n        data = self.call('get_item', **kwargs)\n        return Result(self.dynamizer, data, 'Item')", "response": "Fetch a single item from a table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete an item from a table.", "response": "def delete_item(self, tablename, key, expected=None, returns=NONE,\n                    return_capacity=None, expect_or=False, **kwargs):\n        \"\"\"\n        Delete an item\n\n        This uses the older version of the DynamoDB API.\n        See also: :meth:`~.delete_item2`.\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to delete from\n        key : dict\n            Primary key dict specifying the hash key and, if applicable, the\n            range key of the item.\n        expected : dict, optional\n            DEPRECATED (use **kwargs instead).\n            If present, will check the values in Dynamo before performing the\n            write. If values do not match, will raise an exception. (Using None\n            as a value checks that the field does not exist).\n        returns : {NONE, ALL_OLD}, optional\n            If ALL_OLD, return the data that was deleted (default NONE)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        expect_or : bool, optional\n            If True, the **kwargs conditionals will be OR'd together. If False,\n            they will be AND'd. (default False).\n        **kwargs : dict, optional\n            Conditional filter on the DELETE. Same format as the kwargs for\n            :meth:`~.scan`.\n\n        \"\"\"\n        key = self.dynamizer.encode_keys(key)\n        keywords = {\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n        }\n        if kwargs:\n            keywords['Expected'] = encode_query_kwargs(self.dynamizer, kwargs)\n            if len(keywords['Expected']) > 1:\n                keywords['ConditionalOperator'] = 'OR' if expect_or else 'AND'\n        elif expected is not None:\n            keywords['Expected'] = build_expected(self.dynamizer, expected)\n        ret = self.call('delete_item', TableName=tablename, Key=key,\n                        ReturnValues=returns, **keywords)\n        if ret:\n            return Result(self.dynamizer, ret, 'Attributes')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete an item from a table For many parameters you will want to reference the DynamoDB API: http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteItem.html Parameters ---------- tablename : str Name of the table to update key : dict Primary key dict specifying the hash key and, if applicable, the range key of the item. expr_values : dict, optional See docs for ExpressionAttributeValues. See also: kwargs alias : dict, optional See docs for ExpressionAttributeNames condition : str, optional See docs for ConditionExpression returns : {NONE, ALL_OLD, UPDATED_OLD, ALL_NEW, UPDATED_NEW}, optional Return either the old or new values, either all attributes or just the ones that changed. (default NONE) return_capacity : {NONE, INDEXES, TOTAL}, optional INDEXES will return the consumed capacity for indexes, TOTAL will return the consumed capacity for the table and the indexes. (default NONE) return_item_collection_metrics : (NONE, SIZE), optional SIZE will return statistics about item collections that were modified. **kwargs : dict, optional If expr_values is not provided, the kwargs dict will be used as the ExpressionAttributeValues (a ':' will be automatically prepended to all keys).", "response": "def delete_item2(self, tablename, key, expr_values=None, alias=None,\n                     condition=None, returns=NONE, return_capacity=None,\n                     return_item_collection_metrics=NONE, **kwargs):\n        \"\"\"\n        Delete an item from a table\n\n        For many parameters you will want to reference the DynamoDB API:\n        http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteItem.html\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to update\n        key : dict\n            Primary key dict specifying the hash key and, if applicable, the\n            range key of the item.\n        expr_values : dict, optional\n            See docs for ExpressionAttributeValues. See also: kwargs\n        alias : dict, optional\n            See docs for ExpressionAttributeNames\n        condition : str, optional\n            See docs for ConditionExpression\n        returns : {NONE, ALL_OLD, UPDATED_OLD, ALL_NEW, UPDATED_NEW}, optional\n            Return either the old or new values, either all attributes or just\n            the ones that changed. (default NONE)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        return_item_collection_metrics : (NONE, SIZE), optional\n            SIZE will return statistics about item collections that were\n            modified.\n        **kwargs : dict, optional\n            If expr_values is not provided, the kwargs dict will be used as the\n            ExpressionAttributeValues (a ':' will be automatically prepended to\n            all keys).\n\n        \"\"\"\n        keywords = {\n            'TableName': tablename,\n            'Key': self.dynamizer.encode_keys(key),\n            'ReturnValues': returns,\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n            'ReturnItemCollectionMetrics': return_item_collection_metrics,\n        }\n        values = build_expression_values(self.dynamizer, expr_values, kwargs)\n        if values:\n            keywords['ExpressionAttributeValues'] = values\n        if alias:\n            keywords['ExpressionAttributeNames'] = alias\n        if condition:\n            keywords['ConditionExpression'] = condition\n        result = self.call('delete_item', **keywords)\n        if result:\n            return Result(self.dynamizer, result, 'Attributes')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a batch write on a table.", "response": "def batch_write(self, tablename, return_capacity=None,\n                    return_item_collection_metrics=NONE):\n        \"\"\"\n        Perform a batch write on a table\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to write to\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        return_item_collection_metrics : (NONE, SIZE), optional\n            SIZE will return statistics about item collections that were\n            modified.\n\n        Examples\n        --------\n        .. code-block:: python\n\n            with connection.batch_write('mytable') as batch:\n                batch.put({'id': 'id1', 'foo': 'bar'})\n                batch.delete({'id': 'oldid'})\n\n        \"\"\"\n        return_capacity = self._default_capacity(return_capacity)\n        return BatchWriter(self, tablename, return_capacity=return_capacity,\n                           return_item_collection_metrics=return_item_collection_metrics)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef batch_get(self, tablename, keys, attributes=None, alias=None,\n                  consistent=False, return_capacity=None):\n        \"\"\"\n        Perform a batch get of many items in a table\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to fetch from\n        keys : list or iterable\n            List or iterable of primary key dicts that specify the hash key and\n            the optional range key of each item to fetch\n        attributes : str or list, optional\n            See docs for ProjectionExpression. If list, it will be joined by\n            commas.\n        alias : dict, optional\n            See docs for ExpressionAttributeNames\n        consistent : bool, optional\n            Perform a strongly consistent read of the data (default False)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n\n        \"\"\"\n        keys = [self.dynamizer.encode_keys(k) for k in keys]\n        return_capacity = self._default_capacity(return_capacity)\n        ret = GetResultSet(self, tablename, keys,\n                           consistent=consistent, attributes=attributes,\n                           alias=alias, return_capacity=return_capacity)\n        return ret", "response": "Perform a batch get of many items in a table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate a single item in a table.", "response": "def update_item(self, tablename, key, updates, returns=NONE,\n                    return_capacity=None, expect_or=False, **kwargs):\n        \"\"\"\n        Update a single item in a table\n\n        This uses the older version of the DynamoDB API.\n        See also: :meth:`~.update_item2`.\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to update\n        key : dict\n            Primary key dict specifying the hash key and, if applicable, the\n            range key of the item.\n        updates : list\n            List of :class:`~dynamo3.batch.ItemUpdate`\n        returns : {NONE, ALL_OLD, UPDATED_OLD, ALL_NEW, UPDATED_NEW}, optional\n            Return either the old or new values, either all attributes or just\n            the ones that changed. (default NONE)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        expect_or : bool, optional\n            If True, the updates conditionals will be OR'd together. If False,\n            they will be AND'd. (default False).\n        **kwargs : dict, optional\n            Conditional filter on the PUT. Same format as the kwargs for\n            :meth:`~.scan`.\n\n        Notes\n        -----\n        There are two ways to specify the expected values of fields. The\n        simplest is via the list of updates. Each updated field may specify a\n        constraint on the current value of that field. You may pass additional\n        constraints in via the **kwargs the same way you would for put_item.\n        This is necessary if you have constraints on fields that are not being\n        updated.\n\n        \"\"\"\n        key = self.dynamizer.encode_keys(key)\n        attr_updates = {}\n        expected = {}\n        keywords = {\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n        }\n        for update in updates:\n            attr_updates.update(update.attrs(self.dynamizer))\n            expected.update(update.expected(self.dynamizer))\n\n        # Pull the 'expected' constraints from the kwargs\n        for k, v in six.iteritems(encode_query_kwargs(self.dynamizer, kwargs)):\n            if k in expected:\n                raise ValueError(\"Cannot have more than one condition on a single field\")\n            expected[k] = v\n\n        if expected:\n            keywords['Expected'] = expected\n            if len(expected) > 1:\n                keywords['ConditionalOperator'] = 'OR' if expect_or else 'AND'\n\n        result = self.call('update_item', TableName=tablename, Key=key,\n                           AttributeUpdates=attr_updates,\n                           ReturnValues=returns,\n                           **keywords)\n        if result:\n            return Result(self.dynamizer, result, 'Attributes')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a full - table scan of the items in the specified table.", "response": "def scan(self, tablename, attributes=None, count=False, limit=None,\n             return_capacity=None, filter_or=False, exclusive_start_key=None,\n             **kwargs):\n        \"\"\"\n        Perform a full-table scan\n\n        This uses the older version of the DynamoDB API.\n        See also: :meth:`~.scan2`.\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to scan\n        attributes : list\n            If present, only fetch these attributes from the item\n        count : bool, optional\n            If True, return a count of matched items instead of the items\n            themselves (default False)\n        limit : int, optional\n            Maximum number of items to return\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        filter_or : bool, optional\n            If True, multiple filter kwargs will be OR'd together. If False,\n            they will be AND'd together. (default False)\n        exclusive_start_key : dict, optional\n            The ExclusiveStartKey to resume a previous query\n        **kwargs : dict, optional\n            Filter arguments (examples below)\n\n        Examples\n        --------\n        You may pass in constraints using the Django-style '__' syntax. For\n        example:\n\n        .. code-block:: python\n\n            connection.scan('mytable', tags__contains='searchtag')\n            connection.scan('mytable', name__eq='dsa')\n            connection.scan('mytable', action__in=['wibble', 'wobble'])\n\n        \"\"\"\n        keywords = {\n            'TableName': tablename,\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n        }\n        if attributes is not None:\n            keywords['AttributesToGet'] = attributes\n        if exclusive_start_key is not None:\n            keywords['ExclusiveStartKey'] = \\\n                self.dynamizer.maybe_encode_keys(exclusive_start_key)\n        if kwargs:\n            keywords['ScanFilter'] = encode_query_kwargs(\n                self.dynamizer, kwargs)\n            if len(kwargs) > 1:\n                keywords['ConditionalOperator'] = 'OR' if filter_or else 'AND'\n        if not isinstance(limit, Limit):\n            limit = Limit(limit)\n        if count:\n            keywords['Select'] = COUNT\n            return self._count('scan', limit, keywords)\n        else:\n            return ResultSet(self, limit, 'scan', **keywords)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a full - table scan of the table and return the consumed capacity for the table and the indexes.", "response": "def scan2(self, tablename, expr_values=None, alias=None, attributes=None,\n              consistent=False, select=None, index=None, limit=None,\n              return_capacity=None, filter=False, segment=None,\n              total_segments=None, exclusive_start_key=None, **kwargs):\n        \"\"\"\n        Perform a full-table scan\n\n        For many parameters you will want to reference the DynamoDB API:\n        http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Scan.html\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to scan\n        expr_values : dict, optional\n            See docs for ExpressionAttributeValues. See also: kwargs\n        alias : dict, optional\n            See docs for ExpressionAttributeNames\n        attributes : str or list, optional\n            See docs for ProjectionExpression. If list, it will be joined by\n            commas.\n        consistent : bool, optional\n            Perform a strongly consistent read of the data (default False)\n        select : str, optional\n            See docs for Select\n        index : str, optional\n            The name of the index to query\n        limit : int, optional\n            Maximum number of items to return\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        filter : str, optional\n            See docs for FilterExpression\n        segment : int, optional\n            When doing a parallel scan, the unique thread identifier for this\n            scan. If present, total_segments must also be present.\n        total_segments : int, optional\n            When doing a parallel scan, the total number of threads performing\n            the scan.\n        exclusive_start_key : dict, optional\n            The ExclusiveStartKey to resume a previous query\n        **kwargs : dict, optional\n            If expr_values is not provided, the kwargs dict will be used as the\n            ExpressionAttributeValues (a ':' will be automatically prepended to\n            all keys).\n\n        Examples\n        --------\n\n        .. code-block:: python\n\n            connection.scan2('mytable', filter='contains(tags, :search)', search='text)\n            connection.scan2('mytable', filter='id = :id', expr_values={':id': 'dsa'})\n\n        \"\"\"\n        keywords = {\n            'TableName': tablename,\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n            'ConsistentRead': consistent,\n        }\n        values = build_expression_values(self.dynamizer, expr_values, kwargs)\n        if values:\n            keywords['ExpressionAttributeValues'] = values\n        if attributes is not None:\n            if not isinstance(attributes, six.string_types):\n                attributes = ', '.join(attributes)\n            keywords['ProjectionExpression'] = attributes\n        if index is not None:\n            keywords['IndexName'] = index\n        if alias:\n            keywords['ExpressionAttributeNames'] = alias\n        if select:\n            keywords['Select'] = select\n        if filter:\n            keywords['FilterExpression'] = filter\n        if segment is not None:\n            keywords['Segment'] = segment\n        if total_segments is not None:\n            keywords['TotalSegments'] = total_segments\n        if exclusive_start_key is not None:\n            keywords['ExclusiveStartKey'] = \\\n                self.dynamizer.maybe_encode_keys(exclusive_start_key)\n        if not isinstance(limit, Limit):\n            limit = Limit(limit)\n\n        if select == COUNT:\n            return self._count('scan', limit, keywords)\n        else:\n            return ResultSet(self, limit, 'scan', **keywords)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self, tablename, attributes=None, consistent=False, count=False,\n              index=None, limit=None, desc=False, return_capacity=None,\n              filter=None, filter_or=False, exclusive_start_key=None, **kwargs):\n        \"\"\"\n        Perform an index query on a table\n\n        This uses the older version of the DynamoDB API.\n        See also: :meth:`~.query2`.\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to query\n        attributes : list\n            If present, only fetch these attributes from the item\n        consistent : bool, optional\n            Perform a strongly consistent read of the data (default False)\n        count : bool, optional\n            If True, return a count of matched items instead of the items\n            themselves (default False)\n        index : str, optional\n            The name of the index to query\n        limit : int, optional\n            Maximum number of items to return\n        desc : bool, optional\n            If True, return items in descending order (default False)\n        return_capacity : {NONE, INDEXES, TOTAL}, optional\n            INDEXES will return the consumed capacity for indexes, TOTAL will\n            return the consumed capacity for the table and the indexes.\n            (default NONE)\n        filter : dict, optional\n            Query arguments. Same format as **kwargs, but these arguments\n            filter the results on the server before they are returned. They\n            will NOT use an index, as that is what the **kwargs are for.\n        filter_or : bool, optional\n            If True, multiple filter args will be OR'd together. If False, they\n            will be AND'd together. (default False)\n        exclusive_start_key : dict, optional\n            The ExclusiveStartKey to resume a previous query\n        **kwargs : dict, optional\n            Query arguments (examples below)\n\n        Examples\n        --------\n        You may pass in constraints using the Django-style '__' syntax. For\n        example:\n\n        .. code-block:: python\n\n            connection.query('mytable', foo__eq=5)\n            connection.query('mytable', foo__eq=5, bar__lt=22)\n            connection.query('mytable', foo__eq=5, bar__between=(1, 10))\n\n        \"\"\"\n        keywords = {\n            'TableName': tablename,\n            'ReturnConsumedCapacity': self._default_capacity(return_capacity),\n            'ConsistentRead': consistent,\n            'ScanIndexForward': not desc,\n            'KeyConditions': encode_query_kwargs(self.dynamizer, kwargs),\n        }\n        if attributes is not None:\n            keywords['AttributesToGet'] = attributes\n        if index is not None:\n            keywords['IndexName'] = index\n        if filter is not None:\n            if len(filter) > 1:\n                keywords['ConditionalOperator'] = 'OR' if filter_or else 'AND'\n            keywords['QueryFilter'] = encode_query_kwargs(self.dynamizer,\n                                                          filter)\n        if exclusive_start_key is not None:\n            keywords['ExclusiveStartKey'] = \\\n                self.dynamizer.maybe_encode_keys(exclusive_start_key)\n        if not isinstance(limit, Limit):\n            limit = Limit(limit)\n        if count:\n            keywords['Select'] = COUNT\n            return self._count('query', limit, keywords)\n        else:\n            return ResultSet(self, limit, 'query', **keywords)", "response": "Query the DynamoDB table for the items in the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_table(self, tablename, throughput=None, global_indexes=None,\n                     index_updates=None):\n        \"\"\"\n        Update the throughput of a table and/or global indexes\n\n        Parameters\n        ----------\n        tablename : str\n            Name of the table to update\n        throughput : :class:`~dynamo3.fields.Throughput`, optional\n            The new throughput of the table\n        global_indexes : dict, optional\n            DEPRECATED. Use index_updates now.\n            Map of index name to :class:`~dynamo3.fields.Throughput`\n        index_updates : list of :class:`~dynamo3.fields.IndexUpdate`, optional\n            List of IndexUpdates to perform\n\n        \"\"\"\n        kwargs = {\n            'TableName': tablename\n        }\n        all_attrs = set()\n        if throughput is not None:\n            kwargs['ProvisionedThroughput'] = throughput.schema()\n        if index_updates is not None:\n            updates = []\n            for update in index_updates:\n                all_attrs.update(update.get_attrs())\n                updates.append(update.serialize())\n            kwargs['GlobalSecondaryIndexUpdates'] = updates\n        elif global_indexes is not None:\n            kwargs['GlobalSecondaryIndexUpdates'] = [\n                {\n                    'Update': {\n                        'IndexName': key,\n                        'ProvisionedThroughput': value.schema(),\n                    }\n                }\n                for key, value in six.iteritems(global_indexes)\n            ]\n        if all_attrs:\n            attr_definitions = [attr.definition() for attr in all_attrs]\n            kwargs['AttributeDefinitions'] = attr_definitions\n        return self.call('update_table', **kwargs)", "response": "Update the throughput of a table and or global indexes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbreaks catalog into chunks by healpix pixel. Parameters: ----------- infiles : List of input files config : Configuration file force : Overwrite existing files (depricated) Returns: -------- None", "response": "def pixelizeCatalog(infiles, config, force=False):\n    \"\"\"\n    Break catalog into chunks by healpix pixel.\n    \n    Parameters:\n    -----------\n    infiles : List of input files\n    config  : Configuration file\n    force   : Overwrite existing files (depricated)\n    \n    Returns:\n    --------\n    None\n    \"\"\"\n    nside_catalog = config['coords']['nside_catalog']\n    nside_pixel = config['coords']['nside_pixel']\n    coordsys = config['coords']['coordsys'].upper()\n    outdir = mkdir(config['catalog']['dirname'])\n    filenames = config.getFilenames()\n    lon_field = config['catalog']['lon_field'].upper()\n    lat_field = config['catalog']['lat_field'].upper()\n\n    # ADW: It would probably be better (and more efficient) to do the\n    # pixelizing and the new column insertion separately.\n    for i,filename in enumerate(infiles):\n        logger.info('(%i/%i) %s'%(i+1, len(infiles), filename))\n        data = fitsio.read(filename)\n        logger.info(\"%i objects found\"%len(data))\n        if not len(data): continue\n\n        columns = map(str.upper,data.dtype.names)\n        names,arrs = [],[]\n\n        if (lon_field in columns) and (lat_field in columns):\n            lon,lat = data[lon_field],data[lat_field]\n        elif coordsys == 'GAL':\n            msg = \"Columns '%s' and '%s' not found.\"%(lon_field,lat_field)\n            msg += \"\\nConverting from RA,DEC\"\n            logger.warning(msg)\n            lon,lat = cel2gal(data['RA'],data['DEC'])\n            names += [lon_field,lat_field]\n            arrs  += [lon,lat]\n        elif coordsys == 'CEL':\n            msg = \"Columns '%s' and '%s' not found.\"%(lon_field,lat_field)\n            msg += \"\\nConverting from GLON,GLAT\"\n            lon,lat = gal2cel(data['GLON'],data['GLAT'])\n            names  += [lon_field,lat_field]\n            arrs   += [lon,lat]\n\n        cat_pix = ang2pix(nside_catalog,lon,lat)\n        pix_pix = ang2pix(nside_pixel,lon,lat)\n        cat_pix_name = 'PIX%i'%nside_catalog\n        pix_pix_name = 'PIX%i'%nside_pixel\n\n        try:\n            names += [cat_pix_name,pix_pix_name]\n            arrs  += [cat_pix,pix_pix]\n            data=mlab.rec_append_fields(data,names=names,arrs=arrs)\n        except ValueError as e:\n            logger.warn(str(e)+'; not adding column.')\n            #data[cat_pix_name] = cat_pix\n            #data[pix_pix_name] = pix_pix\n                               \n        for pix in np.unique(cat_pix):\n            logger.debug(\"Processing pixel %s\"%pix)\n\n            arr = data[cat_pix == pix]\n            outfile = filenames.data['catalog'][pix]\n\n            if not os.path.exists(outfile):\n                logger.debug(\"Creating %s\"%outfile)\n                out=fitsio.FITS(outfile,mode='rw')\n                out.write(arr)\n\n                hdr=healpix.header_odict(nside=nside_catalog,\n                                                     coord=coordsys[0])\n                for key in ['PIXTYPE','ORDERING','NSIDE','COORDSYS']:\n                    out[1].write_key(*list(hdr[key].values()))\n                out[1].write_key('PIX',pix,comment='HEALPIX pixel for this file')\n            else:\n                out=fitsio.FITS(outfile,mode='rw')\n                out[1].append(arr)\n\n            logger.debug(\"Writing %s\"%outfile)\n            out.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_logodata(handle):\n    seqs = weblogolib.read_seq_data(handle,\n                                    alphabet=unambiguous_protein_alphabet)\n    ldata = weblogolib.LogoData.from_seqs(seqs)\n    letters = ldata.alphabet.letters()\n    counts = ldata.counts.array\n    logodata = []\n    for i, coldata, entropy, weight in zip(range(len(counts)), counts,\n                                           ldata.entropy, ldata.weight):\n        cnts = dict((let, int(cnt))\n                    for let, cnt in zip(letters, coldata))\n        logodata.append((i + 1, cnts, entropy, weight))\n    return logodata", "response": "Get weblogo data for a sequence alignment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets weblogo data for an alignment object.", "response": "def aln2logodata(aln):\n    \"\"\"Get weblogo data for an alignment object.\n\n    Returns a list of tuples: (posn, letter_counts, entropy, weight)\n    \"\"\"\n    handle = StringIO(aln.format('fasta'))\n    logodata = read_logodata(handle)\n    handle.close()\n    return logodata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert letter counts to frequencies sorted increasing.", "response": "def letter_scales(counts):\n    \"\"\"Convert letter counts to frequencies, sorted increasing.\"\"\" \n    try:\n        scale = 1.0 / sum(counts.values())\n    except ZeroDivisionError:\n        # This logo is all gaps, nothing can be done\n        return []\n    freqs = [(aa, cnt*scale) for aa, cnt in counts.iteritems() if cnt]\n    freqs.sort(key=lambda pair: pair[1])\n    return freqs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check(doc, pointer, expected, raise_onerror=False):\n    return Target(doc).check(pointer, expected, raise_onerror)", "response": "Check if value exists into object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(doc, pointer, value):\n    return Target(doc).add(pointer, value).document", "response": "Add element to sequence member to mapping. it\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace(doc, pointer, value):\n\n    return Target(doc).replace(pointer, value).document", "response": "Replace element from sequence member from mapping."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving element from sequence member from mapping.", "response": "def move(doc, dest, src):\n    \"\"\"Move element from sequence, member from mapping.\n\n    :param doc: the document base\n    :param dest: the destination\n    :type dest: Pointer\n    :param src: the source\n    :type src: Pointer\n    :return: the new object\n\n    .. note::\n\n        it delete then it add to the new location\n        soo the dest must refer to the middle object.\n\n    \"\"\"\n\n    return Target(doc).move(dest, src).document"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy element from sequence member from mapping.", "response": "def copy(doc, dest, src):\n    \"\"\"Copy element from sequence, member from mapping.\n\n    :param doc: the document base\n    :param dest: the destination\n    :type dest: Pointer\n    :param src: the source\n    :type src: Pointer\n    :return: the new object\n    \"\"\"\n\n    return Target(doc).copy(dest, src).document"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting member from network It is method for DELETE /network/{id}/member/{address}", "response": "def deleteMember(self, address, id, headers=None, query_params=None, content_type=\"application/json\"):\n        \"\"\"\n        Delete member from network\n        It is method for DELETE /network/{id}/member/{address}\n        \"\"\"\n        uri = self.client.base_url + \"/network/\"+id+\"/member/\"+address\n        return self.client.delete(uri, None, headers, query_params, content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getMember(self, address, id, headers=None, query_params=None, content_type=\"application/json\"):\n        uri = self.client.base_url + \"/network/\"+id+\"/member/\"+address\n        return self.client.get(uri, None, headers, query_params, content_type)", "response": "Get member settings for a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating member settings It is method for POST /network/{id}/member/{address}", "response": "def updateMember(self, data, address, id, headers=None, query_params=None, content_type=\"application/json\"):\n        \"\"\"\n        Update member settings\n        It is method for POST /network/{id}/member/{address}\n        \"\"\"\n        uri = self.client.base_url + \"/network/\"+id+\"/member/\"+address\n        return self.client.post(uri, data, headers, query_params, content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of network members It is method for GET /network/{id}/member", "response": "def listMembers(self, id, headers=None, query_params=None, content_type=\"application/json\"):\n        \"\"\"\n        Get a list of network members\n        It is method for GET /network/{id}/member\n        \"\"\"\n        uri = self.client.base_url + \"/network/\"+id+\"/member\"\n        return self.client.get(uri, None, headers, query_params, content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deleteNetwork(self, id, headers=None, query_params=None, content_type=\"application/json\"):\n        uri = self.client.base_url + \"/network/\"+id\n        return self.client.delete(uri, None, headers, query_params, content_type)", "response": "This method deletes the specified network from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updateNetwork(self, data, id, headers=None, query_params=None, content_type=\"application/json\"):\n        uri = self.client.base_url + \"/network/\"+id\n        return self.client.post(uri, data, headers, query_params, content_type)", "response": "This method is used to update the configuration of a specific resource in a network."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new network in the cluster.", "response": "def createNetwork(self, data, headers=None, query_params=None, content_type=\"application/json\"):\n        \"\"\"\n        Create a new network\n        It is method for POST /network\n        \"\"\"\n        uri = self.client.base_url + \"/network\"\n        return self.client.post(uri, data, headers, query_params, content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_options(self, **kw):\n        for k, v in kw.iteritems():\n            if k in self.__options:\n                self.__options[k] = v", "response": "r Set parser options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xml2object(self, content):\n        content = self.xml_filter(content)\n        element = ET.fromstring(content)\n        tree = self.parse(element) if self.__options['strip_attr'] else self.parse_full(element)\n        if not self.__options['strip_root']:\n            node = self.get_node(element)\n            if not self.__options['strip_attr']:\n                tree['attrs'] = node['attr']\n            return {node['tag']: tree}\n        return tree", "response": "Convert xml content to python object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters and preprocess xml content", "response": "def xml_filter(self, content):\n        r\"\"\"Filter and preprocess xml content\n\n        :param content: xml content\n        :rtype: str\n        \"\"\"\n        content = utils.strip_whitespace(content, True) if self.__options['strip'] else content.strip()\n\n        if not self.__options['encoding']:\n            encoding = self.guess_xml_encoding(content) or self.__encoding\n            self.set_options(encoding=encoding)\n\n        if self.__options['encoding'].lower() != self.__encoding:\n            # \u7f16\u7801\u8f6c\u6362\u53bb\u9664xml\u5934\n            content = self.strip_xml_header(content.decode(self.__options['encoding'], errors=self.__options['errors']))\n\n        if self.__options['unescape']:\n            content = utils.html_entity_decode(content)\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nguessing encoding from xml header declaration.", "response": "def guess_xml_encoding(self, content):\n        r\"\"\"Guess encoding from xml header declaration.\n\n        :param content: xml content\n        :rtype: str or None\n        \"\"\"\n        matchobj = self.__regex['xml_encoding'].match(content)\n        return matchobj and matchobj.group(1).lower()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, element):\n        values = {}\n        for child in element:\n            node = self.get_node(child)\n            subs = self.parse(child)\n            value = subs or node['value']\n            if node['tag'] not in values:\n                values[node['tag']] = value\n            else:\n                if not isinstance(values[node['tag']], list):\n                    values[node['tag']] = [values.pop(node['tag'])]\n                values[node['tag']].append(value)\n        return values", "response": "Parse xml element.\n\n        returns a dict of the keys and the values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_full(self, element):\n        values = collections.defaultdict(dict)\n        for child in element:\n            node = self.get_node(child)\n            subs = self.parse_full(child)\n            value = subs or {'values': node['value']}\n            value['attrs'] = node['attr']\n            if node['tag'] not in values['values']:\n                values['values'][node['tag']] = value\n            else:\n                if not isinstance(values['values'][node['tag']], list):\n                    values['values'][node['tag']] = [values['values'].pop(node['tag'])]\n                values['values'][node['tag']].append(value)\n        return values", "response": "Parse xml element include the node attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses an element and get the node tag info. Include tag name value attribute namespace.", "response": "def get_node(self, element):\n        r\"\"\"Get node info.\n\n        Parse element and get the element tag info. Include tag name, value, attribute, namespace.\n\n        :param element: an :class:`~xml.etree.ElementTree.Element` instance\n        :rtype: dict\n        \"\"\"\n        ns, tag = self.split_namespace(element.tag)\n        return {'tag': tag, 'value': (element.text or '').strip(), 'attr': element.attrib, 'namespace': ns}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_namespace(self, tag):\n        matchobj = self.__regex['xml_ns'].search(tag)\n        return matchobj.groups() if matchobj else ('', tag)", "response": "r Split tag namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef instantiate_probes(probes, instruments):\n\n    probe_instances = {}\n    for name, sub_dict in probes.items():\n        assert isinstance(sub_dict, dict)\n        assert \"probe_name\" in sub_dict\n        assert \"instrument_name\" in sub_dict\n\n        probe_name = sub_dict['probe_name']\n        instrument_name = sub_dict['instrument_name']\n\n        if \"probe_info\" in sub_dict:\n            probe_info = sub_dict['probe_info']\n        else:\n            probe_info = ''\n\n        assert instrument_name in instruments, \"{:s} not in {:s}\".format(instrument_name, list(instruments.keys()))\n        assert probe_name in instruments[instrument_name]._PROBES\n\n        probe_instances.update({name: Probe(instruments[instrument_name], probe_name, name, probe_info)})\n\n    return probe_instances", "response": "Creates a list of probes that are already instantiated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nderiving the address from the public key.", "response": "def derivesha256address(self):\n        \"\"\" Derive address using ``RIPEMD160(SHA256(x))`` \"\"\"\n        pkbin = unhexlify(repr(self._pubkey))\n        addressbin = ripemd160(hexlify(hashlib.sha256(pkbin).digest()))\n        return Base58(hexlify(addressbin).decode('ascii'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nderiving the public key address using SHA512.", "response": "def derivesha512address(self):\n        \"\"\" Derive address using ``RIPEMD160(SHA512(x))`` \"\"\"\n        pkbin = unhexlify(repr(self._pubkey))\n        addressbin = ripemd160(hexlify(hashlib.sha512(pkbin).digest()))\n        return Base58(hexlify(addressbin).decode('ascii'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compressed(self):\n        order = ecdsa.SECP256k1.generator.order()\n        p = ecdsa.VerifyingKey.from_string(\n            compat_bytes(self), curve=ecdsa.SECP256k1).pubkey.point\n        x_str = ecdsa.util.number_to_string(p.x(), order)\n        # y_str = ecdsa.util.number_to_string(p.y(), order)\n        compressed = hexlify(\n            compat_bytes(chr(2 + (p.y() & 1)), 'ascii') + x_str).decode(\n            'ascii')\n        return (compressed)", "response": "Derive compressed public key"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nderives uncompressed key from the public key.", "response": "def unCompressed(self):\n        \"\"\" Derive uncompressed key \"\"\"\n        public_key = repr(self._pk)\n        prefix = public_key[0:2]\n        if prefix == \"04\":\n            return public_key\n        assert prefix == \"02\" or prefix == \"03\"\n        x = int(public_key[2:], 16)\n        y = self._derive_y_from_x(x, (prefix == \"02\"))\n        key = '04' + '%064x' % x + '%064x' % y\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nderives compressed public key", "response": "def compressedpubkey(self):\n        \"\"\" Derive uncompressed public key \"\"\"\n        secret = unhexlify(repr(self._wif))\n        order = ecdsa.SigningKey.from_string(\n            secret, curve=ecdsa.SECP256k1).curve.generator.order()\n        p = ecdsa.SigningKey.from_string(\n            secret, curve=ecdsa.SECP256k1).verifying_key.pubkey.point\n        x_str = ecdsa.util.number_to_string(p.x(), order)\n        y_str = ecdsa.util.number_to_string(p.y(), order)\n        compressed = hexlify(\n            chr(2 + (p.y() & 1)).encode('ascii') + x_str\n        ).decode('ascii')\n        uncompressed = hexlify(\n            chr(4).encode('ascii') + x_str + y_str).decode(\n            'ascii')\n        return [compressed, uncompressed]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_private(self):\n        a = compat_bytes(self.account + self.role + self.password, 'utf8')\n        s = hashlib.sha256(a).digest()\n        return PrivateKey(hexlify(s).decode('ascii'))", "response": "Derive private key from the brain key and the current sequence\n            number"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(activeBridge=None, address=None, authorized=None, clock=None, identity=None, ipAssignments=None, memberRevision=None, nwid=None):\n\n        return NetworkMemberConfig(\n            activeBridge=activeBridge,\n            address=address,\n            authorized=authorized,\n            clock=clock,\n            identity=identity,\n            ipAssignments=ipAssignments,\n            memberRevision=memberRevision,\n            nwid=nwid,\n        )", "response": "Creates a new NetworkMemberConfig object from the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cell_length(flow_model):\n        assert flow_model.lower() in FlowModelConst.d8_lens\n        return FlowModelConst.d8_lens.get(flow_model.lower())", "response": "Get flow direction induced cell length dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget flow direction induced cell shift dict.", "response": "def get_cell_shift(flow_model):\n        \"\"\"Get flow direction induced cell shift dict.\n        Args:\n            flow_model: Currently, \"TauDEM\", \"ArcGIS\", and \"Whitebox\" are supported.\n        \"\"\"\n        assert flow_model.lower() in FlowModelConst.d8_deltas\n        return FlowModelConst.d8_deltas.get(flow_model.lower())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds downslope coordinate for D8 direction", "response": "def downstream_index(dir_value, i, j, alg='taudem'):\n        \"\"\"find downslope coordinate for D8 direction.\"\"\"\n        assert alg.lower() in FlowModelConst.d8_deltas\n        delta = FlowModelConst.d8_deltas.get(alg.lower())\n        drow, dcol = delta[int(dir_value)]\n        return i + drow, j + dcol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting D8 flow direction code from one algorithm to another.", "response": "def convert_code(in_file, out_file, in_alg='taudem', out_alg='arcgis', datatype=None):\n        \"\"\"\n        convert D8 flow direction code from one algorithm to another.\n        Args:\n            in_file: input raster file path\n            out_file: output raster file path\n            in_alg: available algorithms are in FlowModelConst.d8_dirs. \"taudem\" is the default\n            out_alg: same as in_alg. \"arcgis\" is the default\n            datatype: default is None and use the datatype of the in_file\n        \"\"\"\n        FileClass.check_file_exists(in_file)\n        in_alg = in_alg.lower()\n        out_alg = out_alg.lower()\n        if in_alg not in FlowModelConst.d8_dirs or out_alg not in FlowModelConst.d8_dirs:\n            raise RuntimeError('The input algorithm name should one of %s' %\n                               ', '.join(list(FlowModelConst.d8_dirs.keys())))\n        convert_dict = dict()\n        in_code = FlowModelConst.d8_dirs.get(in_alg)\n        out_code = FlowModelConst.d8_dirs.get(out_alg)\n        assert len(in_code) == len(out_code)\n        for i, tmp_in_code in enumerate(in_code):\n            convert_dict[tmp_in_code] = out_code[i]\n        if datatype is not None and datatype in GDALDataType:\n            RasterUtilClass.raster_reclassify(in_file, convert_dict, out_file, datatype)\n        else:\n            RasterUtilClass.raster_reclassify(in_file, convert_dict, out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search_seqs(self, seqrec, in_seq, locus, run=0, partial_ann=None):\n        # Extract out the sequences and feature names\n        # from the reference sequences\n\n        # The mapped features will be subtracted from seq_covered\n        # so the final seq_covered number will reflect the remaining\n        # number of base pairs that haven't been mapped.\n        #\n        # The coordinates and mapping will help determine what positions\n        # in the sequence have been mapped and to what features. The\n        # missing blocks variable will be generated using these.\n        structures = get_structures()\n        seq_covered = len(in_seq.seq)\n        coordinates = dict(map(lambda x: [x, 1],\n                           [i for i in range(0, len(in_seq.seq)+1)]))\n\n        mapping = dict(map(lambda x: [x, 1],\n                           [i for i in range(0, len(in_seq.seq)+1)]))\n\n        ambig_map = {}\n        found_feats = {}\n        feat_missing = {}\n\n        method = \"nt_search\" if not partial_ann else partial_ann.method\n\n        # If the partial annotation is provided\n        # then make the found_feats equal to\n        # what has already been annotated\n        feats = get_features(seqrec)\n        if partial_ann:\n\n            found_feats = partial_ann.features\n\n            if self.verbose and self.verbosity > 4:\n                self.logger.info(\"Found partial features:\")\n                for f in found_feats:\n                    self.logger.info(f)\n\n            # Skip references that only have features\n            # that have already been annoated\n            if len([f for f in feats if f in found_feats]) == len(feats):\n                if self.verbose:\n                    self.logger.info(\"Skipping incomplete refseq\")\n                return partial_ann\n\n            if self.verbose and self.verbosity > 1:\n                self.logger.info(\"Using partial annotation | \"\n                                 + locus + \" \"\n                                 + str(len(partial_ann.features)))\n\n            coordinates = dict(map(lambda l: [l, 1],\n                                   [item for sublist\n                                    in partial_ann.blocks\n                                    for item in sublist]))\n            seq_covered = partial_ann.covered\n            mapping = partial_ann.mapping\n\n            if self.verbose and self.verbosity > 2:\n                self.logger.info(\"Partial sequence coverage = \"\n                                 + str(seq_covered))\n                self.logger.info(\"Partial sequence metho = \"\n                                 + method)\n\n        added_feat = {}\n        deleted_coords = {}\n        for feat_name in sorted(feats,\n                                key=lambda k: structures[locus][k]):\n\n            # skip if partial annotation is provided\n            # and the feat name is not one of the\n            # missing features\n            if partial_ann and feat_name not in partial_ann.refmissing:\n                if self.verbose and self.verbosity > 1:\n                    self.logger.info(\"Skipping \" + feat_name\n                                     + \" - Already annotated\")\n                continue\n\n            if self.verbose and self.verbosity > 1:\n                self.logger.info(\"Running seqsearch for \" + feat_name)\n\n            # Search for the reference feature sequence in the\n            # input sequence. Record the coordinates if it's\n            # found and if it's found in multiple spots. If it\n            # is not found, then record that feature as missing.\n            seq_search = nt_search(str(in_seq.seq), str(feats[feat_name]))\n\n            if len(seq_search) == 2:\n\n                if self.verbose and self.verbosity > 0:\n                    self.logger.info(\"Found exact match for \" + feat_name)\n\n                seq_covered -= len(str(feats[feat_name]))\n                end = int(len(str(feats[feat_name])) + seq_search[1])\n\n                if feat_name == 'three_prime_UTR' \\\n                        and len(str(in_seq.seq)) > end:\n                        end = len(str(in_seq.seq))\n\n                # If the feature is found and it's a five_prime_UTR then\n                # the start should always be 0, so insertions at the\n                # beinging of the sequence will be found.\n                start = seq_search[1] if feat_name != 'five_prime_UTR' else 0\n                si = seq_search[1]+1 if seq_search[1] != 0 and \\\n                    feat_name != 'five_prime_UTR' else 0\n\n                # check if this features has already been mapped\n                mapcheck = set([0 if i in coordinates else 1\n                               for i in range(si, end+1)])\n\n                # Dont map features if they are out of order\n                skip = False\n                if found_feats and len(found_feats) > 0:\n                    for f in found_feats:\n                        o1 = structures[locus][feat_name]\n                        o2 = structures[locus][f]\n                        loctyp = loctype(found_feats[f].location.start,\n                                         found_feats[f].location.end,\n                                         start, end)\n\n                        if o1 < o2 and loctyp:\n                            skip = True\n                            if self.verbose:\n                                self.logger.info(\"Skipping map for \"\n                                                 + feat_name)\n                        elif o2 < o1 and not loctyp:\n                            skip = True\n                            if self.verbose:\n                                self.logger.info(\"Skipping map for \"\n                                                 + feat_name)\n\n                if 1 not in mapcheck and not skip:\n                    for i in range(si, end+1):\n                        if i in coordinates:\n                            if feat_name == \"exon_8\" or feat_name == 'three_prime_UTR':\n                                deleted_coords.update({i: coordinates[i]})\n                            del coordinates[i]\n                        else:\n                            if self.verbose:\n                                self.logger.error(\"seqsearch - should't be here \"\n                                                  + locus + \" - \"\n                                                  + \" - \" + feat_name)\n                        mapping[i] = feat_name\n\n                    found_feats.update({feat_name:\n                                        SeqFeature(\n                                            FeatureLocation(\n                                                ExactPosition(start),\n                                                ExactPosition(end), strand=1),\n                                            type=feat_name)})\n\n                    if feat_name == \"exon_8\" or feat_name == 'three_prime_UTR':\n                        added_feat.update({feat_name: feats[feat_name]})\n                    if self.verbose and self.verbosity > 3:\n                        self.logger.info(\"Coordinates | Start = \" + str(start) + \" - End = \" + str(end))\n\n            elif(len(seq_search) > 2):\n                if self.verbose and self.verbosity > 1:\n                    self.logger.info(\"Found \" + str(len(seq_search))\n                                     + \" matches for \" + feat_name)\n\n                new_seq = [seq_search[0]]\n                for i in range(1, len(seq_search)):\n                    tnp = seq_search[i]+1\n                    if seq_search[i] in coordinates or tnp in coordinates:\n                        new_seq.append(seq_search[i])\n\n                seq_search = new_seq\n                if(partial_ann and feat_name == \"exon_8\" and run > 0):\n                    missing_feats = sorted(list(partial_ann.missing.keys()))\n\n                    # * HARD CODED LOGIC * #\n                    # > exon8 in class I maps to multiple spots in a sequence,\n                    #   often in the 3' UTR. These features need to be mapped\n                    #   last to make sure it's not mapping exon8 incorrectly.\n                    if(missing_feats == ['exon_8', 'three_prime_UTR']\n                       and len(seq_search) <= 3):\n                        if self.verbose and self.verbosity > 0:\n                            self.logger.info(\"Resolving exon_8\")\n\n                        seq_covered -= len(str(feats[feat_name]))\n                        end = int(len(str(feats[feat_name])) + seq_search[1])\n\n                        # If the feature is found and it's a five_prime_UTR then\n                        # the start should always be 0, so insertions at the\n                        # beinging of the sequence will be found.\n                        start = seq_search[1]\n                        si = seq_search[1]+1 if seq_search[1] != 0 else 0\n\n                        # check if this features has already been mapped\n                        mapcheck = set([0 if i in coordinates else 1\n                                        for i in range(si, end+1)])\n\n                        for i in range(si, end+1):\n                            if i in coordinates:\n                                del coordinates[i]\n                            else:\n                                if self.verbose:\n                                    self.logger.error(\"seqsearch - should't be here \"\n                                                      + locus + \" - \"\n                                                      + \" - \" + feat_name)\n                            mapping[i] = feat_name\n\n                        found_feats.update({feat_name:\n                                            SeqFeature(\n                                                FeatureLocation(\n                                                    ExactPosition(start),\n                                                    ExactPosition(end), strand=1),\n                                                type=feat_name)})\n\n                        if self.verbose and self.verbosity > 0:\n                            self.logger.info(\"Coordinates | Start = \" + str(start) + \" - End = \" + str(end))\n                    else:\n                        if self.verbose and self.verbosity > 0:\n                            self.logger.info(\"Adding ambig feature \" + feat_name)\n                        feat_missing.update({feat_name: feats[feat_name]})\n                        ambig_map.update({feat_name:\n                                          seq_search[1:len(seq_search)]})\n                else:\n                    if self.verbose and self.verbosity > 0:\n                        self.logger.info(\"Adding ambig feature \" + feat_name)\n                    feat_missing.update({feat_name: feats[feat_name]})\n                    ambig_map.update({feat_name: seq_search[1:len(seq_search)]})\n            else:\n                if self.verbose and self.verbosity > 1:\n                    self.logger.info(\"No match for \" + feat_name)\n                feat_missing.update({feat_name: feats[feat_name]})\n\n        blocks = getblocks(coordinates)\n        exact_matches = list(found_feats.keys())\n\n        # * HARD CODED LOGIC * #\n        # > \n        #\n        #  HLA-DRB1 exon3 exact match - with intron1 and 3 missing\n        if('exon_3' in exact_matches and run == 99 and locus == 'HLA-DRB1'\n           and 'exon_2' in feat_missing and (len(blocks) == 1 or len(blocks) == 2)):\n\n            for b in blocks:\n                x = b[len(b)-1]\n                if x == max(list(mapping.keys())):\n                    featname = \"intron_3\"\n                    found_feats.update({featname:\n                                        SeqFeature(\n                                            FeatureLocation(\n                                                ExactPosition(b[0]-1),\n                                                ExactPosition(b[len(b)-1]),\n                                                strand=1),\n                                            type=featname)})\n                else:\n                    featname = \"exon_2\"\n                    found_feats.update({featname:\n                                        SeqFeature(\n                                            FeatureLocation(\n                                                ExactPosition(b[0]),\n                                                ExactPosition(b[len(b)-1]),\n                                                strand=1),\n                                            type=featname)})\n                    seq_covered -= len(b)\n\n                if self.verbose and self.verbosity > 1:\n                    self.logger.info(\"Successfully annotated class DRB1 II sequence\")\n\n                return Annotation(features=found_feats,\n                                  covered=seq_covered,\n                                  seq=in_seq,\n                                  missing=feat_missing,\n                                  ambig=ambig_map,\n                                  method=method,\n                                  mapping=mapping,\n                                  exact_match=exact_matches)\n\n        # If it's a class II sequence and\n        # exon_2 is an exact match\n        # * HARD CODED LOGIC * #\n        # > It's common for exon2 to be fully sequenced\n        #   but intron_2 and intron_1 to be partially sequenced,\n        #   which can make it hard to annotate those to features.\n        #   If there are two missing blocks that is small enough\n        #   and they are before and after exon2, then it's very\n        #   very likely to be intron_2 and intron_1.\n        if 'exon_2' in exact_matches and len(blocks) == 2 \\\n                and is_classII(locus) and seq_covered < 300:\n\n            if self.verbose and self.verbosity > 1:\n                self.logger.info(\"Running search for class II sequence\")\n\n            r = True\n            for b in blocks:\n                x = b[len(b)-1]\n                if x == max(list(mapping.keys())):\n                    x = b[0]-1\n                else:\n                    x += 1\n                f = mapping[x]\n                if f != 'exon_2':\n                    r = False\n            if r:\n                for b in blocks:\n                    x = b[len(b)-1]\n                    if x == max(list(mapping.keys())):\n                        featname = \"intron_2\"\n                        found_feats.update({featname:\n                                            SeqFeature(\n                                                FeatureLocation(\n                                                    ExactPosition(b[0]-1),\n                                                    ExactPosition(b[len(b)-1]),\n                                                    strand=1),\n                                                type=featname)})\n                    else:\n                        featname = \"intron_1\"\n                        found_feats.update({featname:\n                                            SeqFeature(\n                                                FeatureLocation(\n                                                    ExactPosition(b[0]),\n                                                    ExactPosition(b[len(b)-1]),\n                                                    strand=1),\n                                                type=featname)})\n                    seq_covered -= len(b)\n\n                if self.verbose and self.verbosity > 1:\n                    self.logger.info(\"Successfully annotated class II sequence\")\n\n                return Annotation(features=found_feats,\n                                  covered=seq_covered,\n                                  seq=in_seq,\n                                  missing=feat_missing,\n                                  ambig=ambig_map,\n                                  method=method,\n                                  mapping=mapping,\n                                  exact_match=exact_matches)\n\n        annotated_feats, mb, mapping = self._resolve_unmapped(blocks,\n                                                              feat_missing,\n                                                              ambig_map,\n                                                              mapping,\n                                                              found_feats,\n                                                              locus,\n                                                              seq_covered\n                                                              )\n\n        # * HARD CODED LOGIC * #\n        if(not mb and blocks and len(feat_missing.keys()) == 0\n           and len(ambig_map.keys()) == 0):\n            mb = blocks\n\n        if mb:\n\n            # Unmap exon 8\n            if locus in ['HLA-C', 'HLA-A'] and len(in_seq.seq) < 3000 \\\n                    and 'exon_8' in exact_matches:\n                for i in deleted_coords:\n                    mapping[i] = 1\n                coordinates.update(deleted_coords)\n                mb = getblocks(coordinates)\n                feat_missing.update(added_feat)\n\n                # Delte from found features\n                del exact_matches[exact_matches.index('exon_8')]\n                del found_feats['exon_8']\n\n                if 'exon_8' in annotated_feats:\n                    del annotated_feats['exon_8']\n                if 'three_prime_UTR' in found_feats:\n                    del found_feats['three_prime_UTR']\n                if 'three_prime_UTR' in annotated_feats:\n                    del annotated_feats['three_prime_UTR']\n\n            refmissing = [f for f in structures[locus]\n                          if f not in annotated_feats]\n\n            if self.verbose and self.verbosity > 1:\n                self.logger.info(\"* Annotation not complete *\")\n\n            # Print out what features were missing by the ref\n            if self.verbose and self.verbosity > 2:\n                self.logger.info(\"Refseq was missing these features = \" + \",\".join(list(refmissing)))\n\n            # Print out what features were ambig matches\n            if self.verbose and self.verbosity > 1 and len(ambig_map) > 1:\n                self.logger.info(\"Features with ambig matches = \" + \",\".join(list(ambig_map)))\n\n            # Print out what features were exact matches\n            if self.verbose and self.verbosity > 2 and len(exact_matches) > 1:\n                self.logger.info(\"Features exact matches = \" + \",\".join(list(exact_matches)))\n\n            # Print out what features have been annotated\n            if self.verbose and self.verbosity > 1 and len(annotated_feats) > 1:\n                self.logger.info(\"Features annotated = \" + \",\".join(list(annotated_feats)))\n\n            # Print out what features are missing\n            if self.verbose and self.verbosity > 1 and len(feat_missing) > 1:\n                self.logger.info(\"Features missing = \" + \",\".join(list(feat_missing)))\n\n            annotation = Annotation(features=annotated_feats,\n                                    covered=seq_covered,\n                                    seq=in_seq,\n                                    missing=feat_missing,\n                                    ambig=ambig_map,\n                                    blocks=mb,\n                                    method=method,\n                                    refmissing=refmissing,\n                                    mapping=mapping,\n                                    exact_match=exact_matches,\n                                    annotation=None)\n        else:\n\n            mb = None\n            # Unmap exon 8\n            if locus in ['HLA-C', 'HLA-A'] and len(in_seq.seq) < 600 \\\n                    and 'exon_8' in exact_matches \\\n                    and 'three_prime_UTR' in annotated_feats\\\n                    and 'three_prime_UTR' not in exact_matches:\n\n                for i in deleted_coords:\n                    mapping[i] = 1\n\n                coordinates.update(deleted_coords)\n                mb = getblocks(coordinates)\n                feat_missing.update(added_feat)\n                del exact_matches[exact_matches.index('exon_8')]\n                del found_feats['exon_8']\n                if 'exon_8' in annotated_feats:\n                    del annotated_feats['exon_8']\n                if 'three_prime_UTR' in found_feats:\n                    del found_feats['three_prime_UTR']\n                if 'three_prime_UTR' in annotated_feats:\n                    del annotated_feats['three_prime_UTR']\n\n            if self.verbose:\n                self.logger.info(\"* No missing blocks after seq_search *\")\n\n            # Print out what features were ambig matches\n            if self.verbose and self.verbosity > 0 and len(ambig_map) > 1:\n                self.logger.info(\"Features with ambig matches = \" + \",\".join(list(ambig_map)))\n\n            # Print out what features were exact matches\n            if self.verbose and self.verbosity > 0 and len(exact_matches) > 1:\n                self.logger.info(\"Features exact matches = \" + \",\".join(list(exact_matches)))\n\n            # Print out what features have been annotated\n            if self.verbose and self.verbosity > 0 and len(annotated_feats) > 1:\n                self.logger.info(\"Features annotated = \" + \",\".join(list(annotated_feats)))\n\n            # Print out what features are missing\n            if self.verbose and self.verbosity > 0 and len(feat_missing) > 1:\n                self.logger.info(\"Features missing = \" + \",\".join(list(feat_missing)))\n\n            annotation = Annotation(features=annotated_feats,\n                                    covered=seq_covered,\n                                    seq=in_seq,\n                                    missing=feat_missing,\n                                    ambig=ambig_map,\n                                    method=method,\n                                    blocks=mb,\n                                    mapping=mapping,\n                                    exact_match=exact_matches,\n                                    annotation=None)\n\n        return annotation", "response": "This method searches for a sequence in a given locus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert parseResult from to ConfGroup type.", "response": "def convert_group(tokens):\n    \"\"\"Converts parseResult from to ConfGroup type.\"\"\"\n    tok = tokens.asList()\n    dic = dict(tok)\n    if not (len(dic) == len(tok)):\n        raise ParseFatalException(\"Names in group must be unique: %s\" % tokens)\n    return ConfGroup(dic)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the name has been changed and ignores the name change if the item is an existing script or a new script", "response": "def name_changed(self, changed_item):\n        \"\"\"\n        checks if name has been changed and ignores the name change if the changed_item is an existing script\n        Args:\n            changed_item:\n        \"\"\"\n        name = str(changed_item.text())\n\n        # if the item has been moved we ignore this because the item only went from one tree to the other without changing names\n        if name != '':\n            if name != self.selected_element_name:\n                self.elements_from_file[name] = self.elements_from_file[self.selected_element_name]\n                del self.elements_from_file[self.selected_element_name]\n                self.selected_element_name = name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay the doc string of the selected element", "response": "def show_info(self):\n        \"\"\"\n        displays the doc string of the selected element\n        \"\"\"\n        sender = self.sender()\n        tree = sender.parent()\n        index = tree.selectedIndexes()\n        info = ''\n        if index != []:\n            index = index[0]\n            name = str(index.model().itemFromIndex(index).text())\n            self.selected_element_name = name\n\n\n\n            if name in self.elements_old:\n                info = self.elements_old[name].__doc__\n\n            #TODO: check if this is portable\n            elif name in self.elements_from_file:\n                class_name = self.elements_from_file[name]['class']\n                if 'filepath' in self.elements_from_file[name]:\n                    filepath = self.elements_from_file[name]['filepath']\n                if 'info' in self.elements_from_file[name]:\n                    info = self.elements_from_file[name]['info']\n                #\n                # path_to_src_scripts = filepath[:filepath.find('\\\\pylabcontrol\\\\scripts\\\\')]\n                # module_name = path_to_src_scripts[path_to_src_scripts.rfind('\\\\')+1:]\n                # module = __import__('{:s}.pylabcontrol.{:s}'.format(module_name, self.elements_type), fromlist=[class_name])\n                # info = getattr(module, class_name).__doc__\n\n\n\n            if info is None:\n                info = name\n\n            if tree == self.tree_infile:\n                self.lbl_info.setText(info)\n                self.tree_loaded.clearSelection()\n\n            elif tree == self.tree_loaded:\n                self.lbl_info.setText(info)\n                self.tree_infile.clearSelection()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nopen a file dialog to get the path to a file and the name of the file and the class name of the class.", "response": "def open_file_dialog(self):\n        \"\"\"\n        opens a file dialog to get the path to a file and\n        \"\"\"\n        dialog = QtWidgets.QFileDialog\n        filename, _ = dialog.getOpenFileName(self, 'Select a file:', self.txt_probe_log_path.text())\n        if str(filename) != '':\n            self.txt_probe_log_path.setText(filename)\n            # load elements from file and display in tree\n            elements_from_file = self.load_elements(filename)\n            self.fill_tree(self.tree_infile, elements_from_file)\n            # append new elements to internal dictionary\n            self.elements_from_file.update(elements_from_file)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_elements(self, filename):\n        input_data = load_b26_file(filename)\n        if isinstance(input_data, dict) and self.elements_type in input_data:\n            return input_data[self.elements_type]\n        else:\n            return {}", "response": "loads the elements from file filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fill_tree(self, tree, input_dict):\n\n        def add_element(item, key, value):\n            child_name = QtGui.QStandardItem(key)\n            child_name.setDragEnabled(False)\n            child_name.setSelectable(False)\n            child_name.setEditable(False)\n\n            if isinstance(value, dict):\n                for ket_child, value_child in value.items():\n                    add_element(child_name, ket_child, value_child)\n                child_value = QtGui.QStandardItem('')\n            else:\n                child_value = QtGui.QStandardItem(str(value))\n                child_value.setData(value)\n\n            child_value.setDragEnabled(False)\n            child_value.setSelectable(False)\n            child_value.setEditable(False)\n            item.appendRow([child_name, child_value])\n\n        for index, (loaded_item, loaded_item_settings) in enumerate(input_dict.items()):\n            # print(index, loaded_item, loaded_item_settings)\n            item = QtGui.QStandardItem(loaded_item)\n\n            for key, value in loaded_item_settings['settings'].items():\n                add_element(item, key, value)\n\n            value = QtGui.QStandardItem('')\n            tree.model().appendRow([item, value])\n\n            if tree == self.tree_loaded:\n                item.setEditable(False)\n            tree.setFirstColumnSpanned(index, self.tree_infile.rootIndex(), True)", "response": "Fills a tree with nested parameters from a dictionary or Parameter object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_values(self):\n        elements_selected = {}\n        for index in range(self.tree_loaded_model.rowCount()):\n            element_name = str(self.tree_loaded_model.item(index).text())\n            if element_name in self.elements_old:\n                elements_selected.update({element_name: self.elements_old[element_name]})\n            elif element_name in self.elements_from_file:\n                elements_selected.update({element_name: self.elements_from_file[element_name]})\n\n\n        return elements_selected", "response": "Returns the selected instruments and instruments from the tree loaded model"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_script_sequence(self):\n\n        def empty_tree(tree_model):\n            # COMMENT_ME\n            def add_children_to_list(item, somelist):\n                if item.hasChildren():\n                    for rownum in range(0, item.rowCount()):\n                        somelist.append(str(item.child(rownum, 0).text()))\n\n            output_list = []\n            root = tree_model.invisibleRootItem()\n            add_children_to_list(root, output_list)\n            tree_model.clear()\n            return output_list\n\n        name = str(self.txt_script_sequence_name.text())\n        new_script_list = empty_tree(self.tree_script_sequence_model)\n        new_script_dict = {}\n        for script in new_script_list:\n            if script in self.elements_old:\n                new_script_dict.update({script: self.elements_old[script]})\n            elif script in self.elements_from_file:\n                new_script_dict.update({script: self.elements_from_file[script]})\n\n        new_script_parameter_dict = {}\n        for index, script in enumerate(new_script_list):\n            new_script_parameter_dict.update({script: index})\n        # QtGui.QTextEdit.toPlainText()\n\n\n        # get the module of the current dialogue\n\n        package = get_python_package(inspect.getmodule(self).__file__)\n\n        assert package is not None # check that we actually find a module\n\n        # class_name = Script.set_up_dynamic_script(factory_scripts, new_script_parameter_list, self.cmb_looping_variable.currentText() == 'Parameter Sweep')\n        new_script_dict = {name: {'class': 'ScriptIterator', 'package': package, 'scripts': new_script_dict,\n                                  'info': str(self.txt_info.toPlainText()),\n                                  'settings': {'script_order': new_script_parameter_dict,\n                                               'iterator_type': str(self.cmb_looping_variable.currentText())}}}\n        self.selected_element_name = name\n        self.fill_tree(self.tree_loaded, new_script_dict)\n        self.elements_from_file.update(new_script_dict)", "response": "Creates a script sequence based on the selected scripts and sends it to the tree_loaded attribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(func=None, name=None):\n    if not name:\n        raise CompilationError('Name is required')\n    if not func:\n        return partial(register, name=name)\n    return FormatRegistry.register(name, func)", "response": "Decorator to expose a function to expose a format\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a individual object based on the indata.", "response": "def get_individual(self, family_id, sample_id, father_id, mother_id, sex, phenotype,\n            genetic_models = None, proband='.', consultand='.', alive='.'):\n        \"\"\"\n        Return a individual object based on the indata.\n        \n        Arguments:\n            family_id (str): The id for this family\n            sample_id (str): The id for this sample\n            father_id (str): The id for this samples father\n            mother_id (str): The id for this samples mother\n            sex (str): The id for the sex of this sample\n            phenotype (str): The id for the phenotype of this sample\n            genetic_models (str): A ';'-separated string with the expected \n            models of inheritance for this sample\n            proband (str): 'Yes', 'No' or '.'\n            consultand (str): 'Yes', 'No' or '.' if the individual is sequenced\n            alive (str): 'Yes', 'No' or '.'\n        \n        returns:\n            individual (Individual): A Individual object with the information \n        \"\"\"\n        if sex not in ['1', '2']:\n            sex = '0'\n        if phenotype not in ['1', '2']:\n            phenotype = '0'\n        if mother_id == '.':\n            mother_id = '0'\n        if father_id == '.':\n            father_id = '0'\n        if genetic_models:\n            genetic_models = genetic_models.split(';')\n        \n        if proband == 'Yes':\n            proband = 'Y'\n        elif proband == 'No':\n            proband = 'N'\n        else:\n            proband = '.'\n        \n        if consultand == 'Yes':\n            consultand = 'Y'\n        elif consultand == 'No':\n            consultand = 'N'\n        else:\n            consultand = '.'\n        \n        if alive == 'Yes':\n            alive = 'Y'\n        elif alive == 'No':\n            alive = 'N'\n        else:\n            alive = '.'\n        \n        individual = Individual(\n                                 sample_id, \n                                 family_id, \n                                 mother_id, \n                                 father_id, \n                                 sex, \n                                 phenotype, \n                                 genetic_models, \n                                 proband, \n                                 consultand, \n                                 alive\n                              )\n        \n        return individual"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_line_length(self, splitted_line, expected_length):\n        if len(splitted_line) != expected_length:\n            raise WrongLineFormat(\n                            message='WRONG FORMATED PED LINE!',\n                            ped_line = '\\t'.join(splitted_line))\n        return", "response": "Check if the line is correctly formated. Throw a SyntaxError if it is not."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ped_parser(self, family_info):\n        \n        for line in family_info:\n            # Check if commented line or empty line:\n            if not line.startswith('#') and not all(c in whitespace for c in line.rstrip()):\n                splitted_line = line.rstrip().split('\\t')\n                if len(splitted_line) != 6:\n                    # Try to split the line on another symbol:\n                    splitted_line = line.rstrip().split()\n                try:\n                    self.check_line_length(splitted_line, 6)\n                except WrongLineFormat as e:\n                    self.logger.error(e)\n                    self.logger.info(\"Ped line: {0}\".format(e.ped_line))\n                    raise e\n                \n                sample_dict = dict(zip(self.header, splitted_line))\n                family_id = sample_dict['family_id']\n                \n                if sample_dict['family_id'] not in self.families:\n                    self.families[family_id] = Family(family_id, {})\n                \n                ind_object = self.get_individual(**sample_dict)\n                self.individuals[ind_object.individual_id] = ind_object\n                self.families[ind_object.family].add_individual(ind_object)", "response": "Parse. ped formatted family info."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses alternative formatted family info This parses a information with more than six columns. For alternative information header comlumn must exist and each row must have the same amount of columns as the header. First six columns must be the same as in the ped format. Arguments: family_info (iterator): An iterator with family info", "response": "def alternative_parser(self, family_file):\n        \"\"\"\n        Parse alternative formatted family info\n        \n        This parses a information with more than six columns. \n        For alternative information header comlumn must exist and each row \n        must have the same amount of columns as the header. \n        First six columns must be the same as in the ped format.\n        \n        Arguments:\n            family_info (iterator): An iterator with family info\n        \"\"\"\n        \n        alternative_header = None\n        \n        for line in family_file:\n            if line.startswith('#'):\n                alternative_header = line[1:].rstrip().split('\\t')\n                self.logger.info(\"Alternative header found: {0}\".format(line))\n            elif line.strip():\n                if not alternative_header:\n                    raise WrongLineFormat(message=\"Alternative ped files must have \"\\\n                                        \"headers! Please add a header line.\")\n                \n                splitted_line = line.rstrip().split('\\t')\n                if len(splitted_line) < 6:\n                    # Try to split the line on another symbol:\n                    splitted_line = line.rstrip().split()\n                try:\n                    self.check_line_length(splitted_line, len(alternative_header))\n                except SyntaxError as e:\n                    self.logger.error('Number of entrys differ from header.')\n                    self.logger.error(\"Header:\\n{0}\".format('\\t'.join(alternative_header)))\n                    self.logger.error(\"Ped Line:\\n{0}\".format('\\t'.join(splitted_line)))\n                    self.logger.error(\"Length of Header: {0}. Length of \"\\\n                                      \"Ped line: {1}\".format(\n                                          len(alternative_header), \n                                          len(splitted_line))\n                                    )\n                    raise e\n                \n                if len(line) > 1:\n                    \n                    sample_dict = dict(zip(self.header, splitted_line[:6]))\n                    \n                    family_id = sample_dict['family_id']\n                    \n                    all_info = dict(zip(alternative_header, splitted_line))\n                    \n                    if sample_dict['family_id'] not in self.families:\n                        self.families[family_id] = Family(family_id, {})\n                    \n                    sample_dict['genetic_models'] = all_info.get('InheritanceModel', None)\n                    # Try other header naming:\n                    if not sample_dict['genetic_models']:\n                        sample_dict['genetic_models'] = all_info.get('Inheritance_model', None)\n                        \n                    sample_dict['proband'] = all_info.get('Proband', '.')\n                    sample_dict['consultand'] = all_info.get('Consultand', '.')\n                    sample_dict['alive'] = all_info.get('Alive', '.')\n                    \n                    ind_object = self.get_individual(**sample_dict)\n                    \n                    self.individuals[ind_object.individual_id] = ind_object\n                    self.families[ind_object.family].add_individual(ind_object)\n                    \n                    if sample_dict['genetic_models']:\n                        for model in self.get_models(sample_dict['genetic_models']):\n                            self.families[ind_object.family].models_of_inheritance.add(model)\n                    \n                    # If requested, we try is it is an id in the CMMS format:\n                    sample_id_parts = ind_object.individual_id.split('-')\n                    if self.cmms_check and (len(sample_id_parts) == 3):\n                        # If the id follow the CMMS convention we can\n                        # do a sanity check\n                        if self.check_cmms_id(ind_object.individual_id):\n                            self.logger.debug(\"Id follows CMMS convention: {0}\".format(\n                                ind_object.individual_id\n                            ))\n                            self.logger.debug(\"Checking CMMS id affections status\")\n                            try:\n                                self.check_cmms_affection_status(ind_object)\n                            except WrongAffectionStatus as e:\n                                self.logger.error(\"Wrong affection status for\"\\\n                                \" {0}. Affection status can be in\"\\\n                                \" {1}\".format(e.cmms_id, e.valid_statuses))\n                                raise e\n                            except WrongPhenotype as e:\n                                self.logger.error(\"Affection status for {0} \"\\\n                                \"({1}) disagrees with phenotype ({2})\".format(\n                                    e.cmms_id, e.phenotype, e.affection_status\n                                ))\n                                raise e\n                            \n                            try:\n                                self.check_cmms_gender(ind_object)\n                            except WrongGender as e:\n                                self.logger.error(\"Gender code for id {0}\"\\\n                                \"({1}) disagrees with sex:{2}\".format(\n                                    e.cmms_id, e.sex_code, e.sex\n                                ))\n                                raise e\n                                \n                    for i in range(6, len(splitted_line)):\n                        ind_object.extra_info[alternative_header[i]] = splitted_line[i]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_cmms_id(self, ind_id):\n        ind_id = ind_id.split('-')\n        # This in A (=affected), U (=unaffected) or X (=unknown)\n        family_id = ind_id[0]\n        try:\n            int(family_id)\n        except ValueError:\n            return False\n        affection_status = ind_id[-1][-1]\n        try:\n            type(affection_status.isalpha())\n        except ValueError:\n            return False\n        \n        return True", "response": "Check if the ID is correct for the cmms standard."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_cmms_affection_status(self, ind_object):\n        valid_affection_statuses = ['A', 'U', 'X']\n        ind_id = ind_object.individual_id.split('-')\n        phenotype = ind_object.phenotype\n        affection_status = ind_id[-1][-1]\n        \n        if affection_status not in valid_affection_statuses:\n            raise WrongAffectionStatus(ind_object.individual_id, \n                                        valid_affection_statuses)\n        \n        if (affection_status == 'A' and phenotype != 2 or \n            affection_status == 'U' and phenotype != 1):\n            raise WrongPhenotype(ind_object.individual_id, phenotype, \n                                 affection_status)\n        \n        return True", "response": "Checks if the affection status of an object is correct."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_cmms_gender(self, ind_object):\n        ind_id = ind_object.individual_id.split('-')\n        sex = ind_object.sex\n        sex_code = int(ind_id[-1][:-1])# Males allways have odd numbers and womans even\n        if (sex_code % 2 == 0 and sex != 2) or (sex_code % 2 != 0 and sex != 1):\n            raise WrongGender(ind_object.individual_id, sex, sex_code)\n        \n        return True", "response": "Checks if the sex of the assessment is correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks what genetic models are found and return them as a set.", "response": "def get_models(self, genetic_models):\n        \"\"\"\n        Check what genetic models that are found and return them as a set.\n        \n        Args:\n            genetic_models  : A string with genetic models\n        \n        Yields:\n             correct_model_names  : A set with the correct model names\n        \"\"\"\n        correct_model_names = set()\n        genetic_models = genetic_models.split(';')\n        correct_model_names = set()\n        for model in genetic_models:\n            # We need to allow typos\n            if model in self.legal_ar_hom_names:\n                model = 'AR_hom'\n            elif model in self.legal_ar_hom_dn_names:\n                model = 'AR_hom_dn'\n            elif model in self.legal_ad_names:\n                model = 'AD_dn'\n            elif model in self.legal_compound_names:\n                model = 'AR_comp'\n            elif model in self.legal_x_names:\n                model = 'X'\n            elif model in self.legal_na_names:\n                model = 'NA'\n            else:\n                self.logger.warning(\"Incorrect model name: {0}.\"\\\n                                    \" Ignoring model.\".format(model))\n            correct_model_names.add(model)\n        return correct_model_names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the information from the pedigree file as a dictionary.", "response": "def to_dict(self):\n        \"\"\"\n        Return the information from the pedigree file as a dictionary.\n        family id is key and a list with dictionarys for each individual \n        as value.\n        \n        Returns:\n            families (dict): A dictionary with the families\n        \"\"\"\n        \n        self.logger.debug(\"Return the information as a dictionary\")\n        families = {}\n        for family_id in self.families:\n            family = []\n            for individual_id in self.families[family_id].individuals:\n                individual = self.families[family_id].individuals[individual_id]\n                family.append(individual.to_json())\n                self.logger.debug(\"Adding individual {0} to family {1}\".format(\n                    individual_id, family_id\n                ))\n            self.logger.debug(\"Adding family {0}\".format(family_id))\n            families[family_id] = family\n        \n        return families"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_madeline(self):\n        \n        madeline_header = [\n            'FamilyID', \n            'IndividualID', \n            'Gender', \n            'Father', \n            'Mother', \n            'Affected',\n            'Proband',\n            'Consultand',\n            'Alive'\n        ]\n        \n        yield '\\t'.join(madeline_header)\n        \n        for family_id in self.families:\n            for individual_id in self.families[family_id].individuals:\n                individual = self.families[family_id].individuals[individual_id]\n                \n                yield individual.to_madeline()", "response": "Return a generator with the info in madeline format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_ped(self):\n        \n        ped_header = [\n            '#FamilyID',\n            'IndividualID',\n            'PaternalID',\n            'MaternalID', \n            'Sex',\n            'Phenotype',\n        ]\n        \n        extra_headers = [\n            'InheritanceModel',\n            'Proband',\n            'Consultand',\n            'Alive'\n        ]\n        \n        for individual_id in self.individuals:\n            individual = self.individuals[individual_id]\n            for info in individual.extra_info:\n                if info in extra_headers:\n                    if info not in ped_header:\n                        ped_header.append(info)\n        \n        self.logger.debug(\"Ped headers found: {0}\".format(\n            ', '.join(ped_header)\n        ))\n        \n        \n        \n        yield '\\t'.join(ped_header)\n        \n        for family_id in self.families:\n            for individual_id in self.families[family_id].individuals:\n                individual = self.families[family_id].individuals[individual_id].to_json()\n                ped_info = []\n                ped_info.append(individual['family_id'])\n                ped_info.append(individual['id'])\n                ped_info.append(individual['father'])\n                ped_info.append(individual['mother'])\n                ped_info.append(individual['sex'])\n                ped_info.append(individual['phenotype'])\n            \n                if len(ped_header) > 6:\n                    for header in ped_header[6:]:\n                        ped_info.append(individual['extra_info'].get(header, '.'))\n            \n                yield '\\t'.join(ped_info)", "response": "Return a generator with the family info in ped format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deck_vote_tag(deck: Deck) -> str:\n    '''deck vote tag address'''\n\n    if deck.id is None:\n        raise Exception(\"deck.id is required\")\n\n    deck_vote_tag_privkey = sha256(unhexlify(deck.id) + b\"vote_init\").hexdigest()\n    deck_vote_tag_address = Kutil(network=deck.network, privkey=bytearray.fromhex(deck_vote_tag_privkey))\n    return deck_vote_tag_address.address", "response": "deck vote tag address"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode vote init tx op_return protobuf message and validate it.", "response": "def parse_vote_info(protobuf: bytes) -> dict:\n    '''decode vote init tx op_return protobuf message and validate it.'''\n\n    vote = pavoteproto.Vote()\n    vote.ParseFromString(protobuf)\n\n    assert vote.version > 0, {\"error\": \"Vote info incomplete, version can't be 0.\"}\n    assert vote.start_block < vote.end_block, {\"error\": \"vote can't end in the past.\"}\n\n    return {\n        \"version\": vote.version,\n        \"description\": vote.description,\n        \"count_mode\": vote.MODE.Name(vote.count_mode),\n        \"choices\": vote.choices,\n        \"start_block\": vote.start_block,\n        \"end_block\": vote.end_block,\n        \"vote_metainfo\": vote.vote_metainfo\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vote_init(vote: Vote, inputs: dict, change_address: str) -> bytes:\n    '''initialize vote transaction, must be signed by the deck_issuer privkey'''\n\n    network_params = net_query(vote.deck.network)\n    deck_vote_tag_address = deck_vote_tag(vote.deck)\n\n    tx_fee = network_params.min_tx_fee  # settle for min tx fee for now\n\n    for utxo in inputs['utxos']:\n        utxo['txid'] = unhexlify(utxo['txid'])\n        utxo['scriptSig'] = unhexlify(utxo['scriptSig'])\n\n    outputs = [\n        {\"redeem\": 0.01, \"outputScript\": transactions.monosig_script(deck_vote_tag_address)},\n        {\"redeem\": 0, \"outputScript\": transactions.op_return_script(vote.to_protobuf)},\n        {\"redeem\": float(inputs['total']) - float(tx_fee) - float(0.01),\n         \"outputScript\": transactions.monosig_script(change_address)\n         }]\n\n    return transactions.make_raw_transaction(inputs['utxos'], outputs)", "response": "initialize vote transaction must be signed by the deck_issuer privkey"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind vote_inits on this deck", "response": "def find_vote_inits(provider: Provider, deck: Deck) -> Iterable[Vote]:\n    '''find vote_inits on this deck'''\n\n    vote_ints = provider.listtransactions(deck_vote_tag(deck))\n\n    for txid in vote_ints:\n        try:\n            raw_vote = provider.getrawtransaction(txid)\n            vote = parse_vote_info(read_tx_opreturn(raw_vote))\n            vote[\"vote_id\"] = txid\n            vote[\"sender\"] = find_tx_sender(provider, raw_vote)\n            vote[\"deck\"] = deck\n            yield Vote(**vote)\n        except AssertionError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding and verify vote_casts on this vote_choice_address", "response": "def find_vote_casts(provider: Provider, vote: Vote, choice_index: int) -> Iterable[VoteCast]:\n    '''find and verify vote_casts on this vote_choice_address'''\n\n    vote_casts = provider.listtransactions(vote.vote_choice_address[choice_index])\n    for tx in vote_casts:\n        raw_tx = provider.getrawtransaction(tx, 1)\n\n        sender = find_tx_sender(provider, raw_tx)\n        confirmations = raw_tx[\"confirmations\"]\n        blocknum = provider.getblock(raw_tx[\"blockhash\"])[\"height\"]\n        yield VoteCast(vote, sender, blocknum, confirmations, raw_tx[\"blocktime\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes vote into protobuf", "response": "def to_protobuf(self) -> str:\n        '''encode vote into protobuf'''\n\n        vote = pavoteproto.Vote()\n        vote.version = self.version\n        vote.description = self.description\n        vote.count_mode = vote.MODE.Value(self.count_mode)\n        vote.start_block = self.start_block\n        vote.end_block = self.end_block\n        vote.choices.extend(self.choices)\n\n        if not isinstance(self.vote_metainfo, bytes):\n            vote.vote_metainfo = self.vote_metainfo.encode()\n        else:\n            vote.vote_metainfo = self.vote_metainfo\n\n        proto = vote.SerializeToString()\n\n        if len(proto) > 80:\n            warnings.warn('\\nMetainfo size exceeds maximum of 80 bytes allowed by OP_RETURN.')\n\n        return proto"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self) -> dict:\n        '''vote info as dict'''\n\n        return {\n            \"version\": self.version,\n            \"description\": self.description,\n            \"count_mode\": self.count_mode,\n            \"start_block\": self.start_block,\n            \"end_block\": self.end_block,\n            \"choices\": self.choices,\n            \"vote_metainfo\": self.vote_metainfo\n        }", "response": "vote info as dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef vote_choice_address(self) -> List[str]:\n        '''calculate the addresses on which the vote is casted.'''\n\n        if self.vote_id is None:\n            raise Exception(\"vote_id is required\")\n\n        addresses = []\n        vote_init_txid = unhexlify(self.vote_id)\n\n        for choice in self.choices:\n            vote_cast_privkey = sha256(vote_init_txid + bytes(\n                                    list(self.choices).index(choice))\n                                    ).hexdigest()\n            addresses.append(Kutil(network=self.deck.network,\n                                   privkey=bytearray.fromhex(vote_cast_privkey)).address)\n\n        return addresses", "response": "calculate the addresses on which the vote is casted."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid(self) -> bool:\n        '''check if VoteCast is valid'''\n\n        if not (self.blocknum >= self.vote.start_block and\n                self.blocknum <= self.vote.end_block):\n            return False\n\n        if not self.confirmations >= 6:\n            return False\n\n        return True", "response": "check if VoteCast is valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef seqrecord(self, allele, locus):\n        try:\n            db = self.server[self.dbversion + \"_\" + locus]\n        except:\n            self.logger.error(\"The database \" + self.dbversion + \"_\"\n                              + locus + \" does not exist!\")\n            return ''\n\n        seqrecord = db.lookup(name=allele)\n        return seqrecord", "response": "Gets the Annotation from the found sequence\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the Annotation from the found sequence record", "response": "def seqannotation(self, seqrecord, allele, loc):\n        \"\"\"\n        Gets the Annotation from the found sequence\n\n        :return: The Annotation from the found sequence\n        :rtype: Annotation\n        \"\"\"\n        #seqrecord = self.seqrecord(allele, loc)\n        complete_annotation = get_features(seqrecord)\n        annotation = Annotation(annotation=complete_annotation,\n                                method='match',\n                                complete_annotation=True)\n\n        if self.alignments:\n            alignment = {f: self.annoated_alignments[loc][allele][f]['Seq']\n                         for f in self.annoated_alignments[loc][allele].keys()}\n            annotation.aligned = alignment\n\n        return annotation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the values of the items of the resource table.", "response": "def update(self, *args):\n        \"\"\"\n        updates the values of the parameter, just as a regular dictionary\n        \"\"\"\n        for d in args:\n            for (key, value) in d.items():\n                self.__setitem__(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the value is valid", "response": "def is_valid(value, valid_values):\n        \"\"\"\n        check is the value is valid\n        Args:\n            value: value to be tested\n            valid_values: allowed valid values (type or list of values)\n\n        Returns:\n\n        \"\"\"\n\n        valid = False\n\n        if isinstance(valid_values, type) and type(value) is valid_values:\n            valid = True\n        elif isinstance(valid_values, type) and valid_values == float and type(value) == int:\n            #special case to allow ints as float inputs\n            valid = True\n        elif isinstance(value, dict) and isinstance(valid_values, dict):\n            # check that all values actually exist in valid_values\n            # assert value.keys() & valid_values.keys() == value.keys() # python 3 syntax\n            assert set(value.keys()) & set(valid_values.keys()) == set(value.keys()) # python 2\n            # valid = True\n            for k ,v in value.items():\n                valid = Parameter.is_valid(v, valid_values[k])\n                if valid ==False:\n                    break\n\n        elif isinstance(value, dict) and valid_values == Parameter:\n            valid = True\n\n        elif isinstance(valid_values, list) and value in valid_values:\n            valid = True\n\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting error message and raise RuntimeError.", "response": "def error(msg, log_file=None):\n        \"\"\"Print, output error message and raise RuntimeError.\"\"\"\n        UtilClass.print_msg(msg + os.linesep)\n        if log_file is not None:\n            UtilClass.writelog(log_file, msg, 'append')\n        raise RuntimeError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the existence of the given file and directory path.", "response": "def check_infile_and_wp(curinf, curwp):\n        \"\"\"Check the existence of the given file and directory path.\n           1. Raise Runtime exception of both not existed.\n           2. If the ``curwp`` is None, the set the base folder of ``curinf`` to it.\n        \"\"\"\n        if not os.path.exists(curinf):\n            if curwp is None:\n                TauDEM.error('You must specify one of the workspace and the '\n                             'full path of input file!')\n            curinf = curwp + os.sep + curinf\n            curinf = os.path.abspath(curinf)\n            if not os.path.exists(curinf):\n                TauDEM.error('Input files parameter %s is not existed!' % curinf)\n        else:\n            curinf = os.path.abspath(curinf)\n            if curwp is None:\n                curwp = os.path.dirname(curinf)\n        return curinf, curwp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun TauDEM function. - 1. The command will not execute if any input file does not exist. - 2. An error will be detected after running the TauDEM command if any output file does not exist; Args: function_name (str): Full path of TauDEM function. in_files (dict, required): Dict of pairs of parameter id (string) and file path (string or list) for input files, e.g.:: {'-z': '/full/path/to/dem.tif'} wp (str, optional): Workspace for outputs. If not specified, the directory of the first input file in ``in_files`` will be used. in_params (dict, optional): Dict of pairs of parameter id (string) and value (or None for a flag parameter without a value) for input parameters, e.g.:: {'-nc': None} {'-thresh': threshold} {'-m': 'ave' 's', '-nc': None} out_files (dict, optional): Dict of pairs of parameter id (string) and file path (string or list) for output files, e.g.:: {'-fel': 'filleddem.tif'} {'-maxS': ['harden.tif', 'maxsimi.tif']} mpi_params (dict, optional): Dict of pairs of parameter id (string) and value or path for MPI setting, e.g.:: {'mpipath':'/soft/bin','hostfile':'/soft/bin/cluster.node','n':4} {'mpipath':'/soft/bin', 'n':4} {'n':4} log_params (dict, optional): Dict of pairs of parameter id (string) and value or path for runtime and log output parameters. e.g.:: {'logfile': '/home/user/log.txt', 'runtimefile': '/home/user/runtime.txt'} Returns: True if TauDEM run successfully, otherwise False.", "response": "def run(function_name, in_files, wp=None, in_params=None, out_files=None, mpi_params=None,\n            log_params=None):\n        \"\"\"\n        Run TauDEM function.\n\n         - 1. The command will not execute if any input file does not exist.\n         - 2. An error will be detected after running the TauDEM command if\n              any output file does not exist;\n\n        Args:\n            function_name (str): Full path of TauDEM function.\n            in_files (dict, required): Dict of pairs of parameter id (string) and file path\n                (string or list) for input files, e.g.::\n\n                    {'-z': '/full/path/to/dem.tif'}\n\n            wp (str, optional): Workspace for outputs. If not specified, the directory of the\n                first input file in ``in_files`` will be used.\n            in_params (dict, optional): Dict of pairs of parameter id (string) and value\n                (or None for a flag parameter without a value) for input parameters, e.g.::\n\n                    {'-nc': None}\n                    {'-thresh': threshold}\n                    {'-m': 'ave' 's', '-nc': None}\n\n            out_files (dict, optional): Dict of pairs of parameter id (string) and file\n                path (string or list) for output files, e.g.::\n\n                    {'-fel': 'filleddem.tif'}\n                    {'-maxS': ['harden.tif', 'maxsimi.tif']}\n\n            mpi_params (dict, optional): Dict of pairs of parameter id (string) and value or\n                path for MPI setting, e.g.::\n\n                    {'mpipath':'/soft/bin','hostfile':'/soft/bin/cluster.node','n':4}\n                    {'mpipath':'/soft/bin', 'n':4}\n                    {'n':4}\n\n            log_params (dict, optional): Dict of pairs of parameter id (string) and value or\n                path for runtime and log output parameters. e.g.::\n\n                    {'logfile': '/home/user/log.txt',\n                     'runtimefile': '/home/user/runtime.txt'}\n\n        Returns:\n            True if TauDEM run successfully, otherwise False.\n        \"\"\"\n        # Check input files\n        if in_files is None:\n            TauDEM.error('Input files parameter is required!')\n        if not isinstance(in_files, dict):\n            TauDEM.error('The input files parameter must be a dict!')\n        for (pid, infile) in iteritems(in_files):\n            if infile is None:\n                continue\n            if isinstance(infile, list) or isinstance(infile, tuple):\n                for idx, inf in enumerate(infile):\n                    if inf is None:\n                        continue\n                    inf, wp = TauDEM.check_infile_and_wp(inf, wp)\n                    in_files[pid][idx] = inf\n                continue\n            if os.path.exists(infile):\n                infile, wp = TauDEM.check_infile_and_wp(infile, wp)\n                in_files[pid] = os.path.abspath(infile)\n            else:\n                # For more flexible input files extension.\n                # e.g., -inputtags 1 <path/to/tag1.tif> 2 <path/to/tag2.tif> ...\n                # in such unpredictable circumstance, we cannot check the existance of\n                # input files, so the developer will check it in other place.\n                if len(StringClass.split_string(infile, ' ')) > 1:\n                    continue\n                else:  # the infile still should be a existing file, so check in workspace\n                    if wp is None:\n                        TauDEM.error('Workspace should not be None!')\n                    infile = wp + os.sep + infile\n                    if not os.path.exists(infile):\n                        TauDEM.error('Input files parameter %s: %s is not existed!' %\n                                     (pid, infile))\n                    in_files[pid] = os.path.abspath(infile)\n        # Make workspace dir if not existed\n        UtilClass.mkdir(wp)\n        # Check the log parameter\n        log_file = None\n        runtime_file = None\n        if log_params is not None:\n            if not isinstance(log_params, dict):\n                TauDEM.error('The log parameter must be a dict!')\n            if 'logfile' in log_params and log_params['logfile'] is not None:\n                log_file = log_params['logfile']\n                # If log_file is just a file name, then save it in the default workspace.\n                if os.sep not in log_file:\n                    log_file = wp + os.sep + log_file\n                    log_file = os.path.abspath(log_file)\n            if 'runtimefile' in log_params and log_params['runtimefile'] is not None:\n                runtime_file = log_params['runtimefile']\n                # If log_file is just a file name, then save it in the default workspace.\n                if os.sep not in runtime_file:\n                    runtime_file = wp + os.sep + runtime_file\n                    runtime_file = os.path.abspath(runtime_file)\n\n        # remove out_files to avoid any file IO related error\n        new_out_files = list()\n        if out_files is not None:\n            if not isinstance(out_files, dict):\n                TauDEM.error('The output files parameter must be a dict!')\n            for (pid, out_file) in iteritems(out_files):\n                if out_file is None:\n                    continue\n                if isinstance(out_file, list) or isinstance(out_file, tuple):\n                    for idx, outf in enumerate(out_file):\n                        if outf is None:\n                            continue\n                        outf = FileClass.get_file_fullpath(outf, wp)\n                        FileClass.remove_files(outf)\n                        out_files[pid][idx] = outf\n                        new_out_files.append(outf)\n                else:\n                    out_file = FileClass.get_file_fullpath(out_file, wp)\n                    FileClass.remove_files(out_file)\n                    out_files[pid] = out_file\n                    new_out_files.append(out_file)\n\n        # concatenate command line\n        commands = list()\n        # MPI header\n        if mpi_params is not None:\n            if not isinstance(mpi_params, dict):\n                TauDEM.error('The MPI settings parameter must be a dict!')\n            if 'mpipath' in mpi_params and mpi_params['mpipath'] is not None:\n                commands.append(mpi_params['mpipath'] + os.sep + 'mpiexec')\n            else:\n                commands.append('mpiexec')\n            if 'hostfile' in mpi_params and mpi_params['hostfile'] is not None \\\n                    and not StringClass.string_match(mpi_params['hostfile'], 'none') \\\n                    and os.path.isfile(mpi_params['hostfile']):\n                commands.append('-f')\n                commands.append(mpi_params['hostfile'])\n            if 'n' in mpi_params and mpi_params['n'] > 1:\n                commands.append('-n')\n                commands.append(str(mpi_params['n']))\n            else:  # If number of processor is less equal than 1, then do not call mpiexec.\n                commands = []\n        # append TauDEM function name, which can be full path or just one name\n        commands.append(function_name)\n        # append input files\n        for (pid, infile) in iteritems(in_files):\n            if infile is None:\n                continue\n            if pid[0] != '-':\n                pid = '-' + pid\n            commands.append(pid)\n            if isinstance(infile, list) or isinstance(infile, tuple):\n                commands.append(' '.join(tmpf for tmpf in infile))\n            else:\n                commands.append(infile)\n        # append input parameters\n        if in_params is not None:\n            if not isinstance(in_params, dict):\n                TauDEM.error('The input parameters must be a dict!')\n            for (pid, v) in iteritems(in_params):\n                if pid[0] != '-':\n                    pid = '-' + pid\n                commands.append(pid)\n                # allow for parameter which is an flag without value\n                if v != '' and v is not None:\n                    if MathClass.isnumerical(v):\n                        commands.append(str(v))\n                    else:\n                        commands.append(v)\n        # append output parameters\n        if out_files is not None:\n            for (pid, outfile) in iteritems(out_files):\n                if outfile is None:\n                    continue\n                if pid[0] != '-':\n                    pid = '-' + pid\n                commands.append(pid)\n                if isinstance(outfile, list) or isinstance(outfile, tuple):\n                    commands.append(' '.join(tmpf for tmpf in outfile))\n                else:\n                    commands.append(outfile)\n        # run command\n        runmsg = UtilClass.run_command(commands)\n        TauDEM.log(runmsg, log_file)\n        TauDEM.output_runtime_to_log(function_name, runmsg, runtime_file)\n        # Check out_files, raise RuntimeError if not exist.\n        for of in new_out_files:\n            if not os.path.exists(of):\n                TauDEM.error('%s failed, and the %s was not generated!' % (function_name, of))\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pitremove(np, dem, filleddem, workingdir=None, mpiexedir=None, exedir=None, log_file=None,\n                  runtime_file=None, hostfile=None):\n        \"\"\"Run pit remove using the flooding approach \"\"\"\n        fname = TauDEM.func_name('pitremove')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-z': dem}, workingdir,\n                          None,\n                          {'-fel': filleddem},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "response": "Run the flooding approach of the pit remove command"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef d8flowdir(np, filleddem, flowdir, slope, workingdir=None, mpiexedir=None, exedir=None,\n                  log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run D8 flow direction\"\"\"\n        fname = TauDEM.func_name('d8flowdir')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-fel': filleddem}, workingdir,\n                          None,\n                          {'-p': flowdir, '-sd8': slope},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "response": "Run D8 flow direction"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dinfflowdir(np, filleddem, flowangle, slope, workingdir=None, mpiexedir=None, exedir=None,\n                    log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run Dinf flow direction\"\"\"\n        fname = TauDEM.func_name('dinfflowdir')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-fel': filleddem}, workingdir,\n                          None,\n                          {'-ang': flowangle, '-slp': slope},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "response": "Run Dinf flow direction"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef aread8(np, flowdir, acc, outlet=None, streamskeleton=None, edgecontaimination=False,\n               workingdir=None, mpiexedir=None, exedir=None,\n               log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run Accumulate area according to D8 flow direction\"\"\"\n        # -nc means do not consider edge contaimination\n        if not edgecontaimination:\n            in_params = {'-nc': None}\n        else:\n            in_params = None\n        fname = TauDEM.func_name('aread8')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-p': flowdir, '-o': outlet, '-wg': streamskeleton}, workingdir,\n                          in_params,\n                          {'-ad8': acc},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "response": "Run Accumulate area according to D8 flow direction"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef areadinf(np, angfile, sca, outlet=None, wg=None, edgecontaimination=False,\n                 workingdir=None, mpiexedir=None, exedir=None,\n                 log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run Accumulate area according to Dinf flow direction\"\"\"\n        # -nc means do not consider edge contaimination\n        if edgecontaimination:\n            in_params = {'-nc': None}\n        else:\n            in_params = None\n        fname = TauDEM.func_name('areadinf')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-ang': angfile, '-o': outlet, '-wg': wg}, workingdir,\n                          in_params,\n                          {'-sca': sca},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "response": "Run Accumulate area according to Dinf flow direction"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading an ad8 contributing area file and generates a mask layer for the largest ad8 value.", "response": "def connectdown(np, p, acc, outlet, wtsd=None, workingdir=None, mpiexedir=None,\n                    exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Reads an ad8 contributing area file,\n        identifies the location of the largest ad8 value as the outlet of the largest watershed\"\"\"\n        # If watershed is not specified, use acc to generate a mask layer.\n        if wtsd is None or not os.path.isfile(wtsd):\n            p, workingdir = TauDEM.check_infile_and_wp(p, workingdir)\n            wtsd = workingdir + os.sep + 'wtsd_default.tif'\n            RasterUtilClass.get_mask_from_raster(p, wtsd, True)\n        fname = TauDEM.func_name('connectdown')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-p': p, '-ad8': acc, '-w': wtsd},\n                          workingdir,\n                          None,\n                          {'-o': outlet},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning gridnet with specified parameters", "response": "def gridnet(np, pfile, plenfile, tlenfile, gordfile, outlet=None, workingdir=None,\n                mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run gridnet\"\"\"\n        fname = TauDEM.func_name('gridnet')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-p': pfile, '-o': outlet}, workingdir,\n                          None,\n                          {'-plen': plenfile, '-tlen': tlenfile, '-gord': gordfile},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun threshold for stream raster", "response": "def threshold(np, acc, stream_raster, threshold=100., workingdir=None,\n                  mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run threshold for stream raster\"\"\"\n        fname = TauDEM.func_name('threshold')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-ssa': acc}, workingdir,\n                          {'-thresh': threshold},\n                          {'-src': stream_raster},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun move the given outlets to stream", "response": "def moveoutletstostrm(np, flowdir, streamRaster, outlet, modifiedOutlet,\n                          workingdir=None, mpiexedir=None,\n                          exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run move the given outlets to stream\"\"\"\n        fname = TauDEM.func_name('moveoutletstostrm')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-p': flowdir, '-src': streamRaster, '-o': outlet},\n                          workingdir,\n                          None,\n                          {'-om': modifiedOutlet},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting distance method to h v p and s.", "response": "def convertdistmethod(method_str):\n        \"\"\"Convert distance method to h, v, p, and s.\"\"\"\n        if StringClass.string_match(method_str, 'Horizontal'):\n            return 'h'\n        elif StringClass.string_match(method_str, 'Vertical'):\n            return 'v'\n        elif StringClass.string_match(method_str, 'Pythagoras'):\n            return 'p'\n        elif StringClass.string_match(method_str, 'Surface'):\n            return 's'\n        elif method_str.lower() in ['h', 'v', 'p', 's']:\n            return method_str.lower()\n        else:\n            return 's'"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convertstatsmethod(method_str):\n        if StringClass.string_match(method_str, 'Average'):\n            return 'ave'\n        elif StringClass.string_match(method_str, 'Maximum'):\n            return 'max'\n        elif StringClass.string_match(method_str, 'Minimum'):\n            return 'min'\n        elif method_str.lower() in ['ave', 'max', 'min']:\n            return method_str.lower()\n        else:\n            return 'ave'", "response": "Convert statistics method to ave min and max."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef d8hdisttostrm(np, p, src, dist, thresh, workingdir=None,\n                      mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run D8 horizontal distance down to stream.\n        \"\"\"\n        fname = TauDEM.func_name('d8hdisttostrm')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-p': p, '-src': src},\n                          workingdir,\n                          {'-thresh': thresh},\n                          {'-dist': dist},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "response": "Run D8 horizontal distance down to stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning D8 distance down to stream by different method for distance.", "response": "def d8distdowntostream(np, p, fel, src, dist, distancemethod, thresh, workingdir=None,\n                           mpiexedir=None, exedir=None,\n                           log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run D8 distance down to stream by different method for distance.\n        This function is extended from d8hdisttostrm by Liangjun.\n\n        Please clone `TauDEM by lreis2415`_ and compile for this program.\n\n        .. _TauDEM by lreis2415:\n           https://github.com/lreis2415/TauDEM\n        \"\"\"\n        fname = TauDEM.func_name('d8distdowntostream')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-fel': fel, '-p': p, '-src': src},\n                          workingdir,\n                          {'-thresh': thresh, '-m': TauDEM.convertdistmethod(distancemethod)},\n                          {'-dist': dist},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun D - inf distance down to stream", "response": "def dinfdistdown(np, ang, fel, slp, src, statsm, distm, edgecontamination, wg, dist,\n                     workingdir=None, mpiexedir=None, exedir=None,\n                     log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run D-inf distance down to stream\"\"\"\n        in_params = {'-m': '%s %s' % (TauDEM.convertstatsmethod(statsm),\n                                      TauDEM.convertdistmethod(distm))}\n        if StringClass.string_match(edgecontamination, 'false') or edgecontamination is False:\n            in_params['-nc'] = None\n        fname = TauDEM.func_name('dinfdistdown')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-fel': fel, '-slp': slp, '-ang': ang, '-src': src, '-wg': wg},\n                          workingdir,\n                          in_params,\n                          {'-dd': dist},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef peukerdouglas(np, fel, streamSkeleton, workingdir=None, mpiexedir=None, exedir=None,\n                      log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run peuker-douglas function\"\"\"\n        fname = TauDEM.func_name('peukerdouglas')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-fel': fel}, workingdir,\n                          None,\n                          {'-ss': streamSkeleton},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "response": "Run peuker - douglas function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndrop analysis for optimal threshold for extracting stream.", "response": "def dropanalysis(np, fel, p, ad8, ssa, outlet, minthresh, maxthresh, numthresh,\n                     logspace, drp, workingdir=None,\n                     mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Drop analysis for optimal threshold for extracting stream.\"\"\"\n        parstr = '%f %f %f' % (minthresh, maxthresh, numthresh)\n        if logspace == 'false':\n            parstr += ' 1'\n        else:\n            parstr += ' 0'\n        fname = TauDEM.func_name('dropanalysis')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-fel': fel, '-p': p, '-ad8': ad8, '-ssa': ssa, '-o': outlet},\n                          workingdir,\n                          {'-par': parstr},\n                          {'-drp': drp},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nkeeping the same color for the same graph.", "response": "def keep_color(ax=None):\n    ''' Keep the same color for the same graph. \n    Warning: due to the structure of Python iterators I couldn't help but\n    iterate over all the cycle twice. One first time to get the number of elements\n    in the cycle, one second time to stop just before the last. And this still \n    only works assuming your cycle doesn't contain the object twice\n\n    Note: when setting color= it looks like the color cycle state is not called\n\n    TODO: maybe implement my own cycle structure '''\n\n    if ax is None:\n        ax = mpl.pyplot.gca()\n\n    i = 1  # count number of elements\n    cycle = ax._get_lines.prop_cycler\n    a = next(cycle)     # a is already the next one.\n    while(a != next(cycle)):\n        i += 1\n    # We want a-1 to show up on next call to next. So a-2 must be set now\n    for j in range(i - 2):\n        next(cycle)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the next color to be used in the given color cycle.", "response": "def get_next_color(ax=None, nonintrusive=True):\n    ''' Return the next color to be used in the given color cycle. \n    \n    Warning: due to the structure of Python iterators I couldn't help but\n    iterate over all the color cycle once. \n\n    If nonintrusive is True, then leave the color cycle in the same state as \n    before    \n    '''\n\n    if ax is None:\n        ax = mpl.pyplot.gca()\n\n    i = 1  # count number of elements\n    cycle = ax._get_lines.prop_cycler  # color_cycle\n    color = None\n    a = next(cycle)     # a is already the next one.\n    while(a != next(cycle)):\n        i += 1\n        color = a['color']\n\n    if nonintrusive:\n        # We want a-1 to show up on next call to next. So a-2 must be set now\n        for j in range(i - 1):\n            next(cycle)\n\n    return color"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(self, pointer):\n\n        dp = DocumentPointer(pointer)\n        obj, fetcher = self.prototype(dp)\n\n        for token in dp.pointer:\n            obj = token.extract(obj, bypass_ref=True)\n            reference = ref(obj)\n            if reference:\n                obj = fetcher.resolve(reference)\n        return obj", "response": "Resolve from documents.\n\n        :param pointer: foo\n        :type pointer: DocumentPointer"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\naligns sequences with clustalo", "response": "def align_seqs(found_seqs, sequence, locus, start_pos, missing,\n               annotated, cutoff=0.90, verbose=False, verbosity=0):\n    \"\"\"\n    align_seqs - Aligns sequences with clustalo\n\n    :param found_seqs: List of the reference sequences\n    :type found_seqs: ``List``\n    :param sequence: The input consensus sequence.\n    :type sequence: SeqRecord\n    :param locus: The gene locus associated with the sequence.\n    :type locus: ``str``\n    :param annotated: dictonary of the annotated features\n    :type annotated: ``dict``\n    :param start_pos: Where the reference sequence starts\n    :type start_pos: ``int``\n    :param missing: List of the unmapped features\n    :type missing: ``List``\n    :param cutoff: The alignment cutoff\n    :type cutoff: ``float``\n    :param verbose: Flag for running in verbose mode.\n    :type verbose: ``bool``\n    :param verbosity: Numerical value to indicate how verbose the output will be in verbose mode.\n    :type verbosity: ``int``\n    :rtype: :ref:`ann`\n    \"\"\"\n    logger = logging.getLogger(\"Logger.\" + __name__)\n    seqs = [found_seqs, sequence]\n\n    if verbose and verbosity > 0:\n        logger.info(\"found_seqs length = \" + str(len(found_seqs)))\n        logger.info(\"sequence length = \" + str(len(sequence)))\n\n    seqs = []\n    seqs.append(found_seqs)\n    seqs.append(sequence)\n\n    align = []\n\n    # piping to clustalo failed\n    # when sequences were over ~7k bp\n    if len(sequence) > 7000:\n\n        # Writing sequences out to fasta files..\n        if verbose:\n            logger.info(\"Sequence too large to use pipe\")\n        randid = randomid()\n        input_fasta = str(randid) + \".fasta\"\n        output_clu = str(randid) + \".clu\"\n        SeqIO.write(seqs, input_fasta, \"fasta\")\n        clustalomega_cline = ClustalOmegaCommandline(infile=input_fasta,\n                                                     outfile=output_clu,\n                                                     outfmt='clu', wrap=20000,\n                                                     verbose=True, auto=True)\n        stdout, stderr = clustalomega_cline()\n        aligns = AlignIO.read(output_clu, \"clustal\")\n        for aln in aligns:\n            align.append(str(aln.seq))\n\n        # Delete files\n        cleanup(randid)\n    else:\n        # Running clustalo by piping in sequences\n        indata = flatten([[\">\" + str(s.id), str(s.seq)]\n                          for s in seqs])\n        child = Popen(['clustalo',\n                       '--outfmt', 'clu',\n                       '--wrap=50000',\n                       '--auto', '-i', '-'],\n                      stdout=PIPE,\n                      stderr=STDOUT,\n                      stdin=PIPE)\n\n        stdout = child.communicate(input=str.encode(\"\\n\".join(indata)))\n        child.wait()\n\n        lines = bytes.decode(stdout[0]).split(\"\\n\")\n        for line in lines:\n            if re.search(\"\\w\", line) and not re.search(\"CLUSTAL\", line):\n                alignment = re.findall(r\"[\\S']+\", line)\n                if len(alignment) == 2:\n                    align.append(list(alignment[1]))\n        child.terminate()\n\n    # Print out what blocks haven't been annotated\n    if verbose and len(align) > 0:\n        logger.info(\"* ClustalOmega alignment succeeded *\")\n\n    insers, dels = 0, 0\n    all_features = []\n    if len(align)-2 == 0:\n        infeats = get_seqfeat(seqs[0])\n        diffs = count_diffs(align, infeats, sequence, locus, cutoff, verbose, verbosity)\n        if isinstance(diffs, Annotation):\n            if verbose:\n                logger.info(\"Run alignment with \" + found_seqs.id)\n                logger.info(\"***********************\")\n            return diffs, 0, 0\n        else:\n            insers, dels = diffs[0], diffs[1]\n        f = find_features(infeats, align[0], annotated, start_pos, cutoff)\n        all_features.append(f)\n    else:\n        for i in range(0, len(align)-2):\n            infeats = get_seqfeat(seqs[i])\n            f = find_features(infeats, align[i], annotated, start_pos, cutoff)\n            all_features.append(f)\n\n    if len(all_features) > 0:\n        if verbose:\n            logger.info(\"-- Resolving features -- \")\n            for f in all_features[0]:\n                logger.info(\"Resolving -> \" + f)\n\n        annotation = resolve_feats(all_features,\n                                   align[len(align)-1],\n                                   align[0],\n                                   start_pos,\n                                   locus,\n                                   missing,\n                                   verbose,\n                                   verbosity)\n        if verbose:\n            logger.info(\"Run alignment with \" + found_seqs.id)\n            logger.info(\"Missing features = \" + \",\".join(list(missing.keys())))\n            logger.info(\"Number of features found = \" + str(len(all_features)))\n            logger.info(\"Features found = \" + \",\".join(list(all_features[0].keys())))\n            logger.info(\"Features annotated = \" + \",\".join(list(annotation.annotation.keys())))\n            logger.info(\"***********************\")\n\n        return annotation, insers, dels\n    else:\n        if verbose:\n            logger.info(\"***********************\")\n        return Annotation(complete_annotation=False), 0, 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_feats(feat_list, seqin, seqref, start, locus, missing, verbose=False, verbosity=0):\n    structures = get_structures()\n    logger = logging.getLogger(\"Logger.\" + __name__)\n    seq = SeqRecord(seq=Seq(\"\".join(seqin), SingleLetterAlphabet()))\n    seq_covered = len(seq.seq)\n    coordinates = dict(map(lambda x: [x, 1],\n                       [i for i in range(0, len(seq.seq)+1)]))\n\n    mapping = dict(map(lambda x: [x, 1],\n                       [i for i in range(0, len(seq.seq)+1)]))\n\n    diff = 0\n    if len(feat_list) > 1:\n        if verbose:\n            logger.error(\"resolve_feats error\")\n        return Annotation(complete_annotation=False)\n    else:\n        features = {}\n        full_annotation = {}\n        features = feat_list[0]\n\n        # Need to sort\n        feature_list = sorted(features.keys(),\n                              key=lambda f: structures[locus][f])\n\n        diff_f = True\n        for feat in feature_list:\n            if feat in missing:\n                f = features[feat]\n                seqrec = f.extract(seq)\n                seq_covered -= len(seqrec.seq)\n                if re.search(\"-\", str(seqrec.seq)):\n                    l1 = len(seqrec.seq)\n                    newseq = re.sub(r'-', '', str(seqrec.seq))\n                    seqrec.seq = Seq(newseq, IUPAC.unambiguous_dna)\n                    tmdiff = l1 - len(newseq)\n                    diff += tmdiff\n\n                if seqrec.seq:\n                    #logger.error(\"FEAT HAS SEQ \" + feat)\n                    if diff_f and diff > 0:\n                        sp = f.location.start + start\n                        diff_f = False\n                    else:\n                        sp = f.location.start + start - diff\n\n                    ep = f.location.end + start - diff\n                    featn = SeqFeature(FeatureLocation(ExactPosition(sp),\n                                                       ExactPosition(ep),\n                                                       strand=1), type=f.type)\n\n                    features.update({feat: featn})\n                    full_annotation.update({feat: seqrec})\n\n                    for i in range(featn.location.start, featn.location.end):\n                        if i in coordinates:\n                            del coordinates[i]\n                        mapping[i] = feat\n            else:\n                f = features[feat]\n                seqrec = f.extract(seq)\n                seq_covered -= len(seqrec.seq)\n                if re.search(\"-\", str(seqrec.seq)):\n                    l1 = len(seqrec.seq)\n                    newseq = re.sub(r'-', '', str(seqrec.seq))\n                    seqrec.seq = Seq(newseq, IUPAC.unambiguous_dna)\n                    tmdiff = l1 - len(newseq)\n                    diff += tmdiff\n\n        blocks = getblocks(coordinates)\n        rmapping = {k+start: mapping[k] for k in mapping.keys()}\n\n        # Print out what features are missing\n        if verbose and verbosity > 0 and len(full_annotation.keys()) > 1:\n            logger.info(\"Features resolved:\")\n            for f in full_annotation:\n                logger.info(f)\n        else:\n            if verbose:\n                logger.info(\"Failed to resolve\")\n\n        if not full_annotation or len(full_annotation) == 0:\n            if verbose:\n                logger.info(\"Failed to align missing features\")\n            return Annotation(complete_annotation=False)\n        else:\n            return Annotation(annotation=full_annotation,\n                              method=\"clustalo\",\n                              features=features,\n                              mapping=rmapping,\n                              blocks=blocks,\n                              seq=seq)", "response": "Resolves features from alignments in a sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef count_diffs(align, feats, inseq, locus, cutoff,\n                verbose=False, verbosity=0):\n    \"\"\"\n    count_diffs - Counts the number of mismatches, gaps, and insertions and then determines if those are within an acceptable range.\n\n    :param align: The alignment\n    :type align: ``List``\n    :param feats: Dictonary of the features\n    :type feats: ``dict``\n    :param locus: The gene locus associated with the sequence.\n    :type locus: ``str``\n    :param inseq: The input sequence\n    :type inseq: ``str``\n    :param cutoff: The alignment cutoff\n    :type cutoff: ``float``\n    :param verbose: Flag for running in verbose mode.\n    :type verbose: ``bool``\n    :param verbosity: Numerical value to indicate how verbose the output will be in verbose mode.\n    :type verbosity: ``int``\n    :rtype: ``List``\n    \"\"\"\n\n    nfeats = len(feats.keys())\n    mm = 0\n    insr = 0\n    dels = 0\n    gaps = 0\n    match = 0\n    lastb = ''\n    l = len(align[0]) if len(align[0]) > len(align[1]) else len(align[1])\n\n    # Counting gaps, mismatches and insertions\n    for i in range(0, l):\n        if align[0][i] == \"-\" or align[1][i] == \"-\":\n            if align[0][i] == \"-\":\n                insr += 1\n                if lastb != '-':\n                    gaps += 1\n                lastb = \"-\"\n            if align[1][i] == \"-\":\n                dels += 1\n                if lastb != '-':\n                    gaps += 1\n                lastb = \"-\"\n        else:\n            lastb = ''\n            if align[0][i] != align[1][i]:\n                mm += 1\n            else:\n                match += 1\n\n    gper = gaps / nfeats\n    delper = dels / l\n    iper = insr / l\n    mmper = mm / l\n    mper = match / l\n    mper2 = match / len(inseq)\n\n    logger = logging.getLogger(\"Logger.\" + __name__)\n\n    if verbose and verbosity > 0:\n        logger.info(\"Features algined = \" + \",\".join(list(feats.keys())))\n        logger.info('{:<22}{:<6d}'.format(\"Number of feats: \", nfeats))\n        logger.info('{:<22}{:<6d}{:<1.2f}'.format(\"Number of gaps: \", gaps, gper))\n        logger.info('{:<22}{:<6d}{:<1.2f}'.format(\"Number of deletions: \", dels, delper))\n        logger.info('{:<22}{:<6d}{:<1.2f}'.format(\"Number of insertions: \", insr, iper))\n        logger.info('{:<22}{:<6d}{:<1.2f}'.format(\"Number of mismatches: \", mm, mmper))\n        logger.info('{:<22}{:<6d}{:<1.2f}'.format(\"Number of matches: \", match, mper))\n        logger.info('{:<22}{:<6d}{:<1.2f}'.format(\"Number of matches: \", match, mper2))\n    indel = iper + delper\n\n    # ** HARD CODED LOGIC ** #\n    if len(inseq) > 6000 and mmper < .10 and mper2 > .80:\n        if verbose:\n            logger.info(\"Alignment coverage high enough to complete annotation 11\")\n        return insr, dels\n    else:\n        # TODO: These numbers need to be fine tuned\n        indel_mm = indel + mper2\n        if (indel > 0.5 or mmper > 0.05) and mper2 < cutoff and indel_mm != 1:\n            if verbose:\n                logger.info(\"Alignment coverage NOT high enough to return annotation\")\n            return Annotation(complete_annotation=False)\n        else:\n            if verbose:\n                logger.info(\"Alignment coverage high enough to complete annotation\")\n            return insr, dels", "response": "Count the number of mismatches gaps and insertions and return if those are within an acceptable range."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints some debug info for this script", "response": "def cmd_debug(self, argv, help):\n        \"\"\"Prints some debug info for this script\"\"\"\n        parser = argparse.ArgumentParser(\n            prog=\"%s debug\" % self.progname,\n            description=help,\n        )\n        instances = self.instances\n        parser.add_argument(\"instance\", nargs=1,\n                            metavar=\"instance\",\n                            help=\"Name of the instance from the config.\",\n                            choices=sorted(instances))\n        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\",\n                            action=\"store_true\", help=\"Print more info and output the startup script\")\n        parser.add_argument(\"-c\", \"--console-output\", dest=\"console_output\",\n                            action=\"store_true\", help=\"Prints the console output of the instance if available\")\n        parser.add_argument(\"-i\", \"--interactive\", dest=\"interactive\",\n                            action=\"store_true\", help=\"Creates a connection and drops you into an interactive Python session\")\n        parser.add_argument(\"-r\", \"--raw\", dest=\"raw\",\n                            action=\"store_true\", help=\"Outputs the raw possibly compressed startup script\")\n        parser.add_argument(\"-o\", \"--override\", nargs=\"*\", type=str,\n                            dest=\"overrides\", metavar=\"OVERRIDE\",\n                            help=\"Option to override instance config for startup script (name=value).\")\n        args = parser.parse_args(argv)\n        overrides = self._parse_overrides(args)\n        overrides['instances'] = self.instances\n        instance = instances[args.instance[0]]\n        if hasattr(instance, 'startup_script'):\n            startup_script = instance.startup_script(overrides=overrides, debug=True)\n            max_size = getattr(instance, 'max_startup_script_size', 16 * 1024)\n            log.info(\"Length of startup script: %s/%s\", len(startup_script['raw']), max_size)\n            if args.verbose:\n                if 'startup_script' in instance.config:\n                    if startup_script['original'] == startup_script['raw']:\n                        log.info(\"Startup script:\")\n                    elif args.raw:\n                        log.info(\"Compressed startup script:\")\n                    else:\n                        log.info(\"Uncompressed startup script:\")\n                else:\n                    log.info(\"No startup script specified\")\n            if args.raw:\n                print(startup_script['raw'], end='')\n            elif args.verbose:\n                print(startup_script['original'], end='')\n        if args.console_output:\n            if hasattr(instance, 'get_console_output'):\n                print(instance.get_console_output())\n            else:\n                log.error(\"The instance doesn't support console output.\")\n        if args.interactive:  # pragma: no cover\n            import readline\n            from pprint import pprint\n            local = dict(\n                ctrl=self,\n                instances=self.instances,\n                instance=instance,\n                pprint=pprint)\n            readline.parse_and_bind('tab: complete')\n            try:\n                import rlcompleter\n                readline.set_completer(rlcompleter.Completer(local).complete)\n            except ImportError:\n                pass\n            __import__(\"code\").interact(local=local)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmd_list(self, argv, help):\n        parser = argparse.ArgumentParser(\n            prog=\"%s list\" % self.progname,\n            description=help,\n        )\n        parser.add_argument(\"list\", nargs=1,\n                            metavar=\"listname\",\n                            help=\"Name of list to show.\",\n                            choices=sorted(self.list_cmds))\n        parser.add_argument(\"listopts\",\n                            metavar=\"...\",\n                            nargs=argparse.REMAINDER,\n                            help=\"list command options\")\n        args = parser.parse_args(argv)\n        for name, func in sorted(self.list_cmds[args.list[0]]):\n            func(args.listopts, func.__doc__)", "response": "Return a list of various things"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging into the instance with ssh using the automatically generated known hosts", "response": "def cmd_ssh(self, argv, help):\n        \"\"\"Log into the instance with ssh using the automatically generated known hosts\"\"\"\n        parser = argparse.ArgumentParser(\n            prog=\"%s ssh\" % self.progname,\n            description=help,\n        )\n        instances = self.get_instances(command='init_ssh_key')\n        parser.add_argument(\"instance\", nargs=1,\n                            metavar=\"instance\",\n                            help=\"Name of the instance from the config.\",\n                            choices=sorted(instances))\n        parser.add_argument(\"...\", nargs=argparse.REMAINDER,\n                            help=\"ssh options\")\n        iargs = enumerate(argv)\n        sid_index = None\n        user = None\n        for i, arg in iargs:\n            if not arg.startswith('-'):\n                sid_index = i\n                break\n            if arg[1] in '1246AaCfgKkMNnqsTtVvXxYy':\n                continue\n            elif arg[1] in 'bcDeFiLlmOopRSw':\n                value = iargs.next()\n                if arg[1] == 'l':\n                    user = value[1]\n                continue\n        # fake parsing for nice error messages\n        if sid_index is None:\n            parser.parse_args([])\n        else:\n            sid = argv[sid_index]\n            if '@' in sid:\n                user, sid = sid.split('@', 1)\n            parser.parse_args([sid])\n        instance = instances[sid]\n        if user is None:\n            user = instance.config.get('user')\n        try:\n            ssh_info = instance.init_ssh_key(user=user)\n        except (instance.paramiko.SSHException, socket.error) as e:\n            log.error(\"Couldn't validate fingerprint for ssh connection.\")\n            log.error(unicode(e))\n            log.error(\"Is the instance finished starting up?\")\n            sys.exit(1)\n        client = ssh_info['client']\n        client.get_transport().sock.close()\n        client.close()\n        argv[sid_index:sid_index + 1] = instance.ssh_args_from_info(ssh_info)\n        argv[0:0] = ['ssh']\n        os.execvp('ssh', argv)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting help for the specified command", "response": "def cmd_help(self, argv, help):\n        \"\"\"Print help\"\"\"\n        parser = argparse.ArgumentParser(\n            prog=\"%s help\" % self.progname,\n            description=help,\n        )\n        parser.add_argument('-z', '--zsh',\n                            action='store_true',\n                            help=\"Print info for zsh autocompletion\")\n        parser.add_argument(\"command\", nargs='?',\n                            metavar=\"command\",\n                            help=\"Name of the command you want help for.\",\n                            choices=self.subparsers.keys())\n        args = parser.parse_args(argv)\n        if args.zsh:\n            if args.command is None:\n                for cmd in self.subparsers.keys():\n                    print(cmd)\n            else:  # pragma: no cover\n                if hasattr(self.cmds[args.command], 'get_completion'):\n                    for item in self.cmds[args.command].get_completion():\n                        print(item)\n                elif args.command in ('do', 'ssh'):\n                    for instance in self.get_instances(command='init_ssh_key'):\n                        print(instance)\n                elif args.command == 'debug':\n                    for instance in sorted(self.instances):\n                        print(instance)\n                elif args.command == 'list':\n                    for subcmd in sorted(self.list_cmds):\n                        print(subcmd)\n                elif args.command != 'help':\n                    for instance in sorted(self.get_instances(command=args.command)):\n                        print(instance)\n        else:\n            if args.command is None:\n                parser.print_help()\n            else:\n                cmd = self.cmds[args.command]\n                cmd(['-h'], cmd.__doc__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialize(self, store):\r\n        assert isinstance(store, stores.BaseStore)\r\n        self.messages = Queue()\r\n        self.store = store\r\n        self.store.register(self)", "response": "Common initialization of handlers happens here."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npushes data to a listener.", "response": "async def publish(self, message):\r\n        \"\"\"Pushes data to a listener.\"\"\"\r\n        try:\r\n            self.write('data: {}\\n\\n'.format(message))\r\n            await self.flush()\r\n        except StreamClosedError:\r\n            self.finished = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def open(self):\r\n        self.store.register(self)\r\n        while not self.finished:\r\n            message = await self.messages.get()\r\n            await self.publish(message)", "response": "Open the channel with the publisher."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def publish(self, message):\r\n        try:\r\n            self.write_message(dict(data=message))\r\n        except WebSocketClosedError:\r\n            self._close()", "response": "Push a new message to the client."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a key, given an iv, and message do whatever operation asked in the operation field. Operation will be checked for: \"decrypt\" and \"encrypt\" strings. Returns the message encrypted/decrypted. message must be a multiple by 16 bytes (for division in 16 byte blocks) key must be 32 byte iv must be 32 byte (it's not internally used in AES 256 ECB, but it's needed for IGE)", "response": "def _ige(message, key, iv, operation=\"decrypt\"):\n    \"\"\"Given a key, given an iv, and message\n     do whatever operation asked in the operation field.\n     Operation will be checked for: \"decrypt\" and \"encrypt\" strings.\n     Returns the message encrypted/decrypted.\n     message must be a multiple by 16 bytes (for division in 16 byte blocks)\n     key must be 32 byte\n     iv must be 32 byte (it's not internally used in AES 256 ECB, but it's\n     needed for IGE)\"\"\"\n    message = bytes(message)\n    if len(key) != 32:\n        raise ValueError(\"key must be 32 bytes long (was \" +\n                         str(len(key)) + \" bytes)\")\n    if len(iv) != 32:\n        raise ValueError(\"iv must be 32 bytes long (was \" +\n                         str(len(iv)) + \" bytes)\")\n\n    cipher = AES.new(key, AES.MODE_ECB, iv)\n    blocksize = cipher.block_size\n\n    if len(message) % blocksize != 0:\n        raise ValueError(\"message must be a multiple of 16 bytes (try adding \" +\n                         str(16 - len(message) % 16) + \" bytes of padding)\")\n\n    ivp = iv[0:blocksize]\n    ivp2 = iv[blocksize:]\n\n    ciphered = bytes()\n\n    for i in range(0, len(message), blocksize):\n        indata = message[i:i+blocksize]\n        if operation == \"decrypt\":\n            xored = strxor(indata, ivp2)\n            decrypt_xored = cipher.decrypt(xored)\n            outdata = strxor(decrypt_xored, ivp)\n            ivp = indata\n            ivp2 = outdata\n        elif operation == \"encrypt\":\n            xored = strxor(indata, ivp)\n            encrypt_xored = cipher.encrypt(xored)\n            outdata = strxor(encrypt_xored, ivp2)\n            ivp = outdata\n            ivp2 = indata\n        else:\n            raise ValueError(\"operation must be either 'decrypt' or 'encrypt'\")\n        ciphered += outdata\n    return ciphered"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nswitching to specific chunk", "response": "def _useChunk(self, index) -> None:\n        \"\"\"\n        Switch to specific chunk\n\n        :param index:\n        \"\"\"\n\n        if self.currentChunk is not None:\n            if self.currentChunkIndex == index and \\\n                    not self.currentChunk.closed:\n                return\n            self.currentChunk.close()\n\n        self.currentChunk = self._openChunk(index)\n        self.currentChunkIndex = index\n        self.itemNum = self.currentChunk.numKeys + 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iterator(self, includeKey=True, includeValue=True, prefix=None):\n\n        if not (includeKey or includeValue):\n            raise ValueError(\"At least one of includeKey or includeValue \"\n                             \"should be true\")\n        lines = self._lines()\n        if includeKey and includeValue:\n            return self._keyValueIterator(lines, prefix=prefix)\n        if includeValue:\n            return self._valueIterator(lines, prefix=prefix)\n        return self._keyIterator(lines, prefix=prefix)", "response": "Iterator for all the items in the store."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef numKeys(self) -> int:\n        chunks = self._listChunks()\n        num_chunks = len(chunks)\n        if num_chunks == 0:\n            return 0\n        count = (num_chunks-1)*self.chunkSize\n        last_chunk = self._openChunk(chunks[-1])\n        count += sum(1 for _ in last_chunk._lines())\n        last_chunk.close()\n        return count", "response": "This will iterate over the last chunk and return the number of lines in total"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexposes compiler to factory. :param compiler: the callable to expose :type compiler: callable :param spec: name of the spec :type spec: str It can be used as a decorator:: @register(spec='my:first:spec') def my_compiler(schema, pointer, context): return Validator(schema) or as a function:: def my_compiler(schema, pointer, context): return Validator(schema) register(my_compiler, 'my:second:spec')", "response": "def register(compiler=None, spec=None):\n    \"\"\"\n    Expose compiler to factory.\n\n    :param compiler: the callable to expose\n    :type compiler: callable\n    :param spec: name of the spec\n    :type spec: str\n\n    It can be used as a decorator::\n\n        @register(spec='my:first:spec')\n        def my_compiler(schema, pointer, context):\n            return Validator(schema)\n\n    or as a function::\n\n        def my_compiler(schema, pointer, context):\n            return Validator(schema)\n\n        register(my_compiler, 'my:second:spec')\n\n    \"\"\"\n    if not spec:\n        raise CompilationError('Spec is required')\n    if not compiler:\n        return partial(register, spec=spec)\n    return Factory.register(spec, compiler)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register(self, subscriber):\r\n        assert isinstance(subscriber, RequestHandler)\r\n        logger.debug('New subscriber')\r\n        self.subscribers.add(subscriber)", "response": "Register a new subscriber."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstop publishing to a subscriber.", "response": "def deregister(self, subscriber):\r\n        \"\"\"Stop publishing to a subscriber.\"\"\"\r\n        try:\r\n            logger.debug('Subscriber left')\r\n            self.subscribers.remove(subscriber)\r\n        except KeyError:\r\n            logger.debug(\r\n                'Error removing subscriber: ' +\r\n                str(subscriber))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shutdown(self):\r\n        self._done.set()\r\n        self.executor.shutdown(wait=False)", "response": "Stop the publishing loop."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef annotate(self, sequence: Seq=None, locus: str=None,\n                 nseqs: int=20, alignseqs: int=10,\n                 skip: List=[],\n                 rerun: bool=True,\n                 full: bool=True) -> Annotation:\n        \"\"\"\n        annotate - method for annotating a BioPython sequence\n\n        :param sequence: The input consensus sequence.\n        :type sequence: Seq\n        :param locus: The gene locus associated with the sequence.\n        :type locus: ``str``\n        :param nseqs: The number of blast sequences to use.\n        :type nseqs: ``int``\n        :param alignseqs: The number of sequences to use for targeted alignments.\n        :type alignseqs: ``int``\n        :param skip: A list of alleles to skip for using as a reference. This is used for validation and testing.\n        :type skip: ``List``\n        :rtype: :ref:`ann`\n\n        Returns:\n            The annotate function return an ``Annotation`` object that\n            contains the sequence features and names associated with them.\n\n            Example output::\n\n                {\n                     'complete_annotation': True,\n                     'annotation': {'exon_1': SeqRecord(seq=Seq('AGAGACTCTCCCG', SingleLetterAlphabet()), id='HLA:HLA00630', name='HLA:HLA00630', description='HLA:HLA00630 DQB1*03:04:01 597 bp', dbxrefs=[]),\n                                    'exon_2': SeqRecord(seq=Seq('AGGATTTCGTGTACCAGTTTAAGGCCATGTGCTACTTCACCAACGGGACGGAGC...GAG', SingleLetterAlphabet()), id='HLA:HLA00630', name='HLA:HLA00630', description='HLA:HLA00630 DQB1*03:04:01 597 bp', dbxrefs=[]),\n                                    'exon_3': SeqRecord(seq=Seq('TGGAGCCCACAGTGACCATCTCCCCATCCAGGACAGAGGCCCTCAACCACCACA...ATG', SingleLetterAlphabet()), id='HLA:HLA00630', name='<unknown name>', description='HLA:HLA00630', dbxrefs=[])},\n                     'features': {'exon_1': SeqFeature(FeatureLocation(ExactPosition(0), ExactPosition(13), strand=1), type='exon_1'),\n                                  'exon_2': SeqFeature(FeatureLocation(ExactPosition(13), ExactPosition(283), strand=1), type='exon_2')\n                                  'exon_3': SeqFeature(FeatureLocation(ExactPosition(283), ExactPosition(503), strand=1), type='exon_3')},\n                     'method': 'nt_search and clustalo',\n                     'gfe': 'HLA-Aw2-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-4',\n                     'seq': SeqRecord(seq=Seq('AGAGACTCTCCCGAGGATTTCGTGTACCAGTTTAAGGCCATGTGCTACTTCACC...ATG', SingleLetterAlphabet()), id='HLA:HLA00630', name='HLA:HLA00630', description='HLA:HLA00630 DQB1*03:04:01 597 bp', dbxrefs=[])\n                }\n\n\n        Example usage:\n\n            >>> from Bio.Seq import Seq\n            >>> from seqann import BioSeqAnn\n            >>> sequence = Seq('AGAGACTCTCCCGAGGATTTCGTGTACCAGTTTAAGGCCATGTGCTACTTCACC')\n            >>> seqann = BioSeqAnn()\n            >>> ann = seqann.annotate(sequence)\n            >>> for f in ann.annotation:\n            ...    print(f, ann.method, str(ann.annotation[f].seq), sep=\"\\t\")\n            exon_2  nt_search and clustalo  AGGATTTCGTGTACCAGTTTAAGGCCATGTGCTACTTCACCAACGGGACGGAGCGCGTGCGTTATGTGACCAGATACATCTATAACCGAGAGGAGTACGCACGCTTCGACAGCGACGTGGAGGTGTACCGGGCGGTGACGCCGCTGGGGCCGCCTGCCGCCGAGTACTGGAACAGCCAGAAGGAAGTCCTGGAGAGGACCCGGGCGGAGTTGGACACGGTGTGCAGACACAACTACCAGTTGGAGCTCCGCACGACCTTGCAGCGGCGAG\n            exon_3  nt_search and clustalo  TGGAGCCCACAGTGACCATCTCCCCATCCAGGACAGAGGCCCTCAACCACCACAACCTGCTGGTCTGCTCAGTGACAGATTTCTATCCAGCCCAGATCAAAGTCCGGTGGTTTCGGAATGACCAGGAGGAGACAACCGGCGTTGTGTCCACCCCCCTTATTAGGAACGGTGACTGGACCTTCCAGATCCTGGTGATGCTGGAAATGACTCCCCAGCATGGAGACGTCTACACCTGCCACGTGGAGCACCCCAGCCTCCAGAACCCCATCACCGTGGAGTGGC\n            exon_1  nt_search and clustalo  AGAGACTCTCCCG\n            exon_4  nt_search and clustalo  GGGCTCAGTCTGAATCTGCCCAGAGCAAGATG\n\n        \"\"\"\n\n        # If sequence is now a biopython\n        # sequence record convert it to one\n        if isinstance(sequence, Seq):\n            sequence = SeqRecord(seq=sequence,\n                                 id=\"NO_ID\")\n\n        if isinstance(sequence, str):\n            sequence = SeqRecord(seq=Seq(sequence),\n                                 id=\"NO_ID\")\n\n        # If sequence contains any characters\n        # other than ATCG then the GFE notation\n        # can not be created\n        create_gfe = checkseq(sequence.seq)\n\n        if self.verbose and not create_gfe:\n            self.logger.warning(self.logname + \" Sequence alphabet \"\n                                + \"contains non DNA\")\n            self.logger.warning(self.logname\n                                + \" No GFE string will be generated\")\n\n        # Check it the locus exists\n        if not locus:\n            if self.verbose:\n                self.logger.info(self.logname + \" No locus provided! \")\n\n            # Guessing locus with blastn\n            locus = get_locus(sequence, kir=self.kir, refdata=self.refdata)\n\n            if locus and self.verbose:\n                self.logger.info(self.logname + \" Locus prediction = \" + locus)\n\n            if not locus:\n                if self.verbose:\n                    self.logger.error(self.logname\n                                      + \" Locus could not be determined!\")\n                # TODO: Raise exception\n                #raise NoLocusException(\"\")\n                return\n\n        # Exact match found\n        matched_annotation = self.refdata.search_refdata(sequence, locus)\n        if matched_annotation and not skip:\n\n            matched_annotation.exact = True\n\n            # TODO: return name of allele\n            if self.verbose:\n                self.logger.info(self.logname + \" exact match found\")\n\n            # Create GFE\n            if create_gfe:\n                feats, gfe = self.gfe.get_gfe(matched_annotation, locus)\n                matched_annotation.gfe = gfe\n                matched_annotation.structure = feats\n            return matched_annotation\n\n        # Run blast to get ref sequences\n        blast = blastn(sequence, locus, nseqs,\n                       kir=self.kir, verbose=self.verbose,\n                       refdata=self.refdata)\n\n        # If the blastn fails..\n        if blast.failed:\n\n            if self.verbose:\n                self.logger.info(self.logname + \" Blastn failed!!\")\n\n            # Try and determine the locus and rerun. This is\n            # useful for cases when the sequences is associated\n            # with the wrong locus.\n            locus = get_locus(sequence, kir=self.kir, refdata=self.refdata)\n\n            if locus and self.verbose:\n                self.logger.info(self.logname + \" Locus prediction = \" + locus)\n\n            # Check if the locus could not be found\n            if not locus:\n                self.logger.error(self.logname\n                                  + \" Locus could not be determined!\")\n                # TODO: Raise exception\n                return\n            return self.annotate(sequence, locus,\n                                 nseqs=nseqs,\n                                 alignseqs=alignseqs)\n            return\n\n        # Do seq_search first on all blast sequences\n        # retain what the largest ref seq is\n        leastmissing = 100\n        partial_ann = None\n        leastmissing_feat = None\n        found = blast.match_seqs\n        for i in range(0, len(found)):\n\n            run = 0\n            if i == len(found)-1:\n                run = 1\n\n            # Skip a reference\n            # * For validation *\n            if found[i].name in skip:\n                continue\n\n            if self.verbose:\n                self.logger.info(self.logname\n                                 + \" Running seq_search with \"\n                                 + found[i].name)\n\n            # * Running sequence search *\n            # This does a simple string search for the\n            # reference features within the provided sequence\n            ann = self.seqsearch.search_seqs(found[i],\n                                             sequence, locus,\n                                             partial_ann=partial_ann,\n                                             run=run)\n\n            if ann.complete_annotation:\n                if self.verbose:\n                    self.logger.info(self.logname\n                                     + \" Finished annotation using \"\n                                     + found[i].name)\n\n                # Add alignment flag is specified\n                if self.align:\n                    if self.verbose:\n                        self.logger.info(self.logname + \" Adding alignment\")\n                    ann = self.add_alignment(found[i], ann)\n\n                if self.verbose and self.verbosity > 0:\n                    self.logger.info(self.logname\n                                     + \" Features annotated = \"\n                                     + \",\".join(list(ann\n                                                     .annotation.keys())))\n                    if self.verbosity > 3:\n                        for f in ann.features:\n                            self.logger.info(self.logname\n                                             + \" \" + f + \" = \"\n                                             + str(ann\n                                                   .annotation[f].seq))\n\n                # Create GFE\n                if create_gfe:\n                    feats, gfe = self.gfe.get_gfe(ann, locus)\n                    ann.gfe = gfe\n                    ann.structure = feats\n                ann.clean()\n                return ann\n            else:\n                partial_ann = ann\n\n                if hasattr(partial_ann, 'refmissing'):\n                    if len(partial_ann.refmissing) < leastmissing:\n                        leastmissing_feat = found[i]\n                        leastmissing = len(partial_ann.refmissing)\n                else:\n                    leastmissing_feat = found[i]\n                    leastmissing = 0\n\n                if self.verbose and self.verbosity > 0:\n                    self.logger.info(self.logname\n                                     + \" Using partial annotation * run \"\n                                     + str(i) + \" *\")\n                    self.logger.info(self.logname\n                                     + \" Features found (\"\n                                     + str(len(ann.features.keys())) + \") = \"\n                                     + \",\".join(list(ann\n                                                     .features.keys())))\n                    self.logger.info(self.logname\n                                     + \" Features missing (\"\n                                     + str(len(ann.missing.keys())) + \") = \"\n                                     + \",\".join(list(ann\n                                                     .missing.keys())))\n                    self.logger.info(self.logname\n                                     + \" Sequence unmapped = \"\n                                     + str(ann.covered))\n                    self.logger.info(self.logname + \" ############\" +\n                                     \"##################\")\n\n        # The number of sequences being used for alignment\n        # can't be greater than the number of sequences\n        # to be returned from the blast results\n        if alignseqs > len(found):\n            alignseqs = len(found)-1\n\n        # * HARD CODED LOGIC * #\n        # > After testing with multiple thresholds\n        #   this value seemed to work best.\n        #\n        # Aligned % cutoff\n        align_cutoff = .90\n        if((not hasattr(partial_ann, 'features') or\n           len(partial_ann.features) == 0)\n           and len(sequence) > 700 and self.safemode):\n            self.logger.error(\"No feature matches!\")\n            self.logger.error(\"Running in safe mode. \" +\n                              \"No alignments will be done!\")\n\n            if rerun:\n                # Check to see if reverse comp\n                # TODO: Add note for reverse complement\n                self.logger.info(\"Running with reverse complement.\")\n                sequence = sequence.reverse_complement()\n                return self.annotate(sequence=sequence,\n                                     locus=locus,\n                                     rerun=False)\n            return\n\n        # Now loop through doing alignment\n        for i in range(0, alignseqs):\n\n            # Skip a reference\n            # * For validation *\n            if found[i].name in skip:\n                continue\n\n            if self.verbose:\n                self.logger.info(self.logname\n                                 + \" running ref_align with \"\n                                 + found[i].name)\n\n            aligned_ann = self.ref_align(found[i], sequence, locus,\n                                         annotation=partial_ann,\n                                         run=i,\n                                         cutoff=align_cutoff)\n\n            # * HARD CODED LOGIC * #\n            # > If sequences are very novel, then the alignment\n            #   cutoff may be to stringent. Incrementally decreasing\n            #   the cutoff improves the likelihood of these sequences\n            #   being annotated.\n            align_cutoff -= .01\n            if aligned_ann and aligned_ann.complete_annotation:\n                if self.align:\n                    if self.verbose:\n                        self.logger.info(self.logname + \" Adding alignment\")\n                    aligned_ann = self.add_alignment(found[i], aligned_ann)\n\n                if self.verbose:\n                    self.logger.info(self.logname\n                                     + \" Finished ref_align annotation using \"\n                                     + found[i].name)\n\n                if self.verbose and self.verbosity > 0:\n                    self.logger.info(self.logname\n                                     + \" Features annotated = \"\n                                     + \",\".join(list(aligned_ann\n                                                     .annotation.keys())))\n                    if self.verbosity > 2:\n                        for f in aligned_ann.annotation:\n                            self.logger.info(self.logname\n                                             + \" \" + f + \" = \"\n                                             + str(aligned_ann\n                                                   .annotation[f].seq))\n\n                # Create GFE\n                if create_gfe:\n                    feats, gfe = self.gfe.get_gfe(aligned_ann, locus)\n                    aligned_ann.gfe = gfe\n                    aligned_ann.structure = feats\n                aligned_ann.clean()\n                return aligned_ann\n            elif(aligned_ann):\n\n                if self.verbose and self.verbosity > 0:\n                    self.logger.info(self.logname\n                                     + \" Using partial annotation \"\n                                     + \"for alignment * run \"\n                                     + str(i) + \" - cutoff = \"\n                                     + str(align_cutoff)\n                                     + \" *\")\n                    self.logger.info(self.logname\n                                     + \" Features found = \"\n                                     + \",\".join(list(aligned_ann\n                                                     .features.keys())))\n                    self.logger.info(self.logname\n                                     + \" Features missing = \"\n                                     + \",\".join(list(aligned_ann\n                                                     .missing.keys())))\n                    self.logger.info(self.logname + \" ############\" +\n                                     \"##################\")\n                partial_ann = aligned_ann\n\n                if(hasattr(partial_ann, 'annotation')\n                   and partial_ann.annotation):\n                    exon_only = True\n                    for f in partial_ann.annotation:\n                        if re.search(\"intron\", f) or re.search(\"UTR\", f):\n                            exon_only = False\n\n                    if(is_classII(locus) and exon_only\n                       and len(partial_ann.annotation.keys()) > 0\n                       and align_cutoff < .9):\n                        align_cutoff = .80\n\n            if not is_classII(locus) and align_cutoff < .88:\n                align_cutoff = .88\n\n        # Don't run full\n        # annotation if flag is passed\n        if not full:\n            return\n\n        if self.verbose:\n            self.logger.info(self.logname + \" running full alignment\")\n\n        # Try doing full alignment\n        full_align = self.ref_align(leastmissing_feat,\n                                    sequence,\n                                    locus,\n                                    partial_ann=partial_ann,\n                                    cutoff=.80)\n\n        if self.verbose:\n            self.logger.info(self.logname\n                             + \" Finished ref_align annotation using full \"\n                             + leastmissing_feat.name)\n\n        # Check to see if an annotation was returned\n        if(not isinstance(full_align, Annotation)\n           or isinstance(full_align, str)):\n            if(not rerun or len(sequence) > 4000):\n                self.logger.info(self.logname + \" Failed annotation!\")\n                return Annotation()\n            else:\n                if self.verbose and self.verbosity > 0:\n                    self.logger.info(self.logname\n                                     + \" Reruning annotation!\")\n                return self.annotate(sequence=sequence,\n                                     locus=locus,\n                                     alignseqs=2,\n                                     nseqs=nseqs+1,\n                                     skip=[found[0].name],\n                                     rerun=False)\n\n        # Check if the annotation is complete\n        if not full_align.complete_annotation and self.verbose:\n            self.logger.info(self.logname + \" Incomplete annotation!\")\n\n        # Add the alignment to the annotation\n        if self.align and full_align.complete_annotation:\n            if self.verbose:\n                self.logger.info(self.logname + \" Adding alignment\")\n            full_align = self.add_alignment(leastmissing_feat, full_align)\n\n        if self.verbose and self.verbosity > 0:\n            self.logger.info(self.logname\n                             + \" Features annotated = \"\n                             + \",\".join(list(full_align\n                                             .annotation.keys())))\n            if self.verbosity > 2:\n                for f in full_align.annotation:\n                    self.logger.info(self.logname\n                                     + \" \" + f + \" = \"\n                                     + str(full_align\n                                           .annotation[f].seq))\n        # Create GFE\n        if create_gfe and full_align.complete_annotation:\n            feats, gfe = self.gfe.get_gfe(full_align, locus)\n            full_align.gfe = gfe\n            full_align.structure = feats\n\n        full_align.clean()\n        if(full_align.complete_annotation\n           or not rerun or len(sequence) > 4000):\n            return full_align\n        else:\n            if self.verbose and self.verbosity > 0:\n                self.logger.info(self.logname\n                                 + \" Reruning annotation!\")\n            return self.annotate(sequence=sequence,\n                                 locus=locus,\n                                 nseqs=nseqs+1,\n                                 alignseqs=2,\n                                 skip=[found[0].name],\n                                 rerun=False)", "response": "This method is used to annotate a BioPython sequence with the specified features and names associated with the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ref_align(self, found_seqs, sequence: Seq=None,\n                  locus: str=None, annotation: Annotation=None,\n                  partial_ann: Annotation=None,\n                  run: int=0,\n                  cutoff: float=.90) -> Annotation:\n        \"\"\"\n        ref_align - Method for doing targeted alignments on partial annotations\n\n        :param found_seqs: The input sequence record.\n        :type found_seqs: Seq\n        :param sequence: The input sequence record.\n        :type sequence: Seq\n        :param locus: The gene locus associated with the sequence.\n        :type locus: ``str``\n        :param annotation: The incomplete annotation from a previous iteration.\n        :type annotation: :ref:`ann`\n        :param partial_ann: The partial annotation after looping through all of the blast sequences.\n        :type partial_ann: :ref:`ann`\n        :rtype: :ref:`ann`\n\n        \"\"\"\n        if annotation and isinstance(annotation, Annotation):\n            if 0 in annotation.mapping \\\n                    and not isinstance(annotation.mapping[0], int):\n                ft = annotation.mapping[0]\n                start_order = self.refdata.structures[locus][ft]\n            else:\n                start_order = 0\n\n            # Check whether it's exons only\n            exon_only = True\n            if hasattr(annotation, 'annotation') and annotation.annotation:\n                for f in annotation.annotation:\n                    if re.search(\"intron\", f) or re.search(\"UTR\", f):\n                        exon_only = False\n            elif(len(sequence.seq) > 900):\n                exon_only = False\n\n            annoated = []\n            if hasattr(annotation, 'annotation') and annotation.annotation:\n                annoated = list(annotation.annotation.keys())\n\n            # Extract the missing blocks and\n            # only align those blocks to the known\n            # missing features\n            # Start with all blocks missing\n            # and then delete block if it is found\n            tmp_missing = []\n            missing_blocks = annotation.blocks\n            for b in sorted(annotation.blocks):\n                # **** Check if block equals full input sequence *** #\n                # - If it does, then just align the ful\n\n                start = b[0]-1 if b[0] != 0 else 0\n                seq_feat = \\\n                    SeqFeature(\n                        FeatureLocation(\n                            ExactPosition(start),\n                            ExactPosition(b[len(b)-1]),\n                            strand=1),\n                        type=\"unmapped\")\n\n                feat = seq_feat.extract(annotation.seq)\n                combosrecs, exons, fullrec = self._refseqs(locus,\n                                                           start,\n                                                           annotation,\n                                                           feat,\n                                                           b)\n\n                # Print out different sequence types being align\n                if self.verbose and self.verbosity > 3:\n                    for combseqr in combosrecs:\n                        self.logger.info(self.logname + \" \" + combseqr.id)\n\n                ic = 0\n\n                # Looping through the generated combos\n                # from the reference sequence. (ex. intron1-exon1)\n                for combseqr in combosrecs:\n                    if self.verbose:\n                        self.logger.info(self.logname + \" aligning ->\" + combseqr.id)\n                    \n                    # Running the alignment\n                    an, ins, dels = align_seqs(combseqr, feat, locus,\n                                               start,\n                                               annotation.missing,\n                                               len(annoated),\n                                               cutoff=cutoff,\n                                               verbose=self.align_verbose,\n                                               verbosity=self.align_verbosity)\n\n                    # Checking if any features were mapped\n                    mapped_feat = list(an.annotation.keys())\n                    if len(mapped_feat) >= 1:\n\n                        # loop through the annotated features\n                        for f in an.annotation:\n                            f_order = self.refdata.structures[locus][f]\n\n                            # Only map features if they are in the correct order\n                            if f in annotation.missing \\\n                                    and f_order >= start_order:\n\n                                # * HARD CODED LOGIC *\n                                # Only accept the alignments if they are\n                                # somewhat similar to what's been observed already\n                                length, lengthsd = 0, 0\n                                length = float(self.refdata.feature_lengths[locus][f][0])\n                                lengthsd = float(self.refdata.feature_lengths[locus][f][1])\n\n                                incr = 3 if not is_classII(locus) else 4\n\n                                max_length = length + (lengthsd*incr) + ins\n                                min_length = length - (lengthsd*incr) - dels\n\n                                if f == \"exon_8\" and not is_classII(f):\n                                    max_length = 10\n\n                                # Check ordering when it's only exon sequences\n                                if exon_only:\n                                    f_order = self.refdata.structures[locus][f]\n                                    endp = an.features[f].location.end + 1\n\n                                    #Make sure order of alignment make sense\n                                    if an.features[f].location.start == 0 \\\n                                            and f != \"five_prime_UTR\" \\\n                                            and not isexon(f):\n                                        del an.features[f]\n                                        continue\n\n                                    if endp in annotation.mapping and not isinstance(annotation.mapping[endp], int):\n                                        mf = annotation.mapping[endp]\n                                        expected_order = f_order + 1\n                                        expected_order2 = f_order + 2\n                                        if expected_order != self.refdata.structures[locus][mf] and expected_order2 != self.refdata.structures[locus][mf]:\n                                            self.logger.info(self.logname + \" out of order1 -> \" + mf + \" \" + f)                                        \n                                            del an.features[f]\n                                            continue\n\n                                    startp = an.features[f].location.start - 1\n                                    if startp in annotation.mapping and not isinstance(annotation.mapping[startp], int):\n                                        mf = annotation.mapping[startp]\n                                        expected_order = f_order - 1\n                                        expected_order2 = f_order - 2\n                                        if expected_order != self.refdata.structures[locus][mf] and expected_order2 != self.refdata.structures[locus][mf]:\n                                            self.logger.info(self.logname + \" out of order2 -> \" + mf + \" \" + f)\n                                            del an.features[f]\n                                            continue\n                                else:\n\n                                    ##Make sure order of alignment make sense\n                                    if an.features[f].location.start == 0 \\\n                                            and f != \"five_prime_UTR\" \\\n                                            and 'three_prime_UTR' in annotation.annotation:\n                                        del an.features[f]\n                                        continue\n\n                                    f_order = self.refdata.structures[locus][f]\n                                    endp = an.features[f].location.end + 1\n                                    if endp in annotation.mapping and not isinstance(annotation.mapping[endp], int):\n                                        mf = annotation.mapping[endp]\n                                        expected_order = f_order + 1\n                                        if expected_order != self.refdata.structures[locus][mf]:\n                                            self.logger.info(self.logname + \" out of order12 -> \" + mf + \" \" + f)\n                                            del an.features[f]\n                                            continue\n\n                                    startp = an.features[f].location.start - 1\n                                    if startp in annotation.mapping and not isinstance(annotation.mapping[startp], int):\n                                        mf = annotation.mapping[startp]\n                                        expected_order = f_order - 1\n                                        if expected_order != self.refdata.structures[locus][mf]:\n                                            self.logger.info(self.logname + \" out of order22 -> \" + mf + \" \" + f)\n                                            del an.features[f]\n                                            continue\n\n                                if self.verbose and self.verbosity > 0:\n                                    sl = str(len(an.annotation[f]))\n\n                                    self.logger.info(self.logname + \" \" + locus\n                                                     + \" \" + f\n                                                     + \" len = \" + sl\n                                                     + \" | max = \"\n                                                     + str(max_length)\n                                                     + \" | min = \"\n                                                     + str(min_length))\n\n                                    if len(an.annotation[f]) <= max_length:\n                                        self.logger.info(self.logname\n                                                         + \" \" + locus\n                                                         + \" \" + f\n                                                         + \" \" + sl + \" <= \"\n                                                         + str(max_length))\n                                    else:\n                                        self.logger.info(self.logname\n                                                         + \" \" + locus\n                                                         + \" \" + f\n                                                         + \" \" + sl + \" ! <= !\"\n                                                         + str(max_length))\n\n                                    if len(an.annotation[f]) >= min_length:\n                                        self.logger.info(self.logname\n                                                         + \" \" + locus\n                                                         + \" \" + f\n                                                         + \" \" + sl + \" >= \"\n                                                         + str(min_length))\n                                    else:\n                                        self.logger.info(self.logname + \" \"\n                                                         + locus\n                                                         + \" \" + f\n                                                         + \" \" + sl + \" ! >= !\"\n                                                         + str(min_length))\n\n                                # Update the original annotation\n                                # with the returned annotation\n                                if(len(an.annotation[f]) <= max_length and\n                                        len(an.annotation[f]) >= min_length):\n                                    if self.verbose and self.verbosity > 0:\n                                        self.logger.info(self.logname\n                                                         + \" Annotated \" + f\n                                                         + \" with clustalo using \" +\n                                                         combseqr.id)\n                                        self.logger.info(self.logname\n                                                         + \" Coordinates for \" + f\n                                                         + str(an.features[f].location.start)\n                                                         + \" - \" + str(an.features[f].location.end)\n                                                         )\n                                    if annotation.annotation:\n                                        annotation.annotation.update({f:\n                                                                    an.annotation[f]\n                                                                  })\n                                        annotation.features.update({f:\n                                                                    an.features[f]\n                                                                  })  \n                                    else:\n                                        annotation.annotation = {}\n                                        annotation.annotation.update({f:\n                                                                    an.annotation[f]\n                                                                  })\n                                        annotation.features.update({f:\n                                                                    an.features[f]\n                                                                  })\n                                    if f in annotation.refmissing:\n                                        i = annotation.refmissing.index(f)\n                                        del annotation.refmissing[i]\n\n                                    if f in annotation.missing:\n                                        del annotation.missing[f]\n\n                                    #if b in annotation.blocks:\n                                    #    del annotation.blocks[annotation.blocks.index(b)]\n                                    #     print(annotation.blocks)\n                                    # if an.blocks:\n                                    #     #print(\"PARTIAL BLOCK\")\n                                    #     #print(an.blocks)\n                                    #     if b in missing_blocks:\n                                    #         del missing_blocks[missing_blocks.index(b)]\n\n                                    #     if self.verbose and self.verbosity > 0:\n                                    #         self.logger.info(self.logname\n                                    #                          + \" Part of block mapped\")\n\n                                    # else:\n                                    #     if self.verbose and self.verbosity > 0:\n                                    #         self.logger.info(self.logname\n                                    #                          + \" All blocks mapped\")\n                                        \n                                    #     del annotation.blocks[annotation.blocks.index(b)]\n                                    #     if b in missing_blocks:\n                                    #         del missing_blocks[missing_blocks.index(b)]\n                                else:\n                                    self.logger.info(self.logname + \" FAILED HERE!!!!!!!!!\")\n                                #elif b not in mbtmp and b in missing_blocks:\n                                #    mbtmp.append(b)\n                            else:\n                                self.logger.info(self.logname + \" OUT OF ORDER !!!!!!!!!!!!!!!!!!!\")\n\n                    # Update the coordinates\n                    coordinates = dict(map(lambda x: [x, 1], [i for i in range(0, len(sequence.seq)+1)]))\n                    for f in annotation.features:\n                        s = annotation.features[f].location.start\n                        e = annotation.features[f].location.end\n                        if s != 0:\n                            s += 1\n                            e += 1\n                        else:\n                            e += 1\n\n                        for i in range(s, e):\n                            annotation.mapping[i] = f\n                            if i in coordinates:\n                                del coordinates[i]\n\n                    # Get any remaining blocks after updated\n                    # annotation\n                    blocks = getblocks(coordinates)\n                    annotation.blocks = blocks\n\n                    # Check to see if that annotation is complete\n                    annotation.check_annotation()\n                    if annotation.complete_annotation:\n                        if self.verbose:\n                            self.logger.info(self.logname\n                                             + \" Completed annotation\"\n                                             + \" with targeted ref_align\")\n                        return annotation\n                    else:\n\n                        if an.features:\n                            # for f in an.features:\n                            #     f_order = self.refdata.structures[locus][f]\n                            #     # Only add features if they are after the\n                            #     # first feature mapped\n                            #     if f_order >= start_order and f not in annotation.features \\\n                            #             and f in annotation.annotation:\n                            #         annotation.features[f] = an.features[f]\n\n                            # Rerunning seqsearch with\n                            # new annotation from alignment\n                            tmpann = self.seqsearch.search_seqs(found_seqs,\n                                                                sequence,\n                                                                locus,\n                                                                partial_ann=annotation,\n                                                                run=run)\n\n                            if tmpann.complete_annotation:\n                                for f in tmpann.annotation:\n                                    if f not in annotation.annotation:\n                                        annotation.annotation[f] = tmpann.annotation[f]\n                                if self.verbose:\n                                    self.logger.info(self.logname\n                                                     + \" Completed annotation\"\n                                                     + \" with targeted ref_align and seqsearch!\")\n\n                                return tmpann\n                            annotation = tmpann\n\n                    ic += 1\n\n                # Has to be missing exons\n                exons_n = 0\n                for f in annotation.missing:\n                    if re.search(\"intron\", f) or re.search(\"UTR\", f):\n                        exons_n += 1\n\n                # Run exon only alignment\n                if len(exons.seq) >= 4 and exons_n > 0:\n                    exonan, ins, dels = align_seqs(exons, feat, locus, start,\n                                                   annotation.missing,\n                                                   len(annoated),\n                                                   cutoff=cutoff,\n                                                   verbose=self.align_verbose,\n                                                   verbosity=self.align_verbosity)\n                    mapped_exons = list(exonan.annotation.keys())\n                    if len(mapped_exons) >= 1:\n                        if self.verbose:\n                            self.logger.info(self.logname\n                                             + \" Annotated exons with align\")\n                        for f in exonan.annotation:\n                            if self.verbose and self.verbosity > 0:\n                                self.logger.info(self.logname\n                                                 + \" Annotated \"\n                                                 + f + \" len = \"\n                                                 + str(len(exonan\n                                                           .annotation[f])))\n                            annotation.annotation.update({f: exonan.annotation[f]})\n                            annotation.features.update({f: exonan.features[f]})\n\n                        coordinates = dict(map(lambda x: [x, 1], [i for i in range(0, len(sequence.seq)+1)]))\n                        for f in annotation.features:\n                            s = annotation.features[f].location.start\n                            e = annotation.features[f].location.end\n                            if s != 0:\n                                s += 1\n                                e += 1\n                            else:\n                                e += 1\n                            for i in range(s, e):\n                                annotation.mapping[i] = f\n                                if i in coordinates:\n                                    del coordinates[i]\n\n                        blocks = getblocks(coordinates)\n                        annotation.blocks = blocks\n                        annotation.check_annotation()\n                        if annotation.complete_annotation:\n                            if self.verbose:\n                                self.logger.info(self.logname + \" Completed annotation with targeted exons ref_align\")\n                            return annotation\n\n            return annotation\n        elif partial_ann:\n\n            annoated = []\n            if hasattr(partial_ann, 'annotation') and partial_ann.annotation:\n                annoated = list(partial_ann.annotation.keys())\n\n            # Do full sequence alignments\n            # any only extract out the part\n            # that couldn't be explained from above\n            if 0 in partial_ann.mapping \\\n                    and not isinstance(partial_ann.mapping[0], int):\n                ft = partial_ann.mapping[0]\n                start_order = self.refdata.structures[locus][ft]\n            else:\n                start_order = 0\n\n            # Extract the missing blocks and\n            # only align those blocks to the known\n            # missing features\n            # Start with all blocks missing\n            # and then delete block if it is found\n            tmp_missing = []\n            missing_blocks = partial_ann.blocks\n            for b in sorted(partial_ann.blocks):\n\n                # **** Check if block equals full input sequence *** #\n                # - If it does, then just align the ful\n                start = b[0]-1 if b[0] != 0 else 0\n                seq_feat = \\\n                    SeqFeature(\n                        FeatureLocation(\n                            ExactPosition(start),\n                            ExactPosition(b[len(b)-1]),\n                            strand=1),\n                        type=\"unmapped\")\n\n                feat = seq_feat.extract(partial_ann.seq)\n                combosrecs, exons, fullrec = self._refseqs(locus,\n                                                           start,\n                                                           partial_ann,\n                                                           feat,\n                                                           b)\n                if len(fullrec.seq) >= 4:\n\n                    fullref, ins, dels = align_seqs(fullrec, feat,\n                                                    locus, start,\n                                                    partial_ann.missing,\n                                                    len(annoated),\n                                                    cutoff=cutoff,\n                                                    verbose=self.align_verbose,\n                                                    verbosity=self.align_verbosity)\n\n                    if hasattr(fullref, 'features') and fullref.features:\n                        mapped_full = list(fullref.annotation.keys())\n                        if len(mapped_full) >= 1:\n\n                            if self.verbose:\n                                self.logger.info(self.logname\n                                                 + \" Annotated fullrec\"\n                                                 + \" with clustalo\")\n\n                            # If it wasn't found\n                            del missing_blocks[missing_blocks.index(b)]\n\n                            for f in fullref.annotation:\n                                if self.verbose and self.verbosity > 0:\n                                    self.logger.info(self.logname + \" Annotated \" + f + \" len = \" + str(len(fullref.annotation[f])))\n                                partial_ann.annotation.update({f: fullref.annotation[f]})\n\n                        if b in missing_blocks:\n                            del missing_blocks[missing_blocks.index(b)]\n                        else:\n                            for bm in tmp_missing:\n                                if bm in missing_blocks:\n                                    del missing_blocks[missing_blocks.index(bm)]\n\n                        for f in fullref.features:\n                            f_order = self.refdata.structures[locus][f]\n                            # Only add features if they are after the\n                            # first feature mapped\n                            if f_order >= start_order and f not in partial_ann.features \\\n                                    and f in partial_ann.annotation:\n                                partial_ann.features[f] = fullref.features[f]\n\n                        coordinates = dict(map(lambda x: [x, 1], [i for i in range(0, len(sequence.seq)+1)]))\n                        for f in partial_ann.features:\n                            s = partial_ann.features[f].location.start\n                            e = partial_ann.features[f].location.end\n                            if s != 0:\n                                s += 1\n                                e += 1\n                            else:\n                                e += 1\n                            for i in range(s, e):\n                                partial_ann.mapping[i] = f\n                                if i in coordinates:\n                                    del coordinates[i]\n\n                        blocks = getblocks(coordinates)\n                        partial_ann.check_annotation()\n                        if partial_ann.complete_annotation:\n                            if self.verbose:\n                                self.logger.info(self.logname + \" Annotated all features with clustalo\")\n                        \n                    return partial_ann\n\n            if self.verbose:\n                self.logger.info(self.logname\n                                 + \" Failed to annotate features\")\n            return ''", "response": "Aligns the found sequences with the given locus and annotation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_alignment(self, ref_seq, annotation) -> Annotation:\n        seq_features = get_seqs(ref_seq)\n        annoated_align = {}\n        allele = ref_seq.description.split(\",\")[0]\n        locus = allele.split(\"*\")[0].split(\"-\")[1]\n        for feat in seq_features:\n            if feat in annotation.annotation:\n                if isinstance(annotation.annotation[feat], DBSeq):\n                    seq_len = len(str(annotation.annotation[feat]))\n                    ref_len = len(seq_features[feat])\n                else:\n                    seq_len = len(str(annotation.annotation[feat].seq))\n                    ref_len = len(seq_features[feat])\n                if seq_len == ref_len:\n                    seq = list(annotation.annotation[feat].seq)\n                    gaps = self.refdata.annoated_alignments[locus][allele][feat]['Gaps']\n                    if self.verbose and self.verbosity > 0:\n                        self.logger.info(self.logname + \" Lengths match for \" + feat)\n                        self.logger.info(self.logname + \" Gaps at \" + feat)\n                        self.logger.info(self.logname +\n                                         \"-\".join([\",\".join([str(s)\n                                                             for s in g])\n                                                   for g in gaps]))\n                    for i in range(0, len(gaps)):\n                        for j in gaps[i]:\n                            loc = j\n                            seq.insert(loc, '-')\n                    nseq = ''.join(seq)\n                    annoated_align.update({feat: nseq})\n                else:\n                    in_seq = str(annotation.annotation[feat].seq)\n                    ref_seq = self.refdata.annoated_alignments[locus][allele][feat]['Seq']\n                    alignment = pairwise2.align.globalxx(in_seq, ref_seq)\n                    if self.verbose and self.verbosity > 0:\n                        self.logger.info(self.logname + \" Align2 -> in_seq != ref_len \" + feat)\n                        self.logger.info(self.logname + \" \" + str(len(in_seq)) + \" == \" + str(ref_len))\n                    annoated_align.update({feat: alignment[0][0]})\n            else:\n                nseq = ''.join(list(repeat('-', len(seq_features[feat]))))\n                annoated_align.update({feat: nseq})\n        annotation.aligned = annoated_align\n        return annotation", "response": "Method to add an alignment to an annotation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef object2xml(self, data):\n        if not self.__options['encoding']:\n            self.set_options(encoding=self.__encoding)\n\n        if self.__options['header_declare']:\n            self.__tree.append(self.build_xml_header())\n\n        root = self.__options['root']\n        if not root:\n            assert (isinstance(data, utils.DictTypes) and len(data) == 1), \\\n                'if root not specified, the data that dict object and length must be one required.'\n            root, data = data.items()[0]\n\n        self.build_tree(data, root)\n        xml = unicode(''.join(self.__tree).strip())\n\n        if self.__options['encoding'] != self.__encoding:\n            xml = xml.encode(self.__options['encoding'], errors=self.__options['errors'])\n        return xml", "response": "r Convert python object to xml string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_tree(self, data, tagname, attrs=None, depth=0):\n        if data is None:\n            data = ''\n        indent = ('\\n%s' % (self.__options['indent'] * depth)) if self.__options['indent'] else ''\n        if isinstance(data, utils.DictTypes):\n            if self.__options['hasattr'] and self.check_structure(data.keys()):\n                attrs, values = self.pickdata(data)\n                self.build_tree(values, tagname, attrs, depth)\n            else:\n                self.__tree.append('%s%s' % (indent, self.tag_start(tagname, attrs)))\n                iter = data.iteritems()\n                if self.__options['ksort']:\n                    iter = sorted(iter, key=lambda x:x[0], reverse=self.__options['reverse'])\n                for k, v in iter:\n                    attrs = {}\n                    if self.__options['hasattr'] and isinstance(v, utils.DictTypes) and self.check_structure(v.keys()):\n                        attrs, v = self.pickdata(v)\n                    self.build_tree(v, k, attrs, depth+1)\n                self.__tree.append('%s%s' % (indent, self.tag_end(tagname)))\n        elif utils.is_iterable(data):\n            for v in data:\n                self.build_tree(v, tagname, attrs, depth)\n        else:\n            self.__tree.append(indent)\n            data = self.safedata(data, self.__options['cdata'])\n            self.__tree.append(self.build_tag(tagname, data, attrs))", "response": "r Builds xml tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pickdata(self, data):\n        attrs = data.get(self.__options['attrkey']) or {}\n        values = data.get(self.__options['valuekey']) or ''\n        return (attrs, values)", "response": "r Pick data from attrkey and valuekey option."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts xml special chars to entities.", "response": "def safedata(self, data, cdata=True):\n        r\"\"\"Convert xml special chars to entities.\n\n        :param data: the data will be converted safe.\n        :param cdata: whether to use cdata. Default\uff1a``True``. If not, use :func:`cgi.escape` to convert data.\n        :type cdata: bool\n        :rtype: str\n        \"\"\"\n        safe = ('<![CDATA[%s]]>' % data) if cdata else cgi.escape(str(data), True)\n        return safe"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_tag(self, tag, text='', attrs=None):\n        return '%s%s%s' % (self.tag_start(tag, attrs), text, self.tag_end(tag))", "response": "r Build a tag full info include the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_attr(self, attrs):\n        attrs = sorted(attrs.iteritems(), key=lambda x: x[0])\n        return ' '.join(map(lambda x: '%s=\"%s\"' % x, attrs))", "response": "r Build tag attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tag_start(self, tag, attrs=None):\n        return '<%s %s>' % (tag, self.build_attr(attrs)) if attrs else '<%s>' % tag", "response": "r Build started tag info."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _function(self):\n\n        # some generic function\n        import time\n        import random\n        self.data['random data'] = None\n        self.data['image data'] = None\n        count = self.settings['count']\n        name = self.settings['name']\n        wait_time = self.settings['wait_time']\n\n        data = []\n        self.log('I ({:s}) am a test function counting to {:d} and creating random values'.format(self.name, count))\n        for i in range(count):\n            time.sleep(wait_time)\n            self.log('{:s} count {:02d}'.format(self.name, i))\n            data.append(random.random())\n            self.data = {'random data': data}\n            self.progress = 100. * (i + 1) / count\n            self.updateProgress.emit(self.progress)\n\n\n        self.data = {'random data':data}\n\n\n        # create image data\n        Nx = int(np.sqrt(len(self.data['random data'])))\n        img = np.array(self.data['random data'][0:Nx ** 2])\n        img = img.reshape((Nx, Nx))\n        self.data.update({'image data': img})", "response": "This is the actual function that will be executed by the main function. It will be called by the main function. It will create the random values and update the image data with the new random values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the data only the axes objects that are provided in axes_list", "response": "def _plot(self, axes_list, data = None):\n        \"\"\"\n        plots the data only the axes objects that are provided in axes_list\n        Args:\n            axes_list: a list of axes objects, this should be implemented in each subscript\n            data: data to be plotted if empty take self.data\n        Returns: None\n\n        \"\"\"\n\n        plot_type = self.settings['plot_style']\n        if data is None:\n            data = self.data\n\n        if data is not None and data is not {}:\n            if plot_type in ('main', 'two'):\n                if not data['random data'] is None:\n                    axes_list[0].plot(data['random data'])\n                    axes_list[0].hold(False)\n            if plot_type in ('aux', 'two', '2D'):\n                if not data['random data'] is None:\n                    axes_list[1].plot(data['random data'])\n                    axes_list[1].hold(False)\n            if plot_type == '2D':\n                if 'image data' in data and not data['image data'] is None:\n                    fig = axes_list[0].get_figure()\n                    implot = axes_list[0].imshow(data['image data'], cmap='pink', interpolation=\"nearest\", extent=[-1,1,1,-1])\n                    fig.colorbar(implot, label='kcounts/sec')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update(self, axes_list):\n        plot_type = self.settings['plot_style']\n        if plot_type == '2D':\n            # we expect exactely one image in the axes object (see ScriptDummy.plot)\n            implot = axes_list[1].get_images()[0]\n            # now update the data\n            implot.set_data(self.data['random data'])\n\n            colorbar = implot.colorbar\n\n            if not colorbar is None:\n                colorbar.update_bruteforce(implot)\n\n        else:\n            # fall back to default behaviour\n            Script._update(self, axes_list)", "response": "Update the data in already existing plots."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _function(self):\n        plant = self.instruments['plant']['instance']\n        controler = self.instruments['controler']['instance']\n        plant.update(self.instruments['plant']['settings'])\n        controler.update(self.instruments['controler']['settings'])\n\n        time_step = 1./self.settings['sample rate']\n\n        controler.update({'time_step': time_step})\n        self.last_plot = datetime.datetime.now()\n\n        controler.reset()\n        # if length changed we have to redefine the queue and carry over the data\n        if self.data['plant_output'].maxlen != self.settings['buffer_length']:\n            plant_output = deepcopy(self.data['plant_output'])\n            control_output = deepcopy(self.data['control_output'])\n            self.data = {'plant_output': deque(maxlen=self.settings['buffer_length']),\n                         'control_output': deque(maxlen=self.settings['buffer_length'])}\n\n            x = list(range(min(len(plant_output), self.settings['buffer_length'])))\n            x.reverse()\n            for i in x:\n                self.data['plant_output'].append(plant_output[-i-1])\n                self.data['control_output'].append(control_output[-i - 1])\n\n        while not self._abort:\n\n            measurement = plant.output\n\n            self.data['plant_output'].append(measurement)\n            control_value = controler.controler_output(measurement)\n            self.data['control_output'].append(control_value)\n\n            if self.settings['on/off']:\n                print(('set plant control', control_value))\n                plant.control = float(control_value)\n\n            self.progress = 50\n            self.updateProgress.emit(self.progress)\n\n            time.sleep(time_step)", "response": "This is the actual function that is executed by the main function. It is called by the main function that is run by the main function. It is called by the main function that is run by the main function. It is called by the main function that is run by the main function. It is called by the main function that is run by the main function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef session_scope(session_cls=None):\n    session = session_cls() if session_cls else Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()", "response": "Provide a transactional scope around a series of operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_file_dialog(self):\n        dialog = QtWidgets.QFileDialog\n        sender = self.sender()\n        if sender == self.btn_open_source:\n            textbox = self.source_path\n        elif sender == self.btn_open_target:\n            textbox = self.target_path\n        folder = dialog.getExistingDirectory(self, 'Select a file:', textbox.text(), options = QtWidgets.QFileDialog.ShowDirsOnly)\n        if str(folder) != '':\n            textbox.setText(folder)\n            # load elements from file and display in tree\n            if sender == self.btn_open_source:\n                self.reset_avaliable(folder)", "response": "Opens a file dialog to get the path to a file and put tha tpath in the correct textbox\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_avaliable(self, folder):\n        try:\n            self.list_script_model.removeRows(0, self.list_script_model.rowCount())\n            if self.cmb_select_type.currentText() == 'Script':\n                self.avaliable = find_scripts_in_python_files(folder)\n            elif self.cmb_select_type.currentText() == 'Instrument':\n                self.avaliable = find_instruments_in_python_files(folder)\n            self.fill_list(self.list_script, self.avaliable.keys())\n            for key in self.avaliable.keys():\n                self.error_array.update({key: ''})\n        except Exception:\n            msg = QtWidgets.QMessageBox()\n            msg.setText(\"Unable to parse all of the files in this folder to find possible scripts and instruments. There are non-python files or python files that are unreadable. Please select a folder that contains only pylabcontrol style python files.\")\n            msg.exec_()", "response": "Resets the dialog box by finding all avaliable scripts that can be imported in the input folder."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef class_type_changed(self):\n        if self.source_path.text():\n            self.reset_avaliable(self.source_path.text())", "response": "Resets the avaliable flag if the class type is changed from instruments to scripts or vice versa\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfill a tree with nested parameters", "response": "def fill_list(self, list, input_list):\n        \"\"\"\n        fills a tree with nested parameters\n        Args:\n            tree: QtGui.QTreeView to fill\n            parameters: dictionary or Parameter object which contains the information to use to fill\n        \"\"\"\n        for name in input_list:\n            # print(index, loaded_item, loaded_item_settings)\n            item = QtGui.QStandardItem(name)\n            item.setSelectable(True)\n            item.setEditable(False)\n\n            list.model().appendRow(item)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind apropriate utxos to include in rawtx while being careful to never spend old transactions with a lot of coin age.", "response": "def select_inputs(self, address: str, amount: int) -> dict:\n        '''finds apropriate utxo's to include in rawtx, while being careful\n        to never spend old transactions with a lot of coin age.\n        Argument is intiger, returns list of apropriate UTXO's'''\n\n        utxos = []\n        utxo_sum = Decimal(0)\n        for tx in sorted(self.listunspent(address=address), key=itemgetter('confirmations')):\n\n            if tx[\"address\"] not in (self.pa_parameters.P2TH_addr,\n                                     self.pa_parameters.test_P2TH_addr):\n\n                utxos.append(\n                        MutableTxIn(txid=tx['txid'],\n                                    txout=tx['vout'],\n                                    sequence=Sequence.max(),\n                                    script_sig=ScriptSig.empty())\n                         )\n\n                utxo_sum += Decimal(tx[\"amount\"])\n                if utxo_sum >= amount:\n                    return {'utxos': utxos, 'total': utxo_sum}\n\n        if utxo_sum < amount:\n            raise InsufficientFunds(\"Insufficient funds.\")\n\n        raise Exception(\"undefined behavior :.(\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef listunspent(\n        self,\n        address: str=\"\",\n        minconf: int=1,\n        maxconf: int=999999,\n    ) -> list:\n        '''list UTXOs\n        modified version to allow filtering by address.\n        '''\n        if address:\n            return self.req(\"listunspent\", [minconf, maxconf, [address]])\n\n        return self.req(\"listunspent\", [minconf, maxconf])", "response": "list unspent UTXOs items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef score_pairwise(aseq, bseq):\n    assert len(aseq) == len(bseq)\n\n    # Affine gap penalties -- default values from EMBOSS needle/water\n    GAP_OPEN = -10.0\n    GAP_EXTEND = -0.5\n    GAP_CHARS = frozenset('-.')\n\n    score = 0.0\n    in_gap = True   # Don't apply the opening penalty to the N-terminal gap\n\n    for ares, bres in zip(aseq.upper(), bseq.upper()):\n        if ares in GAP_CHARS and bres in GAP_CHARS:\n            # Both are gaps -- this happens in multiple sequence alignments\n            continue\n        match = blosum62.get((ares, bres), None)\n        if match is None:\n            assert GAP_CHARS.intersection((ares, bres)), \\\n                    \"Expected one gap in: \" + str((ares, bres))\n            # Gap\n            if not in_gap:\n                score += GAP_OPEN\n                in_gap = True\n            score += GAP_EXTEND\n        else:\n            in_gap = False\n            score += match\n\n    if in_gap:\n        # Correct for a penalty on the C-terminal gap\n        score -= GAP_OPEN\n\n    return score", "response": "Compute pairwise distances between two sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef identity_abs(aseq, bseq):\n    assert len(aseq) == len(bseq)\n    return sum(a == b\n               for a, b in zip(aseq, bseq)\n               if not (a in '-.' and b in '-.'))", "response": "Compute absolute identity between sequence strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes absolute identity between sequence strings.", "response": "def identity_avg(aseq, bseq):\n    \"\"\"Compute absolute identity (# matching sites) between sequence strings.\"\"\"\n    match = identity_abs(aseq, bseq)\n    alen = len(aseq.replace('-', '').replace('.', ''))\n    blen = len(bseq.replace('-', '').replace('.', ''))\n    avg_len = 0.5 * (alen + blen)\n    return match / avg_len"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncommands line tool to convert from Remind to iCalendar", "response": "def rem2ics():\n    \"\"\"Command line tool to convert from Remind to iCalendar\"\"\"\n    # pylint: disable=maybe-no-member\n    from argparse import ArgumentParser, FileType\n    from dateutil.parser import parse\n    from sys import stdin, stdout\n\n    parser = ArgumentParser(description='Converter from Remind to iCalendar syntax.')\n    parser.add_argument('-s', '--startdate', type=lambda s: parse(s).date(),\n                        default=date.today() - timedelta(weeks=12),\n                        help='Start offset for remind call (default: -12 weeks)')\n    parser.add_argument('-m', '--month', type=int, default=15,\n                        help='Number of month to generate calendar beginning wit startdate (default: 15)')\n    parser.add_argument('-a', '--alarm', type=int, default=-10,\n                        help='Trigger time for the alarm before the event in minutes (default: -10)')\n    parser.add_argument('-z', '--zone',\n                        help='Timezone of Remind file (default: local timezone)')\n    parser.add_argument('infile', nargs='?', default=expanduser('~/.reminders'),\n                        help='The Remind file to process (default: ~/.reminders)')\n    parser.add_argument('outfile', nargs='?', type=FileType('w'), default=stdout,\n                        help='Output iCalendar file (default: stdout)')\n    args = parser.parse_args()\n\n    zone = timezone(args.zone) if args.zone else None\n\n    if args.infile == '-':\n        remind = Remind(args.infile, zone, args.startdate, args.month, timedelta(minutes=args.alarm))\n        vobject = remind.stdin_to_vobject(stdin.read())\n        if vobject:\n            args.outfile.write(vobject.serialize())\n    else:\n        remind = Remind(args.infile, zone, args.startdate, args.month, timedelta(minutes=args.alarm))\n        args.outfile.write(remind.to_vobject().serialize())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ics2rem():\n    from argparse import ArgumentParser, FileType\n    from sys import stdin, stdout\n\n    parser = ArgumentParser(description='Converter from iCalendar to Remind syntax.')\n    parser.add_argument('-l', '--label', help='Label for every Remind entry')\n    parser.add_argument('-p', '--priority', type=int,\n                        help='Priority for every Remind entry (0..9999)')\n    parser.add_argument('-t', '--tag', action='append',\n                        help='Tag(s) for every Remind entry')\n    parser.add_argument('--tail',\n                        help='Text to append to every remind summary, following final %%\"')\n    parser.add_argument('--sep', default=\" \",\n                        help='String to separate summary (and tail) from description')\n    parser.add_argument('--postdate',\n                        help='String to follow the date in every Remind entry. '\n                        'Useful for entering \"back\" and \"delta\" fields (see man remind).')\n    parser.add_argument('--posttime',\n                        help='String to follow the time in every timed Remind entry. '\n                        'Useful for entering \"tdelta\" and \"trepeat\" fields (see man remind).')\n    parser.add_argument('-z', '--zone',\n                        help='Timezone of Remind file (default: local timezone)')\n    parser.add_argument('infile', nargs='?', type=FileType('r'), default=stdin,\n                        help='Input iCalendar file (default: stdin)')\n    parser.add_argument('outfile', nargs='?', type=FileType('w'), default=stdout,\n                        help='Output Remind file (default: stdout)')\n    args = parser.parse_args()\n\n    zone = timezone(args.zone) if args.zone else None\n\n    vobject = readOne(args.infile.read())\n    rem = Remind(localtz=zone).to_reminders(\n        vobject, args.label, args.priority, args.tag, args.tail, args.sep,\n        args.postdate, args.posttime)\n    args.outfile.write(rem)", "response": "Command line tool to convert from iCalendar to Remind"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall remind and parses the output into a dict", "response": "def _parse_remind(self, filename, lines=''):\n        \"\"\"Calls remind and parses the output into a dict\n\n        filename -- the remind file (included files will be used as well)\n        lines -- used as stdin to remind (filename will be set to -)\n        \"\"\"\n        files = {}\n        reminders = {}\n        if lines:\n            filename = '-'\n            files[filename] = lines\n            reminders[filename] = {}\n\n        cmd = ['remind', '-l', '-s%d' % self._month, '-b1', '-y', '-r',\n               filename, str(self._startdate)]\n        try:\n            rem = Popen(cmd, stdin=PIPE, stdout=PIPE).communicate(input=lines.encode('utf-8'))[0].decode('utf-8')\n        except OSError:\n            raise OSError('Error running: %s' % ' '.join(cmd))\n\n        rem = rem.splitlines()\n        for (fileinfo, line) in zip(rem[::2], rem[1::2]):\n            fileinfo = fileinfo.split()\n\n            src_filename = fileinfo[3]\n            if src_filename not in files:\n                # There is a race condition with the remind call above here.\n                # This could be solved by parsing the remind -de output,\n                # but I don't see an easy way to do that.\n                files[src_filename] = open(src_filename).readlines()\n                reminders[src_filename] = {}\n                mtime = getmtime(src_filename)\n                if mtime > self._mtime:\n                    self._mtime = mtime\n\n            text = files[src_filename][int(fileinfo[2]) - 1]\n            event = self._parse_remind_line(line, text)\n            if event['uid'] in reminders[src_filename]:\n                reminders[src_filename][event['uid']]['dtstart'] += event['dtstart']\n                reminders[src_filename][event['uid']]['line'] += line\n            else:\n                reminders[src_filename][event['uid']] = event\n                reminders[src_filename][event['uid']]['line'] = line\n\n        # Find included files without reminders and add them to the file list\n        for source in files.values():\n            for line in source:\n                if line.startswith('include'):\n                    new_file = line.split(' ')[1].strip()\n                    if new_file not in reminders:\n                        reminders[new_file] = {}\n                        mtime = getmtime(new_file)\n                        if mtime > self._mtime:\n                            self._mtime = mtime\n\n        return reminders"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a line of remind output into a dict containing the keys of the event that are available in the remind module.", "response": "def _parse_remind_line(self, line, text):\n        \"\"\"Parse a line of remind output into a dict\n\n        line -- the remind output\n        text -- the original remind input\n        \"\"\"\n        event = {}\n        line = line.split(None, 6)\n        dat = [int(f) for f in line[0].split('/')]\n        if line[4] != '*':\n            start = divmod(int(line[4]), 60)\n            event['dtstart'] = [datetime(dat[0], dat[1], dat[2], start[0], start[1], tzinfo=self._localtz)]\n            if line[3] != '*':\n                event['duration'] = timedelta(minutes=int(line[3]))\n        else:\n            event['dtstart'] = [date(dat[0], dat[1], dat[2])]\n\n        msg = ' '.join(line[5:]) if line[4] == '*' else line[6]\n        msg = msg.strip().replace('%_', '\\n').replace('[\"[\"]', '[')\n\n        if ' at ' in msg:\n            (event['msg'], event['location']) = msg.rsplit(' at ', 1)\n        else:\n            event['msg'] = msg\n\n        if '%\"' in text:\n            event['description'] = Remind._gen_description(text)\n\n        tags = line[2].split(',')\n\n        classes = ['PUBLIC', 'PRIVATE', 'CONFIDENTIAL']\n\n        for tag in tags[:-1]:\n            if tag in classes:\n                event['class'] = tag\n\n        event['categories'] = [tag for tag in tags[:-1] if tag not in classes]\n\n        event['uid'] = '%s@%s' % (tags[-1][7:], getfqdn())\n\n        return event"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the distance between all dates and 0 if they are different", "response": "def _interval(dates):\n        \"\"\"Return the distance between all dates and 0 if they are different\"\"\"\n        interval = (dates[1] - dates[0]).days\n        last = dates[0]\n        for dat in dates[1:]:\n            if (dat - last).days != interval:\n                return 0\n            last = dat\n        return interval"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _gen_dtend_rrule(dtstarts, vevent):\n        interval = Remind._interval(dtstarts)\n        if interval > 0 and interval % 7 == 0:\n            rset = rrule.rruleset()\n            rset.rrule(rrule.rrule(freq=rrule.WEEKLY, interval=interval // 7, count=len(dtstarts)))\n            vevent.rruleset = rset\n        elif interval > 1:\n            rset = rrule.rruleset()\n            rset.rrule(rrule.rrule(freq=rrule.DAILY, interval=interval, count=len(dtstarts)))\n            vevent.rruleset = rset\n        elif interval > 0:\n            if isinstance(dtstarts[0], datetime):\n                rset = rrule.rruleset()\n                rset.rrule(rrule.rrule(freq=rrule.DAILY, count=len(dtstarts)))\n                vevent.rruleset = rset\n            else:\n                vevent.add('dtend').value = dtstarts[-1] + timedelta(days=1)\n        else:\n            rset = rrule.rruleset()\n            if isinstance(dtstarts[0], datetime):\n                for dat in dtstarts:\n                    rset.rdate(dat)\n            else:\n                for dat in dtstarts:\n                    rset.rdate(datetime(dat.year, dat.month, dat.day))\n            # temporary set dtstart to a different date, so it's not\n            # removed from rset by python-vobject works around bug in\n            # Android:\n            # https://github.com/rfc2822/davdroid/issues/340\n            vevent.dtstart.value = dtstarts[0] - timedelta(days=1)\n            vevent.rruleset = rset\n            vevent.dtstart.value = dtstarts[0]\n            if not isinstance(dtstarts[0], datetime):\n                vevent.add('dtend').value = dtstarts[0] + timedelta(days=1)", "response": "Generate an rdate or rrule from a list of dates and add it to the vevent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _gen_vevent(self, event, vevent):\n        vevent.add('dtstart').value = event['dtstart'][0]\n        vevent.add('dtstamp').value = datetime.fromtimestamp(self._mtime)\n        vevent.add('summary').value = event['msg']\n        vevent.add('uid').value = event['uid']\n\n        if 'class' in event:\n            vevent.add('class').value = event['class']\n\n        if 'categories' in event and len(event['categories']) > 0:\n            vevent.add('categories').value = event['categories']\n\n        if 'location' in event:\n            vevent.add('location').value = event['location']\n\n        if 'description' in event:\n            vevent.add('description').value = event['description']\n\n        if isinstance(event['dtstart'][0], datetime):\n            if self._alarm != timedelta():\n                valarm = vevent.add('valarm')\n                valarm.add('trigger').value = self._alarm\n                valarm.add('action').value = 'DISPLAY'\n                valarm.add('description').value = event['msg']\n\n            if 'duration' in event:\n                vevent.add('duration').value = event['duration']\n            else:\n                vevent.add('dtend').value = event['dtstart'][0]\n\n        elif len(event['dtstart']) == 1:\n            vevent.add('dtend').value = event['dtstart'][0] + timedelta(days=1)\n\n        if len(event['dtstart']) > 1:\n            Remind._gen_dtend_rrule(event['dtstart'], vevent)", "response": "Generate vevent from given event"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreloading Remind files if the mtime is newer", "response": "def _update(self):\n        \"\"\"Reload Remind files if the mtime is newer\"\"\"\n        update = not self._reminders\n\n        with self._lock:\n            for fname in self._reminders:\n                if getmtime(fname) > self._mtime:\n                    update = True\n                    break\n\n            if update:\n                self._reminders = self._parse_remind(self._filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all UIDs of all reminders in the file excluding included files", "response": "def get_uids(self, filename=None):\n        \"\"\"UIDs of all reminders in the file excluding included files\n        If a filename is specified, only it's UIDs are return, otherwise all.\n\n        filename -- the remind file\n        \"\"\"\n        self._update()\n\n        if filename:\n            if filename not in self._reminders:\n                return []\n            return self._reminders[filename].keys()\n        return [uid for uids in self._reminders.values() for uid in uids]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns iCal objects and etags of all Remind entries in uids", "response": "def to_vobjects(self, filename, uids=None):\n        \"\"\"Return iCal objects and etags of all Remind entries in uids\n\n        filename -- the remind file\n        uids -- the UIDs of the Remind lines (all if None)\n        \"\"\"\n        self._update()\n\n        if not uids:\n            uids = self._reminders[filename]\n\n        items = []\n\n        for uid in uids:\n            cal = iCalendar()\n            self._gen_vevent(self._reminders[filename][uid], cal.add('vevent'))\n            etag = md5()\n            etag.update(self._reminders[filename][uid]['line'].encode(\"utf-8\"))\n            items.append((uid, cal, '\"%s\"' % etag.hexdigest()))\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_vobject(self, filename=None, uid=None):\n        self._update()\n\n        cal = iCalendar()\n        if uid:\n            self._gen_vevent(self._reminders[filename][uid], cal.add('vevent'))\n        elif filename:\n            for event in self._reminders[filename].values():\n                self._gen_vevent(event, cal.add('vevent'))\n        else:\n            for filename in self._reminders:\n                for event in self._reminders[filename].values():\n                    self._gen_vevent(event, cal.add('vevent'))\n        return cal", "response": "Return iCal object of Remind lines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn iCal object of the Remind commands in lines", "response": "def stdin_to_vobject(self, lines):\n        \"\"\"Return iCal object of the Remind commands in lines\"\"\"\n        cal = iCalendar()\n        for event in self._parse_remind('-', lines)['-'].values():\n            self._gen_vevent(event, cal.add('vevent'))\n        return cal"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_rruleset(rruleset):\n        # pylint: disable=protected-access\n\n        if rruleset._rrule[0]._freq == 0:\n            return []\n\n        rep = []\n        if rruleset._rrule[0]._byweekday and len(rruleset._rrule[0]._byweekday) > 1:\n            rep.append('*1')\n        elif rruleset._rrule[0]._freq == rrule.DAILY:\n            rep.append('*%d' % rruleset._rrule[0]._interval)\n        elif rruleset._rrule[0]._freq == rrule.WEEKLY:\n            rep.append('*%d' % (7 * rruleset._rrule[0]._interval))\n        else:\n            return Remind._parse_rdate(rruleset._rrule[0])\n\n        if rruleset._rrule[0]._byweekday and len(rruleset._rrule[0]._byweekday) > 1:\n            daynums = set(range(7)) - set(rruleset._rrule[0]._byweekday)\n            weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n            days = [weekdays[day] for day in daynums]\n            rep.append('SKIP OMIT %s' % ' '.join(days))\n\n        if rruleset._rrule[0]._until:\n            rep.append(rruleset._rrule[0]._until.strftime('UNTIL %b %d %Y').replace(' 0', ' '))\n        elif rruleset._rrule[0]._count:\n            rep.append(rruleset[-1].strftime('UNTIL %b %d %Y').replace(' 0', ' '))\n\n        return rep", "response": "Convert from iCal rrule to Remind recurrence syntax"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _event_duration(vevent):\n        if hasattr(vevent, 'dtend'):\n            return vevent.dtend.value - vevent.dtstart.value\n        elif hasattr(vevent, 'duration') and vevent.duration.value:\n            return vevent.duration.value\n        return timedelta(0)", "response": "unify dtend and duration to the duration of the given vevent"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _gen_msg(vevent, label, tail, sep):\n        rem = ['MSG']\n        msg = []\n        if label:\n            msg.append(label)\n\n        if hasattr(vevent, 'summary') and vevent.summary.value:\n            msg.append(Remind._rem_clean(vevent.summary.value))\n        else:\n            msg.append('empty reminder')\n\n        if hasattr(vevent, 'location') and vevent.location.value:\n            msg.append('at %s' % Remind._rem_clean(vevent.location.value))\n\n        has_desc = hasattr(vevent, 'description') and vevent.description.value\n\n        if tail or has_desc:\n            rem.append('%%\"%s%%\"' % ' '.join(msg))\n        else:\n            rem.append(' '.join(msg))\n\n        if tail:\n            rem.append(tail)\n\n        if has_desc:\n            rem[-1] += sep + Remind._rem_clean(vevent.description.value)\n\n        return ' '.join(rem)", "response": "Generate a Remind MSG from the given vevent."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_remind(self, vevent, label=None, priority=None, tags=None, tail=None,\n                  sep=\" \", postdate=None, posttime=None):\n        \"\"\"Generate a Remind command from the given vevent\"\"\"\n        remind = ['REM']\n\n        trigdates = None\n        if hasattr(vevent, 'rrule'):\n            trigdates = Remind._parse_rruleset(vevent.rruleset)\n\n        dtstart = vevent.dtstart.value\n        # If we don't get timezone information, handle it as a naive datetime.\n        # See https://github.com/jspricke/python-remind/issues/2 for reference.\n        if isinstance(dtstart, datetime) and dtstart.tzinfo:\n            dtstart = dtstart.astimezone(self._localtz)\n\n        dtend = None\n        if hasattr(vevent, 'dtend'):\n            dtend = vevent.dtend.value\n        if isinstance(dtend, datetime) and dtend.tzinfo:\n            dtend = dtend.astimezone(self._localtz)\n\n        if not hasattr(vevent, 'rdate') and not isinstance(trigdates, str):\n            remind.append(dtstart.strftime('%b %d %Y').replace(' 0', ' '))\n\n        if postdate:\n            remind.append(postdate)\n\n        if priority:\n            remind.append('PRIORITY %s' % priority)\n\n        if isinstance(trigdates, list):\n            remind.extend(trigdates)\n\n        duration = Remind._event_duration(vevent)\n\n        if type(dtstart) is date and duration.days > 1:\n            remind.append('*1')\n            if dtend is not None:\n                dtend -= timedelta(days=1)\n                remind.append(dtend.strftime('UNTIL %b %d %Y').replace(' 0', ' '))\n\n        if isinstance(dtstart, datetime):\n            remind.append(dtstart.strftime('AT %H:%M').replace(' 0', ' '))\n\n            if posttime:\n                remind.append(posttime)\n\n            if duration.total_seconds() > 0:\n                remind.append('DURATION %d:%02d' % divmod(duration.total_seconds() / 60, 60))\n\n        if hasattr(vevent, 'rdate'):\n            remind.append(Remind._parse_rdate(vevent.rdate.value))\n        elif isinstance(trigdates, str):\n            remind.append(trigdates)\n\n        if hasattr(vevent, 'class'):\n            remind.append('TAG %s' % Remind._abbr_tag(vevent.getChildValue('class')))\n\n        if tags:\n            remind.extend(['TAG %s' % Remind._abbr_tag(tag) for tag in tags])\n\n        if hasattr(vevent, 'categories_list'):\n            for categories in vevent.categories_list:\n                for category in categories.value:\n                    remind.append('TAG %s' % Remind._abbr_tag(category))\n\n        remind.append(Remind._gen_msg(vevent, label, tail, sep))\n\n        return ' '.join(remind) + '\\n'", "response": "Generate a Remind command from a vevent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn Remind commands for all events of a iCalendar", "response": "def to_reminders(self, ical, label=None, priority=None, tags=None,\n                     tail=None, sep=\" \", postdate=None, posttime=None):\n        \"\"\"Return Remind commands for all events of a iCalendar\"\"\"\n        if not hasattr(ical, 'vevent_list'):\n            return ''\n\n        reminders = [self.to_remind(vevent, label, priority, tags, tail, sep,\n                                    postdate, posttime)\n                     for vevent in ical.vevent_list]\n        return ''.join(reminders)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend a Remind command generated from the iCalendar to the file", "response": "def append_vobject(self, ical, filename=None):\n        \"\"\"Append a Remind command generated from the iCalendar to the file\"\"\"\n        if not filename:\n            filename = self._filename\n        elif filename not in self._reminders:\n            return\n\n        with self._lock:\n            outdat = self.to_reminders(ical)\n            open(filename, 'a').write(outdat)\n\n        return Remind._get_uid(outdat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, uid, filename=None):\n        if not filename:\n            filename = self._filename\n        elif filename not in self._reminders:\n            return\n\n        uid = uid.split('@')[0]\n\n        with self._lock:\n            rem = open(filename).readlines()\n            for (index, line) in enumerate(rem):\n                if uid == md5(line[:-1].encode('utf-8')).hexdigest():\n                    del rem[index]\n                    open(filename, 'w').writelines(rem)\n                    break", "response": "Remove the Remind command with the uid from the file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace(self, uid, ical, filename=None):\n        return self.replace_vobject(uid, readOne(ical), filename)", "response": "Update the Remind command with the uid in the file with the iCalendar"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the Remind command with the uid in the file with the new iCalendar", "response": "def replace_vobject(self, uid, ical, filename=None):\n        \"\"\"Update the Remind command with the uid in the file with the new iCalendar\"\"\"\n        if not filename:\n            filename = self._filename\n        elif filename not in self._reminders:\n            return\n\n        uid = uid.split('@')[0]\n\n        with self._lock:\n            rem = open(filename).readlines()\n            for (index, line) in enumerate(rem):\n                if uid == md5(line[:-1].encode('utf-8')).hexdigest():\n                    rem[index] = self.to_reminders(ical)\n                    new_uid = self._get_uid(rem[index])\n                    open(filename, 'w').writelines(rem)\n                    return new_uid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move_vobject(self, uid, from_file, to_file):\n        if from_file not in self._reminders or to_file not in self._reminders:\n            return\n\n        uid = uid.split('@')[0]\n\n        with self._lock:\n            rem = open(from_file).readlines()\n            for (index, line) in enumerate(rem):\n                if uid == md5(line[:-1].encode('utf-8')).hexdigest():\n                    del rem[index]\n                    open(from_file, 'w').writelines(rem)\n                    open(to_file, 'a').write(line)\n                    break", "response": "Move the Remind command with the uid from from_file to to_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expand_include(filename):\n        open_files = set()\n\n        def _expand_include_rec(filename):\n            if filename in open_files:\n                raise RuntimeError('Recursive include statement detected for '\n                                   'file: ' + filename)\n            else:\n                open_files.add(filename)\n\n            with open(filename) as open_file:\n                for line in open_file:\n                    line_stripped = line.strip().replace(\"//\", \"#\")\n                    if line_stripped.startswith('@include '):\n                        inc_to_clean = line_stripped.split(None, 1)[1]\n                        inc_filename = inc_to_clean.replace('\"',\" \").strip()\n                        for included_line in _expand_include_rec(inc_filename):\n                            yield included_line\n                    else:\n                        yield line\n\n            open_files.remove(filename)\n\n        try:\n            lines = []\n            for line in _expand_include_rec(filename):\n                lines.append(line)\n            return ''.join(lines)\n        except RuntimeError:\n            return None", "response": "Expand the content of a file into a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_info(self):\n\n        sender = self.sender()\n        tree = sender.parent()\n        index = tree.selectedIndexes()\n        info = ''\n        if index != []:\n            index = index[0]\n            name = str(index.model().itemFromIndex(index).text())\n\n            if name in set(list(self.elements_from_file.keys()) + list(self.elements_selected.keys())):\n                probe_name = None\n                instrument_name = name\n            else:\n                instrument_name = str(index.model().itemFromIndex(index).parent().text())\n                probe_name = name\n\n\n\n            module = __import__('pylabcontrol.instruments', fromlist=[instrument_name])\n            if probe_name is None:\n                info = getattr(module, instrument_name).__doc__\n            else:\n                if probe_name in list(getattr(module, instrument_name)._PROBES.keys()):\n                    info = getattr(module, instrument_name)._PROBES[probe_name]\n\n        if info is not None:\n            self.lbl_info.setText(info)", "response": "Displays the doc string of the selected element"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfills a tree with nested parameters from input_dict.", "response": "def fill_tree(self, tree, input_dict):\n        \"\"\"\n        fills a tree with nested parameters\n        Args:\n            tree: QtGui.QTreeView\n            parameters: dictionary or Parameter object\n\n        Returns:\n\n        \"\"\"\n\n\n        def removeAll(tree):\n\n            if tree.model().rowCount() > 0:\n                for i in range(0, tree.model().rowCount()):\n                    item = tree.model().item(i)\n                    del item\n                    tree.model().removeRows(0, tree.model().rowCount())\n                    tree.model().reset()\n\n        def add_probe(tree, instrument, probes):\n            item = QtGui.QStandardItem(instrument)\n            item.setEditable(False)\n\n            for probe in probes.split(','):\n                child_name = QtGui.QStandardItem(probe)\n                child_name.setDragEnabled(True)\n                child_name.setSelectable(True)\n                child_name.setEditable(False)\n                item.appendRow(child_name)\n            tree.model().appendRow(item)\n\n        removeAll(tree)\n\n        for index, (instrument, probes) in enumerate(input_dict.items()):\n            add_probe(tree, instrument, probes)\n            # tree.setFirstColumnSpanned(index, self.tree_infile.rootIndex(), True)\n        tree.expandAll()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if there are any grandparents.", "response": "def check_grandparents(self, mother = None, father = None):\n        \"\"\"\n        Check if there are any grand parents.\n        \n        Set the grandparents id:s\n        \n        Arguments:\n            mother (Individual): An Individual object that represents the mother\n            father (Individual): An Individual object that represents the father\n        \n        \n        \"\"\"\n        if mother:\n            if mother.mother != '0':\n                self.grandparents[mother.mother] =  ''\n            elif mother.father != '0':\n                self.grandparents[mother.father] = ''\n        if father:\n            if father.mother != '0':\n                self.grandparents[father.mother] =  ''\n            elif father.father != '0':\n                self.grandparents[father.father] = ''\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_json(self):\n        self.logger.debug(\"Returning json info\")\n        individual_info = {\n            'family_id': self.family,\n            'id':self.individual_id, \n            'sex':str(self.sex), \n            'phenotype': str(self.phenotype), \n            'mother': self.mother, \n            'father': self.father,\n            'extra_info': self.extra_info\n        }\n        return individual_info", "response": "Return the individual info in a dictionary for json."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the individual info in a madeline formated string", "response": "def to_madeline(self):\n        \"\"\"\n        Return the individual info in a madeline formated string\n        \"\"\"\n        #Convert sex to madeleine type\n        self.logger.debug(\"Returning madeline info\")\n        if self.sex == 1:\n            madeline_gender = 'M'\n        elif self.sex == 2:\n            madeline_gender = 'F'\n        else:\n            madeline_gender = '.'\n        #Convert father to madeleine type\n        if self.father == '0':\n            madeline_father = '.'\n        else:\n            madeline_father = self.father\n        #Convert mother to madeleine type\n        if self.mother == '0':\n            madeline_mother = '.'\n        else:\n            madeline_mother = self.mother\n        #Convert phenotype to madeleine type\n        if self.phenotype == 1:\n            madeline_phenotype = 'U'\n        elif self.phenotype == 2:\n            madeline_phenotype = 'A'\n        else:\n            madeline_phenotype = '.'\n        \n        return \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\".format(\n            self.family, self.individual_id, madeline_gender, \n            madeline_father, madeline_mother, madeline_phenotype,\n            self.proband, self.consultand, self.alive\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_rfc3339(d, local_tz=True):\n    try:\n        if local_tz:\n            d = datetime.datetime.fromtimestamp(d)\n        else:\n            d = datetime.datetime.utcfromtimestamp(d)\n    except TypeError:\n        pass\n\n    if not isinstance(d, datetime.date):\n        raise TypeError('Not timestamp or date object. Got %r.' % type(d))\n\n    if not isinstance(d, datetime.datetime):\n        d = datetime.datetime(*d.timetuple()[:3])\n\n    return ('%04d-%02d-%02dT%02d:%02d:%02d%s' %\n            (d.year, d.month, d.day, d.hour, d.minute, d.second,\n             _generate_timezone(d, local_tz)))", "response": "generate rfc3339 time format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_offset(date, local_tz):\n    if local_tz:\n        #handle year before 1970 most sytem there is no timezone information before 1970.\n        if date.year < 1970:\n            # Use 1972 because 1970 doesn't have a leap day\n            t = time.mktime(date.replace(year=1972).timetuple)\n        else:\n            t = time.mktime(date.timetuple())\n\n        # handle daylightsaving, if daylightsaving use altzone, otherwise use timezone\n        if time.localtime(t).tm_isdst:\n            return -time.altzone\n        else:\n            return -time.timezone\n    else:\n        return 0", "response": "Calculate the offset of a node in a node s tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a timezone string for the given date and local_tz.", "response": "def _generate_timezone(date, local_tz):\n    \"\"\"\n    input :\n    date : date type\n    local_tz : bool\n\n    offset generated from _calculate_offset\n    offset in seconds\n    offset = 0 -> +00:00\n    offset = 1800 -> +00:30\n    offset = -3600 -> -01:00\n    \"\"\"\n    offset = _calculate_offset(date, local_tz)\n\n    hour = abs(offset) // 3600\n    minute = abs(offset) % 3600 // 60\n\n    if offset < 0:\n        return '%c%02d:%02d' % (\"-\", hour, minute)\n    else:\n        return '%c%02d:%02d' % (\"+\", hour, minute)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef module_name_from_path(folder_name, verbose=False):\n    # strip off endings\n    folder_name = folder_name.split('.pyc')[0]\n    folder_name = folder_name.split('.py')[0]\n\n    folder_name = os.path.normpath(folder_name)\n\n    path = folder_name + '/'\n\n    package = get_python_package(path)\n    # path = folder_name\n    module = []\n\n    if verbose:\n        print(('folder_name', folder_name))\n\n    # os_sys_path = os.sys.path\n    #\n    # if os.path.normpath(path) in os_sys_path:\n    #     if verbose:\n    #         print('warning: path in sys.path!')\n    #     os_sys_path.remove(os.path.normpath(path))\n    #\n    #\n    # if verbose:\n    #     for elem in os_sys_path:\n    #\n    #         print('os.sys.path', elem)\n\n\n\n    while True:\n\n        path = os.path.dirname(path)\n\n        module.append(os.path.basename(path))\n        if os.path.basename(path) == package:\n            path = os.path.dirname(path)\n            break\n\n        # failed to identify the module\n        if os.path.dirname(path) == path:\n            path, module = None, None\n            break\n\n        if verbose:\n            print(('path', path, os.path.dirname(path)))\n\n        # if path == os.path.dirname(path):\n        #     if verbose:\n        #         print('break --  os.path.dirname(path)', os.path.dirname(path))\n        #     # path, module = None, None\n        #     break\n        #\n\n        if verbose:\n            print(('module', module))\n\n\n    # OLD START\n    # while path not in os_sys_path:\n    #     path = os.path.dirname(path)\n    #\n    #     if verbose:\n    #         print('path', path, os.path.dirname(path))\n    #\n    #     if path == os.path.dirname(path):\n    #         if verbose:\n    #             print('break --  os.path.dirname(path)', os.path.dirname(path))\n    #         # path, module = None, None\n    #         break\n    #     module.append(os.path.basename(path))\n    #\n    #     if verbose:\n    #         print('module', module)\n    # OLD END\n\n    if verbose:\n        print(('module', module))\n\n\n    # module = module[:-1]\n    # print('mod', module)\n    # from the list construct the path like b26_toolkit.pylabcontrol.scripts and load it\n    module.reverse()\n    module = '.'.join(module)\n\n    return module, path", "response": "Returns the module name from a path to a folder or file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all the packages in the module_name", "response": "def explore_package(module_name):\n    \"\"\"\n    returns all the packages in the module\n\n    Args:\n        module_name: name of module\n\n    Returns:\n\n    \"\"\"\n\n    packages = []\n    loader = pkgutil.get_loader(module_name)\n    for sub_module in pkgutil.walk_packages([os.path.dirname(loader.get_filename())],\n                                            prefix=module_name + '.'):\n        _, sub_module_name, _ = sub_module\n        packages.append(sub_module_name)\n\n    return packages"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a parser by defining which input files it will read from.", "response": "def generate_from_directory(cls, directory):\n        \"\"\"Create a parser by defining which input files it will read from.\n\n        Input:\n            directory - str, directory to read from\n            files - str, list of files from which to search.\n            \"\"\"\n        files = [os.path.join(directory, f) for f in os.listdir(directory)\n                 if os.path.isfile(os.path.join(directory, f))]\n        return cls(files)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets composition of output structure Returns: String - Composition based on output structure", "response": "def get_composition(self):\n        '''Get composition of output structure\n        \n        Returns:\n            String - Composition based on output structure\n        '''\n        strc = self.get_output_structure()\n        counts = Counter(strc.get_chemical_symbols())\n        return ''.join(k if counts[k]==1 else '%s%d'%(k,counts[k]) \\\n                for k in sorted(counts))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the density from the output structure", "response": "def get_density(self):\n        \"\"\"Compute the density from the output structure\"\"\"\n        strc = self.get_output_structure()\n        density = sum(strc.get_masses()) / strc.get_volume() * 1.660539040\n        return Property(scalars=[Scalar(value=density)], units=\"g/(cm^3)\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_converged(self):\n        '''Whether the calculation has converged\n        \n        Returns: Property where \"scalar\" is a boolean indicating\n        '''\n\n        # Check for cached result\n        if self._converged is None:\n            self._converged = self._is_converged()\n        return Property(scalars=[Scalar(value=self._converged)])", "response": "Whether the calculation has converged\n        \n        Returns a Property where scalar is a boolean indicating\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_number_of_atoms(self):\n        strc = self.get_output_structure()\n        if not strc:\n            return None\n        return Property(scalars=[Scalar(value=len(strc))], units=\"/unit cell\")", "response": "Get the number of atoms in the calculated structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_sub_modules(module_type):\n\n    assert module_type in ('scripts', 'instruments')\n\n    path_to_config = '/'.join(os.path.normpath(os.path.dirname(inspect.getfile(import_sub_modules))).split('\\\\')[0:-2]) + '/config.txt'\n    module_list = get_config_value('SCRIPT_MODULES', path_to_config).split(';')\n    module_list = [import_module(module_name + '.pylabcontrol.' + module_type) for module_name in module_list]\n\n    return module_list", "response": "Imports all the module_type from additional modules that contain module_type\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the value for name from path_to_file config file", "response": "def get_config_value(name, path_to_file='config.txt'):\n    \"\"\"\n    gets the value for \"name\" from \"path_to_file\" config file\n    Args:\n        name: name of varibale in config file\n        path_to_file: path to config file\n\n    Returns: path to dll if name exists in the file; otherwise, returns None\n\n    \"\"\"\n\n    # if the function is called from gui then the file has to be located with respect to the gui folder\n    if not os.path.isfile(path_to_file):\n        path_to_file = os.path.join('../instruments/', path_to_file)\n\n    path_to_file = os.path.abspath(path_to_file)\n\n    if not os.path.isfile(path_to_file):\n        print(('path_to_file', path_to_file))\n        #raise IOError('{:s}: config file is not valid'.format(path_to_file))\n        return None\n\n    f = open(path_to_file, 'r')\n    string_of_file_contents = f.read()\n\n    if name[-1] is not ':':\n        name += ':'\n\n    if name not in string_of_file_contents:\n        return None\n    else:\n        config_value = [line.split(name)[1] for line in string_of_file_contents.split('\\n')\n                        if len(line.split(name)) > 1][0].strip()\n        return config_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_b26_file(file_name):\n    # file_name = \"Z:\\Lab\\Cantilever\\Measurements\\\\tmp_\\\\a\"\n\n    assert os.path.exists(file_name)\n\n    with open(file_name, 'r') as infile:\n        data = yaml.safe_load(infile)\n    return data", "response": "Loads a. b26 file into a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving a B26 file.", "response": "def save_b26_file(filename, instruments=None, scripts=None, probes=None, overwrite=False, verbose=False):\n    \"\"\"\n    save instruments, scripts and probes as a json file\n    Args:\n        filename:\n        instruments:\n        scripts:\n        probes: dictionary of the form {instrument_name : probe_1_of_intrument, probe_2_of_intrument, ...}\n\n    Returns:\n\n    \"\"\"\n\n    # if overwrite is false load existing data and append to new instruments\n    if os.path.isfile(filename) and overwrite == False:\n        data_dict = load_b26_file(filename)\n    else:\n        data_dict = {}\n\n    if instruments is not None:\n        if 'instruments' in data_dict:\n            data_dict['instruments'].update(instruments)\n        else:\n            data_dict['instruments'] = instruments\n\n    if scripts is not None:\n        if 'scripts' in data_dict:\n            data_dict['scripts'].update(scripts)\n        else:\n            data_dict['scripts'] = scripts\n\n    if probes is not None:\n        probe_instruments = list(probes.keys())\n        if 'probes' in data_dict:\n            # all the instruments required for old and new probes\n            probe_instruments= set(probe_instruments + list(data_dict['probes'].keys()))\n        else:\n            data_dict.update({'probes':{}})\n\n        for instrument in probe_instruments:\n            if instrument in data_dict['probes'] and instrument in probes:\n                # update the data_dict\n                data_dict['probes'][instrument] = ','.join(set(data_dict['probes'][instrument].split(',') + probes[instrument].split(',')))\n            else:\n                data_dict['probes'].update(probes)\n\n    if verbose:\n        print(('writing ', filename))\n\n    if data_dict != {}:\n\n        # if platform == 'Windows':\n        #     # windows can't deal with long filenames so we have to use the prefix '\\\\\\\\?\\\\'\n        #     if len(filename.split('\\\\\\\\?\\\\')) == 1:\n        #         filename = '\\\\\\\\?\\\\'+ filename\n        # create folder if it doesn't exist\n        if verbose:\n            print(('filename', filename))\n            print(('exists', os.path.exists(os.path.dirname(filename))))\n\n        if os.path.exists(os.path.dirname(filename)) is False:\n            # print(('creating', os.path.dirname(filename)))\n            os.makedirs(os.path.dirname(filename))\n\n        with open(filename, 'w') as outfile:\n            tmp = json.dump(data_dict, outfile, indent=4)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with the class name and path_to_python_file for all the classes in the given folder.", "response": "def get_classes_in_folder(folder_name, class_type, verbose=False):\n    \"\"\"\n    load all the instruments or script objects that are located in folder_name and\n    return a dictionary with the script class name and path_to_python_file\n    Args:\n        folder_name (string): folder in which to search for class objects / or name of module\n        class_type (string or class): class type for which to look for\n\n    Returns:\n        a dictionary with the class name and path_to_python_file:\n        {\n        'class': class_of_instruments,\n        'filepath': path_to_python_file\n        }\n\n    \"\"\"\n\n    assert class_type == Instrument or class_type == Script or class_type.lower() in ['instrument', 'script']\n\n    if isinstance(class_type, str):\n        if class_type.lower() == 'instrument':\n            class_type = Instrument\n        elif class_type.lower() == 'script':\n            class_type = Script\n\n\n    # if the module name was passed instead of a filename, figure out the path to the module\n    if not os.path.isdir(folder_name):\n        try:\n            folder_name = os.path.dirname(inspect.getfile(import_module(folder_name)))\n        except ImportError:\n            raise ImportError('could not find module ' + folder_name)\n\n\n    subdirs = [os.path.join(folder_name, x) for x in os.listdir(folder_name) if\n               os.path.isdir(os.path.join(folder_name, x)) and not x.startswith('.')]\n\n    classes_dict = {}\n    # if there are subdirs in the folder recursively check all the subfolders for scripts\n    for subdir in subdirs:\n        classes_dict.update(get_classes_in_folder(subdir, class_type))\n\n\n    for python_file in [f for f in glob.glob(os.path.join(folder_name, \"*.py\"))if '__init__' not in f]:\n        module, path = module_name_from_path(python_file)\n\n        try:\n            module = import_module(module)\n\n            classes_dict.update({name: {'class': name, 'filepath': inspect.getfile(obj)} for name, obj in\n                               inspect.getmembers(module) if inspect.isclass(obj) and issubclass(obj, class_type)\n                             and not obj in (Instrument, Script, ScriptIterator)})\n        except ImportError as e:\n            if verbose:\n                print(('Could not import module', module))\n                print(e)\n\n    return classes_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting default probes for all instruments in the folder path.", "response": "def export_default_probes(path, module_name = '', raise_errors = False):\n    \"\"\"\n    NOT IMPLEMENTED YET\n    tries to instantiate all the instruments that are imported in /instruments/__init__.py\n    and the probes of each instrument that could be instantiated into a .b26 file in the folder path\n    Args:\n        path: target path for .b26 files\n    \"\"\"\n\n    raise NotImplementedError\n\n\n    import b26_toolkit.b26_toolkit.instruments as instruments\n    from pylabcontrol.core import Probe\n\n    for name, obj in inspect.getmembers(instruments):\n\n        if inspect.isclass(obj):\n\n            try:\n                instrument = obj()\n                print(('--- created ', obj.__name__, ' -- '))\n                for probe_name, probe_info in instrument._PROBES.items():\n                    probe = Probe(instrument, probe_name, info = probe_info)\n                    filename = os.path.join(path, '{:s}.b26'.format(instrument.name))\n                    probe.save(filename)\n            except:\n                print(('failed to create probe file for: {:s}'.format(obj.__name__)))\n                print(('failed to create probe file for: {:s}'.format(obj.__name__)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_default_scripts(target_folder, source_folder = None, raise_errors = False, verbose=False):\n\n    scripts_to_load = get_classes_in_folder(source_folder, Script)\n\n    if verbose:\n        print(('attempt to load {:d} scripts: '.format(len(scripts_to_load))))\n\n    loaded_scripts, failed, loaded_instruments = Script.load_and_append(scripts_to_load, raise_errors=raise_errors)\n\n    for name, value in loaded_scripts.items():\n        filename = os.path.join(target_folder, '{:s}.b26'.format(name))\n        value.save_b26(filename)\n\n    if verbose:\n        print('\\n================================================')\n        print('================================================')\n        print(('saved {:d} scripts, {:d} failed'.format(len(loaded_scripts), len(failed))))\n        if failed != {}:\n            for error_name, error in failed.items():\n                print(('failed to create script: ', error_name, error))", "response": "Loads all the scripts that are imported in source_folder and saves them into target_folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(target_folder, source_folders = None, class_type ='all', raise_errors = False):\n    if class_type not in ('all', 'scripts', 'instruments', 'probes'):\n        print('unknown type to export')\n        return\n\n    if not os.path.isdir(target_folder):\n        try:\n            os.mkdir(target_folder)\n        except:\n            print((target_folder, ' is invalid target folder'))\n            target_folder = None\n\n    if target_folder is not None:\n        if source_folders is None:\n            module_list = [os.path.dirname(os.path.dirname(inspect.getfile(inspect.currentframe())))]\n        elif isinstance(source_folders, str):\n            module_list = [source_folders]\n        elif isinstance(source_folders, list):\n            module_list = source_folders\n        else:\n            raise TypeError('unknown type for source_folders')\n\n        for path_to_module in module_list:\n            if class_type in ('all', 'scripts'):\n                export_default_scripts(target_folder, source_folder=path_to_module, raise_errors=raise_errors)\n            if class_type in ('all', 'instruments'):\n                export_default_instruments(target_folder, path_to_module,  raise_errors=raise_errors)\n            if class_type in ('all', 'probes'):\n                print('WARNING: probes currently not supported')", "response": "Exports the existing scripts and instruments into a folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recv_message(self, debug=False):\n        if debug:\n            packet = self.sock.recv(1024)  # reads how many bytes to read\n            hexdump(packet)\n\n        packet_length_data = self.sock.recv(4)  # reads how many bytes to read\n\n        if len(packet_length_data) < 4:\n            raise Exception(\"Nothing in the socket!\")\n        packet_length = struct.unpack(\"<I\", packet_length_data)[0]\n        packet = self.sock.recv(packet_length - 4)  # read the rest of bytes from socket\n\n        # check the CRC32\n        if not crc32(packet_length_data + packet[0:-4]) == struct.unpack('<I', packet[-4:])[0]:\n            raise Exception(\"CRC32 was not correct!\")\n        x = struct.unpack(\"<I\", packet[:4])\n        auth_key_id = packet[4:12]\n        if auth_key_id == b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00':\n            # No encryption - Plain text\n            (message_id, message_length) = struct.unpack(\"<QI\", packet[12:24])\n            data = packet[24:24+message_length]\n        elif auth_key_id == self.auth_key_id:\n            pass\n            message_key = packet[12:28]\n            encrypted_data = packet[28:-4]\n            aes_key, aes_iv = self.aes_calculate(message_key, direction=\"from server\")\n            decrypted_data = crypt.ige_decrypt(encrypted_data, aes_key, aes_iv)\n            assert decrypted_data[0:8] == self.server_salt\n            assert decrypted_data[8:16] == self.session_id\n            message_id = decrypted_data[16:24]\n            seq_no = struct.unpack(\"<I\", decrypted_data[24:28])[0]\n            message_data_length = struct.unpack(\"<I\", decrypted_data[28:32])[0]\n            data = decrypted_data[32:32+message_data_length]\n        else:\n            raise Exception(\"Got unknown auth_key id\")\n        return data", "response": "Reads a message from the server and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of CategoryList objects associated with this model instance.", "response": "def get_category_lists(self, init_kwargs=None, additional_parents_aliases=None):\n        \"\"\"Returns a list of CategoryList objects, associated with\n        this model instance.\n\n        :param dict|None init_kwargs:\n        :param list|None additional_parents_aliases:\n        :rtype: list|CategoryRequestHandler\n        :return:\n        \"\"\"\n\n        if self._category_editor is not None:  # Return editor lists instead of plain lists if it's enabled.\n            return self._category_editor.get_lists()\n\n        from .toolbox import get_category_lists\n        init_kwargs = init_kwargs or {}\n\n        catlist_kwargs = {}\n        if self._category_lists_init_kwargs is not None:\n            catlist_kwargs.update(self._category_lists_init_kwargs)\n        catlist_kwargs.update(init_kwargs)\n\n        lists = get_category_lists(catlist_kwargs, additional_parents_aliases, obj=self)\n\n        return lists"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenable editor functionality for categories of this object.", "response": "def enable_category_lists_editor(self, request, editor_init_kwargs=None, additional_parents_aliases=None,\n                                     lists_init_kwargs=None, handler_init_kwargs=None):\n        \"\"\"Enables editor functionality for categories of this object.\n\n        :param Request request: Django request object\n        :param dict editor_init_kwargs: Keyword args to initialize category lists editor with.\n            See CategoryList.enable_editor()\n        :param list additional_parents_aliases: Aliases of categories for editor to render\n            even if this object has no tie to them.\n        :param dict lists_init_kwargs: Keyword args to initialize CategoryList objects with\n        :param dict handler_init_kwargs: Keyword args to initialize CategoryRequestHandler object with\n        :return:\n        \"\"\"\n        from .toolbox import CategoryRequestHandler\n        additional_parents_aliases = additional_parents_aliases or []\n        lists_init_kwargs = lists_init_kwargs or {}\n        editor_init_kwargs = editor_init_kwargs or {}\n        handler_init_kwargs = handler_init_kwargs or {}\n        handler = CategoryRequestHandler(request, self, **handler_init_kwargs)\n        lists = self.get_category_lists(\n            init_kwargs=lists_init_kwargs, additional_parents_aliases=additional_parents_aliases)\n        handler.register_lists(lists, lists_init_kwargs=lists_init_kwargs, editor_init_kwargs=editor_init_kwargs)\n        self._category_editor = handler  # Set link to handler to mutate get_category_lists() behaviour.\n        return handler.listen()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding this model instance to a category.", "response": "def add_to_category(self, category, user):\n        \"\"\"Add this model instance to a category.\n\n        :param Category category: Category to add this object to\n        :param User user: User heir who adds\n        :return:\n        \"\"\"\n        init_kwargs = {\n            'category': category,\n            'creator': user,\n            'linked_object': self\n        }\n        tie = self.categories.model(**init_kwargs)  # That's a model of Tie.\n        tie.save()\n        return tie"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving this object from a given category.", "response": "def remove_from_category(self, category):\n        \"\"\"Removes this object from a given category.\n\n        :param Category category:\n        :return:\n        \"\"\"\n        ctype = ContentType.objects.get_for_model(self)\n        self.categories.model.objects.filter(category=category, content_type=ctype, object_id=self.id).delete()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ties_for_categories_qs(cls, categories, user=None, status=None):\n        if not isinstance(categories, list):\n            categories = [categories]\n\n        category_ids = []\n        for category in categories:\n            if isinstance(category, models.Model):\n                category_ids.append(category.id)\n            else:\n                category_ids.append(category)\n        filter_kwargs = {\n            'content_type': ContentType.objects.get_for_model(cls, for_concrete_model=False),\n            'category_id__in': category_ids\n        }\n        if user is not None:\n            filter_kwargs['creator'] = user\n        if status is not None:\n            filter_kwargs['status'] = status\n        ties = get_tie_model().objects.filter(**filter_kwargs)\n        return ties", "response": "Returns a QuerySet of Ties for the given categories."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_from_category_qs(cls, category):\n        ids = cls.get_ties_for_categories_qs(category).values_list('object_id').distinct()\n        filter_kwargs = {'id__in': [i[0] for i in ids]}\n        return cls.objects.filter(**filter_kwargs)", "response": "Returns a QuerySet of objects of this type associated with the given category."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--file\",\n                        required=True,\n                        help=\"input file\",\n                        type=str)\n\n    parser.add_argument(\"-l\", \"--locus\",\n                        required=True,\n                        help=\"Locus\",\n                        type=str)\n\n    parser.add_argument(\"-k\", \"--kir\",\n                        help=\"Option for running with KIR\",\n                        action='store_true')\n\n    parser.add_argument(\"-s\", \"--server\",\n                        help=\"Option for running with a server\",\n                        action='store_true')\n\n    parser.add_argument(\"-v\", \"--verbose\",\n                        help=\"Option for running in verbose\",\n                        action='store_true')\n\n    args = parser.parse_args()\n    fastafile = args.file\n    locus = args.locus\n\n    verbose = False\n    if args.verbose:\n        verbose = True\n\n    verbose = False\n    if args.verbose:\n        verbose = True\n\n    kir = False\n    if args.kir:\n        kir = True\n\n    serv = False\n    if args.server:\n        serv = True\n\n    if verbose:\n        logging.basicConfig(format='%(asctime)s - %(name)-35s - %(levelname)-5s - %(message)s',\n                            datefmt='%m/%d/%Y %I:%M:%S %p',\n                            level=logging.INFO)\n\n    server = None\n    if serv:\n        server = BioSeqDatabase.open_database(driver=\"pymysql\", user=\"root\",\n                                              passwd=\"\", host=\"localhost\",\n                                              db=\"bioseqdb\")\n\n    seqann = BioSeqAnn(verbose=True, kir=kir)\n    for seq in SeqIO.parse(fastafile, \"fasta\"):\n        ann = seqann.annotate(seq, locus=locus)\n        print('{:*^20} {:^20} {:*^20}'.format(\"\", str(seq.description), \"\"))\n        l = 0\n        for f in ann.annotation:\n            if isinstance(ann.annotation[f], DBSeq):\n                print(f, ann.method, str(ann.annotation[f]), sep=\"\\t\")\n                l += len(ann.annotation[f])\n            else:\n                print(f, ann.method, str(ann.annotation[f].seq), sep=\"\\t\")\n                l += len(ann.annotation[f].seq)\n        print(\"\")\n\n    if serv:\n        server.close()", "response": "This is the main function for the\n    command line interface. It is run if file is directly executed but not if imported as\n    module and still able to execute the\n    function for testing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextending with a full subtree < = current minimum subtree.", "response": "def _push_subtree(self, leaves: List[bytes]):\n        \"\"\"Extend with a full subtree <= the current minimum subtree.\n\n        The leaves must form a full subtree, i.e. of size 2^k for some k. If\n        there is a minimum subtree (i.e. __mintree_height > 0), then the input\n        subtree must be smaller or of equal size to the minimum subtree.\n\n        If the subtree is smaller (or no such minimum exists, in an empty tree),\n        we can simply append its hash to self.hashes, since this maintains the\n        invariant property of being sorted in descending size order.\n\n        If the subtree is of equal size, we are in a similar situation to an\n        addition carry. We handle it by combining the two subtrees into a larger\n        subtree (of size 2^(k+1)), then recursively trying to add this new\n        subtree back into the tree.\n\n        Any collection of leaves larger than the minimum subtree must undergo\n        additional partition to conform with the structure of a merkle tree,\n        which is a more complex operation, performed by extend().\n        \"\"\"\n        size = len(leaves)\n        if count_bits_set(size) != 1:\n            raise ValueError(\"invalid subtree with size != 2^k: %s\" % size)\n        # in general we want the highest bit, but here it's also the lowest bit\n        # so just reuse that code instead of writing a new highest_bit_set()\n        subtree_h, mintree_h = lowest_bit_set(size), self.__mintree_height\n        if mintree_h > 0 and subtree_h > mintree_h:\n            raise ValueError(\"subtree %s > current smallest subtree %s\" % (\n                subtree_h, mintree_h))\n        root_hash, hashes = self.__hasher._hash_full(leaves, 0, size)\n        assert hashes == (root_hash,)\n\n        if self.hashStore:\n            for h in hashes:\n                self.hashStore.writeLeaf(h)\n\n        new_node_hashes = self.__push_subtree_hash(subtree_h, root_hash)\n\n        nodes = [(self.tree_size, height, h) for h, height in new_node_hashes]\n        if self.hashStore:\n            for node in nodes:\n                self.hashStore.writeNode(node)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef child(self, name):\n        trace = self.__class__(\n            name, trace_id=self.trace_id, parent_span_id=self.span_id)\n        trace.set_endpoint(self._endpoint)\n\n        return trace", "response": "Create a new instance of this class derived from the current instance and set the endpoint of the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving a local object to a base object", "response": "def resolve(obj, pointer, registry=None):\n    \"\"\"resolve a local object\n\n    :param obj: the local object.\n    :param pointer: the pointer\n    :type pointer: DocumentPointer, str\n    :param registry: the registry.\n                    It mays be omited if inner json references\n                    document don't refer to other documents.\n    :type registry: Provider, dict\n\n    .. warning::\n\n        Once pointer is extracted, it won't follow sub mapping /element!\n        For example, the value of::\n\n            value = resolve({\n                'foo': {'$ref': '#/bar'},\n                'bar': [{'$ref': '#/baz'}],\n                'baz': 'quux',\n            }, '#/foo')\n\n        is::\n\n            assert value == [{'$ref': '#/baz'}]\n\n        and not::\n\n            assert value == ['quux']\n\n    \"\"\"\n\n    registry = LocalRegistry(obj, registry or {})\n    local = DocumentPointer(pointer)\n\n    if local.document:\n        registry[local.document] = obj\n    local.document = '<local>'\n    return registry.resolve(local)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_migrations():\n    with engine.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=Model.metadata)\n\n        with context.begin_transaction():\n            context.run_migrations()", "response": "Run migrations in online mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef addUrlScheme(self, url):\n        '''\n        Add a url scheme to this endpoint. It takes a url string and create\n        the OEmbedUrlScheme object internally.\n\n        Args:\n            url: The url string that represents a url scheme to add.\n        '''\n\n        #@TODO: validate invalid url format according to http://oembed.com/\n        if not isinstance(url, str):\n            raise TypeError('url must be a string value')\n\n        if not url in self._urlSchemes:\n            self._urlSchemes[url] = OEmbedUrlScheme(url)", "response": "Adds a url scheme to this endpoint. It takes a url string and create an OEmbedUrlScheme object internally."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to find if url matches against any of the schemes within this ArcGIS endpoint.", "response": "def match(self, url):\n        '''\n        Try to find if url matches against any of the schemes within this\n        endpoint.\n\n        Args:\n            url: The url to match against each scheme\n\n        Returns:\n            True if a matching scheme was found for the url, False otherwise\n        '''\n        try:\n            urlSchemes = self._urlSchemes.itervalues() # Python 2\n        except AttributeError:\n            urlSchemes = self._urlSchemes.values() # Python 3\n\n        for urlScheme in urlSchemes:\n            if urlScheme.match(url):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request(self, url, **opt):\n        '''\n        Format the input url and optional parameters, and provides the final url\n        where to get the given resource.\n\n        Args:\n            url: The url of an OEmbed resource.\n            **opt: Parameters passed to the url.\n\n        Returns:\n            The complete url of the endpoint and resource.\n        '''\n        params = opt\n        params['url'] = url\n        urlApi = self._urlApi\n\n        if 'format' in params and self._implicitFormat:\n            urlApi = self._urlApi.replace('{format}', params['format'])\n            del params['format']\n\n        if '?' in urlApi:\n            return \"%s&%s\" % (urlApi, urllib.urlencode(params))\n        else:\n            return \"%s?%s\" % (urlApi, urllib.urlencode(params))", "response": "Format the input url and optional parameters and provides the final url of the endpoint and resource."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, url, **opt):\n        '''\n        Convert the resource url to a complete url and then fetch the\n        data from it.\n\n        Args:\n            url: The url of an OEmbed resource.\n            **opt: Parameters passed to the url.\n\n        Returns:\n            OEmbedResponse object according to data fetched\n        '''\n        return self.fetch(self.request(url, **opt))", "response": "Fetch the data from the url and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch(self, url):\n        '''\n        Fetch url and create a response object according to the mime-type.\n\n        Args:\n            url: The url to fetch data from\n\n        Returns:\n            OEmbedResponse object according to data fetched\n        '''\n        opener = self._urllib.build_opener()\n        opener.addheaders = self._requestHeaders.items()\n        response = opener.open(url)\n        headers = response.info()\n        raw = response.read()\n        raw = raw.decode('utf8')\n\n        if not 'Content-Type' in headers:\n            raise OEmbedError('Missing mime-type in response')\n\n        if headers['Content-Type'].find('application/xml') != -1 or \\\n           headers['Content-Type'].find('text/xml') != -1:\n            response = OEmbedResponse.newFromXML(raw)\n        elif headers['Content-Type'].find('application/json') != -1 or \\\n             headers['Content-Type'].find('text/javascript') != -1 or \\\n             headers['Content-Type'].find('text/json') != -1:\n            response = OEmbedResponse.newFromJSON(raw)\n        else:\n            raise OEmbedError('Invalid mime-type in response - %s' % headers['Content-Type'])\n\n        return response", "response": "Fetch url and create a response object according to the mime - type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting an OEmbedResponse object from one of the providers configured in this object.", "response": "def embed(self, url, format='json', **opt):\n        '''\n        Get an OEmbedResponse from one of the providers configured in this\n        consumer according to the resource url.\n\n        Args:\n            url: The url of the resource to get.\n            format: Desired response format.\n            **opt: Optional parameters to pass in the url to the provider.\n\n        Returns:\n            OEmbedResponse object.\n        '''\n        if format not in ['json', 'xml']:\n            raise OEmbedInvalidRequest('Format must be json or xml')\n        opt['format'] = format\n        return self._request(url, **opt)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning function of TauDEM take pitremove as an example. Compare the max min and average of rawDEM and filled DEM", "response": "def pitremove_example():\n    \"\"\"run function of TauDEM, take pitremove as an example.\n    Compare the max, min, and average of rawdem and filled DEM.\n    The result will be::\n\n        RawDEM: Max: 284.07, Min: 139.11, Mean: 203.92\n        FilledDEM: Max: 284.07, Min: 139.11, Mean: 203.93\n\n    \"\"\"\n    dem = '../tests/data/Jamaica_dem.tif'\n    wp = '../tests/data/tmp_results'\n    fel = 'dem_pitremoved.tif'\n    taudem_bin = None\n    mpi_bin = None\n    num_proc = 2\n\n    TauDEM.pitremove(num_proc, dem, fel, wp, mpiexedir=mpi_bin, exedir=taudem_bin)\n\n    rawdem = RasterUtilClass.read_raster(dem)\n    feldem = RasterUtilClass.read_raster(wp + os.sep + fel)\n\n    print('RawDEM: Max: %.2f, Min: %.2f, Mean: %.2f' % (rawdem.get_max(), rawdem.get_min(),\n                                                        rawdem.get_average()))\n    print('FilledDEM: Max: %.2f, Min: %.2f, Mean: %.2f' % (feldem.get_max(), feldem.get_min(),\n                                                           feldem.get_average()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the iterator type based on the script settings and subscripts", "response": "def get_iterator_type(script_settings, subscripts={}):\n        \"\"\"\n        figures out the iterator type based on the script settings and (optionally) subscripts\n        Args:\n            script_settings: iterator_type\n            subscripts: subscripts\n        Returns:\n\n        \"\"\"\n\n        if 'iterator_type' in script_settings:\n            # figure out the iterator type\n            if script_settings['iterator_type'] == 'Loop':\n                iterator_type = 'loop'\n            elif script_settings['iterator_type'] == 'Parameter Sweep':\n                iterator_type = 'sweep'\n            else:\n                raise TypeError('unknown iterator type')\n        else:\n            # asign the correct iterator script type\n            if 'sweep_param' in script_settings:\n                iterator_type = 'sweep'\n            elif 'num_loops' in script_settings:\n                iterator_type = 'loop'\n            else:\n                raise TypeError('unknown iterator type')\n\n        return iterator_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _function(self):\n        '''\n        Runs either a loop or a parameter sweep over the subscripts in the order defined by the parameter_list 'script_order'\n        '''\n        def get_sweep_parameters():\n            \"\"\"\n            Returns: the paramter values over which to sweep\n            \"\"\"\n            #in both cases, param values have tolist to make sure that they are python types (ex float) rather than numpy\n            #types (ex np.float64), the latter of which can cause typing issues\n            sweep_range = self.settings['sweep_range']\n            if self.settings['stepping_mode'] == 'N':\n                param_values = np.linspace(sweep_range['min_value'], sweep_range['max_value'],\n                                           int(sweep_range['N/value_step']), endpoint=True).tolist()\n            elif self.settings['stepping_mode'] == 'value_step':\n                param_values = np.linspace(sweep_range['min_value'], sweep_range['max_value'],\n                                           (sweep_range['max_value'] - sweep_range['min_value']) / sweep_range[\n                                               'N/value_step'] + 1, endpoint=True).tolist()\n            return param_values\n\n        script_names = list(self.settings['script_order'].keys())\n        script_indices = [self.settings['script_order'][name] for name in script_names]\n        _, sorted_script_names = list(zip(*sorted(zip(script_indices, script_names))))\n\n        if self.iterator_type == 'sweep':\n\n\n            def get_script_and_settings_from_str(sweep_param):\n                \"\"\"\n                Args:\n                    sweep_param: astring with the path to the sweep parameter\n                        e.g. script.param.subparam or script->subscript.parameter\n\n                Returns:\n                    script_list: a list with the scripts, e.g. [script] or [script, subscript]\n                    parameter_list: a list with the paraemeters, e.g. [param, subparam] or [parameter] for the cases above\n                \"\"\"\n                split_trace = sweep_param.split('.')\n                script_list = split_trace[0].split('->')\n                parameter_list = split_trace[1:]\n\n                return script_list, parameter_list\n\n            param_values = get_sweep_parameters()\n\n            print('JG parametes before', param_values)\n            if self.settings['sweep_range']['randomize'] == True:\n                random.shuffle(param_values)\n            print('JG parametes after', param_values)\n\n            for i, value in enumerate(param_values):\n                self.iterator_progress = float(i) / len(param_values)\n\n                script_list, parameter_list = get_script_and_settings_from_str(self.settings['sweep_param'])\n                script = self\n                while len(script_list)>0:\n                    script = script.scripts[script_list[0]]\n                    script_list = script_list[1:]\n\n                curr_type = type(reduce(lambda x,y: x[y], parameter_list, script.settings)) #traverse nested dict to get type of variable\n\n                update_dict = reduce(lambda y, x: {x: y}, reversed(parameter_list), curr_type(value)) #creates nested dictionary from list\n\n                script.settings.update(update_dict)\n                parameter_name = parameter_list[-1]\n                if np.abs(value) < 1000:\n                    self.log('setting parameter {:s} to {:.3g}'.format(self.settings['sweep_param'], value))\n                else:\n                    self.log('setting parameter {:s} to {:0.2e}'.format(self.settings['sweep_param'], value))\n\n                for script_name in sorted_script_names:\n                    if self._abort:\n                        break\n                    j = i if self.settings['run_all_first'] else (i+1)\n\n                    curr_script_exec_freq = self.settings['script_execution_freq'][script_name]\n                    if curr_script_exec_freq != 0 and (j % curr_script_exec_freq == 0):\n                        # i+1 so first execution is mth loop, not first\n                        self.log('starting {:s}'.format(script_name))\n                        tag = self.scripts[script_name].settings['tag']\n                        self.scripts[script_name].settings['tag'] = '{:s}_{:s}_{:0.3e}'.format(tag, parameter_name, value)\n                        self.scripts[script_name].run()\n                        self.scripts[script_name].settings['tag'] = tag\n\n        elif self.iterator_type == 'loop':\n\n            num_loops = self.settings['num_loops']\n            if num_loops == 0:\n                self.log('Loop set to run 0 times')\n                return\n\n            self.data = {}\n            for i in range(num_loops):\n                self.iterator_progress = float(i) / num_loops\n\n                for script_name in sorted_script_names:\n                    if self._abort:\n                        break\n                    j = i if self.settings['run_all_first'] else (i+1)\n\n                    curr_script_execution_freq = self.settings['script_execution_freq'][script_name]\n\n                    if curr_script_execution_freq != 0 and (j % curr_script_execution_freq == 0):\n                        # i+1 so first execution is mth loop, not first\n                        self.log('starting {:s} \\t iteration {:d} of {:d}'.format(script_name, i + 1, num_loops))\n                        tag = self.scripts[script_name].settings['tag']\n                        tmp = tag + '_{' + ':0{:d}'.format(len(str(num_loops))) + '}'\n                        self.scripts[script_name].settings['tag'] = tmp.format(i)\n                        self.scripts[script_name].run()\n                        self.scripts[script_name].settings['tag'] = tag\n\n                # from the last script we take the average of the data as the data of the iterator script\n                if isinstance(self.scripts[script_name].data, dict):\n                    data = self.scripts[script_name].data\n                elif isinstance(self.scripts[script_name].data, deque):\n                    data = self.scripts[script_name].data[-1]\n                if i == 0:\n                    self.data.update(data)\n                else:\n                    if self._abort:\n                        break\n\n                    for key in list(data.keys()):\n\n                        # can't add None values\n                        if not data[key] is None:\n                            # if subscript data have differnet length, e.g. fitparameters can be differet, depending on if there is one or two peaks\n                            if len(self.data[key]) != len(data[key]):\n                                print(('warning subscript data {:s} have different lengths'.format(key)))\n                                continue\n\n                            if isinstance(self.data[key], list):\n                                self.data[key] += np.array(data[key])\n                            elif isinstance(self.data[key], dict):\n                                self.data[key] = {x: self.data[key].get(x, 0) + data[key].get(x, 0) for x in list(self.data[key].keys())}\n                            else:\n                                self.data[key] += data[key]\n\n            if not self._abort and num_loops > 0:\n                # normalize data because we just kept adding the values\n                for key in list(data.keys()):\n                    if isinstance(self.data[key], list):\n                        self.data[key] = np.array(self.data[key]) / num_loops\n                    elif isinstance(self.data[key], dict):\n                        self.data[key] = {k:v/num_loops for k, v in self.data[key].items()}\n                    elif self.data[key] is None:\n                        self.log('None type in data! check code')\n                        pass\n                    elif isinstance(self.data[key], int):\n                        self.data[key] = float(self.data[key]) / num_loops # if int we can not devide. Thus we convert explicitely to float\n                    else:\n                        self.data[key] = self.data[key] / num_loops\n\n        else:\n            raise TypeError('wrong iterator type')", "response": "This is the main function that runs the sweep over the subscripts of the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _receive_signal(self, progress_subscript):\n\n        self.progress = self._estimate_progress()\n        self.updateProgress.emit(int(self.progress))", "response": "This function is called by the _process_signal_subscript method to update the internal progress of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns itself as a dictionary", "response": "def to_dict(self):\n        \"\"\"\n        Returns: itself as a dictionary\n        \"\"\"\n        dictator = Script.to_dict(self)\n        # the dynamically created ScriptIterator classes have a generic name\n        # replace this with ScriptIterator to indicate that this class is of type ScriptIterator\n        dictator[self.name]['class'] = 'ScriptIterator'\n\n        return dictator"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_dynamic_script_class(script_information, script_iterators={},verbose=False):\n        '''\n        creates all the dynamic classes in the script and the class of the script itself\n        and updates the script info with these new classes\n\n        Args:\n            script_information: A dictionary describing the ScriptIterator, or an existing object\n            script_iterators: dictionary with the scriptiterators (optional)\n\n        Returns:\n            script_information:  The updated dictionary describing the newly created ScriptIterator class\n            script_iterators: updated dictionary with the scriptiterators\n        Poststate: Dynamically created classes inheriting from ScriptIterator are added to pylabcontrol.scripts\n\n        '''\n\n        def set_up_dynamic_script(script_information, script_iterators, verbose=verbose):\n            '''\n\n            Args:\n                script_information: information about the script as required by Script.get_script_information()\n\n            Returns:\n                script_default_settings: the default settings of the dynamically created script as a list of parameters\n                sub_scripts\n\n                script_iterators: dictionary of the script_iterator classes of the form {'package_name': <script_iterator_classe>}\n                package: name of the package of the script_iterator\n\n            '''\n\n            if verbose:\n                print(('script_information', script_information))\n            sub_scripts = {}  # dictonary of script classes that are to be subscripts of the dynamic class. Should be in the dictionary form {'class_name': <class_object>} (btw. class_object is not the instance)\n            script_order = []  # A list of parameters giving the order that the scripts in the ScriptIterator should be executed. Must be in the form {'script_name': int}. Scripts are executed from lowest number to highest\n            script_execution_freq = [] # A list of parameters giving the frequency with which each script should be executed\n            _, script_class_name, script_settings, _, script_sub_scripts, _, package = Script.get_script_information(script_information)\n\n            if package not in script_iterators:\n                script_iterators.update(ScriptIterator.get_script_iterator(package))\n\n            assert package in script_iterators\n\n            iterator_type = getattr(script_iterators[package], 'get_iterator_type')(script_settings, script_sub_scripts)\n            if verbose:\n                print(('iterator_type  JG', iterator_type))\n\n            if isinstance(script_information, dict):\n\n                for sub_script_name, sub_script_class in script_sub_scripts.items():\n                    if isinstance(sub_script_class, Script):\n                        # script already exists\n\n                        # To implement: add a function to scripts that outputs a script_information dict corresponding\n                        # to the current settings. This can then be fed into Script.get_script_information and things\n                        # can proceed as below. We may also need to add an additional tracker to the dialogue window\n                        # to differentiate between the case of wanting a new script from scratch when there is an\n                        # identically named one already loaded, vs using that loaded script\n\n                        raise NotImplementedError\n                    elif script_sub_scripts[sub_script_name]['class'] == 'ScriptIterator':\n                        # raise NotImplementedError # has to be dynamic maybe???\n                        script_information_subclass,  script_iterators = ScriptIterator.create_dynamic_script_class(script_sub_scripts[sub_script_name], script_iterators)\n                        subscript_class_name = script_information_subclass['class']\n                        #previously in python 2 had used: import pylabcontrol.core.script_iterator\n                        #however, this shouldn't have worked, as this was already imported as part of pylabcontrol, so nothing\n                        #happens and the old version without the dynamic script is used. Here, we force import the new\n                        #version of the module which now contains the script_iterator and everything works as expected\n                        script_iterator_module = __import__('pylabcontrol.core.script_iterator')\n                        sub_scripts.update({sub_script_name: getattr(script_iterator_module, subscript_class_name)})\n                    else:\n                        if verbose:\n                            print(('script_sub_scripts[sub_script_name]', sub_script_class))\n\n                        module = Script.get_script_module(sub_script_class, verbose=verbose)\n\n                        if verbose:\n                            print(('module', module))\n                        new_subscript = getattr(module, script_sub_scripts[sub_script_name]['class'])\n                        sub_scripts.update({sub_script_name: new_subscript})\n\n                # for some iterators have default scripts, e.g. point iteration has select points\n                default_sub_scripts, default_script_settings = getattr(script_iterators[package], 'get_iterator_default_script')(iterator_type)\n\n\n                sub_scripts.update(default_sub_scripts)\n\n                for k, v in default_script_settings.items():\n                    if k in script_settings:\n                        script_settings[k].update(v)\n\n\n\n            elif isinstance(script_information, Script):\n\n                print('old code - DOUBLE CHECK')\n                raise NotImplementedError\n                # if the script already exists, just update the script order parameter\n                sub_scripts.update({script_class_name: script_information})\n            else:\n                raise TypeError('create_dynamic_script_class: unknown type of script_information')\n\n\n\n            script_order, script_execution_freq = getattr(script_iterators[package], 'get_script_order')(script_settings['script_order'])\n            script_default_settings = getattr(script_iterators[package], 'get_default_settings')(sub_scripts, script_order, script_execution_freq, iterator_type)\n            return script_default_settings, sub_scripts, script_iterators, package\n\n        def create_script_iterator_class(sub_scripts, script_settings, script_iterator_base_class, verbose=verbose):\n            \"\"\"\n            A 'factory' to create a ScriptIterator class at runtime with the given inputs.\n\n            Args:\n                sub_scripts: dictonary of script classes that are to be subscripts of the dynamic class. Should be in the dictionary\n                         form {'class_name': <class_object>} (btw. class_object is not the instance)\n                script_default_settings: the default settings of the dynamically created object. Should be a list of Parameter objects.\n\n            Returns: A newly created class inheriting from ScriptIterator, with the given subscripts and default settings\n\n            \"\"\"\n\n\n            # dynamically import the module, i.e. the namespace for the scriptiterator\n            script_iterator_module = __import__(script_iterator_base_class.__module__)\n\n            if verbose:\n                print('\\n\\n======== create_script_iterator_class ========\\n')\n                print(('sub_scripts', sub_scripts))\n                print(('script_settings', script_settings))\n                print(('script_iterator_base_class', script_iterator_base_class))\n\n                print((script_iterator_base_class.__module__.split('.')[0]))\n\n\n            class_name = script_iterator_base_class.__module__.split('.')[0] + '.dynamic_script_iterator' + str(script_iterator_base_class._number_of_classes)\n\n            if verbose:\n                print(('class_name', class_name))\n\n            # If three parameters are passed to type(), it returns a new type object.\n            # Three parameters to the type() function are:\n            #\n            #     name - class name which becomes __name__ attribute\n            #     bases - a tuple that itemizes the base class, becomes __bases__ attribute\n            #     dict - a dictionary which is the namespace containing definitions for class body; becomes __dict__ attribute\n            dynamic_class = type(class_name, (script_iterator_base_class,), {'_SCRIPTS': sub_scripts, '_DEFAULT_SETTINGS': script_settings, '_INSTRUMENTS': {}})\n\n            if verbose:\n                print(('dynamic_class', dynamic_class))\n                print(('__bases__', dynamic_class.__bases__))\n\n                print(('dynamic_class.__name__', dynamic_class.__name__))\n                print(('dynamic_class.__bases__', dynamic_class.__bases__))\n                print(('dynamic_class.__dict__', dynamic_class.__dict__))\n\n            # Now we place the dynamic script into the scope of pylabcontrol.\n            setattr(script_iterator_module, class_name, dynamic_class)\n\n            if verbose:\n                print(('dynamic_class', dynamic_class))\n                print(('__bases__', dynamic_class.__bases__))\n\n                print(('dynamic_class.__name__', dynamic_class.__name__))\n                print(('dynamic_class.__bases__', dynamic_class.__bases__))\n                print(('dynamic_class.__dict__', dynamic_class.__dict__))\n\n            script_iterator_base_class._class_list.append(dynamic_class)\n            script_iterator_base_class._number_of_classes += 1\n\n\n            return class_name, dynamic_class\n\n            # todo: prevent multiple importation of the same script with different names\n            # for someclass in cls._class_list:\n            #     if (vars(ss)['_SCRIPTS'] == vars(someclass)['_SCRIPTS']):\n            #         print('CLASSNAME', vars(someclass)['_CLASS'])\n            #         return vars(someclass)['_CLASS']\n\n         # get default setting, load subscripts, load the script_iterators and identify the package\n        script_default_settings, sub_scripts, script_iterators, package = set_up_dynamic_script(script_information, script_iterators, verbose=verbose)\n\n        # now actually create the classs\n        class_name, dynamic_class = create_script_iterator_class(sub_scripts, script_default_settings, script_iterators[package], verbose = verbose)\n\n        # update the generic name (e.g. ScriptIterator) to a unique name  (e.g. ScriptIterator_01)\n        script_information['class'] = class_name\n\n        if 'iterator_type' in script_information['settings']:\n\n            if verbose:\n                print('WONDER IF WE EVER HAVE THIS CASE: iterator_type in script_information[setting]')\n            script_settings = {}\n            for elem in script_default_settings:\n                script_settings.update(elem)\n            script_information['settings'] = script_settings\n\n            if verbose:\n                print('\\n\\n======== create_dynamic_script_class ========\\n')\n\n                print(('dynamic_class', dynamic_class))\n                print(('sub_scripts', sub_scripts))\n                print(('script_settings', script_settings))\n\n        if verbose:\n            print('\\n======== end create_dynamic_script_class ========\\n')\n\n        return script_information, script_iterators", "response": "Create the dynamic class of the class containing the scripts in the scriptIterator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the script iterators of the given package as a dictionary.", "response": "def get_script_iterator(package_name, verbose = False):\n        \"\"\"\n        Args:\n            package_name: name of package\n\n        Returns: the script_iterators of the package as a dictionary\n\n        \"\"\"\n\n        packs = hf.explore_package(package_name + '.core')\n        print(packs)\n        script_iterator = {}\n\n        for p in packs:\n            for name, c in inspect.getmembers(importlib.import_module(p), inspect.isclass):\n                if verbose:\n                    print(p, name, c)\n\n                if issubclass(c, ScriptIterator):\n                    # update dictionary with 'Package name , e.g. pylabcontrol or b26_toolkit': <ScriptIterator_class>\n                    script_iterator.update({c.__module__.split('.')[0]: c})\n\n        return script_iterator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(portname, baudrate):\n    global SERPORT\n    try:\n        SERPORT = serial.Serial(portname, baudrate, timeout = 0.1)\n\n    except:\n        raise HerkulexError(\"could not open the serial port\")", "response": "Connect to the Herkulex bus and return the object that represents the Herkulex object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the checksum 1 required for the herkulex data packet", "response": "def checksum1(data, stringlength):\n    \"\"\" Calculate Checksum 1\n\n    Calculate the ckecksum 1 required for the herkulex data packet\n\n    Args:\n        data (list): the data of which checksum is to be calculated\n        stringlength (int): the length of the data\n\n    Returns:\n        int:  The calculated checksum 1\n    \"\"\"\n    value_buffer = 0\n    for count in range(0, stringlength):\n        value_buffer = value_buffer ^ data[count]\n    return value_buffer&0xFE"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending data to the herkulex Paketize & write the packet to the serial port", "response": "def send_data(data):\n    \"\"\" Send data to herkulex\n\n    Paketize & write the packet to serial port\n\n    Args:\n        data (list): the data to be sent\n\n    Raises:\n        SerialException: Error occured while opening serial port\n    \"\"\"\n    datalength = len(data)\n    csm1 = checksum1(data, datalength)\n    csm2 = checksum2(csm1)\n    data.insert(0, 0xFF)\n    data.insert(1, 0xFF)\n    data.insert(5, csm1)\n    data.insert(6, csm2)\n    stringtosend = \"\"\n    for i in range(len(data)):\n        byteformat = '%02X' % data[i]\n        stringtosend = stringtosend + \"\\\\x\" + byteformat\n\n    try:\n\n        SERPORT.write(stringtosend.decode('string-escape'))\n        #print stringtosend\n\n    except:\n        raise HerkulexError(\"could not communicate with motors\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clear_errors():\n    data = []\n    data.append(0x0B)\n    data.append(BROADCAST_ID)\n    data.append(RAM_WRITE_REQ)\n    data.append(STATUS_ERROR_RAM)\n    data.append(BYTE2)\n    data.append(0x00)\n    data.append(0x00)\n    send_data(data)", "response": "Clears the errors register of all Herkulex servos."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scale(input_value, input_min, input_max, out_min, out_max):\n    # Figure out how 'wide' each range is\n    input_span = input_max - input_min\n    output_span = out_max - out_min\n    # Convert the left range into a 0-1 range (float)\n    valuescaled = float(input_value - input_min) / float(input_span)\n    # Convert the 0-1 range into a value in the right range.\n    return out_min + (valuescaled * output_span)", "response": "scale a value from one range to another range"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscanning for the herkulex servos connected This function will scan for all the herkulex servos connected to the bus. Args: none Returns: list: a list of tuples of the form [(id, model)]", "response": "def scan_servos():\n\n    \"\"\"Scan for the herkulex servos connected\n\n\tThis function will scan for all the herkulex servos connected\n\tto the bus.\n\n\tArgs:\n\t    none\n\tReturns:\n\t    list: a list of tuples of the form [(id, model)]\n\t\"\"\"\n    servos = []\n    for servo_id in range(0x00, 0xFE):\n        model = get_model(servo_id)\n        if model:\n            servos += [(servo_id, model)]\n    return servos"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the servo model This function gets the model of the herkules servo, provided its id Args: servoid(int): the id of the servo Returns: int: an integer corresponding to the model number 0x06 for DRS-602 0x04 for DRS-402 0x02 for DRS-202", "response": "def get_model(servoid):\n    \"\"\" Get the servo model\n\n    This function gets the model of the herkules servo, provided its id\n\n    Args:\n        servoid(int): the id of the servo\n\n    Returns:\n        int:  an integer corresponding to the model number\n              0x06 for DRS-602\n              0x04 for DRS-402\n              0x02 for DRS-202\n    \"\"\"\n    data = []\n    data.append(0x09)\n    data.append(servoid)\n    data.append(EEP_READ_REQ)\n    data.append(MODEL_NO1_EEP)\n    data.append(BYTE1)\n    send_data(data)\n    rxdata = []\n    try:\n        rxdata = SERPORT.read(12)\n        return ord(rxdata[9])&0xFF\n    except:\n        raise HerkulexError(\"could not communicate with motors\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_servo_status(self):\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(STATUS_ERROR_RAM)\n        data.append(BYTE1)\n        send_data(data)\n\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(12)\n            return ord(rxdata[9])&0xFF\n        except:\n            raise HerkulexError(\"could not communicate with motors\")", "response": "This function returns the error status of the servo"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_servo_status_detail(self):\n\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(STATUS_DETAIL_RAM)\n        data.append(BYTE1)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(12)\n            return ord(rxdata[9])&0xFF\n        except HerkulexError:\n            raise HerkulexError(\"could not communicate with motors\")", "response": "This function returns the ultimate detailed error status of the servo"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef  set_led(self, colorcode):\n        data = []\n        data.append(0x0A)\n        data.append(self.servoid)\n        data.append(RAM_WRITE_REQ)\n        data.append(LED_CONTROL_RAM)\n        data.append(0x01)\n        data.append(colorcode)\n        send_data(data)", "response": "Set the LED Color of Herkulex\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef brake_on(self):\n        data = []\n        data.append(0x0A)\n        data.append(self.servoid)\n        data.append(RAM_WRITE_REQ)\n        data.append(TORQUE_CONTROL_RAM)\n        data.append(0x01)\n        data.append(0x40)\n        send_data(data)", "response": "Set the Brakes of Herkulex\n        in braked mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the torques of Herkulex to zero.", "response": "def torque_off(self):\n        \"\"\" Set the torques of Herkulex to zero\n\n        In this mode, position control and velocity control\n        will not work, enable torque before that. Also the\n        servo shaft is freely movable\n\n        Args:\n            none\n        \"\"\"\n        data = []\n        data.append(0x0A)\n        data.append(self.servoid)\n        data.append(RAM_WRITE_REQ)\n        data.append(TORQUE_CONTROL_RAM)\n        data.append(0x01)\n        data.append(0x00)\n        send_data(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef torque_on(self):\n        data = []\n        data.append(0x0A)\n        data.append(self.servoid)\n        data.append(RAM_WRITE_REQ)\n        data.append(TORQUE_CONTROL_RAM)\n        data.append(0x01)\n        data.append(0x60)\n        send_data(data)", "response": "Enable the torques of Herkulex\n        in this mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the torque state of motor", "response": "def get_torque_state(self):\n        \"\"\" get the torque state of motor\n\n        Returns:\n            bool: True if torque is enabled, else False\n        \"\"\"\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(TORQUE_CONTROL_RAM)\n        data.append(BYTE2)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(13)\n            return bool(ord(rxdata[9]))\n        except HerkulexError:\n            raise HerkulexError(\"could not communicate with motors\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_servo_position(self, goalposition, goaltime, led):\n        goalposition_msb = int(goalposition) >> 8\n        goalposition_lsb = int(goalposition) & 0xff\n\n        data = []\n        data.append(0x0C)\n        data.append(self.servoid)\n        data.append(I_JOG_REQ)\n        data.append(goalposition_lsb)\n        data.append(goalposition_msb)\n        data.append(led)\n        data.append(self.servoid)\n        data.append(goaltime)\n        send_data(data)", "response": "Set the position of the servo in the specified time range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_servo_position(self):\n        #global SERPORT\n\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(CALIBRATED_POSITION_RAM)\n        data.append(BYTE2)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(13)\n            if (self.servomodel==0x06) or (self.servomodel == 0x04):\n                return ((ord(rxdata[10])&0xff)<<8) | (ord(rxdata[9])&0xFF)\n            else:\n                #print ord(rxdata[9]),ord(rxdata[10])\n                return ((ord(rxdata[10])&0x03)<<8) | (ord(rxdata[9])&0xFF)\n\n        except HerkulexError:\n            print \"Could not read from the servos. Check connection\"", "response": "Gets the current position of the Herkulex."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the current temperature of Herkulex", "response": "def get_servo_temperature(self):\n        \"\"\" Gets the current temperature of Herkulex\n\n        Args:\n            none\n\n        Returns:\n            int: the current temperature register of Herkulex\n\n        Raises:\n            SerialException: Error occured while opening serial port\n\n       \"\"\"\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(TEMPERATURE_RAM)\n        data.append(BYTE2)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(13)\n            return ord(rxdata[9])\n        except HerkulexError:\n            raise HerkulexError(\"Could not communicate with motors\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_servo_torque(self):\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(PWM_RAM)\n        data.append(BYTE2)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(13)\n            if ord(rxdata[10])<=127:\n                return ((ord(rxdata[10])&0x03)<<8) | (ord(rxdata[9])&0xFF)\n            else:\n                return (ord(rxdata[10])-0xFF)*0xFF + (ord(rxdata[9])&0xFF)-0xFF\n        except HerkulexError:\n            raise HerkulexError(\"could not communicate with motors\")", "response": "Gets the current torque of Herkulex."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the Herkulex in continuous rotation mode.", "response": "def set_servo_speed(self, goalspeed, led):\n        \"\"\" Set the Herkulex in continuous rotation mode\n\n        Args:\n\n            goalspeed (int): the speed , range -1023 to 1023\n            led (int): the LED color\n                       0x00 LED off\n                       0x04 GREEN\n                       0x08 BLUE\n                       0x10 RED\n\n        \"\"\"\n        if goalspeed>0 :\n            goalspeed_msb = (int(goalspeed)& 0xFF00) >> 8\n            goalspeed_lsb = int(goalspeed) & 0xff\n        elif goalspeed<0 :\n            goalspeed_msb = 64+(255- ((int(goalspeed)& 0xFF00) >> 8))\n            goalspeed_lsb = (abs(goalspeed) & 0xff)\n\n        #print goalspeed_msb,goalspeed_lsb\n        data = []\n        data.append(0x0C)\n        data.append(self.servoid)\n        data.append(I_JOG_REQ)\n        data.append(goalspeed_lsb)\n        data.append(goalspeed_msb)\n        data.append(0x02|led)\n        data.append(self.servoid)\n        data.append(0x00)\n        send_data(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the P gain of the ethernet ethernet", "response": "def set_position_p(self, pvalue):\n        \"\"\" Set the P gain of the  position PID\n\n        Args:\n\n            pvalue (int): P value\n        \"\"\"\n        pvalue_msb = int(pvalue) >> 8\n        pvalue_lsb = int(pvalue) & 0xff\n        data = []\n        data.append(0x0B)\n        data.append(self.servoid)\n        data.append(RAM_WRITE_REQ)\n        data.append(POSITION_KP_RAM)\n        data.append(BYTE2)\n        data.append( pvalue_lsb)\n        data.append( pvalue_msb)\n        send_data(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the I gain of the position PID", "response": "def set_position_i(self, ivalue):\n        \"\"\" Set the I gain of the position PID\n\n        Args:\n            ivalue (int): I value\n        \"\"\"\n        ivalue_msb = int(ivalue) >> 8\n        ivalue_lsb = int(ivalue) & 0xff\n\n        data = []\n        data.append(0x0B)\n        data.append(self.servoid)\n        data.append(RAM_WRITE_REQ)\n        data.append(POSITION_KI_RAM)\n        data.append(BYTE2)\n        data.append(ivalue_lsb)\n        data.append(ivalue_msb)\n        send_data(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the D gain of the PID", "response": "def set_position_d(self, dvalue):\n        \"\"\" Set the D gain of the PID\n\n        Args:\n            dvalue (int): D value\n        \"\"\"\n        dvalue_msb = int(dvalue) >> 8\n        dvalue_lsb = int(dvalue) & 0xff\n        data = []\n        data.append(0x0B)\n        data.append(self.servoid)\n        data.append(RAM_WRITE_REQ)\n        data.append(POSITION_KD_RAM)\n        data.append(BYTE2)\n        data.append(dvalue_lsb)\n        data.append(dvalue_msb)\n        send_data(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_position_p(self):\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(POSITION_KP_RAM)\n        data.append(BYTE2)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(13)\n            return (ord(rxdata[10])*256)+(ord(rxdata[9])&0xff)\n        except HerkulexError:\n            raise HerkulexError(\"could not communicate with motors\")", "response": "Get the P value of the current PID for position\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the I value of the current PID for position", "response": "def get_position_i(self):\n        \"\"\" Get the I value of the current PID for position\n\n        \"\"\"\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(POSITION_KI_RAM)\n        data.append(BYTE2)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(13)\n            return (ord(rxdata[10])*256)+(ord(rxdata[9])&0xff)\n        except HerkulexError:\n            raise HerkulexError(\"Could not read from motors\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the D value of the current PID for position", "response": "def get_position_d(self):\n        \"\"\" Get the D value of the current PID for position\n\n        \"\"\"\n        data = []\n        data.append(0x09)\n        data.append(self.servoid)\n        data.append(RAM_READ_REQ)\n        data.append(POSITION_KD_RAM)\n        data.append(BYTE2)\n        send_data(data)\n        rxdata = []\n        try:\n            rxdata = SERPORT.read(13)\n            return (ord(rxdata[10])*256)+(ord(rxdata[9])&0xff)\n        except HerkulexError:\n            raise HerkulexError(\"could not communicate with motors\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the PID values from RAM to EEPROM.", "response": "def save_pid_eeprom(self):\n        \"\"\" saves the PID values from RAM to EEPROM\n\n        \"\"\"\n        pval = self.get_position_p()\n        ival = self.get_position_i()\n        dval = self.get_position_d()\n\n        #write P value\n        pvalue_msb = int(pval) >> 8\n        pvalue_lsb = int(pval) & 0xff\n\n        data_p = []\n        data_p.append(0x0B)\n        data_p.append(self.servoid)\n        data_p.append(EEP_WRITE_REQ)\n        data_p.append(POSITION_KP_EEP)\n        data_p.append(BYTE2)\n        data_p.append( pvalue_lsb)\n        data_p.append( pvalue_msb)\n        send_data(data_p)\n\n        # write I value\n        ivalue_msb = int(ival) >> 8\n        ivalue_lsb = int(ival) & 0xff\n\n        data_i = []\n        data_i.append(0x0B)\n        data_i.append(self.servoid)\n        data_i.append(EEP_WRITE_REQ)\n        data_i.append(POSITION_KI_EEP)\n        data_i.append(BYTE2)\n        data_i.append( ivalue_lsb)\n        data_i.append( ivalue_msb)\n        send_data(data_i)\n\n        # write D value\n        dvalue_msb = int(dval) >> 8\n        dvalue_lsb = int(dval) & 0xff\n\n        data_d = []\n        data_d.append(0x0B)\n        data_d.append(self.servoid)\n        data_d.append(EEP_WRITE_REQ)\n        data_d.append(POSITION_KD_EEP)\n        data_d.append(BYTE2)\n        data_d.append( dvalue_lsb)\n        data_d.append( dvalue_msb)\n        send_data(data_d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_servo_angle(self, goalangle, goaltime, led):\n        if (self.servomodel==0x06) or (self.servomodel == 0x04):\n            goalposition = scale(goalangle, -159.9, 159.6, 10627, 22129)\n        else:\n            goalposition = scale(goalangle, -150, 150, 21, 1002)\n\n        self.set_servo_position(goalposition, goaltime, led)", "response": "Sets the servo angle in degrees."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the current angle of the servo in degrees", "response": "def get_servo_angle(self):\n        \"\"\" Gets the current angle of the servo in degrees\n\n        Args:\n            none\n        Returns:\n            int : the current servo angle\n        \"\"\"\n\n        servoposition =  self.get_servo_position()\n        if (self.servomodel==0x06) or (self.servomodel == 0x04):\n            return scale(servoposition, 10627, 22129, -159.9, 159.6)\n        else:\n            return scale(servoposition, 21, 1002, -150, 150)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self):\n        if self.probes is None:\n            self._stop = True\n        while True:\n\n            if self._stop:\n                break\n\n            self.probes_values = {\n                instrument_name:\n                    {probe_name: probe_instance.value for probe_name, probe_instance in probe.items()}\n                for instrument_name, probe in self.probes.items()\n                }\n\n            self.updateProgress.emit(1)\n\n            self.msleep(int(1e3*self.refresh_interval))", "response": "This method is the main thread that reads values from the probes_values attribute of the class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the read probe thread", "response": "def start(self, *args, **kwargs):\n        \"\"\"\n        start the read_probe thread\n        \"\"\"\n        self._stop = False\n        super(ReadProbes, self).start(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quit(self, *args, **kwargs):  # real signature unknown\n        self._stop = True\n        super(ReadProbes, self).quit(*args, **kwargs)", "response": "quit the read probe thread\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the current subscript and keeps a counter of how often a particular subscript has been executed and keeps a counter of how often a particular subscript has been executed.", "response": "def _set_current_subscript(self, active):\n        \"\"\"\n        sets the current subscript and keeps a counter of how ofter a particular subscript has been executed\n        this information is usefull when implementing a status update or plotting functions that depend on which subscript is being executed\n\n        keeps track of the following dictionary:\n        self._current_subscript_stage = {\n            'current_subscript' : reference to the current subscrit\n            'subscript_exec_count' : dictionary where key is the subscript name and value how often is has been executed\n            'subscript_exec_duration' : dictionary where key is the subscript name and value the average duration of executing the subscript\n        }\n\n        Args:\n            active: True if the current subscript is just started, False if it just finished\n        \"\"\"\n\n        current_subscript = self.sender()\n\n\n        if active:\n            for subscript_name in list(self._current_subscript_stage['subscript_exec_count'].keys()):\n                if subscript_name == current_subscript.name:\n                    self._current_subscript_stage['subscript_exec_count'][subscript_name] += 1\n            self._current_subscript_stage['current_subscript'] = current_subscript\n        else:\n            self._current_subscript_stage['current_subscript'] = current_subscript\n            for subscript_name in list(self._current_subscript_stage['subscript_exec_count'].keys()):\n                # calculate the average duration to execute the subscript\n                if subscript_name == current_subscript.name:\n                    duration = current_subscript.end_time - current_subscript.start_time\n                    if subscript_name  in self._current_subscript_stage['subscript_exec_duration']:\n                        duration_old = self._current_subscript_stage['subscript_exec_duration'][subscript_name]\n                    else:\n                        duration_old = datetime.timedelta(0)\n                    exec_count = self._current_subscript_stage['subscript_exec_count'][subscript_name]\n\n                    duration_new = (duration_old * (exec_count - 1) + duration)\n                    self._current_subscript_stage['subscript_exec_duration'][subscript_name] = (duration_old * (\n                    exec_count - 1) + duration) / exec_count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending input string to log file and sends it to log function", "response": "def log(self, string):\n        \"\"\"\n        appends input string to log file and sends it to log function (self.log_function)\n        Returns:\n\n        \"\"\"\n\n        self.log_data.append(string)\n        if self.log_function is None:\n            print(string)\n        else:\n            self.log_function(string)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the internal dictionary with the given parameters.", "response": "def update(self, settings):\n        '''\n        updates the internal dictionary\n        Args:\n            settings: parameters to be set\n        # mabe in the future:\n        # Returns: boolean that is true if update successful\n\n        '''\n        if 'settings' in settings:\n            self._settings.update(settings['settings'])\n        else:\n            self._settings.update(settings)\n\n        if 'instruments' in settings:\n            for instrument_name, instrument_setting in settings['instruments'].items():\n                self.instruments[instrument_name]['settings'].update(instrument_setting['settings'])\n\n        if 'scripts' in settings:\n            for script_name, script_setting in settings['scripts'].items():\n                self.scripts[script_name].update(script_setting)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remaining_time(self):\n        elapsed_time = (datetime.datetime.now() - self.start_time).total_seconds()\n        # safety to avoid devision by zero\n        if self.progress == 0:\n            self.progress = 1\n\n        estimated_total_time = 100. / self.progress * elapsed_time\n\n        return datetime.timedelta(seconds = max(estimated_total_time - elapsed_time, 0))", "response": "estimates the time remaining until script is finished"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _receive_signal(self, progress):\n        # print(datetime.datetime.now().strftime(\"%B %d, %Y %H:%M:%S\"), self.name,QtCore.QThread.currentThread(), self._current_subscript_stage['current_subscript'].name,\n        #       'received signal. emitting....')\n\n        self.progress = progress\n        self.updateProgress.emit(progress)", "response": "This function is called by the main thread when the main thread is listening for a signal. It will emit the updateProgress signal and update the progress object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the script and returns True if execution of script finished successfully.", "response": "def run(self):\n        \"\"\"\n        executes the script\n        :return: boolean if execution of script finished succesfully\n        \"\"\"\n        self.log_data.clear()\n        self._plot_refresh = True  # flag that requests that plot axes are refreshed when self.plot is called next time\n        self.is_running = True\n        self.start_time = datetime.datetime.now()\n\n\n        self._current_subscript_stage = {\n            'current_subscript': None,\n            'subscript_exec_count':{},\n            'subscript_exec_duration':{}\n        }\n\n        # update the datapath of the subscripts, connect their progress signal to the receive slot\n        for subscript in list(self.scripts.values()):\n            subscript.data_path = os.path.join(self.filename(create_if_not_existing=False), self.SUBSCRIPT_DATA_DIR)\n            subscript.updateProgress.connect(self._receive_signal)\n            subscript.started.connect(lambda: self._set_current_subscript(True))\n            subscript.finished.connect(lambda: self._set_current_subscript(False))\n            self._current_subscript_stage['subscript_exec_count'].update({subscript.name:0})\n            self._current_subscript_stage['subscript_exec_duration'].update({subscript.name: datetime.timedelta(0)})\n\n                #todo: 170202JG (search for this to find related todos) need to test this:\n                # do we need to connect the log functions of the subscript to the mother script?, e.g\n                # subscript.log.connect(self.log)\n\n\n        self.log('starting script {:s} at {:s} on {:s}'.format(self.name, self.start_time.strftime('%H:%M:%S'),self.start_time.strftime('%d/%m/%y')))\n        self._abort = False\n\n        #saves standard to disk\n        if self.settings['save']:\n            self.save_b26()\n\n        self.started.emit()\n\n        self._function()\n        self.end_time  = datetime.datetime.now()\n        self.log('script {:s} finished at {:s} on {:s}'.format(self.name, self.end_time.strftime('%H:%M:%S'),self.end_time.strftime('%d/%m/%y')))\n\n        #saves standard to disk\n        if self.settings['save']:\n            self.save_data()\n            self.save_log()\n            self.save_image_to_disk()\n\n        success = not self._abort\n\n        # disconnect subscripts\n        for subscript in list(self.scripts.values()):\n            subscript.started.disconnect()\n            subscript.updateProgress.disconnect()\n            subscript.finished.disconnect()\n        self.is_running = False\n        self.finished.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop(self):\n        for subscript in list(self.scripts.values()):\n            subscript.stop()\n        print(('--- stopping: ', self.name))\n        self._abort = True", "response": "Stops all the scripts and all the scripts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filename(self, appendix=None, create_if_not_existing=False):\n\n        # if provided path is a relative path and self.data_path exists, build path\n        if os.path.isabs(self.settings['path']) == False and self.data_path is not None:\n            path = os.path.join(self.data_path, self.settings['path'])\n        else:\n            path = self.settings['path']\n\n        tag = self.settings['tag']#.replace('.','-')\n\n        filename = os.path.join(path, \"{:s}_{:s}\".format(self.start_time.strftime('%y%m%d-%H_%M_%S'),tag))\n\n        if os.path.exists(filename) == False and create_if_not_existing:\n            os.makedirs(filename)\n\n        if appendix is not None:\n            filename = os.path.join(filename,  \"{:s}_{:s}{:s}\".format(self.start_time.strftime('%y%m%d-%H_%M_%S'),tag,appendix))\n\n        # windows can't deal with long filenames so we have to use the prefix '\\\\\\\\?\\\\'\n        # if len(filename.split('\\\\\\\\?\\\\')) == 1:\n        #     filename = '\\\\\\\\?\\\\' + filename\n\n        return filename", "response": "Creates a filename based on the current settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the script settings to a file", "response": "def save_b26(self, filename=None):\n        \"\"\"\n        saves the script settings to a file: filename is filename is not provided, it is created from internal function\n        \"\"\"\n        if filename is None:\n            filename = self.filename('.b26')\n\n        # if platform.system() == 'Windows':\n        #     # windows can't deal with long filenames so we have to use the prefix '\\\\\\\\?\\\\'\n        #     if len(filename.split('\\\\\\\\?\\\\')) == 1:\n        #         filename = '\\\\\\\\?\\\\' + filename\n        save_b26_file(filename, scripts=self.to_dict(), overwrite=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_image_to_disk(self, filename_1 = None, filename_2 = None):\n\n        def axes_empty(ax):\n            \"\"\"\n            takes an axes object and checks if it is empty\n            the axes object is considered empty it doesn't contain any of the following:\n                - lines\n                - images\n                - patches\n            Returns:\n\n            \"\"\"\n\n            is_empty = True\n\n            if ax is not None and len(ax)>0:\n                for a in ax:\n                    if len(a.lines)+len(a.images)+len(a.patches) != 0:\n                        is_empty = False\n\n\n            return is_empty\n\n        # create and save images\n        if (filename_1 is None):\n            filename_1 = self.filename('-plt1.png')\n\n        if (filename_2 is None):\n            filename_2 = self.filename('-plt2.png')\n\n\n        # windows can't deal with long filenames so we have to use the prefix '\\\\\\\\?\\\\'\n        # if len(filename_1.split('\\\\\\\\?\\\\')) == 1:\n        #     filename_1 = '\\\\\\\\?\\\\' + filename_1\n        # if len(filename_2.split('\\\\\\\\?\\\\')) == 1:\n        #     filename_2 = '\\\\\\\\?\\\\' + filename_2\n\n        if os.path.exists(os.path.dirname(filename_1)) is False:\n            os.makedirs(os.path.dirname(filename_1))\n        if os.path.exists(os.path.dirname(filename_2)) is False:\n            os.makedirs(os.path.dirname(filename_2))\n\n\n        fig_1 = Figure()\n        canvas_1 = FigureCanvas(fig_1)\n\n        fig_2 = Figure()\n        canvas_2 = FigureCanvas(fig_2)\n\n        self.force_update()\n\n        self.plot([fig_1, fig_2])\n\n        if filename_1 is not None and not axes_empty(fig_1.axes):\n            fig_1.savefig(filename_1)\n        if filename_2 is not None and not axes_empty(fig_2.axes):\n            fig_2.savefig(filename_2)", "response": "Creates and saves an image to disk."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, filename):\n\n        if filename is None:\n            filename = self.filename('.b26s')\n        # if len(filename.split('\\\\\\\\?\\\\')) == 1:\n        #     filename = '\\\\\\\\?\\\\' + filename\n        with open(filename, 'w') as outfile:\n            outfile.write(pickle.dumps(self.__dict__))", "response": "Saves the instance of the script to a file using pickle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(filename, instruments = None):\n        with open(filename, 'r') as infile:\n            dataPickle = infile.read()\n\n        script_as_dict = pickle.loads(dataPickle)\n        script_class = script_as_dict['_script_class']\n\n        script_instance, _, updated_instruments = Script.load_and_append({'script': script_class}, instruments = instruments)\n        script_instance = script_instance['script']\n\n        # save references to instruments\n        instruments = script_instance._instruments\n\n        # update the script instance\n        script_instance.__dict__ = script_as_dict\n\n        # update references to instruments\n        script_instance._instruments = instruments\n\n        return script_instance, updated_instruments", "response": "Loads an instance of a script instance using pickle"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_data(path):\n\n\n        # check that path exists\n        if not os.path.exists(path):\n            print(path)\n            raise AttributeError('Path given does not exist!')\n\n        # windows can't deal with long filenames (>260 chars) so we have to use the prefix '\\\\\\\\?\\\\'\n        # if len(path.split('\\\\\\\\?\\\\')) == 1:\n        #     path = '\\\\\\\\?\\\\' + os.path.abspath(path)\n\n\n        # if raw_data folder exists, get a list of directories from within it; otherwise, get names of all .csv files in\n        # current directory\n        data = {}\n        # if self.RAW_DATA_DIR in os.listdir(path): #8/26/16 AK: self not defined in static context\n        #     data_files = os.listdir(os.path.join(path, self.RAW_DATA_DIR + '/'))\n        #     path = os.path.join(path, self.RAW_DATA_DIR + '/')\n        #\n        # else:\n        if 'raw_data' in os.listdir(path):  #temporarily hardcoded\n            data_files = os.listdir(os.path.join(path, 'raw_data' + '/'))\n            path = os.path.join(path, 'raw_data' + '/')\n\n        else:\n            data_files = glob.glob(os.path.join(path, '*.csv'))\n\n        # If no data files were found, raise error\n        if not data_files:\n            raise AttributeError('Could not find data files in {:s}'.format(path))\n\n        # import data from each csv\n        for data_file in data_files:\n            # get data name, read the data from the csv, and save it to dictionary\n            data_name = data_file.split('-')[-1][0:-4] # JG: why do we strip of the date?\n            imported_data_df = pd.read_csv(os.path.join(path, data_file))\n\n            # check if there are real headers, if the headers are digits than we ignore them because then they are just indecies\n            # real headers are strings (however, the digits are also of type str! that why we use the isdigit method)\n            column_headers = list(imported_data_df.columns.values)\n            if sum([int(x.isdigit()) for x in column_headers]) != len(column_headers):\n                data[data_name] = {h: imported_data_df[h].as_matrix() for h in column_headers}\n            else:\n                # note, np.squeeze removes extraneous length-1 dimensions from the returned 'matrix' from the dataframe\n                data[data_name] = np.squeeze(imported_data_df.as_matrix())\n\n        return data", "response": "Loads the data that has been saved with Script. save."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps to get the module for a specific script", "response": "def get_script_module(script_information, package='pylabcontrol', verbose=False):\n        \"\"\"\n        wrapper to get the module for a script\n\n        Args:\n            script_information: information of the script. This can be\n                - a dictionary\n                - a Script instance\n                - name of Script class\n            package (optional): name of the package to which the script belongs, i.e. pylabcontrol or b26toolkit only used when script_information is a string\n        Returns:\n            module\n\n        \"\"\"\n\n        module, _, _, _, _, _, _ = Script.get_script_information(script_information=script_information, package=package, verbose=verbose)\n\n        return module"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an duplicate of the script", "response": "def duplicate(self):\n        \"\"\"\n        create an copy of the script\n\n        Returns:\n\n        \"\"\"\n\n        # get settings of script\n        class_of_script = self.__class__\n        script_name = self.name\n        script_instruments = self.instruments\n        sub_scripts = self.scripts\n        script_settings = self.settings\n        log_function = self.log_function\n        data_path = self.data_path\n\n\n        #create a new instance of same script type\n        class_creation_string = ''\n        if script_instruments != {}:\n            class_creation_string += ', instruments = script_instruments'\n        if sub_scripts != {}:\n            class_creation_string += ', scripts = sub_scripts'\n        if script_settings != {}:\n            class_creation_string += ', settings = script_settings'\n        if log_function is not None:\n            class_creation_string += ', log_function = log_function'\n        if data_path is not None:\n            class_creation_string += ', data_path = data_path'\n        class_creation_string = 'class_of_script(name=script_name{:s})'.format(class_creation_string)\n        # create instance\n        script_instance = eval(class_creation_string)\n\n        # copy some other properties that might be checked later for the duplicated script\n        script_instance.data = deepcopy(self.data)\n        script_instance.start_time = self.start_time\n        script_instance.end_time = self.end_time\n        script_instance.is_running = self.is_running\n\n        return script_instance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(self, figure_list):\n        # if there is not data we do not plot anything\n        if not self.data:\n            return\n\n        # if plot function is called when script is not running we request a plot refresh\n        if not self.is_running:\n            self._plot_refresh = True\n\n        axes_list = self.get_axes_layout(figure_list)\n        if self._plot_refresh is True:\n            self._plot(axes_list)\n            self._plot_refresh = False\n            for figure in figure_list:\n                if figure.axes:\n                    figure.set_tight_layout(True)\n        else:\n            self._update_plot(axes_list)", "response": "Plots the data contained in self. data and updates the axes layout of the last entry in self. data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the axes objects that the script needs to plot its data", "response": "def get_axes_layout(self, figure_list):\n        \"\"\"\n        returns the axes objects the script needs to plot its data\n        the default creates a single axes object on each figure\n        This can/should be overwritten in a child script if more axes objects are needed\n        Args:\n            figure_list: a list of figure objects\n        Returns:\n            axes_list: a list of axes objects\n\n        \"\"\"\n        axes_list = []\n        if self._plot_refresh is True:\n            for fig in figure_list:\n                fig.clf()\n                axes_list.append(fig.add_subplot(111))\n\n        else:\n            for fig in figure_list:\n                axes_list.append(fig.axes[0])\n\n        return axes_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the data contained in self.data, which should be a dictionary or a deque of dictionaries for the latter use the last entry", "response": "def plot_validate(self, figure_list):\n        \"\"\"\n        plots the data contained in self.data, which should be a dictionary or a deque of dictionaries\n        for the latter use the last entry\n\n        \"\"\"\n        axes_list = self.get_axes_layout_validate(figure_list)\n        self._plot_validate(axes_list)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uniqueof20(k, rep=10000):\n    alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n    reps = [len(set(random.choice(alphabet)\n                    for i in range(k)))\n            for j in range(rep)]\n    return sum(reps) / len(reps)", "response": "Sample k times out of alphabet how many different?"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_file_dialog(self):\n        dialog = QtGui.QFileDialog\n        filename = dialog.getExistingDirectory(self, 'Select a file:', self.data_filepath.text())\n        if str(filename)!='':\n            self.data_filepath.setText(filename)", "response": "Opens a file dialog to get the path to a file and\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract member or element of obj according to pointer.", "response": "def extract(obj, pointer, bypass_ref=False):\n    \"\"\"Extract member or element of obj according to pointer.\n\n    :param obj: the object source\n    :param pointer: the pointer\n    :type pointer: Pointer, str\n    :param bypass_ref: bypass JSON Reference event\n    :type bypass_ref: boolean\n    \"\"\"\n\n    return Pointer(pointer).extract(obj, bypass_ref)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the amino acid frequencies in a set of SeqRecords.", "response": "def aa_counts(aln, weights=None, gap_chars='-.'):\n    \"\"\"Calculate the amino acid frequencies in a set of SeqRecords.\n\n    Weights for each sequence in the alignment can be given as a list/tuple,\n    usually calculated with the sequence_weights function. For convenience, you\n    can also pass \"weights=True\" and the weights will be calculated with\n    sequence_weights here.\n    \"\"\"\n    if weights is None:\n        counts = Counter()\n        for rec in aln:\n            seq_counts = Counter(str(rec.seq))\n            counts.update(seq_counts)\n    else:\n        if weights == True:\n            # For convenience\n            weights = sequence_weights(aln)\n        else:\n            assert len(weights) == len(aln), (\n                \"Length mismatch: weights = %d, alignment = %d\"\n                % (len(weights), len(aln)))\n        counts = defaultdict(float)\n        for col in zip(*aln):\n            for aa, wt in zip(col, weights):\n                counts[aa] += wt\n\n    # Don't count gaps\n    for gap_char in gap_chars:\n        if gap_char in counts:\n            del counts[gap_char]\n    return counts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of residue type - > frequency.", "response": "def aa_frequencies(aln, weights=None, gap_chars='-.'):\n    \"\"\"Frequency of each residue type in an alignment.\n\n    Alignment is a MultipleSeqAlignment or iterable of SeqRecords.\n    \"\"\"\n    counts = aa_counts(aln, weights, gap_chars)\n    # Reduce to frequencies\n    scale = 1.0 / sum(counts.values())\n    return dict((aa, cnt * scale) for aa, cnt in counts.iteritems())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove gappy columns from an alignment.", "response": "def blocks(aln, threshold=0.5, weights=None):\n    \"\"\"Remove gappy columns from an alignment.\"\"\"\n    assert len(aln)\n    if weights == False:\n        def pct_nongaps(col):\n            return 1 - (float(col.count('-')) / len(col))\n    else:\n        if weights in (None, True):\n            weights = sequence_weights(aln, 'avg1')\n        def pct_nongaps(col):\n            assert len(col) == len(weights)\n            ngaps = sum(wt * (c == '-')\n                        for wt, c in zip(weights, col))\n            return 1 - (ngaps / len(col))\n\n    seqstrs = [str(rec.seq) for rec in aln]\n    clean_cols = [col for col in zip(*seqstrs)\n                  if pct_nongaps(col) >= threshold]\n    alphabet = aln[0].seq.alphabet\n    clean_seqs = [Seq(''.join(row), alphabet)\n                  for row in zip(*clean_cols)]\n    clean_recs = []\n    for rec, seq in zip(aln, clean_seqs):\n        newrec = deepcopy(rec)\n        newrec.seq = seq\n        clean_recs.append(newrec)\n    return MultipleSeqAlignment(clean_recs, alphabet=alphabet)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef col_counts(col, weights=None, gap_chars='-.'):\n    cnt = defaultdict(float)\n    for aa, wt in zip(col, weights):\n        if aa not in gap_chars:\n            cnt[aa] += wt\n    return cnt", "response": "Absolute counts of each residue type in a single column."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of frequencies of each residue type in a single column.", "response": "def col_frequencies(col, weights=None, gap_chars='-.'):\n    \"\"\"Frequencies of each residue type (totaling 1.0) in a single column.\"\"\"\n    counts = col_counts(col, weights, gap_chars)\n    # Reduce to frequencies\n    scale = 1.0 / sum(counts.values())\n    return dict((aa, cnt * scale) for aa, cnt in counts.iteritems())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves all - gap columns from aligned SeqRecords.", "response": "def remove_empty_cols(records):\n    \"\"\"Remove all-gap columns from aligned SeqRecords.\"\"\"\n    # In case it's a generator, turn it into a list\n    records = list(records)\n    seqstrs = [str(rec.seq) for rec in records]\n    clean_cols = [col\n                  for col in zip(*seqstrs)\n                  if not all(c == '-' for c in col)]\n    clean_seqs = [''.join(row)\n                  for row in zip(*clean_cols)]\n    for rec, clean_seq in zip(records, clean_seqs):\n        yield SeqRecord(Seq(clean_seq, rec.seq.alphabet), id=rec.id,\n                        name=rec.name, description=rec.description,\n                        dbxrefs=rec.dbxrefs, features=rec.features,\n                        annotations=rec.annotations,\n                        letter_annotations=rec.letter_annotations)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sequence_weights(aln, scaling='none', gap_chars='-.'):\n    # Probability is hard, let's estimate by sampling!\n    # Sample k from a population of 20 with replacement; how many unique k were\n    # chosen? Average of 10000 runs for k = 0..100\n    expectk = [0.0, 1.0, 1.953, 2.861, 3.705, 4.524, 5.304, 6.026, 6.724, 7.397,\n               8.04, 8.622, 9.191, 9.739, 10.264, 10.758, 11.194, 11.635,\n               12.049, 12.468, 12.806, 13.185, 13.539, 13.863, 14.177, 14.466,\n               14.737, 15.005, 15.245, 15.491, 15.681, 15.916, 16.12, 16.301,\n               16.485, 16.671, 16.831, 16.979, 17.151, 17.315, 17.427, 17.559,\n               17.68, 17.791, 17.914, 18.009, 18.113, 18.203, 18.298, 18.391,\n               18.46, 18.547, 18.617, 18.669, 18.77, 18.806, 18.858, 18.934,\n               18.978, 19.027, 19.085, 19.119, 19.169, 19.202, 19.256, 19.291,\n               19.311, 19.357, 19.399, 19.416, 19.456, 19.469, 19.5, 19.53,\n               19.553, 19.562, 19.602, 19.608, 19.629, 19.655, 19.67, 19.681,\n               19.7, 19.716, 19.724, 19.748, 19.758, 19.765, 19.782, 19.791,\n               19.799, 19.812, 19.82, 19.828, 19.844, 19.846, 19.858, 19.863,\n               19.862, 19.871, 19.882]\n\n    def col_weight(column):\n        \"\"\"Represent the diversity at a position.\n\n        Award each different residue an equal share of the weight, and then\n        divide that weight equally among the sequences sharing the same\n        residue.\n\n        So, if in a position of a multiple alignment, r different residues\n        are represented, a residue represented in only one sequence contributes\n        a score of 1/r to that sequence, whereas a residue represented in s\n        sequences contributes a score of 1/rs to each of the s sequences.\n        \"\"\"\n        # Skip columns of all or mostly gaps (i.e. rare inserts)\n        min_nongap = max(2, .2*len(column))\n        if len([c for c in column if c not in gap_chars]) < min_nongap:\n            return ([0] * len(column), 0)\n        # Count the number of occurrences of each residue type\n        # (Treat gaps as a separate, 21st character)\n        counts = Counter(column)\n        # Get residue weights: 1/rs, where\n        # r = nb. residue types, s = count of a particular residue type\n        n_residues = len(counts)    # r\n        freqs = dict((aa, 1.0 / (n_residues * count))\n                     for aa, count in counts.iteritems())\n        weights = [freqs[aa] for aa in column]\n        return (weights, n_residues)\n\n    seq_weights = [0] * len(aln)\n    tot_nres = 0.0  # Expected no. different types in independent seqs\n    # Sum the contributions from each position along each sequence\n    # -> total weight\n    for col in zip(*aln):\n        wts, nres = col_weight(col)\n        assert sum(wts) <= 20\n        tot_nres += expectk[nres] if nres < len(expectk) else 20\n        for idx, wt in enumerate(wts):\n            seq_weights[idx] += wt\n    # if tot_nres == 0:\n    #     raise ValueError(\"Alignment has no meaningful columns to weight\")\n    # Normalize w/ the given scaling criterion\n    if scaling == 'none':\n        avg_seq_len = tot_nres / len(aln)\n        return [wt/avg_seq_len for wt in seq_weights]\n    if scaling == 'max1':\n        scale = 1.0 / max(seq_weights)\n    elif scaling == 'sum1':\n        scale = 1.0 / sum(seq_weights)\n    elif scaling == 'avg1':\n        scale = len(aln) / sum(seq_weights)\n    elif scaling == 'andy':\n        # \"Robust\" strategy used in CHAIN (Neuwald 2003)\n        scale = len(aln) / sum(seq_weights)\n        return [min(scale * wt, 1.0) for wt in seq_weights]\n    else:\n        raise ValueError(\"Unknown scaling scheme '%s'\" % scaling)\n    return [scale * wt for wt in seq_weights]", "response": "Returns a list of floating - point numbers corresponding to the number of sequences in the alignment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_graph(alnfname, weight_func):\n    import networkx\n    G = networkx.Graph()\n    aln = AlignIO.read(alnfname, 'fasta')\n    for i, arec in enumerate(aln):\n        for brec in aln[i+1:]:\n            ident = weight_func(str(arec.seq), str(brec.seq))\n            G.add_edge(arec.id, brec.id, weight=ident)\n    return G", "response": "Create a NetworkX graph from an alignment file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn Met Office guidance regarding UV exposure based on UV index", "response": "def guidance_UV(index):\n    \"\"\"Return Met Office guidance regarding UV exposure based on UV index\"\"\"\n    if 0 < index < 3:\n        guidance = \"Low exposure. No protection required. You can safely stay outside\"\n    elif 2 < index < 6:\n        guidance = \"Moderate exposure. Seek shade during midday hours, cover up and wear sunscreen\"\n    elif 5 < index < 8:\n        guidance = \"High exposure. Seek shade during midday hours, cover up and wear sunscreen\"\n    elif 7 < index < 11:\n        guidance = \"Very high. Avoid being outside during midday hours. Shirt, sunscreen and hat are essential\"\n    elif index > 10:\n        guidance = \"Extreme. Avoid being outside during midday hours. Shirt, sunscreen and hat essential.\"\n    else:\n        guidance = None\n    return guidance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_sitelist(sitelist):\n    sites = []\n    for site in sitelist[\"Locations\"][\"Location\"]:\n        try:\n            ident = site[\"id\"]\n            name = site[\"name\"]\n        except KeyError:\n            ident = site[\"@id\"] # Difference between loc-spec and text for some reason\n            name = site[\"@name\"]\n        if \"latitude\" in site:\n            lat = float(site[\"latitude\"])\n            lon = float(site[\"longitude\"])\n        else:\n            lat = lon = None\n        s = Site(ident, name, lat, lon)\n        sites.append(s)\n    return sites", "response": "Return list of Site instances from retrieved sitelist data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the nearest site in a list of sites.", "response": "def get_nearest_site(sites, lat, lon):\n    \"\"\"\n    Return a string which can be used as \"request\" in calls to loc_forecast\n    and loc_observations.\n    \n    sites:    List of Site instances\n    lat:      float or int.  Interesting latitude\n    lon:      float or int.  Interesting longitude\n    \"\"\"\n    for site in sites:\n        site.distance_to_coords(lat, lon)\n    sites.sort(key=operator.attrgetter(\"distance\"))\n    return sites[0].ident"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _query(self, data_category, resource_category, field, request, step, isotime=None):\n        rest_url = \"/\".join([HOST, data_category, resource_category, field, DATA_TYPE, request])\n        query_string = \"?\" + \"&\".join([\"res=\" + step, \"time=\" + isotime if isotime is not None else \"\", \"key=\" + self.key])\n        url = rest_url + query_string\n        page = url_lib.urlopen(url)\n        pg = page.read()\n        return pg", "response": "Query DataPoint RESTful API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef loc_forecast(self, request, step, isotime=None):\n        return json.loads(self._query(VAL, FORECAST, ALL, request, step, isotime).decode(errors=\"replace\"))", "response": "Returns a dictionary of location - specific forecast data for a given time step."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loc_observations(self, request):\n        return json.loads(self._query(VAL, OBSERVATIONS, ALL, request, HOURLY).decode(errors=\"replace\"))", "response": "Returns location - specific observation data including a list of sites\n            and a list of times\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nearest_loc_obs(self, lat, lon):\n        sitelist = self.loc_observations(SITELIST)\n        sites = parse_sitelist(sitelist)\n        site = get_nearest_site(sites, lat, lon)\n        return self.loc_observations(site)", "response": "Return the list of observations for the nearest location to lat & lon."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef text_forecast(self, field, request):\n        if request == ALL and field != NATIONAL_PARK: # \"All\" locations only for use with national parks\n            raise TypeError\n        return json.loads(self._query(TEXT, FORECAST, field, request, \"\").decode(errors=\"replace\"))", "response": "Returns the textual forecast data for the given field and request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef text_uk_extremes(self, request):\n        return json.loads(self._query(TEXT, OBSERVATIONS, UK_EXTREMES, request, \"\").decode(errors=\"replace\"))", "response": "Return textual data of UK extremes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stand_alone_imagery(self):\n        return json.loads(self._query(IMAGE, FORECAST, SURFACE_PRESSURE, CAPABILITIES, \"\").decode(errors=\"replace\"))", "response": "Returns a dictionary of all stand alone imagery images and includes\n        URIs for the images."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_overlay_forecast(self):\n        return json.loads(self._query(LAYER, FORECAST, ALL, CAPABILITIES, \"\").decode(errors=\"replace\"))", "response": "Returns capabilities data for forecast map overlays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map_overlay_obs(self):\n        return json.loads(self._query(LAYER, OBSERVATIONS, ALL, CAPABILITIES, \"\").decode(errors=\"replace\"))", "response": "Returns capabilities data for observation map overlays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction is overloaded: - read_probes() - read_probes(key) Args: key: name of requested value Returns: - if called without argument: returns the values of all probes in dictionary form - if called with argument: returns the value the requested key", "response": "def read_probes(self, key = None):\n        \"\"\"\n        function is overloaded:\n            - read_probes()\n            - read_probes(key)\n\n        Args:\n            key: name of requested value\n\n        Returns:\n            - if called without argument: returns the values of all probes in dictionary form\n            - if called with argument: returns the value the requested key\n\n        \"\"\"\n\n        print(('xxxxx probes', key, self._PROBES()))\n\n        if key is None:\n            # return the value all probe in dictionary form\n            d = {}\n            for k in list(self._PROBES.keys()):\n                d[k] = self.read_probes(k)\n            return d\n        else:\n            # return the value of the requested key if the key corresponds to a valid probe\n            assert key in list(self._PROBES.keys())\n\n            value = None\n\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading instrument from instrument_dict and append to instruments", "response": "def load_and_append(instrument_dict, instruments=None, raise_errors=False):\n        \"\"\"\n        load instrument from instrument_dict and append to instruments\n\n        Args:\n            instrument_dict: dictionary of form\n\n                instrument_dict = {\n                name_of_instrument_1 :\n                    {\"settings\" : settings_dictionary, \"class\" : name_of_class}\n                name_of_instrument_2 :\n                    {\"settings\" : settings_dictionary, \"class\" : name_of_class}\n                ...\n                }\n\n            or\n\n                instrument_dict = {\n                name_of_instrument_1 : name_of_class,\n                name_of_instrument_2 : name_of_class\n                ...\n                }\n\n            where name_of_class is either a class or a dictionary of the form {class: name_of__class, filepath: path_to_instr_file}\n\n            instruments: dictionary of form\n\n                instruments = {\n                name_of_instrument_1 : instance_of_instrument_1,\n                name_of_instrument_2 : instance_of_instrument_2,\n                ...\n                }\n\n            raise_errors: if true errors are raised, if False they are caught but not raised\n\n\n\n        Returns:\n                dictionary updated_instruments that contains the old and the new instruments\n\n                and list loaded_failed = [name_of_instrument_1, name_of_instrument_2, ....] that contains the instruments that were requested but could not be loaded\n\n        \"\"\"\n        if instruments is None:\n            instruments = {}\n\n        updated_instruments = {}\n        updated_instruments.update(instruments)\n        loaded_failed = {}\n\n\n\n        for instrument_name, instrument_class_name in instrument_dict.items():\n            instrument_settings = None\n            module = None\n\n            # check if instrument already exists\n            if instrument_name in list(instruments.keys()) \\\n                    and instrument_class_name == instruments[instrument_name].__name__:\n                print(('WARNING: instrument {:s} already exists. Did not load!'.format(instrument_name)))\n                loaded_failed[instrument_name] = instrument_name\n            else:\n                instrument_instance = None\n\n                if isinstance(instrument_class_name, dict):\n                    if 'settings' in instrument_class_name:\n                        instrument_settings = instrument_class_name['settings']\n                    instrument_filepath = str(instrument_class_name['filepath'])\n                    instrument_class_name = str(instrument_class_name['class'])\n                    path_to_module, _ = module_name_from_path(instrument_filepath)\n                    module = import_module(path_to_module)\n                    class_of_instrument = getattr(module, instrument_class_name)\n                    try:\n                        if instrument_settings is None:\n                            # this creates an instance of the class with default settings\n                            instrument_instance = class_of_instrument(name=instrument_name)\n                        else:\n                            # this creates an instance of the class with custom settings\n                            instrument_instance = class_of_instrument(name=instrument_name, settings=instrument_settings)\n                    except Exception as e:\n                        loaded_failed[instrument_name] = e\n                        if raise_errors:\n                            raise e\n                        continue\n                elif isinstance(instrument_class_name, Instrument):\n                    instrument_class_name = instrument_class_name.__class__\n                    instrument_filepath = os.path.dirname(inspect.getfile(instrument_class_name))\n\n                    # here we should also create an instrument instance at some point as in the other cases...\n                    # instrument_instance =\n                    raise NotImplementedError\n                elif issubclass(instrument_class_name, Instrument):\n                    class_of_instrument = instrument_class_name\n                    if instrument_settings is None:\n                        # this creates an instance of the class with default settings\n                        instrument_instance = class_of_instrument(name=instrument_name)\n                    else:\n                        # this creates an instance of the class with custom settings\n                        instrument_instance = class_of_instrument(name=instrument_name,  settings=instrument_settings)\n\n\n                updated_instruments[instrument_name] = instrument_instance\n\n\n\n        return updated_instruments, loaded_failed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompile a JSON schema with JSON Schema_ draft - 04.", "response": "def compile(schema, pointer, context, scope=None):\n    \"\"\"\n    Compiles schema with `JSON Schema`_ draft-04.\n\n    :param schema: obj to compile\n    :type schema: Mapping\n    :param pointer: uri of the schema\n    :type pointer: Pointer, str\n    :param context: context of this schema\n    :type context: Context\n\n    .. _`JSON Schema`: http://json-schema.org\n    \"\"\"\n\n    schm = deepcopy(schema)\n\n    scope = urljoin(scope or str(pointer), schm.pop('id', None))\n\n    if '$ref' in schema:\n        return ReferenceValidator(urljoin(scope, schema['$ref']), context)\n\n    attrs = {}\n\n    if 'additionalItems' in schm:\n        subpointer = pointer_join(pointer, 'additionalItems')\n        attrs['additional_items'] = schm.pop('additionalItems')\n        if isinstance(attrs['additional_items'], dict):\n            compiled = compile(attrs['additional_items'],\n                               subpointer,\n                               context,\n                               scope)\n            attrs['additional_items'] = compiled\n        elif not isinstance(attrs['additional_items'], bool):\n            raise CompilationError('wrong type for {}'.format('additional_items'), schema)  # noqa\n\n    if 'additionalProperties' in schm:\n        subpointer = pointer_join(pointer, 'additionalProperties')\n        attrs['additional_properties'] = schm.pop('additionalProperties')\n        if isinstance(attrs['additional_properties'], dict):\n            compiled = compile(attrs['additional_properties'],\n                               subpointer,\n                               context,\n                               scope)\n            attrs['additional_properties'] = compiled\n        elif not isinstance(attrs['additional_properties'], bool):\n            raise CompilationError('wrong type for {}'.format('additional_properties'), schema)  # noqa\n\n    if 'allOf' in schm:\n        subpointer = pointer_join(pointer, 'allOf')\n        attrs['all_of'] = schm.pop('allOf')\n        if isinstance(attrs['all_of'], (list, tuple)):\n            attrs['all_of'] = [compile(element, subpointer, context, scope) for element in attrs['all_of']]  # noqa\n        else:\n            # should be a boolean\n            raise CompilationError('wrong type for {}'.format('allOf'), schema)  # noqa\n\n    if 'anyOf' in schm:\n        subpointer = pointer_join(pointer, 'anyOf')\n        attrs['any_of'] = schm.pop('anyOf')\n        if isinstance(attrs['any_of'], (list, tuple)):\n            attrs['any_of'] = [compile(element, subpointer, context, scope) for element in attrs['any_of']]  # noqa\n        else:\n            # should be a boolean\n            raise CompilationError('wrong type for {}'.format('anyOf'), schema)  # noqa\n\n    if 'default' in schm:\n        attrs['default'] = schm.pop('default')\n\n    if 'dependencies' in schm:\n        attrs['dependencies'] = schm.pop('dependencies')\n        if not isinstance(attrs['dependencies'], dict):\n            raise CompilationError('dependencies must be an object', schema)\n        for key, value in attrs['dependencies'].items():\n            if isinstance(value, dict):\n                subpointer = pointer_join(pointer, 'dependencies', key)\n                attrs['dependencies'][key] = compile(value,\n                                                     subpointer,\n                                                     context,\n                                                     scope)\n            elif not isinstance(value, sequence_types):\n                raise CompilationError('dependencies must be an array or object', schema)  # noqa\n\n    if 'enum' in schm:\n        attrs['enum'] = schm.pop('enum')\n        if not isinstance(attrs['enum'], sequence_types):\n            raise CompilationError('enum must be a sequence', schema)\n\n    if 'exclusiveMaximum' in schm:\n        attrs['exclusive_maximum'] = schm.pop('exclusiveMaximum')\n        if not isinstance(attrs['exclusive_maximum'], bool):\n            raise CompilationError('exclusiveMaximum must be a boolean', schema)  # noqa\n\n    if 'exclusiveMinimum' in schm:\n        attrs['exclusive_minimum'] = schm.pop('exclusiveMinimum')\n        if not isinstance(attrs['exclusive_minimum'], bool):\n            raise CompilationError('exclusiveMinimum must be a boolean', schema)  # noqa\n\n    if 'format' in schm:\n        attrs['format'] = schm.pop('format')\n        if not isinstance(attrs['format'], string_types):\n            raise CompilationError('format must be a string', schema)\n\n    if 'items' in schm:\n        subpointer = pointer_join(pointer, 'items')\n        attrs['items'] = schm.pop('items')\n        if isinstance(attrs['items'], (list, tuple)):\n            # each value must be a json schema\n            attrs['items'] = [compile(element, subpointer, context, scope) for element in attrs['items']]  # noqa\n        elif isinstance(attrs['items'], dict):\n            # value must be a json schema\n            attrs['items'] = compile(attrs['items'], subpointer, context, scope)  # noqa\n        else:\n            # should be a boolean\n            raise CompilationError('wrong type for {}'.format('items'), schema)  # noqa\n\n    if 'maximum' in schm:\n        attrs['maximum'] = schm.pop('maximum')\n        if not isinstance(attrs['maximum'], number_types):\n            raise CompilationError('maximum must be a number', schema)\n\n    if 'maxItems' in schm:\n        attrs['max_items'] = schm.pop('maxItems')\n        if not isinstance(attrs['max_items'], integer_types):\n            raise CompilationError('maxItems must be integer', schema)\n\n    if 'maxLength' in schm:\n        attrs['max_length'] = schm.pop('maxLength')\n        if not isinstance(attrs['max_length'], integer_types):\n            raise CompilationError('maxLength must be integer', schema)\n\n    if 'maxProperties' in schm:\n        attrs['max_properties'] = schm.pop('maxProperties')\n        if not isinstance(attrs['max_properties'], integer_types):\n            raise CompilationError('maxProperties must be integer', schema)\n\n    if 'minimum' in schm:\n        attrs['minimum'] = schm.pop('minimum')\n        if not isinstance(attrs['minimum'], number_types):\n            raise CompilationError('minimum must be a number', schema)\n\n    if 'minItems' in schm:\n        attrs['min_items'] = schm.pop('minItems')\n        if not isinstance(attrs['min_items'], integer_types):\n            raise CompilationError('minItems must be integer', schema)\n\n    if 'minLength' in schm:\n        attrs['min_length'] = schm.pop('minLength')\n        if not isinstance(attrs['min_length'], integer_types):\n            raise CompilationError('minLength must be integer', schema)\n\n    if 'minProperties' in schm:\n        attrs['min_properties'] = schm.pop('minProperties')\n        if not isinstance(attrs['min_properties'], integer_types):\n            raise CompilationError('minProperties must be integer', schema)\n\n    if 'multipleOf' in schm:\n        attrs['multiple_of'] = schm.pop('multipleOf')\n        if not isinstance(attrs['multiple_of'], number_types):\n            raise CompilationError('multipleOf must be a number', schema)\n\n    if 'not' in schm:\n        attrs['not'] = schm.pop('not')\n        if not isinstance(attrs['not'], dict):\n            raise CompilationError('not must be an object', schema)\n        subpointer = pointer_join(pointer, 'not')\n        attrs['not'] = compile(attrs['not'], subpointer, context, scope)\n\n    if 'oneOf' in schm:\n        subpointer = pointer_join(pointer, 'oneOf')\n        attrs['one_of'] = schm.pop('oneOf')\n        if isinstance(attrs['one_of'], (list, tuple)):\n            # each value must be a json schema\n            attrs['one_of'] = [compile(element, subpointer, context, scope) for element in attrs['one_of']]  # noqa\n        else:\n            # should be a boolean\n            raise CompilationError('wrong type for {}'.format('oneOf'), schema)\n\n    if 'pattern' in schm:\n        attrs['pattern'] = schm.pop('pattern')\n        if not isinstance(attrs['pattern'], string_types):\n            raise CompilationError('pattern must be a string', schema)\n\n    if 'properties' in schm:\n        attrs['properties'] = schm.pop('properties')\n        if not isinstance(attrs['properties'], dict):\n            raise CompilationError('properties must be an object', schema)\n        for subname, subschema in attrs['properties'].items():\n            subpointer = pointer_join(pointer, subname)\n            compiled = compile(subschema, subpointer, context, scope)\n            attrs['properties'][subname] = compiled\n\n    if 'patternProperties' in schm:\n        attrs['pattern_properties'] = schm.pop('patternProperties')\n        if not isinstance(attrs['pattern_properties'], dict):\n            raise CompilationError('patternProperties must be an object', schema)  # noqa\n        for subname, subschema in attrs['pattern_properties'].items():\n            subpointer = pointer_join(pointer, 'patternProperties', subname)\n            compiled = compile(subschema, subpointer, context, scope)\n            attrs['pattern_properties'][subname] = compiled\n\n    if 'required' in schm:\n        attrs['required'] = schm.pop('required')\n        if not isinstance(attrs['required'], list):\n            raise CompilationError('required must be a list', schema)\n        if len(attrs['required']) < 1:\n            raise CompilationError('required cannot be empty', schema)\n\n    if 'type' in schm:\n        attrs['type'] = schm.pop('type')\n        if isinstance(attrs['type'], string_types):\n            attrs['type'] = [attrs['type']]\n        elif not isinstance(attrs['type'], sequence_types):\n            raise CompilationError('type must be string or sequence', schema)\n\n    if 'uniqueItems' in schm:\n        attrs['unique_items'] = schm.pop('uniqueItems')\n        if not isinstance(attrs['unique_items'], bool):\n            raise CompilationError('type must be boolean', schema)\n\n    return Draft04Validator(attrs, str(pointer), context.formats)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self, obj, pointer=None):\n\n        pointer = pointer or '#'\n\n        validator = deepcopy(self)\n        validator.errors = []\n        validator.fail_fast = False\n\n        obj = deepcopy(obj)\n        obj = validator.validate_enum(obj, pointer)\n        obj = validator.validate_type(obj, pointer)\n        obj = validator.validate_not(obj, pointer)\n        obj = validator.validate_all_of(obj, pointer)\n        obj = validator.validate_any_of(obj, pointer)\n        obj = validator.validate_one_of(obj, pointer)\n\n        if self.is_array(obj):\n            obj = validator.validate_items(obj, pointer)\n            obj = validator.validate_max_items(obj, pointer)\n            obj = validator.validate_min_items(obj, pointer)\n            obj = validator.validate_unique_items(obj, pointer)\n        elif self.is_number(obj):\n            obj = validator.validate_maximum(obj, pointer)\n            obj = validator.validate_minimum(obj, pointer)\n            obj = validator.validate_multiple_of(obj, pointer)\n        elif self.is_object(obj):\n            obj = validator.validate_required(obj, pointer)\n            obj = validator.validate_max_properties(obj, pointer)\n            obj = validator.validate_min_properties(obj, pointer)\n            obj = validator.validate_dependencies(obj, pointer)\n            obj = validator.validate_properties(obj, pointer)\n            obj = validator.validate_default_properties(obj, pointer)\n        elif self.is_string(obj):\n            obj = validator.validate_max_length(obj, pointer)\n            obj = validator.validate_min_length(obj, pointer)\n            obj = validator.validate_pattern(obj, pointer)\n            obj = validator.validate_format(obj, pointer)\n\n        if validator.errors:\n            raise ValidationError('multiple errors',\n                                  obj,\n                                  errors=validator.errors)\n\n        return obj", "response": "Validate object against the validator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the object is in the correct format.", "response": "def validate_format(self, obj, pointer=None):\n        \"\"\"\n        ================= ============\n        Expected draft04  Alias of\n        ----------------- ------------\n        date-time         rfc3339.datetime\n        email             email\n        hostname          hostname\n        ipv4              ipv4\n        ipv6              ipv6\n        uri               uri\n        ================= ============\n\n        \"\"\"\n        if 'format' in self.attrs:\n            substituted = {\n                'date-time': 'rfc3339.datetime',\n                'email': 'email',\n                'hostname': 'hostname',\n                'ipv4': 'ipv4',\n                'ipv6': 'ipv6',\n                'uri': 'uri',\n            }.get(self.attrs['format'], self.attrs['format'])\n            logger.debug('use %s', substituted)\n            try:\n                return self.formats[substituted](obj)\n            except ValidationError as error:\n                logger.error(error)\n                self.fail('Forbidden value', obj, pointer)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when validation fails.", "response": "def fail(self, reason, obj, pointer=None):\n        \"\"\"\n        Called when validation fails.\n        \"\"\"\n        pointer = pointer_join(pointer)\n        err = ValidationError(reason, obj, pointer)\n        if self.fail_fast:\n            raise err\n        else:\n            self.errors.append(err)\n        return err"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef siteblock(parser, token):\n    tokens = token.split_contents()\n    tokens_num = len(tokens)\n\n    if tokens_num not in (2, 4):\n        raise template.TemplateSyntaxError(\n            '%r tag requires two or four arguments. '\n            'E.g.: {%% siteblock \"myblock\" %%} or {%% siteblock \"myblock\" as myvar %%}.' % tokens[0])\n\n    block_alias = parser.compile_filter(tokens[1])\n    as_var = None\n    tokens = tokens[2:]\n    if len(tokens) >= 2 and tokens[-2] == 'as':\n        as_var = tokens[-1]\n\n    return siteblockNode(block_alias, as_var)", "response": "Returns a node that can be used to render a siteblock."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef blastn(sequences, locus, nseqs, kir=False,\n           verbose=False, refdata=None, evalue=10):\n    \"\"\"\n    Gets the a list of alleles that are the most similar to the input sequence\n\n    :param sequences: The input sequence record.\n    :type sequences: SeqRecord\n    :param locus: The gene locus associated with the sequence.\n    :type locus: ``str``\n    :param nseqs: The incomplete annotation from a previous iteration.\n    :type nseqs: ``int``\n    :param evalue: The evalue to use (default = 10)\n    :type evalue: ``int``\n    :param kir: Run with KIR or not\n    :type kir: ``bool``\n    :param verbose: Run in versboe\n    :type verbose: ``bool``\n    :param refdata: An object with reference data\n    :type refdata: :ref:`ref`\n    :rtype: :ref:`bl`\n\n    Example usage:\n\n        >>> from Bio.Seq import Seq\n        >>> from seqann.blast_cmd import blastn\n        >>> sequence = Seq('AGAGACTCTCCCGAGGATTTCGTGTACCAGTTTAAGGCCATGTGCTACTTCACC')\n        >>> blast = blastn(sequence, locus, nseqs)\n\n    \"\"\"\n    logger = logging.getLogger(\"Logger.\" + __name__)\n\n    if not refdata:\n        refdata = ReferenceData()\n\n    file_id = str(randomid())\n    input_fasta = file_id + \".fasta\"\n    output_xml = file_id + \".xml\"\n    SeqIO.write(sequences, input_fasta, \"fasta\")\n    blastn_cline = NcbiblastnCommandline(query=input_fasta,\n                                         db=refdata.blastdb,\n                                         evalue=evalue,\n                                         outfmt=5,\n                                         reward=1,\n                                         penalty=-3,\n                                         gapopen=5,\n                                         gapextend=2,\n                                         dust='yes',\n                                         out=output_xml)\n\n    stdout, stderr = blastn_cline()\n    loc = locus\n    if not kir:\n        loc = locus.split(\"-\")[1]\n\n    blast_qresult = SearchIO.read(output_xml, 'blast-xml')\n\n    #   Delete files\n    cleanup(file_id)\n\n    # TODO: Use logging\n    if len(blast_qresult.hits) == 0:\n        if verbose:\n            logger.error(\"Failed blast! No hits!\")\n            logger.error(stderr)\n        return Blast(failed=True)\n\n    alleles = []\n    full_sequences = []\n    load_blast = 70 if nseqs < 70 else nseqs\n    l = len(blast_qresult.hits) if load_blast > len(blast_qresult.hits) else load_blast\n\n    # TODO: update all blast files to have HLA-\n    if locus in refdata.hla_loci and not kir:\n        alleles = [blast_qresult[i].id.split(\"_\")[0] for i in range(0, l)\n                   if blast_qresult[i].id.split(\"*\")[0] == locus or \"HLA-\" + blast_qresult[i].id.split(\"*\")[0] == locus]\n        alleles = [\"HLA-\" + a if not has_hla(a) else a for a in alleles]\n    if kir:\n        alleles = [blast_qresult[i].id.split(\"_\")[0] for i in range(0, l)\n                   if blast_qresult[i].id.split(\"*\")[0] == locus]\n\n    if verbose:\n        logger.info(\"Blast alleles: \" + \",\".join(alleles))\n\n    # TODO: sort alleles by number of features they contain and evalue\n    # Use biosql db if provided\n    # otherwise use IMGT dat file\n    final_seqs = []\n    rmax = refdata.structure_max[locus]\n    if refdata.server_avail:\n        db = refdata.server[refdata.dbversion + \"_\" + loc]\n        full_sequences = []\n        for n in alleles:\n            if n in refdata.hla_names:\n                try:\n                    seq = db.lookup(name=n)\n                    full_sequences.append(seq)\n                except:\n                    logger.error(\"Allele doesnt exist in IMGT BioSQL DB!! \"\n                                 + n)\n    else:\n        if verbose:\n            logger.info(\"Getting sequences from HLA.dat file\")\n\n        full_sequences = [refdata.hlaref[a] for a in alleles\n                          if a in refdata.hlaref]\n\n        for s in full_sequences:\n            s.name = s.description.split(\",\")[0]\n\n    i = 1\n    last_seq = []\n    max_f = 0\n    added_max = False\n    full_feats = False\n    for s in full_sequences:\n\n        fs = len([f.type for f in s.features\n                  if not f.type in ['source', 'CDS']])\n        if i <= nseqs:\n            final_seqs.append(s)\n            max_f = fs if fs > max_f else max_f\n\n        if i <= nseqs and max_f < rmax:\n            full_feats = True\n\n        if(i >= nseqs and fs == max_f and not added_max):\n            if len(last_seq) >= 10:\n                last_seq.insert(3, s)\n            else:\n                last_seq.append(s)\n            added_max = True\n\n        if(fs > max_f and len(last_seq) < 10\n           and i >= nseqs and len(last_seq) < 10):\n            last_seq.append(s)\n\n        i += 1\n\n    if full_feats:\n        for s in last_seq:\n            final_seqs.append(s)\n\n    #   Build Blast object\n    blast_o = Blast(match_seqs=final_seqs, alleles=alleles)\n    return blast_o", "response": "Runs the BLASTN command and returns the list of alleles that are the most similar to the input sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_locus(sequences, kir=False, verbose=False, refdata=None, evalue=10):\n    if not refdata:\n        refdata = ReferenceData()\n\n    file_id = str(randomid())\n    input_fasta = file_id + \".fasta\"\n    output_xml = file_id + \".xml\"\n    SeqIO.write(sequences, input_fasta, \"fasta\")\n    blastn_cline = NcbiblastnCommandline(query=input_fasta,\n                                         db=refdata.blastdb,\n                                         evalue=evalue,\n                                         outfmt=5,\n                                         reward=1,\n                                         penalty=-3,\n                                         gapopen=5,\n                                         gapextend=2,\n                                         dust='yes',\n                                         out=output_xml)\n\n    stdout, stderr = blastn_cline()\n\n    blast_qresult = SearchIO.read(output_xml, 'blast-xml')\n\n    #   Delete files\n    cleanup(file_id)\n\n    if len(blast_qresult.hits) == 0:\n        return ''\n\n    loci = []\n    for i in range(0, 3):\n        if kir:\n            loci.append(blast_qresult[i].id.split(\"*\")[0])\n        else:\n            loci.append(blast_qresult[i].id.split(\"*\")[0])\n\n    locus = set(loci)\n    if len(locus) == 1:\n        if has_hla(loci[0]) or kir:\n            return loci[0]\n        else:\n            return \"HLA-\" + loci[0]\n    else:\n        return ''", "response": "Returns the locus of the sequence in the sequence"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef address(self) -> str:\n        '''generate an address from pubkey'''\n\n        return str(self._public_key.to_address(\n                   net_query(self.network))\n                   )", "response": "generate an address from pubkey"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sign_transaction(self, txins: Union[TxOut],\n                         tx: MutableTransaction) -> MutableTransaction:\n        '''sign the parent txn outputs P2PKH'''\n\n        solver = P2pkhSolver(self._private_key)\n        return tx.spend(txins, [solver for i in txins])", "response": "sign the parent txn outputs P2PKH"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking care of specifics of cryptoid naming system", "response": "def format_name(net: str) -> str:\n        '''take care of specifics of cryptoid naming system'''\n\n        if net.startswith('t') or 'testnet' in net:\n            net = net[1:] + '-test'\n        else:\n            net = net\n\n        return net"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_url(url: str) -> Union[dict, int, float, str]:\n        '''Perform a GET request for the url and return a dictionary parsed from\n        the JSON response.'''\n\n        request = Request(url, headers={\"User-Agent\": \"pypeerassets\"})\n        response = cast(HTTPResponse, urlopen(request))\n        if response.status != 200:\n            raise Exception(response.reason)\n        return json.loads(response.read().decode())", "response": "Perform a GET request for the url and return a dictionary parsed from\n            the JSON response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery block using <blockhash as key.", "response": "def getblock(self, blockhash: str) -> dict:\n        '''query block using <blockhash> as key.'''\n\n        query = 'block.raw.dws?coin={net}&hash={blockhash}'.format(\n            net=self.format_name(self.net),\n            blockhash=blockhash,\n        )\n        return cast(dict, self.get_url(self.explorer_url + query))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects UTXOs and total", "response": "def select_inputs(self, address: str, amount: int) -> dict:\n        '''select UTXOs'''\n\n        utxos = []\n        utxo_sum = Decimal(-0.01)  # starts from negative due to minimal fee\n        for tx in sorted(self.listunspent(address=address), key=itemgetter('confirmations')):\n\n                utxos.append(\n                    TxIn(txid=tx['tx_hash'],\n                         txout=tx['tx_ouput_n'],\n                         sequence=Sequence.max(),\n                         script_sig=ScriptSig.unhexlify(tx['script']))\n                         )\n\n                utxo_sum += Decimal(int(tx['value']) / 100000000)\n                if utxo_sum >= amount:\n                    return {'utxos': utxos, 'total': utxo_sum}\n\n        if utxo_sum < amount:\n            raise InsufficientFunds('Insufficient funds.')\n\n        raise Exception(\"undefined behavior :.(\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_variable_extends(extend_node):\n    if django.VERSION < (1, 4):\n        return extend_node.parent_name_expr  # Django 1.3\n    else:\n        # The FilterExpression.var can be either a string, or Variable object.\n        return not isinstance(extend_node.parent_name.var, six.string_types)", "response": "Check whether a variable extends a filter expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extend_blocks(extend_node, blocks, context):\n    try:\n        # This needs a fresh parent context, or it will detection recursion in Django 1.9+,\n        # and thus skip the base template, which is already loaded.\n        parent = extend_node.get_parent(_get_extend_context(context))\n    except TemplateSyntaxError:\n        if _is_variable_extends(extend_node):\n            # we don't support variable extensions unless they have a default.\n            return\n        else:\n            raise\n\n    # Search for new blocks\n    for parent_block in parent.nodelist.get_nodes_by_type(BlockNode):\n        if not parent_block.name in blocks:\n            blocks[parent_block.name] = parent_block\n        else:\n            # set this node as the super node (for {{ block.super }})\n            block = blocks[parent_block.name]\n            seen_supers = []\n            while hasattr(block.parent, 'nodelist') and block.parent not in seen_supers:\n                seen_supers.append(block.parent)\n                block = block.parent\n            block.parent = parent_block\n\n    # search for further ExtendsNodes in the extended template\n    # There is only one extend block in a template (Django checks for this).\n    parent_extends = parent.nodelist.get_nodes_by_type(ExtendsNode)\n    if parent_extends:\n        _extend_blocks(parent_extends[0], blocks, context)", "response": "Extends the dictionary blocks with new blocks in the parent node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extend_nodelist(extends_node, context, instance_types):\n    results = []\n\n    # Find all blocks in the complete inheritance chain\n    blocks = extends_node.blocks.copy()  # dict with all blocks in the current template\n    _extend_blocks(extends_node, blocks, context)\n\n    # Dive into all blocks of the page one by one\n    all_block_names = list(blocks.keys())\n    for block in list(blocks.values()):\n        results += _scan_nodes(block.nodelist, context, instance_types, block, ignore_blocks=all_block_names)\n\n    # Scan topmost template for nodes that exist outside of blocks\n    parent_template = _find_topmost_template(extends_node, context)\n    if not parent_template:\n        return []\n    else:\n        results += _scan_nodes(parent_template.nodelist, context, instance_types, ignore_blocks=all_block_names)\n        return results", "response": "Returns a list of results found in the parent template ( s ) that are in the current template."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _scan_nodes(nodelist, context, instance_types, current_block=None, ignore_blocks=None):\n    results = []\n    for node in nodelist:\n        # first check if this is the object instance to look for.\n        if isinstance(node, instance_types):\n            results.append(node)\n            # if it's a Constant Include Node ({% include \"template_name.html\" %})\n        # scan the child template\n        elif isinstance(node, IncludeNode):\n            # if there's an error in the to-be-included template, node.template becomes None\n            if node.template:\n                # This is required for Django 1.7 but works on older version too\n                # Check if it quacks like a template object, if not\n                # presume is a template path and get the object out of it\n                if not callable(getattr(node.template, 'render', None)):\n                    template = get_template(node.template.var)\n                else:\n                    template = node.template\n\n                if TemplateAdapter is not None and isinstance(template, TemplateAdapter):\n                    # Django 1.8: received a new object, take original template\n                    template = template.template\n\n                results += _scan_nodes(template.nodelist, context, instance_types, current_block)\n        # handle {% extends ... %} tags\n        elif isinstance(node, ExtendsNode):\n            results += _extend_nodelist(node, context, instance_types)\n        # in block nodes we have to scan for super blocks\n        elif isinstance(node, VariableNode) and current_block:\n            if node.filter_expression.token == 'block.super':\n                # Found a {{ block.super }} line\n                if not hasattr(current_block.parent, 'nodelist'):\n                    raise TemplateSyntaxError(\n                        \"Cannot read {{{{ block.super }}}} for {{% block {0} %}}, \"\n                        \"the parent template doesn't have this block.\".format(\n                        current_block.name\n                    ))\n                results += _scan_nodes(current_block.parent.nodelist, context, instance_types, current_block.parent)\n        # ignore nested blocks which are already handled\n        elif isinstance(node, BlockNode) and ignore_blocks and node.name in ignore_blocks:\n            continue\n        # if the node has the newly introduced 'child_nodelists' attribute, scan\n        # those attributes for nodelists and recurse them\n        elif hasattr(node, 'child_nodelists'):\n            for nodelist_name in node.child_nodelists:\n                if hasattr(node, nodelist_name):\n                    subnodelist = getattr(node, nodelist_name)\n                    if isinstance(subnodelist, NodeList):\n                        if isinstance(node, BlockNode):\n                            current_block = node\n                        results += _scan_nodes(subnodelist, context, instance_types, current_block)\n        # else just scan the node for nodelist instance attributes\n        else:\n            for attr in dir(node):\n                obj = getattr(node, attr)\n                if isinstance(obj, NodeList):\n                    if isinstance(node, BlockNode):\n                        current_block = node\n                    results += _scan_nodes(obj, context, instance_types, current_block)\n    return results", "response": "Scan through the nodelist and return a list of nodes that are found in the current scope level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_node_instances(nodelist, instances):\n    context = _get_main_context(nodelist)\n\n    # The Django 1.8 loader returns an adapter class; it wraps the original Template in a new object to be API compatible\n    if TemplateAdapter is not None and isinstance(nodelist, TemplateAdapter):\n        nodelist = nodelist.template\n\n    return _scan_nodes(nodelist, context, instances)", "response": "Returns a list of Node objects that inherit from a given list of instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_integer(v):\n    # type: (...) -> bool\n    \"\"\"Test whether a value is an integer (of any kind).\n\n    Examples:\n        >>> is_integer(1)\n        True\n        >>> is_integer(-0.123)\n        False\n        >>> is_integer(3.)\n        False\n        >>> is_integer(9223372036854775808)\n        True\n        >>> is_integer('1')\n        False\n        >>> is_integer(None)\n        False\n        >>> is_integer(numpy.int(3))\n        True\n    \"\"\"\n    try:\n        from builtins import int\n        return isinstance(v, int)  # Match both int and long on Py2\n    except ImportError:\n        from past.builtins import long\n        return isinstance(v, (int, long))", "response": "Test whether a value is an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_config_file():\n    # type: () -> AnyStr\n    \"\"\"Get model configuration file name from argv\"\"\"\n    parser = argparse.ArgumentParser(description=\"Read configuration file.\")\n    parser.add_argument('-ini', help=\"Full path of configuration file\")\n    args = parser.parse_args()\n    ini_file = args.ini\n    if not FileClass.is_file_exists(ini_file):\n        print(\"Usage: -ini <full path to the configuration file.>\")\n        exit(-1)\n    return ini_file", "response": "Get model configuration file name from argv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the input x is numerical or not.", "response": "def isnumerical(x):\n        # type: (...) -> bool\n        \"\"\"Check the input x is numerical or not.\n\n        Examples:\n            >>> MathClass.isnumerical('78')\n            True\n            >>> MathClass.isnumerical('1.e-5')\n            True\n            >>> MathClass.isnumerical(None)\n            False\n            >>> MathClass.isnumerical('a1.2')\n            False\n            >>> MathClass.isnumerical(['1.2'])\n            False\n            >>> MathClass.isnumerical(numpy.float64(1.2))\n            True\n\n        \"\"\"\n        try:\n            xx = float(x)\n        except TypeError:\n            return False\n        except ValueError:\n            return False\n        except Exception:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the Nash - Sutcliffe coefficient for a set of observation and simulated values.", "response": "def nashcoef(obsvalues,  # type: Union[numpy.ndarray, List[Union[float, int]]]\n                 simvalues,  # type: Union[numpy.ndarray, List[Union[float, int]]]\n                 log=False,  # type: bool\n                 expon=2  # type: Union[float, int, numpy.ScalarType]\n                 ):\n        # type: (...) -> float\n        \"\"\"Calculate Nash-Sutcliffe coefficient(NSE) proposed by Nash and Sutcliffe (1970)\n        and its variants.\n\n           The following description is referred by Krause et al. (2005) and Moriasi et al. (2007).\n\n             - The range of NSE lies between -inf and 1.0 (prefect fit).\n             - Since the differences between observed and simulated values\n               are calculated as squared values (expon=2), the larger values\n               in a time series are strongly overestimated whereas lower\n               values are neglected (Legates and McCabe, 1999). For the\n               quantification of runoff prediction, this leads to an overestimation\n               of the model performance during peak flows and an underestimation\n               during low flow conditions.\n             - Similar to R-square, NSE is not very sensitive to systematic\n               model over- or underestimation especially during low flow periods.\n             - To reduce the sensitivity of the original NSE to extreme values,\n               the NSE is often calculated with logarithmic values of obseravtion\n               and simulation values, which known as lnE. As a result, the\n               influence of the low flow values is increased in comparison to the\n               flood peaks resulting in an increase in sensitivity of lnE to\n               systematic model over- or underestimation.\n             - A more general form could be used for the same purpose as lnE, i.e.,\n               varying the exponent from 1 to N. With the increase of `expon`, the\n               sensitivity to high flows will increase and could be used when only\n               the high flows are of interest, e.g., for flood prediction.\n\n        Args:\n            obsvalues: observation values array\n            simvalues: simulation values array\n            log: Do logarithmic transformation or not, False by default\n            expon: The exponent range from 1 to N, 2 by default\n\n        Examples:\n            >>> obs = [2.92, 2.75, 2.01, 1.09, 2.87, 1.43, 1.96,\\\n                       4.00, 2.24, 29.28, 5.88, 0.86, 13.21]\n            >>> sim = [2.90, 2.87, 2.85, 2.83, 3.04, 2.81, 2.85,\\\n                       2.78, 2.76, 13.40, 2.70, 2.09, 1.62]\n            >>> MathClass.nashcoef(obs, sim)  # doctest: +ELLIPSIS\n            0.451803966838596...\n            >>> MathClass.nashcoef(obs, sim, log=True)  # doctest: +ELLIPSIS\n            0.2841143016830745...\n            >>> MathClass.nashcoef(obs, sim, expon=1)  # doctest: +ELLIPSIS\n            0.3959646306103376...\n            >>> MathClass.nashcoef(obs, sim, expon=3)  # doctest: +ELLIPSIS\n            0.6122272075952075...\n            >>> MathClass.nashcoef(obs, sim, expon=14)  # doctest: +ELLIPSIS\n            0...\n            >>> MathClass.nashcoef(obs, sim, expon=0)  # doctest: +ELLIPSIS\n            0...\n\n        Returns:\n            NSE, or raise exception\n        \"\"\"\n        if len(obsvalues) != len(simvalues):\n            raise ValueError(\"The size of observed and simulated values must be\"\n                             \" the same for NSE calculation!\")\n        if not isinstance(obsvalues, numpy.ndarray):\n            obsvalues = numpy.array(obsvalues)\n        if not isinstance(simvalues, numpy.ndarray):\n            simvalues = numpy.array(simvalues)\n        if log:  # Be care of zero values\n            obsvalues = numpy.where((obsvalues == 0.) | (simvalues == 0.), numpy.nan, obsvalues)\n            simvalues = numpy.where((obsvalues == 0.) | (simvalues == 0.), numpy.nan, simvalues)\n            obsvalues = numpy.log(obsvalues)\n            simvalues = numpy.log(simvalues)\n        if expon > len(obsvalues) or expon < 1:\n            return 0.\n        ave = numpy.nanmean(obsvalues)\n        a1 = numpy.nansum(numpy.abs(obsvalues - simvalues) ** expon)\n        a2 = numpy.nansum(numpy.abs(obsvalues - ave) ** expon)\n        if a2 == 0.:\n            return 1.\n        return 1. - a1 / a2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rsquare(obsvalues,  # type: Union[numpy.ndarray, List[Union[float, int]]]\n                simvalues  # type: Union[numpy.ndarray, List[Union[float, int]]]\n                ):\n        # type: (...) -> Union[float, numpy.ScalarType]\n        \"\"\"Calculate Coefficient of determination.\n\n        Same as the square of the Pearson correlation coefficient (r),\n        and, the same as the built-in Excel function RSQ().\n        Programmed according to equation (1) in\n        \n        Legates, D.R. and G.J. McCabe, 1999.  Evaluating the use of \"goodness of fit\" measures\n        in hydrologic and hydroclimatic model variation.  Water Resources Research 35:233-241.\n\n        Args:\n            obsvalues: observe values array\n            simvalues: simulate values array\n\n        Examples:\n            >>> obs = [2.92, 2.75, 2.01, 1.09, 2.87, 1.43, 1.96,\\\n                       4.00, 2.24, 29.28, 5.88, 0.86, 13.21]\n            >>> sim = [2.90, 2.87, 2.85, 2.83, 3.04, 2.81, 2.85,\\\n                       2.78, 2.76, 13.40, 2.70, 2.09, 1.62]\n            >>> MathClass.rsquare(obs, sim)  # doctest: +ELLIPSIS\n            0.7528851650345053...\n\n        Returns:\n            R-square value, or raise exception\n        \"\"\"\n        if len(obsvalues) != len(simvalues):\n            raise ValueError(\"The size of observed and simulated values must be \"\n                             \"the same for R-square calculation!\")\n        if not isinstance(obsvalues, numpy.ndarray):\n            obsvalues = numpy.array(obsvalues)\n        if not isinstance(simvalues, numpy.ndarray):\n            simvalues = numpy.array(simvalues)\n        obs_avg = numpy.mean(obsvalues)\n        pred_avg = numpy.mean(simvalues)\n        obs_minus_avg_sq = numpy.sum((obsvalues - obs_avg) ** 2)\n        pred_minus_avg_sq = numpy.sum((simvalues - pred_avg) ** 2)\n        obs_pred_minus_avgs = numpy.sum((obsvalues - obs_avg) * (simvalues - pred_avg))\n        # Calculate R-square\n        yy = obs_minus_avg_sq ** 0.5 * pred_minus_avg_sq ** 0.5\n        if MathClass.floatequal(yy, 0.):\n            return 1.\n        return (obs_pred_minus_avgs / yy) ** 2.", "response": "Calculates the R - square of determination of the given observed and simulated values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rmse(obsvalues,  # type: Union[numpy.ndarray, List[Union[float, int]]]\n             simvalues  # type: Union[numpy.ndarray, List[Union[float, int]]]\n             ):\n        # type: (...) -> Union[float, numpy.ScalarType]\n        \"\"\"Calculate RMSE.\n\n        Args:\n            obsvalues: observe values array\n            simvalues: simulate values array\n\n        Examples:\n            >>> obs = [2.92, 2.75, 2.01, 1.09, 2.87, 1.43, 1.96,\\\n                       4.00, 2.24, 29.28, 5.88, 0.86, 13.21]\n            >>> sim = [2.90, 2.87, 2.85, 2.83, 3.04, 2.81, 2.85,\\\n                       2.78, 2.76, 13.40, 2.70, 2.09, 1.62]\n            >>> MathClass.rmse(obs, sim)  # doctest: +ELLIPSIS\n            5.590926715533082...\n\n        Returns:\n            RMSE value\n        \"\"\"\n        if len(obsvalues) != len(simvalues):\n            raise ValueError(\"The size of observed and simulated values must be \"\n                             \"the same for R-square calculation!\")\n        if not isinstance(obsvalues, numpy.ndarray):\n            obsvalues = numpy.array(obsvalues)\n        if not isinstance(simvalues, numpy.ndarray):\n            simvalues = numpy.array(simvalues)\n        return numpy.sqrt(numpy.mean((obsvalues - simvalues) ** 2.))", "response": "Calculates the R - square error of the observed and simulated values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pbias(obsvalues,  # type: Union[numpy.ndarray, List[Union[float, int]]]\n              simvalues  # type: Union[numpy.ndarray, List[Union[float, int]]]\n              ):\n        # type: (...) -> Union[float, numpy.ScalarType]\n        \"\"\"Calculate PBIAS, or percent model bias.\n\n        Args:\n            obsvalues: observe values array\n            simvalues: simulate values array\n\n        Examples:\n            >>> obs = [2.92, 2.75, 2.01, 1.09, 2.87, 1.43, 1.96,\\\n                       4.00, 2.24, 29.28, 5.88, 0.86, 13.21]\n            >>> sim = [2.90, 2.87, 2.85, 2.83, 3.04, 2.81, 2.85,\\\n                       2.78, 2.76, 13.40, 2.70, 2.09, 1.62]\n            >>> MathClass.pbias(obs, sim)  # doctest: +ELLIPSIS\n            35.46099290780142...\n\n        Returns:\n            PBIAS value (percentage), or raise exception\n        \"\"\"\n        if len(obsvalues) != len(simvalues):\n            raise ValueError(\"The size of observed and simulated values must be\"\n                             \" the same for PBIAS calculation!\")\n        return sum(map(lambda x, y: (x - y) * 100, obsvalues, simvalues)) / sum(obsvalues)", "response": "Calculates the PBIAS value of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the RSR value of an object based on the observed and simulated values.", "response": "def rsr(obsvalues,  # type: Union[numpy.ndarray, List[Union[float, int]]]\n            simvalues  # type: Union[numpy.ndarray, List[Union[float, int]]]\n            ):\n        # type: (...) -> Union[float, numpy.ScalarType]\n        \"\"\"Calculate RSR (RMSE-to-SD Ratio).\n\n        Programmed according to equation (3) in\n        Moriasi et al. 2007.  Model evalutaion guidelines for systematic quantification of accuracy \n        in watershed simulations. Transactions of the ASABE 50(3): 885-900.\n\n        Args:\n            obsvalues: observe values array\n            simvalues: simulate values array\n\n        Examples:\n            >>> obs = [2.92, 2.75, 2.01, 1.09, 2.87, 1.43, 1.96,\\\n                       4.00, 2.24, 29.28, 5.88, 0.86, 13.21]\n            >>> sim = [2.90, 2.87, 2.85, 2.83, 3.04, 2.81, 2.85,\\\n                       2.78, 2.76, 13.40, 2.70, 2.09, 1.62]\n            >>> MathClass.rsr(obs, sim)  # doctest: +ELLIPSIS\n            0.7404026155824978...\n\n        Returns:\n            RSR value, or raise exception\n        \"\"\"\n        if len(obsvalues) != len(simvalues):\n            raise ValueError(\"The size of observed and simulated values must be\"\n                             \" the same for RSR calculation!\")\n        mean_obs = sum(obsvalues) / len(obsvalues)\n        return sqrt(sum(map(lambda x, y: (x - y) ** 2, obsvalues, simvalues))) / \\\n               sqrt(sum(map(lambda x, y: (x - y) ** 2, obsvalues, [mean_obs] * len(obsvalues))))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_str2num(unicode_str  # type: Union[AnyStr, int, float, List[Union[AnyStr, float, int]], Tuple[Union[AnyStr, float, int]]]\n                        ):\n        # type: (...) -> Union[AnyStr, int, float, List[Union[AnyStr, float, int]], Tuple[Union[AnyStr, float, int]]]\n        \"\"\"Convert string to string, integer, or float. Support tuple or list.\n\n        Examples:\n            >>> StringClass.convert_str2num('1.23')\n            1.23\n            >>> StringClass.convert_str2num(u'1.23')\n            1.23\n            >>> StringClass.convert_str2num(u'21.')\n            21\n            >>> StringClass.convert_str2num('abc123')\n            'abc123'\n            >>> StringClass.convert_str2num((123, u'2.3', 3., 'abc', u'edf'))\n            (123, 2.3, 3, 'abc', 'edf')\n            >>> StringClass.convert_str2num([123, u'2.3', 3., 'abc', u'edf'])\n            [123, 2.3, 3, 'abc', 'edf']\n        \"\"\"\n        if MathClass.isnumerical(unicode_str):\n            unicode_str = float(unicode_str)\n            if unicode_str % 1. == 0.:\n                unicode_str = int(unicode_str)\n            return unicode_str\n        elif is_string(unicode_str):\n            return str(unicode_str)\n        elif isinstance(unicode_str, tuple):\n            return tuple(StringClass.convert_str2num(v) for v in unicode_str)\n        elif isinstance(unicode_str, list):\n            return list(StringClass.convert_str2num(v) for v in unicode_str)\n        else:\n            return unicode_str", "response": "Convert string to integer or float."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit string by split character space and indent", "response": "def split_string(str_src, spliters=None, elim_empty=False):\n        # type: (AnyStr, Union[AnyStr, List[AnyStr], None], bool) -> List[AnyStr]\n        \"\"\"Split string by split character space(' ') and indent('\\t') as default\n\n        Examples:\n            >>> StringClass.split_string('exec -ini test.ini', ' ')\n            ['exec', '-ini', 'test.ini']\n\n        Args:\n            str_src: source string\n            spliters: e.g. [' ', '\\t'], [], ' ', None\n            elim_empty: Eliminate empty (i.e., '') or not.\n\n        Returns:\n            split sub-strings as list\n        \"\"\"\n        if is_string(spliters):\n            spliters = [spliters]\n        if spliters is None or not spliters:\n            spliters = [' ', '\\t']\n        dest_strs = list()\n        src_strs = [str_src]\n        while True:\n            old_dest_strs = src_strs[:]\n            for s in spliters:\n                for src_s in src_strs:\n                    temp_strs = src_s.split(s)\n                    for temp_s in temp_strs:\n                        temp_s = temp_s.strip()\n                        if temp_s == '' and elim_empty:\n                            continue\n                        if is_string(temp_s):\n                            temp_s = str(temp_s)\n                        dest_strs.append(temp_s)\n                src_strs = dest_strs[:]\n                dest_strs = list()\n            if old_dest_strs == src_strs:\n                dest_strs = src_strs[:]\n                break\n        return dest_strs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef string_in_list(tmp_str, strlist):\n        # type: (AnyStr, List[AnyStr]) -> bool\n        \"\"\"Is tmp_str in strlist, case insensitive.\"\"\"\n        new_str_list = strlist[:]\n        for i, str_in_list in enumerate(new_str_list):\n            new_str_list[i] = str_in_list.lower()\n        return tmp_str.lower() in new_str_list", "response": "Is tmp_str in strlist case insensitive?"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind numeric values from string, e.g., 1, .7, 1.2, 4e2, 3e-3, -9, etc. Reference: `how-to-extract-a-floating-number-from-a-string-in-python`_ Examples: >>> input_str = '.1 .12 9.1 98.1 1. 12. 1 12' >>> StringClass.extract_numeric_values_from_string(input_str) [0.1, 0.12, 9.1, 98.1, 1, 12, 1, 12] >>> input_str = '-1 +1 2e9 +2E+09 -2e-9' >>> StringClass.extract_numeric_values_from_string(input_str) [-1, 1, 2000000000, 2000000000, -2e-09] >>> input_str = 'current level: -2.03e+2db' >>> StringClass.extract_numeric_values_from_string(input_str) [-203] Args: str_contains_values: string which may contains numeric values Returns: list of numeric values .. _how-to-extract-a-floating-number-from-a-string-in-python: https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string-in-python/4703508#4703508", "response": "def extract_numeric_values_from_string(str_contains_values):\n        # type: (AnyStr) -> Optional[List[Union[int, float]]]\n        \"\"\"\n        Find numeric values from string, e.g., 1, .7, 1.2, 4e2, 3e-3, -9, etc.\n        \n        Reference: `how-to-extract-a-floating-number-from-a-string-in-python`_\n\n        Examples:\n            >>> input_str = '.1 .12 9.1 98.1 1. 12. 1 12'\n            >>> StringClass.extract_numeric_values_from_string(input_str)\n            [0.1, 0.12, 9.1, 98.1, 1, 12, 1, 12]\n            >>> input_str = '-1 +1 2e9 +2E+09 -2e-9'\n            >>> StringClass.extract_numeric_values_from_string(input_str)\n            [-1, 1, 2000000000, 2000000000, -2e-09]\n            >>> input_str = 'current level: -2.03e+2db'\n            >>> StringClass.extract_numeric_values_from_string(input_str)\n            [-203]\n\n        Args:\n            str_contains_values: string which may contains numeric values\n\n        Returns:\n            list of numeric values\n\n        .. _how-to-extract-a-floating-number-from-a-string-in-python:\n            https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string-in-python/4703508#4703508\n        \"\"\"\n        numeric_const_pattern = r'[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?'\n        rx = re.compile(numeric_const_pattern, re.VERBOSE)\n        value_strs = rx.findall(str_contains_values)\n        if len(value_strs) == 0:\n            return None\n        else:\n            return [int(float(v)) if float(v) % 1. == 0 else float(v) for v in value_strs]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_datetime(formatted_str, user_fmt=None):\n        # type: (AnyStr, Optional[AnyStr]) -> datetime\n        \"\"\"get datetime() object from string formatted %Y-%m-%d %H:%M:%S\n\n        Examples:\n            >>> StringClass.get_datetime('2008-11-9')\n            datetime.datetime(2008, 11, 9, 0, 0)\n            >>> StringClass.get_datetime('2008/11/9')\n            datetime.datetime(2008, 11, 9, 0, 0)\n            >>> StringClass.get_datetime('20081109')\n            datetime.datetime(2008, 11, 9, 0, 0)\n            >>> StringClass.get_datetime('11/9/2008')\n            datetime.datetime(2008, 11, 9, 0, 0)\n            >>> StringClass.get_datetime('11-9-2008')\n            datetime.datetime(2008, 11, 9, 0, 0)\n            >>> StringClass.get_datetime('11/09/08')\n            datetime.datetime(2008, 11, 9, 0, 0)\n            >>> StringClass.get_datetime('2008-11-9 11:09')\n            datetime.datetime(2008, 11, 9, 11, 9)\n            >>> StringClass.get_datetime('2008-11-9 11:09:52')\n            datetime.datetime(2008, 11, 9, 11, 9, 52)\n        \"\"\"\n        date_fmts = ['%m-%d-%Y', '%Y-%m-%d', '%m-%d-%y', '%y-%m-%d']\n        date_fmts += [d.replace('-', '/') for d in date_fmts]\n        date_fmts += [d.replace('-', '') for d in date_fmts]\n        time_fmts = ['%H:%M', '%H:%M:%S']\n        fmts = date_fmts + ['%s %s' % (d, t) for d in date_fmts for t in time_fmts]\n        if user_fmt is not None:\n            if is_string(user_fmt):\n                fmts.insert(0, str(user_fmt))\n            elif isinstance(user_fmt, list):\n                fmts = user_fmt + fmts\n            elif isinstance(user_fmt, tuple):\n                for fff in user_fmt:\n                    fmts.insert(0, fff)\n        flag = False\n        for fmt in fmts:\n            try:\n                org_time = time.strptime(formatted_str, fmt)\n                flag = True\n                break\n            except ValueError:\n                pass\n        if not flag:\n            raise ValueError('The DATETIME must be one of the formats: %s' % ','.join(fmts))\n        else:\n            return datetime(org_time.tm_year, org_time.tm_mon, org_time.tm_mday,\n                            org_time.tm_hour, org_time.tm_min, org_time.tm_sec)", "response": "get datetime object from string formatted %Y - m - d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_file_exists(filename):\n        # type: (AnyStr) -> bool\n        \"\"\"Check the existence of file path.\"\"\"\n        if filename is None or not os.path.exists(filename) or not os.path.isfile(filename):\n            return False\n        else:\n            return True", "response": "Check the existence of file path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the existence of folder path.", "response": "def is_dir_exists(dirpath):\n        # type: (AnyStr) -> bool\n        \"\"\"Check the existence of folder path.\"\"\"\n        if dirpath is None or not os.path.exists(dirpath) or not os.path.isdir(dirpath):\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying files with the same name and different suffixes such as ESRI Shapefile.", "response": "def copy_files(filename, dstfilename):\n        # type: (AnyStr, AnyStr) -> None\n        \"\"\"Copy files with the same name and different suffixes, such as ESRI Shapefile.\"\"\"\n        FileClass.remove_files(dstfilename)\n        dst_prefix = os.path.splitext(dstfilename)[0]\n        pattern = os.path.splitext(filename)[0] + '.*'\n        for f in glob.iglob(pattern):\n            ext = os.path.splitext(f)[1]\n            dst = dst_prefix + ext\n            copy(f, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_files(filename):\n        # type: (AnyStr) -> None\n        \"\"\"\n        Delete all files with same root as fileName,\n        i.e. regardless of suffix, such as ESRI shapefile\n        \"\"\"\n        pattern = os.path.splitext(filename)[0] + '.*'\n        for f in glob.iglob(pattern):\n            os.remove(f)", "response": "Delete all files with same root as fileName."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_up_to_date(outfile, basedatetime):\n        # type: (AnyStr, datetime) -> bool\n        \"\"\"Return true if outfile exists and is no older than base datetime.\"\"\"\n        if os.path.exists(outfile):\n            if os.path.getmtime(outfile) >= basedatetime:\n                return True\n        return False", "response": "Return true if outfile exists and is no older than base datetime."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_executable_fullpath(name, dirname=None):\n        # type: (AnyStr, Optional[AnyStr]) -> Optional[AnyStr]\n        \"\"\"get the full path of a given executable name\"\"\"\n        if name is None:\n            return None\n        if is_string(name):\n            name = str(name)\n        else:\n            raise RuntimeError('The input function name or path must be string!')\n        if dirname is not None:  # check the given path first\n            dirname = os.path.abspath(dirname)\n            fpth = dirname + os.sep + name\n            if os.path.isfile(fpth):\n                return fpth\n        # If dirname is not specified, check the env then.\n        if sysstr == 'Windows':\n            findout = UtilClass.run_command('where %s' % name)\n        else:\n            findout = UtilClass.run_command('which %s' % name)\n        if not findout or len(findout) == 0:\n            print(\"%s is not included in the env path\" % name)\n            exit(-1)\n        first_path = findout[0].split('\\n')[0]\n        if os.path.exists(first_path):\n            return first_path\n        return None", "response": "get the full path of a given executable name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn full path if available.", "response": "def get_file_fullpath(name, dirname=None):\n        # type: (AnyStr, Optional[AnyStr]) -> Optional[AnyStr]\n        \"\"\"Return full path if available.\"\"\"\n        if name is None:\n            return None\n        if is_string(name):\n            name = str(name)\n        else:\n            raise RuntimeError('The input function name or path must be string!')\n        for sep in ['\\\\', '/', os.sep]:  # Loop all possible separators\n            if sep in name:  # name is full path already\n                name = os.path.abspath(name)\n                return name\n        if dirname is not None:\n            dirname = os.path.abspath(dirname)\n            name = dirname + os.sep + name\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_filename_by_suffixes(dir_src, suffixes):\n        # type: (AnyStr, Union[AnyStr, List[AnyStr]]) -> Optional[List[AnyStr]]\n        \"\"\"get file names with the given suffixes in the given directory\n\n        Args:\n            dir_src: directory path\n            suffixes: wanted suffixes list, the suffix in suffixes can with or without '.'\n\n        Returns:\n            file names with the given suffixes as list\n        \"\"\"\n        list_files = os.listdir(dir_src)\n        re_files = list()\n        if is_string(suffixes):\n            suffixes = [suffixes]\n        if not isinstance(suffixes, list):\n            return None\n        for i, suf in enumerate(suffixes):\n            if len(suf) >= 1 and suf[0] != '.':\n                suffixes[i] = '.' + suf\n        for f in list_files:\n            name, ext = os.path.splitext(f)\n            if StringClass.string_in_list(ext, suffixes):\n                re_files.append(f)\n        return re_files", "response": "get file names with the given suffixes in the given directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_full_filename_by_suffixes(dir_src, suffixes):\n        # type: (AnyStr, Union[AnyStr, List[AnyStr]]) -> Optional[List[AnyStr]]\n        \"\"\"get full file names with the given suffixes in the given directory\n\n        Args:\n            dir_src: directory path\n            suffixes: wanted suffixes\n\n        Returns:\n            full file names with the given suffixes as list\n        \"\"\"\n        file_names = FileClass.get_filename_by_suffixes(dir_src, suffixes)\n        if file_names is None:\n            return None\n        return list(dir_src + os.sep + name for name in file_names)", "response": "get full file names with the given suffixes in the given directory path\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_core_name_without_suffix(file_path):\n        # type: (AnyStr) -> AnyStr\n        \"\"\"Return core file name without suffix.\n\n        Examples:\n            >>> FileClass.get_core_name_without_suffix(r'/home/zhulj/1990.01.30/test.01.tif')\n            'test.01'\n            >>> FileClass.get_core_name_without_suffix(r'C:\\zhulj\\igsnrr\\lreis.txt')\n            'lreis'\n            >>> FileClass.get_core_name_without_suffix(r'C:\\\\zhulj\\\\igsnrr\\\\lreis.txt')\n            'lreis'\n            >>> FileClass.get_core_name_without_suffix(r'C:/zhulj/igsnrr/lreis.txt')\n            'lreis'\n            >>> FileClass.get_core_name_without_suffix(r'/home/zhulj/dta/taudem/aread8')\n            'aread8'\n            >>> FileClass.get_core_name_without_suffix('singlename')\n            'singlename'\n            >>> FileClass.get_core_name_without_suffix('singlename.txt')\n            'singlename'\n        \"\"\"\n        if '\\\\' in file_path:\n            file_path = file_path.replace('\\\\', '/')\n        file_name = os.path.basename(file_path)\n        core_names = file_name.split('.')\n        if len(core_names) > 1:\n            core_names = core_names[:-1]\n        if isinstance(core_names, list):\n            return str('.'.join(core_names))\n        else:\n            return str(core_names)", "response": "Return the core file name without suffix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_postfix(file_path, postfix):\n        # type: (AnyStr, AnyStr) -> AnyStr\n        \"\"\"Add postfix for a full file path.\n\n        Examples:\n            >>> FileClass.add_postfix('/home/zhulj/dem.tif', 'filled')\n            '/home/zhulj/dem_filled.tif'\n            >>> FileClass.add_postfix('dem.tif', 'filled')\n            'dem_filled.tif'\n            >>> FileClass.add_postfix('dem', 'filled')\n            'dem_filled'\n        \"\"\"\n        cur_sep = ''\n        for sep in ['\\\\', '/', os.sep]:\n            if sep in file_path:\n                cur_sep = sep\n                break\n        corename = FileClass.get_core_name_without_suffix(file_path)\n        tmpspliter = os.path.basename(file_path).split('.')\n        suffix = ''\n        if len(tmpspliter) > 1:\n            suffix = tmpspliter[-1]\n        newname = os.path.dirname(file_path) + cur_sep + corename + '_' + postfix\n        if suffix != '':\n            newname += '.' + suffix\n        return str(newname)", "response": "Add a postfix for a full file path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef day_of_year(dt):\n        # type: (int) -> int\n        \"\"\"Day index of year from 1 to 365 or 366\"\"\"\n        sec = time.mktime(dt.timetuple())\n        t = time.localtime(sec)\n        return t.tm_yday", "response": "Return the day of the year from 1 to 365 or 366"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_command(commands):\n        # type: (Union[AnyStr, List[AnyStr]]) -> List[AnyStr]\n        \"\"\"Execute external command, and return the output lines list. In windows, refers to\n        `handling-subprocess-crash-in-windows`_.\n\n        Args:\n            commands: string or list\n\n        Returns:\n            output lines\n\n        .. _handling-subprocess-crash-in-windows:\n            https://stackoverflow.com/questions/5069224/handling-subprocess-crash-in-windows\n        \"\"\"\n        # commands = StringClass.convert_unicode2str(commands)\n        # print(commands)\n\n        use_shell = False\n        subprocess_flags = 0\n        startupinfo = None\n        if sysstr == 'Windows':\n            if isinstance(commands, list):\n                commands = ' '.join(str(c) for c in commands)\n            import ctypes\n            SEM_NOGPFAULTERRORBOX = 0x0002  # From MSDN\n            ctypes.windll.kernel32.SetErrorMode(SEM_NOGPFAULTERRORBOX)\n            subprocess_flags = 0x8000000  # win32con.CREATE_NO_WINDOW?\n            # this startupinfo structure prevents a console window from popping up on Windows\n            startupinfo = subprocess.STARTUPINFO()\n            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n            # not sure if node outputs on stderr or stdout so capture both\n        else:  # for Linux/Unix OS, commands is better to be a list.\n            if is_string(commands):\n                use_shell = True\n                # https://docs.python.org/2/library/subprocess.html\n                #     Using shell=True can be a security hazard.\n            elif isinstance(commands, list):\n                # the executable path may be enclosed with quotes, if not windows, delete the quotes\n                if commands[0][0] == commands[0][-1] == '\"' or \\\n                        commands[0][0] == commands[0][-1] == \"'\":\n                    commands[0] = commands[0][1:-1]\n                for idx, v in enumerate(commands):\n                    if isinstance(v, int) or isinstance(v, float):\n                        # Fix :TypeError: execv() arg 2 must contain only strings\n                        commands[idx] = repr(v)\n        print(commands)\n        process = subprocess.Popen(commands, shell=use_shell, stdout=subprocess.PIPE,\n                                   stdin=open(os.devnull),\n                                   stderr=subprocess.STDOUT, universal_newlines=True,\n                                   startupinfo=startupinfo,\n                                   creationflags=subprocess_flags)\n        out, err = process.communicate()\n        recode = process.returncode\n\n        if out is None:\n            return ['']\n        if recode is not None and recode != 0:\n            raise subprocess.CalledProcessError(-1, commands,\n                                                \"ERROR occurred when running subprocess!\")\n        if '\\n' in out:\n            return out.split('\\n')\n\n        return [out]", "response": "Execute external command and return the output lines list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef current_path(local_function):\n        from inspect import getsourcefile\n        fpath = getsourcefile(local_function)\n        if fpath is None:\n            return None\n        return os.path.dirname(os.path.abspath(fpath))", "response": "Get current path refers to how - do - i - get - the - path - of - the - current - executed - file - in - python _\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mkdir(dir_path):\n        # type: (AnyStr) -> None\n        \"\"\"Make directory if not existed\"\"\"\n        if not os.path.isdir(dir_path) or not os.path.exists(dir_path):\n            os.makedirs(dir_path)", "response": "Make directory if not existed"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves and make the directory if it exists.", "response": "def rmmkdir(dir_path):\n        # type: (AnyStr) -> None\n        \"\"\"If directory existed, then remove and make; else make it.\"\"\"\n        if not os.path.isdir(dir_path) or not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        else:\n            rmtree(dir_path, True)\n            os.makedirs(dir_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconcatenating message list as single string with line feed.", "response": "def print_msg(contentlist):\n        # type: (Union[AnyStr, List[AnyStr], Tuple[AnyStr]]) -> AnyStr\n        \"\"\"concatenate message list as single string with line feed.\"\"\"\n        if isinstance(contentlist, list) or isinstance(contentlist, tuple):\n            return '\\n'.join(contentlist)\n        else:  # strings\n            if len(contentlist) > 1 and contentlist[-1] != '\\n':\n                contentlist += '\\n'\n            return contentlist"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode_strs_in_dict(unicode_dict  # type: Dict[Union[AnyStr, int], Union[int, float, AnyStr, List[Union[int, float, AnyStr]]]]\n                            ):\n        # type: (...) -> Dict[Union[AnyStr, int], Any]\n        \"\"\"Decode strings in dictionary which may contains unicode strings or numeric values.\n\n        - 1. integer could be key, float cannot;\n        - 2. the function is called recursively\n\n        Examples:\n\n            .. code-block:: python\n\n               input = {u'name': u'zhulj', u'age': u'28', u'1': ['1', 2, 3]}\n               output = {'name': 'zhulj', 'age': 28, 1: [1, 2, 3]}\n               input = {u'name': u'zhulj', 'edu': {'nwsuaf': 2007, u'bnu': '2011', 'igsnrr': 2014}}\n               output = {'name': 'zhulj', 'edu': {'nwsuaf': 2007, 'bnu': 2011, 'igsnrr': 2014}}\n\n        \"\"\"\n        unicode_dict = {StringClass.convert_str2num(k): StringClass.convert_str2num(v) for\n                        k, v in iteritems(unicode_dict)}\n        for k, v in iteritems(unicode_dict):\n            if isinstance(v, dict):\n                unicode_dict[k] = UtilClass.decode_strs_in_dict(v)\n        return unicode_dict", "response": "Decode strings in dictionary which may contain unicode strings or numeric values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pixels_connectivity_compute(raster, i, j, idx):\r\n    nrows, ncols = raster.shape\r\n    value = raster[i][j]\r\n    for di in [-1, 0, 1]:\r\n        for dj in [-1, 0, 1]:\r\n            if 0 <= i + di < nrows and 0 <= j + dj < ncols:\r\n                if raster[i + di][j + dj] == value and not (di == dj and di == 0):\r\n                    if [i + di, j + dj] not in idx:\r\n                        idx.append([i + di, j + dj])\r\n                        pixels_connectivity_compute(raster, i + di, j + dj, idx)", "response": "Compute if the two given values of pixels of raster have connectivity between the i. j pixel and its 8 - neighborhood."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the existed ID pixels who s in nparray idx_array.", "response": "def remove_existID_pixels(givenvalue_pixels_positions, idx_array):\r\n    \"\"\"Delete the existed ID pixels who's in nparray idx_array\r\n\r\n    After computing pixels connectivity, we get a set of pixels which have\r\n    connectivity and have the same ID. We get these pixels positions set as\r\n    nparray idx_array as well. Then we need to remove these pixels from all\r\n    of the given value's pixels and compute the connectivity of rest given\r\n    value's pixels.\r\n\r\n    Args:\r\n        givenvalue_pixels_positions: given value's pixels positions set from\r\n        last time's computing connectivity.\r\n        idx_array: pixels position set which have the same ID(means same that\r\n        these pixels have connectivity)\r\n\r\n    Return:\r\n        givenvalue_pixels_positions: given value's pixels positions set\r\n        after deleting the existed ID pixels.\r\n    \"\"\"\r\n    # We need to give List existID_pixels a 0 value to avoid the error:\r\n    # cannot compute fingerprint of empty list\r\n    existID_pixels = [0]\r\n    for i in range(givenvalue_pixels_positions.shape[0]):\r\n        for j in range(idx_array.shape[0]):\r\n            if ((givenvalue_pixels_positions[i, 0] == idx_array[j, 0]) and\r\n                    (givenvalue_pixels_positions[i, 1] == idx_array[j, 1])):\r\n                existID_pixels.append(i)\r\n    givenvalue_pixels_positions = np.delete(givenvalue_pixels_positions,\r\n                                            existID_pixels, 0)\r\n    return givenvalue_pixels_positions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_ID(ID, idx_array, drawID_raster):\r\n    for i in range(idx_array.shape[0]):\r\n        x = idx_array[i, 0]\r\n        y = idx_array[i, 1]\r\n        drawID_raster[x, y] = ID\r\n    return drawID_raster", "response": "Draw every pixel s ID on the undrawed rasterfile after computing all given value s pixels connectivity"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vis(bs):\n    bs = bytearray(bs)\n    symbols_in_one_line = 8\n    n = len(bs) // symbols_in_one_line\n    i = 0\n    for i in range(n):\n        print(str(i*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[i*symbols_in_one_line:(i+1)*symbols_in_one_line]])) # for every 8 symbols line\n    if not len(bs) % symbols_in_one_line == 0:\n        print(str((i+1)*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[(i+1)*symbols_in_one_line:]])+\"\\n\")", "response": "Function to visualize byte streams. Split into bytes print to console."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_bytes(data):\n    bs = bytearray(data)\n    symbols_in_one_line = 8\n    n = len(bs) // symbols_in_one_line\n    i = 0\n    for i in range(n):\n        print(str(i*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[i*symbols_in_one_line:(i+1)*symbols_in_one_line]])) # for every 8 symbols line\n    if not len(bs) % symbols_in_one_line == 0:\n        print(str((i+1)*symbols_in_one_line)+\" | \"+\" \".join([\"%02X\" % b for b in bs[(i+1)*symbols_in_one_line:]])+\"\\n\")", "response": "Function to visualize byte streams. Split into bytes print to console."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the parameters of the class.", "response": "def set_params(self, data):\n        \"\"\" resPQ#05162463 nonce:int128 server_nonce:int128 pq:string server_public_key_fingerprints:Vector long = ResPQ \"\"\"\n        bytes_io = BytesIO(data)\n\n        assert struct.unpack('<I', bytes_io.read(4))[0] == resPQ.constructor\n\n        self.nonce = bytes_io.read(16)\n        self.server_nonce = bytes_io.read(16)\n\n        self.pq = deserialize_string(bytes_io)\n\n        assert struct.unpack('<I', bytes_io.read(4))[0] == 0x1cb5c415  # long vector\n        count = struct.unpack('<l', bytes_io.read(4))[0]\n        for _ in range(count):\n            self.server_public_key_fingerprints.append(struct.unpack('<q', bytes_io.read(8))[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bytes(self):\n\n        pq_io = BytesIO()\n        serialize_string(pq_io, self.p)\n        serialize_string(pq_io, self.q)\n\n        ret = struct.pack(\"<I16s16s16sq\", req_DH_params.constructor, self.nonce, self.server_nonce,\n                          pq_io.getvalue(), self.public_key_fingerprint)\n\n        bytes_io = BytesIO()\n        bytes_io.write(ret)\n\n        serialize_string(bytes_io, self.encrypted_data)\n\n        return bytes_io.getvalue()", "response": "Return the bytes representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the bytes representation of the client_DH_inner_data field.", "response": "def get_bytes(self):\n        \"\"\"client_DH_inner_data#6643b654 nonce:int128 server_nonce:int128 retry_id:long g_b:string = Client_DH_Inner_Data\"\"\"\n\n        ret = struct.pack(\"<I16s16sQ\", client_DH_inner_data.constructor, self.nonce, self.server_nonce,\n                           self.retry_id)\n\n        bytes_io = BytesIO()\n        bytes_io.write(ret)\n        serialize_string(bytes_io, self.g_b)\n\n        return bytes_io.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bytes(self):\n\n        ret = struct.pack(\"<I16s16s\", set_client_DH_params.constructor, self.nonce, self.server_nonce)\n\n        bytes_io = BytesIO()\n        bytes_io.write(ret)\n        serialize_string(bytes_io, self.encrypted_data)\n\n        return bytes_io.getvalue()", "response": "Return the bytes representation of the Set_Client_DH_params message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo the command using the undo_manager s stack", "response": "def do(self, command: undoredo.Command):\n        \"\"\"\n        Does the command using the undo_manager's stack\n        :param command: Command\n        \"\"\"\n        self.undo_manager.do(command)\n        self.notify_observers()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nundoes the current state of the current locale.", "response": "def undo(self):\n        \"\"\"\n        Rewind the game to the previous state.\n        \"\"\"\n        self.undo_manager.undo()\n        self.notify_observers()\n        logging.debug('undo_manager undo stack={}'.format(self.undo_manager._undo_stack))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef redo(self):\n        self.undo_manager.redo()\n        self.notify_observers()\n        logging.debug('undo_manager redo stack={}'.format(self.undo_manager._redo_stack))", "response": "Redo the latest undone command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrestore the current state of the game object to match the properties and state of the given Game object.", "response": "def restore(self, game):\n        \"\"\"\n        Restore this Game object to match the properties and state of the given Game object\n        :param game: properties to restore to the current (self) Game\n        \"\"\"\n        self.observers = game.observers\n        # self.undo_manager = game.undo_manager\n        self.options = game.options\n        self.players = game.players\n        self.board.restore(game.board)\n        self.robber = game.robber\n        self.catanlog = game.catanlog\n\n        self.state = game.state\n        self.state.game = self\n\n        self.dev_card_state = game.dev_card_state\n\n        self._cur_player = game._cur_player\n        self.last_roll = game.last_roll\n        self.last_player_to_roll = game.last_player_to_roll\n        self._cur_turn = game._cur_turn\n        self.robber_tile = game.robber_tile\n\n        self.notify_observers()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the game. The value of option 'pregame' determines whether the pregame will occur or not. - Resets the board - Sets the players - Sets the game state to the appropriate first turn of the game - Finds the robber on the board, sets the robber_tile appropriately - Logs the catanlog header :param players: players to start the game with", "response": "def start(self, players):\n        \"\"\"\n        Start the game.\n\n        The value of option 'pregame' determines whether the pregame will occur or not.\n\n        - Resets the board\n        - Sets the players\n        - Sets the game state to the appropriate first turn of the game\n        - Finds the robber on the board, sets the robber_tile appropriately\n        - Logs the catanlog header\n\n        :param players: players to start the game with\n        \"\"\"\n        from .boardbuilder import Opt\n        self.reset()\n        if self.board.opts.get('players') == Opt.debug:\n            players = Game.get_debug_players()\n        self.set_players(players)\n        if self.options.get('pregame') is None or self.options.get('pregame') == 'on':\n            logging.debug('Entering pregame, game options={}'.format(self.options))\n            self.set_state(catan.states.GameStatePreGamePlacingPiece(self, catan.pieces.PieceType.settlement))\n        elif self.options.get('pregame') == 'off':\n            logging.debug('Skipping pregame, game options={}'.format(self.options))\n            self.set_state(catan.states.GameStateBeginTurn(self))\n\n        terrain = list()\n        numbers = list()\n        for tile in self.board.tiles:\n            terrain.append(tile.terrain)\n            numbers.append(tile.number)\n\n        for (_, coord), piece in self.board.pieces.items():\n            if piece.type == catan.pieces.PieceType.robber:\n                self.robber_tile = hexgrid.tile_id_from_coord(coord)\n                logging.debug('Found robber at coord={}, set robber_tile={}'.format(coord, self.robber_tile))\n\n        self.catanlog.log_game_start(self.players, terrain, numbers, self.board.ports)\n        self.notify_observers()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef net_query(name: str) -> Constants:\n    '''Find the NetworkParams for a network by its long or short name. Raises\n    UnsupportedNetwork if no NetworkParams is found.\n    '''\n\n    for net_params in networks:\n        if name in (net_params.name, net_params.shortname,):\n            return net_params\n\n    raise UnsupportedNetwork", "response": "Find the NetworkParams for a network by its long or short name. Raises UnsupportedNetwork if no NetworkParams is found."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef once_parser(cards: list) -> Optional[list]:\n    '''\n    parser for ONCE [2] issue mode\n    Only one issuance transaction from asset owner allowed.\n    '''\n\n    first_issue = next(i for i in cards if i.type == \"CardIssue\")  # find first CardIssue\n\n    filtered = [i for i in cards if i.type == \"CardIssue\" if i != first_issue]  #  drop all other CardIssues\n\n    return [i for i in cards if i not in filtered]", "response": "A parser for ONCE [ 2 ] issue mode\n    Only one issuance transaction from asset owner allowed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unflushable_parser(cards: list) -> Optional[list]:\n    '''\n    parser for UNFLUSHABLE [16] issue mode\n    No card transfer transactions allowed except for the card-issue transaction\n    '''\n\n    return [i for i in cards if i.type == \"CardIssue\"]", "response": "Unflushable parser for UNFLUSHABLE [ 16 ] issue mode\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore(self, board):\n        self.tiles = board.tiles\n        self.ports = board.ports\n\n        self.state = board.state\n        self.state.board = self\n\n        self.pieces = board.pieces\n        self.opts = board.opts\n        self.observers = board.observers\n\n        self.notify_observers()", "response": "Restore this Board object to match the properties and state of the given Board object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the port at the given tile_id and direction.", "response": "def get_port_at(self, tile_id, direction):\n        \"\"\"\n        If no port is found, a new none port is made and added to self.ports.\n\n        Returns the port.\n\n        :param tile_id:\n        :param direction:\n        :return: Port\n        \"\"\"\n        for port in self.ports:\n            if port.tile_id == tile_id and port.direction == direction:\n                return port\n        port = Port(tile_id, direction, PortType.none)\n        self.ports.append(port)\n        return port"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrotate the ports 90 degrees. Useful when using the default port setup but the spectator is watching at a true north.", "response": "def rotate_ports(self):\n        \"\"\"\n        Rotates the ports 90 degrees. Useful when using the default port setup but the spectator is watching\n        at a \"rotated\" angle from \"true north\".\n        \"\"\"\n        for port in self.ports:\n            port.tile_id = ((port.tile_id + 1) % len(hexgrid.coastal_tile_ids())) + 1\n            port.direction = hexgrid.rotate_direction(hexgrid.EDGE, port.direction, ccw=True)\n        self.notify_observers()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAuthenticatedUser(self, headers=None, query_params=None, content_type=\"application/json\"):\n        uri = self.client.base_url + \"/self\"\n        return self.client.get(uri, None, headers, query_params, content_type)", "response": "Get currently authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields SeqRecords from the index by matching keys.", "response": "def intersect_keys(keys, reffile, cache=False, clean_accs=False):\n    \"\"\"Extract SeqRecords from the index by matching keys.\n\n    keys - an iterable of sequence identifiers/accessions to select\n    reffile - name of a FASTA file to extract the specified sequences from\n    cache - save an index of the reference FASTA sequence offsets to disk?\n    clean_accs - strip HMMer extensions from sequence accessions?\n    \"\"\"\n    # Build/load the index of reference sequences\n    index = None\n    if cache:\n        refcache = reffile + '.sqlite'\n        if os.path.exists(refcache):\n            if os.stat(refcache).st_mtime < os.stat(reffile).st_mtime:\n                logging.warn(\"Outdated cache; rebuilding index\")\n            else:\n                try:\n                    index = (SeqIO.index_db(refcache,\n                                            key_function=clean_accession)\n                             if clean_accs\n                             else SeqIO.index_db(refcache))\n\n                except Exception:\n                    logging.warn(\"Skipping corrupted cache; rebuilding index\")\n                    index = None\n    else:\n        refcache = ':memory:'\n    if index is None:\n        # Rebuild the index, for whatever reason\n        index = (SeqIO.index_db(refcache, [reffile], 'fasta',\n                                key_function=clean_accession)\n                 if clean_accs\n                 else SeqIO.index_db(refcache, [reffile], 'fasta'))\n\n    # Extract records by key\n    if clean_accs:\n        keys = (clean_accession(k) for k in keys)\n    for key in keys:\n        try:\n            record = index[key]\n        except LookupError:\n            # Missing keys are rare, so it's faster not to check every time\n            logging.info(\"No match: %s\", repr(key))\n            continue\n        yield record"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the amino acid frequencies in a sequence set.", "response": "def aa_frequencies(seq, gap_chars='-.'):\n    \"\"\"Calculate the amino acid frequencies in a sequence set.\"\"\"\n    aa_counts = Counter(seq)\n    # Don't count gaps\n    for gap_char in gap_chars:\n        if gap_char in aa_counts:\n            del aa_counts[gap_char]\n    # Reduce to frequencies\n    scale = 1.0 / sum(aa_counts.values())\n    return dict((aa, cnt * scale) for aa, cnt in aa_counts.iteritems())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef give(self, terrain, num=1):\n        for _ in range(num):\n            logging.debug('terrain={}'.format(terrain))\n            self._give.append(terrain)", "response": "Add a certain number of resources to the trade from giver - > getter\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, terrain, num=1):\n        for _ in range(num):\n            logging.debug('terrain={}'.format(terrain))\n            self._get.append(terrain)", "response": "Add a certain number of resources to the trade from getter - > giver\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef giving(self):\n        logging.debug('give={}'.format(self._give))\n        c = Counter(self._give.copy())\n        return [(n, t) for t, n in c.items()]", "response": "Returns a list of tuples corresponding to the number and type of each\n        resource in the trade from giver -> getter\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of tuples corresponding to the number and type of each resource in the trade from getter - > giver", "response": "def getting(self):\n        \"\"\"\n        Returns tuples corresponding to the number and type of each\n        resource in the trade from getter->giver\n\n        :return: eg [(2, Terrain.wood), (1, Terrain.brick)]\n        \"\"\"\n        c = Counter(self._get.copy())\n        return [(n, t) for t, n in c.items()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(auth, authUserId, id, state, type, _type=None, annot=None, config=None, displayName=None, email=None, emailConfirmed=None, globalPermissionMask=None, password=None, paymentProcessorCustomerId=None, permissions=None, tokens=None, ui=None):\n\n        return User(\n            _type=_type,\n            annot=annot,\n            auth=auth,\n            authUserId=authUserId,\n            config=config,\n            displayName=displayName,\n            email=email,\n            emailConfirmed=emailConfirmed,\n            globalPermissionMask=globalPermissionMask,\n            id=id,\n            password=password,\n            paymentProcessorCustomerId=paymentProcessorCustomerId,\n            permissions=permissions,\n            state=state,\n            tokens=tokens,\n            type=type,\n            ui=ui,\n        )", "response": "Create a new User object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef family_check(self):\n        #TODO Make some tests for these\n        self.logger.info(\"Checking family relations for {0}\".format(\n            self.family_id)\n        )\n        for individual_id in self.individuals:\n            \n            self.logger.debug(\"Checking individual {0}\".format(individual_id))\n            individual = self.individuals[individual_id]\n            \n            self.logger.debug(\"Checking if individual {0} is affected\".format(\n                individual_id))\n            \n            if individual.affected:\n                self.logger.debug(\"Found affected individual {0}\".format(\n                    individual_id)\n                )\n                self.affected_individuals.add(individual_id)\n            \n            father = individual.father\n            mother = individual.mother\n            \n            if individual.has_parents:\n                self.logger.debug(\"Individual {0} has parents\".format(\n                    individual_id))\n                self.no_relations = False\n                try:\n                    self.check_parent(father, father=True)\n                    self.check_parent(mother, father=False)\n                except PedigreeError as e:\n                    self.logger.error(e.message)\n                    raise e\n                \n                # Check if there is a trio\n                if individual.has_both_parents:\n                    self.trios.append(set([individual_id, father, mother]))\n                elif father != '0':\n                    self.duos.append(set([individual_id, father]))\n                else:\n                    self.duos.append(set([individual_id, mother]))\n                \n                ##TODO self.check_grandparents(individual)\n            \n            # Annotate siblings:\n            for individual_2_id in self.individuals:\n                if individual_id != individual_2_id:\n                    if self.check_siblings(individual_id, individual_2_id):\n                        individual.siblings.add(individual_2_id)", "response": "Check if the family members break the structure of the family."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the parent info is correct.", "response": "def check_parent(self, parent_id, father = False):\n        \"\"\"\n        Check if the parent info is correct. If an individual is not present in file raise exeption.\n\n        Input: An id that represents a parent\n               father = True/False\n\n        Raises SyntaxError if\n            The parent id is not present\n            The gender of the parent is wrong.\n        \"\"\"\n        self.logger.debug(\"Checking parent {0}\".format(parent_id))\n        if parent_id != '0':\n            if parent_id not in self.individuals:\n                raise PedigreeError(self.family_id, parent_id, \n                                    'Parent is not in family.')\n            if father:\n                if self.individuals[parent_id].sex != 1:\n                    raise PedigreeError(self.family_id, parent_id, \n                                        'Father is not specified as male.')\n            else:\n                if self.individuals[parent_id].sex != 2:\n                    raise PedigreeError(self.family_id, parent_id, \n                                        'Mother is not specified as female.')\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_siblings(self, individual_1_id, individual_2_id):\n        \n        self.logger.debug(\"Checking if {0} and {1} are siblings\".format(\n            individual_1_id, individual_2_id\n        ))\n        ind_1 = self.individuals[individual_1_id]\n        ind_2 = self.individuals[individual_2_id]\n        if ((ind_1.father != '0' and ind_1.father == ind_2.father) or \n            (ind_1.mother != '0' and ind_1.mother == ind_2.mother)):\n            return True\n        else:\n            return False", "response": "Checks if two family members are siblings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_cousins(self, individual_1_id, individual_2_id):\n        self.logger.debug(\"Checking if {0} and {1} are cousins\".format(\n            individual_1_id, individual_2_id\n        ))\n        \n        #TODO check if any of the parents are siblings\n        pass", "response": "Check if two family members are cousins."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an individual to the family.", "response": "def add_individual(self, individual_object):\n        \"\"\"\n        Add an individual to the family.\n        \n        Arguments:\n            individual_object (Individual)\n            \n        \"\"\"\n        ind_id = individual_object.individual_id\n        self.logger.info(\"Adding individual {0}\".format(ind_id))\n        family_id = individual_object.family\n        if family_id != self.family_id:\n            raise PedigreeError(self.family, individual_object.individual_id,\n                \"Family id of individual is not the same as family id for \"\\\n                                    \"Family object!\")\n        else:\n            self.individuals[ind_id] = individual_object\n            self.logger.debug(\"Individual {0} added to family {1}\".format(\n                ind_id, family_id\n            ))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_phenotype(self, individual_id):\n        phenotype = 0 # This is if unknown phenotype\n        if individual_id in self.individuals:\n            phenotype = self.individuals[individual_id].phenotype\n        \n        return phenotype", "response": "Returns the phenotype of an individual"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the individuals of the family in ped format", "response": "def to_ped(self, outfile=None):\n        \"\"\"\n        Print the individuals of the family in ped format\n        \n        The header will be the original ped header plus all headers found in\n        extra info of the individuals\n        \"\"\"\n        \n        ped_header = [\n            '#FamilyID',\n            'IndividualID',\n            'PaternalID',\n            'MaternalID', \n            'Sex',\n            'Phenotype',\n        ]\n        \n        extra_headers = [\n            'InheritanceModel',\n            'Proband',\n            'Consultand',\n            'Alive'\n        ]\n        \n        for individual_id in self.individuals:\n            individual = self.individuals[individual_id]\n            for info in individual.extra_info:\n                if info in extra_headers:\n                    if info not in ped_header:\n                        ped_header.append(info)\n        \n        self.logger.debug(\"Ped headers found: {0}\".format(\n            ', '.join(ped_header)\n        ))\n        \n        if outfile:\n            outfile.write('\\t'.join(ped_header)+'\\n')\n        else:\n            print('\\t'.join(ped_header))\n        \n        for individual in self.to_json():\n            ped_info = []\n            ped_info.append(individual['family_id'])\n            ped_info.append(individual['id'])\n            ped_info.append(individual['father'])\n            ped_info.append(individual['mother'])\n            ped_info.append(individual['sex'])\n            ped_info.append(individual['phenotype'])\n            \n            if len(ped_header) > 6:\n                for header in ped_header[6:]:\n                    ped_info.append(individual['extra_info'].get(header, '.'))\n            \n            if outfile:\n                outfile.write('\\t'.join(ped_info)+'\\n')\n            else:\n                print('\\t'.join(ped_info))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_all_valid_decks(provider: Provider, deck_version: int,\n                         prod: bool=True) -> Generator:\n    '''\n    Scan the blockchain for PeerAssets decks, returns list of deck objects.\n    : provider - provider instance\n    : version - deck protocol version (0, 1, 2, ...)\n    : test True/False - test or production P2TH\n    '''\n\n    pa_params = param_query(provider.network)\n\n    if prod:\n        p2th = pa_params.P2TH_addr\n    else:\n        p2th = pa_params.test_P2TH_addr\n\n    if isinstance(provider, RpcNode):\n        deck_spawns = (provider.getrawtransaction(i, 1)\n                       for i in find_deck_spawns(provider))\n\n    else:\n        try:\n            deck_spawns = (provider.getrawtransaction(i, 1) for i in\n                           provider.listtransactions(p2th))\n        except TypeError as err:  # it will except if no transactions are found on this P2TH\n            raise EmptyP2THDirectory(err)\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as th:\n        for result in th.map(deck_parser, ((provider, rawtx, deck_version, p2th) for rawtx in deck_spawns)):\n            if result:\n                yield result", "response": "Scan the blockchain for decks and return list of deck objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds specific deck by deck id.", "response": "def find_deck(provider: Provider, key: str, version: int, prod: bool=True) -> Optional[Deck]:\n    '''Find specific deck by deck id.'''\n\n    pa_params = param_query(provider.network)\n    if prod:\n        p2th = pa_params.P2TH_addr\n    else:\n        p2th = pa_params.test_P2TH_addr\n\n    rawtx = provider.getrawtransaction(key, 1)\n    deck = deck_parser((provider, rawtx, 1, p2th))\n\n    return deck"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deck_spawn(provider: Provider, deck: Deck, inputs: dict,\n               change_address: str, locktime: int=0) -> Transaction:\n\n    '''Creates Deck spawn raw transaction.\n\n       : key - Kutil object which we'll use to sign the tx\n       : deck - Deck object\n       : card - CardTransfer object\n       : inputs - utxos (has to be owned by deck issuer)\n       : change_address - address to send the change to\n       : locktime - tx locked until block n=int\n    '''\n\n    network_params = net_query(deck.network)\n    pa_params = param_query(deck.network)\n\n    if deck.production:\n        p2th_addr = pa_params.P2TH_addr\n    else:\n        p2th_addr = pa_params.test_P2TH_addr\n\n    #  first round of txn making is done by presuming minimal fee\n    change_sum = Decimal(inputs['total'] - network_params.min_tx_fee - pa_params.P2TH_fee)\n\n    txouts = [\n        tx_output(network=deck.network, value=pa_params.P2TH_fee,\n                  n=0, script=p2pkh_script(address=p2th_addr,\n                                           network=deck.network)),  # p2th\n\n        tx_output(network=deck.network, value=Decimal(0),\n                  n=1, script=nulldata_script(deck.metainfo_to_protobuf)),  # op_return\n\n        tx_output(network=deck.network, value=change_sum,\n                  n=2, script=p2pkh_script(address=change_address,\n                                           network=deck.network))  # change\n              ]\n\n    unsigned_tx = make_raw_transaction(network=deck.network,\n                                       inputs=inputs['utxos'],\n                                       outputs=txouts,\n                                       locktime=Locktime(locktime)\n                                       )\n    return unsigned_tx", "response": "Creates a deck spawn transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deck_transfer(provider: Provider, deck: Deck,\n                  inputs: list, change_address: str) -> Transaction:\n    '''\n    The deck transfer transaction is a special case of the deck spawn transaction.\n    Instead of registering a new asset, the deck transfer transaction transfers ownership from vin[1] to vin[0],\n    meaning that both parties are required to sign the transfer transaction for it to be accepted in the blockchain.\n    '''\n    raise NotImplementedError", "response": "This function returns a deck spawn transaction that transfers ownership from vin [ 1 to vin [ 0 ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef card_bundler(provider: Provider, deck: Deck, tx: dict) -> CardBundle:\n    '''each blockchain transaction can contain multiple cards,\n       wrapped in bundles. This method finds and returns those bundles.'''\n\n    return CardBundle(deck=deck,\n                      blockhash=tx['blockhash'],\n                      txid=tx['txid'],\n                      timestamp=tx['time'],\n                      blockseq=tx_serialization_order(provider,\n                                                      tx[\"blockhash\"],\n                                                      tx[\"txid\"]),\n                      blocknum=provider.getblock(tx[\"blockhash\"])[\"height\"],\n                      sender=find_tx_sender(provider, tx),\n                      vouts=tx['vout'],\n                      tx_confirmations=tx['confirmations']\n                      )", "response": "This method finds and returns the cards in a bundle. This method finds and returns those bundles. This method finds and returns those bundles."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_card_bundles(provider: Provider, deck: Deck) -> Optional[Iterator]:\n    '''each blockchain transaction can contain multiple cards,\n       wrapped in bundles. This method finds and returns those bundles.'''\n\n    if isinstance(provider, RpcNode):\n        if deck.id is None:\n            raise Exception(\"deck.id required to listtransactions\")\n\n        p2th_account = provider.getaccount(deck.p2th_address)\n        batch_data = [('getrawtransaction', [i[\"txid\"], 1]) for\n                      i in provider.listtransactions(p2th_account)]\n        result = provider.batch(batch_data)\n\n        if result is not None:\n            raw_txns = [i['result'] for i in result if result]\n\n        else:\n            raise EmptyP2THDirectory({'error': 'No cards found on this deck.'})\n\n    else:\n        if deck.p2th_address is None:\n            raise Exception(\"deck.p2th_address required to listtransactions\")\n\n        try:\n            raw_txns = (provider.getrawtransaction(i, 1) for i in\n                        provider.listtransactions(deck.p2th_address))\n        except TypeError:\n            raise EmptyP2THDirectory({'error': 'No cards found on this deck.'})\n\n    return (card_bundler(provider, deck, i) for i in raw_txns)", "response": "This method finds and returns those bundles. This method returns the iterator over the cards in the deck."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all cards bundles if they match the protocol", "response": "def get_card_bundles(provider: Provider, deck: Deck) -> Generator:\n    '''get all <deck> card bundles, if they match the protocol'''\n\n    bundles = find_card_bundles(provider, deck)\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as th:\n        for result in th.map(card_bundle_parser, bundles):\n            if result:\n                yield result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a single card transfer by it s id", "response": "def get_card_transfer(provider: Provider, deck: Deck,\n                      txid: str,\n                      debug: bool=False) -> Iterator:\n    '''get a single card transfer by it's id'''\n\n    rawtx = provider.getrawtransaction(txid, 1)\n\n    bundle = card_bundler(provider, deck, rawtx)\n\n    return card_bundle_parser(bundle, debug)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all the valid cards on this deck", "response": "def find_all_valid_cards(provider: Provider, deck: Deck) -> Generator:\n    '''find all the valid cards on this deck,\n       filtering out cards which don't play nice with deck issue mode'''\n\n    # validate_card_issue_modes must recieve a full list of cards, not batches\n    unfiltered = (card for batch in get_card_bundles(provider, deck) for card in batch)\n\n    for card in validate_card_issue_modes(deck.issue_mode, list(unfiltered)):\n        yield card"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares the CardTransfer Transaction object for transmission.", "response": "def card_transfer(provider: Provider, card: CardTransfer, inputs: dict,\n                  change_address: str, locktime: int=0) -> Transaction:\n\n    '''Prepare the CardTransfer Transaction object\n\n       : card - CardTransfer object\n       : inputs - utxos (has to be owned by deck issuer)\n       : change_address - address to send the change to\n       : locktime - tx locked until block n=int\n       '''\n\n    network_params = net_query(provider.network)\n    pa_params = param_query(provider.network)\n\n    if card.deck_p2th is None:\n        raise Exception(\"card.deck_p2th required for tx_output\")\n\n    outs = [\n        tx_output(network=provider.network,\n                  value=pa_params.P2TH_fee,\n                  n=0, script=p2pkh_script(address=card.deck_p2th,\n                                           network=provider.network)),  # deck p2th\n        tx_output(network=provider.network,\n                  value=Decimal(0), n=1,\n                  script=nulldata_script(card.metainfo_to_protobuf))  # op_return\n    ]\n\n    for addr, index in zip(card.receiver, range(len(card.receiver))):\n        outs.append(   # TxOut for each receiver, index + 2 because we have two outs already\n            tx_output(network=provider.network, value=Decimal(0), n=index+2,\n                      script=p2pkh_script(address=addr,\n                                          network=provider.network))\n        )\n\n    #  first round of txn making is done by presuming minimal fee\n    change_sum = Decimal(inputs['total'] - network_params.min_tx_fee - pa_params.P2TH_fee)\n\n    outs.append(\n        tx_output(network=provider.network,\n                  value=change_sum, n=len(outs)+1,\n                  script=p2pkh_script(address=change_address,\n                                      network=provider.network))\n        )\n\n    unsigned_tx = make_raw_transaction(network=provider.network,\n                                       inputs=inputs['utxos'],\n                                       outputs=outs,\n                                       locktime=Locktime(locktime)\n                                       )\n    return unsigned_tx"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new object of type NetworkConfigNetwork.", "response": "def create(annot=None, config=None, id=None, ui=None):\n        \"\"\"\n        :type annot: dict\n        :type config: NetworkConfig\n        :type id: str\n        :type ui: dict\n        :rtype: Network\n        \"\"\"\n\n        return Network(\n            annot=annot,\n            config=config,\n            id=id,\n            ui=ui,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting camelcase to underscore", "response": "def uncamel(name):\n    \"\"\"converts camelcase to underscore\n    >>> uncamel('fooBar')\n    'foo_bar'\n    >>> uncamel('FooBar')\n    'foo_bar'\n    >>> uncamel('_fooBar')\n    '_foo_bar'\n    >>> uncamel('_FooBar')\n    '__foo_bar'\n    \"\"\"\n    response, name = name[0].lower(), name[1:]\n    for n in name:\n        if n.isupper():\n            response += '_' + n.lower()\n        else:\n            response += n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rfc3339_to_datetime(data):\n    try:\n        ts = time.strptime(data, '%Y-%m-%d')\n        return date(*ts[:3])\n    except ValueError:\n        pass\n\n    try:\n        dt, _, tz = data.partition('Z')\n        if tz:\n            tz = offset(tz)\n        else:\n            tz = offset('00:00')\n        if '.' in dt and dt.rsplit('.', 1)[-1].isdigit():\n            ts = time.strptime(dt, '%Y-%m-%dT%H:%M:%S.%f')\n        else:\n            ts = time.strptime(dt, '%Y-%m-%dT%H:%M:%S')\n        return datetime(*ts[:6], tzinfo=tz)\n    except ValueError:\n        raise ValueError('date-time {!r} is not a valid rfc3339 date representation'.format(data))", "response": "convert a rfc3339 date representation into a Python datetime"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits until stopped to keep script live. Gui must handle calling of Toggle_NV function on mouse click.", "response": "def _function(self):\n        \"\"\"\n        Waits until stopped to keep script live. Gui must handle calling of Toggle_NV function on mouse click.\n        \"\"\"\n\n        self.data = {'nv_locations': [], 'image_data': None, 'extent': None}\n\n        self.progress = 50\n        self.updateProgress.emit(self.progress)\n        # keep script alive while NVs are selected\n        while not self._abort:\n            time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, figure_list):\n        '''\n        Plots a dot on top of each selected NV, with a corresponding number denoting the order in which the NVs are\n        listed.\n        Precondition: must have an existing image in figure_list[0] to plot over\n        Args:\n            figure_list:\n        '''\n        # if there is not image data get it from the current plot\n        if not self.data == {} and self.data['image_data'] is  None:\n            axes = figure_list[0].axes[0]\n            if len(axes.images)>0:\n                self.data['image_data'] = np.array(axes.images[0].get_array())\n                self.data['extent'] = np.array(axes.images[0].get_extent())\n                self.plot_settings['cmap'] = axes.images[0].get_cmap().name\n                self.plot_settings['xlabel'] = axes.get_xlabel()\n                self.plot_settings['ylabel'] = axes.get_ylabel()\n                self.plot_settings['title'] = axes.get_title()\n                self.plot_settings['interpol'] = axes.images[0].get_interpolation()\n\n        Script.plot(self, figure_list)", "response": "Plots a dot on top of each selected NV with a corresponding number denoting the order in which the NVs are listed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the image data over the axes_list.", "response": "def _plot(self, axes_list):\n        '''\n        Plots a dot on top of each selected NV, with a corresponding number denoting the order in which the NVs are\n        listed.\n        Precondition: must have an existing image in figure_list[0] to plot over\n        Args:\n            figure_list:\n        '''\n\n        axes = axes_list[0]\n\n        if self.plot_settings:\n            axes.imshow(self.data['image_data'], cmap=self.plot_settings['cmap'], interpolation=self.plot_settings['interpol'], extent=self.data['extent'])\n            axes.set_xlabel(self.plot_settings['xlabel'])\n            axes.set_ylabel(self.plot_settings['ylabel'])\n            axes.set_title(self.plot_settings['title'])\n\n        self._update(axes_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(annot=None, config=None, networkId=None, nodeId=None, state=None, ui=None):\n\n        return NetworkMember(\n            annot=annot,\n            config=config,\n            networkId=networkId,\n            nodeId=nodeId,\n            state=state,\n            ui=ui,\n        )", "response": "Create a new NetworkMember object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_data(self, filename = None, data_tag = None, verbose=False):\n\n\n        def len(x):\n            \"\"\"\n            overwrite the buildin len function to cover cases that don't have a length, like int or float\n            and to catch string as objects of length 0\n            Args:\n                x: quantity of which we want to find the length\n            Returns: length of x\n\n            \"\"\"\n            if isinstance(x, (int, float, str)) or x is None:\n                result = 0\n            else:\n                result = builtin_len(x)\n            return result\n\n        if filename is None:\n            filename = self.filename('.csv')\n\n        filename = os.path.join(os.path.join(os.path.dirname(filename),self.RAW_DATA_DIR), os.path.basename(filename))\n\n        # windows can't deal with long filenames so we have to use the prefix '\\\\\\\\?\\\\'\n        # if len(filename.split('\\\\\\\\?\\\\')) == 1:\n        filename = self.check_filename(filename)\n\n        if not os.path.exists(os.path.dirname(filename)):\n            os.makedirs(os.path.dirname(filename))\n\n        # if deque object, take the last dataset, which is the most recent\n        if isinstance(self.data, deque):\n            data = self.data[-1]\n        elif isinstance(self.data, dict):\n            data = self.data\n        else:\n            raise TypeError(\"script data variable has an invalid datatype! Must be deque or dict.\")\n\n\n        if data_tag is None:\n            if verbose:\n                print('data_tag is None')\n\n            if len(set([len(v) for v in list(data.values())])) == 1 and set(\n                    [len(np.shape(list(data.values())[i])) for i in range(len(list(data.values())))]) == set([0, 1]):\n                # if all entries of the dictionary are the same length and single column we can write the data into a single file\n\n                if len(np.shape(list(data.values())[0]))==1:\n                    df = pd.DataFrame(data)\n                else:\n                    df = pd.DataFrame.from_records([data])\n\n                if len(df) == 0 or df.empty:\n                    print('warning! Data seems to be empty. Not saved', df)\n                else:\n                    df.to_csv(filename, index=False)\n\n            else:\n                # otherwise, we write each entry into a separate file\n                for key, value in data.items():\n\n                    if verbose:\n                        print('current data', key)\n\n                    if len(value) == 0:\n                        df = pd.DataFrame([value])\n                    else:\n                        if isinstance(value, dict) and isinstance(list(value.values())[0], (int, float)):\n                            # if dictionary values are single numbers\n                            df = pd.DataFrame.from_dict({k: [v] for k, v in value.items()})\n                        elif isinstance(value, dict) and isinstance(list(value.values())[0], (list, np.ndarray)):\n                            # if dictionary values are lists or arrays\n                            df = pd.DataFrame.from_dict(value)\n                        else:\n                            # if not a dictionary\n                            df = pd.DataFrame(value)\n\n                    if len(df) == 0 or df.empty:\n                        print('warning! Data ({:s}) seems to be empty. Not saved'.format(key), df)\n                    else:\n                        df.to_csv(filename.replace('.csv', '-{:s}.csv'.format(key)), index=False)\n\n        else:\n\n            # save only the data for which a key has been provided\n            assert data_tag in list(data.keys())\n\n            if verbose:\n                print('data_tag', data_tag)\n\n            value = data[data_tag]\n            if len(value) == 0:\n                df = pd.DataFrame([value])\n            else:\n                df = pd.DataFrame(value)\n\n            if len(df) == 0 or df.empty:\n                print('warning! Data seems to be empty. Not saved', df)\n            else:\n                df.to_csv(filename, index=False)", "response": "Save the script data to a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving log to file", "response": "def save_log(self, filename = None):\n        \"\"\"\n        save log to file\n        Returns:\n\n        \"\"\"\n        if filename is None:\n            filename = self.filename('-info.txt')\n        filename = self.check_filename(filename)\n        # filename = self.check_filename(filename)\n        # windows can't deal with long filenames so we have to use the prefix '\\\\\\\\?\\\\'\n        # if len(filename.split('\\\\\\\\?\\\\')) == 1:\n        #     filename = '\\\\\\\\?\\\\' + filename\n        with open(filename, 'w') as outfile:\n            for item in self.log_data:\n                outfile.write(\"%s\\n\" % item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the data from the given path.", "response": "def load_data(path, verbose=False, raise_errors = False):\n        \"\"\"\n        loads the data that has been save with Script.save.\n        Args:\n            path: path to folder saved by Script.save or raw_data folder within\n            verbose: if true print additional information\n            raise_errors: if true raise errors if false just print to std out\n        Returns:\n            a dictionary with the data of form\n            data = {param_1_name: param_1_data, ...}\n        \"\"\"\n\n\n        # check that path exists\n        if not os.path.exists(path):\n            if raise_errors:\n                raise AttributeError('Path given does not exist!')\n            else:\n                print('Path given does not exist!')\n                return\n\n        # windows can't deal with long filenames (>260 chars) so we have to use the prefix '\\\\\\\\?\\\\'\n        # if len(path.split('\\\\\\\\?\\\\')) == 1:\n        #     path = '\\\\\\\\?\\\\' + os.path.abspath(path)\n        path = Script.check_filename(path)\n\n        if verbose:\n            print('script path', path)\n\n\n        # if raw_data folder exists, get a list of directories from within it; otherwise, get names of all .csv files in\n        # current directory\n        data = {}\n        # if self.RAW_DATA_DIR in os.listdir(path): #8/26/16 AK: self not defined in static context\n        #     data_files = os.listdir(os.path.join(path, self.RAW_DATA_DIR + '/'))\n        #     path = os.path.join(path, self.RAW_DATA_DIR + '/')\n        #\n        # else:\n        if 'raw_data' in os.listdir(path):  #temporarily hardcoded\n\n            if verbose:\n                print('raw_data subfolder found')\n            data_files = os.listdir(os.path.join(path, 'raw_data' + '/'))\n            path = os.path.join(path, 'raw_data' + '/')\n\n        else:\n            data_files = glob.glob(os.path.join(path, '*.csv'))\n\n        if verbose:\n            print('data_files found', data_files)\n\n        # If no data files were found, raise error\n        if not data_files:\n\n            if raise_errors:\n                raise AttributeError('Could not find data files in {:s}'.format(path))\n            else:\n                print('Could not find data files in {:s}'.format(path))\n                return\n\n        # import data from each csv\n        for data_file in data_files:\n            # get data name, read the data from the csv, and save it to dictionary\n            data_name = data_file.split('-')[-1][0:-4] # JG: why do we strip of the date?\n\n            try:\n                imported_data_df = pd.read_csv(os.path.join(path, data_file))\n\n                # check if there are real headers, if the headers are digits than we ignore them because then they are just indecies\n                # real headers are strings (however, the digits are also of type str! that why we use the isdigit method)\n                column_headers = list(imported_data_df.columns.values)\n                if sum([int(x.isdigit()) for x in column_headers]) != len(column_headers):\n                    data[data_name] = {h: imported_data_df[h].values for h in column_headers}\n                else:\n                    # note, np.squeeze removes extraneous length-1 dimensions from the returned 'matrix' from the dataframe\n                    data[data_name] = np.squeeze(imported_data_df.values)\n            except pd.errors.EmptyDataError as err:\n\n                if raise_errors:\n                    raise err('data file ' + data_file + ' is empty: did not load!')\n                else:\n                    print('data file ' + data_file + ' is empty: did not load!')\n\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the settings that has been saved with Script. save_b26.", "response": "def load_settings(path, setttings_only = True):\n        \"\"\"\n        loads the settings that has been save with Script.save_b26.\n        Args:\n            path: path to folder saved by Script.save_b26\n            setttings_only: if true returns only the settings if the .b26 file contains only a single script\n        Returns:\n            a dictionary with the settings\n        \"\"\"\n\n\n        # check that path exists\n        if not os.path.exists(path):\n            print(path)\n            raise AttributeError('Path given does not exist!')\n\n        tag = '_'.join(os.path.basename(os.path.dirname(os.path.abspath(path) + '/')).split('_')[3:])\n\n        search_str = os.path.abspath(path)+'/*'+tag +'.b26'\n        fname = glob.glob(search_str)\n        if len(fname)>1:\n            print(('warning more than one .b26 file found, loading ', fname[0]))\n        elif len(fname) == 0:\n            print(('no .b26 file found in folder {:s},  check path !'.format(search_str)))\n            return\n        fname = fname[0]\n        fname = Script.check_filename(fname)\n        settings = load_b26_file(fname)['scripts']\n\n        if len(list(settings.keys())) == 1 and setttings_only:\n            settings = settings[list(settings.keys())[0]]['settings']\n\n        return settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_and_append(script_dict, scripts=None, instruments=None, log_function=None, data_path=None,\n                        raise_errors=False, package='pylabcontrol', verbose=False):\n        \"\"\"\n        load script from script_dict and append to scripts, if additional instruments are required create them and add them to instruments\n\n        Args:\n            script_dict: dictionary of form\n\n                script_dict = {\n                name_of_script_1 :\n                    {\"settings\" : settings_dictionary, \"class\" : name_of_class}\n                name_of_instrument_2 :\n                    {\"settings\" : settings_dictionary, \"class\" : name_of_class}\n                ...\n                }\n\n            or\n\n                script_dict = {\n                name_of_script_1 : name_of_class,\n                name_of_script_2 : name_of_class\n                ...\n                }\n\n            where name_of_class is either a class or the name of a class\n\n            scripts: dictionary of form\n\n                scripts = {\n                name_of_script_1 : instance_of_script_1,\n                name_of_script_2 : instance_of_script_2,\n                ...\n                }\n\n            instruments: dictionary of form\n\n                instruments = {\n                name_of_instrument_1 : instance_of_instrument_1,\n                name_of_instrument_2 : instance_of_instrument_2,\n                ...\n                }\n            log_function: function that takes a string\n\n            data_path: absolute path where data is saved, in case the path in the script is definded as a relative path\n\n            raise_errors: if True errors are raised\n        Returns:\n                dictionary of form\n                script_dict = { name_of_script_1 : script_1_instance, name_of_script_2 : script_2_instance, ...}\n                load_failed = {name_of_script_1: exception_1, name_of_script_2: exception_2, ....}\n                updated_instruments = {name_of_instrument_1 : instance_of_instrument_1, ..}\n\n        \"\"\"\n        if scripts is None:\n            scripts = {}\n        if instruments is None:\n            instruments = {}\n\n        load_failed = {}\n        updated_scripts = {}\n        updated_scripts.update(scripts)\n        updated_instruments = {}\n        updated_instruments.update(instruments)\n\n        if verbose:\n            print(('script_dict', script_dict))\n\n        def get_instruments(class_of_script, script_instruments, instruments):\n            \"\"\"\n\n            creates the dictionary with the instruments needed for the script and update the instrument dictionary if new instruments are required\n\n            Args:\n                class_of_script: the class of the script\n                instruments: the instruments that have been loaded already\n\n            Returns: dictionary with the instruments that the script needs and the updated instruments dictionary\n\n            \"\"\"\n\n            default_instruments = getattr(class_of_script, '_INSTRUMENTS')\n            instrument_dict = {}\n            instruments_updated = {}\n            instruments_updated.update(instruments)\n            # check if instruments needed by script already exist, if not create an instance\n            for instrument_name, instrument_class in default_instruments.items():\n                # check if instruments needed by script already exist\n                instrument = [instance for name, instance in instruments_updated.items() if\n                              isinstance(instance, instrument_class) and name == instrument_name]\n\n                if len(instrument) == 0:\n                    # create new instance of instrument\n                    instruments_updated, __ = Instrument.load_and_append({instrument_name: instrument_class}, instruments_updated, raise_errors)\n\n                if script_instruments is not None and instrument_name in script_instruments:\n                    instrument_settings_dict = script_instruments[instrument_name]['settings']\n                else:\n                    instrument_settings_dict = instruments_updated[instrument_name].settings\n\n                instrument_instance = instruments_updated[instrument_name]\n\n                # make a deepcopy of _DEFAULT_SETTINGS to get a parameter object\n                instrument_settings = deepcopy(instrument_instance._DEFAULT_SETTINGS)\n\n                #now update parameter object with new values\n                instrument_settings.update(instrument_settings_dict)\n\n                instrument_dict.update({instrument_name: {\"instance\":instrument_instance, \"settings\":instrument_settings}})\n\n\n            return instrument_dict, instruments_updated\n\n        def get_sub_scripts(class_of_script, instruments, sub_scripts_dict, log_function = None):\n            \"\"\"\n\n            creates the dictionary with the sub scripts needed by the script and updates the instrument dictionary if new instruments are required\n\n            Args:\n                class_of_script: the class of the script\n                instruments: the instruments that have been loaded already\n                sub_scripts_dict: settings of script in dictionary form\n\n            Returns:dictionary with the sub scripts that the script needs\n\n            \"\"\"\n\n            default_scripts = getattr(class_of_script, '_SCRIPTS')\n            #\n            # create instruments that script needs\n            sub_scripts = {}\n            sub_scripts, scripts_failed, instruments_updated = Script.load_and_append(default_scripts, sub_scripts,\n                                                                                      instruments,\n                                                                                      log_function=log_function,\n                                                                                      raise_errors=raise_errors)\n            try:\n                if sub_scripts_dict is not None:\n                    for k, v in sub_scripts_dict.items():\n                        #update settings, updates instrument and settings\n                        sub_scripts[k].update(v)\n            except TypeError: #if actually an object, as with dynamic scripts\n                pass\n\n            if len(scripts_failed)>0:\n                raise ImportError('script {:s}: failed to load subscripts'.format(class_of_script))\n            return sub_scripts, instruments_updated\n\n        for script_name, script_info in script_dict.items():\n\n            # check if script already exists\n            if script_name in list(scripts.keys()):\n                print(('WARNING: script {:s} already exists. Did not load!'.format(script_name)))\n                load_failed[script_name] = ValueError('script {:s} already exists. Did not load!'.format(script_name))\n            else:\n                module, script_class_name, script_settings, script_instruments, script_sub_scripts, script_doc, package = Script.get_script_information(script_info, package=package)\n                # creates all dynamic scripts so they can be imported following the if statement\n                # if script_class_name == 'ScriptIterator':\n                if 'ScriptIterator' in script_class_name:\n                    # creates all the dynamic classes in the script and the class of the script itself\n                    # and updates the script info with these new classes\n                    from pylabcontrol.core.script_iterator import ScriptIterator #CAUTION: imports ScriptIterator, which inherits from script. Local scope should avoid circular imports.\n\n                    script_info, _ = ScriptIterator.create_dynamic_script_class(script_info)\n\n                    # now get the info for the dynamically created class\n                    module, script_class_name, script_settings, script_instruments, script_sub_scripts, script_doc, package = Script.get_script_information(script_info)\n                if verbose:\n                    print(('load_and_append.module', module))\n                    print(('load_and_append.script_info', script_info))\n                    print(('load_and_append.package', package))\n\n                if module is None and inspect.isclass(script_info):\n                    class_of_script = script_info\n                else:\n                    class_of_script = getattr(module, script_class_name)\n\n                #  ========= create the instruments that are needed by the script =========\n                try:\n                    script_instruments, updated_instruments = get_instruments(class_of_script, script_instruments, updated_instruments)\n                except Exception as err:\n                    print(('loading script {:s} failed. Could not load instruments!'.format(script_name)))\n                    load_failed[script_name] = err\n                    if raise_errors:\n                        raise err\n                    continue\n                #  ========= create the subscripts that are needed by the script =========\n                try:\n                    sub_scripts, updated_instruments = get_sub_scripts(class_of_script, updated_instruments, script_sub_scripts, log_function = log_function)\n                except Exception as err:\n                    print(('loading script {:s} failed. Could not load subscripts!'.format(script_name)))\n                    load_failed[script_name] = err\n                    if raise_errors:\n                        raise err\n                    continue\n\n                #  ========= create the script if instruments and subscripts have been loaded successfully =========\n                class_creation_string = ''\n                if script_instruments:\n                    class_creation_string += ', instruments = script_instruments'\n                if sub_scripts:\n                    class_creation_string += ', scripts = sub_scripts'\n                if script_settings:\n                    class_creation_string += ', settings = script_settings'\n                if log_function:\n                    class_creation_string += ', log_function = log_function'\n                if data_path:\n                    class_creation_string += ', data_path = data_path'\n                class_creation_string = 'class_of_script(name=script_name{:s})'.format(class_creation_string)\n\n                if verbose:\n                    print(('class_creation_string', class_creation_string))\n                    print(('class_of_script', class_of_script))\n                    print(('scripts', sub_scripts))\n\n                try:\n                    script_instance = eval(class_creation_string)\n                except Exception as err:\n                    print('loading ' + script_name + ' failed:')\n                    print(traceback.format_exc())\n                    # print(('loading script {:s} failed. Could not create instance of script!'.format(script_name)))\n                    load_failed[script_name] = err\n                    if raise_errors:\n                        raise err\n                    continue\n\n                if script_doc:\n                    script_instance.__doc__ = script_doc\n\n                updated_scripts.update({script_name: script_instance})\n\n\n        return updated_scripts, load_failed, updated_instruments", "response": "Load a script from script_dict and append it to scripts."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts all the relevant information from script_information and returns it as individual variables Args: script_information: information of the script. This can be - a dictionary - a Script instance - name of Script class package (optional): name of the package to which the script belongs, i.e. pylabcontrol or b26toolkit. Only used when script_information is a string Returns: module, script_class_name, script_settings, script_instruments, script_sub_scripts, script_info, package", "response": "def get_script_information(script_information, package='pylabcontrol', verbose=False):\n        \"\"\"\n        extracts all the relevant information from script_information and returns it as individual variables\n        Args:\n            script_information: information of the script. This can be\n                - a dictionary\n                - a Script instance\n                - name of Script class\n            package (optional): name of the package to which the script belongs, i.e. pylabcontrol or b26toolkit.\n                                Only used when script_information is a string\n        Returns:\n            module, script_class_name, script_settings, script_instruments, script_sub_scripts, script_info, package\n        \"\"\"\n\n        script_settings = None\n        script_instruments = None\n        script_sub_scripts = None\n        script_class_name = None\n        module = None  # this is the module that contains the script where we look for scripts\n        script_info = None # this is the docstring that describes the script\n        module_path = package + '.scripts'\n        script_filepath = None\n        module_file = None\n\n        if isinstance(script_information, dict):\n            if 'settings' in script_information:\n                script_settings = script_information['settings']\n            if 'filepath' in script_information:\n                script_filepath = str(script_information['filepath'])\n                module_path, module_file = module_name_from_path(script_filepath, verbose = False)\n            if 'package' in script_information:\n                package = script_information['package']\n            else:\n                assert 'filepath' in script_information  # there should be a filepath if we load form a b26 file\n                # in the case that we generate the script_information from a .py file the package is given by the name of the highest module\n                if 'filepath' in script_information:\n                    package = module_path.split('.')[0]\n\n            script_class_name = str(script_information['class'])\n            if 'ScriptIterator' in script_class_name:\n                module_path = package + '.core.script_iterator'\n            if 'instruments' in script_information:\n                script_instruments = script_information['instruments']\n            if 'scripts' in script_information:\n                script_sub_scripts = script_information['scripts']\n            if 'info' in script_information:\n                script_info = script_information['info']\n                \n        elif isinstance(script_information, str):\n            script_class_name = script_information\n\n\n        elif issubclass(script_information, Script):\n            # watch out when testing this code from __main__, then classes might not be identified correctly because the path is different\n            # to avoid this problem call from pylabcontrol.core import Script (otherwise the path to Script is __main__.Script)\n            script_class_name = script_information.__name__\n            package = script_information.__module__.split('.')[0]\n            module_path = script_information.__module__\n\n        assert isinstance(package, str)\n\n\n        # if the script has not been created yet, i.e. script_class_name: ScriptIteratorB26 or ScriptIterator\n        if verbose:\n            print(('script_filepath', script_filepath))\n            print(('path_to_module', module_path))\n\n        if script_filepath is not None:\n            # scriptiterator loaded from file\n            if os.path.basename(script_filepath.split('.pyc')[0].split('.py')[0]) == 'script_iterator':\n                module_path = package + '.core.script_iterator'\n\n        # if the script has been created already, i.e. script_class_name: package.dynamic_script_iterator\n        # todo: now there is the prefix package\n        if len(script_class_name.split('dynamic_script_iterator')) == 2 and \\\n                script_class_name.split('dynamic_script_iterator')[1].isdigit():\n            # package = 'pylabcontrol' # all the dynamic iterator scripts are defined in the name space of pylabcontrol\n            # all the dynamic iterator scripts are defined in the name space of package.pylabcontrol.core.script_iterator\n            # module = import_module(package + '.pylabcontrol.core.script_iterator')\n            module_path = package\n\n        # the package should be the highest level of the module path\n        # assert module_path.split('.')[0] == package\n        # assert isinstance(module_path, str)  # in that case we should have defined a module_path to load the module\n        # assert module is None  # we haven't loaded the module yet\n\n        # try:\n        #     print(module_path)\n        #     module = import_module(module_path)\n        #     print(module)\n        # except ImportError:\n        #     pass\n        # print('module', module_path)\n\n        #appends path to this module to the python path if it is not present so it can be used\n        if module_file and (module_file not in sys.path):\n            sys.path.append(module_file)\n\n        module = import_module(module_path)\n        # check if module was found!\n        if module is None or not hasattr(module, script_class_name):\n            # import sys\n            print('here is the pythonpath')\n            for path in sys.path:\n                print(path)\n            import time\n            time.sleep(1)\n            print(('Could not find the module that contains ' + script_class_name + ' in module ' + module_path))\n            raise ImportError('Could not find the module that contains ' + script_class_name + ' in module ' + module_path)\n\n        # if the module has a name of type dynamic_script_iteratorX where X is a number the module is script iterator\n        return module, script_class_name, script_settings, script_instruments, script_sub_scripts, script_info, package"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_config(verbose=1):\n    # ENH:\n    # - do print levelname before DEBUG and WARNING\n    # - instead of %module, name the currently running script\n    #   - make a subclass of logging.handlers.X instead?\n    #   - tweak %root?\n    #   - take __file__ as an argument?\n    if verbose == 0:\n        level = logging.WARNING\n        fmt = \"%(module)s: %(message)s\"\n    elif verbose == 1:\n        level = logging.INFO\n        fmt = \"%(module)s [@%(lineno)s]: %(message)s\"\n    else:\n        level = logging.DEBUG\n        fmt = \"%(module)s [%(lineno)s]: %(levelname)s: %(message)s\"\n    logging.basicConfig(format=fmt, level=level)", "response": "Set up logging the way I like it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef closeEvent(self, event):\n\n        self.save_config(self.gui_settings['gui_settings'])\n        self.script_thread.quit()\n        self.read_probes.quit()\n        event.accept()\n\n        print('\\n\\n======================================================')\n        print('================= Closing B26 Python LAB =============')\n        print('======================================================\\n\\n')", "response": "This function is called when the GUI closes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the filename to which the probe logging function will write", "response": "def set_probe_file_name(self, checked):\n        \"\"\"\n        sets the filename to which the probe logging function will write\n        Args:\n            checked: boolean (True: opens file) (False: closes file)\n        \"\"\"\n        if checked:\n            file_name = os.path.join(self.gui_settings['probes_log_folder'], '{:s}_probes.csv'.format(datetime.datetime.now().strftime('%y%m%d-%H_%M_%S')))\n            if os.path.isfile(file_name) == False:\n                self.probe_file = open(file_name, 'a')\n                new_values = self.read_probes.probes_values\n                header = ','.join(list(np.array([['{:s} ({:s})'.format(p, instr) for p in list(p_dict.keys())] for instr, p_dict in new_values.items()]).flatten()))\n                self.probe_file.write('{:s}\\n'.format(header))\n        else:\n            self.probe_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef switch_tab(self):\n        current_tab = str(self.tabWidget.tabText(self.tabWidget.currentIndex()))\n        if self.current_script is None:\n            if current_tab == 'Probes':\n                self.read_probes.start()\n                self.read_probes.updateProgress.connect(self.update_probes)\n            else:\n                try:\n                    self.read_probes.updateProgress.disconnect()\n                    self.read_probes.quit()\n                except TypeError:\n                    pass\n\n            if current_tab == 'Instruments':\n                self.refresh_instruments()\n\n        else:\n            self.log('updating probes / instruments disabled while script is running!')", "response": "switches the current tab to the next tab"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrefreshes the instruments for the current tree level and all children.", "response": "def refresh_instruments(self):\n        \"\"\"\n        if self.tree_settings has been expanded, ask instruments for their actual values\n        \"\"\"\n        def list_access_nested_dict(dict, somelist):\n            \"\"\"\n            Allows one to use a list to access a nested dictionary, for example:\n            listAccessNestedDict({'a': {'b': 1}}, ['a', 'b']) returns 1\n            Args:\n                dict:\n                somelist:\n\n            Returns:\n\n            \"\"\"\n            return reduce(operator.getitem, somelist, dict)\n\n        def update(item):\n            if item.isExpanded():\n                for index in range(item.childCount()):\n                    child = item.child(index)\n\n                    if child.childCount() == 0:\n                        instrument, path_to_instrument = child.get_instrument()\n                        path_to_instrument.reverse()\n                        try: #check if item is in probes\n                            value = instrument.read_probes(path_to_instrument[-1])\n                        except AssertionError: #if item not in probes, get value from settings instead\n                            value = list_access_nested_dict(instrument.settings, path_to_instrument)\n                        child.value = value\n                    else:\n                        update(child)\n\n        #need to block signals during update so that tree.itemChanged doesn't fire and the gui doesn't try to\n        #reupdate the instruments to their current value\n        self.tree_settings.blockSignals(True)\n\n        for index in range(self.tree_settings.topLevelItemCount()):\n            instrument = self.tree_settings.topLevelItem(index)\n            update(instrument)\n\n        self.tree_settings.blockSignals(False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log(self, msg):\n\n        time = self.get_time()\n\n        msg = \"{:s}\\t {:s}\".format(time, msg)\n\n        self.history.append(msg)\n        self.history_model.insertRow(0, QtGui.QStandardItem(msg))", "response": "log function logs the message to the history model"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_figures(self):\n\n\n        try:\n            self.horizontalLayout_14.removeWidget(self.matplotlibwidget_1)\n            self.matplotlibwidget_1.close()\n        except AttributeError:\n            pass\n        try:\n            self.horizontalLayout_15.removeWidget(self.matplotlibwidget_2)\n            self.matplotlibwidget_2.close()\n        except AttributeError:\n            pass\n        self.matplotlibwidget_2 = MatplotlibWidget(self.plot_2)\n        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Expanding)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.matplotlibwidget_2.sizePolicy().hasHeightForWidth())\n        self.matplotlibwidget_2.setSizePolicy(sizePolicy)\n        self.matplotlibwidget_2.setMinimumSize(QtCore.QSize(200, 200))\n        self.matplotlibwidget_2.setObjectName(\"matplotlibwidget_2\")\n        self.horizontalLayout_16.addWidget(self.matplotlibwidget_2)\n        self.matplotlibwidget_1 = MatplotlibWidget(self.plot_1)\n        self.matplotlibwidget_1.setMinimumSize(QtCore.QSize(200, 200))\n        self.matplotlibwidget_1.setObjectName(\"matplotlibwidget_1\")\n        self.horizontalLayout_15.addWidget(self.matplotlibwidget_1)\n\n        self.matplotlibwidget_1.mpl_connect('button_press_event', self.plot_clicked)\n        self.matplotlibwidget_2.mpl_connect('button_press_event', self.plot_clicked)\n\n        # adds a toolbar to the plots\n        self.mpl_toolbar_1 = NavigationToolbar(self.matplotlibwidget_1.canvas, self.toolbar_space_1)\n        self.mpl_toolbar_2 = NavigationToolbar(self.matplotlibwidget_2.canvas, self.toolbar_space_2)\n        self.horizontalLayout_9.addWidget(self.mpl_toolbar_2)\n        self.horizontalLayout_14.addWidget(self.mpl_toolbar_1)\n\n        self.matplotlibwidget_1.figure.set_tight_layout(True)\n        self.matplotlibwidget_2.figure.set_tight_layout(True)", "response": "Creates the maplotlib figures for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef btn_clicked(self):\n        sender = self.sender()\n        self.probe_to_plot = None\n\n        def start_button():\n            \"\"\"\n            starts the selected script\n            \"\"\"\n            item = self.tree_scripts.currentItem()\n\n            # BROKEN 20170109: repeatedly erases updates to gui\n            # self.expanded_items = []\n            # for index in range(self.tree_scripts.topLevelItemCount()):\n            #     someitem = self.tree_scripts.topLevelItem(index)\n            #     if someitem.isExpanded():\n            #         self.expanded_items.append(someitem.name)\n            self.script_start_time = datetime.datetime.now()\n\n\n            if item is not None:\n                # get script and update settings from tree\n                self.running_item = item\n                script, path_to_script, script_item = item.get_script()\n\n                self.update_script_from_item(script_item)\n\n                self.log('starting {:s}'.format(script.name))\n\n                # put script onto script thread\n                print('================================================')\n                print(('===== starting {:s}'.format(script.name)))\n                print('================================================')\n                script_thread = self.script_thread\n\n                def move_to_worker_thread(script):\n\n                    script.moveToThread(script_thread)\n\n                    # move also the subscript to the worker thread\n                    for subscript in list(script.scripts.values()):\n                        move_to_worker_thread(subscript)\n\n                move_to_worker_thread(script)\n\n                script.updateProgress.connect(self.update_status) # connect update signal of script to update slot of gui\n                script_thread.started.connect(script.run) # causes the script to start upon starting the thread\n                script.finished.connect(script_thread.quit)  # clean up. quit thread after script is finished\n                script.finished.connect(self.script_finished) # connect finished signal of script to finished slot of gui\n\n                # start thread, i.e. script\n                script_thread.start()\n\n                self.current_script = script\n                self.btn_start_script.setEnabled(False)\n                # self.tabWidget.setEnabled(False)\n\n                if isinstance(self.current_script, ScriptIterator):\n                    self.btn_skip_subscript.setEnabled(True)\n\n\n            else:\n                self.log('User tried to run a script without one selected.')\n\n        def stop_button():\n            \"\"\"\n            stops the current script\n            \"\"\"\n            if self.current_script is not None and self.current_script.is_running:\n                self.current_script.stop()\n            else:\n                self.log('User clicked stop, but there isn\\'t anything running...this is awkward. Re-enabling start button anyway.')\n            self.btn_start_script.setEnabled(True)\n\n        def skip_button():\n            \"\"\"\n            Skips to the next script if the current script is a Iterator script\n            \"\"\"\n            if self.current_script is not None and self.current_script.is_running and isinstance(self.current_script,\n                                                                                                 ScriptIterator):\n                self.current_script.skip_next()\n            else:\n                self.log('User clicked skip, but there isn\\'t a iterator script running...this is awkward.')\n\n        def validate_button():\n            \"\"\"\n            validates the selected script\n            \"\"\"\n            item = self.tree_scripts.currentItem()\n\n            if item is not None:\n                script, path_to_script, script_item = item.get_script()\n                self.update_script_from_item(script_item)\n                script.is_valid()\n                script.plot_validate([self.matplotlibwidget_1.figure, self.matplotlibwidget_2.figure])\n                self.matplotlibwidget_1.draw()\n                self.matplotlibwidget_2.draw()\n\n        def store_script_data():\n            \"\"\"\n            updates the internal self.data_sets with selected script and updates tree self.fill_dataset_tree\n            \"\"\"\n            item = self.tree_scripts.currentItem()\n            if item is not None:\n                script, path_to_script, _ = item.get_script()\n                script_copy = script.duplicate()\n                time_tag = script.start_time.strftime('%y%m%d-%H_%M_%S')\n\n                self.data_sets.update({time_tag : script_copy})\n\n                self.fill_dataset_tree(self.tree_dataset, self.data_sets)\n\n        def save_data():\n            \"\"\"\"\n            saves the selected script (where is contained in the script itself)\n            \"\"\"\n            indecies = self.tree_dataset.selectedIndexes()\n            model = indecies[0].model()\n            rows = list(set([index.row()for index in indecies]))\n\n            for row in rows:\n                time_tag = str(model.itemFromIndex(model.index(row, 0)).text())\n                name_tag = str(model.itemFromIndex(model.index(row, 1)).text())\n                path = self.gui_settings['data_folder']\n                script = self.data_sets[time_tag]\n                script.update({'tag' : name_tag, 'path': path})\n                script.save_data()\n                script.save_image_to_disk()\n                script.save_b26()\n                script.save_log()\n\n        def delete_data():\n            \"\"\"\n            deletes the data from the dataset\n            Returns:\n            \"\"\"\n            indecies = self.tree_dataset.selectedIndexes()\n            model = indecies[0].model()\n            rows = list(set([index.row()for index in indecies]))\n\n            for row in rows:\n                time_tag = str(model.itemFromIndex(model.index(row, 0)).text())\n                del self.data_sets[time_tag]\n\n                model.removeRows(row,1)\n\n        def load_probes():\n            \"\"\"\n            opens file dialog to load probes into gui\n            \"\"\"\n\n            # if the probe has never been started it can not be disconnected so we catch that error\n            try:\n                self.read_probes.updateProgress.disconnect()\n                self.read_probes.quit()\n                # self.read_probes.stop()\n            except RuntimeError:\n                pass\n            dialog = LoadDialogProbes(probes_old=self.probes, filename=self.gui_settings['probes_folder'])\n            if dialog.exec_():\n                self.gui_settings['probes_folder'] = str(dialog.txt_probe_log_path.text())\n                probes = dialog.get_values()\n                added_instruments = list(set(probes.keys()) - set(self.probes.keys()))\n                removed_instruments = list(set(self.probes.keys()) - set(probes.keys()))\n                # create instances of new probes\n                self.probes, loaded_failed, self.instruments = Probe.load_and_append(\n                    probe_dict=probes,\n                    probes={},\n                    instruments=self.instruments)\n                if not loaded_failed:\n                    print(('WARNING following probes could not be loaded', loaded_failed, len(loaded_failed)))\n\n\n                # restart the readprobes thread\n                del self.read_probes\n                self.read_probes = ReadProbes(self.probes)\n                self.read_probes.start()\n                self.tree_probes.clear() # clear tree because the probe might have changed\n                self.read_probes.updateProgress.connect(self.update_probes)\n                self.tree_probes.expandAll()\n\n        def load_instruments():\n            \"\"\"\n            opens file dialog to load instruments into gui\n            \"\"\"\n            if 'instrument_folder' in self.gui_settings:\n                dialog = LoadDialog(elements_type=\"instruments\", elements_old=self.instruments,\n                                    filename=self.gui_settings['instrument_folder'])\n\n            else:\n                dialog = LoadDialog(elements_type=\"instruments\", elements_old=self.instruments)\n\n            if dialog.exec_():\n                self.gui_settings['instrument_folder'] = str(dialog.txt_probe_log_path.text())\n                instruments = dialog.get_values()\n                added_instruments = set(instruments.keys()) - set(self.instruments.keys())\n                removed_instruments = set(self.instruments.keys()) - set(instruments.keys())\n                # print('added_instruments', {name: instruments[name] for name in added_instruments})\n\n                # create instances of new instruments\n                self.instruments, loaded_failed = Instrument.load_and_append(\n                    {name: instruments[name] for name in added_instruments}, self.instruments)\n                if len(loaded_failed)>0:\n                    print(('WARNING following instrument could not be loaded', loaded_failed))\n                # delete instances of new instruments/scripts that have been deselected\n                for name in removed_instruments:\n                    del self.instruments[name]\n\n        def plot_data(sender):\n            \"\"\"\n            plots the data of the selected script\n            \"\"\"\n            if sender == self.tree_dataset:\n                index = self.tree_dataset.selectedIndexes()[0]\n                model = index.model()\n                time_tag = str(model.itemFromIndex(model.index(index.row(), 0)).text())\n                script = self.data_sets[time_tag]\n                self.plot_script(script)\n            elif sender == self.tree_scripts:\n                item = self.tree_scripts.currentItem()\n                if item is not None:\n                    script, path_to_script, _ = item.get_script()\n                # only plot if script has been selected but not if a parameter has been selected\n                if path_to_script == []:\n                    self.plot_script(script)\n\n        def save():\n            self.save_config(self.gui_settings['gui_settings'])\n        if sender is self.btn_start_script:\n            start_button()\n        elif sender is self.btn_stop_script:\n            stop_button()\n        elif sender is self.btn_skip_subscript:\n            skip_button()\n        elif sender is self.btn_validate_script:\n            validate_button()\n        elif sender in (self.tree_dataset, self.tree_scripts):\n            plot_data(sender)\n        elif sender is self.btn_store_script_data:\n            store_script_data()\n        elif sender is self.btn_save_data:\n            save_data()\n        elif sender is self.btn_delete_data:\n            delete_data()\n        # elif sender is self.btn_plot_probe:\n        elif sender is self.chk_probe_plot:\n            if self.chk_probe_plot.isChecked():\n                item = self.tree_probes.currentItem()\n                if item is not None:\n                    if item.name in self.probes:\n                        #selected item is an instrument not a probe, maybe plot all the probes...\n                        self.log('Can\\'t plot, No probe selected. Select probe and try again!')\n                    else:\n                        instrument = item.parent().name\n                        self.probe_to_plot = self.probes[instrument][item.name]\n                else:\n                    self.log('Can\\'t plot, No probe selected. Select probe and try again!')\n            else:\n                self.probe_to_plot = None\n        elif sender is self.btn_save_gui:\n            # get filename\n            filepath, _ = QtWidgets.QFileDialog.getSaveFileName(self, 'Save gui settings to file', self.config_filepath, filter = '*.b26')\n\n            #in case the user cancels during the prompt, check that the filepath is not an empty string\n            if filepath:\n                filename, file_extension = os.path.splitext(filepath)\n                if file_extension != '.b26':\n                    filepath = filename + \".b26\"\n                filepath = os.path.normpath(filepath)\n                self.save_config(filepath)\n                self.gui_settings['gui_settings'] = filepath\n                self.refresh_tree(self.tree_gui_settings, self.gui_settings)\n        elif sender is self.btn_load_gui:\n            # get filename\n            fname = QtWidgets.QFileDialog.getOpenFileName(self, 'Load gui settings from file',  self.gui_settings['data_folder'], filter = '*.b26')\n            self.load_config(fname[0])\n        elif sender is self.btn_about:\n            msg = QtWidgets.QMessageBox()\n            msg.setIcon(QtWidgets.QMessageBox.Information)\n            msg.setText(\"pylabcontrol: Laboratory Equipment Control for Scientific Experiments\")\n            msg.setInformativeText(\"This software was developed by Arthur Safira, Jan Gieseler, and Aaron Kabcenell at\"\n                                   \"Harvard University. It is licensed under the LPGL licence. For more information,\"\n                                   \"visit the GitHub page at github.com/LISE-B26/pylabcontrol .\")\n            msg.setWindowTitle(\"About\")\n            # msg.setDetailedText(\"some stuff\")\n            msg.setStandardButtons(QtWidgets.QMessageBox.Ok)\n            # msg.buttonClicked.connect(msgbtn)\n            retval = msg.exec_()\n        # elif (sender is self.btn_load_instruments) or (sender is self.btn_load_scripts):\n        elif sender in (self.btn_load_instruments, self.btn_load_scripts, self.btn_load_probes):\n            if sender is self.btn_load_instruments:\n                load_instruments()\n            elif sender is self.btn_load_scripts:\n                self.load_scripts()\n            elif sender is self.btn_load_probes:\n                load_probes()\n            # refresh trees\n            self.refresh_tree(self.tree_scripts, self.scripts)\n            self.refresh_tree(self.tree_settings, self.instruments)\n        elif sender is self.actionSave:\n            self.save_config(self.gui_settings['gui_settings'])\n        elif sender is self.actionGo_to_pylabcontrol_GitHub_page:\n            webbrowser.open('https://github.com/LISE-B26/pylabcontrol')\n        elif sender is self.actionExport:\n            export_dialog = ExportDialog()\n            export_dialog.target_path.setText(self.gui_settings['scripts_folder'])\n            if self.gui_settings_hidden['scripts_source_folder']:\n                export_dialog.source_path.setText(self.gui_settings_hidden['scripts_source_folder'])\n            if export_dialog.source_path.text():\n                export_dialog.reset_avaliable(export_dialog.source_path.text())\n            #exec_() blocks while export dialog is used, subsequent code will run on dialog closing\n            export_dialog.exec_()\n            self.gui_settings.update({'scripts_folder': export_dialog.target_path.text()})\n            self.fill_treeview(self.tree_gui_settings, self.gui_settings)\n            self.gui_settings_hidden.update({'scripts_source_folder': export_dialog.source_path.text()})", "response": "Called when the button is clicked."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows or hide parameters in the tree widget.", "response": "def _show_hide_parameter(self):\n        \"\"\"\n        shows or hides parameters\n        Returns:\n\n        \"\"\"\n\n        assert isinstance(self.sender(), QtWidgets.QCheckBox), 'this function should be connected to a check box'\n\n        if self.sender().isChecked():\n            self.tree_scripts.setColumnHidden(2, False)\n            iterator = QtWidgets.QTreeWidgetItemIterator(self.tree_scripts, QtWidgets.QTreeWidgetItemIterator.Hidden)\n            item = iterator.value()\n            while item:\n                item.setHidden(False)\n                item = iterator.value()\n                iterator += 1\n        else:\n            self.tree_scripts.setColumnHidden(2, True)\n\n            iterator = QtWidgets.QTreeWidgetItemIterator(self.tree_scripts, QtWidgets.QTreeWidgetItemIterator.NotHidden)\n            item = iterator.value()\n            while item:\n                if not item.visible:\n                    item.setHidden(True)\n                item = iterator.value()\n                iterator +=1\n\n\n        self.tree_scripts.setColumnWidth(0, 200)\n        self.tree_scripts.setColumnWidth(1, 400)\n        self.tree_scripts.setColumnWidth(2, 50)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_parameters(self, treeWidget):\n\n        if treeWidget == self.tree_settings:\n\n            item = treeWidget.currentItem()\n\n\n\n            instrument, path_to_instrument = item.get_instrument()\n\n            # build nested dictionary to update instrument\n            dictator = item.value\n            for element in path_to_instrument:\n                dictator = {element: dictator}\n\n            # get old value from instrument\n            old_value = instrument.settings\n            path_to_instrument.reverse()\n            for element in path_to_instrument:\n                old_value = old_value[element]\n\n            # send new value from tree to instrument\n            instrument.update(dictator)\n\n            new_value = item.value\n            if new_value is not old_value:\n                msg = \"changed parameter {:s} from {:s} to {:s} on {:s}\".format(item.name, str(old_value),\n                                                                                str(new_value), instrument.name)\n            else:\n                msg = \"did not change parameter {:s} on {:s}\".format(item.name, instrument.name)\n\n            self.log(msg)\n        elif treeWidget == self.tree_scripts:\n\n            item = treeWidget.currentItem()\n            script, path_to_script, _ = item.get_script()\n\n            # check if changes value is from an instrument\n            instrument, path_to_instrument = item.get_instrument()\n            if instrument is not None:\n\n                new_value = item.value\n\n\n                msg = \"changed parameter {:s} to {:s} in {:s}\".format(item.name,\n                                                                                str(new_value),\n                                                                                script.name)\n            else:\n                new_value = item.value\n                msg = \"changed parameter {:s} to {:s} in {:s}\".format(item.name,\n                                                                            str(new_value),\n                                                                            script.name)\n            self.log(msg)", "response": "Updates the internal dictionary for scripts and instruments with values from the respective trees\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_script(self, script):\n\n        script.plot([self.matplotlibwidget_1.figure, self.matplotlibwidget_2.figure])\n        self.matplotlibwidget_1.draw()\n        self.matplotlibwidget_2.draw()", "response": "Calls the plot function of the script and redraws both plots\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_status(self, progress):\n\n        # interval at which the gui will be updated, if requests come in faster than they will be ignored\n        update_interval = 0.2\n\n        now = datetime.datetime.now()\n\n        if not self._last_progress_update is None and now-self._last_progress_update < datetime.timedelta(seconds=update_interval):\n            return\n\n        self._last_progress_update = now\n\n        self.progressBar.setValue(progress)\n\n        script = self.current_script\n\n        # Estimate remaining time if progress has been made\n        if progress:\n            remaining_time = str(datetime.timedelta(seconds=script.remaining_time.seconds))\n            self.lbl_time_estimate.setText('time remaining: {:s}'.format(remaining_time))\n        if script is not str(self.tabWidget.tabText(self.tabWidget.currentIndex())).lower() in ['scripts', 'instruments']:\n            self.plot_script(script)", "response": "Updates the status of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when the script is finished.", "response": "def script_finished(self):\n        \"\"\"\n        waits for the script to emit the script_finshed signal\n        \"\"\"\n        script = self.current_script\n        script.updateProgress.disconnect(self.update_status)\n        self.script_thread.started.disconnect()\n        script.finished.disconnect()\n\n        self.current_script = None\n\n        self.plot_script(script)\n        self.progressBar.setValue(100)\n        self.btn_start_script.setEnabled(True)\n        self.btn_skip_subscript.setEnabled(False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the plottype of the script and plots it accordingly", "response": "def plot_script_validate(self, script):\n        \"\"\"\n        checks the plottype of the script and plots it accordingly\n        Args:\n            script: script to be plotted\n\n        \"\"\"\n\n        script.plot_validate([self.matplotlibwidget_1.figure, self.matplotlibwidget_2.figure])\n        self.matplotlibwidget_1.draw()\n        self.matplotlibwidget_2.draw()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_probes(self, progress):\n        new_values = self.read_probes.probes_values\n        probe_count = len(self.read_probes.probes)\n\n        if probe_count > self.tree_probes.topLevelItemCount():\n            # when run for the first time, there are no probes in the tree, so we have to fill it first\n            self.fill_treewidget(self.tree_probes, new_values)\n        else:\n            for x in range(probe_count):\n                topLvlItem = self.tree_probes.topLevelItem(x)\n                for child_id in range(topLvlItem.childCount()):\n                    child = topLvlItem.child(child_id)\n                    child.value = new_values[topLvlItem.name][child.name]\n                    child.setText(1, str(child.value))\n\n        if self.probe_to_plot is not None:\n            self.probe_to_plot.plot(self.matplotlibwidget_1.axes)\n            self.matplotlibwidget_1.draw()\n\n\n        if self.chk_probe_log.isChecked():\n            data = ','.join(list(np.array([[str(p) for p in list(p_dict.values())] for instr, p_dict in new_values.items()]).flatten()))\n            self.probe_file.write('{:s}\\n'.format(data))", "response": "update the probes tree"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_script_from_item(self, item):\n\n        script, path_to_script, script_item = item.get_script()\n\n        # build dictionary\n        # get full information from script\n        dictator = list(script_item.to_dict().values())[0]  # there is only one item in the dictionary\n\n        for instrument in list(script.instruments.keys()):\n            # update instrument\n            script.instruments[instrument]['settings'] = dictator[instrument]['settings']\n            # remove instrument\n            del dictator[instrument]\n\n\n        for sub_script_name in list(script.scripts.keys()):\n            sub_script_item = script_item.get_subscript(sub_script_name)\n            self.update_script_from_item(sub_script_item)\n            del dictator[sub_script_name]\n\n        script.update(dictator)\n        # update datefolder path\n        script.data_path = self.gui_settings['data_folder']", "response": "Updates the script based on the information provided in item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_treewidget(self, tree, parameters):\n\n        tree.clear()\n        assert isinstance(parameters, (dict, Parameter))\n\n        for key, value in parameters.items():\n            if isinstance(value, Parameter):\n                B26QTreeItem(tree, key, value, parameters.valid_values[key], parameters.info[key])\n            else:\n                B26QTreeItem(tree, key, value, type(value), '')", "response": "Fills a QTreeWidget with nested parameters in future replace QTreeWidget with QTreeView and call fill_treeviewon\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_treeview(self, tree, input_dict):\n\n        tree.model().removeRows(0, tree.model().rowCount())\n\n        def add_element(item, key, value):\n            child_name = QtWidgets.QStandardItem(key)\n\n            if isinstance(value, dict):\n                for key_child, value_child in value.items():\n                    add_element(child_name, key_child, value_child)\n                item.appendRow(child_name)\n            else:\n                child_value = QtWidgets.QStandardItem(str(value))\n\n                item.appendRow([child_name, child_value])\n\n        for index, (key, value) in enumerate(input_dict.items()):\n\n            if isinstance(value, dict):\n                item = QtWidgets.QStandardItem(key)\n                for sub_key, sub_value in value.items():\n                    add_element(item, sub_key, sub_value)\n                tree.model().appendRow(item)\n            elif isinstance(value, str):\n                item = QtGui.QStandardItem(key)\n                item_value = QtGui.QStandardItem(value)\n                item_value.setEditable(True)\n                item_value.setSelectable(True)\n                tree.model().appendRow([item, item_value])", "response": "Fills a treeview with nested parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef edit_tree_item(self):\n\n        def open_path_dialog_folder(path):\n            \"\"\"\n            opens a file dialog to get the path to a file and\n            \"\"\"\n            dialog = QtWidgets.QFileDialog()\n            dialog.setFileMode(QtWidgets.QFileDialog.Directory)\n            dialog.setOption(QtWidgets.QFileDialog.ShowDirsOnly, True)\n            path = dialog.getExistingDirectory(self, 'Select a folder:', path)\n\n            return path\n\n        tree = self.sender()\n\n        if tree == self.tree_gui_settings:\n\n            index = tree.selectedIndexes()[0]\n            model = index.model()\n\n            if index.column() == 1:\n                path = model.itemFromIndex(index).text()\n                key = str(model.itemFromIndex(model.index(index.row(), 0)).text())\n                if(key == 'gui_settings'):\n                    path, _ = QtWidgets.QFileDialog.getSaveFileName(self, caption = 'Select a file:', directory = path, filter = '*.b26')\n                    if path:\n                        name, extension = os.path.splitext(path)\n                        if extension != '.b26':\n                            path = name + \".b26\"\n                else:\n                    path = str(open_path_dialog_folder(path))\n\n                if path != \"\":\n                    self.gui_settings.update({key : str(os.path.normpath(path))})\n                    self.fill_treeview(tree, self.gui_settings)", "response": "Edit the tree item with the current treeview."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrefreshing the tree with the current settings", "response": "def refresh_tree(self, tree, items):\n        \"\"\"\n        refresh trees with current settings\n        Args:\n            tree: a QtWidgets.QTreeWidget object or a QtWidgets.QTreeView object\n            items: dictionary or Parameter items with which to populate the tree\n            show_all: boolean if true show all parameters, if false only selected ones\n        \"\"\"\n\n        if tree == self.tree_scripts or tree == self.tree_settings:\n            tree.itemChanged.disconnect()\n            self.fill_treewidget(tree, items)\n            tree.itemChanged.connect(lambda: self.update_parameters(tree))\n        elif tree == self.tree_gui_settings:\n            self.fill_treeview(tree, items)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fill_dataset_tree(self, tree, data_sets):\n\n        tree.model().removeRows(0, tree.model().rowCount())\n        for index, (time, script) in enumerate(data_sets.items()):\n            name = script.settings['tag']\n            type = script.name\n\n            item_time = QtGui.QStandardItem(str(time))\n            item_name = QtGui.QStandardItem(str(name))\n            item_type = QtGui.QStandardItem(str(type))\n\n            item_time.setSelectable(False)\n            item_time.setEditable(False)\n            item_type.setSelectable(False)\n            item_type.setEditable(False)\n\n            tree.model().appendRow([item_time, item_name, item_type])", "response": "Fills the tree with data sets where datasets is a dictionary of the form\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_config(self, filepath=None):\n\n        # load config or default if invalid\n\n        def load_settings(filepath):\n            \"\"\"\n            loads a old_gui settings file (a json dictionary)\n            - path_to_file: path to file that contains the dictionary\n\n            Returns:\n                - instruments: depth 1 dictionary where keys are instrument names and values are instances of instruments\n                - scripts:  depth 1 dictionary where keys are script names and values are instances of scripts\n                - probes: depth 1 dictionary where to be decided....?\n            \"\"\"\n\n            instruments_loaded = {}\n            probes_loaded = {}\n            scripts_loaded = {}\n\n            if filepath and os.path.isfile(filepath):\n                in_data = load_b26_file(filepath)\n\n                instruments = in_data['instruments'] if 'instruments' in in_data else {}\n                scripts = in_data['scripts'] if 'scripts' in in_data else {}\n                probes = in_data['probes'] if 'probes' in in_data else {}\n\n                try:\n                    instruments_loaded, failed = Instrument.load_and_append(instruments)\n                    if len(failed) > 0:\n                        print(('WARNING! Following instruments could not be loaded: ', failed))\n\n                    scripts_loaded, failed, instruments_loaded = Script.load_and_append(\n                        script_dict=scripts,\n                        instruments=instruments_loaded,\n                        log_function=self.log,\n                        data_path=self.gui_settings['data_folder'])\n\n                    if len(failed) > 0:\n                        print(('WARNING! Following scripts could not be loaded: ', failed))\n\n                    probes_loaded, failed, instruments_loadeds = Probe.load_and_append(\n                        probe_dict=probes,\n                        probes=probes_loaded,\n                        instruments=instruments_loaded)\n\n                    self.log('Successfully loaded from previous save.')\n                except ImportError:\n                    self.log('Could not load instruments or scripts from file.')\n                    self.log('Opening with blank GUI.')\n            return instruments_loaded, scripts_loaded, probes_loaded\n\n        config = None\n\n        try:\n            config = load_b26_file(filepath)\n            config_settings = config['gui_settings']\n            if config_settings['gui_settings'] != filepath:\n                print((\n                'WARNING path to settings file ({:s}) in config file is different from path of settings file ({:s})'.format(\n                    config_settings['gui_settings'], filepath)))\n            config_settings['gui_settings'] = filepath\n        except Exception as e:\n            if filepath:\n                self.log('The filepath was invalid --- could not load settings. Loading blank GUI.')\n            config_settings = self._DEFAULT_CONFIG\n\n\n            for x in self._DEFAULT_CONFIG.keys():\n                if x in config_settings:\n                    if not os.path.exists(config_settings[x]):\n                        try:\n                            os.makedirs(config_settings[x])\n                        except Exception:\n                            config_settings[x] = self._DEFAULT_CONFIG[x]\n                            os.makedirs(config_settings[x])\n                            print(('WARNING: failed validating or creating path: set to default path'.format(config_settings[x])))\n                else:\n                    config_settings[x] = self._DEFAULT_CONFIG[x]\n                    os.makedirs(config_settings[x])\n                    print(('WARNING: path {:s} not specified set to default {:s}'.format(x, config_settings[x])))\n\n        # check if file_name is a valid filename\n        if filepath is not None and os.path.exists(os.path.dirname(filepath)):\n            config_settings['gui_settings'] = filepath\n\n        self.gui_settings = config_settings\n\n        if(config):\n            self.gui_settings_hidden = config['gui_settings_hidden']\n        else:\n            self.gui_settings_hidden['script_source_folder'] = ''\n\n        self.instruments, self.scripts, self.probes = load_settings(filepath)\n\n\n        self.refresh_tree(self.tree_gui_settings, self.gui_settings)\n        self.refresh_tree(self.tree_scripts, self.scripts)\n        self.refresh_tree(self.tree_settings, self.instruments)\n\n        self._hide_parameters(filepath)", "response": "Loads the config file and returns the config object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhides the parameters that had been hidden", "response": "def _hide_parameters(self, file_name):\n        \"\"\"\n        hide the parameters that had been hidden\n        Args:\n            file_name: config file that has the information about which parameters are hidden\n\n        \"\"\"\n        try:\n            in_data = load_b26_file(file_name)\n        except:\n            in_data = {}\n\n        def set_item_visible(item, is_visible):\n            if isinstance(is_visible, dict):\n                for child_id in range(item.childCount()):\n                    child = item.child(child_id)\n                    if child.name in is_visible:\n                        set_item_visible(child, is_visible[child.name])\n            else:\n                item.visible = is_visible\n\n        if \"scripts_hidden_parameters\" in in_data:\n            # consistency check\n            if len(list(in_data[\"scripts_hidden_parameters\"].keys())) == self.tree_scripts.topLevelItemCount():\n\n                for index in range(self.tree_scripts.topLevelItemCount()):\n                    item = self.tree_scripts.topLevelItem(index)\n                    # if item.name in in_data[\"scripts_hidden_parameters\"]:\n                    set_item_visible(item, in_data[\"scripts_hidden_parameters\"][item.name])\n            else:\n                print('WARNING: settings for hiding parameters does\\'t seem to match other settings')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the gui configuration to a file.", "response": "def save_config(self, filepath):\n        \"\"\"\n        saves gui configuration to out_file_name\n        Args:\n            filepath: name of file\n        \"\"\"\n        def get_hidden_parameter(item):\n\n            num_sub_elements = item.childCount()\n\n            if num_sub_elements == 0:\n                dictator = {item.name : item.visible}\n            else:\n                dictator = {item.name:{}}\n                for child_id in range(num_sub_elements):\n                    dictator[item.name].update(get_hidden_parameter(item.child(child_id)))\n            return dictator\n\n        try:\n            filepath = str(filepath)\n            if not os.path.exists(os.path.dirname(filepath)):\n                os.makedirs(os.path.dirname(filepath))\n\n            # build a dictionary for the configuration of the hidden parameters\n            dictator = {}\n            for index in range(self.tree_scripts.topLevelItemCount()):\n                script_item = self.tree_scripts.topLevelItem(index)\n                dictator.update(get_hidden_parameter(script_item))\n\n            dictator = {\"gui_settings\": self.gui_settings, \"gui_settings_hidden\": self.gui_settings_hidden, \"scripts_hidden_parameters\":dictator}\n\n            # update the internal dictionaries from the trees in the gui\n            for index in range(self.tree_scripts.topLevelItemCount()):\n                script_item = self.tree_scripts.topLevelItem(index)\n                self.update_script_from_item(script_item)\n\n            dictator.update({'instruments': {}, 'scripts': {}, 'probes': {}})\n\n            for instrument in self.instruments.values():\n                dictator['instruments'].update(instrument.to_dict())\n            for script in self.scripts.values():\n                dictator['scripts'].update(script.to_dict())\n\n            for instrument, probe_dict in self.probes.items():\n                dictator['probes'].update({instrument: ','.join(list(probe_dict.keys()))})\n\n            with open(filepath, 'w') as outfile:\n                json.dump(dictator, outfile, indent=4)\n\n            save_config_path = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, 'save_config.json'))\n            if os.path.isfile(save_config_path) and os.access(save_config_path, os.R_OK):\n                with open(save_config_path, 'w') as outfile:\n                    json.dump({'last_save_path': filepath}, outfile, indent=4)\n            else:\n                with io.open(save_config_path, 'w') as save_config_file:\n                    save_config_file.write(json.dumps({'last_save_path': filepath}))\n\n            self.log('Saved GUI configuration (location: {0}'.format(filepath))\n        except Exception:\n            msg = QtWidgets.QMessageBox()\n            msg.setText(\"Saving failed. Please use 'save as' to define a valid path for the gui.\")\n            msg.exec_()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_dataset(self, out_file_name):\n\n        for time_tag, script in self.data_sets.items():\n            script.save(os.path.join(out_file_name, '{:s}.b26s'.format(time_tag)))", "response": "Saves the current dataset to out_file_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    data = {'demo':{'foo': '<foo>', 'bar': ['1', '2']}}\n\n    # xml\u5199\u5165\u6587\u4ef6 \u63d0\u4f9b\u6587\u4ef6\u540d\n    lazyxml.dump(data, 'xml/dump.xml')\n\n    # xml\u5199\u5165\u6587\u4ef6 \u63d0\u4f9b\u6587\u4ef6\u53e5\u67c4\n    with open('xml/dump-fp.xml', 'w') as fp:\n        lazyxml.dump(data, fp)\n\n    # xml\u5199\u5165\u6587\u4ef6 \u63d0\u4f9b\u7c7b\u6587\u4ef6\u5bf9\u8c61\n    from cStringIO import StringIO\n    buffer = StringIO()\n    lazyxml.dump(data, buffer)\n    print buffer.getvalue()\n    # <?xml version=\"1.0\" encoding=\"utf-8\"?><demo><foo><![CDATA[1]]></foo><bar><![CDATA[2]]></bar></demo>\n    buffer.close()\n\n    # \u9ed8\u8ba4\n    print lazyxml.dumps(data)\n    # '<?xml version=\"1.0\" encoding=\"utf-8\"?><demo><foo><![CDATA[<foo>]]></foo><bar><![CDATA[1]]></bar><bar><![CDATA[2]]></bar></demo>'\n\n    # \u4e0d\u58f0\u660exml\u5934\u90e8\n    print lazyxml.dumps(data, header_declare=False)\n    # '<demo><foo><![CDATA[<foo>]]></foo><bar><![CDATA[1]]></bar><bar><![CDATA[2]]></bar></demo>'\n\n    # \u4e0d\u4f7f\u7528CDATA\u683c\u5f0f\n    print lazyxml.dumps(data, cdata=False)\n    # '<?xml version=\"1.0\" encoding=\"utf-8\"?><demo><foo>&lt;foo&gt;</foo><bar>1</bar><bar>2</bar></demo>'\n\n    # \u7f29\u8fdb\u548c\u7f8e\u89c2xml\n    print lazyxml.dumps(data, indent=' ' * 4)\n    # <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    # <demo>\n    #     <foo><![CDATA[<foo>]]></foo>\n    #     <bar><![CDATA[1]]></bar>\n    #     <bar><![CDATA[2]]></bar>\n    # </demo>\n\n    # \u4f7f\u7528\u6807\u7b7e\u540d\u79f0\u6392\u5e8f\n    print lazyxml.dumps(data, ksort=True)\n    # '<?xml version=\"1.0\" encoding=\"utf-8\"?><demo><bar><![CDATA[1]]></bar><bar><![CDATA[2]]></bar><foo><![CDATA[<foo>]]></foo></demo>'\n\n    # \u4f7f\u7528\u6807\u7b7e\u540d\u79f0\u5012\u5e8f\u6392\u5e8f\n    print lazyxml.dumps(data, ksort=True, reverse=True)\n    # '<?xml version=\"1.0\" encoding=\"utf-8\"?><demo><foo><![CDATA[<foo>]]></foo><bar><![CDATA[1]]></bar><bar><![CDATA[2]]></bar></demo>'\n\n    # \u542b\u6709\u5c5e\u6027\u7684xml\u6570\u636e\n    kw = {\n        'hasattr': True,\n        'ksort': True,\n        'indent': ' ' * 4,\n        'attrkey': ATTRKEY,\n        'valuekey': VALUEKEY\n    }\n    print lazyxml.dumps(ATTRDATA, **kw)\n    \"\"\"\n    <root a1=\"1\" a2=\"2\">\n        <test1 a=\"1\" b=\"2\" c=\"3\">\n            <normal index=\"5\" required=\"false\">\n                <bar><![CDATA[1]]></bar>\n                <bar><![CDATA[2]]></bar>\n                <foo><![CDATA[<foo-1>]]></foo>\n            </normal>\n            <repeat1 index=\"1\" required=\"false\">\n                <bar><![CDATA[1]]></bar>\n                <bar><![CDATA[2]]></bar>\n                <foo><![CDATA[<foo-1>]]></foo>\n            </repeat1>\n            <repeat1 index=\"1\" required=\"false\">\n                <bar><![CDATA[3]]></bar>\n                <bar><![CDATA[4]]></bar>\n                <foo><![CDATA[<foo-2>]]></foo>\n            </repeat1>\n            <repeat2 index=\"2\" required=\"true\"><![CDATA[1]]></repeat2>\n            <repeat2 index=\"2\" required=\"true\"><![CDATA[2]]></repeat2>\n            <repeat3 index=\"3\" required=\"true\">\n                <sub><![CDATA[1]]></sub>\n                <sub><![CDATA[2]]></sub>\n            </repeat3>\n            <repeat3 index=\"4\" required=\"true\">\n                <sub><![CDATA[1]]></sub>\n                <sub><![CDATA[2]]></sub>\n                <sub><![CDATA[3]]></sub>\n            </repeat3>\n        </test1>\n        <test2 a=\"1\" b=\"2\" c=\"3\"><![CDATA[\u6d4b\u8bd5\u7528]]></test2>\n    </root>\n    \"\"\"", "response": "main function for the XML dump"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to this peer.", "response": "def send_message(self, text: str, reply: int=None, link_preview: bool=None,\n                     on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send message to this peer.\n        :param text: Text to send.\n        :param reply: Message object or message_id to reply to.\n        :param link_preview: Whether or not to show the link preview for this message\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        self.twx.send_message(self, text=text, reply=reply, link_preview=link_preview, on_success=on_success,\n                              reply_markup=reply_markup)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nforwarding a message to this peer.", "response": "def forward_message(self, message: Message=None, on_success: callable=None):\n        \"\"\"\n        Forward message to this peer.\n        :param message: Message to forward to peer.\n        :param on_success: Callback to call when call is complete.\n        :return:\n        \"\"\"\n        self.twx.forward_message(self, message, on_success=on_success)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend photo to this peer.", "response": "def send_photo(self, photo: str, caption: str=None, reply: Message=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send photo to this peer.\n        :param photo: File path to photo to send.\n        :param caption: Caption for photo\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        self.twx.send_photo(peer=self, photo=photo, caption=caption, reply=reply, reply_markup=reply_markup,\n                            on_success=on_success)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_audio(self, audio: str, reply: Message=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send audio clip to this peer.\n        :param audio: File path to audio to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n\n        self.twx.send_audio(peer=self, audio=audio, reply_to_message_id=reply, on_success=on_success,\n                            reply_markup=reply_markup)", "response": "Send audio clip to this peer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a document to this peer.", "response": "def send_document(self, document: str, reply: Message=None, on_success: callable=None,\n                      reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send document to this peer.\n        :param document: File path to document to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n\n        self.twx.send_document(peer=self, document=document, reply_to_message_id=reply, on_success=on_success,\n                               reply_markup=reply_markup)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_sticker(self, sticker: str, reply: Message=None, on_success: callable=None,\n                     reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send sticker to this peer.\n        :param sticker: File path to sticker to send.\n        :param reply: Message object.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n\n        self.twx.send_sticker(peer=self, sticker=sticker, reply_to_message_id=reply, on_success=on_success,\n                              reply_markup=reply_markup)", "response": "Send a sticker to this peer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_video(self, video: str, reply: Message=None,\n                   on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send video to this peer.\n        :param video: File path to video to send.\n        :param reply: Message object.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n\n        self.twx.send_video(peer=self, video=video, reply_to_message_id=reply, on_success=on_success,\n                            reply_markup=reply_markup)", "response": "Send a video to this peer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_location(self, latitude: float, longitude: float, reply: Message=None,\n                      on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send location to this peer.\n        :param latitude: Latitude of the location.\n        :param longitude: Longitude of the location.\n        :param reply: Message object.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n\n        self.twx.send_location(peer=self, latitude=latitude, longitude=longitude,\n                               reply_to_message_id=reply, on_success=on_success, reply_markup=reply_markup)", "response": "Send a location to this peer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_message(self, peer: Peer, text: str, reply: int=None, link_preview: bool=None, on_success: callable=None):\n        pass", "response": "Send a message to a peer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends photo to peer.", "response": "def send_photo(self, peer: Peer, photo: str, caption: str=None, reply: int=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send photo to peer.\n        :param peer: Peer to send message to.\n        :param photo: File path to photo to send.\n        :param caption: Caption for photo\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending audio clip to peer.", "response": "def send_audio(self, peer: Peer, audio: str, reply: int=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send audio clip to peer.\n        :param peer: Peer to send message to.\n        :param audio: File path to audio to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a document to a peer.", "response": "def send_document(self, peer: Peer, document: str, reply: int=None, on_success: callable=None,\n                      reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send document to peer.\n        :param peer: Peer to send message to.\n        :param document: File path to document to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a sticker to a peer.", "response": "def send_sticker(self, peer: Peer, sticker: str, reply: int=None, on_success: callable=None,\n                     reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send sticker to peer.\n        :param peer: Peer to send message to.\n        :param sticker: File path to sticker to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_video(self, peer: Peer, video: str, reply: int=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send video to peer.\n        :param peer: Peer to send message to.\n        :param video: File path to video to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        pass", "response": "Send video to peer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_location(self, peer: Peer, latitude: float, longitude: float, reply: int=None, on_success: callable=None,\n                      reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send location to peer.\n        :param peer: Peer to send message to.\n        :param latitude: Latitude of the location.\n        :param longitude: Longitude of the location.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        pass", "response": "Send location to peer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_contact(self, phone_number: str, first_name: str, last_name: str=None, on_success: callable=None):\n        pass", "response": "Add a contact to the current contact list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a message to a peer.", "response": "def send_message(self, peer: Peer, text: str, reply: int=None, link_preview: bool=None,\n                     on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send message to peer.\n        :param peer: Peer to send message to.\n        :param text: Text to send.\n        :param reply: Message object or message_id to reply to.\n        :param link_preview: Whether or not to show the link preview for this message\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        if isinstance(reply, Message):\n            reply = reply.id\n\n        botapi.send_message(chat_id=peer.id, text=text, disable_web_page_preview=not link_preview,\n                            reply_to_message_id=reply, on_success=on_success, reply_markup=reply_markup,\n                            **self.request_args).run()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward_message(self, peer: Peer, message: Message, on_success: callable=None):\n        botapi.forward_message(peer.id, message.sender.id, message.id, **self.request_args).run()", "response": "Use this method to forward messages of any kind."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend photo to peer.", "response": "def send_photo(self, peer: Peer, photo: str, caption: str=None, reply: int=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send photo to peer.\n        :param peer: Peer to send message to.\n        :param photo: File path to photo to send.\n        :param caption: Caption for photo\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        if isinstance(reply, Message):\n            reply = reply.id\n\n        photo = botapi.InputFile('photo', botapi.InputFileInfo(photo, open(photo, 'rb'), get_mimetype(photo)))\n\n        botapi.send_photo(chat_id=peer.id, photo=photo, caption=caption, reply_to_message_id=reply, on_success=on_success,\n                          reply_markup=reply_markup, **self.request_args).run()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends audio clip to peer.", "response": "def send_audio(self, peer: Peer, audio: str, reply: int=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send audio clip to peer.\n        :param peer: Peer to send message to.\n        :param audio: File path to audio to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        if isinstance(reply, Message):\n            reply = reply.id\n\n        audio = botapi.InputFile('audio', botapi.InputFileInfo(audio, open(audio, 'rb'), get_mimetype(audio)))\n\n        botapi.send_audio(chat_id=peer.id, audio=audio, reply_to_message_id=reply, on_success=on_success,\n                          reply_markup=reply_markup, **self.request_args).run()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a document to a peer.", "response": "def send_document(self, peer: Peer, document: str, reply: int=None, on_success: callable=None,\n                      reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send document to peer.\n        :param peer: Peer to send message to.\n        :param document: File path to document to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        if isinstance(reply, Message):\n            reply = reply.id\n\n        document = botapi.InputFile('document', botapi.InputFileInfo(document, open(document, 'rb'),\n                                                                     get_mimetype(document)))\n\n        botapi.send_document(chat_id=peer.id, document=document, reply_to_message_id=reply, on_success=on_success,\n                             reply_markup=reply_markup, **self.request_args).run()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a sticker to a peer.", "response": "def send_sticker(self, peer: Peer, sticker: str, reply: int=None, on_success: callable=None,\n                     reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send sticker to peer.\n        :param peer: Peer to send message to.\n        :param sticker: File path to sticker to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        if isinstance(reply, Message):\n            reply = reply.id\n\n        sticker = botapi.InputFile('sticker', botapi.InputFileInfo(sticker, open(sticker, 'rb'),\n                                                                   get_mimetype(sticker)))\n\n        botapi.send_sticker(chat_id=peer.id, sticker=sticker, reply_to_message_id=reply, on_success=on_success,\n                            reply_markup=reply_markup, **self.request_args).run()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_video(self, peer: Peer, video: str, reply: int=None,\n                   on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send video to peer.\n        :param peer: Peer to send message to.\n        :param video: File path to video to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        if isinstance(reply, Message):\n            reply = reply.id\n\n        video = botapi.InputFile('video', botapi.InputFileInfo(video, open(video, 'rb'),\n                                                               get_mimetype(video)))\n\n        botapi.send_video(chat_id=peer.id, video=video, reply_to_message_id=reply, on_success=on_success,\n                          reply_markup=reply_markup, **self.request_args).run()", "response": "Send a video to a peer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_location(self, peer: Peer, latitude: float, longitude: float, reply: int=None,\n                      on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send location to peer.\n        :param peer: Peer to send message to.\n        :param latitude: Latitude of the location.\n        :param longitude: Longitude of the location.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n        if isinstance(reply, Message):\n            reply = reply.id\n\n        botapi.send_location(chat_id=peer.id, latitude=latitude, longitude=longitude,\n                             reply_to_message_id=reply, on_success=on_success, reply_markup=reply_markup,\n                             **self.request_args).run()", "response": "Send location to peer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending chat action to peer.", "response": "def send_chat_action(self, peer: Peer, action: botapi.ChatAction, on_success: callable=None):\n        \"\"\"\n        Send status to peer.\n        :param peer: Peer to send status to.\n        :param action: Type of action to send to peer.\n        :param on_success: Callback to call when call is complete.\n\n        \"\"\"\n        botapi.send_chat_action(chat_id=peer.id, action=action, on_success=on_success, **self.request_args).run()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef message_search(self, text, on_success, peer=None, min_date=None, max_date=None, max_id=None, offset=0, limit=255):\n        raise TWXUnsupportedMethod()", "response": "Search for messages in the local cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if value exists into object.", "response": "def check(self, pointer, expected, raise_onerror=False):\n        \"\"\"Check if value exists into object.\n\n        :param pointer: the path to search in\n        :param expected: the expected value\n        :param raise_onerror: should raise on error?\n        :return: boolean\n        \"\"\"\n        obj = self.document\n        for token in Pointer(pointer):\n            try:\n                obj = token.extract(obj, bypass_ref=True)\n            except ExtractError as error:\n                if raise_onerror:\n                    raise Error(*error.args)\n                logger.exception(error)\n                return False\n        return obj == expected"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove element from sequence member from mapping.", "response": "def remove(self, pointer):\n        \"\"\"Remove element from sequence, member from mapping.\n\n        :param pointer: the path to search in\n        :return: resolved document\n        :rtype: Target\n        \"\"\"\n        doc = deepcopy(self.document)\n        parent, obj = None, doc\n        try:\n            # fetching\n            for token in Pointer(pointer):\n                parent, obj = obj, token.extract(obj, bypass_ref=True)\n\n            # removing\n            if isinstance(parent, Mapping):\n                del parent[token]\n\n            if isinstance(parent, MutableSequence):\n                parent.pop(int(token))\n        except Exception as error:\n            raise Error(*error.args)\n\n        return Target(doc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, pointer, value):\n        doc = deepcopy(self.document)\n        parent, obj = None, doc\n        try:\n            for token in Pointer(pointer):\n                parent, obj = obj, token.extract(obj, bypass_ref=True)\n            else:\n                if isinstance(parent, MutableSequence):\n                    raise OutOfRange(parent)\n                if isinstance(parent, Mapping):\n                    raise OutOfBounds(parent)\n                raise Error('already setted')\n        except (OutOfBounds, OutOfRange, LastElement) as error:\n            if not token.last:\n                raise NonexistentTarget(obj)\n            value = deepcopy(value)\n            if isinstance(error, OutOfBounds):\n                error.obj[str(token)] = value\n            elif isinstance(error, OutOfRange):\n                error.obj.insert(int(token), value)\n            elif isinstance(error, LastElement):\n                error.obj.append(value)\n\n        return Target(doc)", "response": "Adds a new value to the sequence at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move(self, dest, src):\n\n        doc = deepcopy(self.document)\n\n        # delete\n        parent, fragment = None, doc\n        for token in Pointer(src):\n            parent, fragment = fragment, token.extract(fragment,\n                                                       bypass_ref=True)\n\n        if isinstance(parent, Mapping):\n            del parent[token]\n\n        if isinstance(parent, MutableSequence):\n            parent.pop(int(token))\n\n        # insert\n        return Target(doc).add(dest, fragment)", "response": "Move element from sequence member from mapping."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying element from sequence member from mapping.", "response": "def copy(self, dest, src):\n        \"\"\"Copy element from sequence, member from mapping.\n\n        :param dest: the destination\n        :type dest: Pointer\n        :param src: the source\n        :type src: Pointer\n        :return: resolved document\n        :rtype: Target\n        \"\"\"\n        doc = fragment = deepcopy(self.document)\n        for token in Pointer(src):\n            fragment = token.extract(fragment, bypass_ref=True)\n\n        return Target(doc).add(dest, fragment)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the passed address is valid False otherwise.", "response": "def validateaddress(self, address: str) -> bool:\n        \"Returns True if the passed address is valid, False otherwise.\"\n\n        try:\n            Address.from_string(address, self.network_properties)\n        except InvalidAddress:\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(data=None, id=None, message=None):\n\n        return Error(\n            data=data,\n            id=id,\n            message=message,\n        )", "response": "Create an error object from data id and message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading a file to S3 bucket with specified prefix and then rotate all files starting with the file root and extension.", "response": "def upload_rotate(file_path, s3_bucket, s3_key_prefix, aws_key=None, aws_secret=None):\n    '''\n    Upload file_path to s3 bucket with prefix\n    Ex. upload_rotate('/tmp/file-2015-01-01.tar.bz2', 'backups', 'foo.net/')\n    would upload file to bucket backups with key=foo.net/file-2015-01-01.tar.bz2\n    and then rotate all files starting with foo.net/file and with extension .tar.bz2\n    Timestamps need to be present between the file root and the extension and in the same format as strftime(\"%Y-%m-%d\").\n    Ex file-2015-12-28.tar.bz2\n    '''\n    key = ''.join([s3_key_prefix, os.path.basename(file_path)])\n    logger.debug(\"Uploading {0} to {1}\".format(file_path, key))\n    upload(file_path, s3_bucket, key, aws_access_key_id=aws_key, aws_secret_access_key=aws_secret)\n\n    file_root, file_ext = splitext(os.path.basename(file_path))\n    # strip timestamp from file_base\n    regex = '(?P<filename>.*)-(?P<year>[\\d]+?)-(?P<month>[\\d]+?)-(?P<day>[\\d]+?)'\n    match = re.match(regex, file_root)\n    if not match:\n        raise Exception('File does not contain a timestamp')\n    key_prefix = ''.join([s3_key_prefix, match.group('filename')])\n    logger.debug('Rotating files on S3 with key prefix {0} and extension {1}'.format(key_prefix, file_ext))\n    rotate(key_prefix, file_ext, s3_bucket, aws_key=aws_key, aws_secret=aws_secret)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chunker(l, n):\n    for i in ranger(0, len(l), n):\n        yield l[i:i + n]", "response": "Generates n - sized chunks from the list l"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(self, endpoint, data, parallelism=5):\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"x-standardize-only\": \"true\" if self.standardize else \"false\",\n            \"x-include-invalid\": \"true\" if self.invalid else \"false\",\n            \"x-accept-keypair\": \"true\" if self.accept_keypair else \"false\",\n        }\n        if not self.logging:\n            headers[\"x-suppress-logging\"] = \"false\"\n\n        params = {\"auth-id\": self.auth_id, \"auth-token\": self.auth_token}\n        url = self.BASE_URL + endpoint\n\n        rs = (\n            grequests.post(\n                url=url,\n                data=json.dumps(stringify(data_chunk)),\n                params=params,\n                headers=headers,\n            )\n            for data_chunk in chunker(data, 100)\n        )\n\n        responses = grequests.imap(rs, size=parallelism)\n        status_codes = {}\n        addresses = AddressCollection([])\n        for response in responses:\n            if response.status_code not in status_codes.keys():\n                status_codes[response.status_code] = 1\n            else:\n                status_codes[response.status_code] += 1\n\n            if response.status_code == 200:\n                addresses[0:0] = AddressCollection(\n                    response.json()\n                )  # Fast list insertion\n\n            # If an auth error is raised, it's safe to say that this is\n            # going to affect every request, so raise the exception immediately..\n            elif response.status_code == 401:\n                raise ERROR_CODES[401]\n\n        # The return value or exception is simple if it is consistent.\n        if len(status_codes.keys()) == 1:\n            if 200 in status_codes:\n                return addresses, status_codes\n\n            else:\n                raise ERROR_CODES.get(status_codes.keys()[0], SmartyStreetsError)\n\n        # For any other mix not really sure of the best way to handle it. If it's a mix of 200\n        # and error codes, then returning the resultant addresses and status code dictionary\n        # seems pretty sensible. But if it's a mix of all error codes (could be a mix of payment\n        # error, input error, potentially server error) this will probably require careful\n        # checking in the code using this interface.\n        return addresses, status_codes", "response": "Executes the POST request for the given endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    input_tif = \"../tests/data/Jamaica_dem.tif\"\n    rst = RasterUtilClass.read_raster(input_tif)\n    # metadata information\n    print(\"rows: %d, cols: %d\" % (rst.nRows, rst.nCols))\n    print(\"LLCornerX: %.2f, LLCornerY: %.2f\" % (rst.xMin, rst.yMin))\n    print(\"cell size: %.1f\" % rst.dx)\n    # basic statistics, nodata is excluded\n    print(\"mean: %.2f, max: %.2f, min: %.2f\" % (rst.get_average(), rst.get_max(), rst.get_min()))\n    print(\"std: %.2f, sum: %.2f\" % (rst.get_std(), rst.get_sum()))", "response": "Read GeoTiff raster data and print statistics."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cache_init(self):\n        cache_ = cache.get(self.CACHE_KEY)\n        if cache_ is None:\n            cache_ = defaultdict(dict)\n        self._cache = cache_", "response": "Initializes local cache from Django cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the contents of a static block.", "response": "def get_contents_static(self, block_alias, context):\n        \"\"\"Returns contents of a static block.\"\"\"\n\n        if 'request' not in context:\n            # No use in further actions as we won't ever know current URL.\n            return ''\n\n        current_url = context['request'].path\n\n        # Resolve current view name to support view names as block URLs.\n        try:\n            resolver_match = resolve(current_url)\n            namespace = ''\n            if resolver_match.namespaces:\n                # More than one namespace, really? Hmm.\n                namespace = resolver_match.namespaces[0]\n            resolved_view_name = ':%s:%s' % (namespace, resolver_match.url_name)\n        except Resolver404:\n            resolved_view_name = None\n\n        self._cache_init()\n\n        cache_entry_name = cache_get_key(block_alias)\n\n        siteblocks_static = self._cache_get(cache_entry_name)\n        if not siteblocks_static:\n            blocks = Block.objects.filter(alias=block_alias, hidden=False).only('url', 'contents')\n            siteblocks_static = [defaultdict(list), defaultdict(list)]\n            for block in blocks:\n                if block.url == '*':\n                    url_re = block.url\n                elif block.url.startswith(':'):\n                    url_re = block.url\n                    # Normalize URL name to include namespace.\n                    if url_re.count(':') == 1:\n                        url_re = ':%s' % url_re\n                else:\n                    url_re = re.compile(r'%s' % block.url)\n\n                if block.access_guest:\n                    siteblocks_static[self.IDX_GUEST][url_re].append(block.contents)\n                elif block.access_loggedin:\n                    siteblocks_static[self.IDX_AUTH][url_re].append(block.contents)\n                else:\n                    siteblocks_static[self.IDX_GUEST][url_re].append(block.contents)\n                    siteblocks_static[self.IDX_AUTH][url_re].append(block.contents)\n\n            self._cache_set(cache_entry_name, siteblocks_static)\n\n        self._cache_save()\n\n        user = getattr(context['request'], 'user', None)\n\n        is_authenticated = getattr(user, 'is_authenticated', False)\n\n        if not DJANGO_2:\n            is_authenticated = is_authenticated()\n\n        if is_authenticated:\n            lookup_area = siteblocks_static[self.IDX_AUTH]\n        else:\n            lookup_area = siteblocks_static[self.IDX_GUEST]\n\n        static_block_contents = ''\n        if '*' in lookup_area:\n            static_block_contents = choice(lookup_area['*'])\n        elif resolved_view_name in lookup_area:\n            static_block_contents = choice(lookup_area[resolved_view_name])\n        else:\n            for url, contents in lookup_area.items():\n                if url.match(current_url):\n                    static_block_contents = choice(contents)\n                    break\n\n        return static_block_contents"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the contents of a dynamic block.", "response": "def get_contents_dynamic(self, block_alias, context):\n        \"\"\"Returns contents of a dynamic block.\"\"\"\n        dynamic_block = get_dynamic_blocks().get(block_alias, [])\n        if not dynamic_block:\n            return ''\n\n        dynamic_block = choice(dynamic_block)\n        return dynamic_block(block_alias=block_alias, block_context=context)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _hash_full(self, leaves, l_idx, r_idx):\n        width = r_idx - l_idx\n        if width < 0 or l_idx < 0 or r_idx > len(leaves):\n            raise IndexError(\"%s,%s not a valid range over [0,%s]\" % (\n                l_idx, r_idx, len(leaves)))\n        elif width == 0:\n            return self.hash_empty(), ()\n        elif width == 1:\n            leaf_hash = self.hash_leaf(leaves[l_idx])\n            return leaf_hash, (leaf_hash,)\n        else:\n            # next smallest power of 2\n            split_width = 2**((width - 1).bit_length() - 1)\n            assert split_width < width <= 2*split_width\n            l_root, l_hashes = self._hash_full(leaves, l_idx, l_idx+split_width)\n            assert len(l_hashes) == 1 # left tree always full\n            r_root, r_hashes = self._hash_full(leaves, l_idx+split_width, r_idx)\n            root_hash = self.hash_children(l_root, r_root)\n            return (root_hash, (root_hash,) if split_width*2 == width else\n                                l_hashes + r_hashes)", "response": "Hash the leaves between l_idx and r_idx."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hash_full_tree(self, leaves):\n        root_hash, hashes = self._hash_full(leaves, 0, len(leaves))\n        assert len(hashes) == count_bits_set(len(leaves))\n        assert (self._hash_fold(hashes) == root_hash if hashes else\n                root_hash == self.hash_empty())\n        return root_hash", "response": "Hash a set of leaves representing a valid full tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompiling a JSON schema with JSON Schema_ draft - 03.", "response": "def compile(schema, pointer, context, scope=None):\n    \"\"\"\n    Compiles schema with `JSON Schema`_ draft-03.\n\n    :param schema: obj to compile\n    :type schema: Mapping\n    :param pointer: uri of the schema\n    :type pointer: Pointer, str\n    :param context: context of this schema\n    :type context: Context\n\n    .. _`JSON Schema`: http://json-schema.org\n    \"\"\"\n\n    schm = deepcopy(schema)\n\n    scope = urljoin(scope or str(pointer), schm.pop('id', None))\n\n    if '$ref' in schema:\n        return ReferenceValidator(urljoin(scope, schema['$ref']), context)\n\n    attrs = {}\n\n    if 'additionalItems' in schm:\n        subpointer = pointer_join(pointer, 'additionalItems')\n        attrs['additional_items'] = schm.pop('additionalItems')\n        if isinstance(attrs['additional_items'], dict):\n            compiled = compile(attrs['additional_items'],\n                               subpointer,\n                               context,\n                               scope)\n            attrs['additional_items'] = compiled\n        elif not isinstance(attrs['additional_items'], bool):\n            raise CompilationError('wrong type for {}'.format('additional_items'), schema)  # noqa\n\n    if 'additionalProperties' in schm:\n        attrs['additional_properties'] = schm.pop('additionalProperties')\n        if isinstance(attrs['additional_properties'], dict):\n            subpointer = pointer_join(pointer, 'additionalProperties')\n            value = attrs['additional_properties']\n            attrs['additional_properties'] = compile(value,\n                                                     subpointer,\n                                                     context,\n                                                     scope)\n        elif not isinstance(attrs['additional_properties'], bool):\n            raise CompilationError('additionalProperties must be an object or boolean', schema)  # noqa\n\n    if 'dependencies' in schm:\n        attrs['dependencies'] = schm.pop('dependencies')\n        if not isinstance(attrs['dependencies'], dict):\n            raise CompilationError('dependencies must be an object', schema)\n        for key, value in attrs['dependencies'].items():\n            if isinstance(value, dict):\n                subpointer = pointer_join(pointer, 'dependencies', key)\n                attrs['dependencies'][key] = compile(value,\n                                                     subpointer,\n                                                     context,\n                                                     scope)\n            elif isinstance(value, sequence_types):\n                continue\n            elif not isinstance(value, string_types):\n                raise CompilationError('dependencies must be an array, object or string', schema)  # noqa\n\n    if 'disallow' in schm:\n        attrs['disallow'] = schm.pop('disallow')\n        if isinstance(attrs['disallow'], sequence_types):\n            for index, value in enumerate(attrs['disallow']):\n                if isinstance(value, dict):\n                    subpointer = pointer_join(pointer, 'disallow', index)\n                    attrs['disallow'][index] = compile(value,\n                                                       subpointer,\n                                                       context,\n                                                       scope)\n                elif not isinstance(value, string_types):\n                    raise CompilationError('disallow must be an object or string', schema)  # noqa\n        elif not isinstance(attrs['disallow'], string_types):\n            raise CompilationError('disallow must be an array or string', schema)  # noqa\n\n    if 'divisibleBy' in schm:\n        attrs['divisible_by'] = schm.pop('divisibleBy')\n        if not isinstance(attrs['divisible_by'], number_types):\n            raise CompilationError('divisibleBy must be a number', schema)\n\n    if 'enum' in schm:\n        attrs['enum'] = schm.pop('enum')\n        if not isinstance(attrs['enum'], sequence_types):\n            raise CompilationError('enum must be a sequence', schema)\n\n    if 'exclusiveMaximum' in schm:\n        attrs['exclusive_maximum'] = schm.pop('exclusiveMaximum')\n        if not isinstance(attrs['exclusive_maximum'], bool):\n            raise CompilationError('exclusiveMaximum must be a boolean', schema)  # noqa\n\n    if 'exclusiveMinimum' in schm:\n        attrs['exclusive_minimum'] = schm.pop('exclusiveMinimum')\n        if not isinstance(attrs['exclusive_minimum'], bool):\n            raise CompilationError('exclusiveMinimum must be a boolean', schema)  # noqa\n\n    if 'extends' in schm:\n        attrs['extends'] = schm.pop('extends')\n        subpointer = pointer_join(pointer, 'extends')\n        if isinstance(attrs['extends'], dict):\n            attrs['extends'] = compile(attrs['extends'],\n                                       subpointer,\n                                       context,\n                                       scope)\n        elif isinstance(attrs['extends'], sequence_types):\n            for index, value in enumerate(attrs['extends']):\n                attrs['extends'][index] = compile(value,\n                                                  subpointer,\n                                                  context,\n                                                  scope)\n        else:\n            raise CompilationError('extends must be an object or array', schema)  # noqa\n\n    if 'format' in schm:\n        attrs['format'] = schm.pop('format')\n        if not isinstance(attrs['format'], string_types):\n            raise CompilationError('format must be a string', schema)\n\n    if 'items' in schm:\n        subpointer = pointer_join(pointer, 'items')\n        attrs['items'] = schm.pop('items')\n        if isinstance(attrs['items'], (list, tuple)):\n            # each value must be a json schema\n            attrs['items'] = [compile(element, subpointer, context, scope) for element in attrs['items']]  # noqa\n        elif isinstance(attrs['items'], dict):\n            # value must be a json schema\n            attrs['items'] = compile(attrs['items'], subpointer, context, scope)  # noqa\n        else:\n            # should be a boolean\n            raise CompilationError('wrong type for {}'.format('items'), schema)  # noqa\n\n    if 'maximum' in schm:\n        attrs['maximum'] = schm.pop('maximum')\n        if not isinstance(attrs['maximum'], number_types):\n            raise CompilationError('enum must be an integer', schema)\n\n    if 'maxItems' in schm:\n        attrs['max_items'] = schm.pop('maxItems')\n        if not isinstance(attrs['max_items'], integer_types):\n            raise CompilationError('maxItems must be an integer', schema)\n\n    if 'maxLength' in schm:\n        attrs['max_length'] = schm.pop('maxLength')\n        if not isinstance(attrs['max_length'], integer_types):\n            raise CompilationError('maxLength must be integer', schema)\n\n    if 'minimum' in schm:\n        attrs['minimum'] = schm.pop('minimum')\n        if not isinstance(attrs['minimum'], number_types):\n            raise CompilationError('enum must be a number', schema)\n\n    if 'minItems' in schm:\n        attrs['min_items'] = schm.pop('minItems')\n        if not isinstance(attrs['min_items'], integer_types):\n            raise CompilationError('minItems must be an integer', schema)\n\n    if 'minLength' in schm:\n        attrs['min_length'] = schm.pop('minLength')\n        if not isinstance(attrs['min_length'], integer_types):\n            raise CompilationError('minLength must be integer', schema)\n\n    if 'pattern' in schm:\n        attrs['pattern'] = schm.pop('pattern')\n        if not isinstance(attrs['pattern'], string_types):\n            raise CompilationError('pattern must be a string', schema)\n\n    if 'patternProperties' in schm:\n        attrs['pattern_properties'] = schm.pop('patternProperties')\n        if not isinstance(attrs['pattern_properties'], dict):\n            raise CompilationError('patternProperties must be an object', schema)  # noqa\n        for name, value in attrs['pattern_properties'].items():\n            subpointer = pointer_join(pointer, 'patternProperties', name)\n            attrs['pattern_properties'][name] = compile(value,\n                                                        subpointer,\n                                                        context,\n                                                        scope)\n\n    if 'properties' in schm:\n        attrs['properties'] = schm.pop('properties')\n        if not isinstance(attrs['properties'], dict):\n            raise CompilationError('properties must be an object', schema)\n        for name, value in attrs['properties'].items():\n            subpointer = pointer_join(pointer, 'properties', name)\n            attrs['properties'][name] = compile(value,\n                                                subpointer,\n                                                context,\n                                                scope)\n\n    if 'required' in schm:\n        attrs['required'] = schm.pop('required')\n        if not isinstance(attrs['required'], bool):\n            raise CompilationError('required must be a boolean', schema)\n\n    if 'type' in schm:\n        attrs['type'] = schm.pop('type')\n        if isinstance(attrs['type'], sequence_types):\n            for index, value in enumerate(attrs['type']):\n                if isinstance(value, dict):\n                    subpointer = pointer_join(pointer, 'type', index)\n                    attrs['type'][index] = compile(value,\n                                                   subpointer,\n                                                   context,\n                                                   scope)\n                elif not isinstance(value, string_types):\n                    raise CompilationError('type must be an object or string', schema)  # noqa\n        elif not isinstance(attrs['type'], string_types):\n            raise CompilationError('type must be an array or string', schema)  # noqa\n\n    if 'uniqueItems' in schm:\n        attrs['unique_items'] = schm.pop('uniqueItems')\n        if not isinstance(attrs['unique_items'], bool):\n            raise CompilationError('type must be boolean', schema)\n\n    return Draft03Validator(attrs, scope, context.formats)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, obj, pointer=None):\n\n        pointer = pointer or '#'\n\n        validator = deepcopy(self)\n        validator.errors = []\n        validator.fail_fast = False\n\n        obj = deepcopy(obj)\n        obj = validator.validate_enum(obj, pointer)\n        obj = validator.validate_type(obj, pointer)\n        obj = validator.validate_disallow(obj, pointer)\n        obj = validator.validate_extends(obj, pointer)\n\n        if validator.is_array(obj):\n            obj = validator.validate_max_items(obj, pointer)\n            obj = validator.validate_min_items(obj, pointer)\n            obj = validator.validate_items(obj, pointer)\n            obj = validator.validate_unique_items(obj, pointer)\n\n        if validator.is_number(obj):\n            obj = validator.validate_maximum(obj, pointer)\n            obj = validator.validate_minimum(obj, pointer)\n            obj = validator.validate_divisible_by(obj, pointer)\n\n        if validator.is_object(obj):\n            obj = validator.validate_dependencies(obj, pointer)\n            obj = validator.validate_properties(obj, pointer)\n\n        if validator.is_string(obj):\n            obj = validator.validate_max_length(obj, pointer)\n            obj = validator.validate_min_length(obj, pointer)\n            obj = validator.validate_pattern(obj, pointer)\n            obj = validator.validate_format(obj, pointer)\n\n        if validator.errors:\n            raise ValidationError('multiple errors',\n                                  obj,\n                                  errors=validator.errors)\n\n        return obj", "response": "Validate object against the validator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_format(self, obj, pointer=None):\n\n        if 'format' in self.attrs:\n            substituted = {\n                'color': 'css.color',\n                'date-time': 'utc.datetime',\n                'date': 'utc.date',\n                'time': 'utc.time',\n                'utc-millisec': 'utc.millisec',\n                'regex': 'regex',\n                'style': 'css.style',\n                'phone': 'phone',\n                'uri': 'uri',\n                'email': 'email',\n                'ip-address': 'ipv4',\n                'ipv6': 'ipv6',\n                'host-name': 'hostname',\n            }.get(self.attrs['format'], self.attrs['format'])\n            logger.debug('use %s', substituted)\n            return self.formats[substituted](obj)\n        return obj", "response": "Validate the format attribute of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating model performance indexes.", "response": "def cal_model_performance(obsl, siml):\r\n    \"\"\"Calculate model performance indexes.\"\"\"\r\n    nse = MathClass.nashcoef(obsl, siml)\r\n    r2 = MathClass.rsquare(obsl, siml)\r\n    rmse = MathClass.rmse(obsl, siml)\r\n    pbias = MathClass.pbias(obsl, siml)\r\n    rsr = MathClass.rsr(obsl, siml)\r\n    print('NSE: %.2f, R-square: %.2f, PBIAS: %.2f%%, RMSE: %.2f, RSR: %.2f' %\r\n          (nse, r2, pbias, rmse, rsr))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_features(self):\n        # Loading all loci that\n        # are in self.loci variable defined\n        # when the pyGFE object is created\n        for loc in self.loci:\n            if self.verbose:\n                self.logger.info(self.logname + \"Loading features for \" + loc)\n\n            # Loading all features for loc from feature service\n            self.all_feats.update({loc: self.locus_features(loc)})\n\n            if self.verbose:\n                self.logger.info(self.logname + \"Finished loading features for \" + loc)\n\n        if self.verbose:\n            mem = \"{:4.4f}\".format(sys.getsizeof(self.all_feats) / 1000000)\n            self.logger.info(self.logname + \"Finished loading all features * all_feats = \" + mem + \" MB *\")", "response": "Loads all the known features from the feature service"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all features associated with a locus.", "response": "def locus_features(self, locus):\n        \"\"\"\n        Returns all features associated with a locus\n\n        :param locus: string containing HLA locus.\n        :type locus: ``str``\n        :rtype: ``dict``\n        \"\"\"\n        features = self.api.list_features(locus=locus)\n        feat_dict = {\":\".join([a.locus, str(a.rank), a.term, a.sequence]): a.accession for a in features}\n        return feat_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_gfe(self, annotation, locus):\n        features = []\n        accessions = {}\n        for feat in annotation.annotation:\n            if isinstance(annotation.annotation[feat], DBSeq) \\\n                    or isinstance(annotation.annotation[feat], Seq):\n                seq = str(annotation.annotation[feat])\n            else:\n                seq = str(annotation.annotation[feat].seq)\n\n            # TODO: Drop this if statement\n            if isutr(feat):\n                feat_str = \":\".join([locus, str(1), feat, seq])\n\n                # If the feature has been loaded or stored\n                # then use that instead of making a feature request\n                if self.verbose and self.verbosity > 2:\n                    self.logger.info(\"Getting accession \" + feat_str)\n\n                if feat_str in self.all_feats[locus]:\n\n                    if self.verbose and self.verbosity > 2:\n                        self.logger.info(\"Feature found \" + feat_str)\n\n                    accession = self.all_feats[locus][feat_str]\n                    feature = Feature(term=feat,\n                                      rank=1,\n                                      locus=locus,\n                                      sequence=seq,\n                                      accession=accession)\n                    accessions.update({feat: accession})\n                    features.append(feature)\n                else:\n                    if self.verbose and self.verbosity > 2:\n                        self.logger.info(self.logname + \"Making FeatureRequest \" + feat_str)\n\n                    # Create FeatureRequest object\n                    request = FeatureRequest(locus=locus,\n                                             term=feat,\n                                             rank=1,\n                                             sequence=seq)\n\n                    # Attempt to make feature request\n                    try:\n                        feature = self.api.create_feature(body=request)\n                        accessions.update({feat: feature.accession})\n                        features.append(feature)\n                    except ApiException as e:\n                        self.logger.error(self.logname + \"Exception when calling DefaultApi->create_feature\" + e)\n                        blank_feat = Feature(term=feat, rank=1, locus=locus,\n                                             sequence=seq)\n                        accessions.update({feat: 0})\n                        features.append(blank_feat)\n\n                    # Store new features for quick retrieval if flag passed\n                    if self.store_features:\n\n                        # Adding new feature to all_feats\n                        self.all_feats[locus].update({feat_str: feature.accession})\n\n                        # Calculating memory size of all_feats\n                        if self.verbose and self.verbosity > 1:\n                            self.logger.info(self.logname + \"Storing new feature \" + feat_str)\n                            mem = \"{:4.4f}\".format(sys.getsizeof(self.all_feats) / 1000000)\n                            self.logger.info(self.logname + \"Updated * all_feats \" + mem + \" MB *\")\n\n            else:\n                term, rank = feat.split(\"_\")\n                feat_str = \":\".join([locus, str(rank), term, seq])\n\n                # If the feature has been loaded or stored\n                # then use that instead of making a feature request\n                if feat_str in self.all_feats[locus]:\n\n                    if self.verbose and self.verbosity > 2:\n                        self.logger.info(self.logname + \"Feature found \" + feat_str)\n\n                    accession = self.all_feats[locus][feat_str]\n                    feature = Feature(term=term,\n                                      rank=rank,\n                                      locus=locus,\n                                      sequence=seq,\n                                      accession=accession)\n                    accessions.update({feat: accession})\n                    features.append(feature)\n                else:\n\n                    if self.verbose and self.verbosity > 2:\n                        self.logger.info(self.logname + \"Making FeatureRequest \" + feat_str)\n\n                    # Create FeatureRequest object\n                    request = FeatureRequest(locus=locus,\n                                             term=term,\n                                             rank=rank,\n                                             sequence=seq)\n\n                    # Attempt to make feature request\n                    try:\n                        feature = self.api.create_feature(body=request)\n                        accessions.update({feat: feature.accession})\n                        features.append(feature)\n                    except ApiException as e:\n                        self.logger.error(self.logname + \"Exception when calling DefaultApi->create_feature %e\" + e)\n                        blank_feat = Feature(term=term, rank=rank, locus=locus,\n                                             sequence=seq)\n                        accessions.update({feat: 0})\n                        features.append(blank_feat)\n\n                    # Store new features for quick retrieval if flag passed\n                    if self.store_features:\n\n                        # Adding new feature to all_feats\n                        self.all_feats[locus].update({feat_str: feature.accession})\n\n                        # Calculating memory size of all_feats\n                        if self.verbose and self.verbosity > 1:\n                            self.logger.info(self.logname + \"Storing new feature \" + feat_str)\n                            mem = \"{:4.4f}\".format(sys.getsizeof(self.all_feats) / 1000000)\n                            self.logger.info(self.logname + \"Updated * all_feats \" + mem + \" MB *\")\n\n        # Creating GFE\n        gfe = self._make_gfe(accessions, locus)\n\n        if self.verbose:\n            self.logger.info(\"GFE = \" + gfe)\n\n        return features, gfe", "response": "Creates GFE from a sequence annotation object and returns the GFE notation and the associated features in an array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _seq(self, locus, term, rank, accession):\n        try:\n            feature = self.api.get_feature_by_path(locus,\n                                                   term,\n                                                   rank,\n                                                   accession)\n            return feature\n        except ApiException as e:\n            print(\"Exception when calling DefaultApi->get_feature_by_path: %s\\n\" % e)\n            return ''", "response": "creates GFE from HLA sequence and locus"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _breakup_gfe(self, gfe):\n        [locus, feature_accessions] = gfe.split(\"w\")\n        accessions = feature_accessions.split(\"-\")\n        i = 0\n        features = {}\n        for feature_rank in self.structures[locus]:\n            accession = accessions[i]\n            features.update({feature_rank: accession})\n            i += 1\n\n        return(features)", "response": "This function creates GFE object from HLA sequence and locus and locus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_gfe(self, features, locus):\n        gfe_list = []\n        for feat in sorted(self.structures[locus],\n                           key=lambda k: self.structures[locus][k]):\n            acc = str(0)\n            if feat in features:\n                acc = str(features[feat])\n            gfe_list.append(acc)\n\n        gfea = '-'.join(gfe_list)\n        return locus + \"w\" + gfea", "response": "Creates GFE from HLA sequence and locus."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef toggle_NV(self, pt):\n        '''\n        If there is not currently a selected NV within self.settings[patch_size] of pt, adds it to the selected list. If\n        there is, removes that point from the selected list.\n        Args:\n            pt: the point to add or remove from the selected list\n        Poststate: updates selected list\n        '''\n        if not self.data['nv_locations']: #if self.data is empty so this is the first point\n            self.data['nv_locations'].append(pt)\n            self.data['image_data'] = None # clear image data\n        else:\n            # use KDTree to find NV closest to mouse click\n            tree = scipy.spatial.KDTree(self.data['nv_locations'])\n            #does a search with k=1, that is a search for the nearest neighbor, within distance_upper_bound\n            d, i = tree.query(pt,k = 1, distance_upper_bound = self.settings['patch_size'])\n\n            # removes NV if previously selected\n            if d is not np.inf:\n                self.data['nv_locations'].pop(i)\n            # adds NV if not previously selected\n            else:\n                self.data['nv_locations'].append(pt)\n\n            # randomize\n            if self.settings['randomize']:\n                self.log('warning! randomize not avalable when manually selecting points')\n\n        # if type is not free we calculate the total points of locations from the first selected points\n        if self.settings['type'] == 'square' and len(self.data['nv_locations'])>1:\n            # here we create a rectangular grid, where pts a and be define the top left and bottom right corner of the rectangle\n            Nx, Ny = self.settings['Nx'], self.settings['Ny']\n            pta = self.data['nv_locations'][0]\n            ptb = self.data['nv_locations'][1]\n            tmp  = np.array([[[pta[0] + 1.0*i*(ptb[0]-pta[0])/(Nx-1), pta[1] + 1.0*j*(ptb[1]-pta[1])/(Ny-1)] for i in range(Nx)] for j in range(Ny)])\n            nv_pts = np.reshape(tmp, (Nx * Ny, 2))\n\n            # randomize\n            if self.settings['randomize']:\n                random.shuffle(nv_pts)  # shuffles in place\n\n            self.data['nv_locations'] = nv_pts\n\n            self.stop()\n        elif self.settings['type'] == 'line' and len(self.data['nv_locations'])>1:\n            # here we create a straight line between points a and b\n            N = self.settings['Nx']\n            pta = self.data['nv_locations'][0]\n            ptb = self.data['nv_locations'][1]\n            nv_pts = [np.array([pta[0] + 1.0*i*(ptb[0]-pta[0])/(N-1), pta[1] + 1.0*i*(ptb[1]-pta[1])/(N-1)]) for i in range(N)]\n\n\n            # randomize\n            if self.settings['randomize']:\n                random.shuffle(nv_pts)  # shuffles in place\n\n            self.data['nv_locations'] = nv_pts\n\n            self.stop()\n        elif self.settings['type'] == 'ring' and len(self.data['nv_locations'])>1:\n            # here we create a circular grid, where pts a and be define the center and the outermost ring\n            Nx, Ny = self.settings['Nx'], self.settings['Ny']\n            pt_center = self.data['nv_locations'][0] # center\n            pt_outer = self.data['nv_locations'][1] # outermost ring\n            # radius of outermost ring:\n            rmax = np.sqrt((pt_center[0] - pt_outer[0]) ** 2 + (pt_center[1] - pt_outer[1]) ** 2)\n\n            # angles\n            angles = np.linspace(0, 2 * np.pi, Nx+1)[0:-1]\n            # create points on rings\n            nv_pts = []\n            for r in np.linspace(rmax, 0, Ny + 1)[0:-1]:\n                for theta in angles:\n                    nv_pts += [[r * np.sin(theta)+pt_center[0], r * np.cos(theta)+pt_center[1]]]\n\n\n\n            # randomize\n            if self.settings['randomize']:\n                coarray = list(zip(nv_pts, angles))\n                random.shuffle(coarray)  # shuffles in place\n                nv_pts, angles = zip(*coarray)\n\n            self.data['nv_locations'] = np.array(nv_pts)\n            self.data['angles'] = np.array(angles)* 180 / np.pi\n            self.data['ring_data'] = [pt_center, pt_outer]\n            self.stop()\n\n        elif self.settings['type'] == 'arc' and len(self.data['nv_locations']) > 3:\n            # here we create a circular grid, where pts a and be define the center and the outermost ring\n            Nx, Ny = self.settings['Nx'], self.settings['Ny']\n            pt_center = self.data['nv_locations'][0]  # center\n            pt_start = self.data['nv_locations'][1]  # arc point one (radius)\n            pt_dir = self.data['nv_locations'][2]  # arc point two (direction)\n            pt_end = self.data['nv_locations'][3]  # arc point three (angle)\n\n            # radius of outermost ring:\n            rmax = np.sqrt((pt_center[0] - pt_start[0]) ** 2 + (pt_center[1] - pt_start[1]) ** 2)\n            angle_start = np.arctan((pt_start[1] - pt_center[1]) / (pt_start[0] - pt_center[0]))\n            # arctan always returns between -pi/2 and pi/2, so adjust to allow full range of angles\n            if ((pt_start[0] - pt_center[0]) < 0):\n                angle_start += np.pi\n\n            angle_end = np.arctan((pt_end[1] - pt_center[1]) / (pt_end[0] - pt_center[0]))\n            # arctan always returns between -pi/2 and pi/2, so adjust to allow full range of angles\n            if ((pt_end[0] - pt_center[0]) < 0):\n                angle_end += np.pi\n\n            if pt_dir[0] < pt_start[0]:\n                # counter-clockwise: invert the order of the angles\n                angle_start, angle_end = angle_end, angle_start\n\n            if angle_start > angle_end:\n                # make sure that start is the smaller\n                # (e.g. angle_start= 180 deg and angle_end =10, we want to got from 180 to 370 deg)\n                angle_end += 2 * np.pi\n\n            # create points on arcs\n            nv_pts = []\n            for r in np.linspace(rmax, 0, Ny + 1)[0:-1]:\n                for theta in np.linspace(angle_start, angle_end, Nx, endpoint=True):\n                    nv_pts += [[r * np.cos(theta) + pt_center[0], r * np.sin(theta) + pt_center[1]]]\n\n            # randomize\n            if self.settings['randomize']:\n                coarray = list(zip(nv_pts, np.linspace(angle_start, angle_end, Nx, endpoint=True)))\n                random.shuffle(coarray)  # shuffles in place\n                nv_pts, angles = zip(*coarray)\n            else:\n                angles = np.linspace(angle_start, angle_end, Nx, endpoint=True)\n            self.data['nv_locations'] = np.array(nv_pts)\n            self.data['arc_data'] = [pt_center, pt_start, pt_end]\n            self.data['angles'] = np.array(angles) * 180 / np.pi\n            self.stop()", "response": "toggle_NV - Toggles the NV of a specific point in the selected list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tarfile_to_pif(filename, temp_root_dir='', verbose=0):\n    temp_dir = temp_root_dir + str(uuid.uuid4())\n    os.makedirs(temp_dir)\n    try:\n        tar = tarfile.open(filename, 'r')\n        tar.extractall(path=temp_dir)\n        tar.close()\n        for i in os.listdir(temp_dir):\n            cur_dir = temp_dir + '/' + i\n            if os.path.isdir(cur_dir):\n                return directory_to_pif(cur_dir, verbose=verbose)\n        return directory_to_pif(temp_dir, verbose=verbose)\n    finally:\n        shutil.rmtree(temp_dir)", "response": "Process a tar file that contains DFT data and return a PIF file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef archive_to_pif(filename, verbose=0):\n    if tarfile.is_tarfile(filename):\n        return tarfile_to_pif(filename, verbose)\n    raise Exception('Cannot process file type')", "response": "Given a file that contains output from a DFT calculation parse the data and return a PIF object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of files in a directory that contains output from a DFT calculation parse the data and return a pif object.", "response": "def files_to_pif(files, verbose=0, quality_report=True, inline=True):\n    '''Given a directory that contains output from\n    a DFT calculation, parse the data and return\n    a pif object\n\n    Input:\n        files - [str] list of files from which the parser is allowed to read.\n        verbose - int, How much status messages to print\n\n    Output:\n        pif - ChemicalSystem, Results and settings of\n            the DFT calculation in pif format\n    '''\n\n    # Look for the first parser compatible with the directory\n    found_parser = False\n    for possible_parser in [PwscfParser, VaspParser]:\n        try:\n            parser = possible_parser(files)\n            found_parser = True\n            break\n        except InvalidIngesterException:\n            # Constructors fail when they cannot find appropriate files\n            pass\n    if not found_parser:\n        raise Exception('Directory is not in correct format for an existing parser')\n    if verbose > 0:\n        print(\"Found a {} directory\".format(parser.get_name()))\n        \n    # Get information about the chemical system\n    chem = ChemicalSystem()\n    chem.chemical_formula = parser.get_composition()\n        \n    # Get software information, to list as method\n    software = Software(name=parser.get_name(),\n        version=parser.get_version_number())\n        \n    # Define the DFT method object\n    method = Method(name='Density Functional Theory',\n        software=[software])\n        \n    # Get the settings (aka. \"conditions\") of the DFT calculations\n    conditions = []\n    for name, func in parser.get_setting_functions().items():\n        # Get the condition\n        cond = getattr(parser, func)()\n\n        # If the condition is None or False, skip it\n        if cond is None:\n            continue\n\n        if inline and cond.files is not None:\n            continue\n\n        # Set the name\n        cond.name = name\n\n        # Set the types\n        conditions.append(cond)\n    \n    # Get the properties of the system\n    chem.properties = []\n    for name, func in parser.get_result_functions().items():\n        # Get the property\n        prop = getattr(parser, func)()\n        \n        # If the property is None, skip it\n        if prop is None:\n            continue\n\n        if inline and prop.files is not None:\n            continue\n\n        # Add name and other data\n        prop.name = name\n        prop.methods = [method,]\n        prop.data_type='COMPUTATIONAL'\n        if verbose > 0 and isinstance(prop, Value):\n            print(name)\n        if prop.conditions is None:\n            prop.conditions = conditions\n        else:\n            if not isinstance(prop.conditions, list):\n                prop.conditions = [prop.conditions]\n            prop.conditions.extend(conditions)\n\n        # Add it to the output\n        chem.properties.append(prop)\n\n    # Check to see if we should add the quality report\n    if quality_report and isinstance(parser, VaspParser):\n        _add_quality_report(parser, chem)\n\n    return chem"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a directory to a pif", "response": "def directory_to_pif(directory, **kwargs):\n    \"\"\"\n    Convert a directory to a pif\n    :param directory: Directory to convert to a pif\n    :param kwargs: any additional keyword arguments. (See `files_to_pif`)\n    :return: the created pif\n    \"\"\"\n\n    # Get the files\n    files = [os.path.join(directory, f) for f in os.listdir(directory)\n             if os.path.isfile(os.path.join(directory, f))]\n\n    # Run the pif\n    return files_to_pif(files, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert(files, **kwargs):\n\n    if len(files) < 1:\n        raise ValueError(\"Files needs to be a non-empty list\")\n\n    if len(files) == 1:\n        if os.path.isfile(files[0]):\n            return files_to_pif(files, **kwargs)\n        else:\n            return directory_to_pif(files[0], **kwargs)\n    else:\n        return files_to_pif([x for x in files if os.path.isfile(x)], **kwargs)", "response": "Wrap a directory to pif"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for_confirmation(provider, transaction_id):\n    'Sleep on a loop until we see a confirmation of the transaction.'\n    while(True):\n        transaction = provider.gettransaction(transaction_id)\n        if transaction[\"confirmations\"] > 0:\n            break\n        time.sleep(10)", "response": "Sleep on a loop until we see a confirmation of the transaction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating cards against deck_issue modes", "response": "def validate_card_issue_modes(issue_mode: int, cards: list) -> list:\n    \"\"\"validate cards against deck_issue modes\"\"\"\n\n    supported_mask = 63  # sum of all issue_mode values\n\n    if not bool(issue_mode & supported_mask):\n        return []  # return empty list\n\n    for i in [1 << x for x in range(len(IssueMode))]:\n        if bool(i & issue_mode):\n\n            try:\n                parser_fn = cast(\n                    Callable[[list], Optional[list]],\n                    parsers[IssueMode(i).name]\n                )\n            except ValueError:\n                continue\n\n            parsed_cards = parser_fn(cards)\n            if not parsed_cards:\n                return []\n            cards = parsed_cards\n\n    return cards"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencode deck into protobuf", "response": "def metainfo_to_protobuf(self) -> bytes:\n        '''encode deck into protobuf'''\n\n        deck = deckspawnproto()\n        deck.version = self.version\n        deck.name = self.name\n        deck.number_of_decimals = self.number_of_decimals\n        deck.issue_mode = self.issue_mode\n        if self.asset_specific_data:\n            if not isinstance(self.asset_specific_data, bytes):\n                deck.asset_specific_data = self.asset_specific_data.encode()\n            else:\n                deck.asset_specific_data = self.asset_specific_data\n\n        if deck.ByteSize() > net_query(self.network).op_return_max_bytes:\n            raise OverSizeOPReturn('''\n                        Metainfo size exceeds maximum of {max} bytes supported by this network.'''\n                                   .format(max=net_query(self.network)\n                                           .op_return_max_bytes))\n\n        return deck.SerializeToString()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode deck into dictionary", "response": "def metainfo_to_dict(self) -> dict:\n        '''encode deck into dictionary'''\n\n        r = {\n            \"version\": self.version,\n            \"name\": self.name,\n            \"number_of_decimals\": self.number_of_decimals,\n            \"issue_mode\": self.issue_mode\n        }\n\n        if self.asset_specific_data:\n            r.update({'asset_specific_data': self.asset_specific_data})\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_json(self) -> dict:\n        '''export the Deck object to json-ready format'''\n\n        d = self.__dict__\n        d['p2th_wif'] = self.p2th_wif\n        return d", "response": "export the Deck object to json - ready format"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes card_transfer info to protobuf", "response": "def metainfo_to_protobuf(self) -> bytes:\n        '''encode card_transfer info to protobuf'''\n\n        card = cardtransferproto()\n        card.version = self.version\n        card.amount.extend(self.amount)\n        card.number_of_decimals = self.number_of_decimals\n        if self.asset_specific_data:\n            if not isinstance(self.asset_specific_data, bytes):\n                card.asset_specific_data = self.asset_specific_data.encode()\n            else:\n                card.asset_specific_data = self.asset_specific_data\n\n        if card.ByteSize() > net_query(self.network).op_return_max_bytes:\n            raise OverSizeOPReturn('''\n                        Metainfo size exceeds maximum of {max} bytes supported by this network.'''\n                                   .format(max=net_query(self.network)\n                                           .op_return_max_bytes))\n\n        return card.SerializeToString()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding card into dictionary", "response": "def metainfo_to_dict(self) -> dict:\n        '''encode card into dictionary'''\n\n        r = {\n            \"version\": self.version,\n            \"amount\": self.amount,\n            \"number_of_decimals\": self.number_of_decimals\n        }\n\n        if self.asset_specific_data:\n            r.update({'asset_specific_data': self.asset_specific_data})\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sort_cards(self, cards: Generator) -> list:\n        '''sort cards by blocknum and blockseq'''\n\n        return sorted([card.__dict__ for card in cards],\n                            key=itemgetter('blocknum', 'blockseq', 'cardseq'))", "response": "sort cards by blocknum and blockseq"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads GeoTiff raster data and perform log transformation.", "response": "def main():\n    \"\"\"Read GeoTiff raster data and perform log transformation.\n    \"\"\"\n    input_tif = \"../tests/data/Jamaica_dem.tif\"\n    output_tif = \"../tests/data/tmp_results/log_dem.tif\"\n    rst = RasterUtilClass.read_raster(input_tif)\n    # raster data (with noDataValue as numpy.nan) as numpy array\n    rst_valid = rst.validValues\n    output_data = np.log(rst_valid)\n    # write output raster\n    RasterUtilClass.write_gtiff_file(output_tif, rst.nRows, rst.nCols, output_data, rst.geotrans,\n                                     rst.srs, rst.noDataValue, rst.dataType)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an instance of `val` that is of type `datatype`. keep track of exceptions so we can produce meaningful error messages.", "response": "def val_factory(val, datatypes):\n    \"\"\"\n    return an instance of `val` that is of type `datatype`.\n    keep track of exceptions so we can produce meaningful error messages.\n    \"\"\"\n    exceptions = []\n    for dt in datatypes:\n        try:\n            if isinstance(val, dt):\n                return val\n            return type_handler_object(val, dt)\n        except Exception as e:\n            exceptions.append(str(e))\n    # if we get here, we never found a valid value. raise an error\n    raise ValueError('val_factory: Unable to instantiate {val} from types {types}. Exceptions: {excs}'.\n                     format(val=val, types=datatypes, excs=exceptions))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict representation of the Event and its sub - objects . convert_datetime controls whether datetime objects are converted to strings or not", "response": "def to_dict(cls, convert_datetime=True):\n    \"\"\"\n    return a dict representation of the Event and its sub-objects\n    `convert_datetime` controls whether datetime objects are converted to strings or not\n    :rtype: dict\n    \"\"\"\n    def todict(obj):\n        \"\"\"\n        recurse the objects and represent as a dict\n        use the registered handlers if possible\n        \"\"\"\n        data = {}\n        if isinstance(obj, dict):\n            for (key, val) in obj.items():\n                data[key] = todict(val)\n            return data\n        if not convert_datetime and isinstance(obj, datetime):\n            return obj\n        elif type_handler_value(obj):\n            return type_handler_value(obj)\n        elif isinstance(obj, collections.Sequence) and not isinstance(obj, basestring):\n            return [todict(v) for v in obj]\n        elif hasattr(obj, \"__dict__\"):\n            for key, value in obj.__dict__.items():\n                if not callable(value) and not key.startswith('_'):\n                    data[key] = todict(value)\n            return data\n        else:\n            return obj\n\n    return todict(cls)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the handler for the object type", "response": "def handler_for(obj):\n    \"\"\"return the handler for the object type\"\"\"\n    for handler_type in handlers:\n        if isinstance(obj, handler_type):\n            return handlers[handler_type]\n\n    try:\n        for handler_type in handlers:\n            if issubclass(obj, handler_type):\n                return handlers[handler_type]\n    except TypeError:\n        # if obj isn't a class, issubclass will raise a TypeError\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the deserialized value from the registered type", "response": "def type_handler_object(val, objtype):\n    \"\"\"\n    return the deserialized (restored) value from the registered handler for the type\n    \"\"\"\n    handler = handlers.get(objtype)\n    if handler:\n        return handler().restore(val)\n    else:\n        return objtype(val)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests code for Dinf Tau test", "response": "def main():\n    \"\"\"Test code\"\"\"\n    import os\n    # wp = r'C:\\z_code\\subrepos\\PyGeoC\\tests\\data\\tmp_results\\wtsd_delineation'\n    wp = r'C:\\z_data_m\\SEIMS2018\\zhongtianshe_100m\\taudem_delineated'\n    dinfflowang = wp + os.sep + 'flowDirDinfTau.tif'\n    compdinffile = wp + os.sep + 'dirCodeDinfTau.tif'\n    weightfile = wp + os.sep + 'weightDinfTau.tif'\n    DinfUtil.output_compressed_dinf(dinfflowang, compdinffile, weightfile)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compress_dinf(angle, nodata):\n        if MathClass.floatequal(angle, nodata):\n            return DEFAULT_NODATA, DEFAULT_NODATA, DEFAULT_NODATA\n        taud, d = DinfUtil.check_orthogonal(angle)\n        if d != -1:\n            return taud, d, 1\n        if angle < FlowModelConst.ne:\n            a1 = angle\n            d = 129  # 1+128\n        elif angle < FlowModelConst.n:\n            a1 = angle - FlowModelConst.ne\n            d = 192  # 128+64\n        elif angle < FlowModelConst.nw:\n            a1 = angle - FlowModelConst.n\n            d = 96  # 64+32\n        elif angle < FlowModelConst.w:\n            a1 = angle - FlowModelConst.nw\n            d = 48  # 32+16\n        elif angle < FlowModelConst.sw:\n            a1 = angle - FlowModelConst.w\n            d = 24  # 16+8\n        elif angle < FlowModelConst.s:\n            a1 = angle - FlowModelConst.sw\n            d = 12  # 8+4\n        elif angle < FlowModelConst.se:\n            a1 = angle - FlowModelConst.s\n            d = 6  # 4+2\n        else:\n            a1 = angle - FlowModelConst.se\n            d = 3  # 2+1\n        return angle, d, a1 / PI * 4.0", "response": "Compress dinf flow direction to D8 direction with weight follows ArcGIS D8 codes rule"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output_compressed_dinf(dinfflowang, compdinffile, weightfile):\n        dinf_r = RasterUtilClass.read_raster(dinfflowang)\n        data = dinf_r.data\n        xsize = dinf_r.nCols\n        ysize = dinf_r.nRows\n        nodata_value = dinf_r.noDataValue\n\n        cal_dir_code = frompyfunc(DinfUtil.compress_dinf, 2, 3)\n        updated_angle, dir_code, weight = cal_dir_code(data, nodata_value)\n\n        RasterUtilClass.write_gtiff_file(dinfflowang, ysize, xsize, updated_angle,\n                                         dinf_r.geotrans, dinf_r.srs, DEFAULT_NODATA, GDT_Float32)\n        RasterUtilClass.write_gtiff_file(compdinffile, ysize, xsize, dir_code,\n                                         dinf_r.geotrans, dinf_r.srs, DEFAULT_NODATA, GDT_Int16)\n        RasterUtilClass.write_gtiff_file(weightfile, ysize, xsize, weight,\n                                         dinf_r.geotrans, dinf_r.srs, DEFAULT_NODATA, GDT_Float32)", "response": "This function will output the compressed Dinf flow direction raster file and weight to the raster file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dinf_downslope_direction(a):\n        taud, d = DinfUtil.check_orthogonal(a)\n        if d != -1:\n            down = [d]\n            return down\n        else:\n            if a < FlowModelConst.ne:  # 129 = 1+128\n                down = [1, 2]\n            elif a < FlowModelConst.n:  # 192 = 128+64\n                down = [2, 3]\n            elif a < FlowModelConst.nw:  # 96 = 64+32\n                down = [3, 4]\n            elif a < FlowModelConst.w:  # 48 = 32+16\n                down = [4, 5]\n            elif a < FlowModelConst.sw:  # 24 = 16+8\n                down = [5, 6]\n            elif a < FlowModelConst.s:  # 12 = 8+4\n                down = [6, 7]\n            elif a < FlowModelConst.se:  # 6 = 4+2\n                down = [7, 8]\n            else:  # 3 = 2+1\n                down = [8, 1]\n            return down", "response": "Get the downslope directions of an dinf value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef downstream_index_dinf(dinfdir_value, i, j):\n        down_dirs = DinfUtil.dinf_downslope_direction(dinfdir_value)\n        down_coors = []\n        for dir_code in down_dirs:\n            row, col = D8Util.downstream_index(dir_code, i, j)\n            down_coors.append([row, col])\n        return down_coors", "response": "find downslope coordinate for Dinf of TauDEM ArcGIS"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\neliminate reach with zero length and return the reach ID map. Args: streamnet_file: original stream net ESRI shapefile output_reach_file: serialized stream net, ESRI shapefile Returns: id pairs {origin: newly assigned}", "response": "def serialize_streamnet(streamnet_file, output_reach_file):\n        \"\"\"Eliminate reach with zero length and return the reach ID map.\n        Args:\n            streamnet_file: original stream net ESRI shapefile\n            output_reach_file: serialized stream net, ESRI shapefile\n\n        Returns:\n            id pairs {origin: newly assigned}\n        \"\"\"\n        FileClass.copy_files(streamnet_file, output_reach_file)\n        ds_reach = ogr_Open(output_reach_file, update=True)\n        layer_reach = ds_reach.GetLayer(0)\n        layer_def = layer_reach.GetLayerDefn()\n        i_link = layer_def.GetFieldIndex(FLD_LINKNO)\n        i_link_downslope = layer_def.GetFieldIndex(FLD_DSLINKNO)\n        i_len = layer_def.GetFieldIndex(REACH_LENGTH)\n\n        old_id_list = []\n        # there are some reaches with zero length.\n        # this program will remove these zero-length reaches\n        # output_dic is used to store the downstream reaches of these zero-length\n        # reaches\n        output_dic = {}\n        ft = layer_reach.GetNextFeature()\n        while ft is not None:\n            link_id = ft.GetFieldAsInteger(i_link)\n            reach_len = ft.GetFieldAsDouble(i_len)\n            if link_id not in old_id_list:\n                if reach_len < DELTA:\n                    downstream_id = ft.GetFieldAsInteger(i_link_downslope)\n                    output_dic[link_id] = downstream_id\n                else:\n                    old_id_list.append(link_id)\n\n            ft = layer_reach.GetNextFeature()\n        old_id_list.sort()\n\n        id_map = {}\n        for i, old_id in enumerate(old_id_list):\n            id_map[old_id] = i + 1\n        # print(id_map)\n        # change old ID to new ID\n        layer_reach.ResetReading()\n        ft = layer_reach.GetNextFeature()\n        while ft is not None:\n            link_id = ft.GetFieldAsInteger(i_link)\n            if link_id not in id_map:\n                layer_reach.DeleteFeature(ft.GetFID())\n                ft = layer_reach.GetNextFeature()\n                continue\n\n            ds_id = ft.GetFieldAsInteger(i_link_downslope)\n            ds_id = output_dic.get(ds_id, ds_id)\n            ds_id = output_dic.get(ds_id, ds_id)\n\n            ft.SetField(FLD_LINKNO, id_map[link_id])\n            if ds_id in id_map:\n                ft.SetField(FLD_DSLINKNO, id_map[ds_id])\n            else:\n                # print(ds_id)\n                ft.SetField(FLD_DSLINKNO, -1)\n            layer_reach.SetFeature(ft)\n            ft = layer_reach.GetNextFeature()\n        ds_reach.ExecuteSQL(str('REPACK reach'))\n        layer_reach.SyncToDisk()\n        ds_reach.Destroy()\n        del ds_reach\n        return id_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assign_stream_id_raster(stream_file, subbasin_file, out_stream_file):\n        stream_raster = RasterUtilClass.read_raster(stream_file)\n        stream_data = stream_raster.data\n        nrows = stream_raster.nRows\n        ncols = stream_raster.nCols\n        nodata = stream_raster.noDataValue\n        subbain_data = RasterUtilClass.read_raster(subbasin_file).data\n        nodata_array = ones((nrows, ncols)) * DEFAULT_NODATA\n        newstream_data = where((stream_data > 0) & (stream_data != nodata),\n                               subbain_data, nodata_array)\n        RasterUtilClass.write_gtiff_file(out_stream_file, nrows, ncols, newstream_data,\n                                         stream_raster.geotrans, stream_raster.srs,\n                                         DEFAULT_NODATA, GDT_Int16)", "response": "Assign stream link ID according to subbasin ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, settings):\n        '''\n        updates the internal dictionary and sends changed values to instrument\n        Args:\n            settings: parameters to be set\n        # mabe in the future:\n        # Returns: boolean that is true if update successful\n\n        '''\n        Instrument.update(self, settings)\n\n        for key, value in settings.items():\n            if key == 'test1':\n                self._internal_state = value", "response": "Updates the internal dictionary and sends changed values to instrument\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_probes(self, key):\n        assert key in list(self._PROBES.keys())\n\n        import random\n        if key == 'value1':\n            value = random.random()\n        elif key == 'value2':\n            value = self.settings['output probe2']\n        elif key == 'internal':\n            value = self._internal_state\n        elif key == 'deep_internal':\n            value = self._internal_state_deep\n\n        return value", "response": "reads the value of the requested probe from the instrument and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self, *args, **kwargs):\n        self._stop = False\n\n        super(Plant, self).start(*args, **kwargs)", "response": "start the instrument thread\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nquits the instrument thread", "response": "def quit(self, *args, **kwargs):  # real signature unknown\n        \"\"\"\n        quit the  instrument thread\n        \"\"\"\n        self.stop()\n        self._stop = True\n        self.msleep(2* int(1e3 / self.settings['update frequency']))\n        super(Plant, self).quit(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n\n        eta = self.settings['noise_strength']\n        gamma = 2 * np.pi * self.settings['noise_bandwidth']\n        dt = 1. / self.settings['update frequency']\n        control = self.settings['control']\n\n        self._state = self._output\n        while self._stop is False:\n\n            A = -gamma * dt\n\n            noise = np.sqrt(2*gamma*eta)*np.random.randn()\n            self._state *= (1. + A)\n            self._state += noise + control\n            self._output =  self._state\n\n            self.msleep(int(1e3 / self.settings['update frequency']))", "response": "This method is called by the instrument thread to run the state machine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the value of a specific key from the instrument and returns it.", "response": "def read_probes(self, key):\n        \"\"\"\n        requestes value from the instrument and returns it\n        Args:\n            key: name of requested value\n\n        Returns: reads values from instrument\n\n        \"\"\"\n        assert key in list(self._PROBES.keys())\n\n        if key == 'output':\n            value = self._output\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating PI output value for given reference input and feedback", "response": "def controler_output(self, current_value):\n        \"\"\"\n        Calculate PI output value for given reference input and feedback\n        \"\"\"\n\n        set_point = self.settings['set_point']\n        Kp = self.settings['gains']['proportional']\n        Ki = self.settings['gains']['integral']\n        output_range = self.settings['output_range']\n        time_step = self.settings['time_step']\n\n        error_new = set_point - current_value\n        print(('PD- error:\\t', error_new, Ki, Kp, time_step))\n        #proportional action\n        self.u_P = Kp * error_new * time_step\n        print(('PD- self.u_P:\\t', self.u_P, self.u_I))\n\n        #integral action\n        self.u_I += Kp * Ki * (error_new + self.error) / 2.0 * time_step\n\n        self.error = error_new\n\n        print(('PD- self.u_P:\\t', self.u_P, self.u_I))\n\n        # anti-windup\n        if self.u_P + self.u_I > output_range['max']:\n            self.u_I = output_range['max']-self.u_P\n        if self.u_P + self.u_I < output_range['min']:\n            self.u_I = output_range['min']-self.u_P\n\n\n        output = self.u_P + self.u_I\n        print(('PD- output:\\t', output))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates options and apply defaults for options not supplied.", "response": "def get_opts(opts):\n    \"\"\"\n    Validate options and apply defaults for options not supplied.\n\n    :param opts: dictionary mapping str->str.\n    :return: dictionary mapping str->Opt. All possible keys are present.\n    \"\"\"\n    defaults = {\n        'board': None,\n        'terrain': Opt.random,\n        'numbers': Opt.preset,\n        'ports': Opt.preset,\n        'pieces': Opt.preset,\n        'players': Opt.preset,\n    }\n    _opts = defaults.copy()\n    if opts is None:\n        opts = dict()\n    try:\n        for key, val in opts.copy().items():\n            if key == 'board':\n                # board is a string, not a regular opt, and gets special handling\n                # in _read_tiles_from_string\n                continue\n            opts[key] = Opt(val)\n        _opts.update(opts)\n    except Exception:\n        raise ValueError('Invalid options={}'.format(opts))\n    logging.debug('used defaults=\\n{}\\n on opts=\\n{}\\nreturned total opts=\\n{}'.format(\n        pprint.pformat(defaults),\n        pprint.pformat(opts),\n        pprint.pformat(_opts)))\n    return _opts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a new board using the given options.", "response": "def build(opts=None):\n    \"\"\"\n    Build a new board using the given options.\n    :param opts: dictionary mapping str->Opt\n    :return: the new board, Board\n    \"\"\"\n    board = catan.board.Board()\n    modify(board, opts)\n    return board"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset an existing board using the given options.", "response": "def modify(board, opts=None):\n    \"\"\"\n    Reset an existing board using the given options.\n    :param board: the board to reset\n    :param opts: dictionary mapping str->Opt\n    :return: None\n    \"\"\"\n    opts = get_opts(opts)\n    if opts['board'] is not None:\n        board.tiles = _read_tiles_from_string(opts['board'])\n    else:\n        board.tiles = _generate_tiles(opts['terrain'], opts['numbers'])\n    board.ports = _get_ports(opts['ports'])\n    board.state = catan.states.BoardStateModifiable(board)\n    board.pieces = _get_pieces(board.tiles, board.ports, opts['players'], opts['pieces'])\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a list of tiles using the given terrain and numbers options.", "response": "def _get_tiles(board=None, terrain=None, numbers=None):\n    \"\"\"\n    Generate a list of tiles using the given terrain and numbers options.\n\n    terrain options supported:\n    - Opt.empty -> all tiles are desert\n    - Opt.random -> tiles are randomized\n    - Opt.preset ->\n    - Opt.debug -> alias for Opt.random\n\n    numbers options supported:\n    - Opt.empty -> no tiles have numbers\n    - Opt.random -> numbers are randomized\n    - Opt.preset ->\n    - Opt.debug -> alias for Opt.random\n\n    :param terrain_opts: Opt\n    :param numbers_opts: Opt\n    :return: list(Tile)\n    \"\"\"\n    if board is not None:\n        # we have a board given, ignore the terrain and numbers opts and log warnings\n        # if they were supplied\n        tiles = _read_tiles_from_string(board)\n    else:\n        # we are being asked to generate a board\n        tiles = _generate_tiles(terrain, numbers)\n\n    return tiles"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a list of ports using the given options.", "response": "def _get_ports(port_opts):\n    \"\"\"\n    Generate a list of ports using the given options.\n\n    port options supported:\n    - Opt.empty ->\n    - Opt.random ->\n    - Opt.preset -> ports are in default locations\n    - Opt.debug -> alias for Opt.preset\n\n    :param port_opts: Opt\n    :return: list(Port)\n    \"\"\"\n    if port_opts in [Opt.preset, Opt.debug]:\n        _preset_ports = [(1, 'NW', catan.board.PortType.any3),\n                         (2, 'W', catan.board.PortType.wood),\n                         (4, 'W', catan.board.PortType.brick),\n                         (5, 'SW', catan.board.PortType.any3),\n                         (6, 'SE', catan.board.PortType.any3),\n                         (8, 'SE', catan.board.PortType.sheep),\n                         (9, 'E', catan.board.PortType.any3),\n                         (10, 'NE', catan.board.PortType.ore),\n                         (12, 'NE', catan.board.PortType.wheat)]\n        return [catan.board.Port(tile, dir, port_type)\n                for tile, dir, port_type in _preset_ports]\n    elif port_opts in [Opt.empty, Opt.random]:\n        logging.warning('{} option not yet implemented'.format(port_opts))\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a dictionary of pieces based on the given options.", "response": "def _get_pieces(tiles, ports, players_opts, pieces_opts):\n    \"\"\"\n    Generate a dictionary of pieces using the given options.\n\n    pieces options supported:\n    - Opt.empty -> no locations have pieces\n    - Opt.random ->\n    - Opt.preset -> robber is placed on the first desert found\n    - Opt.debug -> a variety of pieces are placed around the board\n\n    :param tiles: list of tiles from _generate_tiles\n    :param ports: list of ports from _generate_ports\n    :param players_opts: Opt\n    :param pieces_opts: Opt\n    :return: dictionary mapping (hexgrid.TYPE, coord:int) -> Piece\n    \"\"\"\n    if pieces_opts == Opt.empty:\n        return dict()\n    elif pieces_opts == Opt.debug:\n        players = catan.game.Game.get_debug_players()\n        return {\n            (hexgrid.NODE, 0x23): catan.pieces.Piece(catan.pieces.PieceType.settlement, players[0]),\n            (hexgrid.EDGE, 0x22): catan.pieces.Piece(catan.pieces.PieceType.road, players[0]),\n            (hexgrid.NODE, 0x67): catan.pieces.Piece(catan.pieces.PieceType.settlement, players[1]),\n            (hexgrid.EDGE, 0x98): catan.pieces.Piece(catan.pieces.PieceType.road, players[1]),\n            (hexgrid.NODE, 0x87): catan.pieces.Piece(catan.pieces.PieceType.settlement, players[2]),\n            (hexgrid.EDGE, 0x89): catan.pieces.Piece(catan.pieces.PieceType.road, players[2]),\n            (hexgrid.EDGE, 0xA9): catan.pieces.Piece(catan.pieces.PieceType.road, players[3]),\n            (hexgrid.TILE, 0x77): catan.pieces.Piece(catan.pieces.PieceType.robber, None),\n        }\n    elif pieces_opts in (Opt.preset, ):\n        deserts = filter(lambda tile: tile.terrain == catan.board.Terrain.desert, tiles)\n        coord = hexgrid.tile_id_to_coord(list(deserts)[0].tile_id)\n        return {\n            (hexgrid.TILE, coord): catan.pieces.Piece(catan.pieces.PieceType.robber, None)\n        }\n    elif pieces_opts in (Opt.random, ):\n        logging.warning('{} option not yet implemented'.format(pieces_opts))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_feature(self, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.create_feature_with_http_info(**kwargs)\n        else:\n            (data) = self.create_feature_with_http_info(**kwargs)\n            return data", "response": "Create an enumerated sequence feature with the specified attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve an enumerated sequence feature by path", "response": "def get_feature_by_path(self, locus, term, rank, accession, **kwargs):\n        \"\"\"\n        Retrieve an enumerated sequence feature\n        \n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please define a `callback` function\n        to be invoked when receiving the response.\n        >>> def callback_function(response):\n        >>>     pprint(response)\n        >>>\n        >>> thread = api.get_feature_by_path(locus, term, rank, accession, callback=callback_function)\n\n        :param callback function: The callback function\n            for asynchronous request. (optional)\n        :param str locus: locus name or URI (required)\n        :param str term: Sequence Ontology (SO) term name, accession, or URI (required)\n        :param int rank: feature rank, must be at least 1 (required)\n        :param int accession: accession, must be at least 1 (required)\n        :return: Feature\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.get_feature_by_path_with_http_info(locus, term, rank, accession, **kwargs)\n        else:\n            (data) = self.get_feature_by_path_with_http_info(locus, term, rank, accession, **kwargs)\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_feature_by_query(self, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.get_feature_by_query_with_http_info(**kwargs)\n        else:\n            (data) = self.get_feature_by_query_with_http_info(**kwargs)\n            return data", "response": "Retrieve an enumerated sequence feature by query"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist the enumerated sequence features at a locus.", "response": "def list_features(self, locus, **kwargs):\n        \"\"\"\n        List the enumerated sequence features at a locus\n        \n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please define a `callback` function\n        to be invoked when receiving the response.\n        >>> def callback_function(response):\n        >>>     pprint(response)\n        >>>\n        >>> thread = api.list_features(locus, callback=callback_function)\n\n        :param callback function: The callback function\n            for asynchronous request. (optional)\n        :param str locus: locus name or URI (required)\n        :return: list[Feature]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.list_features_with_http_info(locus, **kwargs)\n        else:\n            (data) = self.list_features_with_http_info(locus, **kwargs)\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_features_0(self, locus, term, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.list_features_0_with_http_info(locus, term, **kwargs)\n        else:\n            (data) = self.list_features_0_with_http_info(locus, term, **kwargs)\n            return data", "response": "List the enumerated sequence features matching a term at a locus."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_features_1(self, locus, term, rank, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.list_features_1_with_http_info(locus, term, rank, **kwargs)\n        else:\n            (data) = self.list_features_1_with_http_info(locus, term, rank, **kwargs)\n            return data", "response": "List the enumerated sequence features matching a term and rank at a locus."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the data for the item in the tree structure", "response": "def setData(self, column, role, value):\n        \"\"\"\n        if value is valid sets the data to value\n        Args:\n            column: column of item\n            role: role of item (see Qt doc)\n            value: value to be set\n        \"\"\"\n        assert isinstance(column, int)\n        assert isinstance(role, int)\n\n        # make sure that the right row is selected, this is not always the case for checkboxes and\n        # combo boxes because they are items on top of the tree structure\n        if isinstance(value, (QtWidgets.QComboBox, QtWidgets.QCheckBox)):\n            self.treeWidget().setCurrentItem(self)\n\n        # if row 2 (editrole, value has been entered)\n        if role == 2 and column == 1:\n\n            if isinstance(value, str):\n                value = self.cast_type(value) # cast into same type as valid values\n\n            if isinstance(value, QtCore.QVariant):\n                value = self.cast_type(value.toString())  # cast into same type as valid values\n\n            if isinstance(value, QtWidgets.QComboBox):\n                value = self.cast_type(value.currentText())\n\n            if isinstance(value, QtWidgets.QCheckBox):\n                value = bool(int(value.checkState()))  # checkState() gives 2 (True) and 0 (False)\n\n            # save value in internal variable\n            self.value = value\n\n        elif column == 0:\n            # labels should not be changed so we set it back\n            value = self.name\n\n        if value is None:\n            value = self.value\n\n        # 180327(asafira) --- why do we need to do the following lines? Why not just always call super or always\n        # emitDataChanged()?\n        if not isinstance(value, bool):\n            super(B26QTreeItem, self).setData(column, role, value)\n\n        else:\n            self.emitDataChanged()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cast_type(self, var, cast_type=None):\n\n        if cast_type is None:\n            cast_type = self.valid_values\n\n        try:\n            if cast_type == int:\n                return int(var)\n            elif cast_type == float:\n                return float(var)\n            elif type == str:\n                return str(var)\n            elif isinstance(cast_type, list):\n                # cast var to be of the same type as those in the list\n                return type(cast_type[0])(var)\n            else:\n                return None\n        except ValueError:\n            return None\n\n        return var", "response": "cast the value into the type typ\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_instrument(self):\n\n        if isinstance(self.value, Instrument):\n            instrument = self.value\n            path_to_instrument = []\n        else:\n            instrument = None\n            parent = self.parent()\n            path_to_instrument = [self.name]\n            while parent is not None:\n                if isinstance(parent.value, Instrument):\n                    instrument = parent.value\n                    parent = None\n                else:\n                    path_to_instrument.append(parent.name)\n                    parent = parent.parent()\n\n        return instrument, path_to_instrument", "response": "Returns the instrument and the path to the instrument to which this item belongs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the item that contains the sub_script with name sub_script_name", "response": "def get_subscript(self, sub_script_name):\n        \"\"\"\n        finds the item that contains the sub_script with name sub_script_name\n        Args:\n            sub_script_name: name of subscript\n        Returns: B26QTreeItem in QTreeWidget which is a script\n\n        \"\"\"\n\n        # get tree of item\n        tree = self.treeWidget()\n\n        items = tree.findItems(sub_script_name, QtCore.Qt.MatchExactly | QtCore.Qt.MatchRecursive)\n\n        if len(items) >= 1:\n            # identify correct script by checking that it is a sub_element of the current script\n            subscript_item = [sub_item for sub_item in items if isinstance(sub_item.value, Script)\n                               and sub_item.parent() is self]\n\n            subscript_item = subscript_item[0]\n        else:\n            raise ValueError('several elements with name ' + sub_script_name)\n\n\n        return subscript_item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_point(self):\n\n        if self.childCount() == 2:\n                if self.child(0).valid_values == float and self.child(1).valid_values == float:\n                    return True\n        else:\n            return False", "response": "Figure out if the item is a point"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a client request following HTTP redirects.", "response": "def request(self, method, uri, headers=None, bodyProducer=None):\n        \"\"\"\n        Send a client request following HTTP redirects.\n\n        @see: L{Agent.request}.\n        \"\"\"\n        if self._parent_trace is None:\n            trace = Trace(method)\n        else:\n            trace = self._parent_trace.child(method)\n\n        if self._endpoint is not None:\n            trace.set_endpoint(self._endpoint)\n\n        if headers is None:\n            headers = Headers({})\n\n        # These headers are based on the headers used by finagle's tracing\n        # http Codec.\n        #\n        # https://github.com/twitter/finagle/blob/master/finagle-http/\n        #\n        # Currently not implemented are X-B3-Sampled and X-B3-Flags\n        # Tryfer's underlying Trace implementation has no notion of a Sampled\n        # trace and I haven't figured out what flags are for.\n        headers.setRawHeaders('X-B3-TraceId', [hex_str(trace.trace_id)])\n        headers.setRawHeaders('X-B3-SpanId', [hex_str(trace.span_id)])\n\n        if trace.parent_span_id is not None:\n            headers.setRawHeaders('X-B3-ParentSpanId',\n                                  [hex_str(trace.parent_span_id)])\n\n        # Similar to the headers above we use the annotation 'http.uri' for\n        # because that is the standard set forth in the finagle http Codec.\n        trace.record(Annotation.string('http.uri', uri))\n        trace.record(Annotation.client_send())\n\n        def _finished(resp):\n            # TODO: It may be advantageous here to return a wrapped response\n            # whose deliverBody can wrap it's protocol and record when the\n            # application has finished reading the contents.\n            trace.record(Annotation.string(\n                'http.responsecode',\n                '{0} {1}'.format(resp.code, resp.phrase)))\n            trace.record(Annotation.client_recv())\n            return resp\n\n        d = self._agent.request(method, uri, headers, bodyProducer)\n        d.addBoth(_finished)\n\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that *args consist of a consistent type :param f: any client method with *args parameter :return: function f", "response": "def validate_args(f):\n    \"\"\"\n    Ensures that *args consist of a consistent type\n\n    :param f: any client method with *args parameter\n    :return: function f\n    \"\"\"\n\n    def wrapper(self, args):\n        arg_types = set([type(arg) for arg in args])\n        if len(arg_types) > 1:\n            raise TypeError(\"Mixed input types are not allowed\")\n\n        elif list(arg_types)[0] not in (dict, str):\n            raise TypeError(\"Only dict and str types accepted\")\n\n        return f(self, args)\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef truncate_args(f):\n\n    def wrapper(self, args):\n        if len(args) > 100:\n            if self.truncate_addresses:\n                args = args[:100]\n            else:\n                raise ValueError(\n                    \"This exceeds 100 address at a time SmartyStreets limit\"\n                )\n\n        return f(self, args)\n\n    return wrapper", "response": "Decorator that ensures that the given list of arguments is not exceeding 100 address at a time SmartyStreets limit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert all values in the dictionary to strings except for the value for candidate which should just be an integer.", "response": "def stringify(data):\n    \"\"\"\n    Ensure all values in the dictionary are strings, except for the value for `candidate` which\n    should just be an integer.\n\n    :param data: a list of addresses in dictionary format\n    :return: the same list with all values except for `candidate` count as a string\n    \"\"\"\n\n    def serialize(k, v):\n        if k == \"candidates\":\n            return int(v)\n\n        if isinstance(v, numbers.Number):\n            if k == \"zipcode\":\n                # If values are presented as integers then leading digits may be cut off,\n                # and these are significant for the zipcode. Add them back.\n                return str(v).zfill(5)\n\n            return str(v)\n\n        return v\n\n    return [{k: serialize(k, v) for k, v in json_dict.items()} for json_dict in data]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute the HTTP POST request to the specified endpoint and returns the JSON response.", "response": "def post(self, endpoint, data):\n        \"\"\"\n        Executes the HTTP POST request\n\n        :param endpoint: string indicating the URL component to call\n        :param data: the data to submit\n        :return: the dumped JSON response content\n        \"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"x-standardize-only\": \"true\" if self.standardize else \"false\",\n            \"x-include-invalid\": \"true\" if self.invalid else \"false\",\n            \"x-accept-keypair\": \"true\" if self.accept_keypair else \"false\",\n        }\n        if not self.logging:\n            headers[\"x-suppress-logging\"] = \"true\"\n\n        params = {\"auth-id\": self.auth_id, \"auth-token\": self.auth_token}\n        url = self.BASE_URL + endpoint\n        response = self.session.post(\n            url,\n            json.dumps(stringify(data)),\n            params=params,\n            headers=headers,\n            timeout=self.timeout,\n        )\n        if response.status_code == 200:\n            return response.json()\n\n        raise ERROR_CODES.get(response.status_code, SmartyStreetsError)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef street_addresses(self, addresses):\n\n        # While it's okay in theory to accept freeform addresses they do need to be submitted in\n        # a dictionary format.\n        if type(addresses[0]) != dict:\n            addresses = [{\"street\": arg for arg in addresses}]\n\n        return AddressCollection(self.post(\"street-address\", data=addresses))", "response": "This method is used to verify that a list of street addresses is returned."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef street_address(self, address):\n        address = self.street_addresses([address])\n        if not len(address):\n            return None\n\n        return Address(address[0])", "response": "Geocode one and only address get a single Address object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscan for available ports. return a list of tuples ( num name )", "response": "def scan():\n    \"\"\"\n    scan for available ports. return a list of tuples (num, name)\n    Returns:\n\n    \"\"\"\n    available = []\n    for i in range(256):\n        try:\n            s = serial.Serial('COM'+str(i))\n            available.append((i, s.portstr))\n            s.close()\n        except serial.SerialException:\n            pass\n    return available"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Validator that can be used to validate a schema against another schema.", "response": "def load(schema, uri=None, spec=None, provider=None):\n    \"\"\"Scaffold a validator against a schema.\n\n    :param schema: the schema to compile into a Validator\n    :type schema: Mapping\n    :param uri: the uri of the schema.\n                it may be ignored in case of not cross\n                referencing.\n    :type uri: Pointer, str\n    :param spec: fallback to this spec if the schema does not provides ts own\n    :type spec: str\n    :param provider: the other schemas, in case of cross\n                     referencing\n    :type provider: Mapping, Provider...\n    \"\"\"\n    factory = Factory(provider, spec)\n    return factory(schema, uri or '#')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sizeHint(self):\n        w, h = self.get_width_height()\n        return QtCore.QSize(w, h)", "response": "returns the size hint for this widget"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing obj to be staged.", "response": "def stage(obj, parent=None, member=None):\n    \"\"\"\n    Prepare obj to be staged.\n\n    This is almost used for relative JSON Pointers.\n    \"\"\"\n    obj = Staged(obj, parent, member)\n\n    if isinstance(obj, Mapping):\n        for key, value in obj.items():\n            stage(value, obj, key)\n    elif isinstance(obj, Sequence) and not isinstance(obj, string_types):\n        for index, value in enumerate(obj):\n            stage(value, obj, index)\n    elif isinstance(obj, Set):\n        for value in obj:\n            stage(value, obj, None)\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting old files we ve uploaded to S3 according to grandfather father sun strategy", "response": "def rotate(key_prefix, key_ext, bucket_name, daily_backups=7, weekly_backups=4, aws_key=None, aws_secret=None):\n    \"\"\" Delete old files we've uploaded to S3 according to grandfather, father, sun strategy \"\"\"\n\n    session = boto3.Session(\n        aws_access_key_id=aws_key,\n        aws_secret_access_key=aws_secret\n    )\n    s3 = session.resource('s3')\n    bucket = s3.Bucket(bucket_name)\n    keys = bucket.objects.filter(Prefix=key_prefix)\n\n    regex = '{0}-(?P<year>[\\d]+?)-(?P<month>[\\d]+?)-(?P<day>[\\d]+?){1}'.format(key_prefix, key_ext)\n    backups = []\n\n    for key in keys:\n        match = re.match(regex, str(key.key))\n        if not match:\n            continue\n        year = int(match.group('year'))\n        month = int(match.group('month'))\n        day = int(match.group('day'))\n        key_date = datetime(year, month, day)\n        backups[:0] = [key_date]\n    backups = sorted(backups, reverse=True)\n\n    if len(backups) > daily_backups+1 and backups[daily_backups] - backups[daily_backups+1] < timedelta(days=7):\n        key = bucket.Object(\"{0}{1}{2}\".format(key_prefix,backups[daily_backups].strftime(\"-%Y-%m-%d\"), key_ext))\n        logger.debug(\"deleting {0}\".format(key))\n        key.delete()\n        del backups[daily_backups]\n\n    month_offset = daily_backups + weekly_backups\n    if len(backups) > month_offset+1 and backups[month_offset] - backups[month_offset+1] < timedelta(days=30):\n        key = bucket.Object(\"{0}{1}{2}\".format(key_prefix,backups[month_offset].strftime(\"-%Y-%m-%d\"), key_ext))\n        logger.debug(\"deleting {0}\".format(key))\n        key.delete()\n        del backups[month_offset]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef splitext( filename ):\n    index = filename.find('.')\n    if index == 0:\n        index = 1+filename[1:].find('.')\n    if index == -1:\n        return filename, ''\n    return filename[:index], filename[index:]\n    return os.path.splitext(filename)", "response": "Return the filename and extension according to the first dot in the filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_clicked(self, mouse_event):\n        if type(self.peak_vals) is list:\n            self.peak_vals.append([mouse_event.xdata, mouse_event.ydata])\n            axes = self.matplotlibwidget.axes\n            # can't use patches, as they use data coordinates for radius but this is a high aspect ratio plot so the\n            # circle was extremely stretched\n            axes.plot(mouse_event.xdata, mouse_event.ydata, 'ro', markersize = 5)\n\n            # axes.text(mouse_event.xdata, mouse_event.ydata, '{:d}'.format(len(self.peak_vals[-1])),\n            #                  horizontalalignment='center',\n            #                  verticalalignment='center',\n            #                  color='black'\n            #                  )\n            self.matplotlibwidget.draw()", "response": "This method is called when a mouse button is clicked on the plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_fitting(self):\n        self.queue = queue.Queue()\n        self.peak_vals = []\n        self.fit_thread = QThread() #must be assigned as an instance variable, not local, as otherwise thread is garbage\n                                    #collected immediately at the end of the function before it runs\n        self.fitobj = self.do_fit(str(self.data_filepath.text()), self.matplotlibwidget, self.queue, self.peak_vals, self.peak_locs)\n        self.fitobj.moveToThread(self.fit_thread)\n        self.fit_thread.started.connect(self.fitobj.run)\n        self.fitobj.finished.connect(self.fit_thread.quit)  # clean up. quit thread after script is finished\n        self.fitobj.status.connect(self.update_status)\n        self.fit_thread.start()", "response": "Launches the fitting routine on another thread"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_bandgap_from_bands(energies, nelec):\n        nelec = int(nelec)\n        valence = [x[nelec-1] for x in energies]\n        conduction = [x[nelec] for x in energies]\n        return max(min(conduction) - max(valence), 0.0)", "response": "Compute the difference in conduction band min and valence band max"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the bandgap from the EIGENVAL file", "response": "def _get_bandgap_eigenval(eigenval_fname, outcar_fname):\n        \"\"\"Get the bandgap from the EIGENVAL file\"\"\"\n        with open(outcar_fname, \"r\") as f:\n            parser = OutcarParser()\n            nelec = next(iter(filter(lambda x: \"number of electrons\" in x, parser.parse(f.readlines()))))[\"number of electrons\"]\n        with open(eigenval_fname, \"r\") as f:\n            eigenval_info = list(EigenvalParser().parse(f.readlines()))\n        # spin_polarized = (2 == len(next(filter(lambda x: \"kpoint\" in x, eigenval_info))[\"occupancies\"][0]))\n        # if spin_polarized:\n        all_energies = [zip(*x[\"energies\"]) for x in eigenval_info if \"energies\" in x]\n        spin_energies = zip(*all_energies)\n        gaps = [VaspParser._get_bandgap_from_bands(x, nelec/2.0) for x in spin_energies]\n        return min(gaps)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the bandgap from the DOSCAR file", "response": "def _get_bandgap_doscar(filename):\n        \"\"\"Get the bandgap from the DOSCAR file\"\"\"\n        with open(filename) as fp:\n            for i in range(6):\n                l = fp.readline()\n            efermi = float(l.split()[3])\n            step1 = fp.readline().split()[0]\n            step2 = fp.readline().split()[0]\n            step_size = float(step2)-float(step1)\n            not_found = True\n            while not_found:\n                l = fp.readline().split()\n                e = float(l.pop(0))\n                dens = 0.0\n                for i in range(int(len(l)/2)):\n                    dens += float(l[i])\n                if e < efermi and dens > 1e-3:\n                    bot = e\n                elif e > efermi and dens > 1e-3:\n                    top = e\n                    not_found = False\n            if top - bot < step_size*2:\n                bandgap = 0.0\n            else:\n                bandgap = float(top - bot)\n\n        return bandgap"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the bandgap from either the EIGENVAL or DOSCAR files", "response": "def get_band_gap(self):\n        \"\"\"Get the bandgap, either from the EIGENVAL or DOSCAR files\"\"\"\n        if self.outcar is not None and self.eignval is not None:\n            bandgap = VaspParser._get_bandgap_eigenval(self.eignval, self.outcar)\n        elif self.doscar is not None:\n            bandgap = VaspParser._get_bandgap_doscar(self.doscar)\n        else:\n            return None\n        return Property(scalars=[Scalar(value=round(bandgap, 3))], units='eV')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget raster value by row and column number.", "response": "def get_value_by_row_col(self, row, col):\n        \"\"\"Get raster value by (row, col).\n\n        Args:\n            row: row number.\n            col: col number.\n\n        Returns:\n            raster value, None if the input are invalid.\n        \"\"\"\n        if row < 0 or row >= self.nRows or col < 0 or col >= self.nCols:\n            raise ValueError(\"The row or col must be >=0 and less than \"\n                             \"nRows (%d) or nCols (%d)!\" % (self.nRows, self.nCols))\n        else:\n            value = self.data[int(round(row))][int(round(col))]\n            if value == self.noDataValue:\n                return None\n            else:\n                return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_value_by_xy(self, x, y):\n        if x < self.xMin or x > self.xMax or y < self.yMin or y > self.yMax:\n            return None\n            # raise ValueError(\"The x or y value must be within the Min and Max!\")\n        else:\n            row = self.nRows - int(numpy.ceil((y - self.yMin) / self.dx))\n            col = int(numpy.floor((x - self.xMin) / self.dx))\n            value = self.data[row][col]\n            if value == self.noDataValue:\n                return None\n            else:\n                return value", "response": "Get raster value by xy coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_central_coors(self, row, col):\n        if row < 0 or row >= self.nRows or col < 0 or col >= self.nCols:\n            raise ValueError(\"The row (%d) or col (%d) must be >=0 and less than \"\n                             \"nRows (%d) or nCols (%d)!\" % (row, col, self.nRows, self.nCols))\n        else:\n            tmpx = self.xMin + (col + 0.5) * self.dx\n            tmpy = self.yMax - (row + 0.5) * self.dx\n            return tmpx, tmpy", "response": "Get the coordinates of central elements of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_raster(raster_file):\n        ds = gdal_Open(raster_file)\n        band = ds.GetRasterBand(1)\n        data = band.ReadAsArray()\n        xsize = band.XSize\n        ysize = band.YSize\n\n        nodata_value = band.GetNoDataValue()\n        geotrans = ds.GetGeoTransform()\n        dttype = band.DataType\n\n        srs = osr_SpatialReference()\n        srs.ImportFromWkt(ds.GetProjection())\n        # print(srs.ExportToProj4())\n        if nodata_value is None:\n            nodata_value = DEFAULT_NODATA\n        band = None\n        ds = None\n        return Raster(ysize, xsize, data, nodata_value, geotrans, srs, dttype)", "response": "Reads a single raster by GDAL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a mask from a given raster file.", "response": "def get_mask_from_raster(rasterfile, outmaskfile, keep_nodata=False):\n        \"\"\"Generate mask data from a given raster data.\n\n        Args:\n            rasterfile: raster file path.\n            outmaskfile: output mask file path.\n\n        Returns:\n            Raster object of mask data.\n        \"\"\"\n        raster_r = RasterUtilClass.read_raster(rasterfile)\n        xsize = raster_r.nCols\n        ysize = raster_r.nRows\n        nodata_value = raster_r.noDataValue\n        srs = raster_r.srs\n        x_min = raster_r.xMin\n        y_max = raster_r.yMax\n        dx = raster_r.dx\n        data = raster_r.data\n\n        if not keep_nodata:\n            i_min = ysize - 1\n            i_max = 0\n            j_min = xsize - 1\n            j_max = 0\n            for i in range(ysize):\n                for j in range(xsize):\n                    if abs(data[i][j] - nodata_value) > DELTA:\n                        i_min = min(i, i_min)\n                        i_max = max(i, i_max)\n                        j_min = min(j, j_min)\n                        j_max = max(j, j_max)\n\n            # print(i_min, i_max, j_min, j_max)\n            y_size_mask = i_max - i_min + 1\n            x_size_mask = j_max - j_min + 1\n            x_min_mask = x_min + j_min * dx\n            y_max_mask = y_max - i_min * dx\n        else:\n            y_size_mask = ysize\n            x_size_mask = xsize\n            x_min_mask = x_min\n            y_max_mask = y_max\n            i_min = 0\n            j_min = 0\n        print('%dx%d -> %dx%d' % (xsize, ysize, x_size_mask, y_size_mask))\n\n        mask = numpy.zeros((y_size_mask, x_size_mask))\n\n        for i in range(y_size_mask):\n            for j in range(x_size_mask):\n                if abs(data[i + i_min][j + j_min] - nodata_value) > DELTA:\n                    mask[i][j] = 1\n                else:\n                    mask[i][j] = DEFAULT_NODATA\n\n        mask_geotrans = [x_min_mask, dx, 0, y_max_mask, 0, -dx]\n        RasterUtilClass.write_gtiff_file(outmaskfile, y_size_mask, x_size_mask, mask,\n                                         mask_geotrans, srs, DEFAULT_NODATA, GDT_Int32)\n        return Raster(y_size_mask, x_size_mask, mask, DEFAULT_NODATA, mask_geotrans, srs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreclassify raster by given classifier dict.", "response": "def raster_reclassify(srcfile, v_dict, dstfile, gdaltype=GDT_Float32):\n        \"\"\"Reclassify raster by given classifier dict.\n\n        Args:\n            srcfile: source raster file.\n            v_dict: classifier dict.\n            dstfile: destination file path.\n            gdaltype (:obj:`pygeoc.raster.GDALDataType`): GDT_Float32 as default.\n        \"\"\"\n        src_r = RasterUtilClass.read_raster(srcfile)\n        src_data = src_r.data\n        dst_data = numpy.copy(src_data)\n        if gdaltype == GDT_Float32 and src_r.dataType != GDT_Float32:\n            gdaltype = src_r.dataType\n        no_data = src_r.noDataValue\n        new_no_data = DEFAULT_NODATA\n        if gdaltype in [GDT_Unknown, GDT_Byte, GDT_UInt16, GDT_UInt32]:\n            new_no_data = 0\n        if not MathClass.floatequal(new_no_data, src_r.noDataValue):\n            if src_r.noDataValue not in v_dict:\n                v_dict[src_r.noDataValue] = new_no_data\n                no_data = new_no_data\n\n        for (k, v) in iteritems(v_dict):\n            dst_data[src_data == k] = v\n        RasterUtilClass.write_gtiff_file(dstfile, src_r.nRows, src_r.nCols, dst_data,\n                                         src_r.geotrans, src_r.srs, no_data, gdaltype)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_gtiff_file(f_name, n_rows, n_cols, data, geotransform, srs, nodata_value,\n                         gdal_type=GDT_Float32):\n        \"\"\"Output Raster to GeoTiff format file.\n\n        Args:\n            f_name: output gtiff file name.\n            n_rows: Row count.\n            n_cols: Col count.\n            data: 2D array data.\n            geotransform: geographic transformation.\n            srs: coordinate system.\n            nodata_value: nodata value.\n            gdal_type (:obj:`pygeoc.raster.GDALDataType`): output raster data type,\n                                                                  GDT_Float32 as default.\n        \"\"\"\n        UtilClass.mkdir(os.path.dirname(FileClass.get_file_fullpath(f_name)))\n        driver = gdal_GetDriverByName(str('GTiff'))\n        try:\n            ds = driver.Create(f_name, n_cols, n_rows, 1, gdal_type)\n        except Exception:\n            print('Cannot create output file %s' % f_name)\n            return\n        ds.SetGeoTransform(geotransform)\n        try:\n            ds.SetProjection(srs.ExportToWkt())\n        except AttributeError or Exception:\n            ds.SetProjection(srs)\n        ds.GetRasterBand(1).SetNoDataValue(nodata_value)\n        # if data contains numpy.nan, then replaced by nodata_value\n        if isinstance(data, numpy.ndarray) and data.dtype in [numpy.dtype('int'),\n                                                              numpy.dtype('float')]:\n            data = numpy.where(numpy.isnan(data), nodata_value, data)\n        ds.GetRasterBand(1).WriteArray(data)\n        ds = None", "response": "Write a 2D array of raster data to a GeoTiff format file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_asc_file(filename, data, xsize, ysize, geotransform, nodata_value):\n        UtilClass.mkdir(os.path.dirname(FileClass.get_file_fullpath(filename)))\n        header = 'NCOLS %d\\n' \\\n                 'NROWS %d\\n' \\\n                 'XLLCENTER %f\\n' \\\n                 'YLLCENTER %f\\n' \\\n                 'CELLSIZE %f\\n' \\\n                 'NODATA_VALUE %f' % (xsize, ysize, geotransform[0] + 0.5 * geotransform[1],\n                                      geotransform[3] - (ysize - 0.5) * geotransform[1],\n                                      geotransform[1], nodata_value)\n\n        with open(filename, 'w', encoding='utf-8') as f:\n            f.write(header)\n            for i in range(0, ysize):\n                for j in range(0, xsize):\n                    f.write('%s\\t' % repr(data[i][j]))\n                f.write('\\n')\n        f.close()", "response": "Output Raster to ASCII file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts raster format to GeoTIFF format.", "response": "def raster_to_gtiff(tif, geotif, change_nodata=False, change_gdal_type=False):\n        \"\"\"Converting Raster format to GeoTIFF.\n\n        Args:\n            tif: source raster file path.\n            geotif: output raster file path.\n            change_nodata: change NoDataValue to -9999 or not.\n            gdal_type (:obj:`pygeoc.raster.GDALDataType`): GDT_Float32 as default.\n            change_gdal_type: If True, output the Float32 data type.\n        \"\"\"\n        rst_file = RasterUtilClass.read_raster(tif)\n        nodata = rst_file.noDataValue\n        if change_nodata:\n            if not MathClass.floatequal(rst_file.noDataValue, DEFAULT_NODATA):\n                nodata = DEFAULT_NODATA\n                rst_file.data[rst_file.data == rst_file.noDataValue] = DEFAULT_NODATA\n        gdal_type = rst_file.dataType\n        if change_gdal_type:\n            gdal_type = GDT_Float32\n        RasterUtilClass.write_gtiff_file(geotif, rst_file.nRows, rst_file.nCols, rst_file.data,\n                                         rst_file.geotrans, rst_file.srs, nodata,\n                                         gdal_type)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting raster format to ASCII file.", "response": "def raster_to_asc(raster_f, asc_f):\n        \"\"\"Converting Raster format to ASCII raster.\n\n        Args:\n            raster_f: raster file.\n            asc_f: output ASCII file.\n        \"\"\"\n        raster_r = RasterUtilClass.read_raster(raster_f)\n        RasterUtilClass.write_asc_file(asc_f, raster_r.data, raster_r.nCols, raster_r.nRows,\n                                       raster_r.geotrans, raster_r.noDataValue)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef raster_statistics(raster_file):\n        ds = gdal_Open(raster_file)\n        band = ds.GetRasterBand(1)\n        minv, maxv, meanv, std = band.ComputeStatistics(False)\n        return minv, maxv, meanv, std", "response": "Get basic statistics of the raster data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_raster(rs, split_shp, field_name, temp_dir):\n        UtilClass.rmmkdir(temp_dir)\n        ds = ogr_Open(split_shp)\n        lyr = ds.GetLayer(0)\n        lyr.ResetReading()\n        ft = lyr.GetNextFeature()\n        while ft:\n            cur_field_name = ft.GetFieldAsString(field_name)\n            for r in rs:\n                cur_file_name = r.split(os.sep)[-1]\n                outraster = temp_dir + os.sep + \\\n                            cur_file_name.replace('.tif', '_%s.tif' %\n                                                  cur_field_name.replace(' ', '_'))\n                subprocess.call(['gdalwarp', r, outraster, '-cutline', split_shp,\n                                 '-crop_to_cutline', '-cwhere',\n                                 \"'%s'='%s'\" % (field_name, cur_field_name), '-dstnodata',\n                                 '-9999'])\n            ft = lyr.GetNextFeature()\n        ds = None", "response": "Split raster by given shapefile and field name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_negative_dem(raw_dem, neg_dem):\n        origin = RasterUtilClass.read_raster(raw_dem)\n        max_v = numpy.max(origin.data)\n        temp = origin.data < 0\n        neg = numpy.where(temp, origin.noDataValue, max_v - origin.data)\n        RasterUtilClass.write_gtiff_file(neg_dem, origin.nRows, origin.nCols, neg, origin.geotrans,\n                                         origin.srs, origin.noDataValue, origin.dataType)", "response": "Get negative DEM data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mask_raster(in_raster, mask, out_raster):\n        if is_string(in_raster) and is_string(out_raster):\n            in_raster = [str(in_raster)]\n            out_raster = [str(out_raster)]\n        if len(in_raster) != len(out_raster):\n            raise RuntimeError('input raster and output raster must have the same size.')\n\n        maskr = RasterUtilClass.read_raster(mask)\n        rows = maskr.nRows\n        cols = maskr.nCols\n        maskdata = maskr.data\n        temp = maskdata == maskr.noDataValue\n        for inr, outr in zip(in_raster, out_raster):\n            origin = RasterUtilClass.read_raster(inr)\n            if origin.nRows == rows and origin.nCols == cols:\n                masked = numpy.where(temp, origin.noDataValue, origin.data)\n            else:\n                masked = numpy.ones((rows, cols)) * origin.noDataValue\n                # TODO, the following loop should be optimized by numpy or numba\n                for i in range(rows):\n                    for j in range(cols):\n                        if maskdata[i][j] == maskr.noDataValue:\n                            continue\n                        # get the center point coordinate of current cell\n                        tempx, tempy = maskr.get_central_coors(i, j)\n                        tempv = origin.get_value_by_xy(tempx, tempy)\n                        if tempv is None:\n                            continue\n                        masked[i][j] = tempv\n            RasterUtilClass.write_gtiff_file(outr, maskr.nRows, maskr.nCols, masked,\n                                             maskr.geotrans, maskr.srs,\n                                             origin.noDataValue, origin.dataType)", "response": "Mask raster data.\n        Args:\n            in_raster: list or one raster\n            mask: Mask raster data\n            out_raster: list or one raster"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake the raster into binarization. The opening and closing are based on binary image. Therefore we need to make the raster into binarization. Args: given_value: The given value's pixels will be value in 1, other pixels will be value in 0. rasterfilename: The initial rasterfilena,e. Returns: binary_raster: Raster after binarization.", "response": "def raster_binarization(given_value, rasterfilename):\n        \"\"\"Make the raster into binarization.\n\n        The opening and closing are based on binary image. Therefore we need to\n        make the raster into binarization.\n\n        Args:\n            given_value: The given value's pixels will be value in 1,\n            other pixels will be value in 0.\n            rasterfilename: The initial rasterfilena,e.\n\n        Returns:\n            binary_raster: Raster after binarization.\n        \"\"\"\n        origin_raster = RasterUtilClass.read_raster(rasterfilename)\n        binary_raster = numpy.where(origin_raster.data == given_value, 1, 0)\n        return binary_raster"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raster_erosion(rasterfile):\n        if is_string(rasterfile):\n            origin_raster = RasterUtilClass.read_raster(str(rasterfile))\n        elif isinstance(rasterfile, Raster):\n            origin_raster = rasterfile.data\n        elif isinstance(rasterfile, numpy.ndarray):\n            origin_raster = rasterfile\n        else:\n            return \"Your rasterfile has a wrong type. Type must be string or \" \\\n                   \"numpy.array or class Raster in pygeoc.\"\n        max_value_raster = origin_raster.max()\n        erosion_raster = numpy.zeros((origin_raster.shape[0], origin_raster.shape[1]))\n        # In order to compute the raster edges, we need to expand the original\n        # raster's rows and cols. We need to add the edges whose pixels' value is\n        # the max pixel's value in raster.\n        add_row = numpy.full((1, origin_raster.shape[1]), max_value_raster)\n        temp_origin_raster = numpy.vstack((numpy.vstack((add_row, origin_raster)), add_row))\n        add_col = numpy.full((origin_raster.shape[0] + 2, 1), max_value_raster)\n        expand_origin_raster = numpy.hstack((numpy.hstack((add_col, temp_origin_raster)), add_col))\n        # Erode the raster.\n        for i in range(origin_raster.shape[0]):\n            for j in range(origin_raster.shape[1]):\n                min_pixel_value = max_value_raster\n                # Find the min pixel value in the 8-neighborhood.\n                for k in range(3):\n                    for l in range(3):\n                        if expand_origin_raster[i + k, j + l] <= min_pixel_value:\n                            min_pixel_value = expand_origin_raster[i + k, j + l]\n                            # After this loop, we get the min pixel's value of the\n                            # 8-neighborhood. Then we change the compute pixel's value into\n                            # the min pixel's value.\n                    erosion_raster[i, j] = min_pixel_value\n        # Return the result.\n        return erosion_raster", "response": "Erode the raster image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raster_dilation(rasterfile):\n        if is_string(rasterfile):\n            origin_raster = RasterUtilClass.read_raster(str(rasterfile))\n        elif isinstance(rasterfile, Raster):\n            origin_raster = rasterfile.data\n        elif isinstance(rasterfile, numpy.ndarray):\n            origin_raster = rasterfile\n        else:\n            return 'Your rasterfile has a wrong type. Type must be string or ' \\\n                   'numpy.array or class Raster in pygeoc.'\n        min_value_raster = origin_raster.min()\n        dilation_raster = numpy.zeros((origin_raster.shape[0], origin_raster.shape[1]))\n        # In order to compute the raster edges, we need to expand the original\n        # raster's rows and cols. We need to add the edges whose pixels' value is\n        # the max pixel's value in raster.\n        add_row = numpy.full((1, origin_raster.shape[1]), min_value_raster)\n        temp_origin_raster = numpy.vstack((numpy.vstack((add_row, origin_raster)), add_row))\n        add_col = numpy.full((origin_raster.shape[0] + 2, 1), min_value_raster)\n        expand_origin_raster = numpy.hstack((numpy.hstack((add_col, temp_origin_raster)), add_col))\n        # Dilate the raster.\n        for i in range(origin_raster.shape[0]):\n            for j in range(origin_raster.shape[1]):\n                max_pixel_value = min_value_raster\n                # Find the max pixel value in the 8-neighborhood.\n                for k in range(3):\n                    for l in range(3):\n                        if expand_origin_raster[i + k, j + l] >= max_pixel_value:\n                            max_pixel_value = expand_origin_raster[i + k, j + l]\n                            # After this loop, we get the max pixel's value of the\n                            # 8-neighborhood. Then we change the compute pixel's value into\n                            # the max pixel's value.\n                    dilation_raster[i, j] = max_pixel_value\n        # Return the result.\n        return dilation_raster", "response": "Dilate the raster image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef openning(input_rasterfilename, times):\n        input_raster = RasterUtilClass.read_raster(input_rasterfilename)\n        openning_raster = input_raster\n        for i in range(times):\n            openning_raster = RasterUtilClass.raster_erosion(openning_raster)\n        for i in range(times):\n            openning_raster = RasterUtilClass.raster_dilation(openning_raster)\n        return openning_raster", "response": "Do openning.\n\n        Openning: Erode firstly, then Dilate.\n\n        Args:\n            input_rasterfilename: input original raster image filename.\n            times: Erode and Dilate times.\n\n        Returns:\n            openning_raster: raster image after open."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef closing(input_rasterfilename, times):\n        input_raster = RasterUtilClass.read_raster(input_rasterfilename)\n        closing_raster = input_raster\n        for i in range(times):\n            closing_raster = RasterUtilClass.raster_dilation(closing_raster)\n        for i in range(times):\n            closing_raster = RasterUtilClass.raster_erosion(closing_raster)\n        return closing_raster", "response": "Do closing.\n\n        Closing: Dilate firstly, then Erode.\n\n        Args:\n            input_rasterfilename: input original raster image filename.\n            times: Erode and Dilate times.\n\n        Returns:\n            closing_raster: raster image after close."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate tx fee from tx size in bytes", "response": "def calculate_tx_fee(tx_size: int) -> Decimal:\n    '''return tx fee from tx size in bytes'''\n\n    per_kb_cost = 0.01\n    min_fee = Decimal(0.001)\n\n    fee = Decimal((tx_size / 1000) * per_kb_cost)\n\n    if fee <= min_fee:\n        return min_fee\n    else:\n        return fee"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates nulldata (OP_return) script", "response": "def nulldata_script(data: bytes) -> NulldataScript:\n    '''create nulldata (OP_return) script'''\n\n    stack = StackData.from_bytes(data)\n    return NulldataScript(stack)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p2pkh_script(network: str, address: str) -> P2pkhScript:\n    '''create pay-to-key-hash (P2PKH) script'''\n\n    network_params = net_query(network)\n\n    addr = Address.from_string(network=network_params,\n                               string=address)\n\n    return P2pkhScript(addr)", "response": "create pay - to - key - hash script"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_parent_outputs(provider: Provider, utxo: TxIn) -> TxOut:\n    '''due to design of the btcpy library, TxIn object must be converted to TxOut object before signing'''\n\n    network_params = net_query(provider.network)\n    index = utxo.txout  # utxo index\n    return TxOut.from_json(provider.getrawtransaction(utxo.txid,\n                           1)['vout'][index],\n                           network=network_params)", "response": "find parent outputs of utxo"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsigning transaction with Kutil", "response": "def sign_transaction(provider: Provider, unsigned: MutableTransaction,\n                     key: Kutil) -> Transaction:\n    '''sign transaction with Kutil'''\n\n    parent_outputs = [find_parent_outputs(provider, i) for i in unsigned.ins]\n    return key.sign_transaction(parent_outputs, unsigned)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_style(style='basic', **kwargs):\n    ''' Changes Matplotlib basic style to produce high quality graphs. Call \n    this function at the beginning of your script. You can even further improve\n    graphs with a call to fix_style at the end of your script.\n\n    Parameters\n    ----------    \n    style: string\n        'basic', 'article', 'poster', 'B&W', 'talk', 'origin'\n\n    kwargs: dict of rcParams\n        add Matplotlib rcParams    \n\n    Examples\n    --------\n    >>> set_style('article')\n\n    >>> set_style('poster',**{'lines.linewidth':2})\n    \n    See Also\n    --------\n    \n    :func:`~publib.publib.fix_style`,\n    :func:`~publib.tools.tools.reset_defaults`,\n    :func:`~publib.tools.tools.regenerate_fonts`\n\n    '''\n\n    style = _read_style(style)\n\n    # Add basic style as the first style\n    if style[0] != 'basic':\n        style = ['basic'] + style\n\n    # Apply all styles\n    for s in style:\n        _set_style(s, **kwargs)", "response": "Changes Matplotlib basic style to produce high quality graphs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fix_style(style='basic', ax=None, **kwargs):\n    ''' \n    Add an extra formatting layer to an axe, that couldn't be changed directly \n    in matplotlib.rcParams or with styles. Apply this function to every axe \n    you created.\n\n    Parameters\n    ----------    \n    ax: a matplotlib axe. \n        If None, the last axe generated is used \n    style: string or list of string\n        ['basic', 'article', 'poster', 'B&W','talk','origin'] \n        one of the styles previously defined. It should match the style you \n        chose in set_style but nothing forces you to.\n    kwargs: dict\n        edit any of the style_params keys. ex:\n            \n        >>> tight_layout=False\n\n    Examples\n    --------\n    plb.set_style('poster')\n    plt.plot(a,np.cos(a))\n    plb.fix_style('poster',**{'draggable_legend':False})    \n    \n    See Also\n    --------\n    \n    :func:`~publib.publib.set_style`\n    :func:`~publib.tools.tools.reset_defaults`\n\n    '''\n\n    style = _read_style(style)\n\n    # Apply all styles\n    for s in style:\n\n        if not s in style_params.keys():\n            avail = [f.replace('.mplstyle', '') for f in os.listdir(\n                _get_lib()) if f.endswith('.mplstyle')]\n            raise ValueError('{0} is not a valid style. '.format(s) +\n                             'Please pick a style from the list available in ' +\n                             '{0}: {1}'.format(_get_lib(), avail))\n\n    _fix_style(style, ax, **kwargs)", "response": "Add an extra formatting layer to every axe that can be changed directly by matplotlib. rcParams or with styles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndealing with different style format ( str list tuple )", "response": "def _read_style(style):\n    ''' Deal with different style format (str, list, tuple)'''\n\n    if isinstance(style, string_types):\n        style = [style]\n    else:\n        style = list(style)\n\n    return style"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the label for the output files for this calculation", "response": "def _get_label(self):\n        '''Find the label for the output files \n         for this calculation\n        '''\n        if self._label is None:\n            foundfiles = False\n            for f in self._files:\n                if \".files\" in f: \n                    foundfiles = True\n                    self._label = f.split(\".\")[0]\n                    with open(self._label + '.files', 'r') as fp:\n                        line = fp.readline().split()[0]\n                        if line != self._label + \".in\":\n                           fp.close()\n                           raise Exception('first line must be label.in')\n                        line = fp.readline().split()[0]\n                        if line != self._label + \".txt\":\n                           fp.close()\n                           raise Exception('second line must be label.txt')\n                        line = fp.readline().split()[0]\n                        if line != self._label + \"i\":\n                           fp.close()\n                           raise Exception('third line must be labeli')\n                        line = fp.readline().split()[0]\n                        if line != self._label + \"o\":\n                           fp.close()\n                           raise Exception('fourth line must be labelo')\n                        fp.close()\n            if foundfiles:\n                return self._label\n            else:\n                raise Exception('label.files not found')\n                                \n#ASE format\n#        (self.prefix + '.in') # input\n#        (self.prefix + '.txt')# output\n#        (self.prefix + 'i')   # input\n#        (self.prefix + 'o')   # output\n                \n        else:\n            return self._label"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the next player in the game.", "response": "def next_player(self):\n        \"\"\"\n        Returns the player whose turn it will be next.\n\n        Uses regular seat-wise clockwise rotation.\n\n        Compare to GameStatePreGame's implementation, which uses snake draft.\n\n        :return Player\n        \"\"\"\n        logging.warning('turn={}, players={}'.format(\n            self.game._cur_turn,\n            self.game.players\n        ))\n        return self.game.players[(self.game._cur_turn + 1) % len(self.game.players)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _estimate_progress(self):\n        estimate = True\n        # ==== get the current subscript and the time it takes to execute it =====\n        current_subscript = self._current_subscript_stage['current_subscript']\n\n        # ==== get the number of subscripts =====\n        num_subscripts = len(self.scripts)\n\n        # ==== get number of iterations and loop index ======================\n        if self.iterator_type == 'loop':\n            num_iterations = self.settings['num_loops']\n        elif self.iterator_type == 'sweep':\n            sweep_range = self.settings['sweep_range']\n            if self.settings['stepping_mode'] == 'value_step':\n                num_iterations = int((sweep_range['max_value'] - sweep_range['min_value']) / sweep_range['N/value_step']) + 1\n                # len(np.linspace(sweep_range['min_value'], sweep_range['max_value'],\n                #                                        (sweep_range['max_value'] - sweep_range['min_value']) /\n                #                                        sweep_range['N/value_step'] + 1, endpoint=True).tolist())\n            elif self.settings['stepping_mode'] == 'N':\n                num_iterations = sweep_range['N/value_step']\n            else:\n                raise KeyError('unknown key' + self.settings['stepping_mode'])\n\n        else:\n            print('unknown iterator type in Iterator receive signal - can\\'t estimate ramining time')\n            estimate = False\n\n\n        if estimate:\n            # get number of loops (completed + 1)\n            loop_index = self.loop_index\n\n\n            if num_subscripts > 1:\n                # estimate the progress based on the duration the individual subscripts\n\n                loop_execution_time = 0.  # time for a single loop execution in s\n                sub_progress_time = 0.  # progress of current loop iteration in s\n\n                # ==== get typical duration of current subscript ======================\n                if current_subscript is not None:\n                    current_subscript_exec_duration = self._current_subscript_stage['subscript_exec_duration'][\n                        current_subscript.name].total_seconds()\n                else:\n                    current_subscript_exec_duration = 0.0\n\n\n                current_subscript_elapsed_time = (datetime.datetime.now() - current_subscript.start_time).total_seconds()\n                # estimate the duration of the current subscript if the script hasn't been executed once fully and subscript_exec_duration is 0\n                if current_subscript_exec_duration == 0.0:\n                    remaining_time = current_subscript.remaining_time.total_seconds()\n                    current_subscript_exec_duration = remaining_time + current_subscript_elapsed_time\n\n                # ==== get typical duration of one loop iteration ======================\n                remaining_scripts = 0  # script that remain to be executed for the first time\n                for subscript_name, duration in self._current_subscript_stage['subscript_exec_duration'].items():\n                    if duration.total_seconds() == 0.0:\n                        remaining_scripts += 1\n                    loop_execution_time += duration.total_seconds()\n                    # add the times of the subscripts that have been executed in the current loop\n                    # ignore the current subscript, because that will be taken care of later\n                    if self._current_subscript_stage['subscript_exec_count'][subscript_name] == loop_index \\\n                            and subscript_name is not current_subscript.name:\n                        # this subscript has already been executed in this iteration\n                        sub_progress_time += duration.total_seconds()\n\n                # add the proportional duration of the current subscript given by the subscript progress\n                sub_progress_time += current_subscript_elapsed_time\n\n                # if there are scripts that have not been executed yet\n                # assume that all the scripts that have not been executed yet take as long as the average of the other scripts\n                if remaining_scripts == num_subscripts:\n                    # none of the subscript has been finished. assume that all the scripts take as long as the first\n                    loop_execution_time = num_subscripts * current_subscript_exec_duration\n                elif remaining_scripts > 1:\n                    loop_execution_time = 1. * num_subscripts / (num_subscripts - remaining_scripts)\n                elif remaining_scripts == 1:\n                    # there is only one script left which is the current script\n                    loop_execution_time += current_subscript_exec_duration\n\n                if loop_execution_time > 0:\n                    progress_subscript = 100. * sub_progress_time / loop_execution_time\n                else:\n                    progress_subscript = 1. * progress_subscript / num_subscripts\n\n            # print(' === script iterator progress estimation loop_index = {:d}/{:d}, progress_subscript = {:f}'.format(loop_index, number_of_iterations, progress_subscript))\n            progress = 100. * (loop_index - 1. + 0.01 * progress_subscript) / num_iterations\n\n        else:\n            # if can't estimate the remaining time set to half\n            progress = 50\n        return progress", "response": "estimate the current progress of the current subscript and time it takes to execute it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassign the actual script settings depending on the iterator type this might be overwritten by classes that inherit form ScriptIterator Args: sub_scripts: dictionary with the subscripts script_order: execution order of subscripts script_execution_freq: execution frequency of subscripts Returns: the default setting for the iterator", "response": "def get_default_settings(sub_scripts, script_order, script_execution_freq, iterator_type):\n        \"\"\"\n        assigning the actual script settings depending on the iterator type\n\n        this might be overwritten by classes that inherit form ScriptIterator\n\n        Args:\n            sub_scripts: dictionary with the subscripts\n            script_order: execution order of subscripts\n            script_execution_freq: execution frequency of subscripts\n\n        Returns:\n            the default setting for the iterator\n\n        \"\"\"\n        def populate_sweep_param(scripts, parameter_list, trace=''):\n            '''\n\n            Args:\n                scripts: a dict of {'class name': <class object>} pairs\n\n            Returns: A list of all parameters of the input scripts\n\n            '''\n\n            def get_parameter_from_dict(trace, dic, parameter_list, valid_values=None):\n                \"\"\"\n                appends keys in the dict to a list in the form trace.key.subkey.subsubkey...\n                Args:\n                    trace: initial prefix (path through scripts and parameters to current location)\n                    dic: dictionary\n                    parameter_list: list to which append the parameters\n\n                    valid_values: valid values of dictionary values if None dic should be a dictionary\n\n                Returns:\n\n                \"\"\"\n                if valid_values is None and isinstance(dic, Parameter):\n                    valid_values = dic.valid_values\n\n                for key, value in dic.items():\n                    if isinstance(value, dict):  # for nested parameters ex {point: {'x': int, 'y': int}}\n                        parameter_list = get_parameter_from_dict(trace + '.' + key, value, parameter_list,\n                                                                 dic.valid_values[key])\n                    elif (valid_values[key] in (float, int)) or \\\n                            (isinstance(valid_values[key], list) and valid_values[key][0] in (float, int)):\n                        parameter_list.append(trace + '.' + key)\n                    else:  # once down to the form {key: value}\n                        # in all other cases ignore parameter\n                        print(('ignoring sweep parameter', key))\n\n                return parameter_list\n\n            for script_name in list(scripts.keys()):\n                from pylabcontrol.core import ScriptIterator\n                script_trace = trace\n                if script_trace == '':\n                    script_trace = script_name\n                else:\n                    script_trace = script_trace + '->' + script_name\n                if issubclass(scripts[script_name], ScriptIterator):  # gets subscripts of ScriptIterator objects\n                    populate_sweep_param(vars(scripts[script_name])['_SCRIPTS'], parameter_list=parameter_list,\n                                         trace=script_trace)\n                else:\n                    # use inspect instead of vars to get _DEFAULT_SETTINGS also for classes that inherit _DEFAULT_SETTINGS from a superclass\n                    for setting in \\\n                    [elem[1] for elem in inspect.getmembers(scripts[script_name]) if elem[0] == '_DEFAULT_SETTINGS'][0]:\n                        parameter_list = get_parameter_from_dict(script_trace, setting, parameter_list)\n\n            return parameter_list\n\n        if iterator_type == 'loop':\n            script_default_settings = [\n                Parameter('script_order', script_order),\n                Parameter('script_execution_freq', script_execution_freq),\n                Parameter('num_loops', 0, int, 'times the subscripts will be executed'),\n                Parameter('run_all_first', True, bool, 'Run all scripts with nonzero frequency in first pass')\n            ]\n\n        elif iterator_type == 'sweep':\n\n            sweep_params = populate_sweep_param(sub_scripts, [])\n\n            script_default_settings = [\n                Parameter('script_order', script_order),\n                Parameter('script_execution_freq', script_execution_freq),\n                Parameter('sweep_param', sweep_params[0], sweep_params, 'variable over which to sweep'),\n                Parameter('sweep_range',\n                          [Parameter('min_value', 0, float, 'min parameter value'),\n                           Parameter('max_value', 0, float, 'max parameter value'),\n                           Parameter('N/value_step', 0, float,\n                                     'either number of steps or parameter value step, depending on mode')]),\n                Parameter('stepping_mode', 'N', ['N', 'value_step'],\n                          'Switch between number of steps and step amount'),\n                Parameter('run_all_first', True, bool, 'Run all scripts with nonzero frequency in first pass')\n            ]\n        else:\n            print(('unknown iterator type ' + iterator_type))\n            raise TypeError('unknown iterator type ' + iterator_type)\n\n        return script_default_settings"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert raster to ESRI shapefile", "response": "def raster2shp(rasterfile, vectorshp, layername=None, fieldname=None,\n                   band_num=1, mask='default'):\n        \"\"\"Convert raster to ESRI shapefile\"\"\"\n        FileClass.remove_files(vectorshp)\n        FileClass.check_file_exists(rasterfile)\n        # this allows GDAL to throw Python Exceptions\n        gdal.UseExceptions()\n        src_ds = gdal.Open(rasterfile)\n        if src_ds is None:\n            print('Unable to open %s' % rasterfile)\n            sys.exit(1)\n        try:\n            srcband = src_ds.GetRasterBand(band_num)\n        except RuntimeError as e:\n            # for example, try GetRasterBand(10)\n            print('Band ( %i ) not found, %s' % (band_num, e))\n            sys.exit(1)\n        if mask == 'default':\n            maskband = srcband.GetMaskBand()\n        elif mask is None or mask.upper() == 'NONE':\n            maskband = None\n        else:\n            mask_ds = gdal.Open(mask)\n            maskband = mask_ds.GetRasterBand(1)\n        #  create output datasource\n        if layername is None:\n            layername = FileClass.get_core_name_without_suffix(rasterfile)\n        drv = ogr_GetDriverByName(str('ESRI Shapefile'))\n        dst_ds = drv.CreateDataSource(vectorshp)\n        srs = None\n        if src_ds.GetProjection() != '':\n            srs = osr_SpatialReference()\n            srs.ImportFromWkt(src_ds.GetProjection())\n        dst_layer = dst_ds.CreateLayer(str(layername), srs=srs)\n        if fieldname is None:\n            fieldname = layername.upper()\n        fd = ogr_FieldDefn(str(fieldname), OFTInteger)\n        dst_layer.CreateField(fd)\n        dst_field = 0\n        result = gdal.Polygonize(srcband, maskband, dst_layer, dst_field,\n                                 ['8CONNECTED=8'], callback=None)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert shapefile to geojson file", "response": "def convert2geojson(jsonfile, src_srs, dst_srs, src_file):\n        \"\"\"convert shapefile to geojson file\"\"\"\n        if os.path.exists(jsonfile):\n            os.remove(jsonfile)\n        if sysstr == 'Windows':\n            exepath = '\"%s/Lib/site-packages/osgeo/ogr2ogr\"' % sys.exec_prefix\n        else:\n            exepath = FileClass.get_executable_fullpath('ogr2ogr')\n        # os.system(s)\n        s = '%s -f GeoJSON -s_srs \"%s\" -t_srs %s %s %s' % (\n            exepath, src_srs, dst_srs, jsonfile, src_file)\n        UtilClass.run_command(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_line_shp(line_list, out_shp):\n        print('Write line shapefile: %s' % out_shp)\n        driver = ogr_GetDriverByName(str('ESRI Shapefile'))\n        if driver is None:\n            print('ESRI Shapefile driver not available.')\n            sys.exit(1)\n        if os.path.exists(out_shp):\n            driver.DeleteDataSource(out_shp)\n        ds = driver.CreateDataSource(out_shp.rpartition(os.sep)[0])\n        if ds is None:\n            print('ERROR Output: Creation of output file failed.')\n            sys.exit(1)\n        lyr = ds.CreateLayer(str(out_shp.rpartition(os.sep)[2].split('.')[0]), None, wkbLineString)\n        for l in line_list:\n            line = ogr_Geometry(wkbLineString)\n            for i in l:\n                line.AddPoint(i[0], i[1])\n            templine = ogr_CreateGeometryFromJson(line.ExportToJson())\n            feature = ogr_Feature(lyr.GetLayerDefn())\n            feature.SetGeometry(templine)\n            lyr.CreateFeature(feature)\n            feature.Destroy()\n        ds.Destroy()", "response": "Export ESRI Shapefile -- Line feature"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the consensus sequence of an alignment as a string.", "response": "def consensus(aln, weights=None, gap_threshold=0.5, simple=False, trim_ends=True):\n    \"\"\"Get the consensus of an alignment, as a string.\n\n    Emit gap characters for majority-gap columns; apply various strategies to\n    choose the consensus amino acid type for the remaining columns.\n\n    Parameters\n    ----------\n\n    simple : bool\n        If True, use simple plurality to determine the consensus amino acid\n        type, without weighting sequences for similarity. Otherwise, weight\n        sequences for similarity and use relative entropy to choose the\n        consensus amino acid type.\n    weights : dict or None\n        Sequence weights. If given, used to calculate amino acid frequencies;\n        otherwise calculated within this function (i.e. this is a way to speed\n        up the function if sequence weights have already been calculated).\n        Ignored in 'simple' mode.\n    trim_ends : bool\n        If False, stretch the consensus sequence to include the N- and C-tails\n        of the alignment, even if those flanking columns are mostly gap\n        characters. This avoids terminal gaps in the consensus (needed for\n        MAPGAPS).\n    gap_threshold : float\n        If the proportion of gap characters in a column is greater than or equal\n        to this value (after sequence weighting, if applicable), then the\n        consensus character emitted will be a gap instead of an amino acid type.\n\n    \"\"\"\n    # Choose your algorithms!\n    if simple:\n        # Use the simple, unweighted algorithm\n        col_consensus = make_simple_col_consensus(alnutils.aa_frequencies(aln))\n        def is_majority_gap(col):\n            return (float(col.count('-')) / len(col) >= gap_threshold)\n        # ENH (alternatively/additionally): does any aa occur more than once?\n        # ENH: choose gap-decisionmaking separately from col_consensus\n    else:\n        # Use the entropy-based, weighted algorithm\n        if weights is None:\n            seq_weights = alnutils.sequence_weights(aln, 'avg1')\n        else:\n            seq_weights = weights\n        aa_frequencies = alnutils.aa_frequencies(aln, weights=seq_weights)\n        col_consensus = make_entropy_col_consensus(aa_frequencies)\n        def is_majority_gap(col):\n            gap_count = 0.0\n            for wt, char in zip(seq_weights, col):\n                if char == '-':\n                    gap_count += wt\n            return (gap_count / sum(seq_weights) >= gap_threshold)\n\n    # Traverse the alignment, handling gaps etc.\n    def col_wise_consensus(columns):\n        \"\"\"Calculate the consensus chars for an iterable of columns.\"\"\"\n        if not trim_ends:\n            # Track if we're in the N-term or C-term end of the sequence\n            in_left_end = True\n            maybe_right_tail = []\n        # prev_col = None\n        # prev_char = None\n        for col in columns:\n            # Lowercase cols mean explicitly, \"don't include in consensus\"\n            if all(c.islower() for c in col if c not in '.-'):\n                yield '-'\n                continue\n            if any(c.islower() for c in col):\n                logging.warn('Mixed lowercase and uppercase letters in a '\n                        'column: ' + ''.join(col))\n                col = map(str.upper, col)\n\n            # Gap chars\n            is_gap = is_majority_gap(col)\n            if not trim_ends:\n                # Avoid N-terminal gaps in the consensus sequence\n                if in_left_end:\n                    if not is_gap:\n                        # Match -- we're no longer in the left end\n                        in_left_end = False\n                    is_gap = False\n\n            # When to yield a gap here:\n            #   -----------     ---------   ------  ----------\n            #   in_left_end     trim_ends   is_gap  yield gap?\n            #   -----------     ---------   ------  ----------\n            #   True            True        (True)  yes\n            #   True            False       (False) (no -- def. char)\n            #   False           True        T/F     yes, if is_gap\n            #   False           False       (T/F)   NO! use maybe_right_tail\n            #   -----------     ---------   ------  ----------\n\n            if is_gap and trim_ends:\n                yield '-'\n                continue\n\n            # Get the consensus character, using the chosen algorithm\n            cons_char = col_consensus(col)\n\n            if trim_ends:\n                yield cons_char\n            else:\n                # Avoid C-terminal gaps in the consensus sequence\n                if is_gap:\n                    maybe_right_tail.append(cons_char)\n                else:\n                    # Match -> gaps weren't the right tail; emit all gaps\n                    for char in maybe_right_tail:\n                        yield '-'\n                    maybe_right_tail = []\n                    yield cons_char\n\n            # prev_col = col\n            # prev_char = cons_char\n\n        # Finally, if we were keeping a right (C-term) tail, emit it\n        if not trim_ends:\n            for char in maybe_right_tail:\n                yield char\n\n    return ''.join(col_wise_consensus(zip(*aln)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_simple_col_consensus(bg_freqs):\n    # Hack: use default kwargs to persist across iterations\n    def col_consensus(col, prev_col=[], prev_char=[]):\n        # Count the amino acid types in this column\n        aa_counts = sequtils.aa_frequencies(col)\n        assert aa_counts, \"Column is all gaps! That's not allowed.\"\n        # Take the most common residue(s)\n        best_char, best_score = max(aa_counts.iteritems(),\n                                    key=lambda kv: kv[1])\n        # Resolve ties\n        ties = [aa for aa in aa_counts if aa_counts[aa] == best_score]\n        if len(ties) > 1:\n            # Breaker #1: most common after the prev. consensus char\n            # Resolve a tied col by restricting to rows where the preceding\n            # char is the consensus type for that (preceding) col\n            if prev_char and prev_col:\n                mc_next = Counter(\n                        [b for a, b in zip(prev_col, col)\n                            if a == prev_char[0] and b in ties]\n                        ).most_common()\n                ties_next = [x[0] for x in mc_next\n                        if x[1] == mc_next[0][1]]\n                if ties_next:\n                    ties = ties_next\n            if len(ties) > 1:\n                # Breaker #2: lowest overall residue frequency\n                ties.sort(key=lambda aa: bg_freqs[aa])\n            best_char = ties[0]\n        else:\n            assert best_char == ties[0], \\\n                    'WTF %s != %s[0]' % (best_char, ties)\n        # Save values for tie-breaker #1\n        prev_col[:] = col\n        prev_char[:] = best_char\n        return best_char\n    return col_consensus", "response": "Create a simple column of amino acid types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn only the supported consensus residues in each column.", "response": "def supported(aln):\n    \"\"\"Get only the supported consensus residues in each column.\n\n    Meaning:\n    - Omit majority-gap columns\n    - Omit columns where no residue type appears more than once\n    - In case of a tie, return all the top-scoring residue types\n      (no prioritization)\n\n    Returns a *list* -- not a string! -- where elements are strings of the\n    consensus character(s), potentially a gap ('-') or multiple chars ('KR').\n    \"\"\"\n    def col_consensus(columns):\n        \"\"\"Calculate the consensus chars for an iterable of columns.\"\"\"\n        for col in columns:\n            if (# Majority gap chars\n                (col.count('-') >= len(col)/2) or\n                # Lowercase cols mean \"don't include in consensus\"\n                all(c.islower() for c in col if c not in '.-')\n                ):\n                yield '-'\n                continue\n            # Validation - copied from consensus() above\n            if any(c.islower() for c in col):\n                logging.warn('Mixed lowercase and uppercase letters in a '\n                        'column: ' + ''.join(col))\n                col = map(str.upper, col)\n            # Calculate the consensus character\n            most_common = Counter(\n                    [c for c in col if c not in '-']\n                    ).most_common()\n            if not most_common:\n                # XXX ever reached?\n                logging.warn(\"Column is all gaps! How did that happen?\")\n            if most_common[0][1] == 1:\n                # No char has frequency > 1; no consensus char\n                yield '-'\n            elif (len(most_common) > 1 and\n                    most_common[0][1] == most_common[1][1]):\n                # Tie for most-common residue type\n                ties = [x[0] for x in most_common\n                        if x[1] == most_common[0][1]]\n                yield ''.join(ties)\n            else:\n                yield most_common[0][0]\n\n    return list(col_consensus(zip(*aln)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(authorizedMemberCount=None, clock=None, controllerInstanceId=None, creationTime=None, ipAssignmentPools=None, ipLocalRoutes=None, memberRevisionCounter=None, multicastLimit=None, name=None, nwid=None, private=None, relays=None, revision=None, rules=None, v4AssignMode=None, v6AssignMode=None):\n\n        return NetworkConfig(\n            authorizedMemberCount=authorizedMemberCount,\n            clock=clock,\n            controllerInstanceId=controllerInstanceId,\n            creationTime=creationTime,\n            ipAssignmentPools=ipAssignmentPools,\n            ipLocalRoutes=ipLocalRoutes,\n            memberRevisionCounter=memberRevisionCounter,\n            multicastLimit=multicastLimit,\n            name=name,\n            nwid=nwid,\n            private=private,\n            relays=relays,\n            revision=revision,\n            rules=rules,\n            v4AssignMode=v4AssignMode,\n            v6AssignMode=v6AssignMode,\n        )", "response": "create a new NetworkConfig object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install_kernel_spec(self, app, dir_name, display_name,\n                            settings_module, ipython_arguments):\n        \"\"\"install an IPython >= 3.0 kernelspec that loads corral env\n\n        Thanks: django extensions\n\n        \"\"\"\n        ksm = app.kernel_spec_manager\n        try_spec_names = ['python3' if six.PY3 else 'python2', 'python']\n        if isinstance(try_spec_names, six.string_types):\n            try_spec_names = [try_spec_names]\n        ks = None\n        for spec_name in try_spec_names:\n            try:\n                ks = ksm.get_kernel_spec(spec_name)\n                break\n            except Exception:\n                continue\n        if not ks:\n            self.parser.error(\"No notebook (Python) kernel specs found\")\n\n        ks.display_name = display_name\n        ks.env[\"CORRAL_SETTINGS_MODULE\"] = settings_module\n        ks.argv.extend(ipython_arguments)\n\n        in_corral_dir, in_corral = os.path.split(os.path.realpath(sys.argv[0]))\n\n        pythonpath = ks.env.get(\n            'PYTHONPATH', os.environ.get('PYTHONPATH', ''))\n        pythonpath = pythonpath.split(':')\n        if in_corral_dir not in pythonpath:\n            pythonpath.append(in_corral_dir)\n        ks.env['PYTHONPATH'] = ':'.join(filter(None, pythonpath))\n\n        kernel_dir = os.path.join(ksm.user_kernel_dir, conf.PACKAGE)\n        if not os.path.exists(kernel_dir):\n            os.makedirs(kernel_dir)\n            shutil.copy(res.fullpath(\"logo-64x64.png\"), kernel_dir)\n        with open(os.path.join(kernel_dir, 'kernel.json'), 'w') as f:\n            f.write(ks.to_json())", "response": "install an IPython > = 3. 0 kernelspec that loads corral env\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cache_init(self):\n        cache_ = cache.get(self.CACHE_ENTRY_NAME)\n\n        if cache_ is None:\n            categories = get_category_model().objects.order_by('sort_order')\n\n            ids = {category.id: category for category in categories}\n            aliases = {category.alias: category for category in categories if category.alias}\n\n            parent_to_children = OrderedDict()  # Preserve aliases order.\n            for category in categories:\n                parent_category = ids.get(category.parent_id, False)\n                parent_alias = None\n                if parent_category:\n                    parent_alias = parent_category.alias\n                if parent_alias not in parent_to_children:\n                    parent_to_children[parent_alias] = []\n                parent_to_children[parent_alias].append(category.id)\n\n            cache_ = {\n                self.CACHE_NAME_IDS: ids,\n                self.CACHE_NAME_PARENTS: parent_to_children,\n                self.CACHE_NAME_ALIASES: aliases\n            }\n\n            cache.set(self.CACHE_ENTRY_NAME, cache_, self.CACHE_TIMEOUT)\n\n        self._cache = cache_", "response": "Initializes local cache from Django cache if required."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cache_get_entry(self, entry_name, key=ENTIRE_ENTRY_KEY, default=False):\n        if key is self.ENTIRE_ENTRY_KEY:\n            return self._cache[entry_name]\n        return self._cache[entry_name].get(key, default)", "response": "Returns the cache entry parameter value by its name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsorts the given aliases list returns a sorted list.", "response": "def sort_aliases(self, aliases):\n        \"\"\"Sorts the given aliases list, returns a sorted list.\n\n        :param list aliases:\n        :return: sorted aliases list\n        \"\"\"\n        self._cache_init()\n        if not aliases:\n            return aliases\n        parent_aliases = self._cache_get_entry(self.CACHE_NAME_PARENTS).keys()\n        return [parent_alias for parent_alias in parent_aliases if parent_alias in aliases]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a set of parent aliases for a list of child IDs.", "response": "def get_parents_for(self, child_ids):\n        \"\"\"Returns parent aliases for a list of child IDs.\n\n        :param list child_ids:\n        :rtype: set\n        :return: a set of parent aliases\n        \"\"\"\n        self._cache_init()\n        parent_candidates = []\n        for parent, children in self._cache_get_entry(self.CACHE_NAME_PARENTS).items():\n            if set(children).intersection(child_ids):\n                parent_candidates.append(parent)\n        return set(parent_candidates)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list with categories under the given parent.", "response": "def get_children_for(self, parent_alias=None, only_with_aliases=False):\n        \"\"\"Returns a list with with categories under the given parent.\n\n        :param str|None parent_alias: Parent category alias or None for categories under root\n        :param bool only_with_aliases: Flag to return only children with aliases\n        :return: a list of category objects\n        \"\"\"\n        self._cache_init()\n        child_ids = self.get_child_ids(parent_alias)\n        if only_with_aliases:\n            children = []\n            for cid in child_ids:\n                category = self.get_category_by_id(cid)\n                if category.alias:\n                    children.append(category)\n            return children\n        return [self.get_category_by_id(cid) for cid in child_ids]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_child_ids(self, parent_alias):\n        self._cache_init()\n        return self._cache_get_entry(self.CACHE_NAME_PARENTS, parent_alias, [])", "response": "Returns the child IDs of the given parent category"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns Category object by its alias.", "response": "def get_category_by_alias(self, alias):\n        \"\"\"Returns Category object by its alias.\n\n        :param str alias:\n        :rtype: Category|None\n        :return: category object\n        \"\"\"\n        self._cache_init()\n        return self._cache_get_entry(self.CACHE_NAME_ALIASES, alias, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn Category object by its id.", "response": "def get_category_by_id(self, cid):\n        \"\"\"Returns Category object by its id.\n\n        :param str cid:\n        :rtype: Category\n        :return: category object\n        \"\"\"\n        self._cache_init()\n        return self._cache_get_entry(self.CACHE_NAME_IDS, cid)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_category(self, parent_alias, title):\n        found = None\n        child_ids = self.get_child_ids(parent_alias)\n        for cid in child_ids:\n            category = self.get_category_by_id(cid)\n            if category.title.lower() == title.lower():\n                found = category\n                break\n        return found", "response": "Searches parent category children for the given title."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict with categories popularity stats.", "response": "def get_ties_stats(self, categories, target_model=None):\n        \"\"\"Returns a dict with categories popularity stats.\n\n        :param list categories:\n        :param Model|None target_model:\n        :return:\n        \"\"\"\n        filter_kwargs = {\n            'category_id__in': categories\n        }\n        if target_model is not None:\n            is_cls = hasattr(target_model, '__name__')\n            if is_cls:\n                concrete = False\n            else:\n                concrete = True\n                filter_kwargs['object_id'] = target_model.id\n            filter_kwargs['content_type'] = ContentType.objects.get_for_model(\n                target_model, for_concrete_model=concrete\n            )\n\n        return {\n            item['category_id']: item['ties_num'] for item in\n            get_tie_model().objects.filter(**filter_kwargs).values('category_id').annotate(ties_num=Count('category'))\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of subcategories or ties objects extended with information from their categories.", "response": "def get_categories(self, parent_aliases=None, target_object=None, tied_only=True):\n        \"\"\"Returns subcategories (or ties if `target_object` is set)\n        for the given parent category.\n\n        :param str|None|list parent_aliases:\n        :param ModelWithCategory|Model target_object:\n        :param bool tied_only: Flag to get only categories with ties. Ties stats are stored in `ties_num` attrs.\n        :return: a list of category objects or tie objects extended with information from their categories.\n        \"\"\"\n\n        single_mode = False\n        if not isinstance(parent_aliases, list):\n            single_mode = parent_aliases\n            parent_aliases = [parent_aliases]\n\n        all_children = []\n        parents_to_children = OrderedDict()\n        for parent_alias in parent_aliases:\n            child_ids = self.get_child_ids(parent_alias)\n            parents_to_children[parent_alias] = child_ids\n            if tied_only:\n                all_children.extend(child_ids)\n\n        ties = {}\n        if tied_only:\n            source = OrderedDict()\n            ties = self.get_ties_stats(all_children, target_object)\n            for parent_alias, child_ids in parents_to_children.items():\n                common = set(ties.keys()).intersection(child_ids)\n                if common:\n                    source[parent_alias] = common\n\n        else:\n            source = parents_to_children\n\n        categories = OrderedDict()\n        for parent_alias, child_ids in source.items():\n            for cat_id in child_ids:\n                cat = self.get_category_by_id(cat_id)\n                if tied_only:\n                    cat.ties_num = ties.get(cat_id, 0)\n\n                if parent_alias not in categories:\n                    categories[parent_alias] = []\n\n                categories[parent_alias].append(cat)\n\n        if single_mode != False:  # sic!\n            return categories[single_mode]\n\n        return categories"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads PeerAssets P2TH privkey into the local node.", "response": "def load_p2th_privkey_into_local_node(provider: RpcNode, prod: bool=True) -> None:\n    '''Load PeerAssets P2TH privkey into the local node.'''\n\n    assert isinstance(provider, RpcNode), {\"error\": \"Import only works with local node.\"}\n    error = {\"error\": \"Loading P2TH privkey failed.\"}\n    pa_params = param_query(provider.network)\n\n    if prod:\n        provider.importprivkey(pa_params.P2TH_wif, \"PAPROD\")\n        #  now verify if ismine == True\n        if not provider.validateaddress(pa_params.P2TH_addr)['ismine']:\n            raise P2THImportFailed(error)\n    else:\n        provider.importprivkey(pa_params.test_P2TH_wif, \"PATEST\")\n        if not provider.validateaddress(pa_params.test_P2TH_addr)['ismine']:\n            raise P2THImportFailed(error)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind transaction sender vin [ 0 is used in this case.", "response": "def find_tx_sender(provider: Provider, raw_tx: dict) -> str:\n    '''find transaction sender, vin[0] is used in this case.'''\n\n    vin = raw_tx[\"vin\"][0]\n    txid = vin[\"txid\"]\n    index = vin[\"vout\"]\n    return provider.getrawtransaction(txid, 1)[\"vout\"][index][\"scriptPubKey\"][\"addresses\"][0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_deck_spawns(provider: Provider, prod: bool=True) -> Iterable[str]:\n    '''find deck spawn transactions via Provider,\n    it requires that Deck spawn P2TH were imported in local node or\n    that remote API knows about P2TH address.'''\n\n    pa_params = param_query(provider.network)\n\n    if isinstance(provider, RpcNode):\n\n        if prod:\n            decks = (i[\"txid\"] for i in provider.listtransactions(\"PAPROD\"))\n        else:\n            decks = (i[\"txid\"] for i in provider.listtransactions(\"PATEST\"))\n\n    if isinstance(provider, Cryptoid) or isinstance(provider, Explorer):\n\n        if prod:\n            decks = (i for i in provider.listtransactions(pa_params.P2TH_addr))\n        else:\n            decks = (i for i in provider.listtransactions(pa_params.test_P2TH_addr))\n\n    return decks", "response": "find deck spawn transactions via Provider prod means that Deck spawn P2TH was imported in local node or remote API knows about P2TH address."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the index of this tx in the blockid", "response": "def tx_serialization_order(provider: Provider, blockhash: str, txid: str) -> int:\n    '''find index of this tx in the blockid'''\n\n    return provider.getblock(blockhash)[\"tx\"].index(txid)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode OP_RETURN message from vout [ 1 ]", "response": "def read_tx_opreturn(vout: dict) -> bytes:\n    '''Decode OP_RETURN message from vout[1]'''\n\n    asm = vout['scriptPubKey']['asm']\n    n = asm.find('OP_RETURN')\n    if n == -1:\n        raise InvalidNulldataOutput({'error': 'OP_RETURN not found.'})\n    else:\n        # add 10 because 'OP_RETURN ' is 10 characters\n        n += 10\n        data = asm[n:]\n        n = data.find(' ')\n        # make sure that we don't include trailing opcodes\n        if n == -1:\n            return bytes.fromhex(data)\n        else:\n            return bytes.fromhex(data[:n])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninterpret issue mode bitfeg yield list of names", "response": "def deck_issue_mode(proto: DeckSpawnProto) -> Iterable[str]:\n    '''interpret issue mode bitfeg'''\n\n    if proto.issue_mode == 0:\n        yield \"NONE\"\n        return\n\n    for mode, value in proto.MODE.items():\n        if value > proto.issue_mode:\n            continue\n        if value & proto.issue_mode:\n            yield mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef issue_mode_to_enum(deck: DeckSpawnProto, issue_mode: list) -> int:\n    '''encode issue mode(s) as bitfeg'''\n\n    # case where there are multiple issue modes specified\n    if isinstance(issue_mode, list) and len(issue_mode) > 1:\n        r = 0\n        for mode in issue_mode:\n            r += deck.MODE.Value(mode)\n        return r\n\n    elif isinstance(issue_mode, str):  # if single issue mode\n        return deck.MODE.Value(issue_mode)\n\n    else:\n        raise InvalidDeckIssueMode({'error': 'issue_mode given in wrong format.'})", "response": "encode issue mode s as bitfeg"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_deckspawn_metainfo(protobuf: bytes, version: int) -> dict:\n    '''Decode deck_spawn tx op_return protobuf message and validate it,\n       Raise error if deck_spawn metainfo incomplete or version mistmatch.'''\n\n    deck = DeckSpawnProto()\n    deck.ParseFromString(protobuf)\n\n    error = {\"error\": \"Deck ({deck}) metainfo incomplete, deck must have a name.\".format(deck=deck.name)}\n\n    if deck.name == \"\":\n        raise InvalidDeckMetainfo(error)\n\n    if deck.version != version:\n        raise InvalidDeckVersion({\"error\", \"Deck version mismatch.\"})\n\n    return {\n        \"version\": deck.version,\n        \"name\": deck.name,\n        \"issue_mode\": deck.issue_mode,\n        \"number_of_decimals\": deck.number_of_decimals,\n        \"asset_specific_data\": deck.asset_specific_data\n        }", "response": "Decode deck_spawn tx op_return protobuf message and validate it raise InvalidDeckVersion Raise error if deck_spawn metainfo incomplete or version mistmatch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_deckspawn_p2th(provider: Provider, rawtx: dict, p2th: str) -> bool:\n    '''Return True if deck spawn pays to p2th in vout[0] and if the P2TH address\n    is correct. Otherwise raises InvalidDeckSpawn.\n    '''\n\n    try:\n        vout = rawtx[\"vout\"][0][\"scriptPubKey\"].get(\"addresses\")[0]\n    except TypeError:\n        '''TypeError: 'NoneType' object is not subscriptable error on some of the deck spawns.'''\n        raise InvalidDeckSpawn(\"Invalid Deck P2TH.\")\n\n    if not vout == p2th:\n        raise InvalidDeckSpawn(\"InvalidDeck P2TH.\")\n\n    return True", "response": "Return True if deck spawn pays to p2th in vout [ 0 ] and the P2TH address\n    is correct. Otherwise raises InvalidDeckSpawn."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_deck_p2th_into_local_node(provider: RpcNode, deck: Deck) -> None:\n    '''\n    load deck p2th into local node via \"importprivke\",\n    this allows building of proof-of-timeline for this deck\n    '''\n\n    assert isinstance(provider, RpcNode), {\"error\": \"You can load privkeys only into local node.\"}\n    error = {\"error\": \"Deck P2TH import went wrong.\"}\n\n    provider.importprivkey(deck.p2th_wif, deck.id)\n    check_addr = provider.validateaddress(deck.p2th_address)\n\n    if not check_addr[\"isvalid\"] and not check_addr[\"ismine\"]:\n        raise DeckP2THImportError(error)", "response": "Load deck p2th into local node via importprivke"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates if card_transfer transaction pays to deck p2th in vout [ 0 ]", "response": "def validate_card_transfer_p2th(deck: Deck, vout: dict) -> None:\n    '''validate if card_transfer transaction pays to deck p2th in vout[0]'''\n\n    error = {\"error\": \"Card transfer is not properly tagged.\"}\n\n    try:\n        address = vout[\"scriptPubKey\"].get(\"addresses\")[0]\n        if not address == deck.p2th_address:\n            raise InvalidCardTransferP2TH(error)\n    except TypeError as e:\n        raise e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding card spawn protobuf message and validate it against deck. version", "response": "def parse_card_transfer_metainfo(protobuf: bytes, deck_version: int) -> dict:\n    '''decode card_spawn protobuf message and validate it against deck.version\n    :protobuf - bytes from op_return message\n    :deck_version - integer\n    '''\n\n    card = CardTransferProto()\n    card.ParseFromString(protobuf)\n\n    if not card.version == deck_version:\n        raise CardVersionMismatch({'error': 'card version does not match deck version.'})\n\n    return {\n        \"version\": card.version,\n        \"number_of_decimals\": card.number_of_decimals,\n        \"amount\": list(card.amount),\n        \"asset_specific_data\": card.asset_specific_data\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the log file in the proper format.", "response": "def init_log(logger, filename=None, loglevel=None):\n    \"\"\"\n    Initializes the log file in the proper format.\n    \n    Arguments:\n    \n        filename (str): Path to a file. Or None if logging is to\n                         be disabled.\n        loglevel (str): Determines the level of the log output.\n    \"\"\"\n    \n    formatter = logging.Formatter(\n                    '[%(asctime)s] %(levelname)s: %(name)s: %(message)s'\n                )\n    \n    if loglevel:\n        logger.setLevel(getattr(logging, loglevel))\n\n    # We will allways print warnings and higher to stderr\n    ch = logging.StreamHandler()\n    ch.setLevel('WARNING')\n    ch.setFormatter(formatter)\n    \n    if filename:\n        fi = logging.FileHandler(filename, encoding='utf-8')\n        if loglevel:\n            fi.setLevel(getattr(logging, loglevel))\n        fi.setFormatter(formatter)\n        logger.addHandler(fi)\n    # If no logfile is provided we print all log messages that the user has \n    # defined to stderr\n    else:\n        if loglevel:\n            ch.setLevel(getattr(logging, loglevel))\n\n    logger.addHandler(ch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef param_query(name: str) -> PAParams:\n    '''Find the PAParams for a network by its long or short name. Raises\n    UnsupportedNetwork if no PAParams is found.\n    '''\n\n    for pa_params in params:\n        if name in (pa_params.network_name, pa_params.network_shortname,):\n            return pa_params\n\n    raise UnsupportedNetwork", "response": "Find the PAParams for a network by its long or short name. Raises UnsupportedNetwork if no PAParams is found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting activated when the user clicks on a plot Args: mouse_event:", "response": "def plot_clicked(self, mouse_event):\n        \"\"\"\n        gets activated when the user clicks on a plot\n        Args:\n            mouse_event:\n        \"\"\"\n        if isinstance(self.current_script, SelectPoints) and self.current_script.is_running:\n            if (not (mouse_event.xdata == None)):\n                if (mouse_event.button == 1):\n                    pt = np.array([mouse_event.xdata, mouse_event.ydata])\n                    self.current_script.toggle_NV(pt)\n                    self.current_script.plot([self.matplotlibwidget_1.figure])\n                    self.matplotlibwidget_1.draw()\n\n        item = self.tree_scripts.currentItem()\n\n        if item is not None:\n            if item.is_point():\n                item_x = item.child(1)\n                if mouse_event.xdata is not None:\n                    self.tree_scripts.setCurrentItem(item_x)\n                    item_x.value = float(mouse_event.xdata)\n                    item_x.setText(1, '{:0.3f}'.format(float(mouse_event.xdata)))\n                item_y = item.child(0)\n                if mouse_event.ydata is not None:\n                    self.tree_scripts.setCurrentItem(item_y)\n                    item_y.value = float(mouse_event.ydata)\n                    item_y.setText(1, '{:0.3f}'.format(float(mouse_event.ydata)))\n\n                # focus back on item\n                self.tree_scripts.setCurrentItem(item)\n            else:\n                if item.parent() is not None:\n                    if item.parent().is_point():\n                        if item == item.parent().child(1):\n                            if mouse_event.xdata is not None:\n                                item.setData(1, 2, float(mouse_event.xdata))\n                        if item == item.parent().child(0):\n                            if mouse_event.ydata is not None:\n                                item.setData(1, 2, float(mouse_event.ydata))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening file dialog to load scripts into the GUI", "response": "def load_scripts(self):\n            \"\"\"\n            opens file dialog to load scripts into gui\n            \"\"\"\n\n\n            # update scripts so that current settings do not get lost\n            for index in range(self.tree_scripts.topLevelItemCount()):\n                script_item = self.tree_scripts.topLevelItem(index)\n                self.update_script_from_item(script_item)\n\n\n            dialog = LoadDialog(elements_type=\"scripts\", elements_old=self.scripts,\n                                filename=self.gui_settings['scripts_folder'])\n            if dialog.exec_():\n                self.gui_settings['scripts_folder'] = str(dialog.txt_probe_log_path.text())\n                scripts = dialog.get_values()\n                added_scripts = set(scripts.keys()) - set(self.scripts.keys())\n                removed_scripts = set(self.scripts.keys()) - set(scripts.keys())\n\n                if 'data_folder' in list(self.gui_settings.keys()) and os.path.exists(self.gui_settings['data_folder']):\n                    data_folder_name = self.gui_settings['data_folder']\n                else:\n                    data_folder_name = None\n\n                # create instances of new instruments/scripts\n                self.scripts, loaded_failed, self.instruments = Script.load_and_append(\n                    script_dict={name: scripts[name] for name in added_scripts},\n                    scripts=self.scripts,\n                    instruments=self.instruments,\n                    log_function=self.log,\n                    data_path=data_folder_name)\n\n                # delete instances of new instruments/scripts that have been deselected\n                for name in removed_scripts:\n                    del self.scripts[name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nediting the tree item with the current filepath", "response": "def edit_tree_item(self):\n        \"\"\"\n        if sender is self.tree_gui_settings this will open a filedialog and ask for a filepath\n        this filepath will be updated in the field of self.tree_gui_settings that has been double clicked\n        \"\"\"\n\n        def open_path_dialog(path):\n            \"\"\"\n            opens a file dialog to get the path to a file and\n            \"\"\"\n            dialog = QtWidgets.QFileDialog()\n            dialog.setFileMode(QtWidgets.QFileDialog.Directory)\n            dialog.setOption(QtWidgets.QFileDialog.ShowDirsOnly, True)\n            path = dialog.getExistingDirectory(self, 'Select a folder:', path)\n\n            return path\n\n        tree = self.sender()\n\n        if tree == self.tree_gui_settings:\n\n            index = tree.selectedIndexes()[0]\n            model = index.model()\n\n            if index.column() == 1:\n                path = model.itemFromIndex(index).text()\n                path = str(open_path_dialog(path))\n\n                key = str(model.itemFromIndex(model.index(index.row(), 0)).text())\n\n                if path != \"\":\n                    self.gui_settings.update({key : str(path)})\n                    self.fill_treeview(tree, self.gui_settings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_config(self, file_name):\n\n        # load config or default if invalid\n\n        def load_settings(file_name):\n            \"\"\"\n            loads a old_gui settings file (a json dictionary)\n            - path_to_file: path to file that contains the dictionary\n\n            Returns:\n                - instruments: depth 1 dictionary where keys are instrument names and values are instances of instruments\n                - scripts:  depth 1 dictionary where keys are script names and values are instances of scripts\n                - probes: depth 1 dictionary where to be decided....?\n            \"\"\"\n\n            instruments_loaded = {}\n            probes_loaded = {}\n            scripts_loaded = {}\n\n            if os.path.isfile(file_name):\n                in_data = load_b26_file(file_name)\n\n                instruments = in_data['instruments'] if 'instruments' in in_data else {}\n                scripts = in_data['scripts'] if 'scripts' in in_data else {}\n                probes = in_data['probes'] if 'probes' in in_data else {}\n\n                instruments_loaded, failed = Instrument.load_and_append(instruments)\n                if len(failed) > 0:\n                    print(('WARNING! Following instruments could not be loaded: ', failed))\n\n                scripts_loaded, failed, instruments_loaded = Script.load_and_append(\n                    script_dict=scripts,\n                    instruments=instruments_loaded,\n                    log_function=self.log,\n                    data_path=self.gui_settings['data_folder'])\n\n                if len(failed) > 0:\n                    print(('WARNING! Following scripts could not be loaded: ', failed))\n\n                probes_loaded, failed, instruments_loadeds = Probe.load_and_append(\n                    probe_dict=probes,\n                    probes=probes_loaded,\n                    instruments=instruments_loaded)\n            return instruments_loaded, scripts_loaded, probes_loaded\n\n        print(('loading script/instrument/probes config from {:s}'.format(file_name)))\n        try:\n            config = load_b26_file(file_name)['gui_settings']\n            if config['settings_file'] != file_name:\n                print((\n                'WARNING path to settings file ({:s}) in config file is different from path of settings file ({:s})'.format(\n                    config['settings_file'], file_name)))\n            config['settings_file'] = file_name\n            print(('loading of {:s} successful'.format(file_name)))\n        except Exception:\n            print(('WARNING path to settings file ({:s}) invalid use default settings'.format(file_name)))\n            config = self._DEFAULT_CONFIG\n\n\n            for x in list(self._DEFAULT_CONFIG.keys()):\n                if x in config:\n                    if not os.path.exists(config[x]):\n                        try:\n                            os.makedirs(config[x])\n                        except Exception:\n                            config[x] = self._DEFAULT_CONFIG[x]\n                            os.makedirs(config[x])\n                            print(('WARNING: failed validating or creating path: set to default path'.format(config[x])))\n                else:\n                    config[x] = self._DEFAULT_CONFIG[x]\n                    os.makedirs(config[x])\n                    print(('WARNING: path {:s} not specified set to default {:s}'.format(x, config[x])))\n\n        # check if file_name is a valid filename\n        if os.path.exists(os.path.dirname(file_name)):\n            config['settings_file'] = file_name\n\n        self.gui_settings = config\n\n        self.instruments, self.scripts, self.probes = load_settings(file_name)\n\n\n        self.refresh_tree(self.tree_gui_settings, self.gui_settings)\n        self.refresh_tree(self.tree_scripts, self.scripts)\n        self.refresh_tree(self.tree_settings, self.instruments)\n\n        self._hide_parameters(file_name)", "response": "loads a config file and returns a tuple of instruments scripts probes and settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_config(self, out_file_name):\n\n        def get_hidden_parameter(item):\n\n            numer_of_sub_elements = item.childCount()\n\n            if numer_of_sub_elements == 0:\n                dictator = {item.name : item.visible}\n            else:\n                dictator = {item.name:{}}\n                for child_id in range(numer_of_sub_elements):\n                    dictator[item.name].update(get_hidden_parameter(item.child(child_id)))\n            return dictator\n\n        out_file_name = str(out_file_name)\n        if not os.path.exists(os.path.dirname(out_file_name)):\n            os.makedirs(os.path.dirname(out_file_name))\n\n        # build a dictionary for the configuration of the hidden parameters\n        dictator = {}\n        for index in range(self.tree_scripts.topLevelItemCount()):\n            script_item = self.tree_scripts.topLevelItem(index)\n            dictator.update(get_hidden_parameter(script_item))\n\n        dictator = {\"gui_settings\": self.gui_settings, \"scripts_hidden_parameters\":dictator}\n\n        # update the internal dictionaries from the trees in the gui\n        for index in range(self.tree_scripts.topLevelItemCount()):\n            script_item = self.tree_scripts.topLevelItem(index)\n            self.update_script_from_item(script_item)\n\n        dictator.update({'instruments': {}, 'scripts': {}, 'probes': {}})\n\n        for instrument in self.instruments.values():\n            dictator['instruments'].update(instrument.to_dict())\n        for script in self.scripts.values():\n            dictator['scripts'].update(script.to_dict())\n\n        for instrument, probe_dict in self.probes.items():\n            dictator['probes'].update({instrument: ','.join(list(probe_dict.keys()))})\n\n        with open(out_file_name, 'w') as outfile:\n            tmp = json.dump(dictator, outfile, indent=4)", "response": "Save the gui configuration to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getblockhash(self, index: int) -> str:\n        '''Returns the hash of the block at ; index 0 is the genesis block.'''\n\n        return cast(str, self.api_fetch('getblockhash?index=' + str(index)))", "response": "Returns the hash of the block at the specified index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn information about the block with the given hash.", "response": "def getblock(self, hash: str) -> dict:\n        '''Returns information about the block with the given hash.'''\n\n        return cast(dict, self.api_fetch('getblock?hash=' + hash))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getrawtransaction(self, txid: str, decrypt: int=0) -> dict:\n        '''Returns raw transaction representation for given transaction id.\n        decrypt can be set to 0(false) or 1(true).'''\n\n        q = 'getrawtransaction?txid={txid}&decrypt={decrypt}'.format(txid=txid, decrypt=decrypt)\n\n        return cast(dict, self.api_fetch(q))", "response": "Returns raw transaction representation for given transaction id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns information for given address.", "response": "def getaddress(self, address: str) -> dict:\n        '''Returns information for given address.'''\n\n        return cast(dict, self.ext_fetch('getaddress/' + address))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning unspent transactions for given address.", "response": "def listunspent(self, address: str) -> list:\n        '''Returns unspent transactions for given address.'''\n\n        try:\n            return cast(dict, self.ext_fetch('listunspent/' + address))['unspent_outputs']\n        except KeyError:\n            raise InsufficientFunds('Insufficient funds.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef txinfo(self, txid: str) -> dict:\n        '''Returns information about given transaction.'''\n\n        return cast(dict, self.ext_fetch('txinfo/' + txid))", "response": "Returns information about given transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn current balance of given address.", "response": "def getbalance(self, address: str) -> Decimal:\n        '''Returns current balance of given address.'''\n\n        try:\n            return Decimal(cast(float, self.ext_fetch('getbalance/' + address)))\n        except TypeError:\n            return Decimal(0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions that is called when the user clicks on a Toggle_NV button.", "response": "def _function(self):\n        \"\"\"\n        Waits until stopped to keep script live. Gui must handle calling of Toggle_NV function on mouse click.\n        \"\"\"\n\n        start_time = datetime.datetime.now()\n\n        # calculate stop time\n        if self.settings['wait_mode'] == 'absolute':\n            stop_time = start_time + datetime.timedelta(seconds= self.settings['wait_time'])\n        elif self.settings['wait_mode'] == 'loop_interval':\n            if self.last_execution is None:\n                stop_time = start_time\n            else:\n                loop_time = start_time - self.last_execution\n                wait_time = datetime.timedelta(seconds= self.settings['wait_time'])\n                if wait_time.total_seconds() <0:\n                    stop_time = start_time\n                else:\n                    stop_time = start_time + wait_time\n        else:\n            TypeError('unknown wait_mode')\n\n        current_time = start_time\n        while current_time<stop_time:\n            if self._abort:\n                break\n            current_time = datetime.datetime.now()\n\n            time.sleep(1)\n\n            self.progress = 100.*(current_time- start_time).total_seconds() / (stop_time - start_time).total_seconds()\n            self.updateProgress.emit(int(self.progress))\n\n        if self.settings['wait_mode'] == 'absolute':\n            self.last_execution = None\n        else:\n            self.last_execution = start_time"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract subelement from obj according to pointer.", "response": "def extract(self, obj, bypass_ref=False):\n        \"\"\"\n        Extract subelement from obj, according to pointer.\n        It assums that document is the object.\n\n        :param obj: the object source\n        :param bypass_ref: disable JSON Reference errors\n        \"\"\"\n        return self.pointer.extract(obj, bypass_ref)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing pointer into tokens", "response": "def parse(self, pointer):\n        \"\"\"parse pointer into tokens\"\"\"\n        if isinstance(pointer, Pointer):\n            return pointer.tokens[:]\n        elif pointer == '':\n            return []\n\n        tokens = []\n        staged, _, children = pointer.partition('/')\n        if staged:\n            try:\n                token = StagesToken(staged)\n                token.last = False\n                tokens.append(token)\n            except ValueError:\n                raise ParseError('pointer must start with / or int', pointer)\n\n        if _:\n            for part in children.split('/'):\n                part = part.replace('~1', '/')\n                part = part.replace('~0', '~')\n                token = ChildToken(part)\n                token.last = False\n                tokens.append(token)\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract(self, obj, bypass_ref=False):\n        for token in self.tokens:\n            obj = token.extract(obj, bypass_ref)\n        return obj", "response": "Extracts the subelement from obj according to the tokens."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts parent of the object according to current token.", "response": "def extract(self, obj, bypass_ref=False):\n        \"\"\"\n        Extract parent of obj, according to current token.\n\n        :param obj: the object source\n        :param bypass_ref: not used\n        \"\"\"\n        for i in range(0, self.stages):\n            try:\n                obj = obj.parent_obj\n            except AttributeError:\n                raise UnstagedError(obj, '{!r} must be staged before '\n                                         'exploring its parents'.format(obj))\n        if self.member:\n            return obj.parent_member\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract(self, obj, bypass_ref=False):\n        try:\n            if isinstance(obj, Mapping):\n                if not bypass_ref and '$ref' in obj:\n                    raise RefError(obj, 'presence of a $ref member')\n                obj = self.extract_mapping(obj)\n            elif isinstance(obj, Sequence) and not isinstance(obj, string_types):\n                obj = self.extract_sequence(obj)\n            else:\n                raise WrongType(obj, '{!r} does not apply '\n                                     'for {!r}'.format(str(self), obj))\n\n            if isinstance(obj, Mapping):\n                if not bypass_ref and '$ref' in obj:\n                    raise RefError(obj, 'presence of a $ref member')\n            return obj\n        except ExtractError as error:\n            logger.exception(error)\n            raise\n        except Exception as error:\n            logger.exception(error)\n            args = [arg for arg in error.args if arg not in (self, obj)]\n            raise ExtractError(obj, *args)", "response": "Extracts the subelement of obj according to current token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef digester(data):\n    if not isinstance(data, six.binary_type):\n        data = data.encode('utf_8')\n    hashof = hashlib.sha1(data).digest()\n    encoded_hash = base64.b64encode(hashof)\n    if not isinstance(encoded_hash, six.string_types):\n        encoded_hash = encoded_hash.decode('utf_8')\n    chunked = splitter(encoded_hash, chunksize=60)\n    lines = '\\n'.join(chunked)\n    return lines", "response": "Create SHA - 1 hash get digest b64 encode split every 60 char."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normpath(path):\n    return os.path.abspath(os.path.normpath(os.path.expanduser(path)))", "response": "Normalizes a path. Expands ~ s resolves relative paths normalizes and returns\n    an absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef splitter(iterable, chunksize=60):\n    return (iterable[0+i:chunksize+i]\n            for i in range(0, len(iterable), chunksize))", "response": "Split an iterable that supports indexing into chunks of chunksize."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the canonical request string.", "response": "def canonical_request(self, method, path, content, timestamp):\n        \"\"\"Return the canonical request string.\"\"\"\n        request = collections.OrderedDict([\n            ('Method', method.upper()),\n            ('Hashed Path', path),\n            ('X-Ops-Content-Hash', content),\n            ('X-Ops-Timestamp', timestamp),\n            ('X-Ops-UserId', self.user_id),\n        ])\n        return '\\n'.join(['%s:%s' % (key, value)\n                          for key, value in request.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_pem(cls, private_key, password=None):\n        # TODO(sam): try to break this in tests\n        maybe_path = normpath(private_key)\n        if os.path.isfile(maybe_path):\n            with open(maybe_path, 'rb') as pkf:\n                private_key = pkf.read()\n        if not isinstance(private_key, six.binary_type):\n            private_key = private_key.encode('utf-8')\n\n        pkey = serialization.load_pem_private_key(\n            private_key,\n            password=password,\n            backend=crypto_backends.default_backend())\n        return cls(pkey)", "response": "Load a PrivateKey instance from a PEM - formatted private key file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsigns data with the private key and return the signed data.", "response": "def sign(self, data, b64=True):\n        \"\"\"Sign data with the private key and return the signed data.\n\n        The signed data will be Base64 encoded if b64 is True.\n        \"\"\"\n        padder = padding.PKCS1v15()\n        signer = self.private_key.signer(padder, None)\n        if not isinstance(data, six.binary_type):\n            data = data.encode('utf_8')\n        signer.update(data)\n        signed = signer.finalize()\n        if b64:\n            signed = base64.b64encode(signed)\n        return signed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checkseq(sequence: str=None,  code=\"ATGC\") -> bool:\n    for base in sequence:\n        if base not in code:\n            return False\n    return True", "response": "Checks if the input sequence contains the given code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deserialize_date(string):\n    try:\n        from dateutil.parser import parse\n        return parse(string).date()\n    except ImportError:\n        return string", "response": "Deserializes string to date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deserialize_model(data, klass):\n    instance = klass()\n\n    if not instance.swagger_types:\n        return data\n\n    for attr, attr_type in iteritems(instance.swagger_types):\n        if data is not None \\\n                and instance.attribute_map[attr] in data \\\n                and isinstance(data, (list, dict)):\n            value = data[instance.attribute_map[attr]]\n            setattr(instance, attr, _deserialize(value, attr_type))\n\n    return instance", "response": "Deserializes list or dict to model object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dump(obj, fp, **kw):\n    xml = dumps(obj, **kw)\n    if isinstance(fp, basestring):\n        with open(fp, 'w') as fobj:\n            fobj.write(xml)\n    else:\n        fp.write(xml)", "response": "r Dump a Python object to xml."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the power spectral density of the time trace x at intervals time_step", "response": "def power_spectral_density(x, time_step, freq_range = None):\n    \"\"\"\n    returns the *single sided* power spectral density of the time trace x which is sampled at intervals time_step\n    Args:\n        x (array):  timetrace\n        time_step (float): sampling interval of x\n        freq_range (array or tuple): frequency range in the form [f_min, f_max] to return only the spectrum within this range\n\n    Returns:\n\n    \"\"\"\n    N = len(x)\n    P = 2 * np.abs(np.fft.rfft(x))**2 / N * time_step\n    F = np.fft.rfftfreq(len(x), time_step)\n\n    if freq_range is not None:\n        brange = np.all([F >= freq_range[0], F <= freq_range[1]], axis=0)\n        P = P[brange]\n        F = F[brange]\n\n    return F, P"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef value(self):\n\n        value = getattr(self.instrument, self.probe_name)\n        self.buffer.append(value)\n\n        return value", "response": "reads the value from the instrument and adds it to the buffer"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the geolocation as a lat / lng pair", "response": "def location(self):\n        \"\"\"\n        Returns the geolocation as a lat/lng pair\n        \"\"\"\n        try:\n            lat, lng = self[\"metadata\"][\"latitude\"], self[\"metadata\"][\"longitude\"]\n        except KeyError:\n            return None\n\n        if not lat or not lng:\n            return None\n\n        return lat, lng"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an address by user controlled input ID", "response": "def get(self, key):\n        \"\"\"\n        Returns an address by user controlled input ID\n\n        :param key: an input_id used to tag a lookup address\n        :return: a matching Address\n        \"\"\"\n        try:\n            return self[self.id_lookup.get(key)]\n\n        except TypeError:\n            raise KeyError"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an address by input index", "response": "def get_index(self, key):\n        \"\"\"\n        Returns an address by input index, a value that matches the list index of the provided\n        lookup value, not necessarily the result.\n\n        :param key: an input_index matching the index of the provided address\n        :return: a matching Address\n        \"\"\"\n        try:\n            return self[self.index_lookup.get(key)]\n\n        except TypeError:\n            raise KeyError"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_default_instruments(target_folder, source_folder = None, raise_errors = False, verbose=True):\n    print('export_def_instr called')\n    instruments_to_load = get_classes_in_folder(source_folder, Instrument, verbose = True)\n    print('instruments to load:')\n    print(instruments_to_load)\n\n    if verbose:\n        print(('attempt to load {:d} instruments: '.format(len(instruments_to_load))))\n    loaded_instruments, failed = Instrument.load_and_append(instruments_to_load, raise_errors = raise_errors)\n    print('loaded instruments:')\n    print(loaded_instruments, failed)\n\n    for name, value in loaded_instruments.items():\n        filename = os.path.join(target_folder, '{:s}.b26'.format(name))\n\n        value.save_b26(filename)\n\n    if verbose:\n        print('\\n================================================')\n        print('================================================')\n        print(('saved {:d} instruments, {:d} failed'.format(len(loaded_instruments), len(failed))))\n        if failed != {}:\n            for error_name, error in failed.items():\n                print(('failed to create instruments: ', error_name, error))", "response": "Exports all the instruments that are imported in source_folder into target_folder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new NetworkRule object.", "response": "def create(action=None, etherType=None, ruleNo=None):\n        \"\"\"\n        :type action: str\n        :type etherType: int\n        :type ruleNo: int\n        :rtype: NetworkRule\n        \"\"\"\n\n        return NetworkRule(\n            action=action,\n            etherType=etherType,\n            ruleNo=ruleNo,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the Chisq probability value for a given number of degrees of freedom x.", "response": "def chisqprob(x, df):\n    \"\"\"\n    Probability value (1-tail) for the Chi^2 probability distribution.\n\n    Broadcasting rules apply.\n\n    Parameters\n    ----------\n    x : array_like or float > 0\n\n    df : array_like or float, probably int >= 1\n\n    Returns\n    -------\n    chisqprob : ndarray\n        The area from `chisq` to infinity under the Chi^2 probability\n        distribution with degrees of freedom `df`.\n\n    \"\"\"\n    if x <= 0:\n        return 1.0\n    if x == 0:\n        return 0.0\n    if df <= 0:\n        raise ValueError(\"Domain error.\")\n    if x < 1.0 or x < df:\n        return 1.0 - _igam(0.5*df, 0.5*x)\n    return _igamc(0.5*df, 0.5*x)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _igamc(a, x):\n    # Compute  x**a * exp(-x) / Gamma(a)\n    ax = math.exp(a * math.log(x) - x - math.lgamma(a))\n\n    # Continued fraction\n    y = 1.0 - a\n    z = x + y + 1.0\n    c = 0.0\n    pkm2 = 1.0\n    qkm2 = x\n    pkm1 = x + 1.0\n    qkm1 = z * x\n    ans = pkm1 / qkm1\n    while True:\n        c += 1.0\n        y += 1.0\n        z += 2.0\n        yc = y * c\n        pk = pkm1 * z - pkm2 * yc\n        qk = qkm1 * z - qkm2 * yc\n        if qk != 0:\n            r = pk/qk\n            t = abs((ans - r) / r)\n            ans = r\n        else:\n            t = 1.0;\n        pkm2 = pkm1\n        pkm1 = pk\n        qkm2 = qkm1\n        qkm1 = qk\n        if abs(pk) > BIG:\n                pkm2 *= BIGINV;\n                pkm1 *= BIGINV;\n                qkm2 *= BIGINV;\n                qkm1 *= BIGINV;\n        if t <= MACHEP:\n            return ans * ax", "response": "Complemented incomplete Gamma integral."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nleave tail of incomplete Gamma function", "response": "def _igam(a, x):\n    \"\"\"Left tail of incomplete Gamma function:\n\n            inf.      k\n     a  -x   -       x\n    x  e     >   ----------\n             -     -\n            k=0   | (a+k+1)\n    \"\"\"\n    # Compute  x**a * exp(-x) / Gamma(a)\n    ax = math.exp(a * math.log(x) - x - math.lgamma(a))\n\n    # Power series\n    r = a\n    c = 1.0\n    ans = 1.0\n\n    while True:\n        r += 1.0\n        c *= x/r\n        ans += c\n        if c / ans <= MACHEP:\n            return ans * ax / a"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    dem = '../tests/data/Jamaica_dem.tif'\n    num_proc = 2\n    wp = '../tests/data/tmp_results/wtsd_delineation'\n\n    TauDEMWorkflow.watershed_delineation(num_proc, dem, workingdir=wp)", "response": "The simplest usage of watershed delineation based on TauDEM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new SystemStatus object.", "response": "def create(clock=None, loginMethods=None, online=None, paidPlans=None, smsEnabled=None, stripePublishableKey=None, uptime=None, user=None, version=None):\n        \"\"\"\n        :type clock: int\n        :type loginMethods: dict\n        :type online: bool\n        :type paidPlans: dict\n        :type smsEnabled: bool\n        :type stripePublishableKey: str\n        :type uptime: int\n        :type user: User\n        :type version: str\n        :rtype: SystemStatus\n        \"\"\"\n\n        return SystemStatus(\n            clock=clock,\n            loginMethods=loginMethods,\n            online=online,\n            paidPlans=paidPlans,\n            smsEnabled=smsEnabled,\n            stripePublishableKey=stripePublishableKey,\n            uptime=uptime,\n            user=user,\n            version=version,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind all the instruments or scripts that are located in folder_name and return a dictionary with the class name and path_to_python_file.", "response": "def find_exportable_in_python_files(folder_name, class_type, verbose = True):\n    \"\"\"\n    load all the instruments or script objects that are located in folder_name and\n    return a dictionary with the script class name and path_to_python_file\n    Args:\n        folder_name (string): folder in which to search for class objects / or name of module\n        class_type (string or class): class type for which to look for\n\n    Returns:\n        a dictionary with the class name and path_to_python_file:\n        {\n        'class': class_of_instruments,\n        'filepath': path_to_python_file\n        }\n    \"\"\"\n\n    # if the module name was passed instead of a filename, figure out the path to the module\n    if not os.path.isdir(folder_name):\n        try:\n            folder_name = os.path.dirname(inspect.getfile(import_module(folder_name)))\n        except ImportError:\n            raise ImportError('could not find module ' + folder_name)\n\n    subdirs = [os.path.join(folder_name, x) for x in os.listdir(folder_name) if\n               os.path.isdir(os.path.join(folder_name, x)) and not x.startswith('.')]\n\n    classes_dict = {}\n    # if there are subdirs in the folder recursively check all the subfolders for scripts\n    for subdir in subdirs:\n        classes_dict.update(find_exportable_in_python_files(subdir, class_type))\n\n    if class_type.lower() == 'instrument':\n        class_type = Instrument\n    elif class_type.lower() == 'script':\n        class_type = Script\n\n    for python_file in [f for f in glob.glob(os.path.join(folder_name, \"*.py\"))if '__init__' not in f and 'setup' not in f]:\n        module, path = module_name_from_path(python_file)\n\n        #appends path to this module to the python path if it is not present so it can be used\n        if path not in sys.path:\n            sys.path.append(path)\n\n        try:\n            module = import_module(module)\n\n            classes_dict.update({name: {'class': name, 'filepath': inspect.getfile(obj), 'info': inspect.getdoc(obj)} for name, obj in\n                               inspect.getmembers(module) if inspect.isclass(obj) and issubclass(obj, class_type)\n                             and not obj in (Instrument, Script, ScriptIterator)})\n\n        except (ImportError, ModuleNotFoundError) as e:\n            print(e)\n            if verbose:\n                print('Could not import module', module)\n\n    return classes_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default_qai(qareport):\n    TP = 1. if qareport.is_test_sucess else 0.\n    PCN = qareport.processors_number + qareport.commands_number\n    PT_div_PCN = float(qareport.pc_tested_number) / PCN\n    COV = qareport.coverage_line_rate\n    tau = get_tau()\n\n    total_tau = float(tau) * len(qareport.project_modules)\n    style = 1 + math.exp(qareport.style_errors / total_tau)\n\n    result = (2 * TP * PT_div_PCN * COV) / style\n    return result", "response": "Default QAI for the given Qareport."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_line(self, search_string, search_file, return_string=True, case_sens=True):\n        '''Return the first line containing a set of strings in a file.\n\n        If return_string is False, we just return whether such a line\n        was found. If case_sens is False, the search is case\n        insensitive.\n\n        '''\n        if os.path.isfile(search_file):\n            # if single search string\n            if type(search_string) == type(''): search_string = [search_string]\n            # if case insensitive, convert everything to lowercase\n            if not case_sens: search_string = [i.lower() for i in search_string]\n            with open(search_file) as fp:\n                # search for the strings line by line\n                for line in fp:\n                    query_line = line if case_sens else line.lower()\n                    if all([i in query_line for i in search_string]):\n                        return line if return_string else True\n                if return_string:\n                    raise Exception('%s not found in %s'%(' & '.join(search_string), search_file))\n                else: return False\n        else: raise Exception('%s file does not exist'%search_file)", "response": "Return the first line containing a set of strings in a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cutoff_energy(self):\n        '''Determine the cutoff energy from the output'''\n        return Value(\n            scalars=[Scalar(value=self.settings[\"kinetic-energy cutoff\"])],\n            units=self.settings['kinetic-energy cutoff units']\n        )", "response": "Determine the cutoff energy from the output"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if calculation converged ; for a relaxation run we look for ionic run we look for convergence in the output", "response": "def _is_converged(self):\n        '''Determine if calculation converged; for a relaxation (static) run\n        we look for ionic (electronic) convergence in the output'''\n        if self.is_relaxed():\n            # relaxation run case\n            return self._get_line(['End of', 'Geometry Optimization'], self.outputf, return_string=False)\n        else:\n            # static run case\n            return self._get_line('convergence has been achieved', self.outputf, return_string=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_KPPRA(self):\n        '''Determine the no. of k-points in the BZ (from the input) times the\n        no. of atoms (from the output)'''\n        # Find the no. of k-points\n        fp = open(self.inputf).readlines()\n        for l,ll in enumerate(fp):\n            if \"K_POINTS\" in ll:\n                # determine the type of input\n                if len(ll.split()) > 1:\n                    if \"gamma\" in ll.split()[1].lower():\n                        ktype = 'gamma'\n                    elif \"automatic\" in ll.split()[1].lower():\n                        ktype = 'automatic'\n                    else:\n                        ktype = ''\n                else: ktype = ''\n                if ktype == 'gamma':\n                    # gamma point:\n                    # K_POINTS {gamma}\n                    nk = 1\n                elif ktype == 'automatic':\n                    # automatic:\n                    # K_POINTS automatic\n                    #  12 12 1 0 0 0\n                    line = [int(i) for i in fp[l+1].split()[0:3]]\n                    nk = line[0]*line[1]*line[2]\n                else:\n                    # manual:\n                    # K_POINTS\n                    #  3\n                    #  0.125  0.125  0.0  1.0\n                    #  0.125  0.375  0.0  2.0\n                    #  0.375  0.375  0.0  1.0\n                    nk = 0\n                    for k in range(int(fp[l+1].split()[0])):\n                        nk += int(float(fp[l+2+k].split()[3]))\n                # Find the no. of atoms\n                natoms = int(self._get_line('number of atoms/cell', self.outputf).split()[4])\n                return Value(scalars=[Scalar(value=nk*natoms)])\n        fp.close()\n        raise Exception('%s not found in %s'%('KPOINTS',self.inputf))", "response": "Determine the no. of k - points in the BZ ( from the input ) times the\n        no. of atoms ( from the output )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_pp_name(self):\n        '''Determine the pseudopotential names from the output'''\n        ppnames = []\n        # Find the number of atom types\n        natomtypes = int(self._get_line('number of atomic types', self.outputf).split()[5])\n        # Find the pseudopotential names\n        with open(self.outputf) as fp:\n            for line in fp:\n                if \"PseudoPot. #\" in line:\n                    ppnames.append(Scalar(value=next(fp).split('/')[-1].rstrip()))\n                    if len(ppnames) == natomtypes:\n                        return Value(scalars=ppnames)\n            raise Exception('Could not find %i pseudopotential names'%natomtypes)", "response": "Determine the pseudopotential names from the output file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_U_settings(self):\n        '''Determine the DFT+U type and parameters from the output'''\n        with open(self.outputf) as fp:\n            for line in fp:\n                if \"LDA+U calculation\" in line:\n                    U_param = {}\n                    U_param['Type'] = line.split()[0]\n                    U_param['Values'] = {}\n                    # look through next several lines\n                    for nl in range(15):\n                        line2 = next(fp).split()\n                        if len(line2) > 1 and line2[0] == \"atomic\":\n                            pass # column titles\n                        elif len(line2) == 6:\n                            U_param['Values'][line2[0]] = {}\n                            U_param['Values'][line2[0]]['L'] = float(line2[1])\n                            U_param['Values'][line2[0]]['U'] = float(line2[2])\n                            U_param['Values'][line2[0]]['J'] = float(line2[4])\n                        else: break # end of data block\n                    return Value(**U_param)\n            return None", "response": "Determine the DFT + U type and parameters from the output file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the vdW type if using vdW xc functional or correction scheme from the input otherwise return None", "response": "def get_vdW_settings(self):\n        '''Determine the vdW type if using vdW xc functional or correction\n        scheme from the input otherwise'''\n        xc = self.get_xc_functional().scalars[0].value\n        if 'vdw' in xc.lower(): # vdW xc functional\n            return Value(scalars=[Scalar(value=xc)])\n        else:\n            # look for vdw_corr in input\n            vdW_dict = {'xdm':'Becke-Johnson XDM', 'ts':\n                        'Tkatchenko-Scheffler', 'ts-vdw':\n                        'Tkatchenko-Scheffler',\n                        'tkatchenko-scheffler':\n                        'Tkatchenko-Scheffler', 'grimme-d2': 'Grimme D2', 'dft-d': 'Grimme D2'}\n            if self._get_line('vdw_corr', self.inputf, return_string=False, case_sens=False):\n                line = self._get_line('vdw_corr', self.inputf, return_string=True, case_sens=False)\n                vdwkey = str(line.split('=')[-1].replace(\"'\", \"\").replace(',', '').lower().rstrip())\n                return Value(scalars=[Scalar(value=vdW_dict[vdwkey])])\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stresses(self):\n        '''Determine the stress tensor from the output'''\n        if \"stress\" not in self.settings:\n            return None\n        wrapped = [[Scalar(value=x) for x in y] for y in self.settings[\"stress\"]]\n        return Property(matrices=[wrapped], units=self.settings[\"stress units\"])", "response": "Determine the stress tensor from the output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the structure from the output file", "response": "def get_output_structure(self):\n        '''Determine the structure from the output'''\n        bohr_to_angstrom = 0.529177249\n\n        # determine the number of atoms\n        natoms = int(float(self._get_line('number of atoms/cell', self.outputf).split('=')[-1]))\n\n        # determine the initial lattice parameter\n        alat = float(self._get_line('lattice parameter (alat)', self.outputf).split('=')[-1].split()[0])\n\n        # find the initial unit cell\n        unit_cell = []\n        with open(self.outputf, 'r') as fp:\n            for line in fp:\n                if \"crystal axes:\" in line:\n                    for i in range(3):\n                        unit_cell.append([float(j)*alat*bohr_to_angstrom for j in next(fp).split('(')[-1].split(')')[0].split()])\n                    break\n            if len(unit_cell) == 0: raise Exception('Cannot find the initial unit cell')\n\n        # find the initial atomic coordinates\n        coords = [] ; atom_symbols = []\n        with open(self.outputf, 'r') as fp:\n            for line in fp:\n                if \"site n.\" in line and \"atom\" in line and \"positions\" in line and \"alat units\" in line:\n                    for i in range(natoms):\n                        coordline = next(fp)\n                        atom_symbols.append(''.join([i for i in coordline.split()[1] if not i.isdigit()]))\n                        coord_conv_factor = alat*bohr_to_angstrom\n                        coords.append([float(j)*coord_conv_factor for j in coordline.rstrip().split('=')[-1].split('(')[-1].split(')')[0].split()])\n                    break\n            if len(coords) == 0: raise Exception('Cannot find the initial atomic coordinates')\n\n        if type(self.is_relaxed()) == type(None):\n            # static run: create, populate, and return the initial structure\n            structure = Atoms(symbols=atom_symbols, cell=unit_cell, pbc=True)\n            structure.set_positions(coords)\n            return structure\n        else:\n            # relaxation run: update with the final structure\n            with open(self.outputf) as fp:\n                for line in fp:\n                    if \"Begin final coordinates\" in line:\n                        if 'new unit-cell volume' in next(fp):\n                            # unit cell allowed to change\n                            next(fp) # blank line\n                            # get the final unit cell\n                            unit_cell = []\n                            cellheader = next(fp)\n                            if 'bohr' in cellheader.lower():\n                                cell_conv_factor = bohr_to_angstrom\n                            elif 'angstrom' in cellheader.lower():\n                                cell_conv_factor = 1.0\n                            else:\n                                alat = float(cellheader.split('alat=')[-1].replace(')', ''))\n                                cell_conv_factor = alat*bohr_to_angstrom\n                            for i in range(3):\n                                unit_cell.append([float(j)*cell_conv_factor for j in next(fp).split()])\n                            next(fp) # blank line\n\n                        # get the final atomic coordinates\n                        coordtype = next(fp).split()[-1].replace('(', '').replace(')', '')\n                        if coordtype == 'bohr':\n                            coord_conv_factor = bohr_to_angstrom\n                        elif coordtype == 'angstrom' or coordtype == 'crystal':\n                            coord_conv_factor = 1.0\n                        else:\n                            coord_conv_factor = alat*bohr_to_angstrom\n                        coords = [] # reinitialize the coords\n                        for i in range(natoms):\n                            coordline = next(fp).split()\n                            coords.append([float(j)*coord_conv_factor for j in coordline[1:4]])\n\n                        # create, populate, and return the final structure\n                        structure = Atoms(symbols=atom_symbols, cell=unit_cell, pbc=True)\n                        if coordtype == 'crystal':\n                            structure.set_scaled_positions(coords) # direct coord\n                        else:\n                            structure.set_positions(coords) # cartesian coord\n                        return structure\n                raise Exception('Cannot find the final coordinates')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the total DOS shifted by the Fermi energy", "response": "def get_dos(self):\n        '''Find the total DOS shifted by the Fermi energy'''\n        # find the dos file\n        fildos = ''\n        for f in self._files:\n            with open(f, 'r') as fp:\n                first_line = next(fp)\n                if \"E (eV)\" in first_line and \"Int dos(E)\" in first_line:\n                    fildos = f\n                    ndoscol = len(next(fp).split())-2 # number of spin channels\n                    fp.close()\n                    break\n                fp.close()\n        if not fildos:\n            return None # cannot find DOS\n\n        # get the Fermi energy\n        line = self._get_line('the Fermi energy is', self.outputf)\n        efermi = float(line.split('is')[-1].split()[0])\n\n        # grab the DOS\n        energy = [] ; dos = []\n        fp = open(fildos, 'r')\n        next(fp) # comment line\n        for line in fp:\n            ls = line.split()\n            energy.append(Scalar(value=float(ls[0])-efermi))\n            dos.append(Scalar(value=sum([float(i) for i in ls[1:1+ndoscol]])))\n        return Property(scalars=dos, units='number of states per unit cell', conditions=Value(name='energy', scalars=energy, units='eV'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the band gap from the DOS", "response": "def get_band_gap(self):\n        '''Compute the band gap from the DOS'''\n        dosdata = self.get_dos()\n        if type(dosdata) == type(None):\n            return None # cannot find DOS\n        else:\n            energy = dosdata.conditions.scalars\n            dos = dosdata.scalars\n            step_size = energy[1].value - energy[0].value\n            not_found = True ; l = 0 ; bot = 10**3 ; top = -10**3\n            while not_found and l < len(dos):\n                # iterate through the data\n                e = float(energy[l].value)\n                dens = float(dos[l].value)\n                # note: dos already shifted by efermi\n                if e < 0 and dens > 1e-3:\n                    bot = e\n                elif e > 0 and dens > 1e-3:\n                    top = e\n                    not_found = False\n                l += 1\n            if top < bot:\n                raise Exception('Algorithm failed to find the band gap')\n            elif top - bot < step_size*2:\n                return Property(scalars=[Scalar(value=0)], units='eV')\n            else:\n                bandgap = float(top-bot)\n                return Property(scalars=[Scalar(value=round(bandgap,3))], units='eV')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_category_aliases_under(parent_alias=None):\n    return [ch.alias for ch in get_cache().get_children_for(parent_alias, only_with_aliases=True)]", "response": "Returns a list of category aliases under the given parent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of CategoryList objects optionally associated with a given model instance.", "response": "def get_category_lists(init_kwargs=None, additional_parents_aliases=None, obj=None):\n    \"\"\"Returns a list of CategoryList objects, optionally associated with\n    a given model instance.\n\n    :param dict|None init_kwargs:\n    :param list|None additional_parents_aliases:\n    :param Model|None obj: Model instance to get categories for\n    :rtype: list\n    :return:\n    \"\"\"\n    init_kwargs = init_kwargs or {}\n    additional_parents_aliases = additional_parents_aliases or []\n\n    parent_aliases = additional_parents_aliases\n    if obj is not None:\n        ctype = ContentType.objects.get_for_model(obj)\n        cat_ids = [\n            item[0] for item in\n            get_tie_model().objects.filter(content_type=ctype, object_id=obj.id).values_list('category_id').all()\n        ]\n        parent_aliases = list(get_cache().get_parents_for(cat_ids).union(additional_parents_aliases))\n    lists = []\n\n    aliases = get_cache().sort_aliases(parent_aliases)\n    categories_cache = get_cache().get_categories(aliases, obj)\n\n    for parent_alias in aliases:\n        catlist = CategoryList(parent_alias, **init_kwargs)  # TODO Burned in class name. Make more customizable.\n        if obj is not None:\n            catlist.set_obj(obj)\n        # Optimization. To get DB hits down.\n        cache = []\n        try:\n            cache = categories_cache[parent_alias]\n        except KeyError:\n            pass\n        catlist.set_get_categories_cache(cache)\n        lists.append(catlist)\n\n    return lists"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_lists(self, category_lists, lists_init_kwargs=None, editor_init_kwargs=None):\n        lists_init_kwargs = lists_init_kwargs or {}\n        editor_init_kwargs = editor_init_kwargs or {}\n\n        for lst in category_lists:\n            if isinstance(lst, string_types):  # Spawn CategoryList object from base category alias.\n                lst = self.list_cls(lst, **lists_init_kwargs)\n            elif not isinstance(lst, CategoryList):\n                raise SitecatsConfigurationError(\n                    '`CategoryRequestHandler.register_lists()` accepts only '\n                    '`CategoryList` objects or category aliases.'\n                )\n\n            if self._obj:\n                lst.set_obj(self._obj)\n\n            for name, val in lists_init_kwargs.items():  # Setting CategoryList attributes from kwargs.\n                setattr(lst, name, val)\n\n            lst.enable_editor(**editor_init_kwargs)\n\n            self._lists[lst.get_id()] = lst", "response": "Registers CategoryList objects to handle their requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef action_remove(cls, request, category_list):\n        if not category_list.editor.allow_remove:\n            raise SitecatsSecurityException(\n                '`action_remove()` is not supported by parent `%s`category.' % category_list.alias)\n\n        category_id = int(request.POST.get('category_id', 0))\n        if not category_id:\n            raise SitecatsSecurityException(\n                'Unsupported `category_id` value - `%s` - is passed to `action_remove()`.' % category_id)\n\n        category = get_cache().get_category_by_id(category_id)\n        if not category:\n            raise SitecatsSecurityException('Unable to get `%s` category in `action_remove()`.' % category_id)\n\n        cat_ident = category.alias or category.id\n\n        if category.is_locked:\n            raise SitecatsSecurityException('`action_remove()` is not supported by `%s` category.' % cat_ident)\n\n        if category.parent_id != category_list.get_id():\n            raise SitecatsSecurityException(\n                '`action_remove()` is unable to remove `%s`: '\n                'not a child of parent `%s` category.' % (cat_ident, category_list.alias)\n            )\n\n        min_num = category_list.editor.min_num\n\n        def check_min_num(num):\n            if min_num is not None and num-1 < min_num:\n                subcats_str = ungettext_lazy('subcategory', 'subcategories', min_num)\n                error_msg = _(\n                    'Unable to remove \"%(target_category)s\" category from \"%(parent_category)s\": '\n                    'parent category requires at least %(num)s %(subcats_str)s.'\n                ) % {\n                    'target_category': category.title,\n                    'parent_category': category_list.get_title(),\n                    'num': min_num,\n                    'subcats_str': subcats_str\n                }\n                raise SitecatsValidationError(error_msg)\n\n        child_ids = get_cache().get_child_ids(category_list.alias)\n        check_min_num(len(child_ids))\n        if category_list.obj is None:  # Remove category itself and children.\n            category.delete()\n        else:  # Remove just a category-to-object tie.\n            # TODO filter user/status\n            check_min_num(category_list.obj.get_ties_for_categories_qs(child_ids).count())\n            category_list.obj.remove_from_category(category)\n\n        return True", "response": "Handles remove action from CategoryList editor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle add action from CategoryList editor.", "response": "def action_add(cls, request, category_list):\n        \"\"\"Handles `add` action from CategoryList editor.\n\n        Adds an actual category if a target object is not set for the list.\n        Adds a tie-to-category object if a target object is set for the list.\n\n        :param Request request: Django request object\n        :param CategoryList category_list: CategoryList object to operate upon.\n        :return: CategoryModel object on success otherwise and exception from SitecatsException family is raised.\n        \"\"\"\n        if not category_list.editor.allow_add:\n            raise SitecatsSecurityException('`action_add()` is not supported by `%s` category.' % category_list.alias)\n\n        titles = request.POST.get('category_title', '').strip()\n        if not titles:\n            raise SitecatsSecurityException(\n                'Unsupported `category_title` value - `%s` - is passed to `action_add()`.' % titles)\n\n        if category_list.editor.category_separator is None:\n            titles = [titles]\n        else:\n            titles = [\n                title.strip() for title in titles.split(category_list.editor.category_separator) if title.strip()\n            ]\n\n        def check_max_num(num, max_num, category_title):\n            if max_num is not None and num+1 > max_num:\n                subcats_str = ungettext_lazy('subcategory', 'subcategories', max_num)\n                error_msg = _(\n                    'Unable to add \"%(target_category)s\" category into \"%(parent_category)s\": '\n                    'parent category can have at most %(num)s %(subcats_str)s.'\n                ) % {\n                    'target_category': category_title,\n                    'parent_category': category_list.get_title(),\n                    'num': max_num,\n                    'subcats_str': subcats_str\n                }\n                raise SitecatsValidationError(error_msg)\n\n        target_category = None\n        for category_title in titles:\n            exists = get_cache().find_category(category_list.alias, category_title)\n\n            if exists and category_list.obj is None:  # Already exists.\n                return exists\n\n            if not exists and not category_list.editor.allow_new:\n                error_msg = _(\n                    'Unable to create a new \"%(new_category)s\" category inside of \"%(parent_category)s\": '\n                    'parent category does not support this action.'\n                ) % {\n                    'new_category': category_title,\n                    'parent_category': category_list.get_title()\n                }\n                raise SitecatsNewCategoryException(error_msg)\n\n            max_num = category_list.editor.max_num\n            child_ids = get_cache().get_child_ids(category_list.alias)\n            if not exists:  # Add new category.\n                if category_list.obj is None:\n                    check_max_num(len(child_ids), max_num, category_title)\n                # TODO status\n                target_category = get_category_model().add(\n                    category_title, request.user, parent=category_list.get_category_model()\n                )\n            else:\n                target_category = exists  # Use existing one for a tie.\n\n            if category_list.obj is not None:\n                # TODO status\n                check_max_num(category_list.obj.get_ties_for_categories_qs(child_ids).count(), max_num, category_title)\n                category_list.obj.add_to_category(target_category, request.user)\n\n        return target_category"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstruct handler to listen to Django request and handle CategoryList editor requests.", "response": "def listen(self):\n        \"\"\"Instructs handler to listen to Django request and handle\n        CategoryList editor requests (if any).\n\n        :return: None on success otherwise and exception from SitecatsException family is raised.\n        \"\"\"\n        requested_action = self._request.POST.get('category_action', False)\n\n        if not requested_action:\n            return None  # No action supplied. Pass.\n\n        if requested_action not in self.KNOWN_ACTIONS:\n            raise SitecatsSecurityException('Unknown `category_action` - `%s` - requested.')\n\n        category_base_id = self._request.POST.get('category_base_id', False)\n        if category_base_id == 'None':\n            category_base_id = None\n        else:\n            category_base_id = int(category_base_id)\n        if category_base_id not in self._lists.keys():\n            raise SitecatsSecurityException('Unknown `category_base_id` - `%s` - requested.')\n\n        category_list = self._lists[category_base_id]\n        if category_list.editor is None:\n            raise SitecatsSecurityException('Editor is disabled for `%s` category.' % category_list.alias)\n\n        action_method = getattr(self, 'action_%s' % requested_action)\n\n        try:\n            return action_method(self._request, category_list)\n        except SitecatsNewCategoryException as e:\n            messages.error(self._request, e, extra_tags=self.error_messages_extra_tags, fail_silently=True)\n            return None\n        except SitecatsValidationError as e:\n            messages.error(self._request, e.messages[0], extra_tags=self.error_messages_extra_tags, fail_silently=True)\n            return None\n        finally:\n            self._request.POST = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef use(self, algorithm):\n        try:\n            self.hash_algo = self._mapper[algorithm.strip().lower()]\n        except IndexError:  # pragma: no cover\n            template = \"'%s' is not supported, try one of %s.\"\n            raise ValueError(template % (algorithm, list(self._mapper)))", "response": "Change the hash algorithm you gonna use."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef of_bytes(self, py_bytes):\n        m = self.hash_algo()\n        m.update(py_bytes)\n        return self.digest(m)", "response": "Return the hash value of the given bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef of_text(self, text, encoding=\"utf-8\"):\n        m = self.hash_algo()\n        m.update(text.encode(encoding))\n        return self.digest(m)", "response": "Return the hash value of a piece of text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef of_pyobj(self, pyobj):\n        m = self.hash_algo()\n        m.update(pickle.dumps(pyobj, protocol=self.pk_protocol))\n        return self.digest(m)", "response": "Returns the hash value of a piece of Python\n        picklable object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef of_file(self, abspath, nbytes=0, chunk_size=1024):\n        if nbytes < 0:\n            raise ValueError(\"chunk_size cannot smaller than 0\")\n        if chunk_size < 1:\n            raise ValueError(\"chunk_size cannot smaller than 1\")\n        if (nbytes > 0) and (nbytes < chunk_size):\n            chunk_size = nbytes\n\n        m = self.hash_algo()\n        with open(abspath, \"rb\") as f:\n            if nbytes:  # use first n bytes\n                have_reads = 0\n                while True:\n                    have_reads += chunk_size\n                    if have_reads > nbytes:\n                        n = nbytes - (have_reads - chunk_size)\n                        if n:\n                            data = f.read(n)\n                            m.update(data)\n                        break\n                    else:\n                        data = f.read(chunk_size)\n                        m.update(data)\n            else:  # use entire content\n                while True:\n                    data = f.read(chunk_size)\n                    if not data:\n                        break\n                    m.update(data)\n\n        return m.hexdigest()", "response": "Return the hash value of a piece of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_response(self, resp):\n        if resp.status_code == 400:\n            raise ApiException(json.loads(resp.content).get('message'))\n        return resp", "response": "raise an exception on a bad request response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shrink(image, apikey):\n\n    def _handle_response(response):\n        body = json.loads(response.read())\n        if response.code == TinyPNGResponse.SUCCESS_CODE:\n            body['location'] = response.headers.getheader(\"Location\")\n            try:\n                body['bytes'] = urlopen(body['location']).read()\n            except:\n                body['bytes'] = None\n        return response.code, body\n\n    auth = b64encode(bytes(\"api:\" + apikey)).decode(\"ascii\")\n\n    request = Request(TINYPNG_SHRINK_URL, image)\n    request.add_header(\"Authorization\", \"Basic %s\" % auth)\n\n    try:\n        response = urlopen(request)\n        (code, response_dict) = _handle_response(response)\n    except HTTPError as e:\n        (code, response_dict) = _handle_response(e)\n\n    return TinyPNGResponse(code, **response_dict)", "response": "This function is used to shrink a PNG image to a TinyPNG image file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload and install dependencies for Python 3. 6 +", "response": "def download_and_install_dependencies():\n    \"\"\"Setup URLS and download dependencies for Python 3.6+\n    \"\"\"\n\n    try:\n        import requests\n    except ImportError:\n        raise ValueError(\"Python 3.6+ is required.\")\n\n    dependencies = {\"hmm_databases\": HMM_URL}\n\n    if sys.platform.startswith(\"linux\") or \"bsd\" in sys.platform:\n\n        dependencies[\"prodigal\"] = \"{}.linux\".format(BASE_PRODIGAL)\n        dependencies[\"louvain\"] = (\n            \"https://lip6.github.io/Louvain-BinaryBuild/\"\n            \"louvain_linux.tar.gz\"\n        )\n    elif sys.platform == \"darwin\":\n\n        dependencies[\"prodigal\"] = \"{}.osx.10.9.5\".format(BASE_PRODIGAL)\n        dependencies[\"louvain\"] = (\n            \"https://github.com/lip6/Louvain-BinaryBuilds/raw/osx/\"\n            \"louvain_osx.tar.gz\"\n        )\n    elif sys.platform.startswith(\"win\") or sys.platform == \"cygwin\":\n\n        dependencies[\"prodigal\"] = \"{}.windows.exe\"\n        dependencies[\"louvain\"] = (\n            \"https://ci.appveyor.com/api/projects/yanntm/\"\n            \"Louvain-BinaryBuild/artifacts/website/\"\n            \"louvain_windows.tar.gz\"\n        )\n\n    else:\n        raise NotImplementedError(\n            \"Your platform is not supported: {}\".format(sys.platform)\n        )\n\n    cache_dir = pathlib.Path.cwd() / pathlib.Path(\"cache\")\n\n    try:\n        print(\"Downloading dependencies...\")\n        cache_dir.mkdir()\n        for dependency_name, url in dependencies.items():\n            print(\"Downloading {} at {}\".format(dependency_name, url))\n            request = requests.get(url)\n            basename = url.split(\"/\")[-1]\n\n            with open(cache_dir / basename, \"wb\") as handle:\n                print(dependency_name, basename, cache_dir / basename)\n                handle.write(request.content)\n    except FileExistsError:\n        print(\"Using cached dependencies...\")\n\n    share_dir = pathlib.Path.cwd()\n    tools_dir = share_dir / \"tools\"\n    louvain_dir = tools_dir / \"louvain\"\n\n    louvain_dir.mkdir(parents=True, exist_ok=True)\n\n    louvain_basename = dependencies[\"louvain\"].split(\"/\")[-1]\n    louvain_path = louvain_dir / louvain_basename\n    (cache_dir / louvain_basename).replace(louvain_path)\n\n    with tarfile.open(louvain_path, \"r:gz\") as tar:\n        tar.extractall()\n\n    hmm_basename = dependencies[\"hmm_databases\"].split(\"/\")[-1]\n    hmm_path = share_dir / hmm_basename\n    (cache_dir / hmm_basename).replace(hmm_path)\n\n    prodigal_basename = dependencies[\"prodigal\"].split(\"/\")[-1]\n    prodigal_path = tools_dir / \"prodigal\"\n    (cache_dir / prodigal_basename).replace(prodigal_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n\n    metator_args = sys.argv[1:]\n    entry_point = pkg_resources.resource_filename(\"metator\", \"bin/metator.sh\")\n\n    try:\n        metator_process = subprocess.Popen((entry_point, *metator_args))\n    except PermissionError:  # some issues occured for non-bash users\n        metator_process = subprocess.Popen(\n            (entry_point, *metator_args), shell=True\n        )\n    metator_process.wait()", "response": "This module is used to run the bulk of the pipeline. It is intended to be used by the user as a command line interface to the bulk of the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self):\n        ret_list = []\n        if hasattr(self, \"font\"):\n            ret_list.append(self.font)\n        if hasattr(self, \"size\"):\n            ret_list.append(self.size)\n        if hasattr(self, \"text\"):\n            ret_list.append(self.text)\n        return ret_list", "response": "method to fetch all contents as a list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_by_prefix_surfix(text,\n                             prefix,\n                             surfix,\n                             minlen=None,\n                             maxlen=None,\n                             include=False):\n    \"\"\"Extract the text in between a prefix and surfix. It use non-greedy match.\n\n    :param text: text body\n    :type text: str\n\n    :param prefix: the prefix\n    :type prefix: str\n\n    :param surfix: the surfix\n    :type surfix: str\n\n    :param minlen: the min matched string length\n    :type minlen: int\n\n    :param maxlen: the max matched string length\n    :type maxlen: int\n\n    :param include: whether if include prefix and surfix\n    :type include: bool\n    \"\"\"\n    if minlen is None:\n        minlen = 0\n    if maxlen is None:\n        maxlen = 2 ** 30\n    pattern = r\"\"\"(?<=%s)[\\s\\S]{%s,%s}?(?=%s)\"\"\" % (\n        prefix, minlen, maxlen, surfix)\n    if include:\n        return [prefix + s + surfix for s in re.findall(pattern, text)]\n    else:\n        return re.findall(pattern, text)", "response": "Extract the text in between a prefix and surfix. It use non - greedy match."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_number(text):\n    result = list()\n    chunk = list()\n    valid_char = set(\".1234567890\")\n    for char in text:\n        if char in valid_char:\n            chunk.append(char)\n        else:\n            result.append(\"\".join(chunk))\n            chunk = list()\n    result.append(\"\".join(chunk))\n\n    result_new = list()\n    for number in result:\n        if \".\" in number:\n            try:\n                result_new.append(float(number))\n            except:\n                pass\n        else:\n            try:\n                result_new.append(int(number))\n            except:\n                pass\n\n    return result_new", "response": "Extract digit character from text.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_email(text):\n    result = list()\n    for tp in re.findall(_regex_extract_email, text.lower()):\n        for email in tp:\n            if re.match(_regex_validate_email, email):\n                result.append(email)\n    return result", "response": "Extract email from text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd Signature Authorization header to case - insensitive header dict.", "response": "def sign(self, headers: Mapping, method=None, path=None):\n        \"\"\"\n        Add Signature Authorization header to case-insensitive header dict.\n\n        `headers` is a case-insensitive dict of mutable headers.\n        `host` is a override for the 'host' header (defaults to value in\n            headers).\n        `method` is the HTTP method (required when using '(request-target)').\n        `path` is the HTTP path (required when using '(request-target)').\n        \"\"\"\n        required_headers = self.header_list\n        message = generate_message(required_headers, headers, method, path)\n\n        signature = encode_string(self._signer.sign(message), 'base64')\n        ret_headers = multidict.CIMultiDict(headers)\n        ret_headers['Authorization'] = self._signature_tpl % signature.decode('ascii')\n\n        return ret_headers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def verify(self, headers: Mapping, method=None, path=None):\n\n        if not 'authorization' in headers:\n            return False\n\n        auth_type, auth_params = parse_authorization_header(headers['authorization'])\n        if auth_type.lower() != 'signature':\n            return False\n\n        for param in ('algorithm', 'keyId', 'signature'):\n            if param not in auth_params:\n                raise VerifierException(\"Unsupported HTTP signature, missing '{}'\".format(param))\n\n        auth_headers = (auth_params.get('headers') or 'date').lower().strip().split()\n\n        missing_reqd = set(self._required_headers) - set(auth_headers)\n        if missing_reqd:\n            error_headers = ', '.join(missing_reqd)\n            raise VerifierException(\n                'One or more required headers not provided: {}'.format(error_headers))\n\n        key_id, algo = auth_params['keyId'], auth_params['algorithm']\n\n        if not self._handlers.supports(algo):\n            raise VerifierException(\"Unsupported HTTP signature algorithm '{}'\".format(algo))\n\n        pubkey = await self._key_finder.find_key(key_id, algo)\n        if not pubkey:\n            raise VerifierException(\"Cannot locate public key for '{}'\".format(key_id))\n        LOGGER.debug(\"Got %s public key for '%s': %s\", algo, key_id, pubkey)\n\n        handler = self._handlers.create_verifier(algo, pubkey)\n        message = generate_message(auth_headers, headers, method, path)\n\n        signature = auth_params['signature']\n        raw_signature = decode_string(signature, 'base64')\n\n        if handler.verify(message, raw_signature):\n            return {\n                'verified': True,\n                'algorithm': algo,\n                'headers': auth_headers,\n                'keyId': key_id,\n                'key': pubkey,\n                'signature': signature\n            }\n        raise VerifierException(\"Signature could not be verified for keyId '{}'\".format(key_id))", "response": "Verify the signature of a request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmatching the argv for each usages return a dict.", "response": "def docpie(self, argv=None):\n        \"\"\"match the argv for each usages, return dict.\n\n        if argv is None, it will use sys.argv instead.\n        if argv is str, it will call argv.split() first.\n        this function will check the options in self.extra and handle it first.\n        Which means it may not try to match any usages because of the checking.\n        \"\"\"\n\n        token = self._prepare_token(argv)\n        # check first, raise after\n        # so `-hwhatever` can trigger `-h` first\n        self.check_flag_and_handler(token)\n\n        if token.error is not None:\n            # raise DocpieExit('%s\\n\\n%s' % (token.error, help_msg))\n            self.exception_handler(token.error)\n\n        try:\n            result, dashed = self._match(token)\n        except DocpieExit as e:\n            self.exception_handler(e)\n\n        # if error is not None:\n        #     self.exception_handler(error)\n\n        value = result.get_value(self.appeared_only, False)\n        self.clear()\n        self.update(value)\n        if self.appeared_only:\n            self._drop_non_appeared()\n\n        logger.debug('get all matched value %s', self)\n        rest = list(self.usages)  # a copy\n        rest.remove(result)\n        self._add_rest_value(rest)\n        logger.debug('merged rest values, now %s', self)\n        self._add_option_value()\n        self._dashes_value(dashed)\n\n        return dict(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clone_exception(error, args):\n        new_error = error.__class__(*args)\n        new_error.__dict__ = error.__dict__\n        return new_error", "response": "Clone an exception and return a new cloned error."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef help_handler(docpie, flag):\n        help_type = docpie.help\n        helpstyle = docpie.helpstyle\n        if helpstyle == 'python':\n            doc = Docpie.help_style_python(docpie.doc)\n        elif helpstyle == 'dedent':\n            doc = Docpie.help_style_dedent(docpie.doc)\n        # elif help_style == 'raw':\n        #     doc = Docpie.help_style_raw(docpie.doc)\n        else:\n            doc = docpie.doc\n\n        if help_type == 'short_brief':\n            if flag.startswith('--'):\n                print(doc)\n            else:\n                print(docpie.usage_text.rstrip())\n                option_sections = docpie.option_sections\n                if option_sections:\n                    print('')\n                    print('\\n'.join(option_sections.values()))\n        elif help_type == 'short_brief_notice':\n            if flag.startswith('--'):\n                sys.stdout.write(doc)\n            else:\n                print(docpie.usage_text)\n                option_sections = docpie.option_sections\n                if option_sections:\n                    print('')\n                    print('\\n'.join(option_sections.values()).rstrip())\n                print('')\n                print('Use `--help` to see the full help messsage.')\n        else:\n            sys.stdout.write(doc)\n        sys.exit()", "response": "Default help handler. Prints the help string and exit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dict(self):  # cls, self):\n        config = {\n            'stdopt': self.stdopt,\n            'attachopt': self.attachopt,\n            'attachvalue': self.attachvalue,\n            'auto2dashes': self.auto2dashes,\n            'case_sensitive': self.case_sensitive,\n            'namedoptions': self.namedoptions,\n            'appearedonly': self.appeared_only,\n            'optionsfirst': self.options_first,\n            'option_name': self.option_name,\n            'usage_name': self.usage_name,\n            'name': self.name,\n            'help': self.help,\n            'version': self.version\n        }\n\n        text = {\n            'doc': self.doc,\n            'usage_text': self.usage_text,\n            'option_sections': self.option_sections,\n        }\n\n        # option = [convert_2_dict(x) for x in self.options]\n\n        option = {}\n        for title, options in self.options.items():\n            option[title] = [convert_2_dict(x) for x in options]\n\n        usage = [convert_2_dict(x) for x in self.usages]\n\n        return {\n            '__version__': self._version,\n            '__class__': 'Docpie',\n            '__config__': config,\n            '__text__': text,\n            'option': option,\n            'usage': usage,\n            'option_names': [list(x) for x in self.opt_names],\n            'opt_names_required_max_args': self.opt_names_required_max_args\n        }", "response": "Convert the Docpie into a JSONlizable dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts dict generated by convert_2_dict into Docpie instance", "response": "def from_dict(cls, dic):\n        \"\"\"Convert dict generated by `convert_2_dict` into Docpie instance\n\n        You can do this:\n        pie = Docpie(__doc__)\n        clone_pie = json.loads(pie.convert_2_docpie(\n            json.dumps(pie.convert_2_dict())\n        ))\n\n        Note if you changed `extra`, it will be lost.\n        You can use `set_config(extra={...})` to set it back.\n        \"\"\"\n        if '__version__' not in dic:\n            raise ValueError('Not support old docpie data')\n\n        data_version = int(dic['__version__'].replace('.', ''))\n        this_version = int(cls._version.replace('.', ''))\n        logger.debug('this: %s, old: %s', this_version, data_version)\n        if data_version < this_version:\n            raise ValueError('Not support old docpie data')\n\n        assert dic['__class__'] == 'Docpie'\n\n        config = dic['__config__']\n        help = config.pop('help')\n        version = config.pop('version')\n        option_name = config.pop('option_name')\n        usage_name = config.pop('usage_name')\n        self = cls(None, **config)\n        self.option_name = option_name\n        self.usage_name = usage_name\n\n        text = dic['__text__']\n        self.doc = text['doc']\n        self.usage_text = text['usage_text']\n        self.option_sections = text['option_sections']\n\n        self.opt_names = [set(x) for x in dic['option_names']]\n        self.opt_names_required_max_args = dic['opt_names_required_max_args']\n        self.set_config(help=help, version=version)\n        self.options = o = {}\n        for title, options in dic['option'].items():\n            opt_ins = [convert_2_object(x, {}, self.namedoptions)\n                       for x in options]\n            o[title] = opt_ins\n\n        self.usages = [convert_2_object(x, self.options, self.namedoptions)\n                       for x in dic['usage']]\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshadows all the current config.", "response": "def set_config(self, **config):\n        \"\"\"Shadow all the current config.\"\"\"\n        reinit = False\n        if 'stdopt' in config:\n            stdopt = config.pop('stdopt')\n            reinit = (stdopt != self.stdopt)\n            self.stdopt = stdopt\n        if 'attachopt' in config:\n            attachopt = config.pop('attachopt')\n            reinit = reinit or (attachopt != self.attachopt)\n            self.attachopt = attachopt\n        if 'attachvalue' in config:\n            attachvalue = config.pop('attachvalue')\n            reinit = reinit or (attachvalue != self.attachvalue)\n            self.attachvalue = attachvalue\n        if 'auto2dashes' in config:\n            self.auto2dashes = config.pop('auto2dashes')\n        if 'name' in config:\n            name = config.pop('name')\n            reinit = reinit or (name != self.name)\n            self.name = name\n        if 'help' in config:\n            self.help = config.pop('help')\n            self._set_or_remove_extra_handler(\n                self.help, ('--help', '-h'), self.help_handler)\n        if 'version' in config:\n            self.version = config.pop('version')\n            self._set_or_remove_extra_handler(\n                self.version is not None,\n                ('--version', '-v'),\n                self.version_handler)\n        if 'case_sensitive' in config:\n            case_sensitive = config.pop('case_sensitive')\n            reinit = reinit or (case_sensitive != self.case_sensitive)\n            self.case_sensitive = case_sensitive\n        if 'optionsfirst' in config:\n            self.options_first = config.pop('optionsfirst')\n        if 'appearedonly' in config:\n            self.appeared_only = config.pop('appearedonly')\n        if 'namedoptions' in config:\n            namedoptions = config.pop('namedoptions')\n            reinit = reinit or (namedoptions != self.namedoptions)\n            self.namedoptions = namedoptions\n        if 'extra' in config:\n            self.extra.update(self._formal_extra(config.pop('extra')))\n\n        if config:  # should be empty\n            raise ValueError(\n                '`%s` %s not accepted key argument%s' % (\n                    '`, `'.join(config),\n                    'is' if len(config) == 1 else 'are',\n                    '' if len(config) == 1 else 's'\n                ))\n\n        if self.doc is not None and reinit:\n            logger.warning(\n                'You changed the config that requires re-initialized'\n                ' `Docpie` object. Create a new one instead'\n            )\n            self._init()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn alias set of a flag ; return None if flag is not defined in the options.", "response": "def find_flag_alias(self, flag):\n        \"\"\"Return alias set of a flag; return None if flag is not defined in\n        \"Options\".\n        \"\"\"\n        for each in self.opt_names:\n            if flag in each:\n                result = set(each)  # a copy\n                result.remove(flag)\n                return result\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_auto_handler(self, flag, handler):\n        assert flag.startswith('-') and flag not in ('-', '--')\n        alias = self.find_flag_alias(flag) or []\n        self.extra[flag] = handler\n        for each in alias:\n            self.extra[each] = handler", "response": "Set the handler for a flag."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh_core(self):\n        self.log.info('Sending out mass query for all attributes')\n        for key in ATTR_CORE:\n            self.query(key)", "response": "Query all attributes that exist regardless of power state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nkeep requesting all attributes until power on event is detected.", "response": "def poweron_refresh(self):\n        \"\"\"Keep requesting all attributes until it works.\n\n        Immediately after a power on event (POW1) the AVR is inconsistent with\n        which attributes can be successfully queried.  When we detect that\n        power has just been turned on, we loop every second making a bulk\n        query for every known attribute.  This continues until we detect that\n        values have been returned for at least one input name (this seems to\n        be the laggiest of all the attributes)\n        \"\"\"\n        if self._poweron_refresh_successful:\n            return\n        else:\n            self.refresh_all()\n            self._loop.call_later(2, self.poweron_refresh)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef refresh_all(self):\n        self.log.info('refresh_all')\n        for key in LOOKUP:\n            self.query(key)", "response": "Query all known attributes and refresh the internal state table for all known attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when asyncio. Protocol establishes the network connection.", "response": "def connection_made(self, transport):\n        \"\"\"Called when asyncio.Protocol establishes the network connection.\"\"\"\n        self.log.info('Connection established to AVR')\n        self.transport = transport\n\n        #self.transport.set_write_buffer_limits(0)\n        limit_low, limit_high = self.transport.get_write_buffer_limits()\n        self.log.debug('Write buffer limits %d to %d', limit_low, limit_high)\n\n        self.command('ECH1')\n        self.refresh_core()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when asyncio. Protocol detects received data from network.", "response": "def data_received(self, data):\n        \"\"\"Called when asyncio.Protocol detects received data from network.\"\"\"\n        self.buffer += data.decode()\n        self.log.debug('Received %d bytes from AVR: %s', len(self.buffer), self.buffer)\n        self._assemble_buffer()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when asyncio. Protocol loses the network connection.", "response": "def connection_lost(self, exc):\n        \"\"\"Called when asyncio.Protocol loses the network connection.\"\"\"\n        if exc is None:\n            self.log.warning('eof from receiver?')\n        else:\n            self.log.warning('Lost connection to receiver: %s', exc)\n\n        self.transport = None\n\n        if self._connection_lost_callback:\n            self._loop.call_soon(self._connection_lost_callback)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassemble the buffer into individual commands.", "response": "def _assemble_buffer(self):\n        \"\"\"Split up received data from device into individual commands.\n\n        Data sent by the device is a sequence of datagrams separated by\n        semicolons.  It's common to receive a burst of them all in one\n        submission when there's a lot of device activity.  This function\n        disassembles the chain of datagrams into individual messages which\n        are then passed on for interpretation.\n        \"\"\"\n        self.transport.pause_reading()\n\n        for message in self.buffer.split(';'):\n            if message != '':\n                self.log.debug('assembled message '+message)\n                self._parse_message(message)\n\n        self.buffer = \"\"\n\n        self.transport.resume_reading()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _populate_inputs(self, total):\n        total = total + 1\n        for input_number in range(1, total):\n            self.query('ISN'+str(input_number).zfill(2))", "response": "Populate the names for all active configured inputs on the device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_message(self, data):\n        recognized = False\n        newdata = False\n\n        if data.startswith('!I'):\n            self.log.warning('Invalid command: %s', data[2:])\n            recognized = True\n        elif data.startswith('!R'):\n            self.log.warning('Out-of-range command: %s', data[2:])\n            recognized = True\n        elif data.startswith('!E'):\n            self.log.warning('Cannot execute recognized command: %s', data[2:])\n            recognized = True\n        elif data.startswith('!Z'):\n            self.log.warning('Ignoring command for powered-off zone: %s', data[2:])\n            recognized = True\n        else:\n\n            for key in LOOKUP:\n                if data.startswith(key):\n                    recognized = True\n\n                    value = data[len(key):]\n                    oldvalue = getattr(self, '_'+key)\n                    if oldvalue != value:\n                        changeindicator = 'New Value'\n                        newdata = True\n                    else:\n                        changeindicator = 'Unchanged'\n\n                    if key in LOOKUP:\n                        if 'description' in LOOKUP[key]:\n                            if value in LOOKUP[key]:\n                                self.log.info('%s: %s (%s) -> %s (%s)',\n                                              changeindicator,\n                                              LOOKUP[key]['description'], key,\n                                              LOOKUP[key][value], value)\n                            else:\n                                self.log.info('%s: %s (%s) -> %s',\n                                              changeindicator,\n                                              LOOKUP[key]['description'], key,\n                                              value)\n                    else:\n                        self.log.info('%s: %s -> %s', changeindicator, key, value)\n\n                    setattr(self, '_'+key, value)\n\n                    if key == 'Z1POW' and value == '1' and oldvalue == '0':\n                        self.log.info('Power on detected, refreshing all attributes')\n                        self._poweron_refresh_successful = False\n                        self._loop.call_later(1, self.poweron_refresh)\n\n                    if key == 'Z1POW' and value == '0' and oldvalue == '1':\n                        self._poweron_refresh_successful = False\n\n                    break\n\n        if data.startswith('ICN'):\n            self.log.warning('ICN update received')\n            recognized = True\n            self._populate_inputs(int(value))\n\n        if data.startswith('ISN'):\n            recognized = True\n            self._poweron_refresh_successful = True\n\n            input_number = int(data[3:5])\n            value = data[5:]\n\n            oldname = self._input_names.get(input_number, '')\n\n            if oldname != value:\n                self._input_numbers[value] = input_number\n                self._input_names[input_number] = value\n                self.log.info('New Value: Input %d is called %s', input_number, value)\n                newdata = True\n\n        if newdata:\n            if self._update_callback:\n                self._loop.call_soon(self._update_callback, data)\n        else:\n            self.log.debug('no new data encountered')\n\n        if not recognized:\n            self.log.warning('Unrecognized response: %s', data)", "response": "Interpret each message datagram from the device and do the needful."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nissuing a raw formatted command to the device.", "response": "def formatted_command(self, command):\n        \"\"\"Issue a raw, formatted command to the device.\n\n        This function is invoked by both query and command and is the point\n        where we actually send bytes out over the network.  This function does\n        the wrapping and formatting required by the Anthem API so that the\n        higher-level function can just operate with regular strings without\n        the burden of byte encoding and terminating device requests.\n\n            :param command: Any command as documented in the Anthem API\n            :type command: str\n\n        :Example:\n\n        >>> formatted_command('Z1VOL-50')\n        \"\"\"\n        command = command\n        command = command.encode()\n\n        self.log.debug('> %s', command)\n        try:\n            self.transport.write(command)\n            time.sleep(0.01)\n        except:\n            self.log.warning('No transport found, unable to send command')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump_rawdata(self):\n        if hasattr(self, 'transport'):\n            attrs = vars(self.transport)\n            return ', '.join(\"%s: %s\" % item for item in attrs.items())", "response": "Return contents of the transport object for debugging forensics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_upsert(self, value, criteria):\n\n        value = value.strip()\n        v = value.lower()\n        self.lower_val_to_val[v] = value\n        criteria_array = self.upserts.get(v)\n        if criteria_array is None:\n            criteria_array = []\n            # start with # '{\"value\": \"some_value\", \"criteria\": []}, '\n            self.upserts_size[v] = 31 + len(value)\n        criteria_array.append(criteria.to_dict())\n        self.upserts[v] = criteria_array\n        self.upserts_size[v] += criteria.json_size()", "response": "Add a tag or populator to the batch by value and criteria"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a delete tag or populator by value - these are processed before upserts", "response": "def add_delete(self, value):\n        \"\"\"Delete a tag or populator by value - these are processed before upserts\"\"\"\n\n        value = value.strip()\n        v = value.lower()\n        self.lower_val_to_val[v] = value\n        if len(v) == 0:\n            raise ValueError(\"Invalid value for delete. Value is empty.\")\n\n        self.deletes.add(v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parts(self):\n\n        parts = []\n\n        upserts = dict()\n        deletes = []\n\n        # we keep track of the batch size as we go (pretty close approximation!) so we can chunk it small enough\n        # to limit the HTTP posts to under 700KB - server limits to 750KB, so play it safe\n        max_upload_size = 700000\n\n        # loop upserts first - fit the deletes in afterward\n        # '{\"replace_all\": true, \"complete\": false, \"guid\": \"6659fbfc-3f08-42ee-998c-9109f650f4b7\", \"upserts\": [], \"deletes\": []}'\n        base_part_size = 118\n        if not self.replace_all:\n            base_part_size += 1  # yeah, this is totally overkill :)\n\n        part_size = base_part_size\n        for value in self.upserts:\n            if (part_size + self.upserts_size[value]) >= max_upload_size:\n                # this record would put us over the limit - close out the batch part and start a new one\n                parts.append(BatchPart(self.replace_all, upserts, deletes))\n                upserts = dict()\n                deletes = []\n                part_size = base_part_size\n\n            # for the new upserts dict, drop the lower-casing of value\n            upserts[self.lower_val_to_val[value]] = self.upserts[value]\n            part_size += self.upserts_size[value]    # updating the approximate size of the batch\n\n        for value in self.deletes:\n            # delete adds length of string plus quotes, comma and space\n            if (part_size + len(value) + 4) >= max_upload_size:\n                parts.append(BatchPart(self.replace_all, upserts, deletes))\n                upserts = dict()\n                deletes = []\n                part_size = base_part_size\n\n            # for the new deletes set, drop the lower-casing of value\n            deletes.append({'value': self.lower_val_to_val[value]})\n            part_size += len(value) + 4\n\n        if len(upserts) + len(deletes) > 0:\n            # finish the batch\n            parts.append(BatchPart(self.replace_all, upserts, deletes))\n\n        if len(parts) == 0:\n            if not self.replace_all:\n                raise ValueError(\"Batch has no data, and 'replace_all' is False\")\n            parts.append(BatchPart(self.replace_all, dict(), []))\n\n        # last part finishes the batch\n        parts[-1].set_last_part()\n        return parts", "response": "Return an array of batch parts to submit"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_json(self, guid):\n        upserts = []\n        for value in self.upserts:\n            upserts.append({\"value\": value, \"criteria\": self.upserts[value]})\n        return json.dumps({'replace_all': self.replace_all, 'guid': guid,\n                           'complete': self.complete, 'upserts': upserts, 'deletes': self.deletes})", "response": "Build the JSON with the input guid"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure a non - array field", "response": "def _ensure_field(self, key):\n        \"\"\"Ensure a non-array field\"\"\"\n        if self._has_field:\n            self._size += 2  # comma, space\n        self._has_field = True\n\n        self._size += len(key) + 4"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ensure_array(self, key, value):\n        if key not in self._json_dict:\n            self._json_dict[key] = []\n\n            self._size += 2             # brackets\n            self._ensure_field(key)\n\n        if len(self._json_dict[key]) > 0:\n            # this array already has an entry, so add comma and space\n            self._size += 2\n\n        if isinstance(value, str):\n            self._size += 2  # quotes\n        self._size += len(str(value))\n\n        self._json_dict[key].append(value)", "response": "Ensure a field is an array"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a single TCP flag to the bitmask", "response": "def add_tcp_flag(self, tcp_flag):\n        \"\"\"Add a single TCP flag - will be OR'd into the existing bitmask\"\"\"\n\n        if tcp_flag not in [1, 2, 4, 8, 16, 32, 64, 128]:\n            raise ValueError(\"Invalid TCP flag. Valid: [1, 2, 4, 8, 16,32, 64, 128]\")\n\n        prev_size = 0\n\n        if self._json_dict.get('tcp_flags') is None:\n            self._json_dict['tcp_flags'] = 0\n        else:\n            prev_size = len(str(self._json_dict['tcp_flags'])) + len('tcp_flags') + 3  # str, key, key quotes, colon\n        self._json_dict['tcp_flags'] |= tcp_flag\n\n        # update size\n        new_size = len(str(self._json_dict['tcp_flags'])) + len('tcp_flags') + 3  # str, key, key quotes, colon\n        self._size += new_size - prev_size\n\n        if prev_size == 0 and self._has_field:\n            # add the comma and space\n            self._size += 2\n        self._has_field = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the complete tcp flag bitmask", "response": "def set_tcp_flags(self, tcp_flags):\n        \"\"\"Set the complete tcp flag bitmask\"\"\"\n\n        if tcp_flags < 0 or tcp_flags > 255:\n            raise ValueError(\"Invalid tcp_flags. Valid: 0-255.\")\n\n        prev_size = 0\n        if self._json_dict.get('tcp_flags') is not None:\n            prev_size = len(str(self._json_dict['tcp_flags'])) + len('tcp_flags') + 3  # str, key, key quotes, colon\n\n        self._json_dict['tcp_flags'] = tcp_flags\n\n        # update size\n        new_size = len(str(self._json_dict['tcp_flags'])) + len('tcp_flags') + 3  # str, key, key quotes, colon\n        self._size += new_size - prev_size\n\n        if prev_size == 0 and self._has_field:\n            # add the comma and space\n            self._size += 2\n        self._has_field = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit the batch and return the JSON - > dict from the last HTTP response", "response": "def _submit_batch(self, url, batch):\n        \"\"\"Submit the batch, returning the JSON->dict from the last HTTP response\"\"\"\n        # TODO: validate column_name\n        batch_parts = batch.parts()\n\n        guid = \"\"\n        headers = {\n            'User-Agent': 'kentik-python-api/0.1',\n            'Content-Type': 'application/json',\n            'X-CH-Auth-Email': self.api_email,\n            'X-CH-Auth-API-Token': self.api_token\n        }\n\n        # submit each part\n        last_part = dict()\n        for batch_part in batch_parts:\n            # submit\n            resp = requests.post(url, headers=headers, data=batch_part.build_json(guid))\n\n            # print the HTTP response to help debug\n            print(resp.text)\n\n            # break out at first sign of trouble\n            resp.raise_for_status()\n            last_part = resp.json()\n            guid = last_part['guid']\n            if guid is None or len(guid) == 0:\n                raise RuntimeError('guid not found in batch response')\n\n        return last_part"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubmits a batch of populators for a given column.", "response": "def submit_populator_batch(self, column_name, batch):\n        \"\"\"Submit a populator batch\n\n        Submit a populator batch as a series of HTTP requests in small chunks,\n        returning the batch GUID, or raising exception on error.\"\"\"\n        if not set(column_name).issubset(_allowedCustomDimensionChars):\n            raise ValueError('Invalid custom dimension name \"%s\": must only contain letters, digits, and underscores' % column_name)\n        if len(column_name) < 3 or len(column_name) > 20:\n            raise ValueError('Invalid value \"%s\": must be between 3-20 characters' % column_name)\n\n        url = '%s/api/v5/batch/customdimensions/%s/populators' % (self.base_url, column_name)\n        resp_json_dict = self._submit_batch(url, batch)\n        if resp_json_dict.get('error') is not None:\n            raise RuntimeError('Error received from server: %s' % resp_json_dict['error'])\n\n        return resp_json_dict['guid']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsubmitting a tag batch", "response": "def submit_tag_batch(self, batch):\n        \"\"\"Submit a tag batch\"\"\"\n        url = '%s/api/v5/batch/tags' % self.base_url\n        self._submit_batch(url, batch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches the status of a batch given the guid", "response": "def fetch_batch_status(self, guid):\n        \"\"\"Fetch the status of a batch, given the guid\"\"\"\n        url = '%s/api/v5/batch/%s/status' % (self.base_url, guid)\n        headers = {\n            'User-Agent': 'kentik-python-api/0.1',\n            'Content-Type': 'application/json',\n            'X-CH-Auth-Email': self.api_email,\n            'X-CH-Auth-API-Token': self.api_token\n        }\n\n        resp = requests.get(url, headers=headers)\n\n        # break out at first sign of trouble\n        resp.raise_for_status()\n        return BatchResponse(guid, resp.json())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npredicting on the images in imgs and returns the result", "response": "def predict_imgs(self, imgs):\n        '''\n        takes an image input and predicts on it\n        this expects an ndarray (heightxwidthxchannels)\n        this model shouldbe a (Nx224x224x3) numpy array\n        this method it noce if you want to do preprocessing\n        then predict results on those preprocessed images\n        this function expects the image array to be jpg\n        '''\n        imgs = preprocess_input(imgs)\n        return self.model.predict(imgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_files(self, files):\n        '''\n        reads files off disk, resizes them\n        and then predicts them, files should\n        be a list or itrerable of file paths\n        that lead to images, they are then\n        loaded with opencv, resized, and predicted\n        '''\n        imgs = [0]*len(files)\n        for i, file in enumerate(files):\n            img = cv2.imread(file).astype('float64')\n            img = cv2.resize(img, (224,224))\n            img = preprocess_input(img)\n            if img is None:\n                print('failed to open: {}, continuing...'.format(file))\n            imgs[i] = img\n        return self.model.predict(np.array(imgs))", "response": "reads files off disk resizes them and predicts them"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rename_genome(genome_in, genome_out=None):\n\n    if genome_out is None:\n        genome_out = \"{}_renamed.fa\".format(genome_in.split(\".\")[0])\n\n    with open(genome_out, \"w\") as output_handle:\n        for record in SeqIO.parse(genome_in, \"fasta\"):\n\n            # Replace hyphens, tabs and whitespace with underscores\n            new_record_id = record.id.replace(\" \", \"_\")\n            new_record_id = new_record_id.replace(\"-\", \"_\")\n            new_record_id = new_record_id.replace(\"\\t\", \"_\")\n\n            # Remove anything that's weird, i.e. not alphanumeric\n            # or an underscore\n            new_record_id = re.sub(\"[^_A-Za-z0-9]+\", \"\", new_record_id)\n            header = \">{}\\n\".format(new_record_id)\n\n            output_handle.write(header)\n            output_handle.write(\"{}\\n\".format(str(record.seq)))", "response": "Rename genome and slugify headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter a genome file according to various parameters.", "response": "def filter_genome(genome_in, threshold=500, list_records=None):\n    \"\"\"Filter fasta file according to various parameters.\n\n    Filter a fasta file according to size and/or an explicit list of records\n    to keep.\n\n    Parameters\n    ----------\n    genome_in: file, str or pathlib.Path\n        The input genome in FASTA format.\n    threshold: int, optional\n        The size below which genome records are discarded. Default is the\n        default minimum chunk size, i.e. 500.\n    list_records: array_like, optional\n        A list of record ids to keep. If not None, records that don't belong\n        to that list are discarded. Default is None, i.e. all records are\n        kept.\n\n    Returns\n    -------\n    records_to_write: generator\n        Filtered records that were kept.\n    \"\"\"\n\n    if list_records is None:\n\n        def truth(*args):\n            del args\n            return True\n\n        is_a_record_to_keep = truth\n\n    else:\n        try:\n            with open(list_records) as records_handle:\n                records_to_keep = records_handle.readlines()\n        except OSError:\n            if not hasattr(list_records, \"__contains__\"):\n                raise\n            else:\n                records_to_keep = list_records\n\n        is_a_record_to_keep = records_to_keep.__contains__\n\n    records_to_write = (\n        record\n        for record in SeqIO.parse(genome_in, \"fasta\")\n        if (len(record.seq) >= threshold and is_a_record_to_keep(record.id))\n    )\n\n    return records_to_write"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrenaming proteins in a file in FASTA format.", "response": "def rename_proteins(prot_in, prot_out=None, chunk_size=DEFAULT_CHUNK_SIZE):\n    \"\"\"Rename prodigal output files\n\n    Rename output files from prodigal according to the following naming\n    scheme: >contigX_chunkY__geneZ\n\n    Chunk numbering starts at 0 and gene identification is taken from prodigal.\n\n    Parameters\n    ----------\n    prot_in : file, str or pathlib.Path\n        The input protein file in FASTA format to be renamed.\n    prot_out : file, str or pathlib.Path\n        The output protein file to be renamed into.\n    chunk_size : int, optional\n        The size of the chunks (in bp) used in the pipeline. Default is 1000.\n    \"\"\"\n\n    if prot_out is None:\n        prot_out = \"{}_renamed.fa\".format(prot_in.split(\".\")[0])\n\n    with open(prot_out, \"w\") as prot_out_handle:\n\n        for record in SeqIO.parse(prot_in, \"fasta\"):\n            header = record.description\n            name, pos_start, _, _, _ = header.split(\"#\")\n\n            chunk_start = int(pos_start) // chunk_size\n\n            name_split = name.split(\"_\")\n            contig_name = \"_\".join(name_split[:-1])\n            gene_id = name_split[-1]\n\n            new_record_id = \"{}_{}__gene{}\".format(\n                contig_name, chunk_start, gene_id\n            )\n\n            prot_out_handle.write(\">{}\\n\".format(new_record_id))\n            prot_out_handle.write(\"{}\\n\".format(str(record.seq)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_records(records, output_file, split=False):\n\n    \"\"\"Write FASTA records\n\n    Write a FASTA file from an iterable of records.\n\n    Parameters\n    ----------\n    records : iterable\n        Input records to write.\n    output_file : file, str or pathlib.Path\n        Output FASTA file to be written into.\n    split : bool, optional\n        If True, each record is written into its own separate file. Default is\n        False.\n    \"\"\"\n\n    if split:\n        for record in records:\n            with open(\n                \"{}{}.fa\".format(output_file, record.id), \"w\"\n            ) as record_handle:\n                SeqIO.write(record, record_handle, \"fasta\")\n    else:\n        SeqIO.write(records, output_file, \"fasta\")", "response": "Write a FASTA file from an iterable of records into a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_sample(self, **data):\n        missing_dimensions = set(data).difference(self.dimensions)\n\n        if missing_dimensions:\n            raise KeyError('Dimensions not defined in this series: %s'\n                           % ', '.join(missing_dimensions))\n\n        for dim in self.dimensions:\n            getattr(self, dim).append(data.get(dim))", "response": "Add a sample to this series."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield the samples as dicts keyed by dimensions.", "response": "def samples(self):\n        \"\"\"Yield the samples as dicts, keyed by dimensions.\"\"\"\n        names = self.series.dimensions\n        for values in zip(*(getattr(self.series, name) for name in names)):\n            yield dict(zip(names, values))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate path to filename and saves binary data", "response": "def write_binary(filename, data):\n    \"\"\"Create path to filename and saves binary data\"\"\"\n    dir = os.path.dirname(filename)\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n    with open(filename, 'wb') as f:\n        f.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef files_with_exts(root='.', suffix=''):\n    return (os.path.join(rootdir, filename)\n            for rootdir, dirnames, filenames in os.walk(root)\n            for filename in filenames\n            if filename.endswith(suffix))", "response": "Returns generator that contains filenames from root directory and ends with suffix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_apikey():\n    env_keys = ['TINYPNG_APIKEY', 'TINYPNG_API_KEY']\n    paths = []\n    paths.append(os.path.join(os.path.abspath(\".\"), \"tinypng.key\"))  # local directory\n    paths.append(os.path.expanduser(\"~/.tinypng.key\"))  # home directory\n\n    for env_key in env_keys:\n        if os.environ.get(env_key):\n            return os.environ.get(env_key)\n\n    for path in paths:\n        if os.path.exists(path):\n            return open(path, 'rt').read().strip()\n\n    return None", "response": "Finds TinyPNG API key in following order"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomparing two RPM packages and returns 1 if the RPM string is newer or - 1 if the RPM string is newer.", "response": "def compare_packages(rpm_str_a, rpm_str_b, arch_provided=True):\n    \"\"\"Compare two RPM strings to determine which is newer\n\n    Parses version information out of RPM package strings of the form\n    returned by the ``rpm -q`` command and compares their versions to\n    determine which is newer. Provided strings *do not* require an\n    architecture at the end, although if providing strings without\n    architecture, the ``arch_provided`` parameter should be set to\n    False.\n\n    Note that the packages do not have to be the same package (i.e.\n    they do not require the same name or architecture).\n\n    :param str rpm_str_a: an rpm package string\n    :param str rpm_str_b: an rpm package string\n    :param bool arch_provided: whether package strings contain\n        architecture information\n    :return: 1 (``a`` is newer), 0 (versions are equivalent), or -1\n        (``b`` is newer)\n    :rtype: int\n    \"\"\"\n    logger.debug('resolve_versions(%s, %s)', rpm_str_a, rpm_str_b)\n    evr_a = parse_package(rpm_str_a, arch_provided)['EVR']\n    evr_b = parse_package(rpm_str_b, arch_provided)['EVR']\n    return labelCompare(evr_a, evr_b)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomparing two EVR tuples to determine which is newer", "response": "def compare_evrs(evr_a, evr_b):\n    \"\"\"Compare two EVR tuples to determine which is newer\n\n    This method compares the epoch, version, and release of the\n    provided package strings, assuming that epoch is 0 if not provided.\n    Comparison is performed on the epoch, then the version, and then\n    the release. If at any point a non-equality is found, the result is\n    returned without any remaining comparisons being performed (e.g. if\n    the epochs of the packages differ, the versions are releases are\n    not compared).\n\n    :param tuple evr_a: an EVR tuple\n    :param tuple evr_b: an EVR tuple\n    \"\"\"\n    a_epoch, a_ver, a_rel = evr_a\n    b_epoch, b_ver, b_rel = evr_b\n    if a_epoch != b_epoch:\n        return a_newer if a_epoch > b_epoch else b_newer\n    ver_comp = compare_versions(a_ver, b_ver)\n    if ver_comp != a_eq_b:\n        return ver_comp\n    rel_comp = compare_versions(a_rel, b_rel)\n    return rel_comp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compare_versions(version_a, version_b):\n    logger.debug('compare_versions(%s, %s)', version_a, version_b)\n    if version_a == version_b:\n        return a_eq_b\n    try:\n        chars_a, chars_b = list(version_a), list(version_b)\n    except TypeError:\n        raise RpmError('Could not compare {0} to '\n                       '{1}'.format(version_a, version_b))\n    while len(chars_a) != 0 and len(chars_b) != 0:\n        logger.debug('starting loop comparing %s '\n                     'to %s', chars_a, chars_b)\n        _check_leading(chars_a, chars_b)\n        if chars_a[0] == '~' and chars_b[0] == '~':\n            map(lambda x: x.pop(0), (chars_a, chars_b))\n        elif chars_a[0] == '~':\n            return b_newer\n        elif chars_b[0] == '~':\n            return a_newer\n        if len(chars_a) == 0 or len(chars_b) == 0:\n            break\n        block_res = _get_block_result(chars_a, chars_b)\n        if block_res != a_eq_b:\n            return block_res\n    if len(chars_a) == len(chars_b):\n        logger.debug('versions are equal')\n        return a_eq_b\n    else:\n        logger.debug('versions not equal')\n        return a_newer if len(chars_a) > len(chars_b) else b_newer", "response": "Compares two RPM version strings and returns the result of the comparison."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an RPM version string and returns a common. Package object containing all parsed information.", "response": "def package(package_string, arch_included=True):\n    \"\"\"Parse an RPM version string\n\n    Parses most (all tested) RPM version strings to get their name,\n    epoch, version, release, and architecture information. Epoch (also\n    called serial) is an optional component for RPM versions, and it\n    is also optional when providing a version string to this function.\n    RPM assumes the epoch to be 0 if it is not provided, so that\n    behavior is mirrored here.\n\n    :param str package_string:\n    :param bool arch_included:\n    :return: A :any:`common.Package` object containing all parsed\n        information\n    :rtype: common.Package\n    \"\"\"\n    logger.debug('package(%s, %s)', package_string, arch_included)\n    pkg_info = parse_package(package_string, arch_included)\n    pkg = Package(pkg_info['name'], pkg_info['EVR'][0], pkg_info['EVR'][1],\n                  pkg_info['EVR'][2], pkg_info['arch'],\n                  package_str=package_string)\n    return pkg"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_package(package_string, arch_included=True):\n    # Yum sets epoch values to 0 if they are not specified\n    logger.debug('parse_package(%s, %s)', package_string, arch_included)\n    default_epoch = '0'\n    arch = None\n    if arch_included:\n        char_list = list(package_string)\n        arch = _pop_arch(char_list)\n        package_string = ''.join(char_list)\n        logger.debug('updated version_string: %s', package_string)\n    try:\n        name, epoch, version, release = _rpm_re.match(package_string).groups()\n    except AttributeError:\n        raise RpmError('Could not parse package string: %s' % package_string)\n    if epoch == '' or epoch is None:\n        epoch = default_epoch\n    info = {\n        'name': name,\n        'EVR': (epoch, version, release),\n        'arch': arch\n    }\n    logger.debug('parsed information: %s', info)\n    return info", "response": "Parse an RPM version string to get name version and arch."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _pop_arch(char_list):\n    logger.debug('_pop_arch(%s)', char_list)\n    arch_list = []\n    char = char_list.pop()\n    while char != '.':\n        arch_list.insert(0, char)\n        try:\n            char = char_list.pop()\n        except IndexError:  # Raised for a string with no periods\n            raise RpmError('Could not parse an architecture. Did you mean to '\n                           'set the arch_included flag to False?')\n    logger.debug('arch chars: %s', arch_list)\n    return ''.join(arch_list)", "response": "Pop the architecture from a version string and return it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove any non - alphanumeric or non - ~ leading characters from the beginning of any provided list.", "response": "def _check_leading(*char_lists):\n    \"\"\"Remove any non-alphanumeric or non-~ leading characters\n\n    Checks the beginning of any provided lists for non-alphanumeric or\n    non-~ (tilde) leading characters and removes them if found.\n    Operates on (and possibly alters) the passed list.\n\n    :param list char_list: a list or lists of characters\n    :return: None\n    :rtype: None\n    \"\"\"\n    logger.debug('_check_leading(%s)', char_lists)\n    for char_list in char_lists:\n        while (len(char_list) != 0 and not char_list[0].isalnum() and\n                not char_list[0] == '~'):\n            char_list.pop(0)\n        logger.debug('updated list: %s', char_list)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _trim_zeros(*char_lists):\n    logger.debug('_trim_zeros(%s)', char_lists)\n    for char_list in char_lists:\n        while len(char_list) != 0 and char_list[0] == '0':\n            char_list.pop(0)\n        logger.debug('updated block: %s', char_list)", "response": "Removes any leading zeros from the passed list of characters and returns the new list of the passed list of the passed list of characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pop_digits(char_list):\n    logger.debug('_pop_digits(%s)', char_list)\n    digits = []\n    while len(char_list) != 0 and char_list[0].isdigit():\n        digits.append(char_list.pop(0))\n    logger.debug('got digits: %s', digits)\n    logger.debug('updated char list: %s', char_list)\n    return digits", "response": "Pops any and all consecutive digits from the front of the provided character list and returns them as a list of string digits."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pop_letters(char_list):\n    logger.debug('_pop_letters(%s)', char_list)\n    letters = []\n    while len(char_list) != 0 and char_list[0].isalpha():\n        letters.append(char_list.pop(0))\n    logger.debug('got letters: %s', letters)\n    logger.debug('updated char list: %s', char_list)\n    return letters", "response": "Pop consecutive letters from the front of a list and return them as a list of characters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompare two blocks of characters in the order of the character type.", "response": "def _compare_blocks(block_a, block_b):\n    \"\"\"Compare two blocks of characters\n\n    Compares two blocks of characters of the form returned by either\n    the :any:`_pop_digits` or :any:`_pop_letters` function. Blocks\n    should be character lists containing only digits or only letters.\n    Both blocks should contain the same character type (digits or\n    letters).\n\n    The method of comparison mirrors the method used by RPM. If the\n    blocks are digit blocks, any leading zeros are trimmed, and\n    whichever block is longer is assumed to be larger. If the resultant\n    blocks are the same length, or if the blocks are non-numeric, they\n    are checked for string equality and considered equal if the string\n    equality comparison returns True. If not, whichever evaluates as\n    greater than the other (again in string comparison) is assumed to be\n    larger.\n\n    :param list block_a: an all numeric or all alphabetic character\n        list\n    :param list block_b: an all numeric or all alphabetic character\n        list. Alphabetic or numeric character should match ``block_a``\n    :return: 1 (if ``a`` is newer), 0 (if versions are equal) or\n        -1 (if ``b`` is newer)\n    :rtype: int\n    \"\"\"\n    logger.debug('_compare_blocks(%s, %s)', block_a, block_b)\n    if block_a[0].isdigit():\n        _trim_zeros(block_a, block_b)\n        if len(block_a) != len(block_b):\n            logger.debug('block lengths are not equal')\n            return a_newer if len(block_a) > len(block_b) else b_newer\n    if block_a == block_b:\n        logger.debug('blocks are equal')\n        return a_eq_b\n    else:\n        logger.debug('blocks are not equal')\n        return a_newer if block_a > block_b else b_newer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_block_result(chars_a, chars_b):\n    logger.debug('_get_block_result(%s, %s)', chars_a, chars_b)\n    first_is_digit = chars_a[0].isdigit()\n    pop_func = _pop_digits if first_is_digit else _pop_letters\n    return_if_no_b = a_newer if first_is_digit else b_newer\n    block_a, block_b = pop_func(chars_a), pop_func(chars_b)\n    if len(block_b) == 0:\n        logger.debug('blocks are equal')\n        return return_if_no_b\n    return _compare_blocks(block_a, block_b)", "response": "Get the first block from two character lists and compare\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(name: str, *,\n           validate: bool=None) -> snug.Query[Channel]:\n    \"\"\"create a new channel\"\"\"\n    return {'name': name, 'validate': validate}", "response": "create a new channel"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tube(self, name):\n        if name in self._tubes:\n            return self._tubes[name]\n\n        assert name, 'Tube name must be specified'\n        t = self._tube_cls(self, name)\n        self._tubes[name] = t\n        return t", "response": "Returns a tube by its name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns queue statistics (coroutine) :param tube_name: If specified, statistics by a specific tube is returned, else statistics about all tubes is returned", "response": "async def statistics(self, tube_name=None):\n        \"\"\"\n            Returns queue statistics (coroutine)\n\n            :param tube_name:\n                If specified, statistics by a specific tube is returned,\n                else statistics about all tubes is returned\n        \"\"\"\n        args = None\n        if tube_name is not None:\n            args = (tube_name,)\n\n        res = await self._conn.call('{}.statistics'.format(self._namespace), args)\n        if self._conn.version < (1, 7):  # pragma: nocover\n            return res.body[0][0]\n        return res.body[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef call(self, args=None, kwargs=None, node=None, send_timeout=1000, recv_timeout=5000, zmq_ctx=None):\n\n        context = zmq_ctx or zmq.Context()\n        assert isinstance(context, zmq.Context)\n\n        args = args or ()\n        assert isinstance(args, tuple)\n        kwargs = kwargs or {}\n        assert isinstance(kwargs, dict)\n\n        socket = context.socket(zmq.REQ)\n\n        # connect to all addresses ( optionally matching node name )\n        for n, a in [(n, a) for (n, a) in self.providers if (not node or n == node)]:\n            socket.connect(a)\n\n        # build message\n        fullreq = ServiceRequest(service=self.name, args=pickle.dumps(args), kwargs=pickle.dumps(kwargs))\n\n        poller = zmq.Poller()\n        poller.register(socket)  # POLLIN for recv, POLLOUT for send\n\n        evts = dict(poller.poll(send_timeout))\n        if socket in evts and evts[socket] == zmq.POLLOUT:\n            socket.send(fullreq.serialize())\n            # TODO : find a way to get rid fo these timeouts when debugging\n            # TODO : when timeout Exception should occur ( not returning None )\n            evts = dict(poller.poll(recv_timeout))  # blocking until answer\n            if socket in evts and evts[socket] == zmq.POLLIN:\n                resp = socket.recv()\n                fullresp = ServiceResponse_dictparse(resp)\n\n                if fullresp.has_field('response'):\n                    return pickle.loads(fullresp.response)\n                elif fullresp.has_field('exception'):\n                    svcexc = fullresp.exception  # This has already been parsed by ServiceResponse_dictparse\n                    tb = pickle.loads(svcexc.traceback)\n                    if Traceback and isinstance(tb, Traceback):\n                        reraise(pickle.loads(svcexc.exc_type), pickle.loads(svcexc.exc_value), tb.as_traceback())\n                    else:  # traceback not usable\n                        reraise(pickle.loads(svcexc.exc_type), pickle.loads(svcexc.exc_value), None)\n                else:\n                    raise UnknownResponseTypeException(\"Unknown Response Type {0}\".format(type(fullresp)))\n            else:\n                raise ServiceCallTimeout(\"Did not receive response through ZMQ socket.\")\n        else:\n            raise ServiceCallTimeout(\"Can not send request through ZMQ socket.\")", "response": "Calls a service on a node with req as arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef device_measurement(device,\n                       ts=None,\n                       part=None,\n                       result=None,\n                       code=None,\n                       **kwargs):\n    \"\"\"Returns a JSON MeasurementPayload ready to be send through a\n    transport.\n\n    If `ts` is not given, the current time is used. `part` is an\n    optional `Part` object, and `result` and `code` are the respective\n    fields of the `Measurement` object. All other arguments are\n    interpreted as dimensions.\n\n    Minimal example, using a `Device` object to send two\n    measurements:\n\n    >>> d = Device(\"12345\")\n    >>> def publish(msg):\n    ...     pass\n    >>> publish(d.measurement(temperature=22.8))\n    >>> publish(d.measurement(pressure=4.1))\n\n    \"\"\"\n    if ts is None:\n        ts = local_now()\n\n    payload = MeasurementPayload(device=device, part=part)\n    m = Measurement(ts, result, code, list(kwargs))\n    payload.measurements.append(m)\n    m.add_sample(ts, **kwargs)\n    return dumps(payload)", "response": "Returns a JSON MeasurementPayload ready to be sent through a\n    transport."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_sample(self, ts, **kwargs):\n        if not self.series.offsets:\n            self.ts = ts\n            offset = 0\n        else:\n            dt = ts - self.ts\n            offset = (dt.days * 24 * 60 * 60 * 1000 + dt.seconds * 1000 +\n                      dt.microseconds // 1000)\n        self.series.add_sample(offset, **kwargs)", "response": "Add a sample to this measurements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield samples as dictionaries keyed by dimensions.", "response": "def samples(self):\n        \"\"\"Yield samples as dictionaries, keyed by dimensions.\"\"\"\n        names = self.series.dimensions\n        for n, offset in enumerate(self.series.offsets):\n            dt = datetime.timedelta(microseconds=offset * 1000)\n            d = {\"ts\": self.ts + dt}\n            for name in names:\n                d[name] = getattr(self.series, name)[n]\n            yield d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the format of an audio file.", "response": "def determine_format(data, extension=None):\n\t\"\"\"Determine the format of an audio file.\n\n\tParameters:\n\t\tdata (bytes-like object, str, os.PathLike, or file-like object):\n\t\t\tA bytes-like object, filepath, path-like object\n\t\t\tor file-like object of an audio file.\n\t\textension (str): The file extension of the file.\n\t\t\tUsed as a tie-breaker for formats that can\n\t\t\tbe used in multiple containers (e.g. ID3).\n\t\"\"\"\n\n\tif isinstance(data, (os.PathLike, str)):\n\t\tdata = open(data, 'rb')\n\n\tdata_reader = DataReader(data)\n\tdata_reader.seek(0, os.SEEK_SET)\n\td = data_reader.read(4)\n\n\tif d.startswith((b'ID3', b'\\xFF\\xFB')):  # TODO: Catch all MP3 possibilities.\n\t\tif extension is None or extension.endswith('.mp3'):\n\t\t\treturn MP3\n\n\tif d.startswith((b'fLaC', b'ID3')):\n\t\tif extension is None or extension.endswith('.flac'):\n\t\t\treturn FLAC\n\n\tif d.startswith(b'RIFF'):\n\t\tif extension is None or extension.endswith('.wav'):\n\t\t\treturn WAV\n\n\treturn None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(f):\n\n\tif isinstance(f, (os.PathLike, str)):\n\t\tfileobj = open(f, 'rb')\n\telse:\n\t\ttry:\n\t\t\tf.read(0)\n\t\texcept AttributeError:\n\t\t\traise ValueError(\"Not a valid file-like object.\")\n\t\texcept Exception:\n\t\t\traise ValueError(\"Can't read from file-like object.\")\n\n\t\tfileobj = f\n\n\tparser_cls = determine_format(fileobj, os.path.splitext(fileobj.name)[1])\n\n\tif parser_cls is None:\n\t\traise UnsupportedFormat(\"Supported format signature not found.\")\n\telse:\n\t\tfileobj.seek(0, os.SEEK_SET)\n\n\treturn parser_cls.load(fileobj)", "response": "Load audio metadata from file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads audio metadata from a bytes - like object.", "response": "def loads(b):\n\t\"\"\"Load audio metadata from a bytes-like object.\n\n\tParameters:\n\t\tb (bytes-like object): A bytes-like object of an audio file.\n\n\tReturns:\n\t\tFormat: An audio format object.\n\n\tRaises:\n\t\tUnsupportedFormat: If file is not of a supported format.\n\t\"\"\"\n\n\tparser_cls = determine_format(b)\n\n\tif parser_cls is None:\n\t\traise UnsupportedFormat(\"Supported format signature not found.\")\n\n\treturn parser_cls.load(b)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _urllib_send(opener, req, **kwargs):\n    if req.content and not any(h.lower() == 'content-type'\n                               for h in req.headers):\n        req = req.with_headers({'Content-Type': 'application/octet-stream'})\n    url = req.url + '?' + urlencode(req.params)\n    raw_req = urllib_request.Request(url, req.content, headers=req.headers)\n    set_urllib_method(raw_req, req.method)\n    try:\n        res = opener.open(raw_req, **kwargs)\n    except urllib_http_error_cls as http_err:\n        res = http_err\n    return Response(res.getcode(), content=res.read(), headers=res.headers)", "response": "Send a request with an urllib opener"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Find(self, node_type, item_type):\n        '''\n        method for finding specific types of notation from nodes.\n        will currently return the first one it encounters because this method's only really intended\n        for some types of notation for which the exact value doesn't really\n        matter.\n\n\n        :param node_type: the type of node to look under\n\n        :param item_type: the type of item (notation) being searched for\n\n        :return: first item_type object encountered\n        '''\n\n        if node_type == OtherNodes.DirectionNode:\n            child = self.GetChild(len(self.children) - 1)\n            while child is not None and not isinstance(\n                    child.GetItem(),\n                    item_type):\n                if child.GetItem().__class__.__name__ == item_type.__name__:\n                    return True\n                child = child.GetChild(0)\n        if node_type == OtherNodes.ExpressionNode:\n            child = self.GetChild(len(self.children) - 2)\n            while child is not None and not isinstance(\n                    child.GetItem(),\n                    item_type):\n                if child.GetItem().__class__.__name__ == item_type.__name__:\n                    return True\n                child = child.GetChild(0)", "response": "method for finding specific types of notation from nodes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef UpdateArpeggiates(self, type=\"start\"):\n        '''\n        method which searches for all arpeggiates and updates the top one of each chord to be a start,\n        and the bottom one to be a stop ready for lilypond output\n        :param type:\n        :return:\n        '''\n        result = self.item.Search(Arpeggiate)\n        if result is not None:\n            if type == \"start\":\n                result.type = type\n            child = self.GetChild(0)\n            if child is not None:\n                if child.item.Search(Arpeggiate) is None:\n                    new_obj = copy.deepcopy(result)\n                    new_obj.type = \"none\"\n                    child.GetItem().addNotation(new_obj)\n                if child is not None and hasattr(child, \"UpdateArpeggiates\"):\n                    child.UpdateArpeggiates(type=\"stop\")\n            else:\n                result.type = type\n        else:\n            result = self.item.Search(NonArpeggiate)\n            if result is not None:\n                if type == \"start\":\n                    result.type = type\n                child = self.GetChild(0)\n                if child is not None:\n                    search = child.item.Search(NonArpeggiate)\n                    if search is None:\n                        cpy = copy.deepcopy(result)\n                        cpy.type = \"none\"\n                        child.item.addNotation(cpy)\n                    if hasattr(child, \"UpdateArpeggiates\"):\n                        child.UpdateArpeggiates(type=\"bottom\")\n                else:\n                    result.type = type", "response": "method which searches for all arpeggiates and updates the top one of each chord to be a start ready for lilypond output\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toLily(self):\n        '''\n        Method which converts the object instance, its attributes and children to a string of lilypond code\n\n        :return: str of lilypond code\n        '''\n        lilystring = \"\"\n        if self.item is not None:\n            if not isinstance(self.GetChild(0), NoteNode):\n                if hasattr(self.item, \"chord\") and self.item.chord:\n                    self.item.chord = \"stop\"\n            if isinstance(self.GetChild(0), NoteNode):\n                if not hasattr(self.item, \"chord\") or not self.item.chord:\n                    self.item.chord = \"start\"\n            lilystring += self.item.toLily()\n        children = self.GetChildrenIndexes()\n        written = False\n        for child in children:\n            if self.GetChild(child) is not None:\n                if isinstance(self.GetChild(child), NoteNode):\n                    lilystring += \" \"\n                return_val = self.GetChild(child).toLily()\n                if isinstance(return_val, str):\n                    lilystring += return_val\n                else:\n                    lilystring = return_val[0] + lilystring + return_val[1]\n                if isinstance(child, OtherNodes.ExpressionNode):\n                    written = True\n                    lilystring += self.item.GetClosingNotationLilies()\n\n        if len(children) == 0 or not written:\n            lilystring += self.item.GetClosingNotationLilies()\n        return lilystring", "response": "Method which converts the object instance its attributes and children to a string of lilypond code."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncounts how many lines in a pure text file.", "response": "def count_lines(abspath):\n    \"\"\"Count how many lines in a pure text file.\n    \"\"\"\n    with open(abspath, \"rb\") as f:\n        i = 0\n        for line in f:\n            i += 1\n            pass\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlining count of selected files under a directory.", "response": "def lines_stats(dir_path, file_filter):\n    \"\"\"Lines count of selected files under a directory.\n\n    :return n_files: number of files\n    :return n_lines: number of lines\n    \"\"\"\n    n_files = 0\n    n_lines = 0\n    for p in Path(dir_path).select_file(file_filter):\n        n_files += 1\n        n_lines += count_lines(p.abspath)\n    return n_files, n_lines"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing section to formal format ecords. For help access. For parse instance. For help access.", "response": "def parse_content(self, text):\r\n        \"\"\"parse section to formal format\r\n\r\n        raw_content: {title: section(with title)}. For `help` access.\r\n\r\n        formal_content: {title: section} but the section has been dedented\r\n        without title. For parse instance\"\"\"\r\n        raw_content = self.raw_content\r\n        raw_content.clear()\r\n        formal_collect = {}\r\n\r\n        with warnings.catch_warnings():\r\n            warnings.simplefilter(\"ignore\")\r\n            try:\r\n                split = self.visible_empty_line_re.split(text)\r\n            except ValueError:  # python >= 3.5\r\n                split = [text]\r\n\r\n        option_split_re = self.option_split_re\r\n        name = re.compile(re.escape(self.option_name), re.IGNORECASE)\r\n        for text in filter(lambda x: x and x.strip(), split):\r\n\r\n            # logger.warning('get options group:\\n%r', text)\r\n            with warnings.catch_warnings():\r\n                warnings.simplefilter(\"ignore\")\r\n                try:\r\n                    split_options = option_split_re.split(text)\r\n                except ValueError:  # python >= 3.5\r\n                    continue\r\n\r\n            split_options.pop(0)\r\n\r\n            for title, section in zip(split_options[::2], split_options[1::2]):\r\n                prefix, end = name.split(title)\r\n\r\n                prefix = prefix.strip()\r\n\r\n                section = section.rstrip()\r\n                if end.endswith('\\n'):\r\n                    formal = section\r\n                else:\r\n                    formal = ' ' * len(title) + section\r\n\r\n                formal_collect.setdefault(prefix, []).append(formal)\r\n\r\n                # logger.error((title, section))\r\n                if prefix in raw_content:\r\n                    # TODO: better handling way?\r\n                    if self.namedoptions:\r\n                        log = logger.warning\r\n                    else:\r\n                        log = logger.debug\r\n                    log('duplicated options section %s', prefix)\r\n\r\n                    raw_content[prefix] += '\\n%s%s' % (title, section)\r\n                else:\r\n                    raw_content[prefix] = title + section\r\n\r\n        if formal_collect:\r\n            for each_title, values in formal_collect.items():\r\n                value = '\\n'.join(map(textwrap.dedent, values))\r\n                formal_collect[each_title] = value\r\n\r\n        self.formal_content = formal_collect"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing for `parse_content` {title: [('-a, --all=STH', 'default'), ...]}", "response": "def parse_names_and_default(self):\r\n        \"\"\"parse for `parse_content`\r\n        {title: [('-a, --all=STH', 'default'), ...]}\"\"\"\r\n        result = {}\r\n        for title, text in self.formal_content.items():\r\n\r\n            if not text:\r\n                result[title] = []\r\n                continue\r\n\r\n            logger.debug('\\n' + text)\r\n\r\n            collect = []\r\n            to_list = text.splitlines()\r\n\r\n            # parse first line. Should NEVER failed.\r\n            # this will ensure in `[default: xxx]`,\r\n            # the `xxx`(e.g: `\\t`, `,`) will not be changed by _format_line\r\n            previous_line = to_list.pop(0)\r\n            collect.append(self.parse_line_option_indent(previous_line))\r\n\r\n            for line in to_list:\r\n                indent_match = self.indent_re.match(line)\r\n                this_indent = len(indent_match.groupdict()['indent'])\r\n\r\n                if this_indent >= collect[-1]['indent']:\r\n                    # A multi line description\r\n                    previous_line = line\r\n                    continue\r\n\r\n                # new option line\r\n                # deal the default for previous option\r\n                collect[-1]['default'] = self.parse_default(previous_line)\r\n                # deal this option\r\n                collect.append(self.parse_line_option_indent(line))\r\n                logger.debug(collect[-1])\r\n                previous_line = line\r\n            else:\r\n                collect[-1]['default'] = self.parse_default(previous_line)\r\n\r\n            result[title] = [\r\n                (each['option'], each['default']) for each in collect]\r\n\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the options from a dictionary of title - > name - > default - > instance", "response": "def parse_to_instance(self, title_of_name_and_default):\r\n        \"\"\"{title: [Option(), ...]}\"\"\"\r\n        result = {}\r\n        for title, name_and_default in title_of_name_and_default.items():\r\n            logger.debug((title, name_and_default))\r\n            result[title] = opts = []\r\n            for opt_str, default in name_and_default:\r\n                logger.debug('%s:%r' % (opt_str, default))\r\n                opt, repeat = self.parse_opt_str(opt_str)\r\n                opt.default = default\r\n                opt_ins = Optional(opt, repeat=repeat)\r\n                for name in opt.names:\r\n                    self.name_2_instance[name] = opt_ins\r\n                opts.append(opt_ins)\r\n\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_option_name_2_instance(self, options):\r\n        title_opt_2_ins = self.titled_opt_to_ins\r\n        title_opt_2_ins.clear()\r\n        for title, opts in options.items():\r\n            title_opt_2_ins[title] = opt_2_ins = {}\r\n            for each in opts:\r\n                opt_ins = each[0]  # get Option inside Optional/Required\r\n                for name in opt_ins.names:\r\n                    opt_2_ins[name] = each", "response": "Set the option name to instance of the last option."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget Usage section and set to raw_content self. formal_content self. title and empty - line version", "response": "def parse_content(self, text):\r\n        \"\"\"get Usage section and set to `raw_content`, `formal_content` of no\r\n        title and empty-line version\"\"\"\r\n        match = re.search(\r\n            self.usage_re_str.format(self.usage_name),\r\n            text,\r\n            flags=(re.DOTALL\r\n                   if self.case_sensitive\r\n                   else (re.DOTALL | re.IGNORECASE)))\r\n\r\n        if match is None:\r\n            return\r\n\r\n        dic = match.groupdict()\r\n        logger.debug(dic)\r\n        self.raw_content = dic['raw']\r\n        if dic['sep'] in ('\\n', '\\r\\n'):\r\n            self.formal_content = dic['section']\r\n            return\r\n\r\n        reallen = len(dic['name'])\r\n        replace = ''.ljust(reallen)\r\n        drop_name = match.expand('%s\\\\g<sep>\\\\g<section>' % replace)\r\n        self.formal_content = self.drop_started_empty_lines(drop_name).rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spaceless_pdf_plot_maker(array, filename, vmax=None, dpi=DEFAULT_DPI):\n\n    if vmax is None:\n        vmax = np.percentile(array, DEFAULT_SATURATION_THRESHOLD)\n    plt.gca().set_axis_off()\n    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n    plt.margins(0, 0)\n    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n    plt.figure()\n    if SEABORN:\n        sns.heatmap(array, vmax=vmax, cmap=\"Reds\")\n    else:\n        plt.imshow(array, vmax=vmax, cmap=\"Reds\", interpolation=\"none\")\n        plt.colorbar()\n    plt.savefig(filename, bbox_inches=\"tight\", pad_inches=0.0, dpi=dpi)\n    plt.close()", "response": "This function is a function that creates a pretty plot from an array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_sparse_matrix(\n    array_filename,\n    output_image,\n    vmax=DEFAULT_SATURATION_THRESHOLD,\n    max_size_matrix=DEFAULT_MAX_SIZE_MATRIX,\n):\n    \"\"\"Draw a quick preview of a sparse matrix with automated\n    binning and normalization.\n    \"\"\"\n\n    matrix = np.loadtxt(array_filename, dtype=np.int32, skiprows=1)\n    try:\n        row, col, data = matrix.T\n    except ValueError:\n        row, col, data = matrix\n    size = max(np.amax(row), np.amax(col)) + 1\n    S = sparse.coo_matrix((data, (row, col)), shape=(size, size))\n    if max_size_matrix <= 0:\n        binning = 1\n    else:\n        binning = (size // max_size_matrix) + 1\n    binned_S = hcs.bin_sparse(S, subsampling_factor=binning)\n    dense_S = binned_S.todense()\n    dense_S = dense_S + dense_S.T - np.diag(np.diag(dense_S))\n    normed_S = hcs.normalize_dense(dense_S)\n    spaceless_pdf_plot_maker(normed_S, output_image, vmax=vmax)", "response": "Draw a quick preview of a sparse matrix with automated binning and normalization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nflattening arbitrary depth of nesting. Good for unknown nesting structure iterable object.", "response": "def flatten_all(nested_iterable):\n    \"\"\"Flatten arbitrary depth of nesting. Good for unknown nesting structure\n    iterable object.\n\n    Example::\n\n        >>> list(flatten_all([[1, 2], \"abc\", [3, [\"x\", \"y\", \"z\"]], 4]))\n        [1, 2, \"abc\", 3, \"x\", \"y\", \"z\", 4]\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u5c06\u4efb\u610f\u7ef4\u5ea6\u7684\u5217\u8868\u538b\u5e73\u6210\u4e00\u7ef4\u5217\u8868\u3002\n\n    \u6ce8: \u4f7f\u7528hasattr(i, \"__iter__\")\u65b9\u6cd5\u505a\u662f\u5426\u662f\u53ef\u5faa\u73af\u5bf9\u8c61\u7684\u5224\u65ad, \u6027\u80fd\u8981\u9ad8\u4e8e\u5176\u4ed6\n    \u4efb\u4f55\u65b9\u6cd5, \u4f8b\u5982: isinstance(i, collections.Iterable)\n    \"\"\"\n    for item in nested_iterable:\n        if hasattr(item, \"__iter__\") and not isinstance(item, string_types):\n            for i in flatten_all(item):\n                yield i\n        else:\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nth(iterable, n, default=None):\n    return next(itertools.islice(iterable, n, None), default)", "response": "Returns the nth item or a default value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns last n items of the iterable as a list.", "response": "def pull(iterable, n):\n    \"\"\"Return last n items of the iterable as a list.\n\n    Example::\n\n        >>> pull([0, 1, 2], 3)\n        [1, 2]\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u53d6\u51fa\u53ef\u5faa\u73af\u5bf9\u8c61\u4e2d\u7684\u6700\u540en\u4e2a\u5143\u7d20\u3002\u7b49\u6548\u4e8elist(iterable)[-n:], \u4f46\u5360\u7528\u6781\u5c0f\u7684\u5185\u5b58\u3002\n    \u56e0\u4e3alist(iterable)\u8981\u5c06\u6240\u6709\u5143\u7d20\u653e\u5728\u5185\u5b58\u4e2d\u5e76\u751f\u6210\u4e00\u4e2a\u65b0\u5217\u8868\u3002\u8be5\u65b9\u6cd5\u5e38\u7528\u8bed\u5bf9\u4e8e\n    \u90a3\u4e9b\u53d6index\u64cd\u4f5c\u88ab\u6539\u5199\u4e86\u7684\u53ef\u5faa\u73af\u5bf9\u8c61\u3002\n    \"\"\"\n    fifo = collections.deque(maxlen=n)\n    for i in iterable:\n        fifo.append(i)\n    return list(fifo)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate n - size running window.", "response": "def running_window(iterable, size):\n    \"\"\"Generate n-size running window.\n\n    Example::\n\n        >>> for i in running_windows([1, 2, 3, 4, 5], size=3):\n        ...     print(i)\n        [1, 2, 3]\n        [2, 3, 4]\n        [3, 4, 5]\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u7b80\u5355\u6ed1\u7a97\u51fd\u6570\u3002\n    \"\"\"\n    if size > len(iterable):\n        raise ValueError(\"size can not be greater than length of iterable.\")\n\n    fifo = collections.deque(maxlen=size)\n    for i in iterable:\n        fifo.append(i)\n        if len(fifo) == size:\n            yield list(fifo)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cycle_running_window(iterable, size):\n    if size > len(iterable):\n        raise ValueError(\"size can not be greater than length of iterable.\")\n\n    fifo = collections.deque(maxlen=size)\n    cycle = itertools.cycle(iterable)\n    counter = itertools.count(1)\n    length = len(iterable)\n    for i in cycle:\n        fifo.append(i)\n        if len(fifo) == size:\n            yield list(fifo)\n            if next(counter) == length:\n                break", "response": "Generate n - size cycle running window."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a list return a right hand cycle direction slice from start to end.", "response": "def cycle_slice(sliceable, start, end):\n    \"\"\"Given a list, return right hand cycle direction slice from start to end.\n\n    Example::\n\n        >>> array = [0, 1, 2, 3]\n        >>> cycle_slice(array, 1, 3) # from array[1] to array[3]\n        [1, 2]\n\n        >>> cycle_slice(array, 3, 1) # from array[3] to array[1]\n        [3, 0]\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \"\"\"\n    if type(sliceable) != list:\n        sliceable = list(sliceable)\n    length = len(sliceable)\n\n    if length == 0:\n        raise ValueError(\"sliceable cannot be empty!\")\n    start = start % length\n    end = end % length\n\n    if end > start:\n        return sliceable[start:end]\n    elif end <= start:\n        return sliceable[start:] + sliceable[:end]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cycle_dist(x, y, perimeter):\n    dist = abs(x - y) % perimeter\n    if dist > 0.5 * perimeter:\n        dist = perimeter - dist\n    return dist", "response": "Find Distance between x y by means of a n - length cycle."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shift_and_trim(array, dist):\n    length = len(array)\n    if length == 0:\n        return []\n\n    if (dist >= length) or (dist <= -length):\n        return []\n    elif dist < 0:\n        return array[-dist:]\n    elif dist > 0:\n        return array[:-dist]\n    else:\n        return list(array)", "response": "Shift and trim unneeded item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shift_and_pad(array, dist, pad=\"__null__\"):\n    length = len(array)\n    if length == 0:\n        return []\n\n    if pad == \"__null__\":\n        if dist > 0:\n            padding_item = array[0]\n        elif dist < 0:\n            padding_item = array[-1]\n        else:\n            padding_item = None\n    else:\n        padding_item = pad\n\n    if abs(dist) >= length:\n        return length * [padding_item, ]\n    elif dist == 0:\n        return list(array)\n    elif dist > 0:\n        return [padding_item, ] * dist + array[:-dist]\n    elif dist < 0:\n        return array[-dist:] + [padding_item, ] * -dist\n    else:  # Never get in this logic\n        raise Exception", "response": "Shift and pad with item."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the size of a generator function.", "response": "def size_of_generator(generator, memory_efficient=True):\n    \"\"\"Get number of items in a generator function.\n\n    - memory_efficient = True, 3 times slower, but memory_efficient.\n    - memory_efficient = False, faster, but cost more memory.\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u8ba1\u7b97\u4e00\u4e2a\u751f\u6210\u5668\u51fd\u6570\u4e2d\u7684\u5143\u7d20\u7684\u4e2a\u6570\u3002\u4f7f\u7528memory_efficient=True\u7684\u65b9\u6cd5\u53ef\u4ee5\u907f\u514d\u5c06\u751f\u6210\u5668\u4e2d\u7684\n    \u6240\u6709\u5143\u7d20\u653e\u5165\u5185\u5b58, \u4f46\u662f\u901f\u5ea6\u7a0d\u6162\u4e8ememory_efficient=False\u7684\u65b9\u6cd5\u3002\n    \"\"\"\n    if memory_efficient:\n        counter = 0\n        for _ in generator:\n            counter += 1\n        return counter\n    else:\n        return len(list(generator))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the difference between two lists of n - k elements.", "response": "def difference(array, k=1):\n    \"\"\"Calculate l[n] - l[n-k]\n    \"\"\"\n    if (len(array) - k) < 1:\n        raise ValueError()\n    if k < 0:\n        raise ValueError(\"k has to be greater or equal than zero!\")\n    elif k == 0:\n        return [i - i for i in array]\n    else:\n        return [j - i for i, j in zip(array[:-k], array[k:])]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self, value):\n        errors = []\n        self._used_validator = []\n        for val in self._validators:\n            try:\n                val.validate(value)\n                self._used_validator.append(val)\n            except ValidatorException as e:\n                errors.append(e)\n            except Exception as e:\n                errors.append(ValidatorException(\"Unknown Error\", e))\n        if len(errors) > 0:\n            raise ValidatorException.from_list(errors)\n        return value", "response": "validate function form OrValidator\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the total value of the bar according to its time signature", "response": "def GetTotalValue(self):\n        \"\"\"Gets the total value of the bar according to it's time signature\"\"\"\n        value = \"\"\n        if hasattr(self, \"meter\"):\n            top_value = self.meter.beats\n            bottom = self.meter.type\n            fraction = top_value / bottom\n            if fraction == 1:\n                value = \"1\"\n            else:\n                if fraction > 1:\n                    value = \"1.\"\n                if fraction < 1:\n                    if fraction >= 0.5:\n                        fraction -= 0.5\n                        value = \"2\"\n                        if fraction == 0.25:\n                            value += \".\"\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetLastKey(self, voice=1):\n\n        voice_obj = self.GetChild(voice)\n        if voice_obj is not None:\n            key = BackwardSearch(KeyNode, voice_obj, 1)\n            if key is not None:\n                return key\n            else:\n                if hasattr(self, \"key\"):\n                    return self.key\n        else:\n            if hasattr(self, \"key\"):\n                return self.key", "response": "returns the last key in the tree"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addClef(self, item, voice=1):\n        '''\n        method to use when adding a clef. will either add it to the node itself or add it onto the first voice's children\n        list\n        :param item:\n        :param voice:\n        :return:\n        '''\n        if not hasattr(self, \"clef\"):\n            self.clef = item\n        else:\n            voice_obj = self.GetChild(voice)\n            node = ClefNode()\n            node.SetItem(item)\n            if voice_obj is not None:\n                voice_obj.AddChild(node)\n                self.index += 1", "response": "add a clef to the node if it doesn t exist"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Backup(self, duration=0):\n        '''\n        method to use when a backup tag is encountered in musicXML. Moves back in the bar by <duration>\n        :param duration:\n        :return:\n        '''\n        total = 0\n        duration_total = duration * 4\n        children = self.GetChildrenIndexes()\n        notes = 0\n        for voice in children:\n            v = self.GetChild(voice)\n            indexes = v.GetChildrenIndexes()\n            if len(indexes) > 1:\n                indexes.reverse()\n            for index in indexes:\n                notes += 1\n                note = v.GetChild(index)\n                if hasattr(note, \"duration\"):\n                    total += note.duration\n                    if total >= duration_total:\n                        break\n            gap = [\n                v.GetChild(i).duration for i in range(\n                    0,\n                    self.index -\n                    notes) if hasattr(\n                    v.GetChild(i),\n                    \"duration\")]\n            previous = 0\n            for item in gap:\n                if item == previous:\n                    self.gap -= previous\n                    item = item / 2\n                self.gap += item\n                previous = item\n            #self.gap = sum([])\n        self.index -= notes", "response": "method to use when a backup tag is encountered in musicXML. Moves back in the bar by <duration >."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef SplitString(value):\n    string_length = len(value)\n    chunks = int(string_length / 10)\n    string_list = list(value)\n    lstring = \"\"\n\n    if chunks > 1:\n        lstring = \"\\\\markup { \\n\\r \\column { \"\n        for i in range(int(chunks)):\n            lstring += \"\\n\\r\\r \\\\line { \\\"\"\n            index = i * 10\n            for i in range(index):\n                lstring += string_list[i]\n            lstring += \"\\\" \\r\\r}\"\n        lstring += \"\\n\\r } \\n }\"\n    if lstring == \"\":\n        indexes = [\n            i for i in range(\n                len(string_list)) if string_list[i] == \"\\r\" or string_list[i] == \"\\n\"]\n        lstring = \"\\\\markup { \\n\\r \\column { \"\n        if len(indexes) == 0:\n            lstring += \"\\n\\r\\r \\\\line { \\\"\" + \\\n                \"\".join(string_list) + \"\\\" \\n\\r\\r } \\n\\r } \\n }\"\n        else:\n            rows = []\n            row_1 = string_list[:indexes[0]]\n            rows.append(row_1)\n            for i in range(len(indexes)):\n                start = indexes[i]\n                if i != len(indexes) - 1:\n                    end = indexes[i + 1]\n                else:\n                    end = len(string_list)\n                row = string_list[start:end]\n                rows.append(row)\n\n            for row in rows:\n                lstring += \"\\n\\r\\r \\\\line { \\\"\"\n                lstring += \"\".join(row)\n                lstring += \"\\\" \\r\\r}\"\n            lstring += \"\\n\\r } \\n }\"\n    return lstring", "response": "simple method that puts in spaces every 10 characters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetID(attrs, tag, val):\n\n    if tag in attrs:\n        if val in attrs[tag]:\n            return attrs[tag][val]", "response": "getID - Gets the id of a nested element"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CheckTotals(self):\n        staves = self.GetChildrenIndexes()\n        for staff in staves:\n            child = self.getStaff(staff)\n            child.CheckTotals()", "response": "method to calculate the maximum total lilypond value for a measure without a time signature"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef docpie(doc, argv=None, help=True, version=None,\n           stdopt=True, attachopt=True, attachvalue=True,\n           helpstyle='python',\n           auto2dashes=True, name=None, case_sensitive=False,\n           optionsfirst=False, appearedonly=False, namedoptions=False,\n           extra=None):\n    \"\"\"\n    Parse `argv` based on command-line interface described in `doc`.\n\n    `docpie` creates your command-line interface based on its\n    description that you pass as `doc`. Such description can contain\n    --options, <positional-argument>, commands, which could be\n    [optional], (required), (mutually | exclusive) or repeated...\n    Parameters\n    ----------\n    doc : str\n        Description of your command-line interface.\n    argv : list of str, optional\n        Argument vector to be parsed. sys.argv is used if not\n        provided.\n    help : bool (default: True)\n        Set to False to disable automatic help on -h or --help\n        options.\n    version : any object but None\n        If passed, the object will be printed if --version is in\n        `argv`.\n    stdopt : bool (default: True)\n        When it's True, long flag should only starts with --\n    attachopt: bool (default: True)\n        write/pass several short flag into one, e.g. -abc can mean -a -b -c.\n        This only works when stdopt=True\n    attachvalue: bool (default: True)\n        allow you to write short flag and its value together,\n        e.g. -abc can mean -a bc\n    auto2dashes: bool (default: True)\n        automaticly handle -- (which means \"end of command line flag\")\n    name: str (default: None)\n        the \"name\" of your program. In each of your \"usage\" the \"name\" will be\n        ignored. By default docpie will ignore the first element of your\n        \"usage\".\n    case_sensitive: bool (deprecated / default: False)\n        specifies if it need case sensitive when matching\n        \"Usage:\" and \"Options:\"\n    optionsfirst: bool (default: False)\n        everything after first positional argument will be interpreted as\n        positional argument\n    appearedonly: bool (default: False)\n        when set True, the options that never appear in argv will not\n        be put in result. Note this only affect options\n    extra: dict\n        customize pre-handled options. See\n        http://docpie.comes.today/document/advanced-apis/\n        for more infomation.\n    Returns\n    -------\n    args : dict\n        A dictionary, where keys are names of command-line elements\n        such as e.g. \"--verbose\" and \"<path>\", and values are the\n        parsed values of those elements.\n    Example\n    -------\n    >>> from docpie import docpie\n    >>> doc = '''\n    ... Usage:\n    ...     my_program tcp <host> <port> [--timeout=<seconds>]\n    ...     my_program serial <port> [--baud=<n>] [--timeout=<seconds>]\n    ...     my_program (-h | --help | --version)\n    ...\n    ... Options:\n    ...     -h, --help  Show this screen and exit.\n    ...     --baud=<n>  Baudrate [default: 9600]\n    ... '''\n    >>> argv = ['my_program', 'tcp', '127.0.0.1', '80', '--timeout', '30']\n    >>> docpie(doc, argv)\n    {\n     '--': False,\n     '-h': False,\n     '--baud': '9600',\n     '--help': False,\n     '--timeout': '30',\n     '--version': False,\n     '<host>': '127.0.0.1',\n     '<port>': '80',\n     'serial': False,\n     'tcp': True}\n    See also\n    --------\n    * Full documentation is available in README.md as well as online\n      at http://docpie.comes.today/document/quick-start/\n    \"\"\"\n\n    if case_sensitive:\n        warnings.warn('`case_sensitive` is deprecated, `docpie` is always '\n                      'case insensitive')\n\n    kwargs = locals()\n    argv = kwargs.pop('argv')\n    pie = Docpie(**kwargs)\n    pie.docpie(argv)\n    return pie", "response": "Parse a doc string into a sequence of command - line interface objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __read(self, i: int) -> bytes:\n        b = self.data[self.idx: self.idx + i]\n        self.idx += i\n        if len(b) != i:\n            raise bencodepy.DecodingError(\n                \"Incorrect byte length returned between indexes of {0} and {1}. Possible unexpected End of File.\"\n                    .format(str(self.idx), str(self.idx - i)))\n        return b", "response": "Returns a set number ( i ) of bytes from self. data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading bytes from self. data starting at index self. idx until terminator character.", "response": "def __read_to(self, terminator: bytes) -> bytes:\n        \"\"\"Returns bytes from self.data starting at index (self.idx) until terminator character.\"\"\"\n        try:\n            # noinspection PyTypeChecker\n            i = self.data.index(terminator, self.idx)\n            b = self.data[self.idx:i]\n            self.idx = i + 1\n            return b\n        except ValueError:\n            raise bencodepy.DecodingError(\n                'Unable to locate terminator character \"{0}\" after index {1}.'.format(str(terminator), str(self.idx)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __parse(self) -> object:\n        char = self.data[self.idx: self.idx + 1]\n        if char in [b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', b'9', b'0']:\n            str_len = int(self.__read_to(b':'))\n            return self.__read(str_len)\n        elif char == b'i':\n            self.idx += 1\n            return int(self.__read_to(b'e'))\n        elif char == b'd':\n            return self.__parse_dict()\n        elif char == b'l':\n            return self.__parse_list()\n        elif char == b'':\n            raise bencodepy.DecodingError('Unexpected End of File at index position of {0}.'.format(str(self.idx)))\n        else:\n            raise bencodepy.DecodingError(\n                'Invalid token character ({0}) at position {1}.'.format(str(char), str(self.idx)))", "response": "Selects the appropriate method to decode next bencode element and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode(self) -> Iterable:\n        if self.data[0:1] not in (b'd', b'l'):\n            return self.__wrap_with_tuple()\n        return self.__parse()", "response": "Start of decode process. Returns final results."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __wrap_with_tuple(self) -> tuple:\n        l = list()\n        length = len(self.data)\n        while self.idx < length:\n            l.append(self.__parse())\n        return tuple(l)", "response": "Returns a tuple of all nested bencode elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __parse_dict(self) -> OrderedDict:\n        self.idx += 1\n        d = OrderedDict()\n        key_name = None\n        while self.data[self.idx: self.idx + 1] != b'e':\n            if key_name is None:\n                key_name = self.__parse()\n            else:\n                d[key_name] = self.__parse()\n                key_name = None\n        self.idx += 1\n        return d", "response": "Returns an OrderedDict of nested bencode elements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __parse_list(self) -> list:\n        self.idx += 1\n        l = []\n        while self.data[self.idx: self.idx + 1] != b'e':\n            l.append(self.__parse())\n        self.idx += 1\n        return l", "response": "Returns an array of nested bencode elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef toLily(self):\n        '''\n        Method which converts the object instance, its attributes and children to a string of lilypond code\n\n        :return: str of lilypond code\n        '''\n\n        lilystring = \"\"\n        children = self.GetChildrenIndexes()\n        total = self.note_total\n        counter = 0\n        for child in range(len(children)):\n            note = self.GetChild(children[child])\n            item = note.GetItem()\n            if item is not None:\n                item.autoBeam = self.autoBeam\n            if hasattr(note, \"duration\"):\n                try:\n                    counter += int(note.duration)\n                except:\n                    if note.duration == \"\\\\longa\":\n                        counter += 0.25\n                    if note.duration == \"\\\\breve\":\n                        counter += 0.5\n            if counter > total / 2:\n                if hasattr(self, \"mid_barline\"):\n                    lilystring += self.mid_barline.toLily()\n                    self.__delattr__(\"mid_barline\")\n            if hasattr(self, \"rest\") and hasattr(self, \"total\"):\n                lilystring += \"R\" + self.total\n            else:\n                lilystring += note.toLily() + \" \"\n        return lilystring", "response": "Method which converts the object instance its attributes and children to a string of lilypond code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef BackwardSearch(cls_type, node, index, depth=0, start_index=0):\n    '''\n    Helper method which backwards-recursively searches for objects\n    :param cls_type: class type of the object we are in search of\n    :param node: object instance to start at\n    :param index: number of the object to look for e.g <cls_type> num 1\n    :param depth: current depth in the tree\n    :param start_index: index to start with in children\n    :return: object <index> of <cls_type>\n    '''\n    counter = depth\n    if isinstance(node, cls_type):\n        counter += 1\n        if counter == index:\n            return node\n    if node is None:\n        return None\n    else:\n        children = node.GetChildrenIndexes()\n        if len(children) == 0 and isinstance(node, cls_type):\n            return counter\n        else:\n            children.reverse()\n            for child in children:\n                result = Search(\n                    cls_type,\n                    node.GetChild(child),\n                    index,\n                    depth=counter)\n                if isinstance(result, int):\n                    counter = result\n                    if counter == index:\n                        return node.GetChild(child)\n                if isinstance(result, cls_type):\n                    return result\n            if isinstance(node, cls_type):\n                if counter == index:\n                    return node\n                else:\n                    return counter", "response": "This method is used to search backwards - recursively for objects of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef PopAllChildren(self):\n        '''\n        Method to remove and return all children of current node\n\n        :return: list of children\n        '''\n        indexes = self.GetChildrenIndexes()\n        children = []\n        for c in indexes:\n            child = self.PopChild(c)\n            children.append(child)\n        return children", "response": "Method to remove and return all children of current node\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nraising an exception on a bad request response", "response": "def handle_errors(resp):\n    \"\"\"raise a descriptive exception on a \"bad request\" response\"\"\"\n    if resp.status_code == 400:\n        raise ApiException(json.loads(resp.content).get('message'))\n    return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_file(input_file, output_file, apikey):\n    bytes_ = read_binary(input_file)\n    compressed = shrink(bytes_, apikey)\n\n    if compressed.success and compressed.bytes:\n        write_binary(output_file, compressed.bytes)\n    else:\n        if compressed.errno in FATAL_ERRORS:\n            raise StopProcessing(compressed)\n        elif compressed.errno == TinyPNGError.InternalServerError:\n            raise RetryProcessing(compressed)\n\n    return compressed", "response": "This function takes input_file tries to shrink it and saves compressed image to output_file if it fails raise exception."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_directory(source, target, apikey, handler, overwrite=False):\n\n    handler.on_start()\n\n    attempts = defaultdict(lambda: 0)\n    input_files = files_with_exts(source, suffix='.png')\n    next_ = lambda: next(input_files, None)\n\n    current_file = next_()\n    response = None\n    last_processed = None\n\n    while current_file:\n        output_file = target_path(source, target, current_file)\n\n        if os.path.exists(output_file) and not overwrite:\n            handler.on_skip(current_file, source=source)\n            current_file = next_()\n            continue\n\n        try:\n            handler.on_pre_item(current_file)\n\n            last_processed = current_file\n\n            response = _process_file(current_file, output_file, apikey)\n            current_file = next_()\n        except StopProcessing as e:\n            # Unauthorized or exceed number of allowed monthly calls\n            response = e.response\n            handler.on_stop(response.errmsg)\n            break\n        except RetryProcessing as e:\n            # handle InternalServerError on tinypng side\n            response = e.response\n            if attempts[current_file] < 9:\n                handler.on_retry(current_file)\n                time.sleep(TINYPNG_SLEEP_SEC)\n                attempts[current_file] += 1\n            else:\n                current_file = next_()\n        finally:\n            handler.on_post_item(response, input_file=last_processed, source=source)\n\n    handler.on_finish(output_dir=target)", "response": "Optimize and save png files form source to target directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _main(args):\n\n    if not args.apikey:\n        print(\"\\nPlease provide TinyPNG API key\")\n        print(\"To obtain key visit https://api.tinypng.com/developers\\n\")\n        sys.exit(1)\n\n    input_dir = realpath(args.input)\n\n    if not args.output:\n        output_dir = input_dir + \"-output\"\n    else:\n        output_dir = realpath(args.output)\n\n    if input_dir == output_dir:\n        print(\"\\nPlease specify different output directory\\n\")\n        sys.exit(1)\n\n    handler = ScreenHandler()\n\n    try:\n        process_directory(input_dir, output_dir, args.apikey, handler)\n    except KeyboardInterrupt:\n        handler.on_finish(output_dir=output_dir)", "response": "Main function for batch compression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an ENVI Engine Task object. See ENVI Engine Task for examples.", "response": "def task(self, task_name):\n        \"\"\"\n        Returns an ENVI Py Engine Task object. See ENVI Py Engine Task for examples.\n\n        :param task_name: The name of the task to retrieve.\n        :return: An ENVI Py Engine Task object.\n        \"\"\"\n        return Task(uri=':'.join((self._engine_name, task_name)), cwd=self._cwd)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of all tasks known to the engine.", "response": "def tasks(self):\n        \"\"\"\n        Returns a list of all tasks known to the engine.\n\n        :return: A list of task names.\n        \"\"\"\n        task_input = {'taskName': 'QueryTaskCatalog'}\n        output = taskengine.execute(task_input, self._engine_name, cwd=self._cwd)\n        return output['outputParameters']['TASKS']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a query returning its result", "response": "def execute(query, auth=None, client=urllib_request.build_opener()):\n    \"\"\"Execute a query, returning its result\n\n    Parameters\n    ----------\n    query: Query[T]\n        The query to resolve\n    auth: ~typing.Tuple[str, str] \\\n        or ~typing.Callable[[Request], Request] or None\n        This may be:\n\n        * A (username, password)-tuple for basic authentication\n        * A callable to authenticate requests.\n        * ``None`` (no authentication)\n    client\n        The HTTP client to use.\n        Its type must have been registered\n        with :func:`~snug.clients.send`.\n        If not given, the built-in :mod:`urllib` module is used.\n\n    Returns\n    -------\n    T\n        the query result\n    \"\"\"\n    exec_fn = getattr(type(query), '__execute__', _default_execute_method)\n    return exec_fn(query, client, _make_auth(auth))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute_async(query, auth=None, client=event_loop):\n    exc_fn = getattr(type(query), '__execute_async__', Query.__execute_async__)\n    return exc_fn(query, client, _make_auth(auth))", "response": "Execute a query asynchronously returning its result\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread random bytes from the system s random entropy pool.", "response": "def dev_random_entropy(numbytes, fallback_to_urandom=True):\n    \"\"\" Reads random bytes from the /dev/random entropy pool.\n\n        NOTE: /dev/random is a blocking pseudorandom number generator.\n        If the entropy pool runs out, this function will block until more\n        environmental noise is gathered.\n        If entropy re-use is unnacceptable use this over \"dev_urandom_entropy\".\n\n        If \"fallback_to_urandom\" is set, this function will fallback to\n        /dev/urandom on operating systems without /dev/random.\n    \"\"\"\n    if os.name == 'nt' and fallback_to_urandom:\n        return dev_urandom_entropy(numbytes)\n    return open(\"/dev/random\", \"rb\").read(numbytes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a random integer N such that a < = N < = max_value.", "response": "def secure_randint(min_value, max_value, system_random=None):\n    \"\"\" Return a random integer N such that a <= N <= b.\n\n        Uses SystemRandom for generating random numbers.\n        (which uses os.urandom(), which pulls from /dev/urandom)\n    \"\"\"\n    if not system_random:\n        system_random = random.SystemRandom()\n    return system_random.randint(min_value, max_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_maps(m1, m2):\n    return type(m1)(chain(m1.items(), m2.items()))", "response": "merge two Mapping objects keeping the type of the first mapping"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef basic_auth(credentials):\n    encoded = b64encode(':'.join(credentials).encode('ascii')).decode()\n    return header_adder({'Authorization': 'Basic ' + encoded})", "response": "Create an HTTP basic authentication callable which returns a request object which will be used to add a basic authentication to the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new request with added headers", "response": "def with_headers(self, headers):\n        \"\"\"Create a new request with added headers\n\n        Parameters\n        ----------\n        headers: Mapping\n            the headers to add\n        \"\"\"\n        return self.replace(headers=_merge_maps(self.headers, headers))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new request with added query parameters.", "response": "def with_params(self, params):\n        \"\"\"Create a new request with added query parameters\n\n        Parameters\n        ----------\n        params: Mapping\n            the query parameters to add\n        \"\"\"\n        return self.replace(params=_merge_maps(self.params, params))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AddSlur(self, item):\n        '''\n        Very simple method which is used for adding slurs.\n        :param item:\n        :return:\n        '''\n        if not hasattr(self, \"slurs\"):\n            self.slurs = []\n        self.slurs.append(item)", "response": "Very simple method which is used for adding slurs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetNotation(self, id, type):\n        '''\n        method which searches for notation from <type> list at position <id>\n        :param id: the number to look for - i.e if you're looking for the first one in wrap notation, id will be 0\n        :param type: post, pre or wrap\n        :return: the notation class searched for or none\n        '''\n        if type == \"post\":\n            if (id == -\n                1 and len(self.postnotation) > 0) or (id != -\n                                                      1 and len(self.postnotation) > id):\n                return self.postnotation[id]\n        if type == \"pre\":\n            if (id == -\n                1 and len(self.prenotation) > 0) or (id != -\n                                                     1 and len(self.postnotation) > id):\n                return self.prenotation[id]\n        if type == \"wrap\":\n            if (id == -\n                1 and len(self.wrap_notation) > 0) or (id != -\n                                                       1 and len(self.postnotation) > id):\n                return self.wrap_notation[id]", "response": "method which searches for the notation class at the given id and type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetClosingNotationLilies(self):\n        '''\n        Converts notation in closing_notation into a lilypond string.\n        :return: str\n        '''\n        lstring = \"\"\n        for notation in self.closing_notation:\n            result = notation.toLily()\n            if type(result) == list:\n                result = \"\".join(result)\n            lstring += result\n        return lstring", "response": "Converts the closing_notation into a lilypond string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the type of the class.", "response": "def SetType(self, vtype):\n        '''\n        Sets the type, i.e duration of the note. Types are given as keys inside options\n        :param vtype: str - see keys in options for full list\n        :return: None, side effects modifying the class\n        '''\n        self.val_type = vtype\n        options = {\n            \"128th\": 128,\n            \"64th\": 64,\n            \"32nd\": 32,\n            \"16th\": 16,\n            \"eighth\": 8,\n            \"quarter\": 4,\n            \"half\": 2,\n            \"whole\": 1,\n            \"h\": 8,\n            \"long\": \"\\\\longa\",\n            \"breve\": \"\\\\breve\"}\n        if vtype in options:\n            self.duration = options[self.val_type]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine if the n - th bit of passed bytes is 1 or 0.", "response": "def _get_bit(self, n, hash_bytes):\n        \"\"\"\n        Determines if the n-th bit of passed bytes is 1 or 0.\n\n        Arguments:\n\n          hash_bytes - List of hash byte values for which the n-th bit value\n          should be checked. Each element of the list should be an integer from\n          0 to 255.\n\n        Returns:\n\n          True if the bit is 1. False if the bit is 0.\n        \"\"\"\n\n        if hash_bytes[n // 8] >> int(8 - ((n % 8) + 1)) & 1 == 1:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the matrix that describes which blocks should be coloured.", "response": "def _generate_matrix(self, hash_bytes):\n        \"\"\"\n        Generates matrix that describes which blocks should be coloured.\n\n        Arguments:\n          hash_bytes - List of hash byte values for which the identicon is being\n          generated. Each element of the list should be an integer from 0 to\n          255.\n\n        Returns:\n          List of rows, where each element in a row is boolean. True means the\n          foreground colour should be used, False means a background colour\n          should be used.\n        \"\"\"\n\n        # Since the identicon needs to be symmetric, we'll need to work on half\n        # the columns (rounded-up), and reflect where necessary.\n        half_columns = self.columns // 2 + self.columns % 2\n        cells = self.rows * half_columns\n\n        # Initialise the matrix (list of rows) that will be returned.\n        matrix = [[False] * self.columns for _ in range(self.rows)]\n\n        # Process the cells one by one.\n        for cell in range(cells):\n\n            # If the bit from hash correpsonding to this cell is 1, mark the\n            # cell as foreground one. Do not use first byte (since that one is\n            # used for determining the foreground colour.\n            if self._get_bit(cell, hash_bytes[1:]):\n\n                # Determine the cell coordinates in matrix.\n                column = cell // self.columns\n                row = cell % self.rows\n\n                # Mark the cell and its reflection. Central column may get\n                # marked twice, but we don't care.\n                matrix[row][column] = True\n                matrix[row][self.columns - column - 1] = True\n\n        return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a digest of data returning it as a list where every element is a single byte of digest.", "response": "def _data_to_digest_byte_list(self, data):\n        \"\"\"\n        Creates digest of data, returning it as a list where every element is a\n        single byte of digest (an integer between 0 and 255).\n\n        No digest will be calculated on the data if the passed data is already a\n        valid hex string representation of digest, and the passed value will be\n        used as digest in hex string format instead.\n\n        Arguments:\n\n          data - Raw data or hex string representation of existing digest for\n          which a list of one-byte digest values should be returned.\n\n        Returns:\n\n          List of integers where each element is between 0 and 255, and\n          repesents a single byte of a data digest.\n        \"\"\"\n\n        # If data seems to provide identical amount of entropy as digest, it\n        # could be a hex digest already.\n        if len(data) // 2 == self.digest_entropy // 8:\n            try:\n                binascii.unhexlify(data.encode('utf-8'))\n                digest = data.encode('utf-8')\n            # Handle Python 2.x exception.\n            except (TypeError):\n                digest = self.digest(data.encode('utf-8')).hexdigest()\n            # Handle Python 3.x exception.\n            except (binascii.Error):\n                digest = self.digest(data.encode('utf-8')).hexdigest()\n        else:\n            digest = self.digest(data.encode('utf-8')).hexdigest()\n\n        return [int(digest[i * 2:i * 2 + 2], 16) for i in range(16)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_image(self, matrix, width, height, padding, foreground, background, image_format):\n\n        # Set-up a new image object, setting the background to provided value.\n        image = Image.new(\"RGBA\", (width + padding[2] + padding[3], height + padding[0] + padding[1]), background)\n\n        # Set-up a draw image (for drawing the blocks).\n        draw = ImageDraw.Draw(image)\n\n        # Calculate the block widht and height.\n        block_width = width // self.columns\n        block_height = height // self.rows\n\n        # Go through all the elements of a matrix, and draw the rectangles.\n        for row, row_columns in enumerate(matrix):\n            for column, cell in enumerate(row_columns):\n                if cell:\n                    # Set-up the coordinates for a block.\n                    x1 = padding[2] + column * block_width\n                    y1 = padding[0] + row * block_height\n                    x2 = padding[2] + (column + 1) * block_width - 1\n                    y2 = padding[0] + (row + 1) * block_height - 1\n\n                    # Draw the rectangle.\n                    draw.rectangle((x1, y1, x2, y2), fill=foreground)\n\n        # Set-up a stream where image will be saved.\n        stream = BytesIO()\n\n        if image_format.upper() == \"JPEG\":\n            image = image.convert(mode=\"RGB\")\n\n        # Save the image to stream.\n        try:\n            image.save(stream, format=image_format, optimize=True)\n        except KeyError:\n            raise ValueError(\"Pillow does not support requested image format: %s\" % image_format)\n        image_raw = stream.getvalue()\n        stream.close()\n\n        # Return the resulting image.\n        return image_raw", "response": "Generate an image from the passed matrix width height padding foreground colour background and image format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates an ASCII image from the matrix.", "response": "def _generate_ascii(self, matrix, foreground, background):\n        \"\"\"\n        Generates an identicon \"image\" in the ASCII format. The image will just\n        output the matrix used to generate the identicon.\n\n        Arguments:\n\n          matrix - Matrix describing which blocks in the identicon should be\n          painted with foreground (background if inverted) colour.\n\n          foreground - Character which should be used for representing\n          foreground.\n\n          background - Character which should be used for representing\n          background.\n\n        Returns:\n\n          ASCII representation of an identicon image, where one block is one\n          character.\n        \"\"\"\n\n        return \"\\n\".join([\"\".join([foreground if cell else background for cell in row]) for row in matrix])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate(self, data, width, height, padding=(0, 0, 0, 0), output_format=\"png\", inverted=False):\n\n        # Calculate the digest, and get byte list.\n        digest_byte_list = self._data_to_digest_byte_list(data)\n\n        # Create the matrix describing which block should be filled-in.\n        matrix = self._generate_matrix(digest_byte_list)\n\n        # Determine the background and foreground colours.\n        if output_format == \"ascii\":\n            foreground = \"+\"\n            background = \"-\"\n        else:\n            background = self.background\n            foreground = self.foreground[digest_byte_list[0] % len(self.foreground)]\n\n        # Swtich the colours if inverted image was requested.\n        if inverted:\n            foreground, background = background, foreground\n\n        # Generate the identicon in requested format.\n        if output_format == \"ascii\":\n            return self._generate_ascii(matrix, foreground, background)\n        else:\n            return self._generate_image(matrix, width, height, padding, foreground, background, output_format)", "response": "Generates an identicon image with the given data width height padding and output_format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the local timezone to value to make it aware.", "response": "def local_timezone(value):\n    \"\"\"Add the local timezone to `value` to make it aware.\"\"\"\n    if hasattr(value, \"tzinfo\") and value.tzinfo is None:\n        return value.replace(tzinfo=dateutil.tz.tzlocal())\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loads(data, validate=False, **kwargs):\n    d = json.loads(data, **kwargs)\n    content_spec = d[\"content-spec\"]\n    Payload = CONTENT_SPECS[content_spec]\n    payload = Payload.load(d)\n    if validate:\n        errors = payload.problems()\n        if errors:\n            raise ValidationError(errors)\n    return payload", "response": "Load a PPMP message from the JSON - formatted string in data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a PPMP entity to JSON. Additional arguments are the same as accepted by json. dumps.", "response": "def dumps(data, **kwargs):\n    \"\"\"Convert a PPMP entity to JSON. Additional arguments are the same as\n    accepted by `json.dumps`.\"\"\"\n\n    def _encoder(value):\n        if isinstance(value, datetime.datetime):\n            return value.isoformat()\n\n        if hasattr(value, \"_data\"):\n            return value._data\n\n        raise TypeError('Could not encode %r' % value)\n\n    return json.dumps(data, default=_encoder, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_lilypond(path_to_lilypond_folder=\"default\"):\n    '''\n    Optional helper method which works out the platform and calls the relevant setup method\n\n    * param path_to_lilypond_folder: the path where lilypond.exe or the lilypond runner tool in mac is located. Not needed if\n    setup is default, or if using linux\n\n    * :return: None\n    '''\n    options = {\"win32\": setup_lilypond_windows, \"darwin\": setup_lilypond_osx}\n    if platform.startswith(\"linux\"):\n        setup_lilypond_linux()\n    else:\n        options[platform](path_to_lilypond_folder)", "response": "This function is used to setup the lilypond system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_lilypond_windows(path=\"default\"):\n    '''\n    Optional helper method which does the environment setup for lilypond in windows. If you've ran this method, you do not need and should not provide\n    a lyscript when you instantiate this class. As this method is static, you can run this method before you set up the LilypondRenderer\n    instance.\n\n    * parameter: path_to_lilypond is the path to the folder which contains the file \"lilypond.exe\". Usually ProgramFiles/Lilypond/usr/bin.\n    Leave at default to set to this path.\n\n    * returns: None\n    '''\n    default = \"C:/Program Files (x86)/LilyPond/usr/bin\"\n    path_variable = os.environ['PATH'].split(\";\")\n    if path == \"default\":\n        path_variable.append(default)\n    else:\n        path_variable.append(path)\n    os.environ['PATH'] = \";\".join(path_variable)", "response": "This method is used to set up the environment for lilypond in windows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a recursive dict to a plain ol' dict.", "response": "def recursive_dict_to_dict(rdict):\n    \"\"\" Convert a recursive dict to a plain ol' dict.\n    \"\"\"\n    d = {}\n    for (k, v) in rdict.items():\n        if isinstance(v, defaultdict):\n            d[k] = recursive_dict_to_dict(v)\n        else:\n            d[k] = v\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_json_type(obj, classkey=None):\n    if isinstance(obj, dict):\n        data = {}\n        for (k, v) in obj.items():\n            data[k] = _to_json_type(v, classkey)\n        return data\n    elif hasattr(obj, \"_ast\"):\n        return _to_json_type(obj._ast())\n    elif hasattr(obj, \"__iter__\"):\n        return [_to_json_type(v, classkey) for v in obj]\n    elif hasattr(obj, \"__dict__\"):\n        data = dict([\n            (key, _to_json_type(value, classkey))\n            for key, value in obj.__dict__.iteritems()\n            if not callable(value) and not key.startswith('_')\n        ])\n        if classkey is not None and hasattr(obj, \"__class__\"):\n            data[classkey] = obj.__class__.__name__\n        return data\n    else:\n        return obj", "response": "Recursively convert the object instance into a valid JSON type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert an instance of an object into a dict.", "response": "def to_dict(obj):\n    \"\"\" Convert an instance of an object into a dict.\n    \"\"\"\n    d = _to_json_type(obj)\n    if isinstance(d, dict):\n        return scrub_dict(d)\n    else:\n        raise ValueError(\"The value provided must be an object.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint normal traceback information with some local arg values", "response": "def print_exc_plus(stream=sys.stdout):\n    '''print normal traceback information with some local arg values'''\n    # code of this mothod is mainly from <Python Cookbook>\n    write = stream.write    # assert the mothod exists\n    flush = stream.flush\n    tp, value, tb = sys.exc_info()\n    while tb.tb_next:\n        tb = tb.tb_next\n    stack = list()\n    f = tb.tb_frame\n    while f:\n        stack.append(f)\n        f = f.f_back\n    stack.reverse()\n    try:\n        traceback.print_exc(None, stream)\n    except BaseException as e:\n        write(u(\"FAILED PRINTING TRACE\\n\\n\"))\n        write(u(str(value)))\n        write(u('\\n\\n'))\n    finally:\n        flush()\n\n    write(u('Locals by frame, innermost last\\n'))\n    for frame in stack:\n        write(u('\\nFrame %s in %s at line %s\\n' % (frame.f_code.co_name,\n                                                   frame.f_code.co_filename,\n                                                   frame.f_lineno)))\n    for key, value, in frame.f_locals.items():\n        write(u('\\t%20s = ' % key))\n        try:\n            write(u('%s\\n' % value))\n        except BaseException:\n            write(u('<ERROR WHILE PRINTING VALUE>\\n'))\n    flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_single_space_only(text):\n    return \" \".join([word for word in text.strip().split(\" \") if len(word) >= 1])", "response": "Revise consecutive empty space to single space."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats the title of the text.", "response": "def format_title(text):\n    \"\"\"Capitalize first letter for each words except function words.\n\n    Example::\n\n        title = \"Beautiful is Better than Ugly\"\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u5c06\u6587\u672c \"\u6807\u9898\u5316\", \u5373\u9664\u4e86\u865a\u8bcd, \u6bcf\u4e00\u4e2a\u82f1\u6587\u5355\u8bcd\u7684\u7b2c\u4e00\u4e2a\u5b57\u6bcd\u5927\u5199\u3002\n    \"\"\"\n    text = text.strip()\n    # if empty string, return \"\"\n    if len(text) == 0:\n        return text\n    else:\n        text = text.lower()  # lower all char\n\n        # Change to in single space format\n        words = [word for word in text.strip().split(\" \") if len(word) >= 1]\n\n        # Capitalize all words except function word\n        words_new = list()\n        for word in words:\n            if word not in FUNCTION_WORD:\n                word = word[0].upper() + word[1:]\n            words_new.append(word)\n\n        # Make sure first word always be capitalized\n        words_new[0] = words_new[0][0].upper() + words_new[0][1:]\n\n        return \" \".join(words_new)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_person_name(text):\n    text = text.strip()\n    if len(text) == 0:  # if empty string, return it\n        return text\n    else:\n        text = text.lower()  # lower all char\n        # delete redundant empty space\n        words = [word for word in text.strip().split(\" \") if len(word) >= 1]\n        words = [word[0].upper() + word[1:] for word in words]\n        return \" \".join(words)", "response": "Format a person name into a random name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting a string to be used in a camel case version of the language", "response": "def format_camel_case(text):\n    \"\"\"\n    Example::\n\n        ThisIsVeryGood\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u5c06\u6587\u672c\u683c\u5f0f\u5316\u4e3a\u5404\u5355\u8bcd\u9996\u5b57\u6bcd\u5927\u5199, \u62fc\u63a5\u800c\u6210\u7684\u957f\u53d8\u91cf\u540d\u3002\n    \"\"\"\n    text = text.strip()\n    if len(text) == 0:  # if empty string, return it\n        raise ValueError(\"can not be empty string!\")\n    else:\n        text = text.lower()  # lower all char\n        # delete redundant empty space\n        words = list()\n        word = list()\n        for char in text:\n            if char in ALPHA_DIGITS:\n                word.append(char)\n            else:\n                if len(word):\n                    words.append(\"\".join(word))\n                    word = list()\n        if len(word):\n            words.append(\"\".join(word))\n\n        words = [word[0].upper() + word[1:] for word in words]\n        return \"\".join(words)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat a string in Unix format.", "response": "def format_unix_var(text):\n    \"\"\"\n    Example::\n\n        this_is_very_good\n    \"\"\"\n    text = text.strip()\n    if len(text) == 0:  # if empty string, return it\n        raise ValueError(\"can not be empty string!\")\n    else:\n        if text[0] in string.digits:\n            raise ValueError(\"variable can not start with digits!\")\n        text = text.lower()\n        # delete redundant empty space\n        words = list()\n        word = list()\n        for char in text:\n            if char in ALPHA_DIGITS:\n                word.append(char)\n            else:\n                if len(word):\n                    words.append(\"\".join(word))\n                    word = list()\n        if len(word):\n            words.append(\"\".join(word))\n\n        return \"_\".join(words)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef daemon(self):\n        if self._process:\n            return self._process.daemon\n        else:\n            return self._pargs.get('daemonic', False)", "response": "Return whether the process is a daemon or not"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the daemonic flag.", "response": "def daemon(self, daemonic):\n        \"\"\"\n        Set whether process is a daemon\n        :param daemonic:\n        :return:\n        \"\"\"\n        if self._process:\n            self._process.daemonic = daemonic\n        else:\n            self._pargs['daemonic'] = daemonic"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(self, timeout=None):\n\n        # we lazily create our process delegate (with same arguments)\n        if self.daemon:\n            daemonic = True\n        else:\n            daemonic = False\n\n        pargs = self._pargs.copy()\n        pargs.pop('daemonic', None)\n\n        self._process = multiprocessing.Process(**pargs)\n\n        self._process.daemon = daemonic\n\n        if self.is_alive():\n            # if already started, we shutdown and join before restarting\n            # not timeout will bock here (default join behavior).\n            # otherwise we simply use the same timeout.\n            self.shutdown(join=True, timeout=timeout)  # TODO : only restart if no error (check exitcode)\n            self.start(timeout=timeout)  # recursive to try again if needed\n        else:\n            self._process.start()\n\n        # timeout None means we want to wait and ensure it has started\n        # deterministic behavior, like is_alive() from multiprocess.Process is always true after start()\n        if self.started.wait(timeout=timeout):  # blocks until we know true or false\n\n\n            return True\n            # return self._svc_address  # returning the zmp url as a way to connect to the node\n            # CAREFUL : doesnt make sense if this node only run a one-time task...\n        # TODO: futures and ThreadPoolExecutor (so we dont need to manage the pool ourselves)\n        else:\n            return False", "response": "Start a child process and return the zmp url."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncleans shutdown of the node.", "response": "def shutdown(self, join=True, timeout=None):\n        \"\"\"\n        Clean shutdown of the node.\n        :param join: optionally wait for the process to end (default : True)\n        :return: None\n        \"\"\"\n        if self.is_alive():  # check if process started\n            print(\"Shutdown initiated\")\n            self.exit.set()\n            if join:\n                self.join(timeout=timeout)\n                # TODO : timeout before forcing terminate (SIGTERM)\n\n        exitcode = self._process.exitcode if self._process else None  # we return None if the process was never started\n        return exitcode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhand crafted event loop, with only one event possible : exit More events ( and signals ) can be added later, after converting to asyncio.", "response": "def eventloop(self, *args, **kwargs):\n        \"\"\"\n        Hand crafted event loop, with only one event possible : exit\n        More events ( and signals ) can be added later, after converting to asyncio.\n        \"\"\"\n\n        # Setting status\n        status = None\n\n        # Starting the clock\n        start = time.time()\n\n        first_loop = True\n        # loop running target, maybe more than once\n        while not self.exit.is_set():\n\n            if first_loop:\n                first_loop = False\n                # signalling startup only the first time, just after having check for exit request.\n                # We need to return control before starting, but after entering context...\n                self.started.set()\n                # TODO : check if better outside of loop maybe ??\n                # It will change semantics, but might be more intuitive...\n\n            # time is ticking\n            # TODO : move this out of here. this class should require only generic interface to any method.\n            now = time.time()\n            timedelta = now - start\n            start = now\n\n            # replacing the original Process.run() call, passing arguments to our target\n            if self._target:\n                # bwcompat\n                kwargs['timedelta'] = timedelta\n\n                # TODO : use return code to determine when/how we need to run this the next time...\n                # Also we need to keep the exit status to be able to call external process as an update...\n\n                logging.debug(\n                    \"[{self.name}] calling {self._target.__name__} with args {args} and kwargs {kwargs}...\".format(\n                        **locals()))\n                status = self._target(*args, **kwargs)\n\n            if status is not None:\n                break\n\n        if self.started.is_set() and status is None and self.exit.is_set():\n            # in the not so special case where we started, we didnt get exit code and we exited,\n            # this is expected as a normal result and we set an exitcode here of 0\n            # As 0 is the conventional success for unix process successful run\n            status = 0\n\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps DictTree data to json files.", "response": "def dump(self, path):\n        \"\"\"\n        dump DictTree data to json files.\n        \"\"\"\n        try:\n            with open(path, \"wb\") as f:\n                f.write(self.__str__().encode(\"utf-8\"))\n        except:\n            pass\n\n        with open(path, \"wb\") as f:\n            pickle.dump(self.__data__, f)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading DictTree from json files.", "response": "def load(cls, path):\n        \"\"\"\n        load DictTree from json files.\n        \"\"\"\n        try:\n            with open(path, \"rb\") as f:\n                return cls(__data__=json.loads(f.read().decode(\"utf-8\")))\n        except:\n            pass\n\n        with open(path, \"rb\") as f:\n            return cls(__data__=pickle.load(f))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef keys_at(self, depth, counter=1):\n        if depth < 1:\n            yield ROOT\n        else:\n            if counter == depth:\n                for key in self.keys():\n                    yield key\n            else:\n                counter += 1\n                for dict_tree in self.values():\n                    for key in dict_tree.keys_at(depth, counter):\n                        yield key", "response": "Iterate keys at specified depth."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef values_at(self, depth):\n        if depth < 1:\n            yield self\n        else:\n            for dict_tree in self.values():\n                for value in dict_tree.values_at(depth - 1):\n                    yield value", "response": "Iterate values at specified depth."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate items at specified depth.", "response": "def items_at(self, depth):\n        \"\"\"\n        Iterate items at specified depth.\n        \"\"\"\n        if depth < 1:\n            yield ROOT, self\n        elif depth == 1:\n            for key, value in self.items():\n                yield key, value\n        else:\n            for dict_tree in self.values():\n                for key, value in dict_tree.items_at(depth - 1):\n                    yield key, value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef length_at(self, depth):\n        if depth == 0:\n            return 1\n\n        counter = 0\n        for dict_tree in self.values_at(depth - 1):\n            counter += len(dict_tree)\n        return counter", "response": "Get the number of nodes on specific depth."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays the node stats on specific depth in this dict.", "response": "def stats(self, result=None, counter=0):\n        \"\"\"\n        Display the node stats info on specific depth in this dict.\n\n        ::\n\n            [\n                {\"depth\": 0, \"leaf\": M0, \"root\": N0},\n                {\"depth\": 1, \"leaf\": M1, \"root\": N1},\n                ...\n                {\"depth\": k, \"leaf\": Mk, \"root\": Nk},\n            ]\n        \"\"\"\n        if result is None:\n            result = dict()\n\n        if counter == 0:\n            if len(self):\n                result[0] = {\"depth\": 0, \"leaf\": 0, \"root\": 1}\n            else:\n                result[0] = {\"depth\": 0, \"leaf\": 1, \"root\": 0}\n\n        counter += 1\n        if len(self):\n            result.setdefault(\n                counter, {\"depth\": counter, \"leaf\": 0, \"root\": 0})\n            for dict_tree in self.values():\n                if len(dict_tree):  # root\n                    result[counter][\"root\"] += 1\n                else:  # leaf\n                    result[counter][\"leaf\"] += 1\n                dict_tree.stats(result, counter)\n\n        return [\n            collections.OrderedDict([\n                (\"depth\", info[\"depth\"]),\n                (\"leaf\", info[\"leaf\"]),\n                (\"root\", info[\"root\"]),\n            ]) for info in\n            sorted(result.values(), key=lambda x: x[\"depth\"])\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nput data to the queue and returns a Task instance", "response": "async def put(self, data, *, pri=None, ttl=None, ttr=None, delay=None):\n        \"\"\"\n            Puts data to the queue and returns a newly created Task\n\n            :param data: Arbitrary task payload\n            :param pri: Task priority (0 by default)\n            :param ttl: Task time-to-live\n            :param ttr: Task time-to-run\n            :param delay: Task delay\n            :return: Task instance\n        \"\"\"\n        opts = {}\n        if pri is not None:\n            opts['pri'] = pri\n\n        if ttl is not None:\n            opts['ttl'] = ttl\n\n        if ttr is not None:\n            opts['ttr'] = ttr\n\n        if delay is not None:\n            opts['delay'] = delay\n\n        args = (data, opts)\n        res = await self.conn.call(self.__funcs['put'], args)\n        return self._create_task(res.body)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def take(self, timeout=None):\n        args = None\n        if timeout is not None:\n            args = (timeout,)\n\n        res = await self.conn.call(self.__funcs['take'], args)\n        if len(res.body) > 0:\n            return self._create_task(res.body)\n        return None", "response": "Takes a task from the queue waiting the timeout if specified"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def touch(self, task_id, increment):\n        args = (task_id, increment)\n        res = await self.conn.call(self.__funcs['touch'], args)\n        return self._create_task(res.body)", "response": "Update task ttl and ttr by increment value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreleasing a task from the queue.", "response": "async def release(self, task_id, *, delay=None):\n        \"\"\"\n            Release task (return to queue) with delay if specified\n\n            :param task_id: Task id\n            :param delay: Time in seconds before task will become ready again\n            :return: Task instance\n        \"\"\"\n        opts = {}\n        if delay is not None:\n            opts['delay'] = delay\n        args = (task_id, opts)\n        res = await self.conn.call(self.__funcs['release'], args)\n        return self._create_task(res.body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets task without changing its state", "response": "async def peek(self, task_id):\n        \"\"\"\n            Get task without changing its state\n\n            :param task_id: Task id\n            :return: Task instance\n        \"\"\"\n\n        args = (task_id,)\n        res = await self.conn.call(self.__funcs['peek'], args)\n        return self._create_task(res.body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def kick(self, count):\n        args = (count,)\n        res = await self.conn.call(self.__funcs['kick'], args)\n        if self.conn.version < (1, 7):\n            return res.body[0][0]\n        return res.body[0]", "response": "Kick count tasks from queue\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the response body as JSON raise on errors", "response": "def _parse_content(response):\n    \"\"\"parse the response body as JSON, raise on errors\"\"\"\n    if response.status_code != 200:\n        raise ApiError(f'unknown error: {response.content.decode()}')\n    result = json.loads(response.content)\n    if not result['ok']:\n        raise ApiError(f'{result[\"error\"]}: {result.get(\"detail\")}')\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef json_post(methodname, rtype, key):\n    return compose(\n        reusable,\n        map_return(registry(rtype), itemgetter(key)),\n        basic_interaction,\n        map_yield(partial(_json_as_post, methodname)),\n        oneyield,\n    )", "response": "decorator factory for json POST queries"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the full path to the settings. cfg file.", "response": "def _user_config_file():\n    \"\"\"\n    Returns the path to the settings.cfg file. On Windows the file is\n    located in the AppData/Local/envipyengine directory. On Unix, the file\n    will be located in the ~/.envipyengine directory.\n\n    :return: String specifying the full path to the settings.cfg file\n    \"\"\"\n    if sys.platform == 'win32':\n        if 'LOCALAPPDATA' in os.environ:\n            user_dir = os.getenv('LOCALAPPDATA')\n        else:\n            user_dir = os.path.join(os.path.expanduser('~'), 'AppData', 'Local')\n        config_path = os.path.join(user_dir, _APP_DIRNAME, _CONFIG_FILENAME)\n    elif sys.platform.startswith('darwin'):\n        user_dir = os.path.expanduser('~')\n        config_path = os.path.sep.join([user_dir, 'Library', 'Preferences',\n                                        _APP_DIRNAME, _CONFIG_FILENAME])\n    else:\n        user_dir = os.path.expanduser('~')\n        config_path = os.path.sep.join([user_dir, '.' + _APP_DIRNAME,\n                                        _CONFIG_FILENAME])\n    return config_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _system_config_file():\n    if sys.platform == 'win32':\n        config_path = os.path.sep.join([_windows_system_appdata(),\n                                        _APP_DIRNAME,\n                                        _CONFIG_FILENAME])\n    elif sys.platform.startswith('darwin'):\n        config_path = os.path.sep.join([os.path.sep + 'Library', 'Preferences',\n                                        _APP_DIRNAME, _CONFIG_FILENAME])\n    else:\n        config_path = os.path.sep.join(['', 'var', 'lib', _APP_DIRNAME,\n                                        _CONFIG_FILENAME])\n    return config_path", "response": "Returns the full path to the system config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the path to the Windows Common App Data folder.", "response": "def _windows_system_appdata():\n    \"\"\"\n    Return the path to the Windows Common App Data folder.\n    On Windows 7, for example, this is C:\\\\ProgramData\n\n    :return: String reprsentation the path to the Windows Common\n             App Data folder\n    \"\"\"\n    # Could also use os.environ['ALLUSERSPROFILE'] - maybe?\n    csidl_common_appdata = 35\n    sh_get_folder_path = windll.shell32.SHGetFolderPathW\n    sh_get_folder_path.argtypes = [wintypes.HWND,\n                                   ctypes.c_int,\n                                   wintypes.HANDLE,\n                                   wintypes.DWORD,\n                                   wintypes.LPCWSTR]\n    path_buf = ctypes.create_unicode_buffer(wintypes.MAX_PATH)\n    result = sh_get_folder_path(0, csidl_common_appdata, 0, 0, path_buf)\n    return str(path_buf.value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the config file and return a ConfigParser object populated from the settings. cfg file.", "response": "def _read_config(cfg_file):\n    \"\"\"\n    Return a ConfigParser object populated from the settings.cfg file.\n\n    :return: A Config Parser object.\n    \"\"\"\n    config = ConfigParser()\n    # maintain case of options\n    config.optionxform = lambda option: option\n    if not os.path.exists(cfg_file):\n        # Create an empty config\n        config.add_section(_MAIN_SECTION_NAME)\n        config.add_section(_ENVIRONMENT_SECTION_NAME)\n    else:\n        config.read(cfg_file)\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_config(config, cfg_file):\n    directory = os.path.dirname(cfg_file)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    with open(cfg_file, \"w+\") as output_file:\n        config.write(output_file)", "response": "Write a ConfigParser object to the settings. cfg file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all environment settings from the config files.", "response": "def get_environment():\n    \"\"\"\n    Return all environment values from the config files. Values\n    stored in the user configuration file will take precedence\n    over values stored in the system configuration file.\n\n    :return: A dictionary containing the name/value pairs of all\n             environment settings in the config file.\n    \"\"\"\n    section = _ENVIRONMENT_SECTION_NAME\n    # Read system\n    sys_cfg = _read_config(_SYSTEM_CONFIG_FILE)\n    sys_env = \\\n        dict(sys_cfg.items(section)) if sys_cfg.has_section(section) else {}\n\n    # Read user\n    usr_cfg = _read_config(_USER_CONFIG_FILE)\n    usr_env = \\\n        dict(usr_cfg.items(section)) if usr_cfg.has_section(section) else {}\n\n    # Merge user into system\n    for k in usr_env.keys():\n        sys_env[k] = usr_env[k]\n    return sys_env"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the environment values in the config file.", "response": "def set_environment(environment, system=False):\n    \"\"\"\n    Set engine environment values in the config file.\n\n    :param environment: A dictionary containing the environment variable\n                        settings as key/value pairs.\n    :keyword system: Set to True to modify the system configuration file.\n                     If not set, the user config file will be modified.\n    \"\"\"\n    config_filename = \\\n        _SYSTEM_CONFIG_FILE if system is True else _USER_CONFIG_FILE\n    config = _read_config(config_filename)\n\n    section = _ENVIRONMENT_SECTION_NAME\n    for key in environment.keys():\n        config.set(section, key, environment[key])\n    _write_config(config, config_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the specified environment setting from the appropriate config file.", "response": "def remove_environment(environment_var_name, system=False):\n    \"\"\"\n    Remove the specified environment setting from the appropriate config file.\n\n    :param environment_var_name: The name of the environment setting to remove.\n    :keyword system: Set to True to modify the system configuration file.\n                     If not set, the user config file will be modified.\n    \"\"\"\n    config_filename = \\\n        _SYSTEM_CONFIG_FILE if system is True else _USER_CONFIG_FILE\n    config = _read_config(config_filename)\n\n    section = _ENVIRONMENT_SECTION_NAME\n    config.remove_option(section, environment_var_name)\n    _write_config(config, config_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the value of the specified configuration property.", "response": "def get(property_name):\n    \"\"\"\n    Returns the value of the specified configuration property.\n    Property values stored in the user configuration file take\n    precedence over values stored in the system configuration\n    file.\n\n    :param property_name: The name of the property to retrieve.\n    :return: The value of the property.\n    \"\"\"\n    config = _read_config(_USER_CONFIG_FILE)\n    section = _MAIN_SECTION_NAME\n    try:\n        property_value = config.get(section, property_name)\n    except (NoOptionError, NoSectionError) as error:\n\n        # Try the system config file\n        try:\n            config = _read_config(_SYSTEM_CONFIG_FILE)\n            property_value = config.get(section, property_name)\n        except (NoOptionError, NoSectionError) as error:\n            raise NoConfigOptionError(error)\n\n    return property_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value of a configuration property in the current configuration file.", "response": "def set(property_name, value, system=False):\n    \"\"\"\n    Sets the configuration property to the specified value.\n\n    :param property_name: The name of the property to set.\n    :param value: The value for the property.\n    :keyword system: Set to True to modify the system configuration file.\n                     If not set, the user config file will be modified.\n    \"\"\"\n    config_filename = \\\n        _SYSTEM_CONFIG_FILE if system is True else _USER_CONFIG_FILE\n    config = _read_config(config_filename)\n\n    section = _MAIN_SECTION_NAME\n    config.set(section, property_name, value)\n    _write_config(config, config_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(property_name, system=False):\n    config_filename = \\\n        _SYSTEM_CONFIG_FILE if system is True else _USER_CONFIG_FILE\n    config = _read_config(config_filename)\n\n    section = _MAIN_SECTION_NAME\n    config.remove_option(section, property_name)\n    _write_config(config, config_filename)", "response": "Removes a configuration property value setting from the config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_app(self, app):\n\n        if self.__inited:\n            return\n\n        config = app.config.get(\n            'FLASK_LOGGING_EXTRAS', {})\n\n        blueprint_config = config.get('BLUEPRINT', {})\n        self.bp_var = blueprint_config.get('FORMAT_NAME', 'blueprint')\n        self.bp_app = blueprint_config.get('APP_BLUEPRINT', '<app>')\n        self.bp_noreq = blueprint_config.get('NO_REQUEST_BLUEPRINT', '<not a request>')\n\n        for var_name, resolver_fqn in config.get('RESOLVERS', {}).items():\n            if resolver_fqn is None:\n                resolver = None\n            else:\n                try:\n                    resolver = _import_by_string(resolver_fqn)\n                except ImportError:\n                    resolver = resolver_fqn\n\n            self.resolvers[var_name] = resolver\n\n            self.__inited = True", "response": "Initialise the formatter with app - specific values from app s configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register(self, id, name, address, port=None, tags=None, check=None):\n        service = {}\n        service['ID'] = id\n        service['Name'] = name\n        service['Address'] = address\n        if port:\n            service['Port'] = int(port)\n        if tags:\n            service['Tags'] = tags\n        if check:\n            service['Check'] = check\n        r = requests.put(self.url_register, json=service)\n        if r.status_code != 200:\n            raise consulRegistrationError(\n                'PUT returned {}'.format(r.status_code))\n        return r", "response": "Register a new service with the local consul agent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deregister(self, id):\n        r = requests.put('{}/{}'.format(self.url_deregister, id))\n        if r.status_code != 200:\n            raise consulDeregistrationError(\n                'PUT returned {}'.format(r.status_code))\n        return r", "response": "Deregister a service with the local consul agent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef info(self, name):\n        r = requests.get('{}/{}'.format(self.url_service, name))\n        return r.json()", "response": "Get information about a given service"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef toLily(self):\n        '''\n        Method which converts the object instance, its attributes and children to a string of lilypond code\n\n        :return: str of lilypond code\n        '''\n        lilystring = \"\\\\version \\\"2.18.2\\\" \\n\"\n\n        partstrings = []\n        ids_loaded = []\n        groupings = []\n        if len(self.groups) > 0:\n            # here we need to do some set union theory\n            lstring, groupings, ids_loaded = self.handleGroups()\n            lilystring += lstring\n        children = [\n            child for child in self.GetSortedChildren() if child not in ids_loaded]\n        for child in children:\n            part = self.getPart(child)\n            partstring = part.toLily()\n            lilystring += partstring[0]\n            partstrings.append(partstring[1])\n        lilystring += self.item.toLily()\n        lilystring += \"<<\"\n        lilystring += \"\".join([gstring for gstring in groupings])\n        lilystring += \"\".join([partstring for partstring in partstrings])\n        lilystring += \">>\"\n        return lilystring", "response": "Method which converts the object instance its attributes and children to a string of lilypond code."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the command line", "response": "def run(self):\n        \"\"\"runner\"\"\"\n\n        subprocess.check_call(\"python setup.py sdist\", shell=True)\n        subprocess.check_call(\"python setup.py bdist_wheel\", shell=True)\n        # OLD way:\n        # os.system(\"python setup.py sdist bdist_wheel upload\")\n        # NEW way:\n        # Ref: https://packaging.python.org/distributing/\n        subprocess.check_call(\"twine upload dist/*\", shell=True)\n\n        subprocess.check_call(\"git tag -a {0} -m 'version {0}'\".format(__version__), shell=True)\n        subprocess.check_call(\"git push --tags\", shell=True)\n        sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef device_message(device,\n                   code,\n                   ts=None,\n                   origin=None,\n                   type=None,\n                   severity=None,\n                   title=None,\n                   description=None,\n                   hint=None,\n                   **metaData):\n    # pylint: disable=redefined-builtin, too-many-arguments\n    \"\"\"This quickly builds a time-stamped message. If `ts` is None, the\n    current time is used.\n    \"\"\"\n    if ts is None:\n        ts = local_now()\n    payload = MessagePayload(device=device)\n    payload.messages.append(\n        Message(\n            code=code,\n            ts=ts,\n            origin=origin,\n            type=type,\n            severity=severity,\n            title=title,\n            description=description,\n            hint=hint,\n            **metaData))\n    return dumps(payload)", "response": "Builds a message from a device."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dump(obj, abspath, serializer_type,\n          dumper_func=None,\n          compress=True,\n          overwrite=False,\n          verbose=False,\n          **kwargs):\n    \"\"\"Dump object to file.\n\n    :param abspath: The file path you want dump to.\n    :type abspath: str\n\n    :param serializer_type: 'binary' or 'str'.\n    :type serializer_type: str\n\n    :param dumper_func: A dumper function that takes an object as input, return\n        binary or string.\n    :type dumper_func: callable function\n\n    :param compress: default ``False``. If True, then compress binary.\n    :type compress: bool\n\n    :param overwrite: default ``False``, If ``True``, when you dump to\n      existing file, it silently overwrite it. If ``False``, an alert\n      message is shown. Default setting ``False`` is to prevent overwrite\n      file by mistake.\n    :type overwrite: boolean\n\n    :param verbose: default True, help-message-display trigger.\n    :type verbose: boolean\n    \"\"\"\n    _check_serializer_type(serializer_type)\n\n    if not inspect.isfunction(dumper_func):\n        raise TypeError(\"dumper_func has to be a function take object as input \"\n                        \"and return binary!\")\n\n    prt_console(\"\\nDump to '%s' ...\" % abspath, verbose)\n    if os.path.exists(abspath):\n        if not overwrite:\n            prt_console(\n                \"    Stop! File exists and overwrite is not allowed\",\n                verbose,\n            )\n            return\n\n    st = time.clock()\n\n    b_or_str = dumper_func(obj, **kwargs)\n    if serializer_type is \"str\":\n        b = b_or_str.encode(\"utf-8\")\n    else:\n        b = b_or_str\n\n    if compress:\n        b = zlib.compress(b)\n\n    with atomic_write(abspath, overwrite=overwrite, mode=\"wb\") as f:\n        f.write(b)\n\n    elapsed = time.clock() - st\n    prt_console(\"    Complete! Elapse %.6f sec.\" % elapsed, verbose)\n\n    if serializer_type is \"str\":\n        return b_or_str\n    else:\n        return b", "response": "Dump object to file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load(abspath, serializer_type,\n          loader_func=None,\n          decompress=True,\n          verbose=False,\n          **kwargs):\n    \"\"\"load object from file.\n\n    :param abspath: The file path you want load from.\n    :type abspath: str\n\n    :param serializer_type: 'binary' or 'str'.\n    :type serializer_type: str\n\n    :param loader_func: A loader function that takes binary as input, return\n        an object.\n    :type loader_func: callable function\n\n    :param decompress: default ``False``. If True, then decompress binary.\n    :type decompress: bool\n\n    :param verbose: default True, help-message-display trigger.\n    :type verbose: boolean\n    \"\"\"\n    _check_serializer_type(serializer_type)\n\n    if not inspect.isfunction(loader_func):\n        raise TypeError(\"loader_func has to be a function take binary as input \"\n                        \"and return an object!\")\n\n    prt_console(\"\\nLoad from '%s' ...\" % abspath, verbose)\n    if not os.path.exists(abspath):\n        raise ValueError(\"'%s' doesn't exist.\" % abspath)\n\n    st = time.clock()\n\n    with open(abspath, \"rb\") as f:\n        b = f.read()\n        if decompress:\n            b = zlib.decompress(b)\n\n    if serializer_type is \"str\":\n        obj = loader_func(b.decode(\"utf-8\"), **kwargs)\n    else:\n        obj = loader_func(b, **kwargs)\n\n    elapsed = time.clock() - st\n    prt_console(\"    Complete! Elapse %.6f sec.\" % elapsed, verbose)\n\n    return obj", "response": "Load object from file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump_func(serializer_type):\n\n    def outer_wrapper(dumper_func):\n        def wrapper(*args, **kwargs):\n            return _dump(\n                *args,\n                dumper_func=dumper_func, serializer_type=serializer_type,\n                **kwargs\n            )\n\n        return wrapper\n\n    return outer_wrapper", "response": "A decorator for _dump"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_func(serializer_type):\n\n    def outer_wrapper(loader_func):\n        def wrapper(*args, **kwargs):\n            return _load(\n                *args,\n                loader_func=loader_func, serializer_type=serializer_type,\n                **kwargs\n            )\n\n        return wrapper\n\n    return outer_wrapper", "response": "A decorator for _load"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_response(self, method, endpoint, data=None):\n        url = urljoin(IVONA_REGION_ENDPOINTS[self.region], endpoint)\n\n        response = getattr(self.session, method)(\n            url, json=data,\n        )\n\n        if 'x-amzn-ErrorType' in response.headers:\n            raise IvonaAPIException(response.headers['x-amzn-ErrorType'])\n\n        if response.status_code != requests.codes.ok:\n            raise IvonaAPIException(\n                \"Something wrong happened: {}\".format(response.json())\n            )\n\n        return response", "response": "Wrapper method for wrapping API requests."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_available_voices(self, language=None, gender=None):\n        endpoint = 'ListVoices'\n\n        data = dict()\n        if language:\n            data.update({'Voice': {'Language': language}})\n\n        if gender:\n            data.update({'Voice': {'Gender': gender}})\n\n        print(data)\n\n        response = self._get_response('get', endpoint, data)\n\n        return response.json()['Voices']", "response": "Returns a list of available voices via ListVoices endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves given text synthesized audio file via CreateSpeech endpoint", "response": "def text_to_speech(self, text, file, voice_name=None, language=None):\n        \"\"\"\n        Saves given text synthesized audio file, via 'CreateSpeech' endpoint\n\n        Docs:\n            http://developer.ivona.com/en/speechcloud/actions.html#CreateSpeech\n\n        :param text: text to synthesize\n        :type text: str\n        :param file: file that will be used to save the audio\n        :type file: file\n        :param voice_name: voice name\n        :type voice_name: str\n        :param language: voice language\n        :type language: str\n        \"\"\"\n        endpoint = 'CreateSpeech'\n\n        data = {\n            'Input': {\n                'Data': text,\n            },\n            'OutputFormat': {\n                'Codec': self.codec.upper(),\n            },\n            'Parameters': {\n                'Rate': self.rate,\n                'Volume': self.volume,\n                'SentenceBreak': self.sentence_break,\n                'ParagraphBreak': self.paragraph_break,\n            },\n            'Voice': {\n                'Name': voice_name or self.voice_name,\n                'Language': language or self.language,\n            },\n        }\n\n        response = self._get_response('post', endpoint, data)\n\n        file.write(response.content)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_client_with_auto_poll(api_key, poll_interval_seconds=60, max_init_wait_time_seconds=5,\n                                 on_configuration_changed_callback=None, config_cache_class=None,\n                                 base_url=None):\n    \"\"\"\n    Create an instance of ConfigCatClient and setup Auto Poll mode with custom options\n\n    :param api_key: ConfigCat ApiKey to access your configuration.\n    :param poll_interval_seconds: The client's poll interval in seconds. Default: 60 seconds.\n    :param on_configuration_changed_callback: You can subscribe to configuration changes with this callback\n    :param max_init_wait_time_seconds: maximum waiting time for first configuration fetch in polling mode.\n    :param config_cache_class: If you want to use custom caching instead of the client's default InMemoryConfigCache,\n    You can provide an implementation of ConfigCache.\n    :param base_url: You can set a base_url if you want to use a proxy server between your application and ConfigCat\n    \"\"\"\n\n    if api_key is None:\n        raise ConfigCatClientException('API Key is required.')\n\n    if poll_interval_seconds < 1:\n        poll_interval_seconds = 1\n\n    if max_init_wait_time_seconds < 0:\n        max_init_wait_time_seconds = 0\n\n    return ConfigCatClient(api_key, poll_interval_seconds, max_init_wait_time_seconds,\n                           on_configuration_changed_callback, 0, config_cache_class, base_url)", "response": "Create an instance of ConfigCatClient with custom options"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_client_with_lazy_load(api_key, cache_time_to_live_seconds=60, config_cache_class=None,\n                                 base_url=None):\n    \"\"\"\n    Create an instance of ConfigCatClient and setup Lazy Load mode with custom options\n\n    :param api_key: ConfigCat ApiKey to access your configuration.\n    :param cache_time_to_live_seconds: The cache TTL.\n    :param config_cache_class: If you want to use custom caching instead of the client's default InMemoryConfigCache,\n    You can provide an implementation of ConfigCache.\n    :param base_url: You can set a base_url if you want to use a proxy server between your application and ConfigCat\n    \"\"\"\n\n    if api_key is None:\n        raise ConfigCatClientException('API Key is required.')\n\n    if cache_time_to_live_seconds < 1:\n        cache_time_to_live_seconds = 1\n\n    return ConfigCatClient(api_key, 0, 0, None, cache_time_to_live_seconds, config_cache_class, base_url)", "response": "Create an instance of ConfigCatClient and setup Lazy Load mode with custom options."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_client_with_manual_poll(api_key, config_cache_class=None,\n                                   base_url=None):\n    \"\"\"\n    Create an instance of ConfigCatClient and setup Manual Poll mode with custom options\n\n    :param api_key: ConfigCat ApiKey to access your configuration.\n    :param config_cache_class: If you want to use custom caching instead of the client's default InMemoryConfigCache,\n    You can provide an implementation of ConfigCache.\n    :param base_url: You can set a base_url if you want to use a proxy server between your application and ConfigCat\n    \"\"\"\n\n    if api_key is None:\n        raise ConfigCatClientException('API Key is required.')\n\n    return ConfigCatClient(api_key, 0, 0, None, 0, config_cache_class, base_url)", "response": "Create an instance of ConfigCatClient and setup Manual Poll mode with custom options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget departures for a station", "response": "def departures(station: str) -> snug.Query[t.List[Departure]]:\n    \"\"\"departures for a station\"\"\"\n    return snug.GET('avt', params={'station': station})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef journey_options(origin:      str,\n                    destination: str,\n                    via:         t.Optional[str]=None,\n                    before:      t.Optional[int]=None,\n                    after:       t.Optional[int]=None,\n                    time:        t.Optional[datetime]=None,\n                    hsl:         t.Optional[bool]=None,\n                    year_card:   t.Optional[bool]=None) -> (\n                        snug.Query[t.List[Journey]]):\n    \"\"\"journey recommendations from an origin to a destination station\"\"\"\n    return snug.GET('treinplanner', params={\n        'fromStation':     origin,\n        'toStation':       destination,\n        'viaStation':      via,\n        'previousAdvices': before,\n        'nextAdvices':     after,\n        'dateTime':        time,\n        'hslAllowed':      hsl,\n        'yearCard':        year_card,\n    })", "response": "Returns a list of journey recommendations from an origin to a destination station"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rand_str(length, allowed=CHARSET_ALPHA_DIGITS):\n    res = list()\n    for _ in range(length):\n        res.append(random.choice(allowed))\n    return \"\".join(res)", "response": "Generate a fixed - length random string from your allowed character pool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rand_alphastr(length, lower=True, upper=True):\n    if lower is True and upper is True:\n        return rand_str(length, allowed=string.ascii_letters)\n    if lower is True and upper is False:\n        return rand_str(length, allowed=string.ascii_lowercase)\n    if lower is False and upper is True:\n        return rand_str(length, allowed=string.ascii_uppercase)\n    else:\n        raise Exception", "response": "Generate fixed - length random alpha only string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rand_email():\n    name = rand_str(random.randint(4, 14), string.ascii_lowercase) + \\\n           rand_str(random.randint(1, 4), string.digits)\n    domain = rand_str(random.randint(2, 10), string.ascii_lowercase)\n    surfix = random.choice(DOMAIN_SURFIX)\n    return \"%s@%s.%s\" % (name, domain, surfix)", "response": "Random email.\n\n    Usage Example::\n\n        >>> rand_email()\n        Z4Lljcbdw7m@npa.net"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toLily(self):\n        '''\n        Method which converts the object instance and its attributes to a string of lilypond code\n\n        :return: str of lilypond code\n        '''\n        val = \"\\clef \"\n        clef = \"\"\n        if hasattr(self, \"sign\") and self.sign is not None:\n            key = self.sign.upper()\n            if key == \"TAB\":\n                return \"\\clef moderntab\"\n            if hasattr(self, \"line\") and self.line is not None:\n                key += str(self.line)\n            if key in clef_type:\n                clef = clef_type[key]\n            else:\n                val = \"\"\n        else:\n            val = \"\"\n        if hasattr(self, \"octave_change\") and self.octave_change is not None:\n            options = {1: \"^8\", 2: \"^15\", -1: \"_8\", -2: \"_15\"}\n            if self.octave_change in options:\n                clef = \"\\\"\" + clef + options[self.octave_change] + \"\\\"\"\n        val += clef\n        return val", "response": "Method which converts the object instance and its attributes to a string of lilypond code"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _resolve_dep(self, key):\n        if key in self.future_values_key_dep:\n            # there are some dependencies that can be resoled\n            dep_list = self.future_values_key_dep[key]\n            del self.future_values_key_dep[key]  # remove dependencies\n            also_finish = []\n            # iterate over the dependencies that can now be resoled\n            for dep in dep_list:\n                if self.__resolve_dep_helper(dep, key) is True:\n                    also_finish.append(dep)\n            # maybe the resolving process leed to new deps that can be resolved\n            for dep in also_finish:\n                self._resolve_dep(dep)", "response": "this method resolves dependencies for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets al list of all dependencies for the given item dep", "response": "def _get_all_refs(self, dep, handled_refs=None):\n        \"\"\"\n        get al list of all dependencies for the given item \"dep\"\n        \"\"\"\n        if handled_refs is None:\n            handled_refs = [dep]\n        else:\n            if dep in handled_refs:\n                return []\n        res = []\n        if dep in self.future_values_key_item:\n            res.extend(\n                self.future_values_key_item[dep][\"dependencies\"].values())\n            add = []\n            for h_d in res:\n                add.extend(self._get_all_refs(h_d, handled_refs))\n            res.extend(add)\n        return list(set(res))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(cls, host='localhost', port=14999,\n               auto_reconnect=True, loop=None, protocol_class=AVR,\n               update_callback=None):\n        \"\"\"Initiate a connection to a specific device.\n\n        Here is where we supply the host and port and callback callables we\n        expect for this AVR class object.\n\n        :param host:\n            Hostname or IP address of the device\n        :param port:\n            TCP port number of the device\n        :param auto_reconnect:\n            Should the Connection try to automatically reconnect if needed?\n        :param loop:\n            asyncio.loop for async operation\n        :param update_callback\"\n            This function is called whenever AVR state data changes\n\n        :type host:\n            str\n        :type port:\n            int\n        :type auto_reconnect:\n            boolean\n        :type loop:\n            asyncio.loop\n        :type update_callback:\n            callable\n        \"\"\"\n        assert port >= 0, 'Invalid port value: %r' % (port)\n        conn = cls()\n\n        conn.host = host\n        conn.port = port\n        conn._loop = loop or asyncio.get_event_loop()\n        conn._retry_interval = 1\n        conn._closed = False\n        conn._closing = False\n        conn._halted = False\n        conn._auto_reconnect = auto_reconnect\n\n        def connection_lost():\n            \"\"\"Function callback for Protocoal class when connection is lost.\"\"\"\n            if conn._auto_reconnect and not conn._closing:\n                ensure_future(conn._reconnect(), loop=conn._loop)\n\n        conn.protocol = protocol_class(\n            connection_lost_callback=connection_lost, loop=conn._loop,\n            update_callback=update_callback)\n\n        yield from conn._reconnect()\n\n        return conn", "response": "Initiate a connection to a specific device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclosing the AVR device connection and don t try to reconnect.", "response": "def close(self):\n        \"\"\"Close the AVR device connection and don't try to reconnect.\"\"\"\n        self.log.warning('Closing connection to AVR')\n        self._closing = True\n        if self.protocol.transport:\n            self.protocol.transport.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef halt(self):\n        self.log.warning('Halting connection to AVR')\n        self._halted = True\n        if self.protocol.transport:\n            self.protocol.transport.close()", "response": "Close the AVR connection and wait for a resume request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compress_obj(obj, level):\n    return zlib.compress(pickle.dumps(obj, protocol=2), level)", "response": "Compress object to bytes.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncompressing any object to bytes or base64 string.", "response": "def compress(obj, level=6, return_type=\"bytes\"):\n    \"\"\"Compress anything to bytes or string.\n\n    :param obj: could be any object, usually it could be binary, string, or\n        regular python objec.t\n    :param level:\n    :param return_type: if bytes, then return bytes; if str, then return\n        base64.b64encode bytes in utf-8 string.\n    \"\"\"\n    if isinstance(obj, binary_type):\n        b = _compress_bytes(obj, level)\n    elif isinstance(obj, string_types):\n        b = _compress_str(obj, level)\n    else:\n        b = _compress_obj(obj, level)\n\n    if return_type == \"bytes\":\n        return b\n    elif return_type == \"str\":\n        return base64.b64encode(b).decode(\"utf-8\")\n    else:\n        raise ValueError(\"'return_type' has to be one of 'bytes', 'str'!\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decompress(obj, return_type=\"bytes\"):\n    if isinstance(obj, binary_type):\n        b = zlib.decompress(obj)\n    elif isinstance(obj, string_types):\n        b = zlib.decompress(base64.b64decode(obj.encode(\"utf-8\")))\n    else:\n        raise TypeError(\"input cannot be anything other than str and bytes!\")\n\n    if return_type == \"bytes\":\n        return b\n    elif return_type == \"str\":\n        return b.decode(\"utf-8\")\n    elif return_type == \"obj\":\n        return pickle.loads(b)\n    else:\n        raise ValueError(\n            \"'return_type' has to be one of 'bytes', 'str' or 'obj'!\")", "response": "Decompress a single object into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_tag(attrs=None, content=None):\n    builder = '<pre><code{attrs}>{content}</code></pre>'\n    return builder.format(\n        attrs=flatatt(attrs) if attrs else '',\n        content=escape(text_value(content)),\n    )", "response": "Render a HTML tag with the given attributes and content."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the Signature template for use with the Authorization header.", "response": "def build_signature_template(key_id, algorithm, headers):\n    \"\"\"\n    Build the Signature template for use with the Authorization header.\n\n    key_id is the mandatory label indicating to the server which secret to use\n    algorithm is one of the supported algorithms\n    headers is a list of http headers to be included in the signing string.\n\n    The signature must be interpolated into the template to get the final\n    Authorization header value.\n    \"\"\"\n    param_map = {'keyId': key_id,\n                 'algorithm': algorithm,\n                 'signature': '%s'}\n    if headers:\n        headers = [h.lower() for h in headers]\n        param_map['headers'] = ' '.join(headers)\n    kv = map('{0[0]}=\"{0[1]}\"'.format, param_map.items())\n    kv_string = ','.join(kv)\n    sig_string = 'Signature {0}'.format(kv_string)\n    return sig_string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfeed data into database.", "response": "def train(self, data, key_id, key_lat, key_lng, clear_old=True):\n        \"\"\"\n        Feed data into database.\n\n        :type data: list\n        :param data: list of point object, can have other metadata, for example:\n            [{\"id\": 10001, \"lat\": xxx, \"lng\": xxx}, ...]\n\n        :type key_id: callable\n        :param key_id: callable function, take point object as input, return object\n            id, for example: lambda x: x[\"id\"]\n\n        :type key_lat: callable\n        :param key_lat: callable function, take point object as input, return object\n            latitude, for example: lambda x: x[\"lat\"]\n\n        :type key_lng: callable\n        :param key_lng: callable function, take point object as input, return object\n            longitude, for example: lambda x: x[\"lng\"]\n        \"\"\"\n        engine, t_point = self.engine, self.t_point\n        if clear_old:\n            try:\n                t_point.drop(engine)\n            except:\n                pass\n        t_point.create(engine)\n\n        table_data = list()\n        for record in data:\n            id = key_id(record)\n            lat = key_lat(record)\n            lng = key_lng(record)\n            row = {\"id\": id, \"lat\": lat, \"lng\": lng, \"data\": record}\n            table_data.append(row)\n\n        ins = t_point.insert()\n        engine.execute(ins, table_data)\n\n        index = Index('idx_lat_lng', t_point.c.lat, t_point.c.lng)\n        index.create(engine)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_n_nearest(self, lat, lng, n=5, radius=None):\n        engine, t_point = self.engine, self.t_point\n        if radius:\n            # Use a simple box filter to minimize candidates\n            # Define latitude longitude boundary\n            dist_btwn_lat_deg = 69.172\n            dist_btwn_lon_deg = cos(lat) * 69.172\n            lat_degr_rad = abs(radius * 1.05 / dist_btwn_lat_deg)\n            lon_degr_rad = abs(radius * 1.05 / dist_btwn_lon_deg)\n\n            lat_lower = lat - lat_degr_rad\n            lat_upper = lat + lat_degr_rad\n            lng_lower = lng - lon_degr_rad\n            lng_upper = lng + lon_degr_rad\n\n            filters = [\n                t_point.c.lat >= lat_lower,\n                t_point.c.lat <= lat_upper,\n                t_point.c.lat >= lng_lower,\n                t_point.c.lat >= lng_upper,\n            ]\n        else:\n            radius = 999999.9\n            filters = []\n\n        s = select([t_point]).where(and_(*filters))\n\n        heap = list()\n        for row in engine.execute(s):\n            dist = great_circle((lat, lng), (row.lat, row.lng))\n            if dist <= radius:\n                heap.append((dist, row.data))\n\n        # Use heap sort to find top-K nearest\n        n_nearest = heapq.nsmallest(n, heap, key=lambda x: x[0])\n        return n_nearest", "response": "Find n nearest point within certain distance from a point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new copy of self with k items randomly generated from the collections", "response": "def sample(self, k):\n        '''\n        this method is especially troublesome\n        i do not reccommend making any changes to it\n        you may notice it uplicates code fro smappdragon\n        there is no way around this as far as i can tell\n        it really  might screw up a lot of stuff, stip tweets\n        has been purposely omitted as it isnt supported in pysmap\n        '''\n        def new_get_iterators():\n            tweet_parser = smappdragon.TweetParser()\n            it = iter(self.get_collection_iterators())\n            sample = list(itertools.islice(it, k))\n            random.shuffle(sample)\n            for i, item in enumerate(it, start=k+1):\n                j = random.randrange(i)\n                if j < k:\n                    sample[j] = item\n            for tweet in sample:\n                if all([collection.limit != 0 and collection.limit <= count for collection in self.collections]):\n                    return\n                elif all([tweet_parser.tweet_passes_filter(collection.filter, tweet) \\\n                and tweet_parser.tweet_passes_custom_filter_list(collection.custom_filters, tweet) for collection in self.collections]):\n                    yield tweet\n\n        cp = copy.deepcopy(self)\n        cp.get_collection_iterators = new_get_iterators\n        return cp"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a network file from an alignment file.", "response": "def alignment_to_contacts(\n    sam_merged,\n    assembly,\n    output_dir,\n    output_file_network=DEFAULT_NETWORK_FILE_NAME,\n    output_file_chunk_data=DEFAULT_CHUNK_DATA_FILE_NAME,\n    parameters=DEFAULT_PARAMETERS,\n):\n    \"\"\"Generates a network file (in edgelist form) from an\n    alignment in sam or bam format. Contigs are virtually split into\n    'chunks' of nearly fixed size (by default between 500 and 1000 bp)\n    to reduce size bias. The chunks are the network nodes and the edges\n    are the contact counts.\n\n    The network is in a strict barebone form so that it can be reused and\n    imported quickly into other applications etc. Verbose information about\n    every single node in the network is written on a 'chunk data' file,\n    by default called 'idx_contig_hit_size_cov.txt'\n\n    Parameters\n    ----------\n    sam_merged : file, str or pathlib.Path\n        The alignment file in SAM/BAM format to be processed.\n    assembly : file, str or pathlib.Path\n        The initial assembly acting as the alignment file's reference genome.\n    output_dir : str or pathlib.Path\n        The output directory to write the network and chunk data into.\n    output_dir_file_network : str or pathlib.Path, optional\n        The specific file name for the output network file. Default is\n        network.txt\n    output_file_chunk_data : str or pathlib.Path, optional\n        The specific file name for the output chunk data file. Default is\n        idx_contig_hit_size_cov.txt\n    parameters : dict, optional\n        A dictionary of parameters for converting the alignment file into a\n        network. These are:\n        -size_chunk_threshold: the size (in bp) under which chunks are\n        discarded. Default is 500.\n        -mapq_threshold: the mapping quality under which alignments are\n        discarded. Default is 10.\n        -chunk_size: the default chunk size (in bp) when applicable, save\n        smaller contigs or tail-ends. Default is 1000.\n        -read_size: the size of reads used for mapping. Default is 65.\n        -self_contacts: whether to count alignments between a chunk and\n        itself. Default is False.\n        -normalized: whether to normalize contacts by their coverage.\n        Default is False.\n\n    Returns\n    -------\n    chunk_complete_data : dict\n        A dictionary where the keys are chunks in (contig, position) form and\n        the values are their id, name, total contact count, size and coverage.\n    all_contacts : dict\n        A counter dictionary where the keys are chunk pairs and the values are\n        their contact count.\n    \"\"\"\n\n    all_contacts = collections.Counter()\n    all_chunks = collections.Counter()\n\n    #   Initialize parameters\n    chunk_size = int(parameters[\"chunk_size\"])\n    mapq_threshold = int(parameters[\"mapq_threshold\"])\n    size_chunk_threshold = int(parameters[\"size_chunk_threshold\"])\n    read_size = int(parameters[\"read_size\"])\n    self_contacts = parameters[\"self_contacts\"]\n    normalized = parameters[\"normalized\"]\n\n    logger.info(\"Establishing chunk list...\")\n    chunk_complete_data = dict()\n\n    #   Get all information about all chunks from all contigs\n    #   (this gets updated at the end)\n    global_id = 1\n    for record in SeqIO.parse(assembly, \"fasta\"):\n        length = len(record.seq)\n\n        n_chunks = length // chunk_size\n        n_chunks += (length % chunk_size) >= size_chunk_threshold\n\n        for i in range(n_chunks):\n\n            if (i + 1) * chunk_size <= length:\n                size = chunk_size\n            else:\n                size = length % chunk_size\n\n            chunk_name = \"{}_{}\".format(record.id, i)\n            chunk_complete_data[chunk_name] = {\n                \"id\": global_id,\n                \"hit\": 0,\n                \"size\": size,\n                \"coverage\": 0,\n            }\n            global_id += 1\n\n    logger.info(\"Opening alignment files...\")\n\n    current_read = None\n\n    # Read the BAM file to detect contacts.\n    with pysam.AlignmentFile(sam_merged, \"rb\") as alignment_merged_handle:\n\n        names = alignment_merged_handle.references\n        lengths = alignment_merged_handle.lengths\n        names_and_lengths = {\n            name: length for name, length in zip(names, lengths)\n        }\n\n        logger.info(\"Reading contacts...\")\n\n        # Since the BAM file is supposed to be sorted and interleaved,\n        # pairs should be always grouped with one below the other (the exact\n        # order doesn't matter since the network is symmetric, so we simply\n        # treat the first one as 'forward' and the second one as 'reverse')\n\n        # We keep iterating until two consecutive reads have the same name,\n        # discarding ones that don't.\n\n        while \"Reading forward and reverse alignments alternatively\":\n            try:\n                my_read = next(alignment_merged_handle)\n                if current_read is None:\n                    # First read\n                    current_read = my_read\n                    continue\n\n                elif current_read.query_name != my_read.query_name:\n\n                    # print(\"{}_{}\".format(current_read, my_read))\n                    current_read = my_read\n                    continue\n\n                read_forward, read_reverse = current_read, my_read\n\n            except StopIteration:\n                break\n\n            # Get a bunch of info about the alignments to pass the tests below\n            read_name_forward = read_forward.query_name\n            read_name_reverse = read_reverse.query_name\n\n            flag_forward, flag_reverse = read_forward.flag, read_reverse.flag\n\n            try:\n                assert read_name_forward == read_name_reverse\n            except AssertionError:\n                logger.error(\n                    \"Reads don't have the same name: \" \"%s and %s\",\n                    read_name_forward,\n                    read_name_reverse,\n                )\n                raise\n\n            # To check if a flag contains 4\n            # (digit on the third position from the right in base 2),\n            # 4 = unmapped in SAM spec\n            def is_unmapped(flag):\n                return np.base_repr(flag, padding=3)[-3] == \"1\"\n\n            if is_unmapped(flag_forward) or is_unmapped(flag_reverse):\n                # print(\"Detected unmapped read on one end, skipping\")\n                continue\n\n            contig_name_forward = read_forward.reference_name\n            contig_name_reverse = read_reverse.reference_name\n\n            len_contig_for = names_and_lengths[contig_name_forward]\n            len_contig_rev = names_and_lengths[contig_name_reverse]\n\n            position_forward = read_forward.reference_start\n            position_reverse = read_reverse.reference_start\n\n            mapq_forward = read_forward.mapping_quality\n            mapq_reverse = read_reverse.mapping_quality\n\n            # Some more tests: checking for size, map quality, map status etc.\n            mapq_test = min(mapq_forward, mapq_reverse) > mapq_threshold\n\n            min_length = min(len_contig_for, len_contig_rev)\n            length_test = min_length > size_chunk_threshold\n\n            # Trickest test:\n            #\n            #\n            #                contig\n            #    pos1                          pos2\n            #     ^                             ^\n            # |-------|-------|-------|-------|---|\n            # <-------><------><------><------><-->            <->\n            #   chunk   chunk                  tail   size_chunk_threshold\n            #\n            # Test is passed if tail >= size_chunk_threshold (pos2)\n            # or if the position is a non-tail chunk (pos1)\n\n            if position_forward < chunk_size * (len_contig_for // chunk_size):\n                current_chunk_forward_size = chunk_size\n            else:\n                current_chunk_forward_size = len_contig_for % chunk_size\n\n            if position_reverse < chunk_size * (len_contig_rev // chunk_size):\n                current_chunk_reverse_size = chunk_size\n            else:\n                current_chunk_reverse_size = len_contig_rev % chunk_size\n\n            min_chunk_size = min(\n                current_chunk_forward_size, current_chunk_reverse_size\n            )\n\n            chunk_test = min_chunk_size >= size_chunk_threshold\n\n            if mapq_test and length_test and chunk_test:\n\n                chunk_forward = position_forward // chunk_size\n                chunk_reverse = position_reverse // chunk_size\n\n                chunk_name_forward = \"{}_{}\".format(\n                    contig_name_forward, chunk_forward\n                )\n                chunk_name_reverse = \"{}_{}\".format(\n                    contig_name_reverse, chunk_reverse\n                )\n\n                if self_contacts or chunk_name_forward != chunk_name_reverse:\n\n                    contact = tuple(\n                        sorted((chunk_name_forward, chunk_name_reverse))\n                    )\n\n                    all_contacts[contact] += 1\n\n                    chunk_key_forward = (\n                        chunk_name_forward,\n                        current_chunk_forward_size,\n                    )\n                    all_chunks[chunk_key_forward] += 1\n\n                    chunk_key_reverse = (\n                        chunk_name_reverse,\n                        current_chunk_reverse_size,\n                    )\n                    all_chunks[chunk_key_reverse] += 1\n\n    logger.info(\"Writing chunk data...\")\n\n    # Now we can update the chunk dictionary\n    # with the info we gathered from the BAM file\n\n    output_chunk_data_path = os.path.join(output_dir, output_file_chunk_data)\n\n    with open(output_chunk_data_path, \"w\") as chunk_data_file_handle:\n\n        for name in sorted(chunk_complete_data.keys()):\n\n            chunk_data = chunk_complete_data[name]\n            size = chunk_data[\"size\"]\n            chunk = (name, chunk_data[\"size\"])\n            hit = all_chunks[chunk]\n            coverage = hit * read_size * 1.0 / size\n            try:\n                chunk_complete_data[name][\"hit\"] = hit\n                chunk_complete_data[name][\"coverage\"] = coverage\n            except KeyError:\n                logger.error(\n                    \"A mismatch was detected between the reference \"\n                    \"genome and the genome used for the alignment \"\n                    \"file, some sequence names were not found\"\n                )\n                raise\n\n            idx = chunk_complete_data[name][\"id\"]\n            line = \"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\n                idx, name, hit, size, coverage\n            )\n            chunk_data_file_handle.write(line)\n\n    # Lastly, generate the network proper\n\n    logger.info(\"Writing network...\")\n\n    output_network_path = os.path.join(output_dir, output_file_network)\n\n    with open(output_network_path, \"w\") as network_file_handle:\n\n        for chunks in sorted(all_contacts.keys()):\n\n            chunk_name1, chunk_name2 = chunks\n            contact_count = all_contacts[chunks]\n\n            if normalized:\n                coverage1 = chunk_complete_data[chunk_name1][\"coverage\"]\n                coverage2 = chunk_complete_data[chunk_name2][\"coverage\"]\n                mean_coverage = np.sqrt(coverage1 * coverage2)\n                effective_count = contact_count * 1.0 / mean_coverage\n            else:\n                effective_count = contact_count\n\n            try:\n                idx1 = chunk_complete_data[chunk_name1][\"id\"]\n                idx2 = chunk_complete_data[chunk_name2][\"id\"]\n                line = \"{}\\t{}\\t{}\\n\".format(idx1, idx2, effective_count)\n                network_file_handle.write(line)\n            except KeyError as e:\n                logger.warning(\"Mismatch detected: %s\", e)\n\n    return chunk_complete_data, all_contacts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_networks(output_file=\"merged_network.txt\", *files):\n\n    contacts = dict()\n    for network_file in files:\n        with open(network_file) as network_file_handle:\n            for line in network_file_handle:\n                id_a, id_b, n_contacts = line.split(\"\\t\")\n                pair = sorted((id_a, id_b))\n                try:\n                    contacts[pair] += n_contacts\n                except KeyError:\n                    contacts[pair] = n_contacts\n\n    sorted_contacts = sorted(contacts)\n    with open(output_file, \"w\") as output_handle:\n        for index_pair in sorted_contacts:\n            id_a, id_b = index_pair\n            n_contacts = contacts[index_pair]\n            output_handle.write(\"{}\\t{}\\t{}\\n\".format(id_a, id_b, n_contacts))", "response": "A naive implementation for merging two networks into a larger network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_chunk_data(output_file=\"merged_idx_contig_hit_size_cov.txt\", *files):\n\n    chunks = dict()\n    for chunk_file in files:\n        with open(chunk_file) as chunk_file_handle:\n            for line in chunk_file_handle:\n                chunk_id, chunk_name, hit, size, cov = line.split(\"\\t\")\n                try:\n                    chunks[chunk_id][\"hit\"] += hit\n                    chunks[chunk_id][\"cov\"] += cov\n                except KeyError:\n                    chunks[chunk_id] = {\n                        \"name\": chunk_name,\n                        \"hit\": hit,\n                        \"size\": size,\n                        \"cov\": cov,\n                    }\n\n    sorted_chunks = sorted(chunks)\n    with open(output_file, \"w\") as output_handle:\n        for chunk_id in sorted_chunks:\n            my_chunk = chunks[chunk_id]\n            name, hit, size, cov = (\n                my_chunk[\"name\"],\n                my_chunk[\"hit\"],\n                my_chunk[\"size\"],\n                my_chunk[\"cov\"],\n            )\n\n            my_line = \"{}\\t{}\\t{}\\t{}\\t{}\".format(\n                chunk_id, name, hit, size, cov\n            )\n            output_handle.write(my_line)", "response": "This function merges chunk data from different networks into a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alignment_to_reads(\n    sam_merged,\n    output_dir,\n    parameters=DEFAULT_PARAMETERS,\n    save_memory=True,\n    *bin_fasta\n):\n    \"\"\"Generate reads from ambiguous alignment file\n\n    Extract reads found to be mapping an input FASTA bin.\n    If one read maps, the whole pair is extracted and written\n    to the output paired-end FASTQ files. Reads that mapped\n    and weren't part of a pair are kept in a third 'single'\n    file for people who need it (e.g. to get extra paired reads\n    by fetching the opposite one from the original FASTQ library).\n\n    Parameters\n    ----------\n    sam_merged : file, str or pathlib.Path\n        The input alignment file in SAM/BAM format to be processed.\n    output_dir : str or pathlib.Path\n        The output directory to write the network and chunk data into.\n    parameters : dict, optional\n        Parameters for the network to read conversion, similar to\n        alignment_to_network.\n    save_memory : bool, optional\n        Whether to keep the read names into memory or write them in different\n        files, which takes longer but may prevent out-of-memory crashes.\n        Default is True.\n    `*bin_fasta` : file, str or pathlib.Path\n        The bin FASTA files with appropriately named records.\n\n    Returns\n    -------\n    A dictionary of files with read names for each bin if save_memory is True,\n    and a dictionary of the read names lists themselves otherwise.\n\n\n    Note\n    ----\n    This will throw an IOError ('close failed in file object destructor') on\n    exit with older versions of pysam for some reason. It's harmless but\n    you may consider upgrading to a later version of pysam if it comes up in\n    a pipeline.\n    \"\"\"\n\n    #   Just in case file objects are sent as input\n    def get_file_string(file_thing):\n        try:\n            file_string = file_thing.name\n        except AttributeError:\n            file_string = str(file_thing)\n        return file_string\n\n    #   Global set of chunks against which reads are required to\n    #   map - we store them in a tuple that keeps track of the\n    #   original bin each chunk came from so we can reattribute the reads later\n\n    bin_chunks = set()\n    for bin_file in bin_fasta:\n        for record in SeqIO.parse(bin_file, \"fasta\"):\n            bin_chunks.add((get_file_string(bin_file), record.id))\n\n    chunk_size = int(parameters[\"chunk_size\"])\n\n    mapq_threshold = int(parameters[\"mapq_threshold\"])\n\n    def read_name(read):\n        return read.query_name.split()[0]\n\n    #   Since reading a huge BAM file can take up a\n    #   lot of time and resources, we only do it once\n    #   but that requires opening fastq files for writing\n    #   as matching reads get detected along the\n    #   bam and keeping track of which ones are\n    #   currently open.\n\n    def get_base_name(bin_file):\n\n        base_name = \".\".join(os.path.basename(bin_file).split(\".\")[:-1])\n\n        output_path = os.path.join(\n            output_dir, \"{}.readnames\".format(base_name)\n        )\n\n        return output_path\n\n    if save_memory:\n        opened_files = dict()\n    else:\n        read_names = dict()\n\n    with pysam.AlignmentFile(sam_merged, \"rb\") as alignment_merged_handle:\n\n        for (my_read_name, alignment_pool) in itertools.groupby(\n            alignment_merged_handle, read_name\n        ):\n\n            for my_alignment in alignment_pool:\n\n                relative_position = my_alignment.reference_start\n                contig_name = my_alignment.reference_name\n\n                chunk_position = relative_position // chunk_size\n\n                # The 'chunk name' is used to detect macthing positions\n                chunk_name = \"{}_{}\".format(contig_name, chunk_position)\n\n                # But such matching positions have to map acceptably\n                quality_test = my_alignment.mapping_quality > mapq_threshold\n\n                for bin_file in bin_fasta:\n                    chunk_tuple = (bin_file, chunk_name)\n                    if chunk_tuple in bin_chunks and quality_test:\n                        if save_memory:\n                            output_path = get_base_name(bin_file)\n                            try:\n                                output_handle = opened_files[bin_file]\n                            except KeyError:\n                                output_handle = open(output_path, \"w\")\n                                opened_files[bin_file] = output_handle\n\n                            output_handle.write(\"@{}\\n\".format(my_read_name))\n\n                        else:\n                            try:\n                                read_names[my_read_name].append(bin_file)\n                            except KeyError:\n                                read_names[my_read_name] = [bin_file]\n\n    for file_handle in opened_files.values():\n        file_handle.close()\n    #   Return unpaired file names for pair_unpaired_reads() to process\n    if save_memory:\n        return opened_files.keys()\n    else:\n        return read_names", "response": "Generate reads from ambiguous alignment file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear(self):\n        '''\n        Method which resets any variables held by this class, so that the parser can be used again\n        :return: Nothing\n        '''\n\n        self.tags = []\n        '''the current list of tags which have been opened in the XML file'''\n\n        self.chars = {}\n        '''the chars held by each tag, indexed by their tag name'''\n\n        self.attribs = {}\n        '''the attributes of each tag, indexed by their tag name'''\n\n        self.handler = None\n        ''' the method which will handle the current tag, and the data currently in the class '''\n\n        self.piece = PieceTree.PieceTree()\n        '''the class tree top'''\n\n        self.isDynamic = False\n        '''Indicator of whether the current thing being processed is a dynamic'''\n\n        self.data[\"note\"] = None\n        self.data[\"direction\"] = None\n        self.data[\"expression\"] = None\n        self.data[\"degree\"] = None\n        self.data[\"frame_note\"] = None\n        self.data[\"staff_id\"] = 1\n        self.data[\"voice\"] = 1\n        self.data[\"handleType\"] = \"\"", "response": "This method resets all the variables held by this class so that the parser can be used again."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef StartTag(self, name, attrs):\n        '''\n        A method which is called by the SAX parser when a new tag is encountered\n        :param name: name of the tag\n        :param attrs: the tag's attributes\n        :return: none, side effect of modifying bits of the current class\n        '''\n        if name not in self.excluded:\n            if name in self.structure.keys():\n                self.handler = self.structure[name]\n\n            self.tags.append(name)\n            if attrs is not None:\n                self.attribs[name] = attrs\n            self.isDynamic = CheckDynamics(name)\n            if self.isDynamic and \"dynamics\" in self.tags:\n                self.handler(\n                    self.tags, self.attribs, self.chars, self.piece, self.data)\n            if name in self.closed_tags and self.handler is not None:\n                self.handler(\n                    self.tags, self.attribs, self.chars, self.piece, self.data)", "response": "This method is called by the SAX parser when a new tag is encountered."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CopyNote(self, part, measure_id, new_note):\n        '''\n         handles copying the latest note into the measure note list.\n         done at end of note loading to make sure staff_id is right as staff id could be encountered\n         any point during the note tag\n        :param part: the part class to copy it into\n        :param measure_id: the id of the measure in which the note belongs\n        :param new_note: the new note class to be copied in\n        :return: None, side effects modifying the piece tree\n        '''\n\n        if part.getMeasure(measure_id, self.data[\"staff_id\"]) is None:\n            part.addEmptyMeasure(measure_id, self.data[\"staff_id\"])\n        measure = part.getMeasure(measure_id, self.data[\"staff_id\"])\n        voice_obj = measure.getVoice(self.data[\"voice\"])\n        if voice_obj is None:\n            measure.addVoice(id=self.data[\"voice\"])\n            voice_obj = measure.getVoice(self.data[\"voice\"])\n        add = True\n        notes = voice_obj.GetChildrenIndexes()\n        for n in notes:\n            no = voice_obj.GetChild(n)\n            if new_note == no:\n                add = False\n                break\n        if add:\n            chord = False\n            if hasattr(new_note, \"chord\"):\n                chord = new_note.chord\n\n            measure.addNote(new_note, self.data[\"voice\"], chord=chord)\n            if hasattr(\n                    new_note, \"BarlinesAndMarkersRest\") and new_note.BarlinesAndMarkersRest:\n                measure.rest = True\n                voice_obj.rest = True", "response": "Copies the latest note into the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreleases the task from the queue.", "response": "async def release(self, *, delay=None):\n        \"\"\"\n            Release task (return to queue) with delay if specified\n\n            :param delay: Time in seconds before task will become ready again\n            :return: Task instance\n        \"\"\"\n        return await self._tube.release(self._task_id, delay=delay)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cluster(self, label):\n        for cluster in self._clusters:\n            if label == cluster['label']:\n                return self._get_connection(cluster)\n        raise AttributeError('No such cluster %s.' % label)", "response": "Returns a connection to a mongo - clusters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the provided configuration is valid.", "response": "def _validate_config(config):\n        \"\"\"Validate that the provided configurtion is valid.\n\n        Each dictionary in the configuration list must have the following\n        mandatory entries :\n            {label: {host(string), port(int), dbpath(string|list of strings)}}\n        It can also contain 1 optional key:\n            {read_preference(string)}\n\n        Args:\n            config: the list of configurations provided at instantiation\n\n        Raises:\n            TypeError: a fault in the configurations is found\n        \"\"\"\n        if not isinstance(config, list):\n            raise TypeError('Config must be a list')\n\n        for config_dict in config:\n            if not isinstance(config_dict, dict):\n                raise TypeError('Config must be a list of dictionaries')\n            label = config_dict.keys()[0]\n            cfg = config_dict[label]\n            if not isinstance(cfg, dict):\n                raise TypeError('Config structure is broken')\n\n            if 'host' not in cfg:\n                raise TypeError('Config entries must have a value for host')\n            if not isinstance(cfg['host'], str) and not isinstance(cfg['host'], list):\n                raise TypeError('Host must be a string or a list.')\n\n            if 'port' not in cfg:\n                raise TypeError('Config entries must have a value for port')\n            if not isinstance(cfg['port'], int):\n                raise TypeError('Port must be an int')\n\n            if 'dbpath' not in cfg:\n                raise TypeError('Config entries must have a value for dbpath')\n            if not isinstance(cfg['dbpath'], str):\n                if not isinstance(cfg['dbpath'], list):\n                    raise TypeError('Dbpath must either a string or a list of '\n                                    'strings')\n                for dbpath in cfg['dbpath']:\n                    if not isinstance(dbpath, str):\n                        raise TypeError('Dbpath must either a string or a list '\n                                        'of strings')\n\n            if ('read_preference' in cfg and\n                not isinstance(cfg['read_preference'], str)):\n                raise TypeError('Read_preference must be a string')\n\n            if ('replicaSet' in cfg and\n                not isinstance(cfg['replicaSet'], str)):\n                raise TypeError('replicaSet must be a string')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_configs(self, config):\n        for config_dict in config:\n            label = config_dict.keys()[0]\n            cfg = config_dict[label]\n            # Transform dbpath to something digestable by regexp.\n            dbpath = cfg['dbpath']\n            pattern = self._parse_dbpath(dbpath)\n\n            read_preference = cfg.get('read_preference', 'primary').upper()\n            read_preference = self._get_read_preference(read_preference)\n\n            # Put all parameters that could be passed to pymongo.MongoClient\n            # in a separate dict, to ease MongoClient creation.\n            cluster_config = {\n                'params': {\n                    'host': cfg['host'],\n                    'port': cfg['port'],\n                    'read_preference': read_preference,\n                    'replicaSet': cfg.get('replicaSet')\n                },\n                'pattern': pattern,\n                'label': label\n            }\n\n            self._clusters.append(cluster_config)", "response": "Parses the list of configuration dictionaries passed by the user and creates a dict with information to connect to Clusters and matching database names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_dbpath(dbpath):\n        if isinstance(dbpath, list):\n            # Support dbpath param as an array.\n            dbpath = '|'.join(dbpath)\n\n        # Append $ (end of string) so that twit will not match twitter!\n        if not dbpath.endswith('$'):\n            dbpath = '(%s)$' % dbpath\n\n        return dbpath", "response": "Converts the dbpath to a regexp pattern."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_read_preference(read_preference):\n        read_preference = getattr(pymongo.ReadPreference, read_preference, None)\n        if read_preference is None:\n            raise ValueError('Invalid read preference: %s' % read_preference)\n        return read_preference", "response": "Converts read_preference from string to pymongo. ReadPreference value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_timeout(self, network_timeout):\n        # Do nothing if attempting to set the same timeout twice.\n        if network_timeout == self._network_timeout:\n            return\n        self._network_timeout = network_timeout\n        self._disconnect()", "response": "Set the timeout for existing and future Clients."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisconnecting from all MongoDB Clients.", "response": "def _disconnect(self):\n        \"\"\"Disconnect from all MongoDB Clients.\"\"\"\n        for cluster in self._clusters:\n            if 'connection' in cluster:\n                connection = cluster.pop('connection')\n                connection.close()\n        # Remove all attributes that are database names so that next time\n        # when they are accessed, __getattr__ will be called and will create\n        # new Clients\n        for dbname in self._mapped_databases:\n            self.__delattr__(dbname)\n        self._mapped_databases = []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_connection(self, cluster):\n        # w=1 because:\n        # http://stackoverflow.com/questions/14798552/is-mongodb-2-x-write-concern-w-1-truly-equals-to-safe-true\n        if 'connection' not in cluster:\n            cluster['connection'] = self._connection_class(\n                socketTimeoutMS=self._network_timeout,\n                w=1,\n                j=self.j,\n                **cluster['params'])\n\n        return cluster['connection']", "response": "Return a connection to a Cluster."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _match_dbname(self, dbname):\n        for config in self._clusters:\n            if re.match(config['pattern'], dbname):\n                return config\n        raise Exception('No such database %s.' % dbname)", "response": "Map a database name to the Cluster that holds the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef try_ntime(max_try, func, *args, **kwargs):\n    if max_try < 1:\n        raise ValueError\n\n    for i in range(max_try):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            last_exception = e\n\n    raise last_exception", "response": "Try execute a function n times until no exception raised or tried\n    is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef highlightjs_javascript(jquery=None):\n\n    javascript = ''\n    # See if we have to include jQuery\n    if jquery is None:\n        jquery = get_highlightjs_setting('include_jquery', False)\n    if jquery:\n        url = highlightjs_jquery_url()\n        if url:\n            javascript += '<script src=\"{url}\"></script>'.format(url=url)\n    url = highlightjs_url()\n    if url:\n        javascript += '<script src=\"{url}\"></script>'.format(url=url)\n    javascript += '<script>hljs.initHighlightingOnLoad();</script>'\n    return javascript", "response": "Return HTML for highlightjs JavaScript."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toLily(self):\n        '''\n        Method which converts the object instance, its attributes and children to a string of lilypond code\n\n        :return: str of lilypond code\n        '''\n\n        lilystring = \"\"\n\n        if not self.autoBeam:\n            lilystring += \"\\\\autoBeamOff\"\n        children = self.SortedChildren()\n        if not hasattr(self, \"transpose\"):\n            self.transpose = None\n        for child in range(len(children)):\n            measureNode = self.GetChild(children[child])\n            measureNode.autoBeam = self.autoBeam\n            lilystring += \" % measure \" + str(children[child]) + \"\\n\"\n            lilystring += measureNode.toLily() + \"\\n\\n\"\n        return lilystring", "response": "Method which converts the object instance its attributes and children to a string of lilypond code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef repo(name: str, owner: str) -> snug.Query[dict]:\n    return json.loads((yield f'/repos/{owner}/{name}').content)", "response": "a repository lookup by owner and name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self, timeout=None):\n\n        started = super(Node, self).start(timeout=timeout)\n        if started:\n            # TODO : return something produced in the context manager passed\n            return self._svc_address  # returning the zmp url as a way to connect to the node\n            # CAREFUL : doesnt make sense if this node only run a one-time task...\n        # TODO: futures and ThreadPoolExecutor (so we dont need to manage the pool ourselves)\n        else:\n            return False", "response": "Start the child process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef repo(name: str, owner: str) -> snug.Query[dict]:\n    request = snug.GET(f'https://api.github.com/repos/{owner}/{name}')\n    response = yield request\n    return json.loads(response.content)", "response": "a repo lookup by owner and name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef taskinfo(self):\n\n        task_input = {'taskName': 'QueryTask',\n                      'inputParameters': {\"Task_Name\": self._name}}\n\n        info = taskengine.execute(task_input, self._engine, cwd=self._cwd)\n\n        task_def = info['outputParameters']['DEFINITION']\n\n        task_def['name'] = str(task_def.pop('NAME'))\n        task_def['description'] = str(task_def.pop('DESCRIPTION'))\n        task_def['displayName'] = str(task_def.pop('DISPLAY_NAME'))\n\n        if 'COMMUTE_ON_SUBSET' in task_def:\n            task_def['commute_on_subset'] = task_def.pop('COMMUTE_ON_SUBSET')\n        if 'COMMUTE_ON_DOWNSAMPLE' in task_def:\n            task_def['commute_on_downsample'] = task_def.pop('COMMUTE_ON_DOWNSAMPLE')\n\n        # Convert PARAMETERS into a list instead of a dictionary\n        # which matches the gsf side things\n        task_def['parameters'] = \\\n            [v for v in task_def['PARAMETERS'].values()]\n        task_def.pop('PARAMETERS')\n\n        parameters = task_def['parameters']\n        for parameter in parameters:\n            parameter['name'] = str(parameter.pop('NAME'))\n            parameter['description'] = str(parameter.pop('DESCRIPTION'))\n            parameter['display_name'] = str(parameter.pop('DISPLAY_NAME'))\n            parameter['required'] = bool(parameter.pop('REQUIRED'))\n\n            if 'MIN' in parameter:\n                parameter['min'] = parameter.pop('MIN')\n\n            if 'MAX' in parameter:\n                parameter['max'] = parameter.pop('MAX')\n\n            if parameter['TYPE'].count('['):\n                parameter['type'], parameter['dimensions'] = parameter.pop('TYPE').split('[')\n                parameter['dimensions'] = '[' + parameter['dimensions']\n                parameter['type'] = str(parameter['type'])\n            else:\n                parameter['type'] = str(parameter.pop('TYPE').split('ARRAY')[0])\n\n            if 'DIMENSIONS' in parameter:\n                parameter['dimensions'] = parameter.pop('DIMENSIONS')\n\n            if 'DIRECTION' in parameter:\n                parameter['direction'] = parameter.pop('DIRECTION').lower()\n\n            if 'DEFAULT' in parameter:\n                if parameter['DEFAULT'] is not None:\n                    parameter['default_value'] = parameter.pop('DEFAULT')\n                else:\n                    parameter.pop('DEFAULT')\n\n            if 'CHOICE_LIST' in parameter:\n                if parameter['CHOICE_LIST'] is not None:\n                    parameter['choice_list'] = parameter.pop('CHOICE_LIST')\n                else:\n                    parameter.pop('CHOICE_LIST')\n\n            if 'FOLD_CASE' in parameter:\n                parameter['fold_case'] = parameter.pop('FOLD_CASE')\n\n            if 'AUTO_EXTENSION' in parameter:\n                parameter['auto_extension'] = parameter.pop('AUTO_EXTENSION')\n\n            if 'IS_TEMPORARY' in parameter:\n                parameter['is_temporary'] = parameter.pop('IS_TEMPORARY')\n\n            if 'IS_DIRECTORY' in parameter:\n                parameter['is_directory'] = parameter.pop('IS_DIRECTORY')\n\n        return task_def", "response": "Retrieve the Task Information"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfixes the argument only of the tuple.", "response": "def fix_argument_only(self):\r\n        '''\r\n        fix_argument_only() -> Either or Unit(Argument)\r\n        `<arg> | ARG | <arg3>` ->\r\n            `Required(Argument('<arg>', 'ARG', '<arg3>'))`\r\n        `[<arg>] | [ARG] | [<arg3>]` ->\r\n            `Optional(Argument('<arg>', 'ARG', '<arg3>'))`\r\n        `(<arg>) | [ARG]` -> not change, return self\r\n        `-a | --better` -> not change\r\n        '''\r\n        # for idx, branch in enumerate(self):\r\n        #     if isinstance(branch[0], Either):\r\n        #         self[idx] = branch.fix()\r\n        first_type = type(self[0])\r\n        if first_type not in (Required, Optional):\r\n            return self\r\n        for branch in self:\r\n            if not (len(branch) == 1 and\r\n                    isinstance(branch, first_type) and\r\n                    isinstance(branch[0], Argument)):\r\n                logger.debug('fix %r not change', self)\r\n                return self\r\n        else:\r\n            first = self[0][0]\r\n            for each in self:\r\n                first.names.update(each[0].names)\r\n            result = first_type(first)\r\n            logger.debug('fix %r -> %r', self, result)\r\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef despeckle_simple(B, th2=2):\n\n    A = np.copy(B)\n    n1 = A.shape[0]\n    dist = {u: np.diag(A, u) for u in range(n1)}\n\n    medians, stds = {}, {}\n    for u in dist:\n        medians[u] = np.median(dist[u])\n        stds[u] = np.std(dist[u])\n\n    for nw, j in itertools.product(range(n1), range(n1)):\n        lp = j + nw\n        kp = j - nw\n        if lp < n1:\n            if A[j, lp] > medians[nw] + th2 * stds[nw]:\n                A[j, lp] = medians[nw]\n        if kp >= 0:\n            if A[j, kp] > medians[nw] + th2 * stds[nw]:\n                A[j, kp] = medians[nw]\n    return A", "response": "Single - chromosome despeckling function on a single chromomsome."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef despeckle_global(M, positions=None, stds=2):\n\n    N = np.array(M, dtype=np.float64)\n    n, m = M.shape\n    if positions is None:\n        positions = range(min(n, m))\n\n    lengths = np.abs(np.diff(positions))\n\n    def distance(i, j):\n        mean_length = (lengths[i] + lengths[j]) / 2.\n        if positions[i] < positions[j]:\n            d = ((positions[j] - positions[i] - lengths[i]) + mean_length) / \\\n                1000.\n        else:\n            d = ((positions[i] - positions[j] - lengths[j]) + mean_length) / \\\n                1000.\n        return d\n\n    measurements, bins = {}, []\n    for i in range(n):\n        for j in range(1, i):\n            d = distance(i, j)\n            bins.append(np.abs(d))\n            try:\n                measurements[np.abs(d)].append(M[i, j])\n            except KeyError:\n                measurements[np.abs(d)] = [M[i, j]]\n\n    mean = [np.mean(np.array(measurements[k])) for k in measurements.keys()]\n    std = [np.std(np.array(measurements[k])) for k in measurements.keys()]\n\n    for i, j in itertools.product(range(stds, n - stds),\n                                  range(stds, m - stds)):\n        d = distance(i, j)\n        if M[i, j] >= mean[d] + std * stds:\n            N[i, j] = mean[d]\n\n    return (N + N.T) / 2", "response": "Compute a trend by averaging all contacts of equal\n    distance then sets outstanding values above stds standard\n    deviations to the expected value from the trend."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreplacing outstanding values (above stds standard deviations) in a matrix by the average of a surrounding window of desired width.", "response": "def despeckle_local(M, stds=2, width=2):\n    \"\"\"Replace outstanding values (above stds standard deviations)\n    in a matrix by the average of a surrounding window of desired width.\n    \"\"\"\n\n    N = np.array(M, dtype=np.float64)\n    n, m = M.shape\n    for i, j in itertools.product(range(width, n - width),\n                                  range(width, m - width)):\n        square = M[i - width:i + width, j - width:j + width]\n        avg = np.average(square)\n        std = np.std(square)\n        if M[i, j] >= avg + stds * std:\n            N[i, j] = avg\n    return (N + N.T) / 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bin_dense(M, subsampling_factor=3):\n\n    m = min(M.shape)\n    n = (m // subsampling_factor) * subsampling_factor\n\n    if n == 0:\n        return np.array([M.sum()])\n\n    N = np.array(M[:n, :n], dtype=np.float64)\n    N = N.reshape(n // subsampling_factor, subsampling_factor,\n                  n // subsampling_factor, subsampling_factor).sum(axis=(1, 3))\n    if m > n:\n        remaining_row = M[n:, :n]\n        remaining_col = M[:n, n:]\n        remaining_square = M[n:m, n:m]\n        R = remaining_row.reshape(m % subsampling_factor,\n                                  m // subsampling_factor,\n                                  subsampling_factor).sum(axis=(0, 2))\n        C = remaining_col.T.reshape(m % subsampling_factor,\n                                    m // subsampling_factor,\n                                    subsampling_factor).sum(axis=(0, 2)).T\n        S = remaining_square.sum()\n        N = np.append(N, [R], axis=0)\n        result = np.append(N, np.array([list(C) + [S]]).T, axis=1)\n    else:\n        result = N\n\n    return result", "response": "Sum over each block of given subsampling factor returns a matrix whose dimensions are this much as small"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bin_sparse(M, subsampling_factor=3):\n\n    try:\n        from scipy.sparse import coo_matrix\n    except ImportError as e:\n        print(str(e))\n        print(\"I am peforming dense binning by default.\")\n        return bin_dense(M.todense())\n\n    N = M.tocoo()\n    n, m = N.shape\n    row, col, data = N.row, N.col, N.data\n\n    # Divide row and column indices - duplicate coordinates are added in\n    # sparse matrix construction\n\n    binned_row = row // subsampling_factor\n    binned_col = col // subsampling_factor\n    binned_n = n // subsampling_factor\n    binned_m = m // subsampling_factor\n\n    # Attach remaining columns and rows to the last one\n\n    binned_row[binned_row >= binned_n] -= n % subsampling_factor\n    binned_col[binned_col >= binned_m] -= m % subsampling_factor\n\n    result = coo_matrix((data, (binned_row, binned_col)),\n                        shape=(binned_n, binned_m))\n    return result", "response": "Perform the bin_dense procedure for sparse matrices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bin_matrix(M, subsampling_factor=3):\n\n    try:\n        from scipy.sparse import issparse\n        if issparse(M):\n            return bin_sparse(M, subsampling_factor=subsampling_factor)\n        else:\n            raise ImportError\n    except ImportError:\n        return bin_dense(M, subsampling_factor=subsampling_factor)", "response": "Bin either sparse or dense matrices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform binning on genome annotations such as contig information or bin positions.", "response": "def bin_annotation(annotation=None, subsampling_factor=3):\n    \"\"\"Perform binning on genome annotations such as contig information or bin\n    positions.\n    \"\"\"\n\n    if annotation is None:\n        annotation = np.array([])\n    n = len(annotation)\n    binned_positions = [annotation[i] for i in range(n) if\n                        i % subsampling_factor == 0]\n    if len(binned_positions) == 0:\n        binned_positions.append(0)\n    return np.array(binned_positions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms binning on genome - wide measurements by summing each component in a window of variable size subsampling_factor.", "response": "def bin_measurement(measurement=None, subsampling_factor=3):\n    \"\"\"Perform binning on genome-wide measurements by summing each component\n    in a window of variable size (subsampling_factor).\n    \"\"\"\n\n    subs = int(subsampling_factor)\n    if measurement is None:\n        measurement = np.array([])\n    n = len(measurement)\n    binned_measurement = [measurement[i - subs + 1:i].sum()\n                          for i in range(n) if i % subs == 0 and i > 0]\n    return np.array(binned_measurement)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_pyramid(M, subsampling_factor=3):\n\n    subs = int(subsampling_factor)\n    if subs < 1:\n        raise ValueError(\n            \"Subsampling factor needs to be an integer greater than 1.\")\n    N = [M]\n    while min(N[-1].shape) > 1:\n        N.append(bin_matrix(N[-1], subsampling_factor=subs))\n    return N", "response": "Build a list of numpy arrays that can be used to compute smaller and smaller matrices with bin_dense."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming binning with a fixed genomic length in kilobase pairs.", "response": "def bin_kb_dense(M, positions, length=10, contigs=None):\n    \"\"\"Perform binning with a fixed genomic length in\n    kilobase pairs (kb). Fragments will be binned such\n    that their total length is closest to the specified input.\n    If a contig list is specified, binning will be performed\n    such that fragments never overlap two contigs.\n    \"\"\"\n\n    unit = 10**3\n    ul = unit * length\n    unit = positions / ul\n    n = len(positions)\n    idx = [i for i in range(\n        n - 1) if np.ceil(unit[i]) < np.ceil(unit[i + 1])]\n    binned_positions = positions[idx]\n    m = len(idx) - 1\n    N = np.zeros((m, m))\n    for i in range(m):\n        N[i] = np.array([M[idx[j]:idx[j + 1], idx[i]:idx[i + 1]].sum()\n                        for j in range(m)])\n\n    return N, binned_positions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bin_exact_kb_dense(M, positions, length=10):\n\n    unit = 10**3\n    ul = unit * length\n    units = positions / ul\n    n = len(positions)\n    idx = [i for i in range(\n        n - 1) if np.ceil(units[i]) < np.ceil(units[i + 1])]\n    m = len(idx) - 1\n    N = np.zeros((m, m))\n    remainders = [0] + [np.abs(units[i] - units[i + 1]) for i in range(m)]\n    for i in range(m):\n        N[i] = np.array([(M[idx[j]:idx[j + 1], idx[i]:idx[i + 1]].sum() -\n                         remainders[j] * M[i][j] +\n                         remainders[j + 1] * M[i + 1][j])\n                         for j in range(m)])\n    return N", "response": "Perform the kb - binning procedure with total bin lengths being exactly\n    set to that of the specified input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bin_kb_sparse(M, positions, length=10):\n\n    try:\n        from scipy.sparse import coo_matrix\n    except ImportError as e:\n        print(str(e))\n        print(\"I am peforming dense normalization by default.\")\n        return bin_kb_dense(M.todense(), positions=positions)\n    r = M.tocoo()\n    unit = 10**3\n    ul = unit * length\n    units = positions / ul\n    n = len(positions)\n    indices = np.floor(units)\n    row = [indices[np.floor(i)] for i in r.row / ul]\n    col = [indices[np.floor(j)] for j in r.col / ul]\n    binned_indices = positions[\n        [i for i in range(n - 1) if np.ceil(units[i]) < np.ceil(units[i + 1])]]\n    return coo_matrix((r.data, (row, col))), binned_indices", "response": "Perform the exact kb - binning procedure on a sparse matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef trim_dense(M, n_std=3, s_min=None, s_max=None):\n\n    M = np.array(M)\n    sparsity = M.sum(axis=1)\n    mean = np.mean(sparsity)\n    std = np.std(sparsity)\n    if s_min is None:\n        s_min = mean - n_std * std\n    if s_max is None:\n        s_max = mean + n_std * std\n    elif s_max == 0:\n        s_max = np.amax(M)\n    f = (sparsity > s_min) * (sparsity < s_max)\n    N = M[f][:, f]\n    return N", "response": "By default return a matrix stripped of component\n    vectors whose sparsity is greater than specified number\n    of standard deviations from the mean."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies the trimming procedure to a sparse matrix.", "response": "def trim_sparse(M, n_std=3, s_min=None, s_max=None):\n    \"\"\"Apply the trimming procedure to a sparse matrix.\n    \"\"\"\n\n    try:\n        from scipy.sparse import coo_matrix\n    except ImportError as e:\n        print(str(e))\n        print(\"I am peforming dense normalization by default.\")\n        return trim_dense(M.todense())\n    r = M.tocoo()\n    sparsity = np.array(r.sum(axis=1)).flatten()\n    mean = np.mean(sparsity)\n    std = np.std(sparsity)\n    if s_min is None:\n        s_min = mean - n_std * std\n    if s_max is None:\n        s_max = mean + n_std * std\n    f = (sparsity > s_min) * (sparsity < s_max)\n    indices = [u for u in range(len(r.data)) if f[r.row[u]] and f[r.col[u]]]\n    rows = np.array([r.row[i] for i in indices])\n    cols = np.array([r.col[j] for j in indices])\n    data = np.array([r.data[k] for k in indices])\n\n    N = coo_matrix((data, (rows, cols)))\n    return N"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normalize_dense(M, norm=\"frag\", order=1, iterations=3):\n\n    s = np.array(M, np.float64)\n    floatorder = np.float64(order)\n\n    if norm == \"SCN\":\n        for _ in range(0, iterations):\n\n            sumrows = s.sum(axis=1)\n            maskrows = (sumrows != 0)[:, None] * (sumrows != 0)[None, :]\n            sums_row = sumrows[:, None] * np.ones(sumrows.shape)[None, :]\n            s[maskrows] = 1. * s[maskrows] / sums_row[maskrows]\n\n            sumcols = s.sum(axis=0)\n            maskcols = (sumcols != 0)[:, None] * (sumcols != 0)[None, :]\n            sums_col = sumcols[None, :] * np.ones(sumcols.shape)[:, None]\n            s[maskcols] = 1. * s[maskcols] / sums_col[maskcols]\n\n    elif norm == \"mirnylib\":\n        try:\n            from mirnylib import numutils as ntls\n            s = ntls.iterativeCorrection(s, iterations)[0]\n        except ImportError as e:\n            print(str(e))\n            print(\"I can't find mirnylib.\")\n            print(\"Please install it from \"\n                  \"https://bitbucket.org/mirnylab/mirnylib\")\n            print(\"I will use default norm as fallback.\")\n            return normalize_dense(M, order=order, iterations=iterations)\n\n    elif norm == \"frag\":\n        for _ in range(1, iterations):\n            s_norm_x = np.linalg.norm(s, ord=floatorder, axis=0)\n            s_norm_y = np.linalg.norm(s, ord=floatorder, axis=1)\n            s_norm = np.tensordot(s_norm_x, s_norm_y, axes=0)\n            s[s_norm != 0] = 1. * s[s_norm != 0] / s_norm[s_norm != 0]\n\n    elif norm == \"global\":\n        s_norm = np.linalg.norm(s, ord=floatorder)\n        s /= 1. * s_norm\n\n    elif callable(norm):\n        s = norm(M)\n\n    else:\n        print(\"Unknown norm. Returning input as fallback\")\n\n    return (s + s.T) / 2", "response": "Apply one of the many normalization types to input dense\n            matrix. Will also apply any callable norms such as a user - made\n            or a lambda function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply a normalization type to a sparse matrix.", "response": "def normalize_sparse(M, norm=\"frag\", order=1, iterations=3):\n    \"\"\"Applies a normalization type to a sparse matrix.\n    \"\"\"\n\n    try:\n        from scipy.sparse import csr_matrix\n    except ImportError as e:\n        print(str(e))\n        print(\"I am peforming dense normalization by default.\")\n        return normalize_dense(M.todense())\n    r = csr_matrix(M)\n    if norm == \"SCN\":\n        for _ in range(1, iterations):\n            row_sums = np.array(r.sum(axis=1)).flatten()\n            col_sums = np.array(r.sum(axis=0)).flatten()\n            row_indices, col_indices = r.nonzero()\n            r.data /= row_sums[row_indices] * col_sums[col_indices]\n\n    elif norm == \"global\":\n        try:\n            from scipy.sparse import linalg\n            r = linalg.norm(M, ord=order)\n        except (ImportError, AttributeError) as e:\n            print(str(e))\n            print(\"I can't import linalg tools for sparse matrices.\")\n            print(\"Please upgrade your scipy version to 0.16.0.\")\n\n    elif callable(norm):\n        r = norm(M)\n\n    else:\n        print(\"Unknown norm. Returning input as fallback\")\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GC_wide(genome, window=1000):\n\n    GC = []\n    from Bio import SeqIO\n    with open(genome) as handle:\n        sequence = \"\".join([str(record.seq)\n                            for record in SeqIO.parse(handle, \"fasta\")])\n\n    n = len(sequence)\n    for i in range(0, n, window):\n        portion = sequence[i:min(i + window, n)]\n        GC.append(GC_partial(portion))\n\n    return GC", "response": "Compute a window of length window across a genome."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting a genome into chunks of fixed size", "response": "def split_genome(genome, chunk_size=10000):\n    \"\"\"Split genome into chunks of fixed size (save the last one).\n    \"\"\"\n\n    chunks = []\n    from Bio import SeqIO\n    with open(genome) as handle:\n        for record in SeqIO.parse(handle, \"fasta\"):\n            sequence = record.seq\n            n = len(sequence)\n            chunks += [str(sequence[i:min(i + chunk_size, n)])\n                       for i in range(0, n, chunk_size)]\n    return np.array(chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_dade_matrix(filename, header=False):\n\n    A = np.genfromtxt(filename, delimiter=\"\\t\", dtype=None, filling_values=0)\n    M, headers = np.array(A[1:, 1:], dtype=np.float64), A[0]\n    matrix = M + M.T - np.diag(np.diag(M))\n    parsed_header = list(\n        zip(*[str(h)[:-1].strip('\"').strip(\"'\").split(\"~\")\n            for h in headers[1:]]))\n    if header:\n        return matrix, parsed_header\n    else:\n        return matrix", "response": "Loads a numpy array from a Dade matrix file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Dade matrix from input numpy matrix. Any annotations are added as header.", "response": "def to_dade_matrix(M, annotations=\"\", filename=None):\n    \"\"\"Returns a Dade matrix from input numpy matrix. Any annotations are added\n    as header. If filename is provided and valid, said matrix is also saved\n    as text.\n    \"\"\"\n\n    n, m = M.shape\n    A = np.zeros((n + 1, m + 1))\n    A[1:, 1:] = M\n    if not annotations:\n        annotations = np.array([\"\" for _ in n], dtype=str)\n    A[0, :] = annotations\n    A[:, 0] = annotations.T\n    if filename:\n        try:\n            np.savetxt(filename, A, fmt='%i')\n            print(\"I saved input matrix in dade format as \" + str(filename))\n        except ValueError as e:\n            print(\"I couldn't save input matrix.\")\n            print(str(e))\n        finally:\n            return A\n\n    return A"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns contact data from a 3D structure.", "response": "def from_structure(structure):\n    \"\"\"Return contact data from a 3D structure (in pdb format).\n    \"\"\"\n\n    try:\n        from Bio import PDB\n        if isinstance(structure, str):\n            p = PDB.PDBParser()\n            structure = p.get_structure('S', structure)\n        if isinstance(structure, PDB.Structure.Structure):\n            for _ in structure.get_chains():\n                atoms = [np.array(atom.get_coord())\n                         for atom in structure.get_atoms()]\n    except ImportError:\n        print(\"Biopython not found.\")\n        raise\n\n    atoms = np.array(structure)\n    try:\n        import scipy\n        D = scipy.spatial.distance.pdist(atoms, 'euclidean')\n        D = scipy.spatial.distance.squareform(D)\n    except ImportError:\n        print(\"Scipy not found.\")\n        raise\n    m = np.max(1 / D[D != 0])\n    M = np.zeros(D.shape)\n    M[D != 0] = 1 / D[D != 0]\n    M[D == 0] = m\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the adjacency matrix of the largest connected component of the given matrix.", "response": "def largest_connected_component(matrix):\n    \"\"\"Compute the adjacency matrix of the largest connected component of the\n    graph whose input matrix is adjacent.\n    \"\"\"\n\n    try:\n        import scipy.sparse\n        n, components = scipy.sparse.csgraph.connected_components(\n            matrix, directed=False)\n        print(\"I found \" + str(n) + \" connected components.\")\n        component_dist = collections.Counter(components)\n        print(\"Distribution of components: \" + str(component_dist))\n        most_common, _ = component_dist.most_common(1)[0]\n        ilcc = (components == most_common)\n        return matrix[:, ilcc][ilcc]\n\n    except ImportError as e:\n        print(\"I couldn't find scipy which is needed for graph routines.\")\n        print(str(e))\n        print(\"Returning input matrix as fallback.\")\n        return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_structure(matrix, alpha=1):\n\n    connected = largest_connected_component(matrix)\n    distances = to_distance(connected, alpha)\n    n, m = connected.shape\n    bary = np.sum(np.triu(distances, 1)) / (n**2)  # barycenters\n    d = np.array(np.sum(distances**2, 0) / n - bary)  # distances to origin\n    gram = np.array([(d[i] + d[j] - distances[i][j]**2) / 2 for i,\n                     j in itertools.product(range(n), range(m))]).reshape(n, m)\n    normalized = gram / np.linalg.norm(gram, 'fro')\n\n    try:\n        symmetric = np.array((normalized + normalized.T) / 2,\n                             dtype=np.longfloat)  # just in case\n    except AttributeError:\n        symmetric = np.array((normalized + normalized.T) / 2)\n\n    from scipy import linalg\n    eigen_values, eigen_vectors = linalg.eigh(symmetric)\n    if not (eigen_values >= 0).all():\n        warnings.warn(\"Negative eigen values were found.\")\n    idx = eigen_values.argsort()[-3:][::-1]\n    values = eigen_values[idx]\n    vectors = eigen_vectors[:, idx]\n    coordinates = vectors * np.sqrt(values)\n    return coordinates", "response": "Compute best matching 3D genome structure from underlying input matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves indices of a trimmed matrix with respect to the original matrix.", "response": "def get_missing_bins(original, trimmed):\n    \"\"\"Retrieve indices of a trimmed matrix with respect to the original matrix.\n    Fairly fast but is only correct if diagonal values are different, which is\n    always the case in practice.\n    \"\"\"\n\n    original_diag = np.diag(original)\n    trimmed_diag = np.diag(trimmed)\n    index = []\n    m = min(original.shape)\n    for j in range(min(trimmed.shape)):\n        k = 0\n        while original_diag[j + k] != trimmed_diag[j] and k < 2 * m:\n            k += 1\n        index.append(k + j)\n    return np.array(index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a PDB file from a structure or matrix.", "response": "def to_pdb(structure, filename, contigs=None, annotations=None,\n           indices=None, special_bins=None):\n    \"\"\"From a structure (or matrix) generate the corresponding pdb file\n    representing each chain as a contig/chromosome and filling the occupancy\n    field with a custom annotation. If the matrix has been trimmed somewhat,\n    remaining indices may be specified.\n    \"\"\"\n\n    n = len(structure)\n    letters = (string.ascii_uppercase + string.ascii_lowercase +\n               string.digits + string.punctuation) * int(n / 94 + 1)\n    if contigs is None:\n        contigs = np.ones(n + 1)\n    if annotations is None:\n        annotations = np.zeros(n + 1)\n    if indices is None:\n        indices = range(n + 1)\n    if special_bins is None:\n        special_bins = np.zeros(n + 1)\n\n    structure_shapes_match = structure.shape[0] == structure.shape[1]\n    if isinstance(structure, np.ndarray) and structure_shapes_match:\n        structure = (to_structure(structure))\n\n    X, Y, Z = (structure[:, i] for i in range(3))\n    Xmax, Ymax, Zmax = (np.max(np.abs(Xi)) for Xi in (X, Y, Z))\n    X *= 100.0 / Xmax\n    Y *= 100.0 / Ymax\n    Z *= 100.0 / Zmax\n\n    X = np.around(X, 3)\n    Y = np.around(Y, 3)\n    Z = np.around(Z, 3)\n\n    reference = [\"OW\", \"OW\", \"CE\", \"TE\", \"tR\"]\n\n    with open(filename, 'w') as f:\n        for i in range(1, n):\n            line = \"ATOM\"  # 1-4 \"ATOM\"\n            line += \"  \"  # 5-6 unused\n            line += str(i).rjust(5)  # 7-11 atom serial number\n            line += \" \"  # 12 unused\n            line += reference[special_bins[i]].rjust(4)  # 13-16 atom name\n            line += \" \"  # 17 alternate location indicator\n            line += \"SOL\"  # 18-20 residue name\n            line += \" \"  # 21 unused\n            line += letters[int(contigs[indices[i]] - 1)\n                            ]  # 22 chain identifier\n            line += str(i).rjust(4)  # 23-26 residue sequence number\n            line += \" \"  # 27 code for insertion of residues\n            line += \"   \"  # 28-30 unused\n            line += str(X[i]).rjust(8)  # 31-38 X orthogonal \u00c5 coordinate\n            line += str(Y[i]).rjust(8)  # 39-46 Y orthogonal \u00c5 coordinate\n            line += str(Z[i]).rjust(8)  # 47-54 Z orthogonal \u00c5 coordinate\n            line += \"1.00\".rjust(6)  # 55-60 Occupancy\n            # 61-66 Temperature factor\n            line += str(annotations[i - 1]).rjust(6)\n            line += \"      \"  # 67-72 unused\n            line += \"    \"  # 73-76 segment identifier\n            line += \"O\".rjust(2)  # 77-78 element symbol\n            line += \"\\n\"\n            f.write(line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute distance matrix from contact data by applying a negative power law to its nonzero pixels then interpolating on the zeroes using a shortest - path algorithm.", "response": "def to_distance(matrix, alpha=1):\n    \"\"\"Compute distance matrix from contact data by applying a negative power\n    law (alpha) to its nonzero pixels, then interpolating on the zeroes using a\n    shortest-path algorithm.\n    \"\"\"\n    matrix = np.array(matrix)\n    try:\n        import scipy.sparse\n    except ImportError as e:\n        print(\"Scipy not found.\")\n        print(str(e))\n        raise\n\n    if callable(alpha):\n        distance_function = alpha\n    else:\n        try:\n            a = np.float64(alpha)\n\n            def distance_function(x):\n                return 1 / (x ** (1 / a))\n        except TypeError:\n            print(\"Alpha parameter must be callable or an array-like\")\n            raise\n\n    if hasattr(matrix, 'getformat'):\n        distances = scipy.sparse.coo_matrix(matrix)\n        distances.data = distance_function(distances.data)\n    else:\n        distances = np.zeros(matrix.shape)\n        distances[matrix != 0] = distance_function(1 / matrix[matrix != 0])\n\n    return scipy.sparse.csgraph.floyd_warshall(distances, directed=False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distance_to_contact(D, alpha=1):\n\n    if callable(alpha):\n        distance_function = alpha\n    else:\n        try:\n            a = np.float64(alpha)\n\n            def distance_function(x):\n                return 1 / (x ** (1 / a))\n        except TypeError:\n            print(\"Alpha parameter must be callable or an array-like\")\n            raise\n        except ZeroDivisionError:\n            raise ValueError(\"Alpha parameter must be non-zero\")\n\n    m = np.max(distance_function(D[D != 0]))\n    M = np.zeros(D.shape)\n    M[D != 0] = distance_function(D[D != 0])\n    M[D == 0] = m\n    return M", "response": "Compute contact matrix from distance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distance_law(matrix, log_bins=False):\n\n    D = np.array([np.average(np.diagonal(matrix, j))\n                  for j in range(min(matrix.shape))])\n    if not log_bins:\n        return D\n    else:\n        n = min(matrix.shape)\n        n_bins = int(np.log(n) / np.log(2) + 1)\n        logD = np.array([np.average(D[int(2**(i - 1)):min(n, 2**i)])\n                         for i in range(n_bins)])\n        return logD", "response": "Compute distance law as a function of the genomic coordinate aka P ( s )."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pdb_to_structure(filename):\n\n    try:\n        from Bio.PDB import PDB\n    except ImportError:\n        print(\"I can't import Biopython which is needed to handle PDB files.\")\n        raise\n    p = PDB.PDBParser()\n    structure = p.get_structure('S', filename)\n    for _ in structure.get_chains():\n        atoms = [np.array(atom.get_coord()) for atom in structure.get_atoms()]\n    return atoms", "response": "Import a structure object from a PDB file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nflatten and converts a positions array to a contigs array if applicable.", "response": "def positions_to_contigs(positions):\n    \"\"\"Flattens and converts a positions array to a contigs array, if applicable.\n    \"\"\"\n\n    if isinstance(positions, np.ndarray):\n        flattened_positions = positions.flatten()\n    else:\n        try:\n            flattened_positions = np.array(\n                [pos for contig in positions for pos in contig])\n        except TypeError:\n            flattened_positions = np.array(positions)\n\n    if (np.diff(positions) == 0).any() and not (0 in set(positions)):\n        warnings.warn(\"I detected identical consecutive nonzero values.\")\n        return positions\n\n    n = len(flattened_positions)\n    contigs = np.ones(n)\n    counter = 0\n    for i in range(1, n):\n        if positions[i] == 0:\n            counter += 1\n            contigs[i] += counter\n        else:\n            contigs[i] = contigs[i - 1]\n    return contigs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef distance_diagonal_law(matrix, positions=None):\n\n    n = min(matrix.shape)\n    if positions is None:\n        return np.array([np.average(np.diagonal(matrix, j)) for j in range(n)])\n    else:\n        contigs = positions_to_contigs(positions)\n\n    def is_intra(i, j):\n        return contigs[i] == contigs[j]\n\n    max_intra_distance = max((len(contigs == u) for u in set(contigs)))\n\n    intra_contacts = []\n    inter_contacts = [np.average(np.diagonal(matrix, j))\n                      for j in range(max_intra_distance, n)]\n    for j in range(max_intra_distance):\n        D = np.diagonal(matrix, j)\n        for i in range(len(D)):\n            diagonal_intra = []\n            if is_intra(i, j):\n                diagonal_intra.append(D[i])\n#            else:\n#                diagonal_inter.append(D[i])\n#        inter_contacts.append(np.average(np.array(diagonal_inter)))\n        intra_contacts.append(np.average(np.array(diagonal_intra)))\n\n    intra_contacts.extend(inter_contacts)\n\n    return [positions, np.array(intra_contacts)]", "response": "Compute a distance law trend using the contact averages of equal distances."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rippe_parameters(matrix, positions, lengths=None, init=None, circ=False):\n\n    n, _ = matrix.shape\n\n    if lengths is None:\n        lengths = np.abs(np.diff(positions))\n\n    measurements, bins = [], []\n    for i in range(n):\n        for j in range(1, i):\n            mean_length = (lengths[i] + lengths[j]) / 2.\n            if positions[i] < positions[j]:\n                d = (((positions[j] - positions[i] -\n                     lengths[i]) + mean_length) /\n                     1000.)\n            else:\n                d = (((positions[i] - positions[j] -\n                     lengths[j]) + mean_length) /\n                     1000.)\n\n            bins.append(np.abs(d))\n            measurements.append(matrix[i, j])\n    parameters = estimate_param_rippe(measurements, bins, init=init, circ=circ)\n    print(parameters)\n    return parameters[0]", "response": "Estimate parameters from the model described in Rippe et al. 2001."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef estimate_param_rippe(measurements, bins, init=None, circ=False):\n\n    # Init values\n    DEFAULT_INIT_RIPPE_PARAMETERS = [1., 9.6, -1.5]\n    d = 3.\n\n    def log_residuals(p, y, x):\n        kuhn, lm, slope, A = p\n        rippe = (np.log(A) + np.log(0.53) - 3 * np.log(kuhn) +\n                 slope * (np.log(lm * x) - np.log(kuhn)) +\n                 (d - 2) / ((np.power((lm * x / kuhn), 2) + d)))\n        err = y - rippe\n\n        return err\n\n    def peval(x, param):\n\n        if circ:\n            l_cont = x.max()\n            n = param[1] * x / param[0]\n            n0 = param[1] * x[0] / param[0]\n            n_l = param[1] * l_cont / param[0]\n            s = n * (n_l - n) / n_l\n            s0 = n0 * (n_l - n0) / n_l\n            norm_lin = (param[3] * (0.53 * (param[0] ** -3.) *\n                        np.power(n0, (param[2])) *\n                        np.exp((d - 2) / ((np.power(n0, 2) + d)))))\n\n            norm_circ = (param[3] * (0.53 * (param[0] ** -3.) *\n                         np.power(s0, (param[2])) *\n                         np.exp((d - 2) / ((np.power(s0, 2) + d)))))\n\n            rippe = (param[3] * (0.53 * (param[0] ** -3.) *\n                     np.power(s, (param[2])) * np.exp((d - 2) /\n                     ((np.power(s, 2) + d)))) * norm_lin / norm_circ)\n\n        else:\n\n            rippe = (param[3] * (0.53 * (param[0] ** -3.) *\n                     np.power((param[1] * x / param[0]), (param[2])) *\n                     np.exp((d - 2) /\n                     ((np.power((param[1] * x / param[0]), 2) + d)))))\n\n        return rippe\n\n    if init is None:\n        init = DEFAULT_INIT_RIPPE_PARAMETERS\n\n    A = np.sum(measurements)\n\n    p0 = (p for p in init), A\n    from scipy.optimize import leastsq\n    plsq = leastsq(log_residuals, p0, args=(np.log(measurements), bins))\n\n    y_estim = peval(bins, plsq[0])\n    kuhn_x, lm_x, slope_x, A_x = plsq[0]\n    plsq_out = [kuhn_x, lm_x, slope_x, d, A_x]\n\n    np_plsq = np.array(plsq_out)\n\n    if np.any(np.isnan(np_plsq)) or slope_x >= 0:\n        warnings.warn(\"Problem in parameters estimation\")\n        plsq_out = p0\n\n    return plsq_out, y_estim", "response": "Estimate the rippe parameter for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef null_model(matrix, positions=None, lengths=None,\n               model=\"uniform\", noisy=False, circ=False,\n               sparsity=False):\n    \"\"\"Attempt to compute a 'null model' of the matrix given a model\n    to base itself on.\n    \"\"\"\n\n    n, m = matrix.shape\n    positions_supplied = True\n    if positions is None:\n        positions = range(n)\n        positions_supplied = False\n    if lengths is None:\n        lengths = np.diff(positions)\n\n    N = np.copy(matrix)\n\n    contigs = np.array(positions_to_contigs(positions))\n\n    def is_inter(i, j):\n        return contigs[i] != contigs[j]\n\n    diagonal = np.diag(matrix)\n\n    if model == \"uniform\":\n        if positions_supplied:\n            trans_contacts = np.array([matrix[i, j]\n                                      for i, j in itertools.product(range(n),\n                                      range(m)) if is_inter(i, j)])\n            mean_trans_contacts = np.average(trans_contacts)\n        else:\n            mean_trans_contacts = np.average(matrix) - diagonal / len(diagonal)\n\n        N = np.random.poisson(lam=mean_trans_contacts, size=(n, m))\n        np.fill_diagonal(N, diagonal)\n\n    elif model == \"distance\":\n        distances = distance_diagonal_law(matrix, positions)\n        N = np.array([[distances[min(abs(i - j), n)]\n                       for i in range(n)] for j in range(n)])\n\n    elif model == \"rippe\":\n\n        trans_contacts = np.array([matrix[i, j] for i, j in itertools.product(\n            range(n), range(m)) if is_inter(i, j)])\n        mean_trans_contacts = np.average(trans_contacts)\n        kuhn, lm, slope, d, A = rippe_parameters(matrix, positions, circ=circ)\n\n        def jc(s, frag):\n            dist = s - circ * (s**2) / lengths[frag]\n            computed_contacts = (0.53 * A * (kuhn**(-3.)) *\n                                 (dist ** slope) *\n                                 np.exp((d - 2) / (dist + d)))\n            return np.maximum(computed_contacts, mean_trans_contacts)\n\n        for i in range(n):\n            for j in range(n):\n                if not is_inter(i, j) and i != j:\n                    posi, posj = positions[i], positions[j]\n                    N[i, j] = jc(np.abs(posi - posj) * lm / kuhn, frag=j)\n                else:\n                    N[i, j] = mean_trans_contacts\n\n    if sparsity:\n        contact_sum = matrix.sum(axis=0)\n        n = len(contact_sum)\n        try:\n            from Bio.Statistics import lowess\n            trend = lowess(np.array(range(n), dtype=np.float64),\n                           contact_sum, f=0.03)\n        except ImportError:\n            expected_size = int(np.amax(contact_sum) / np.average(contact_sum))\n            w = min(max(expected_size, 20), 100)\n            trend = np.array([np.average(contact_sum[i:min(i + w, n)]) for\n                              i in range(n)])\n\n        cov_score = np.sqrt((trend - np.average(trend)) / np.std(trend))\n\n        N = ((N * cov_score).T) * cov_score\n\n    if noisy:\n        if callable(noisy):\n            noise = noisy\n        return noise(N)\n    else:\n        return N", "response": "Compute a null model of the given matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trim_structure(struct, filtering=\"cube\", n=2):\n\n    X, Y, Z = (struct[:, i] for i in range(3))\n\n    if filtering == \"sphere\":\n        R = (np.std(X)**2 + np.std(Y)**2 + np.std(Z)**2) * (n**2)\n        f = (X - np.mean(X))**2 + (Y - np.mean(Y))**2 + (Z - np.mean(Z))**2 < R\n\n    if filtering == \"cube\":\n        R = min(np.std(X), np.std(Y), np.std(Z)) * n\n        f = np.ones(len(X))\n        for C in (X, Y, Z):\n            f *= np.abs(C - np.mean(C)) < R\n\n    if filtering == \"percentile\":\n        f = np.ones(len(X))\n        for C in (X, Y, Z):\n            f *= np.abs(C - np.mean(C)\n                        ) < np.percentile(np.abs(C - np.mean(C)), n)\n\n    return np.array([X[f], Y[f], Z[f]])", "response": "Removes outlier atoms from a structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes so - called scalograms used to easily generate contacts at different distance scales.", "response": "def scalogram(M, circ=False):\n    \"\"\"Computes so-called 'scalograms' used to easily\n    visualize contacts at different distance scales.\n    Edge cases have been painstakingly taken\n    care of.\n    \"\"\"\n\n    # Sanity checks\n    if not type(M) is np.ndarray:\n        M = np.array(M)\n\n    if M.shape[0] != M.shape[1]:\n        raise ValueError(\"Matrix is not square.\")\n\n    try:\n        n = min(M.shape)\n    except AttributeError:\n        n = M.size\n\n    N = np.zeros(M.shape)\n    for i in range(n):\n        for j in range(n):\n            if i + j < n and i >= j:\n                N[i, j] = M[i, i - j:i + j + 1].sum()\n            elif circ and i + j < n and i < j:\n                N[i, j] = M[i, i - j:].sum() + M[i, :i + j + 1].sum()\n            elif circ and i >= j and i + j >= n:\n                N[i, j] = M[i, i - j:].sum() + M[i, :i + j - n + 1].sum()\n            elif circ and i < j and i + j >= n:\n                N[i, j] = (M[i, i - j:].sum() +\n                           M[i, :].sum() +\n                           M[i, :i + j - n + 1].sum())\n\n    return N"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef asd(M1, M2):\n\n    from scipy.fftpack import fft2\n    spectra1 = np.abs(fft2(M1))\n    spectra2 = np.abs(fft2(M2))\n\n    return np.linalg.norm(spectra2 - spectra1)", "response": "Compute a Fourier transform based distance between two matrices."}
